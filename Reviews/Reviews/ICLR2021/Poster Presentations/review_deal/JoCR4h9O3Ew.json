{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper focuses on adversarial robustness with unlabeled data. The philosophy behind sounds quite interesting to me, namely, utilizing unlabeled data to enforce labeling consistency while reducing adversarial transferability among the networks via diversity \nregularizers. This philosophy leads to a novel algorithm design I have never seen, i.e., ARMOURED, an adversarially robust training method based on semi-supervised learning.\n\nThe clarity and novelty are clearly above the bar of ICLR. While the reviewers had some concerns on the significance, the authors did a particularly good job in their rebuttal. Thus, all of us have agreed to accept this paper for publication! Please carefully address all \ncomments in the final version.\n"
    },
    "Reviews": [
        {
            "title": "Review: clear paper with solid motivation and experimental evaluation. Some questions about the interpretation of the results.",
            "review": "## Summary\nThis paper introduces a semi-supervised learning procedure that does not require labeled adversarial data to learn an ensemble model that is robust to adversarial attacks on classification tasks.\n\n## Quality\nThis paper is very well written; the design decisions of the training procedures are all supported by ablation tests and comparisons to other modern adversarial baselines. However, I am slightly worried by the ablation study results -- from Table 3, it seems like the DPP component of the loss does not provide a statistically significant lift. \n\n## Clarity\nThis paper is very clearly written! The choice of parameterization, experiment settings, and overall proposed methodology were clearly explained and motivated.\n\n## Originality\nAs the authors have mentioned, improving diversity in ensembles is not a novel approach to improving robustness to adversarial attacks; the originality of the method lies in the parameterization of diversity through a DPP component and multi-view complementarity. \n\n## Significance\nImproving robustness to adversarial attacks is in of itself an important contribution to this field. This paper proposes a method that achieves a significant improvement over other competitors (Tables 1, 2); the proposed method is additionally intuitive, and can incorporate prior beliefs over the the data distribution, making it adaptable to several different settings.\n\n## Questions\n### Methodology\nMy main question lies in the uses of the DPP loss: did the authors use the determinant directly? Depending on the choice of kernel, I would expect the determinant portion of the loss to eclipse the other terms, potentially hampering learning. \n\nOn a related note, I am curious to know how sensitive the ARMOURED performance is to the $\\lambda_{DPP}$ and $\\lambda_{NEM}$ hyper-parameters.\n\n### Experiments\nDo the authors have insight in the significant gap between ARMOURED-B and its variants for a large $\\epsilon$ budget with $L_\\infty$ PGD? I am surprised by the significant gap in Table 3.\n\nAgain in Table 3, comparing the -F, no DPP, and -B variants, it seems like the highest lift in performance comes from the diversity in the NEM component. Do you know if a similar lift is observed by removing the NEM component and keeping the DPP? \n\nAt inference time, you state that the augmentation step is skipped. Naively, I would not have been surprised to see these augmentations improve resilience to adversarial attacks. Am I incorrect?",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Simple and good technique to increase model robustness during training",
            "review": "### **Summary**:\n \nThis work introduces ARMOURED, a new method for learning models that are robust against adversarial attacks. The method uses **multi-view**-learning (as in using multiple models with different parameters to cast their votes/views on a given sample), semi-supervised learning to pseudo-label new data based on a consensus, and a diversity regularizer. The approach is evaluated in CIFAR-10 and SVHN against recent baselines in the presence of white-box attacks. The results yield by ARMOURED in these scenarios are superior.\n \n---\n \n### **Reasons for score**: \n \nMy recommendation is to accept the paper. I like the proposal, it is simple yet is able to produce better results than existing methods. It also performs well with clean samples, which is something that many other methods struggle with. The only thing I miss is to see experiments in ImageNet.  \n \n---\n### **Strengths**:\n \n* The problem is relevant for the community and the proposed approach seems to do a better job than the existing state-of-the-art leveraging weak-supervision/semi-supervision (definitely a plus). \n* The method is simple and seems easy to adapt to other architectures and tasks.\n\n---\n\n### **Weaknesses**:\n\n* The ablation experiments point to the unlabelled data as one of the core reasons for the method to work. I was expecting to see additional experiments with extra unlabelled data to get a better feeling of this component.\n\n---\n\n### **Questions to be discussed during the rebuttal period**:\n\nPlease, address the questions/comments expressed in the **Weaknesses** section. \n\n---\n\n### **Other considerations**:\n\nNone\n\n\n---\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting techniques and findings, but needs stronger adversarial evaluation",
            "review": "**Update** : Since most of my issues have been addressed, I have changed my rating from 6 to 7\n\n\nSummary:\n\nThis paper proposed ARMOURED, a semi-supervised training algorithm that combines multi-view learning to utilize unlabeled data, along with diversity regularization to decrease inter-model transferability for adversarial inputs. As a consequence, the final models achieve near state-of-the-art adversarial robustness, without explicitly generating them during its training process. Based on experiments on the CIFAR10 and SVHN classification tasks, the proposed technique produces models that maintain accuracy on clean as well as adversarially perturbed data.\n\n##########################################################################\n\nReasons for score:\n\nThe proposed regularization terms of using unlabeled data and an ensemble of models with different views to indirectly promote adversarial robustness are quite interesting. However, the absence of evaluation against an adaptive adversary, along with signs of gradient masking makes this work in its current form not ready for publication. If the authors can run tests with an appropriately designed adaptive adversary, along with stronger evaluations (including gradient-free and black-box adversaries), and if the model performs well under those settings, the proposed work would be qualified to be part of the conference proceedings. \n\n##########################################################################\n\nPros:\n\n- The idea of increasing inter-model diversity to decrease transferability between models is a very interesting concept. The fact that it leads to better robustness without explicitly training for it shows that the authors' intuition in using this regularization term is, in fact, sound.\n\n- Using the idea that augmented images are distorted views of the same image and thus their deep views should agree is great! However, I must point out that the idea of using augmentations to enforce consistency on unlabeled data is now new. One example of such work is [Unsupervised Data Augmentation for Consistency Training](https://arxiv.org/pdf/1904.12848.pdf). Please cite this (and/or relevant work) and do include a comparison.\n\n- Using the concept of a stable sample (based on consensus) is great, since it might help filter out noisy labels.  \n\n##########################################################################\n\nCons:\n\n- Since this is primarily a paper that focuses on adversarial robustness (ultimately), I feel some very basic yet important changes are required. In general, the authors could follow the advice in the paper [On Evaluating Adversarial Robustness](https://arxiv.org/pdf/1902.06705.pdf). To be a bit more concrete, I would recommend having at least the following:\n  - Using FGSM to evaluate is not recommended, since it is not nearly as strong an attack as something like PGD. Instead, I would recommend swapping FGSM evaluation with a black-box attack like Boundary++, or some perturbation-minimizing attack like C&W \n  - Since the final model is trained using a combination of loss functions and non-trivial processes, it might be beneficial to run evaluations against an adaptive adversary. Space could be made for it by shifting to Figure 3 to supplementary material.\n\n- Is there any particular reason for skipping augmentation at inference time?\n\n- The part that involves using proper class-level information to incorporate it in the DPP regularizer seems similar to work that uses cost-functions to differentiate between different classes when working with adversarial inputs. One such work is [Cost-Sensitive Robustness against Adversarial Examples](https://arxiv.org/abs/1810.09225). It might be worthwhile to talk about and compare the matrices used in the experiments with the matrices used in this work.\n\n- Although the models do theoretically possess similar learning capabilities, it might be interesting to see if different models end up being more \"contributing\" than others in their predictions. Maybe some analysis might reveal that some augmentations/models are much more useful than the rest (which could be used for pruning), or perhaps trends like 'some models help with clean accuracy, while others help with robustness'. If space/time permits, it would be nice to have such analyses (perhaps in additional materials).\n\n- It has been observed in the literature that the performance of PGD attacks is significantly degraded when random restarts are not used. In your experiments, please run them with random-restarts and report numbers.\n\n- Tables 1&2 have '+-' numbers with them. Why are they there? Are these numbers accuracy, or error rates? Were the attacks run multiple times? If yes, how many? Also, are those numbers s.d? Please clarify.\n\n- The use of unlabeled data along in this case, along with multiple models to have an ensemble effect, seems to be somewhat related to this work [A2-LINK](https://ieeexplore.ieee.org/document/9104705). The authors might benefit from looking at similarities in the algorithm, and if some techniques can be borrowed/adapted to make the current algorithm stronger in its performance.\n\n- The fact that even under strong PGD attacks classes remain recognizable, along with almost unscathed performance for ARMOURED-B for increasing perturbation budgets, strong hints at some form of gradient masking in the final model. Please use black-box evaluation and other methods to make sure this isn't the case. One simple check would be to use [Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks](https://arxiv.org/abs/2003.01690). Their code is publicly available [here](https://github.com/fra31/auto-attack)\n\n########################################################################## \n\nTypos/Minor edits:\n\n- Abstract, last line \"substantial gains in accuracy, while maintaining high accuracy...\". Did the authors mean \"substantial gains in robustness...\"?\n\n- Introduction, third line \"..in the input image..\". Since the definition being given is for adversarial attacks in general, it should be for any data points with human-imperceptible changes, not just images. Also, in the more broad sense, the examples lead to unintended/unexpected behavior, not necessarily \"incorrect classifications\". \n\n- Figure 1: Please do not rely on color to differentiate between different arrows (the reader may be color blind). Either switch to some other attribute (like the width of the arrow) or explicitly label them to differentiate between red and blue.\n\n- Formatting: Table 2 could be shifted elsewhere (with text wraparound) to get more space for the authors to focus on things like adaptive adversaries. The same comment for Figure 1 (a lot of extra space on both sides of the image); could remove the padding?\n\nPlease address and clarify the cons above. Most importantly, it is crucial to have evaluation against stronger attacks as well as an adaptive adversary, before claims of robustness can be substantiated.",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A combination of ADP and multi-view training",
            "review": "This paper considers training an adversarially robust model in a semi-supervised setting. The authors propose an ensemble-based algorithm for this goal. The algorithm uses a regularization term to induce diversity in the ensemble. The algorithm also leverages the idea of multi-view training in semi-supervised learning to make use of unlabeled data. The experimental results show that the proposed algorithm outperforms several baselines.\n\nOverall I enjoyed reading this paper. It is relatively clearly written, and easy to follow. I think the basic idea of this paper is a combination of the ADP algorithm by Pang et al and the classic multi-view idea in semi-supervised learning. In fact, some of the figures in this paper are quite similar to those in Pang et al 2019. It is not surprising that this algorithm outperforms ADP since it makes use of many unlabeled data. This makes me feel that the proposed algorithm does not have enough novelty. However, I do think this paper is beneficial to the adversarial robustness (AR) community since it brings an important idea of multi-view semi-supervised learning to AR research. Therefore, I decided to give a score of weakly accept.\n\n======\nAfter author response: I have read other reviews and the revised version. I think the paper's overall quality has improved. I decided to change my score to 7.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}