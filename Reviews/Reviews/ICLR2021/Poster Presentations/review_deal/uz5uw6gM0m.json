{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper shows how multiple tasks can be encoded in a single neural network without the need for explicit modular construction for each task. The idea is very interesting and the research work presented is of high quality. \n\nAll the reviewers underline their interest in the presented work. However, there is a deviation in the reviewers' score with half voting \nfor acceptance and the other half for rejection. The main concern of the fellow reviewers with the below acceptance threshold score was the difficulty in grapsing the theory of the research presented due to the lack of important content from the main manuscript due to space limitations. The authors have an extended supplementary material that covers the whole magnitude of their work. \n\nI understand the reviewers' concern on how such a dense presentation does not do justice and harms the presented effort itself. However, given the edits the authors added to address the issue rasied and the interest and potential of this work - acknowledged by all the reviewers and myself I recommend acceptance. This is a work of a quality I would like to keep seeing in ICLR. "
    },
    "Reviews": [
        {
            "title": "Perspective is Interesting but Writing is Confusing",
            "review": "This paper takes an interesting theoretical dive on the learnability of multiple tasks as one task, while the tasks are constructed with special structures of the cluster, decision tree, or simple program. This paper provides sample complexities analysis, showing that wide two-layer neural networks with standard activation functions and SGD optimization is able to capture the data regularity between input and output, modulated by the three kinds of task code considered. Experiment results show that the networks are flexible enough to fit complex data generated in this way.  \n\nPros: \n\n* This paper is quite novel in many aspects, including modularity v.s. monolithic, constructing task codes by SQL-style aggregation queries, \"inverse counterpart\" of multitask learning, connections to cognitive science.\n\n* This paper provides impressive sample complexity analysis, with both informal and formal versions.   \n\nCons: \n\n* The clarity of writing is low. There are many places unclear, in particular, the \"task distinctiveness\". If we view it as one complex task, with the input-output data relationship modulated by one of the three task codes, then the questions of \"learnability\" by neural networks in handling this case are well addressed by the proposed theoretical analysis and I can see the empirical study is supportive of this claim. But if we interpret them as multiple tasks, how do we know the network \"is capable of simultaneously learning multiple tasks\" instead of just one task? \n\n* Possible gaps between claims and experiment \n\nRelated to the question above, it seems very attractive to me that \"the two-layer network can jointly learn the task coding scheme and the task-specific functions without special engineering of the architecture\", but how to justify \"task-specific\" functions/task code has been successfully learned? \n\nIt seems in Section 2, it is claimed that \"we wish to learn a function g such that g(c^i; x) = f_i(x) from the examples of the form (c^(i); x, f_i(x))\". Is c known? Is the learned network being able to take task code as input? But it seems the experiment shows f is learnable not g. \n\nQuestions: \n\n* How is the distinctness of tasks defined? Although the examples shown in Figure 1 are quite heterogeneous, examples considered such as  \"learning binary classification for well-separated data\" resembles mixture-of-experts classification, which can be viewed as one task to learn directly. \n\n* How the task coding is determined by a balanced decision tree? The setup in Section 2.3 is quite confusing. Moreover, is the construction of the g function different from the cluster setting? How does the complexity of task coding schemes influence the way the g function is constructed? This paper would benefit from explaining the different complexities added to the learning by different task coding schemes, and how do they make multiple tasks potentially conflict/distinct with each other.   \n\nOverall, I think the monolithic task formulation is novel and the problems the authors would like to address are fundamentally important. I am not from the theoretical machine learning side so I am not sure about the technical contribution of the theorems. The experiments look only partially supportive of the main claims. It is very likely for me to miss something important. The lacking of clarity is a big downside. \n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Nice attempt but very little about the proofs are parsable ",
            "review": "This paper posits a very interesting question about provable multi-task learning by neural nets. The idea is quite interesting to encode the objectives as task codes and then to write a smooth approximation to the predictor function as a weighted sum of indicators and the try training a net to learn this smoothening. But the problem with the paper is that the presentation of the details are extremely unclear and almost nothing about the proofs can be easily followed! Let me cite a few specific issues, \n\n-  The key theorem here seems to be Theorem 8 on page 18 and it follows from Remark 2 which itself follows from Theorem 7 on page 15 whose proof is claimed to be essentially that of Theorem 6 which in turn follows from equation 9 of Corollary 3 which isn't proven here - and this corollary 3 isn't the same as the corollary 6.2 of https://arxiv.org/pdf/1901.08584.pdf because this allows for odd powers and maybe more importantly the definition of H in this corollary 3 has no analogue in 1901.08584 and hence its mysterious to me as to where does this come from! What has the H in this corollary 3 got to do with the H^\\infity of 1901.08584 which is the NTK kernel. Its far from obvious that the proof there will go through here again. So as it stands the information given in the paper makes it difficult to trust that the result is correct. I would strongly suggest resubmitting to a future venue a self-contained paper with all the proofs. This writing here is looking too messed up! \n\n- The argument between equation 22 and 23 also make similar claims about analogy to proofs in the paper https://arxiv.org/pdf/1901.08584.pdf but here its not even clear as to which proof in there is being referred to! \n\n- Around equation 4 a claim is being made that https://arxiv.org/pdf/1901.08584.pdf has training only on the second layer for randomly initialized bottom layer. This doesnt seem to be true! Can the authors point to this assumption anywhere in that paper? 1901.08584 is not a NNGP result! \n\n- The proof of Theorem 14 claims that SGD can be implemented as a SQ algorithm. Can you reference this claim? If not, it would be best to put in the proof here since this isnt obvious at all.   ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good quality and quantity of work but not suitable for conference format.",
            "review": " \nVerdict:\n\nRecommendation to REJECT; Please consider for ICLR Special Journal Issue with modifications.\n\n(Unique situation, please read fully)\n \n\n##########################################################################\n### Summary:\n\nThe paper provides insight into the boundaries and feasibilities of a monolithic formulation of multitask learning by \nneural networks. The authors show how complex tasks can be modularly formulated thus yielding a joint monolithic \nlearning possibility. They also show that such modularity can be used to interpret simple algorithms thereby also \nleading to their joint learning.\n\nThe main contributions of the authors are the following:\n\n- showing that \"the two layer neural network can jointly learn the task coding scheme and the task specific \nfunctions without special engineering of the architecture\"\n\n- \"systematic theoretical investication of the extent of this ability\" (ability = single network can successfully \nbe trained to perform a wide variety of tasks) \n\n- \"...primarily interested in the extent to which different tasks may interfere,...\" (in a multitask setting)\n\n\n##########################################################################\n\n### Reasons for recommendation / score: \n\nThe paper is composed of a vast amount of very good research work. The research results seem significantly novel and definitely not incremental or based on other similar contemporary works. The rigorous mathematics and the attention to many details is laudable. \n\nHowever, the content in the paper literally and logically coerces the reader to constantly look at the supplementary material. I could also dare to say that the paper, strictly, without the supplementary material almost seems incomplete or as a compilation of claims, making for a choppy read. This is clearly an effect of the authors trying very hard to squeeze a lot of content into the 8-page limit. This must have taken a lot of efforts and I definitely can see the work that has gone into writing this concisely.\n\nThe paper would read much better with all the appendices and supplementary material introduced in the appropriate places, in proper continuum. I could see this paper be re-written almost as a proper tutorial paper in this topic of research. I would in this context, recommend some extra experiments and discussions (commented below also), to make the work more thorough.\n\nFor these reasons, I must insist that the venue for submitting this work should be a suitable journal such as JMLR, ML, or specifically be rewritten for the Special Journal Issue @ ICLR 2021. It is not suitable to be accepted as a conference contribution, by the sheer magnitude of work and the style it is presented in.\n\nAnother possibility I see, but do not recommend, is that the main motivation of the paper be modified to a solely task solving angle rather than the science of comparison and analysis. Then the theoretical rigour could be reduced and the focus can land on experimental results. This, in my opinion, could be suitable and a definite ACCEPT for ICLR conference.\n\n##########################################################################\n### Note about the reviewer:\n\nMy area of research is Bayesian non-parametrics applied on to multitask learning. I am not very familiar with the mathematical support provided for the theorems in this work. I cannot promise a critical verification of the correctness of the proofs.\n\nPlease also keep this in mind when considering my recommendation above. \n##########################################################################\n### Pros: \n \n1. The research quality and quantity are exceeding requirements for acceptance!\n\n2. The language is clear and crisp when introducing the research area and placing it in context with its related works. I especially like the delineated \"Our Results\" section. The authors clarify and discuss the topic with respect to two landmark papers very well.\n\n3. The flow of thought is clear and makes the reader comfortable with the presented paper structure.\n\n4. All the assumptions are made evident and clarified beyond any doubts, there are no hidden assumptions or simplifications. The scope of the focused research is also well clarified.\n\n5. There is thorough mathematical justifications, case studies of monolithic formulations, guarantees on bounds and learnabilities in the supplementary material. (I have tried to not give it attention as it is not a necessary part of the submission)\n\n##########################################################################\n\n### Cons: \n \n1. The title, abstract and some parts of the conclusion suggests a tone of comparison. This makes me expect a more involved discussion about the topic \"Modular versus Monolithic Task Formulations\". The authors have a lot of insight in this matter, however, when it comes to presentation, they fall short to guide the reader through them.\n\n2. The authors have carefully cherry-picked the theorems and balanced the extent to which they explain them so that it reads with completeness on the whole. I would argue that the details are important and without the proofs and mathematical involvement the scientific reader is forced to question \"why?\" or \"how?\" quite often.\n\n3. I would have liked the authors to place themselves better in the research context. I would have liked to know the findings of the authors in the lists of references (in Sections 1.1 and 1.2) with respect to the title of this work. \n\n4. The writing in many middle sections where details are needed, are overtly compressed. This is an effect of trying to squeeze in too much in too little space. The authors do direct the reader to the supplementary material many times.\n\n5. The authors do not answer the grander questions they begin the paper with. They analyse other attributes which are aligned in the same directions as these questions. Eg. Is modular construction better than the monolithic ones? When should we use which construction?\n\n6. The authors talk about multitask learning in the same context as [Caruana 1997]; That is, they reduce the scope of their analysis to inputs of the same size for all tasks or even same inputs to different functions (or tasks) to be learnt. Is this the general case of \"any\" multitask learning scenario? \n\n7. The experiments themselves do not seem statistically thorough. There should be more than 3 trials to draw conclusions, especially when the experimental setup is based on parameters drawn from a uniform distribution.\n\n8. The authors need to address the few inconsistencies in the graphs they have shown in Fig.2. I definitely would have liked more authors' insight on the observed statistics.\n\n9. In Fig.3 are the numbers significant? They are reported in the third decimal position for the Test R-squared values. Some more explanation is needed.\n\n10. Is the monolithic formulation of multitask learning effectively: joint learning of the switching function and the task function? It would be nice to read some more of the authors' explanation of how and if they are doing something different.\n\n\n#########################################################################\n\n### Suggested Presentation Changes:\n\n1. There could be more figures explaining the schematics of the networks, explaining the setup etc. Especially Sec.2.3 and Sec.2.4.\n\n2. There could be more figures and clearer captions with simpler explanations. This is where the reader looks first. It would be nice to suggest what to expect from such a graph and then highlight any results.\n\n3. The details in Table 1 can be made more readable. It is unclear where the focus lies.\n\n4. I feel it is important, the authors highlight that the Simple Programming Cosntructs part of their research derives from their novelty in the formulation of modules in terms of mathematical functions!\n\n##########################################################boarman###############\n### Suggested Small Corrections: \n\nGenerally, try to break down longer sentences into shorter multiple sentences.\n\n- Sec. Abstract: remove or replace word \"underlying\"\n\n- Sec. Abstract: \"... trees over some task-code attributes.\" Change 'some' --> 'certain' \n\n- Sec. Introduction: \"As techniques, such as neural networks, for learning with relatively rick classes have been developed, it is ...\"\n\n- Please see if you can move the references to the end of sentences than in the middle. It makes for better readability.\n\n- Sec 1.1: move NTK references to end of the sentence.\n\n- Sec 2.2 theorem 2: for omega(1) far A...A subspaces, if A ...\n\n- Sec 2.2 theorem 2: for omega(1) separated c...c prototypes, if ||c ...\n\n- Fig.3. caption: 1-(Test R-squared). Parentheses helps understand the subtraction from unity.\n\n#########################################################################\n\nUpdates: \n",
            "rating": "3: Clear rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "The paper shows that multiple tasks can be encoded within a neural network that has a monolithic structure. Different ways of encoding the tasks are considered. ",
            "review": "This paper sets out to show that multiple tasks can be encoded in a neural network that that does not have explicit modular construction for each tasks, which is in contrast with the work of [Bakker and Heskes, JMLR 2003], and [Jocabs, Jordan, Nowlan and Hinton, Neural Computation 1991]. The premise of the paper is that task coding as indicators can be approximated via the derivative of an approximation (erf + Taylor truncation) of the step function. This approximation is analytic and, with individual tasks being analytic, makes the entire multiple task function g(c; x) possible to be approximated by neural networks due the the universal function approximation power of neural networks.\n\nThe paper starts from Arora et al. 2019b. However, this seems unnecessary and limiting. The paper can be made more general using from earlier work, such as those since [Hornik 1991].  This is specially so when SGD is not really required by the subsequent developments in the paper.\n\nThe paper is lacking much in clarity:\n1. The definition of g(c; x) in section 2.2 should be given from the onset and the related work discussion based around that. This will dispels any notion that the paper will dwell on positive/negative transfer/multi-task learning.\n2. The paper seems to be a theory paper, but much of the important derivations in the appendix beyond the page limit. For example, Lemma 4 and Theorem 8 are the main support of the paper, and should be in the main part of the paper. Lemma 7 is also important, and it is the closest in the entire paper that answers the question set out in the last sentence of the second para of section 1. \n3. The top left box of Figure 1 only makes sense when one reads appendix B1.\n4. Overall, the main paper cannot stand on its own without the appendix, which makes it inappropriate for the a conference paper (and its associated review period). \n\nI recommend *reject* based on the clarity and structure of the paper.\n\nCOMMENTS AFTER REVISION\n=========================\nI've looked through the revised paper, and the authors has addressed my comments. Hence, I am happy to recommend an accept.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}