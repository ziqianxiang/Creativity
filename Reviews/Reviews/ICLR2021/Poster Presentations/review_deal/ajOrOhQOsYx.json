{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "Reviewers generally agree that the main result of the paper, which generalizes the classical Wigner-Eckart Theorem and provides a  basis for the space of G-steerable kernels for any compact group G, is a significant result. There are also several concerns\nthat need to be addressed. R4 notes that the use of the Dirac delta function (e.g. Theorem C.7) is informal and mathematically imprecise and needs to be fixed. R1 notes that it would be helpful to at least describe how this general formulation can be applied in machine learning.\n\nPresentation and accessibility: the current version of the paper will be accessible to only  a small part of the machine learning audience, i.e. those already with advanced knowledge in mathematics and/or theoretical physics, in particular in representation theory. If the authors aim to make it more accessible, the writing would need to be substantially improved."
    },
    "Reviews": [
        {
            "title": "Review of \"A Wigner-Eckart Theorem for Group Equivariant Convolution Kernels\"",
            "review": "The paper under review is a very technical contribution to the study of group-equivariance of convolution kernels. The problem of group-equivariance is studied in the most general setup, thus encompassing the previous achievements of Cohen and Welling, Cohen, Geiger, and Weiler, Weiler and Cesa, etc.\n\nThe most general tools from classical harmonic analysis and Lie group representations are put to work in order to provide the most general framework for the analysis of equivariance. \n\nI learnt a lot about the mathematics of equivariance, a very interesting topic. However, I wonder wether the ICLR conference is the most appropriate venue for such thorough study. The application section is in particular way too sketchy to convince the novice that this impressive work will be useful to the machine learning community and an effort in this direction should be made to clarify the expected impact. \n\n \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "1: The reviewer's evaluation is an educated guess"
        },
        {
            "title": "Important result characterizing group symmetries in learning problems",
            "review": "The paper considers Group Equivariant Convulation Neural Networks (GCNNs) which are convolutional neural networks that are equivariant wrt group symmetries of the underlying space. The equivariance requirement places constraints on the parameterization of the corresponding CNN. This work extends previous results for particular symmetries and outlines a method for obtaining these parameterizations for any compact group symmetry.\n\nTechnically, the paper establishes a Wigner-Eckert Theorem for G-steerable kernels, which in turn allows any admissible kernel to be expressed using a basis of kernels thereby establishing a natural parameterization. This procedure is carried out for U(1), SO(2), SO(3) among others.\n\nThe paper highlights important ideas from representation theory that can be used to obtain paramaterizations for symmetry-constrained learning models, and the mathematical methods could be of independent interest. The results obtained here are significant for any learning problem where there are inherent natural symmetries, as the authors point out this could be especially beneficial for data arising from physical processes.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Theorem 4.1 unifies previous efforts on Steerable CNN and provides a useful method for approaching further G.  This paper is a significant contribution to the field.",
            "review": "\nThe authors prove a theorem (thm 4.1) which describes a basis for the space of kernels in a G-steerable CNN for any compact group G.  Steerable CNNs are similar to CNNs but replace channels with G-reps and enforce an equivariance constraint on the kernels.  Though Cohen et al 2019 state the constraint, and Cohen et al 2019, Cohen & Welling 2016, and Weiler & Cesa 2019 and several other papers solve this constraint for different groups and representations, there has not been a general formulation which applies to all compact groups.  Here, solving the constraint means to construct a basis of the space of steerable kernels.  Any steerable kernel is then a linear combination of this basis and the network can then be trained by learning the coefficients.\n\nThe problem of finding a basis for the space of steerable kernels is non-trivial and critical for constructing G-steerable CNNs.  Up until now this has done group by group.  The theorem proved in this work unifies such previous efforts and provides a useful method for approaching further G.  This paper is a significant contribution to the field.\n\nThe appendix provides a complete and approachable background in the area as well as detailed and precise proofs.  Moreover, the effort by Cohen & Welling to frame equivariant deep learning in the proper context of representation theory is continued and extended here to good profit.  The appendix is quite verbose and the language is more casual than I am accustomed to in a mathematical text or research paper, but it serves the goal of being didactic and approachable.\n\nA practical consideration remains. Though theorem 4.1 reduces construction of a basis of steerable kernels to 1) finding Clebsch-Gordon decomposition of tensor products, 2) describing endomorphisms of irreps, and 3) describing harmonic functions, none of these problems is trivial (or even necessarily solved) for a general compact group G.  That said, Appendix E does a good job providing evidence that this can be done for many individual groups.  However, in that case, we are still back to solving the problem on a group by group basis.    \n\nSpecific Additional Points:\n1.My opinion is that the language of physics does not add to the paper.  While Clebsch-Gordan and harmonic functions first arose in physics, they can be described in terms of representation theory.  In this way, both steerable CNN and quantum mechanics are applications of rep. theory and so it is not necessary to use physics here to describe steerable CNN.\n2.Page 5, the notation $[j] = dim(V_j)$ seems unusual to me. It would be better to use something more standard.  In particular, in Defn 3.5, brackets are used as parenthesis, making this more confusing.\n3.Page 5, the fact that input and output representations $V_{in}$ and $V_{out}$ decompose into irreps does not immediately explain how to construct a steerable kernel basis for $V_{in} \\to V_{out}$ given ones between irreps $V_i \\to V_j$.  Though it is not complicated, I would include this.\n4.Page 5, I was confused by the inclusion of $End_{G,K}(V_j)$ at first since it does not appear when working over $\\mathbb{C}$ due to Shurr’s lemma.  It is explained in the paper and more so in the appendix that it is necessary over $\\mathbb{R}$, but it could be a bit clearer and earlier in the paper.  Namely, you could note that over $\\mathbb{C}$ $End_{G,K}(V_j) = \\mathbb{C}$ and give the possibilities over $\\mathbb{R}$.\n5.Page 5, Thm 3.4 and Page 43, Thm C.7.  Given that the purpose of this paper is partially to formulate Steerable CNN in precise terms, the fact that $\\delta_x$ is informally considered as in $L^2(X)$ is very imprecise.  Not only does this make the proof informal, it means the maps in the theorem are not even defined.  Can you replace $L^2(X)$ with an appropriate space of distributions in order to make the statement precise?  \n6.Page 6, “which is zero for almost all J” should be “which is zero for all but finitely many J”\n7.Appendix E, $U(1)$ is isomorphic to $SO(2)$, so it is strange to use both notations.  The difference between these subsections is whether the representations are real or complex.\n\n**Updates from Author Feedback**\nWhile I would have liked to see a draft with the changes, I feel reasonably sure the authors will improve Appendix C to make the statements mathematically precise. I am confident in the statements and proofs. While the presentation can be verbose and casual, I think it is justified to increase accessibility, so long as the proofs and statements are formal and precise. Based on Author responses I have increased my confidence.\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Physics techniques for group equivariant convolution ",
            "review": "For a physicist familiar with machine learning, the title of this work says it all.  It is a lengthy explanation of how to use well known techniques from physics in constructing convolutional neural networks with a group symmetry.  The paper also spells out what are likely to be the most used cases of U(1) and SU(2) and their quotients by discrete groups.\n\nI give the paper high marks on the value of the topic it addresses, but middling marks on presentation.  The results I spot checked were correct, but the paper reads like a compilation of material from the many math and physics textbooks on the topic, and lacks the coherence of a good textbook.  If I had a student who needed this material I would instead give him or her the original Cohen Welling paper and a representation theory textbook such as Hall 2015 or one of the several textbooks they cite.  In addition the explicit results of appendix E would be rather more useful if implemented in a ML package, and I hope that is on the author's to-do list.\n\nTo summarize, a serviceable reference work which will probably be made obsolete by the appearance of a proper textbook on group theory in ML before long.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}