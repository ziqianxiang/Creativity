{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "## Summary \nThe paper advances the state of the art in training binary neural networks coming out to first place on ImageNet with a controlled computation budget. While any paper making a new record on ImageNet would be a serious candidate for acceptance, it is positive that this one achieves the goal by putting at work the mechanism of conditional computation, innovative for binary networks, and studying in a systematic and clear way how the network width and configuration can be varied while maintaining the computation budged.\n\n## Review Process and Decision\n\nThe paper was thoroughly discussed by reviewers from different aspects. Several weaker spots have been identified (see below and final reviews), but no critical issues that would indicate a necessity of a major revision. In the end, reviewers agreed on acceptance although in some cases they have decided to keep their original <=5 ranking to reflect the scientific value to them from a more global perspective.\nI think it is an example of well done modeling and experimental work: the work is very clear, uses sound methods, the experimental results are systematic and give interpretable evidence, which is in my experience is rather exceptional for the overall very empirical binary NNs field. I estimate high interest because of the concept of conditional computation put to work here and because of making a new record on ImageNet.\n\n## Details\n\n* Computation Cost\n\nIf such networks are to be deployed in low-power devices, the computation cost might need to be estimated more accurately. An example of such estimation is the work by\nDing et al. (2019) Regularizing Activation Distribution for Training Binarized Deep Networks,\nwhere the energy and area are estimated using information from a semiconductor process design kit.\n\nThere is indeed a number of floating point operations around the binary convolutions: first and last layers, experts, skip connections with scale factors and non-linearities. The latency and cost of these operations may not be negligible on target devices. In particular Ding et al. (2019)  argue that XNOR-Net architecture is 3 times more costly because of floating point scale factors.\nHowever the paper does a fair job in comparing in operation counts, which is a good proxy for many devices. The floating point computations needed in various places can be indeed further reduced to lower bit width, the research on quantization techniques shows this is possible and orthogonal to the contribution.\n\n* Novelty of grouped convolutions design and search\n\nThe work of Phan et al. (CVPR 2020): Binarizing MobileNet via Evolution-based Searching\nalso proposed to search for best grouped convolution under computation budget constraints (evolutionary search method).\nStrict budget constraint and merging results from different groups are somewhat novel and the prior work can be objectively contemporaneous.\n\n* Clarity\n\nClarity of the paper has been improved by the revision. One remaining mysticism is still about the gradient estimator for the experts. The paper states: \"we wish to back-propagate gradients for the non-selected experts\", \"allows meaningful gradients to flow to all experts during training\". The problem is that since $\\varphi(z)$ is binary one-hot on the forward pass, the gradient of the scalar product with $\\varphi$ in (2) results in that in the backward pass only the selected expert receives the training signal and by no means all of them. This is regardless of how the gradient is propagated through $\\varphi$. Maybe something is missing? I hope the authors can clarify in the final version. I do not consider it as a serious flow since this training scheme is not claimed as a contribution in any case.\n\nOne more point on the clarity: The paper claims that using experts increases the network representation power / capacity. While this seems logical, and follows the preceding work in real-valued NNs, the paper could provide additional evidence in terms of training performance of these models. Since the teacher with 76% accuracy is used in the distillation, I assume the training never reaches 100% training accuracy in any of the settings. Does the training accuracy improves with experts? This would be a helpful evidence for further work.\n\n* Search method\n\nThe paper was further criticized for that the manual search of the architecture is a step back from automated search methods (NAS, BATS). However these methods are themselves a relaxation of discrete choices (experts, if you like), that need to keep all possible configurations at the same time, which may be less stable and too costly for real architectures and datasets. The principles of gradient-based architecture search are not entirely clear and the resulting models coming out of these methods typically give no insights regarding good (intelligent) design choices. At present, the systematic exploration with analysis of tradeoffs conducted is seen to have advantages.\n\n\n"
    },
    "Reviews": [
        {
            "title": "A collection of some useful techniques",
            "review": "This paper proposes some techniques to improve the accuracy of binary networks without adding much computational overhead. To improve model capacity, the author proposes mixture-of-experts convolution with a winner-takes-all gating mechanisms. To deal with the limited representation power of binary activations, the paper proposes utilizing group convolutions. The performance is further improved by careful selection of hyperparameters and improved training techniques.\n\nClarity: This paper is pretty clear. The methodology is well-motivated and the algorithms / experiments are described clearly.\nOriginality: To the best of my knowledge, the utilization of mixture-of-experts and group convolutions for binary neural networks is novel.\nSignificance: The propose techniques, despite being simple, achieves good performance. They can be a new baseline for future research.\n\nOverall I think this is a good paper and should be accepted.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Achieving very good accuracy with BNNs",
            "review": "Summary:\nThe paper addresses the problem of filling the gap between the performance of binary and real-valued networks. The authors propose a series of procedures to improve the model and representation capacity of binary neural networks. Different binary-network architectures are obtained through a new network-growing approach and compared.\n\nStrengths:\nResearch on BNNs has a pretty short history if compared with studies on real-valued NN. It is a good idea to start translating some of the main tools from the standard NN literature to the binary setup. \nAccording to the authors' claim, the proposed model greatly outperforms other existing and well-known binary networks. \n\nWeaknesses:\nThe majority of the tools proposed for boosting the performance of binary networks are not new and have been already used in standard NNs. A discussion of the technical challenges associated with applying such tools to the binary setup would help understand the main contributions of the paper.   \nThe inclusion of real-valued experts seems to make the final network not completely binary and it is not clear whether the advantages of BNN (e.g. the gain on computational costs) are preserved.  \nThe network design step mainly consists of rearranging a series of pre-defined building blocks. It is not well explained how the architecture space is searched and how to interpret the results in Figure 2. \n\nQuestions:\n- is the cost of Conditional computing included in the total cost when the main results are claimed (e.g. in \"Without increasing the computational budget of previous works, our method improves upon the state-of-the-art by 6%\")? More generally, when does the fixed number of BOPs include any training step? \n- does the expert selection of the proposed method work better in the binary case than in real-valued networks?\n- are the weights in each `expert' binary or real? Is it fair to compare the obtained hybrid model with real-to-bin?\n- has Grouped Convolution with a similar scaling factor been already used somewhere?\n- what is the difference between the proposed gradient-approximation method and standard \"Straight-Through-Estimator\" (STE)?\n\n-----------------------\nI acknowledge that I read and appreciated the author's response (both parts). The authors' reply mainly answers my questions, especially regarding the difference between applying the proposed techniques to the real and binary setups.\n\nI agree with all authors comments but would tend to confirm my overall score for two reasons:\n\nthe architecture search method is not simply a block rearranging but looks more like a heuristic approach than a clear methodological contribution\nthe proposed mixing of real and binary weights may preserve the advantages of fully binary networks but, again, makes less clear the net contribution of the paper from a more theoretical perspective\nHowever, as I recognize that the paper contains significant experimental results, I would be happy to support acceptance if all other reviewers agree on that.\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Strong results, and some questions about the experiments",
            "review": "The authors improved the performance of BNN by adopting group convolution and data-driven expert binary networks which choose a weight group that takes part in inference. In addition, by searching for the network architecture under the condition of the same number of operations, they achieved SOTA accuracy on ImageNet dataset.\n\nThe results in the paper are strong. Especially, the SOTA accuracy (71.2%) of the proposed network which even outperforms the full-precision ResNet-18 is impressive and could be the significant result in BNN research. The adopted data-driven expert network which increases the number of parameters, but maintains that of parameters participating in inference is also interesting.\n\nBelow is my remaining concerns and questions about the experiments.     \n\n1. Let us assume the condition that the proposed BNN which has four experts processes many images (i.e. multiple batches). In worst case, expert weights which a few images (extremely four) select by gating function can be all different, which gives rise for processors to have to fetch 4 times more parameters (i.e. all trained parameters) compared to single image inference.  So, I concern if under the condition of using multiple batches there is the performance degradation in terms of inference speed or energy in hardware compared to other BNN models.\n2. Unlikely other previous works, mixup is used even on ImageNet. According to [1], it is stated that using mixup on ImageNet slightly degrades the accuracy. If an accuracy of your model trained without mixup (and if possible, accuracy of other models like Bi-Real Net or Real-to-Bin trained with mixup) is provided, the proposed model performance will be more clearly shown under the same training condition.  \n\nMinor comment:\n\n1. I could not clearly understand the message or meaning of Fig. 1b. Detailed explanation of this figure seems to be needed.\n2. I think it will be more helpful for readers to understand Fig. 2 if the authors provide clearer information about it: the detailed network information for each constellation in Fig. 2a and for type of lines in Fig. 2b.\n3. I have a question if the searched network architecture is just optimized to BNN or not. I think this can be shown by measuring the accuracy drop of your model compared to the same full-precision network architecture.\n4. As mentioned earlier, the experimental result is impressive, so it seems that other researchers or related people might want to use your model, but the stated training process seems to be little bit complicated. Are you going to make your code public?\n\nReference\n\n[1] Brais Martinez, et al. Training binary neural networks with real-to-binary convolutions. ICLR, 2020.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #2",
            "review": "---Paper summary---: \n\nThis paper proposes three approaches to improve the performance of BNNs. 1) Training a super-network (ensemble of BNNs) and dynamically selecting one BNNs to execute conditioned on input. 2) Widening the layers with group convolution to enhance the representational capacity. 3)  Designing the architecture using EfficientNet considering the width, depth, groups and layer arrangement configurations simultaneously.  The effectiveness of the paper has been justified on ImageNet classification but can be further strengthened. \n\n---Strength---: \n+ The performance of the paper is promising. It will serve as a strong baseline for future works.\n+ Using conditional dynamic routing to improve the BNNs capacity is interesting. Specifically, the paper proposes to learn a supernetwork (i.e., ensemble of several experts) during training and dynamically select the path during testing, which enhances the capacity while preserving the inference efficiency to some extent.\n\n---Weaknesses---: \n\n1: This paper ensembles some existing compression/NAS approaches to improve the performance of BNNs, which is not significant enough. \n+ The dynamic routing strategy (conditional on input) has been widely explored. For example, the proposed dynamic formulation in this paper has been used in several studies [2, 3]. \n+ Varying width and depth has been extensively explored in the quantization literature, especially in AutoML based approaches [Shen et al. 2019, Bulat et al. 2020], to design high capacity quantized networks. \n+ The effectiveness of the group convolution in BNNs was initially studied in [1]. Later works also incorporate the group convolution into the search space in NAS+BNNs methods [e.g., Bulat et al. 2020a] to reduce the complexity.\n\n2:  In each layer, the paper introduces a full-precision fully-connected layer to decide which expert to use. However, for deeper networks, such as ResNet-101, it will include ~100 full-precision layers, which can be very expensive especially in BNNs. As a result, it deteriorates the benefits and practicability of the dynamic routing mechanism.\n\n3:  The actual speedup, memory usage and energy consumption on edge devices (e.g., CPU/GPU/FPGA) or IoT devices must be reported. Even though the full-precision operations only account for a small amount of computations in statistics, it can have a big influence on the efficiency on platforms like FPGA. \n\n4:  This paper proposes to learn the binary gates via gradient-based optimization while exploring the network structure via EfficientNet manner. Then the problem comes. This paper can formulate the <width, depth, groups and layer arrangement> as configuration vectors and optimize them using policy gradients and so on, with the binary gates learning unified in a gradient-based framework. So what is the advantage of the \"semi-automated\" method of EfficientNet over the gradient-based optimization? In addition, how about learning a policy agent via RL to predict the gates? I encourage the authors can add comparsions and discussions with these alternatives.\n\n5: More experiments on deeper networks (e.g., ResNet-50) and other network structures (e.g., MobileNet) are needed to further strengthen the paper.\n\n\nReferences:\n\n[1]  MoBiNet: A Mobile Binary Network for Image Classification, in WACV 2020.\n\n[2]  Dynamic Channel Pruning: Feature Boosting and Suppression, in ICLR2019. \n\n[3]  Learning Dynamic Routing for Semantic Segmentation, in CVPR2020. ",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}