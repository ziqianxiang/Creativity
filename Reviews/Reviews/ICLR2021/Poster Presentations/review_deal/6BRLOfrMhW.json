{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "All of the reviewers thought that this paper addresses an interesting and important problem.  Several of the reviewers thought that the paper gave a creative approach for training bloom filters and this would be of interest to the community. "
    },
    "Reviews": [
        {
            "title": "A somewhat descriptive scheme for constructing memory-efficient partitioned bloom filters incorporating a learned model. ",
            "review": "A clear exposition of the problem and proposed solution, the paper key strength is in the formulation of the partitioned bloom filter as an optimization problem that generalizes previously proposed architectures, and prescribes an interpretable solution for the choice of the optimal partition-thresholds in terms of the properties of the given learned model (specifically, its false-negative and true-negative threshold dependent curves). Furthermore, the experimental section clearly illustrates a significant advantage of the proposed method over state of the art alternatives. \nThe theoretical treatment, however may be considered as a first attempt at combining essential properties of a learned model and bloom-filter design - the paper could be improved by considering and motivating the usage of a particular lower bound for the space cost of a bloom filter and incorporating in the the optimization formulation the relation between the size of the learned model and the qualities of the false positive and false negative curves. The latter, especially, makes a significant practical difference since the model may be part of the design and assuming out the size-quality tradeoff results in a sub-optimal scheme and renders the overall proposed solution as a heuristic still.\n\nUpdate (Nov 30th) In light of the author's responses and the other reviews I increase my score for this paper to 7: Good paper, accept.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "The paper proposes a generalization of the sandwiched Bloom filter model that maintais a set of score partitions instead of just two and an algorithm for optimizing parameters of the partition under the target false-positive rate. Authors evaluate partitioned Bloom filter on three datasets and demonstrate that delivers better false positive rates under for a given model size compared to the baselines.\n\nI find the paper quite innovative and the experimental results impressive. My main concern is regarding the paper clarity.\n1. How can a region's FPR $f_i$ be greater than 1? Perhaps, this is something very obvious, but I couldn't get this immediately and perhaps some other readers might  struggle here too.\n2. The learned model size appears in the optimization objective, but is considered given. I wonder what will change if we allow to trade-off the learned model power for a larger number of regions and larger backup Bloom filters in each region? Does it even make sense to ask such a question? Would the results change if a different model is used? I think it is possible that for a given variant of a learned Bloom filter, a different model may result into different values of optimal parameters and a difference performance, thus these should ideally be optimized independently for each of the baselines. I also think that size of the pickle file is arguably not the best estimate for the learned model size if indeed a different model is used for each of the filters, e.g. a neural network might admit a decent compression rate if a lower precision number format is used etc. Thus, it is important to separate the impact made by a learned model from an algorithmic improvement.\n3. Is there any variance caused by observing particular distributions G and H? Is it small enough to ignore or confidence interals for each of the curves might actually overlap? I would also be interested in understanding behaviour of all considered models as the sample size changes.\n\nI also feel like authors can cite *Meta-learning neural Bloom filters. Rae et al, 2019* as it considers a relevant (although a different as well) setting.\n\nNevertheless, I think the kind of analysis presented in the paper very useful for the community and for further development of learned data structures. I recommend acceptance and I will gladly raise my rather conservative score if authors could clarify the points mentioned above.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Fine tuning partitioned learned Bloom filter",
            "review": "\nThis work proposed a technique to fine tune the partitioned learned Bloom filter to reduce the space consumption given a false positive rate threshold.\n\nThe idea is to formulate the problem into a two-part optimization problem: How to best partition the scores from the model into a given number of regions and how to choose thresholds for the regions to minimize the overall space consumption of Bloom filters. A relaxed version of the latter problem is addressed by using KKT conditions to obtain the optimal thresholds. The former problem is addressed by discretizing the region boundaries and dynamic programming.\n\nOverall, I like the idea of fine tuning the partitioned learned Bloom filter. It seems to me that it will also be possible to fine tune the number of partitions as well, e.g., a simple way is to use a binary search. The evaluation result is impressive compared with baselines in terms of space consumption vs. false positive rate. And the writing of the paper is clear and easy to follow.\n\nHaving said that, I have some concerns with this line of work.\n\nFirst, IMHO, the real challenge of putting these learned Bloom filters into work is how to maintain them under insertions. While it is OK that this is not the focus of this work, it seems to me that the proposed optimal learned Bloom filter can be brittle under insertions. It will be great to understand how the proposed technique degrades compared with the baselines. In addition, with partitioned Bloom filters, it seems to be more prone to resizing upon insertions compared with using a single Bloom filter. \n\nSecond, it is possible that the proposed technique needs to be reoptimized upon insertions. In this case, it will be important to understand the overhead of constructing the Bloom filters proposed in this technique compared with the baselines. However, the overhead of constructing the various variants of Bloom filters is missing in the evaluation. It might be possible to reduce the overhead by using a coarser grained discretization for the DP. The performance, however, can degrade with a coarser grained discretization.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}