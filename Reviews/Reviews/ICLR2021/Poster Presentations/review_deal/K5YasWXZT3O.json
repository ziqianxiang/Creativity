{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "Dear Authors,\n\nThank you very much for your detailed feedback to the initial reviews and also for further answering additional questions raised by a reviewer. Your effort has been certainly contributed to clarifying some of the concerns raised by the reviewers and improving their understanding of this paper.\n\nOverall, all the reviewers found a merit in this paper and thus I suggest its acceptance. However, as Reviewer #2 suggested, investigating the convergence in the stochastic case is very important. More discussion on this would be a valuable addition to the paper, which the authors can incorporate in the final version."
    },
    "Reviews": [
        {
            "title": "The paper explore relevant applications of exponential smoothing in several ML problems. It is well written and should be interesting for ML practioners.",
            "review": "The paper provide nice discussions of a simple modification of ERM paradigm. Mainly, it consists in applying an exponential smoothing to the loss function. The authors provide several interpretations and connexions with (robustness/fairness/quantile regression etc...) literature and show how the TERM can be adapted to such problem. The paper is well written, contains pedagogical illustrations and detailed properties of TERM.\n\n- The success of Term heavily rely on a *magical* tuning of the parameter $t$ depending on the application. Grid search was used in this paper which considerably increases the complexity of the algorithm without necessarily improving significantly the accuracy when compared to competitors.\n\n- The experiments does not report standard deviation in the train/test splitting. Averaging the test accuracy after several random splitting would be beneficial for clarity.\n\n- The classical ERM formulation is written as sum of functions, which allows several advanced stochastic optimization algorithm. Such structure is destroyed in the tilted formulation.\n\n- In Assumption 2, isn't it too restrictive to assume that $f(x_i, \\cdot)$ does not have critical point (which does not seems true for quadratic loss)? \n\n- To help the reader, the authors might recall on which quotient the L'hopital rule is applied (after verification of assumptions)\n\n- In equation (87), the limit and derivation are permuted without justification (uniform convergence might be needed). Same for Eq (90)\n\n- Should be interesting to compare with recent robust estimation such as median of means or robust gradient descent (Holland and Ikeda, 2019).\n\n- Should be also interesting to know how the generalization bound (and sample complexity) of TERM compare with the one of ERM wrt $t$.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper provides a unified framework for solving a bunch of issues in ERM. ",
            "review": "This paper considers a unified framework named TERM for addressing a bunch of problems arising in the simple averaged empirical minimization. By leveraging the key hyper-parameter t in the TERM loss, it can recover the original average loss and approximate robust loss, min/max loss, and the superquantile loss, etc. The authors also propose gradient-based optimization algorithms for solving the TERM problem.  \n\nOne thing that I do not understand very well is the paragraph under Lemma 1. Why is it necessary that outliers can cause a large (positive t) or small (negative t)  losses? Note that outliers can be arbitrary, say adversarial. \n\nAlso, do you have numerical issues for large enough t?\n\nIs it possible to show certain convergence results of the algorithms for solving the TERM? Especially, the TERM has the nice property that it is always smooth (depending on the value of t). \n\nOverall, the TERM seems to be a good unification of different losses used in machine learning society for different purposes. The theoretical justifications also look reasonable and informative. In addition, the authors conduct a series of experiments to show the good performance of TERM for different tasks such as robustness to outliers, handling imbalance, and improving generalization, etc. \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting analysis with a broad range of applications",
            "review": "This work analyzes the LogSumExp aggregated loss (named tiled empirical risk minimization, or TERM, in the paper). It provides several general properties of the loss, such as its relation to min/avg/max-loss, and interpretations of different trade-offs. Empirically, it is shown that TERM can be applied to a diverse set of problems, including robust optimization, fairness and generalization.\n\nStrength:\n1. Provide theoretical analysis on the properties of TERM\n2. Various experiments to showcase the usefulness of TERM as objective\n\nWeakness:\n1. Unclear convergence of the optimization procedure\n2. Missing literature\n\nDetails:\n1. The LogSumExp has been extensively studied in geometric programming (Calafiore and El Ghaoui, 2014, Sec.9.7) and boosting (Mason et al., 2000; Shen and Li, 2010). The current manuscript does have some novel interpretations such as the trade-off between avg-min losses. However, I am not an expert in this field, thus not sure how much the new analysis will contribute to the community (or the analysis may exist somewhere). Additionally, LogSumExp is called \"tilted\" without explanation. Why not call it LogSumExp, which is well-known?\n\nRef:\n- Calafiore, G.C. and El Ghaoui, L., 2014. Optimization models. Cambridge university press.\n- Mason, L., Baxter, J., Bartlett, P.L. and Frean, M.R., 2000. Boosting algorithms as gradient descent. In Advances in neural information processing systems (pp. 512-518).\n- Shen, C. and Li, H., 2010. On the dual formulation of boosting algorithms. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(12), pp.2216-2231.\n\n2. Some of the technical details are missing.\n- What is the range of t in Fig.1?\n- The interpretation 2 is constrained to be t<0 for avg-min trade-off (and t>0 for avg-max, as shown empirically in Fig.9). This should be clear in the informal statement to avoid confusion. Also, the interpretation 3 is valid for t>0.\n- Algorithm 2 claims that t is temperature, but in fact, t is more like the inverse of temperature in common sense (consider softmax).\n\n3. Convergence of the optimization procedure is not convincing. Since Algo.2 is using a non-trivial averaging for the normalization term, the convergence of the stochastic version is unclear. It seems that the convergence discussion in Appendix H is only for the batch version.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review of \"Tilted Empirical Risk Minimization\"",
            "review": "\n\n  *  Summary of the paper.\n  The paper presents a modification of the empirical risk minimization (ERM) framework in order to obtain more robust or fairer results, using tilted objective. The paper exhibit several properties of the tilted empirical risk minimization (TERM) algorithm applied to classification and regression, TERM exhibits a trade-off between average loss and max loss for instance and this trade-off can be handled with a parameter t that control how robust one wants to be or how fair one wants to be. The claims are supported by a number of numerical illustrations that show the practical efficiency of the method for diverse tasks.\n \n  * Strong points: the article is entirely reproducible as the author furnishes a well documented code. The figures and experiments made in the article provide a good illustration of the algorithm and a few theoretical lemma come to help understand the algorithm better.\n   \n  * Weak points: Use of 80% outliers in Table 1 without further explications. I think there is too much material, you made a lot of different experiments and it would have been great to have less experiments but more explications. There is no theoretical risk bound that would give us the efficiency of TERM, for instance in a corrupted setting or with respect to a fairness loss.\n    \n  *  Recommendation.\n    I vote for accept. The algorithm seems efficient and easy to implement and it allows the user a broad choice of different extensions of the basic learning framework in particular to fairness and robustness in classification and regression.\n\n**Most of my review will be about the robustness part of the article because it is a subject I am familiar with.**\n\n  *  Questions:\n    * It is very weird that TERM works in Table 1 even when there are 80% outliers , as my understanding of outliers is that they must be in minority. I would say that if 80% of the points are outliers, maybe the 20% points are in fact the outliers and I don't see a practical application where the inliers would be in minority. It seems counterintuitive. Can you explain that ? This is even more counterintuitive in classification where the error is smaller when the corruption is 80% than when it is 40%. I am not sure what is the task and what it is that your algorithm do in the 80% noise context. All the definitions of noise/outliers that I know of suppose that the proportion of outliers is smaller than 50%. In fact I think the problem with 80% outliers is theoretically impossible except when doing list decoding (see for instance the article \"List-Decodable Robust Mean Estimation and Learning Mixtures of Spherical Gaussians\" by Diakonikolas, Kane and Stewart).\n    \n    * Why did you not compare your algorithms with scikit-learn algorithms or scikit-learn-contrib algorithms ? I am thinking about HuberRegressor, RANSAC, TheilSenRegressor. To go further you could also test algorithms from scikit-lego for fairness or scikit-learn-extra for robust classification. To compare to a second party and not only your algorithms vs your algorithms.\n    \n    * Why did you only corrupt the labels in a robust classification task ? At least in the regression experiment, you could have corrupted the features and it works well. This is a very interesting property of your algorithm, there are not a lot of robust regression and classification algorithms that are robust to outliers in the feature space ! Is it a misunderstanding on my part or do you algorithm really work when feature space is corrupted ? I tested your algorithm on drug experiment with outliers in feature space and it worked. Huber and L1 did not work in this context.\n    \n    * Is the TERM problem convex when t<0 ? As it is, there is no reason that your algorithm will always converge to a global minimum.\n\n  * Additional Feedback.\n    * The 80% noise in Table 1 really bugged me when I read your article, maybe you may want to explain more or to remove it as it can cause misunderstandings I think.\n    * When comparing robust methods, (Table 1), the authors did not compare their methods to the mainstream algorithms like RANSAC or Theil-Sen regression. It would have been interesting to do so.\n    * The classification task considered by the authors in Table 1 (CIFAR-10) is not easy to interpret. What is an outlier for a neural network ? For a linear classifier, an outlier is readily defined for a very non-linear classifier; this is not so easy because most neural networks will be so nonlinear that the introduction of outliers in the training dataset will not change the performances of the algorithm.\n    * It would have been interesting to compare TERM to other robust classification algorithms on an easier dataset in low dimension to exhibit the comparative performances similarly to what is done for regression.\n    * You talk of \"minimax\" in a way that is unusual. Most of the time, at least in theoretical ML, minimax algorithms refer to algorithms which attain the optimal rate of convergence (see Section 14.1 in \"A Probabilistic Theory of Pattern Recognition\" by Devroye, Gyorfi and Lugosi). I think this is not what you mean, what you used I call it minmax. I don't know if yours is a common use of the term and if this is the case, sorry for this comment and don't take it into account.\n    * Typo: Section 5.3 inn -> in.\n    * The fact that the objective is strongly convex when t>0 is fairly important (this proves the convergence), I think that you should include the whole proof and not an abridged version of it (see proof Lemma 3).\n    \n    \n    \n    \n\n\n\n\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}