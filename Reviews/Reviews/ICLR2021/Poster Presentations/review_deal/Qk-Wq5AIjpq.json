{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper provides a method for constructing PAC confidence scores for pre-trained deep learning classifiers. The reviewers were all positive about the paper.\n\nPros:\n- Has provable guarantees on the reliability of the prediction. Such guarantees are quite desirable in practice.\n- The problem of neural network uncertainty is important and timely problem, especially in safety-critical applications.\n- The method is simple and well-motivated.\n- Strong empirical performance.\n- Interesting applications to fast DNN inference and safe planning.\n\nCons:\n- Lack of generalization guarantees-- the guarantees in the paper only hold on the training set; but in practice, performance in test is what's important.\n- Only a handful of baselines tested against, most of which (if not all) were naive."
    },
    "Reviews": [
        {
            "title": "Method to provide prediction guarantees of DNNs, but some crucial experimental results are missing.",
            "review": "This is a paper that focusses on the timely and important problem of uncertainty quantification for the predictions of deep neural network classifiers. The authors propose constructing calibrated outputs that have provable correctness guarantees, using PAC-style arguments. \n\nThe authors also demonstrate how this framework can be utilized for computationally efficient predictions by combining a smaller, faster albeit less accurate model with a larger, more accurate model, utilizing the latter only when the former is less confident. For this to work, one needs good guarantees on the DNN's estimates of its confidence -- and creating such guarantees forms the crux of the paper.\n\nPros:\n+ Paper is well written\n+ Important and timely problem, motivating arguments are well constructed\n+ Paper appears to be mathematically sound though I did not check all the proofs in the appendix.\n\nCons\n+ One of the crucial assumptions is that the data during test time will be from in-distribution. While I understand it is hard and maybe impossible to provide any guarantees for out-of-distribution data,  it is important to realize that one of the most common ways in which DNNs can fail when deployed in safety critical systems is when faced with predicting from an out-of-distribution data. So it is unclear how  practically applicable the proposed method is. \n\n+ If somehow, one is always guaranteed to work within in-distribution data, then the authors should compare other methods that improve calibration (but don't have guarantees) with the proposed method, both in terms of calibration and computational efficiency in the fast-model/slow-model approach.\n\n+ Also I do not see any calibration performance in the experimental results. While I understand the proposed method is using histogram binning and is not a new calibration method per se, these results should be included. That is,show how  accuracy correlates with the softmax predictions (on the test set) using proposed technique.\n\n+ Since the proposed method provides stronger UQ than other methods, I would also like to see accuracy-vs-coverage curves for benchmark datasets, and compare this to such curves for the baseline (where one thresholds on the winning softmax scores).\n\nOverall, this is a very worthwhile line of work, and I feel the paper has merit, but given that some important results are missing, as it stands, does not meet the bar for acceptance. \n\n== Post rebuttal update ==\n\nSee my response to author's rebuttal below. In light of new experimental results, I feel this now meets the bar of acceptance at ICLR, and hence  updating my score to 6.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Certainly interesting, but perhaps needs to address limitations more clearly",
            "review": "In this work, the authors provide a method for a posteriori calibration of DNN uncertainty with emphasis on constructing a classifier that has PAC uncertainty guarantees. The authors define a ‘calibrated’ probability prediction to be one such that given (for example) an image labeled as a cat, that the probability assigned to the class label ‘cat’ by the predictor is equivalent to the probability that the classifier correctly predicts images from the class ‘cat.’ The authors seek to create a ‘provably’ correct classifier under iid data assumptions. \n\nAccounting for and re-calibrating the poor uncertainty of neural networks is a central problem for deep learning especially when operating in safety-critical domains and so the work is certainly attempting to attack a worthwhile problem.\n\nOne concern I have about the theory laid out by the authors is that they place no conditions on \\hat{f}. That is, one could have a classifier which has arbitrarily bad uncertainty calibration. Take for example a predictor \\hat{f} which assigns \\hat{p}(x) = 0.9 to every input ‘x’ regardless of its ultimate accuracy on the class, then given a new input with unknown label, it is not clear to me exactly how the framework would use the intervals to improve the uncertainty of this classifier, especially given that the class of this new point is unknown. I would appreciate if the authors could help me understand this case as I think it is indicative of a misunderstanding on my part. \n\nAnother concern is the fact that getting well calibrated Clopper-Pearson intervals with good statistical guarantees takes a non-trivial number of samples and it appears this would scale with the number of classes. Thus, for a task like ImageNet, the authors inference model would require the set Z to be quite large (quoted at 20000). It seems that considering this overhead would greatly slow the average inference time. Is this overhead considered in figure 2a? I think this is a OK trade-off to make when in safety-critical scenarios, but then the authors give “fast inference” as one of their primary applications, it seems like a bit more discussion of this may be warranted. \n\nThe authors use the word “provably” correct in a couple places (page 2 and 3) where the correct term they should use is “probably” or PAC. Saying that a statistical guarantee is “provably” correct is an over claim and in my view this should be corrected.\n\nMy last concern is not so much with what the authors have presented, just in the fact that the limitation is not clearly stated. When using statistical guarantees such as those given under the PAC framework, the iid assumption is almost always necessary. Yet, it can be limiting in both the case of planning and image classification, especially when we are considering safety-critical applications. In such applications, one often considers worst-case scenarios and in these cases the iid assumption usually does not hold. Thus, the PAC guarantees in this paper would seemingly be invalidated. For example, adversarial examples may be out of the support of the data distribution, but are still valid inputs. In the presence of adversarial examples the uncertainty guarantees presented here are rendered void. Similarly, there is often concept drift in RL and control problems. Ultimately, I don’t think this is only a minor mark against the work, and one that can be overlooked given that such cases are at least clearly stated as limitations of the approach.\n\n\nPost-Rebuttal Comment: \n\nI would like to thank the authors for thoughtfully answering my concerns and questions. I think the small modifications made in response to my comments have made the paper much easier to understand and I think the work is well presented and positioned. Ultimately, I have increased my score to Accept on the basis that I no longer have any major criticism of the work. I do hope the authors can make a more prominent note about appendix F1 in the main text as I think it is an interesting and important thing to highlight for those who may be skeptical. ",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Good idea, good motivation/use cases, needs some clarification + cleaning up",
            "review": "Summary: This paper proposes a method for obtaining probably-approximately correct (PAC) predictions given a pre-trained classifier. The PAC intervals are connected to calibration, and take the form of confidence intervals given the bin a prediction falls in. They demonstrate and explore two use cases: applying this technique to get faster inference in deep neural networks, and using the PAC predictor to do safe planning. Experiments in both of these cases show improvements in speed-accuracy or safety-accuracy tradeoffs, as compared to baselines.\n\nI’m recommending acceptance since the idea seems useful and well-argued both conceptually and experimentally. However, the paper needs some work in terms of clarification of the key ideas – with this clarification I can raise my score.\n\nStrong points: \n-\tThe proposed method provides a provable guarantee on the reliability of a pre-trained methods prediction, which is a very nice property to have in the reliability/safety problem.\n-\t This approach is a simple but good idea, seems grounded in a good motivation and the explored use cases are informative and interesting. \n-\tExperimentally, the method shows improvements over a naïve baselines, and demonstrate that it can obey a given error or safety threshold in practice, an important property\n\nWeak points + Clarifications:\n-\tI am confused about the application of this method to safe planning. In particular, it seems to me like the proposed intervals only hold their PAC guarantee when the test-time distribution matches the training distribution. However, this will not be the case in the safe planning setting as I understand it, since the observed trajectories are drawn from a different policy than the one which will be implemented in the world\n-\tWith respect to these test set questions, it would be nice to see a little discussion in the paper of how these guarantees transfer from training set to the test distribution\n-\tI would like to see more explanation of the proofs in the appendix, right now they are a little too compact for me\n-\tExperimental baselines raise some questions for me. First, I need more explanation on the histogram binning baseline beyond the one sentence given. Second, the authors state this baseline “does not satisfy the desired error” – but I’m not sure why we would expect it to, since that baseline was not tuned to any sort of error level. Finally, I would like to see naïve threshold baselines in the safe planning setting for more cautious values than 0.5 – since that is more aligned with the goals of safety.\n-\tIn your PAC definition, we could just always output [0, 1] to satisfy it. Therefore, when framing the goals of your method, you should be a little more clear about exactly what you want from a PAC prediction.\n-\tA number of notation errors throughout which are important to fix for clarity and neatness, and a decent amount of lack of precision in language throughout which is important to fix for clarity. See Other Feedback\n-\tLine below Eq 5 confuses me: you say you exit at m if \\hat{y}_m correctly classified an example also correctly classified at \\hat{y}_M. But how can you know this without doing inference to the last layer?\n-\tIs there a reason why the greedy approach to Fast DNN inference you take is desirable? Be more clear about why you chose this and if it is optimal somehow or not\n\nOther feedback:\n-\t- Some precision in language could be improved throughout – for instance “by using the accurate model only if the confidence of the accurate one is underconfident” on p2 doesn’t really make sense\n-\t-p2: “a naively trained DNN is not reliable” what does reliable mean here?\n-\t-above eq 2, should this be \\kappa_x ?\n-\t-Defn 1: should this be a nearness constraint rather than equality? We have multiple examples x in a bin each with their own p-hat, and so not all of them can be equal to c(x)\n-\t-Is there a reason why these intervals should be defined as continuous rather than possibly a disjoint set?\n-\t-should make it clearer from the start that this is defined for post-hoc classifiers, you’re not learning these intervals directly\n-\tEq 4 – the bold theta-hat here is different from the one defined in the line above\n-\tNeed more explanation on “The following expression is equivalent due to the relationship between the Binomial and Beta distributions” p. 4\n-\tIn the definition of C-hat, you’re overloading x on the right side of the given sign. Can you use x’ or something?\n-\tIn the “important case” below Thm 1, can you clarify – this is the mean right?\n-\tProblem formulation in Fast DNN Inference: should the \\hat{y}_i be \\hat{y}_m? I feel like i is not scoped here\n-\tYou define d_m below the problem formulation but it isn’t in the formulation itself.\n-\tTop of p5, you train the network in the “standard way” – this is not clear. Do the gradients at the lower levels flow back through the earlier layers? Or are they stopped and the only gradients are from the final prediction task at the last layer? Either could make sense to me\n-\tShould define more carefully what the “composed classifier” refers to\n-\tClarify how rollouts work – will you observe unsafe states? Sometimes in safe planning you assume you don’t actually observe the unsafe states in training but it looks like you need that\n-\tBottom of p6: what is Z’? Not sure what Z is – is it ordered pairs? Why is the second element always 1?\n-\tP13: why is this an upper bound? It’s hard to parse but it looks equal to the expression in 10 at first glance – it’s the appendix so please explain further. You can also note that the E_t are disjoint by definition\n-\tAlso please add comments about the appendix figures? I have no idea what they are\n \n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}