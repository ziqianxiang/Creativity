{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes Dirichlet Neural Architecture Search (DrNAS), a new NAS algorithm that formulates NAS as a distribution architecture search problem. The paper shows theoretically that DrNAS implicitly regularizes the Hessian norm with respect to the architecture parameters (which has been previously shown to allow more robust architecture search) and presents very strong empirical results on several benchmarks, including the tabular benchmark NAS-Bench-201. \n\nThe reviews and discussion put this paper very close to the acceptance threshold, so I read it in detail myself to act as a tie-breaker.\nI see a lot of positive aspects of this paper:\n+ it tackles a very important and timely problem\n+ the method implicitly regularizes the Hessian norm with respect to the architecture parameters (which has been previously shown to allow more robust architecture search)\n+ empirical results are very strong\n+ the paper includes insightful ablation studies for the most important parts of the algorithm\n+ the progressive architecture learning approach is a general contribution that reduces the memory complexity and also improves basic DARTS\n+ the paper uses a tabular benchmark to yield results that are directly comparable to those of other papers\n+ the method's hyperparameters are kept fixed across the different benchmarks, which underlines its robustness.\n\nThere are also some negative aspects:\n- The method was not originally derived from a Bayesian point of view. Per request of the reviewers, the relation to variational inference has been added to the appendix, but the difference between the L2 regularization and using the explicit KL regularization that falls out of the Bayesian treatment remains. Nevertheless, the new appendix helps to clear up the relationship.\n\n- The paper does not mention availability of the code. This is a must in modern NAS research, as many papers have exposed the poor reproducibility of research in NAS. Fortunately for the authors, I have already seen (independently of this submission) that the code is available on github, but I urge the authors to provide an anonymous repo for review in future submissions, since it was purely by chance that I saw it this time.\n\n- Regarding the point of exploration vs. exploitation, the authors emphasize that in contrast to Gumbel-softmax based methods, such as GDAS and SNAS, with DrNAS there is no need for a cooling schedule. While it is nice to keep the number of hyperparameters small, this also appears to give up control of when the method switches from exploration to exploitation. In practical applications of AutoML, there will be a time budget, and while a cooling schedule can be adapted to fit this budget, it would be suboptimal if DrNAS is still in the exploration phase by the end of the budget, or has already switched to exploitation after, e.g., 5% of the budget. It would be good if the authors could briefly discuss this issue in their final version (if only by acknowledging that this can be a problem).\n\n- Minor negative points\n  * The paper sometimes uses jargon, and I believe not even always correctly: e.g., even by googling I did not find such a thing as the \"iregularized incomplete beta function\", it's also not in the original reference by Jankowiak. I only found the \"incomplete beta function\" (and the regular one). \n  * The author names for several references are garbled. This is likely due to not replacing a comma between the authors with an \"and\" in the bibtex file.\n  * The paper lacks citations for several of the methods it uses, e.g., Adam, cutout, cosine annealing, label smoothing, auxiliary towers, etc. There is no limit on references, and it is standard to cite these concepts to remain more self-contained.\n * The experimental results of GDAS on NB201-CIFAR-100 do not seem to align with the numbers in the NB201 paper. Did you use the numbers from the paper or rerun this method yourself? Please clarify and check this for the final version. The point of tabular benchmarks is to have comparability and consistency across papers! \n * Please have the paper proofread for Grammar again, there are several avoidable errors. E.g., in the first sentence, \"lots of attentions\" --> \"lots of attention\". Also things like \"alone\" -> \"along\", \"down\"->\"done\" etc.\n\nOverall, I think this is a very nice paper, introducing an empirically very strong NAS method that is also theoretically shown to implicitly regularize the Hessian norm with respect to the architecture parameters (which has been previously shown to allow more robust architecture search). I am therefore recommending acceptance. I would like to ask the authors to go through all the reviews again and fix any remaining points in the paper for the final version."
    },
    "Reviews": [
        {
            "title": "Interesting model; missing insights",
            "review": "Summary: This work proposes a modified DARTS optimiser for NAS which assumes a  factorised dirichlet distribution over architecture parameters. It uses pathwise derivatives to learn an MLE estimate of these concentration parameters and adds appropriate regulariser terms to stabilise the training. Furthermore, it employs a protocol for progressively increasing channel fraction to stabilise training within a computation budget. The paper is easy to follow, and relevant experiments have been included. \n\n- With the given probabilistic formulation of this work, it would be useful to include details on how the factorisation of appropriate distributions varies between PARSEC and DrNAS? It might be useful to ground the modelling assumptions within a prior and approximate-posterior framework for better clarity.\n- On the same line it would be useful to get insights on how the obtained models qualitatively differ from ProxylassNAS, PARSEC and SNAS. \n- The interplay of progressive learning with modelling assumption is a bit unclear. The number of parameters and test error in Table 2 are inversely correlated across SNAS, ProxylessNAS, PARSEC and DrNAS which is perhaps not as surprising. I was wondering if authors have any insights on what aspect of the algo (with and without the two stage progressive learning) contributed to network size.\n- Would it be possible to employ the progressive learning policy mentioned in section 4.1 across other DART flavours and understand its impact on model performance?\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Official Blind review #4",
            "review": "Summary:\nThe paper proposes Dirichlet Neural Architecture Search which formulates Neural architecture search as a distribution architecture search problem. They derive a bound that shows that their formulation implicitly regularizes the Hessian norm with respect to the architecture parameters which has been shown to allow more robust architecture search. They empirically show that their method keeps the dominant eigenvalue of the Hessian much lower compared to DARTS. To reduce memory usage and increase training speed, they incorporate partial channel connections and propose progressively widening the channel connections during training. They demonstrate strong empirical results on several benchmark datasets and NASbench201.\n\nReasons for score: \nOverall I would recommend accepting this paper. It proposes a new efficient distribution learning-based NAS algorithm which regularizes the Hessian with respect to the architecture parameters. The algorithm is demonstrated by efficiently finding high performing networks on Cifar10, Cifar100, and Imagenet. They provide ablation experiments that demonstrate the effects of different parts of their final algorithm. While the benefits of using the Dirichlet distribution could be somewhat better demonstrated empirically separate from the progressive search, compared to existing NAS distribution learning algorithms which don't discretely sample, it is beneficial that the trade-off between exploration and exploitation can be controlled by a penalty term as compared to the popular Gumbel-softmax based methods which arbitrarily anneal the temperature parameter.\n\nPros:\n1. Proposed approach is shown to regularize the Hessian with respect to the architecture parameters which works have shown to lead to better network generalization.\n\n2. The paper provides many results that show that it can find high performing networks quickly with limited compute quite quickly.\n\n3. Provides ablation experiments are which demonstrate the benefits of different parts of the algorithm.\n\nCons:\n1. The paper would be improved greatly if the robustness of the search and final network architectures was empirically explored. This could be accomplished by analyzing the architecture distribution while searching on NASBench-201. \n\n2. The direct effect of the regularizer parameter on the hessian could be better explored. For example something like figure A4 with different values for $\\lambda$ would be helpful.\n\n2. The paper would benefit from a proper baseline for the proposed progressive search method. Currently it is somewhat unclear if DrNAS with more time and memory would perform better if used to search the architecture space without a proxy or if the progressive search is regularizing the search to perform better. The reverse experiment of using the progressive search with another distribution learning algorithm would also be beneficial.\n\n\nQuestions:\nWas there a particular reason $\\lambda$ = 1e-3 was used in the experiments?\nIn table 4, do you know if there is a particular reason there is no variance in the results for DARTS, ENAS, and your algorithm. It seems a bit strange that DARTS and ENAS all appear to be stuck in the same quite poor performing local minima in this experiment on CIFAR10.\n\n\nUpdates:\nThanks to the authors for addressing my concerns and responding to my questions. The newly added experimental results make the paper stronger and addressed many of my concerns. I recommend this paper to be accepted.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #3",
            "review": "This paper proposes a differentiable NAS algorithm based on the Dirichlet architecture distribution. Different from the previous differentiable and stochastic NAS algorithms that used the Gumbel-softmax trick or the Categorical distribution, the proposed DrNAS does not require any temperature scheduling for balancing exploration and exploitation, and moreover it does not suffer premature convergence and instability during search. In addition, in order to reduce the memory consumption when searching based on the super-net that mixes all possible operations, the proposed DrNAS applies the progressive learning scheme by combining the network widening with the operation pruning.\n\nPros.\nThe proposed algorithm is technically sound, and the motivation and both of the theoretical and empirical analysis are reasonable to support the use of the Dirichlet architecture distribution with the progressive learning. The experimental results also show that the proposed DrNAS consistently outperforms all previous NAS algorithms on CIFAR-10, ImageNet, and especially NAS-Bench-201. \n\nCons.\nMy main concern of the proposed method is how to produce the sparse solution during search to reduce the architectural bias between the search and retraining phases. This naturally brings up the question of how and when it automatically changes the exploration to the exploitation during search, like a temperature annealing for the Gumbel-softmax trick. How can the proposed method remove the retraining?\n\nWhen performing NAS on ImageNet, why did the proposed method use the proxy task with the reduced training set, even though it can retain a low memory overhead like a discrete architecture sampling such as DSNAS? How is the performance variance of the proposed NAS method on ImageNet?\n\nMinor: what if the proposed method searches different cell structures for each layer without the repetition?\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #1",
            "review": "##########################################################################\n\nSummary:\n\nThe authors present a new differentiable Neural Architecture Search (NAS) method which places a Dirichlet prior on the edges of a cell for NAS: DrNAS. The authors adapt the original bilevel optimisation to incorporate this new prior over the operation mixing weight. The authors add a regularisation to the Dirichlet parameters and effectively study its effect in an ablation. The authors also introduce a progressive architecture learning to make the optimisation more computationally efficient. DrNAS produces very strong results on three different NAS scenarios and is compared to a wide variety of baselines.\n\n\n##########################################################################\n\nReasons for score: \n\nOverall I vote for a weak reject. I think there are some issues surrounding the technical presentation of the work - see the Cons. Also I have some issues with the notation (see below), fixing these will help a lot with the readability of the paper. I have asked for some clarification and I am happy to raise my score in light of clarification from the authors.  The experimental analysis on the other hand is very strong.\n \n##########################################################################\n\nPros: \nInteresting use of the Dirichlet prior for selecting an edge in a cell in the search space for NAS. A hyperparameter is introduced to regularise the Dirichlet parameters and an suitable ablation is provided to demonstrate the regularisation’s sensitivity.\nInference of the prior’s parameters is performed using the new pathwise estimator [4].\nImplicitly controlling the largest eigenvalue of the Hessian is very important for producing good results in differentiable NAS [1]. The authors experimentally show that DrNAS is very effective in doing so. I have a question with regards to Proposition 1 (Question 3), if it is indeed correct then the authors also provide an interesting bound to demonstrate that the objective in Equation 2 is implicitly attenuating the largest eigenvalue of the Hessian of the validation loss wrt to the Dirichlet parameters.\nDrNAS produces very strong results compared to a variety of benchmarks on three different NAS scenarios.\n\n \n##########################################################################\n\nCons: \nClaims in the text are not justified in the paper, see Question 2 and Question 3.\nGeneral issues surrounding notation**.\nThe formulation of DrNAS is not strictly formulated within a Bayesian framework with prior and likelihood to infer a posterior over operation mixing weights. The L2 regularization can be formulated as a Gaussian prior about the Dirichlet parameters. Instead the Dirichlet prior is added to the bilevel optimisation in Equation 1 in an ad hoc manner.\n \n##########################################################################\n\nQuestions: \n\nSection 2.2, paragraph 1: if \\theta is drawn from a distribution why is it less prone to overfitting? The references are text books.\nIn the paper, for instance in the abstract, you claim that DrNAS encourages better exploration. Of course the model has some stochasticity by sampling from a Dirichlet, but what explicit evidence do you have that having \\theta ~ Dirichlet leads to better exploration of the search space for NAS?\nIn proposition 1: it is assumed that \\nabla^2_\\mu \\tilde{L}_{val} is psd, in general the Hessian of a loss function is pd and psd if L is strictly convex and convex wrt \\mu respectively. Does this assumption hold for \\tilde{L}_{val}?\nYou use the pathwise derivative estimator, which is not very popular (I might be wrong here), and you also work the Laplace approximation for Proposition 1 and in related work [3]. What is the advantage of the pathwise derivative estimator versus the Laplace for inference over Dirichlet parameters?\nYou include stds in Table 4, but not in Table 3? How are the errors in Table 3 calculated?\n\n \n#########################################################################\n\nSome typos and notations issues and other comments**: \n\nSection 2.2, paragraph 3, first sentence no “the” before “exploration” needed.\nSection 2.2, paragraph 3, \\beta_0 is not defined.  \nEquation 2, d(.) not defined.\nEquation 3 needs a space.\nTitle in section 2.3 needs a “the”.\nSection 2.3 \\alpha and \\mu are not defined.\nAlthough these works regard Continual Learning (CL): it is worth mentioning two related works including [5] uses a Dirichlet prior over networks for CL and [6] learns new neurons in a Bayesian NN using an Indian Buffet Process prior.\n\n\n#########################################################################\n\n[1] A. Zela, T. Elsken, T. Saikia, Y. Marrakchi, T. Brox, and F. Hutter, “Understanding And Robustifying Differentiable Architecture Search,” in ICLR, 2020.\n[2] S. Lee, J. Ha, D. Zhang, and G. Kim, “A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning,” in ICLR, 2020.\n[3] C. Sutton and A. Srivastava, “Autoencoding Variational Inference for Topic Models,” in ICLR, 2017.\n[4] M. Jankowiak and F. Obermeyer, “Pathwise Derivatives Beyond the Reparameterization Trick,” ICML. 2018.\n[5] S. Lee, J. Ha, D. Zhang, and G. Kim, “A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning,” in International Conference on Learning Representations, 2020.\n[6] S. Kessler, V. Nguyen, S. Zohren, and S. Roberts, “Hierarchical Indian Buffet Neural Networks for Bayesian Continual Learning,” arxiv:1912.02290 2019.\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}