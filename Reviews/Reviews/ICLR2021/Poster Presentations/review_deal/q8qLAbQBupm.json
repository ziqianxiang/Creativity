{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "\nThe paper offers a more systematic treatment of various symmetry-related results in the current literature. Concretely, the invariance properties exhibited by loss functions associated with neural networks give rise to various dynamical invariants of gradient flows. The authors address these dynamical invariants in a unified manner and study them wrt different variants of gradient flows aimed at reflecting different algorithmic aspects of real training processes. \n\nThe simplicity and the generality of dynamical invariants are both the strength and the weakness of the approach. On one hand, they provide a simple way of obtaining non-trivial generalities for the dynamics of learning processes. On the other hand, they abstracts away the very structure of neural networks from which they derive, and hence only allow relatively generic statements. Perhaps the approach should be positioned more as a conceptual method for studying invariant loss functions. \n\nOverall, although the technical contributions in the paper are rather incremental, the conceptual contribution of using dynamical invariants to unify and somewhat simplify existing analyses in a clear and clean symmetry-based approach is appreciated by the reviews and warrant a recommendation for borderline acceptance. \n"
    },
    "Reviews": [
        {
            "title": "Very nice analysis and accurate predictions using symmetry",
            "review": "Pros: \n- This paper is very well-written and motivated. \n- The train of thoughts is explained very clearly, such that I (admittedly not being  an expert in this field) was able to follow. \n- The idea to unify invariances of the loss function by using symmetries and derive corresponding conservation laws (for $\\lambda =0$) in the gradient flow is very elegant. \n- By adapting a modification of the gradient flow from previous works that accounts for the discrete approximation of SGD, the derived theory was able to predict the behavior of the relevant quantities during training to a remarkable accuracy.\n\nCons: I did not find any major drawbacks of this work. Just two small questions:\n- Using $\\ell^2$ regularization on a problem with scale symmetry does not seem to make sense, because the cost function\n$$ \\mathcal{L}(\\theta) + \\lambda \\|\\theta\\|^2 $$ \nwill likely not have a minimizer as soon as $\\lambda >0$. Reducing the magnitude of any $\\theta$ that is optimal for $\\mathcal{L}$ reduces the regularization, but in the limit of $\\theta=0$ the loss might jump up. Thus, the costs are not lower semi-continuous.\n- Additionally, the scale symmetry seems to naturally lead to a discontinuous loss function $\\mathcal{L}$. Is there no problem in even defining the gradient flow for such a function? Which properties of $\\mathcal{L}$ do you need to derive the continuous gradient flow equations?\n\n\nOverall, I really enjoyed reading this paper. Since I am not an expert in the field, I cannot really judge the novelty/contribution, but aside from this aspect, I clearly recommend the acceptance of this work. \n\n-----------\n- There is a typo in Section 6.1 \"graident\"\n- I stumbled upon the NeurIPS 2020 paper \"Reconciling Modern Deep Learning with Traditional Optimization Analyses: The Intrinsic Learning Rate\" by Li, Lyu, and Arora. Based on the abstract, this seems to be a relevant related work. \n\n\n----------------\nAfter the rebuttal: I'd like to thank the authors as well as my fellow reviewers for the interesting discussions and corresponding clarifications. Summarizing my impressions from the discussion, the two main points of criticism are that the proposed analysis is not fully predictive (depends on the norm of the gradients that depend on the empirical data), but rather provides the laws that  govern the dynamics, and that the analysis is based on a time-continuous differential equation that seems to approximate SGD well instead of being applicable to the SGD iterates directly. The validity of the continuous dynamics is demonstrated in numerical results only.  I do agree that a fully predictive framework on SGD directly would be very intersting. Yet, I think the authors are taking important steps towards such a framework, and considering the fact that SGD often behaves surprisingly/unexpectedly (as also stated by R3), I am still quite impressed how accurately the theory matches the actual SGD behavior. For our understanding of how symmetries/invariances in the weights of network architectures influence the training, I believe this paper does provide interesting insights such that I recommend its acceptance. As for a final score, I could go down to a 7 to account for the concerns raised by my fellow reviews, but I think it would mainly reflect my uncertainty about my intuition that a fully predictive analysis on SGD directly might be infeasible, and this aspect should be reflected by the confidence rather than the rating. Thus, I'll give the authors the benefit of the doubt and keep my score, since I really enjoyed reading this paper. ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Clear paper but no significant contribution",
            "review": "This paper studies the dynamics of the parameters while training a neural network via SGD. SGD is not studied directly but three continuous time approximations of SGD are considered: the classical gradient flow, a stochastic differential equation of Langevin type, and a \"modified\" gradient flow whose derivation has roots in the literature. For each dynamics, the authors show that some invariant properties of the loss function (which are often satisfied in practice) imply some invariant quantities for the dynamics.  \n\nOverall, the paper is rather clear. It tries to provide a physical meaning behind the dynamics of learning, which is an interesting question.\n\nHowever, I do not see any significant theoretical contribution. For instance, all derivations are simple differential calculus applications. Moreover, several models for SGD are used, each of them have their own invariant properties (which look alike) and then what? The technical contribution is not clear to me. Finally, the experimental contribution is rather mild because the numerical experiments are not discussed in the main text (except marginally in the conclusion). There is a lot of room for improvement (see below) and I don't recommend the paper for publication in this form.\n\n\nSpecific remarks:\n- The first paragraph of Section 3 is not necessary in my opinion (already explained in the intro). Moreover, the notation for the group action is a bit misleading, e.g. there is confusion between \\psi, \\psi(\\theta) and \\psi(\\theta,\\alpha)\n- Eq 2: the translation invariant suddenly applies only to a subset of the parameters.\n- Figure 1, 2 are not commented in the main text. Same for 3,4,5 (except in the conclusion)\n- The authors could recall Noether's theorem for comparison\n- The models used for SGD are not theoretically justified, except for the classical gradient flow, for which it is standard (Kushner & Yin 2003). For Eq (11) it starts to be sloppy (CLT + Forward Euler). For Eq (13), only intuition is provided.\n- Last paragraph of Page 5. It seems that the discussion applies for any \\xi (not necessarily Gaussian). Does it help to remove the Gaussian noise assumption?\n\n\n\n\n\nTo improve the paper, I suggest the authors to \n- clearly locate their work within the existing literature. This would help to understand why the questions answered in this paper are important, and to highlight their contribution.\n- Turn their paper into an experimental one. To this end, dedicate a whole section to **commented** numerical experiments. This does not mean adding more simulations, just explain them in the main text and how they contribute to the main message of the paper.\n\n\nMinor:\n\nPage 2: \"nornalization\"\nBetween Eq 3 and 4: Notation not defined\nPage 4: ReLU not defined (give the formula)\nSection 5.1, 2nd line. \\nabla is missing \nPage 6: graident\nPage 7 (two times): previosly \n\n\n",
            "rating": "5: Marginally below acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting theoretical perspective on dynamics of DNNs but with some confusions",
            "review": "This paper analyzes the learning dynamics of DNNs from the perspective of symmetry of some parameters. It is interesting to borrow ideas from physics, which I believe is a right way to go.\n\nSpecifically, the paper derives analytical form of parameters under the cases of translation, scale and inversion invariances, and also modified the underlying gradient flows to accommodate stochastic gradients. The results are very interesting, which I like a lot. However, I have some confusions about some results, which make me not able to give the paper a pass at this time. I will consider revising the score if the authors can clear me in the rebuttal.\n\n1. The papers talks about three invariances, and also given some examples of DNNs that satisfies these invariances. From my understanding, these invariances only apply to a very limited cases of DNN structures, e.g., the softmax layer. And in Figure 3-5, the authors verify the convolutional layers for these invariances. It is not clear to me why convolutional layers satisfy these invariances:  translation, scale and inversion. Am I missing something?\n\n2. In 6.1, a new solution for the translation invariance case is derived. However, it seems that the parameters following the solution still converge to zero? This is obviously not the case in practice.\n\n3. Third line below eq.16, it says \"there is a competition between the centripetal effect of weight decay and the centrifugal effect of discretization.\" I don;'t understand why the descretization has the centrifugal effect.\n\n\n==========\nAfter rebuttal: The rebuttal resolves most of my concerns. It is more clear to me that this is an interesting paper on describing the dynamics of DNN parameters in the training, which seems novel to me. I also realize that the closed form dynamics are not for individual parameters, but in terms of some statistics of the parameters, e.g., the sum of the parameters. This makes the results not as existing as what I thought. That is why I decide to raise my score to 6.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Dynamics of some weight combinations of real world DNN can be predicted ",
            "review": "The current work studies the implications of continuous symmetries of a DNN on its weight dynamics. Specifically, they consider linearly realized symmetries (such as a shift/translation to the logits), and use the fact that the loss/hessian/mini-batches, are insensitive to such shift to decouple their dynamics. They do so both in the continuous/vanishing-learning-rate case and for small learning rates and manage to provide accurate quantitative predictions for the dynamics of these quantities.\n\nOn the positive side, the current work makes accurate predictions on DNNs trained in a real-world setting (real datasets, convolutional layers, finite learning rates, mini-batches etc...) which is a very complicated problem. They do so with a refreshing toolbox, that of symmetries. \n\nOn the other hand, what makes their quantities tractable, seems to be the very fact that they have no impact on the DNNs final outputs. For example, this is why they are unaffected by the real-world dataset. While being a clever trick, it can be viewed as an inherent limitation of the approach: one understands quantities that have no bearing on what the DNN learns. I don't see, for instance, how one expects to perform architecture exploration (as mentioned in the discussion), using quantities that have no implications on the DNNs predictions. If the authors can argue that their approach has a broader horizon, it may increase its potential impact. \n\nTwo technical comments: \n\n1. I find the use of subset notation and definition of large theta confusing: It seems that small theta is the set of parameters but it is a subgroup of large theta. \n2. Inversion symmetry commonly refers to a discrete Z_2 symmetry, whereas the authors take it to be some representation of GL1.\n\n \n\n\n\n ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}