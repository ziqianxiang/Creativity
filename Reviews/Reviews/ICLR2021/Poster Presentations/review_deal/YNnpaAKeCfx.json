{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "All the reviewers and I agree that the proposed approach is interesting and the paper is overall well written. However, I agree with R3 that the paper  need further re-working the theoretical part (see the post-rebuttal comments of R4). Thus, I would encourage the authors to carefully address the comments of the reviewers in the revised version of the paper, which would ultimately improve the quality of the paper. "
    },
    "Reviews": [
        {
            "title": "Adaptively select mini-batch for sensitive groups to improve model fairness",
            "review": "The problem is well-motivated. The paper is well-organized and written. The claims are well-supported by theoretical analysis and experimental results. In contrast with previous work that requires nontrivial re-configurations in machine learning, FairBatch formalizes the sampling probability as an implicit connection between the inner (for fairness criterion) and outer (task) optimizer in bilevel optimization.\n[Pro 1] This paper provides insights into fairness and machine learning from the lens of bilevel optimization, emphasizing the inverse proportional relationship between sampling probability and loss for sensitive groups.\n[Pro 2] FairBatch is not only consistent with intuition but also easy to implement.\n[Con 1] It would be helpful for the authors to summarize their contributions more from the fairness aspect. I am confused about the ability of FairBatch to mitigate disparate accuracy. Is it designed especially for the unfairness caused by minimizing average error that fits majority populations [to clarify my point, please refer to the 2nd cause in Section 2.1, abs-1810-08810]?\n[abs-1810-08810] Alexandra Chouldechova, Aaron Roth: The Frontiers of Fairness in Machine Learning. CoRR abs/1810.08810 (2018)\n[Con 2] Is the number of total disparities (as the authors claimed in Section 3.2) d in Section 3 the lambda dimension? No explanation is provided for this notion. If true, is d equal to n_z-1? Why not C(n_z, 2)? (C stands for Combination). In practice, when the sensitive space is sometimes continuous and big, there would be too many pairs to compute. More details about d will strengthen the submission. I am not requesting more experiments, but want to understand how FairBatch works in practice.\n[Con 3] As for the experiments, the analysis fails to go beyond single-dimensional performance summaries. Also, alpha is a vital hyper-parameter. However, the criterion used for selecting the final setting is not clear in this paper.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Nice paper; question about sampling versus weighting",
            "review": "This work proposes a modification to stochastic gradient descent with the goal of having the resulting model satisfy fairness constraints. The modification entails changing the sampling procedure to select the minibatches. Rather than sampling the instances at random, the authors propose a procedure which first generates probabilities for each of the instances by first solving an optimization problem which corresponds to different fairness objectives. Training is otherwise carried out as usual. The authors outline the optimizations corresponding to several fairness objectives, and provide an empirical demonstration which compares the proposed approach to the current state of the art where the proposed method performs quite well.\n\nOverall, I think this is a nice, simple, idea which appears to work well in practice. The authors do a commendable job of describing both the problem and solution in plain terms while also providing technical details of the implicit weighting procedure.\n\nWhat I donâ€™t quite understand is the advantage of modifying the batches versus incorporating the inclusion weights in the loss function itself. It seems that the proposed model will be equivalent to weighting the loss as the number of mini batches grows very large, however the experimental results seem to indicate otherwise. Can the authors provide some intuition for this? Specifically, should we expect equivalent performance between the proposed method and example reweighting if the same probability weights are used as proposed in the paper and we weight random examples by those probabilities as the number of minibatches grows? If so, how should we be thinking about this tradeoff in the case where we use a small to moderate number of minibatches? \n\nSmall edits:\n- In the first sentence of 3.1 there is a misspelling of optimization.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review of FairBatch: Batch Selection for Model Fairness",
            "review": "The paper addresses the problem of fairness by viewing the task of learning as a bilevel optimization problem. The task of learning is decomposed to two levels: 1- Choosing a batch of data samples so as to satisfy a given fairness criterion; 2- training the model using the chosen batch. The main contribution of the paper is that it proposes a way to sample batches of data that would satisfy a given fairness criterion and incorporates that into model training. The authors show the performance of models using their proposed approach on synthetic and real-world data. \n\nThe paper is well-written and points to a promising direction in handling fairness using (non-causal) notions of fairness. The point is to find batches of data that mitigate disparities in prediction. The batches are chosen by finding lambdas that minimize the disparity in prediction as opposed to necessarily setting them to zero. This is a fine distinction. The update rules of the resulting optimization problem are also presented. \n\nThe proposed approach is tested on multiple datasets. According to Tables 1 and 2, FairBatch does not necessarily improve performance. In some cases, the performance is lower than that of other methods, but given that overall, the performance is comparable with other methods and FairBatch is sometimes also faster in number of epochs, one can say that the potential in FairBatch is worthwhile. \n\nMinor comments:\n\nIn Figures 3 and 4 of Section B4, it would be better if we can see the convergence rates plotted in one figure, using different colors for each algorithm. This way, one can compare the results more efficiently.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "FairBatch: Batch Selection for Model Fairness",
            "review": "In this paper, the authors study the problem of training fair machine learning models through the lens of bi-level optimization. In particular, they propose a method, denoted FairBatch, that adaptively selects different batch-sizes for different protected groups to impose a certain measure of fairness. This is achieved by solving an outer optimization problem that imposes fairness by computing a batch-size ratio for different groups. This batch selection is then used to train the original model. The proposed approach can impose several prominent fairness measures, among which: equal opportunity, equalized odds, and demographic parity.  To demonstrate the efficiency of FairBatch, the authors conducted several experiments on synthetic and real datasets. The results show the proposed method is easily implementable, scalable and can achieve comparable performance when compared to existing methods. \n\nPros:\nThe authors approached the problem of fair machine learning by iteratively adapting the batch-size ratio for different protected groups. The approach is novel and was implemented through a novel bi-level optimization formulation. \n\nThe proposed method is easy to implement, scalable, and can readily improve fairness for any pre-trained model. \n\nClarity: The main body of the paper is concise and clearly written with very few typos.\n\nCons:\nThe theoretical part of the paper requires a more careful and detailed analysis. First, the inner minimization problem in section 3.1 has $w_{\\lambda}$ appearing in the objective. I think this should be $w$ instead. This also affects the proof of Lemma 2 in the supplementary material. Moreover, the author computes the gradient of the objective of the outer optimization problem, $F(\\lambda)$. Being a finite max, this function might not be differentiable. \n\nIt seems that positive definiteness assumptions in sections A.1 and A.4 are not easy to check. The authors did not comment on the practicality of these assumption nor they provided a method to check whether they hold. Moreover, I believe these assumptions should be moved to the main body of the paper.\n\nI believe the paper is worth publishing after re-working the theoretical part.\n\nMinor Comments:\n1.\tPage 3: In defining the bi-level optimization problem, the objective function of the inner minimization is a function of $w$. This should be explicitly included in the formulation (it is currently hidden in $\\hat{y_i}$) \n2.\tIn the proof of proposition 2 in Appendix A, should it be $\\ell(|1-y|, \\cdot) = 1 - \\ell(y,\\cdot)$?\n3.\tIn Sections A.2 and A.4, the subscript of the functions $f_i$ and $g_i$ are missing in some expressions (also in Lemma 1). \n\nThe related material are referenced and well-discussed in the paper. The authors clearly positioned their work in the related field and discussed their contributions in comparison to other similar works.\n\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nUpdate after the author's response:\n\nI was not convinced by many of the authors' responses.\n\n1. While the author(s) agree that $F(\\lambda)$ is non-differentiable, they keep the gradient updates in the paper with a footnote referring to sub-gradient methods in (Boyd et al. 2004)? Moreover, it is not clear what the sub-gradient would refer too when the loss function is non-convex, what would $H_{\\lambda}$ refer to in section 3.2? One possible way to solve this issue might be through a mild smoothing technique. But as presented the theoretical part is not rigorous.\n\n2. The positive definiteness assumption in Lemma 1, although will most probably hold when the three functions are convex; this limits the application of the results as typical ML loss functions involve non-convexity.\n\n3. My comment on the subscript of the functions $f_i$ and $g_i$ in Lemma 1 was addresses by adding a comment mentioning that the subscript will removed for the simpler notation. I believe one subscript does not make the notation complex. Rather defining two notations might confuse the reader.\n\nDespite my tendency to have this novel idea and work published, I believe the authors need to be more careful in writing and dealing with the theoretical section of the paper. I will hence decrease the score to 4.\n\n\n",
            "rating": "4: Ok but not good enough - rejection",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}