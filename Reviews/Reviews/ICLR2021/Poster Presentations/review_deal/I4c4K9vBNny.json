{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This work proposes a novel network structure, spatial dependency networks that is introduced as an alternative to convolutional neural networks. This new architecture is used successfully to get state of the art performance for a number of common image generation benchmarks when compared with non-autoregressive approach (even much larger CNNs). There is a lot of useful feedback in the reviews themselves: a thing to consider in the final version is the fact that the authors had motivated SDNs as drop-in replacements for CNNs, but do experiments mostly in VAE-like settings. This is a point that was raised by multiple reviewers and is clearly something that should be dealt with as explicitly as possible.\n\nWhile there are legitimate reasons to be wary of the increased computation time, I tend to side with the authors that baselines that are being compared with SDNs are likely to have more optimized primitives. From the inference numbers presented in the rebuttal, it doesn't appear like the speed issues are insurmountable. \n\nGiven the high quality of writing, the excellent performance on image density modeling, the various ablations and understanding of the disentangling effects, I think this is an interesting piece of work that the field would benefit from."
    },
    "Reviews": [
        {
            "title": "Encouraging results, but perhaps somewhat limited in scope",
            "review": "**Summary**: This paper proposes a network architecture, spatial dependency network (SDN), that attempts to more explicitly model spatial dependencies, as compared with convolutional networks. The network involves three steps: projection (via 1x1 CNN), a spatially (by row/column) autoregressive transform applied in each direction, and another projection. The authors use the architecture in place of CNN layers in the IAF VAE architecture. However, note that this is in the conditional mappings and *not* the flows. The model achieves state-of-the-art performance, as compared with non-autoregressive models. The authors also demonstrate improved disentanglement on a single-level VAE.\n\n**Strong Points**: The technical aspects of the approach are generally well-described and clear. I found the diagrams and algorithm box helpful in understanding SDNs. Likewise, the adaptation of the diagram from Kingma et al., 2016 was helpful to distinguish which components of the architecture were modified in SDN-VAE over the IAF architecture. The basics of variational inference and VAEs are described concisely and correctly.\n\nWhile I did not find the experimental results entirely unsurprising (see below), the experiments do appear to be rigorous. The authors demonstrate their network within a relevant architecture (IAF) on multiple benchmark image datasets. Results are compared with relevant recent approaches, where SDN-VAE outperforms non-autoregressive models. While this might be attributed to an increased number of parameters, the authors also conduct an ablation study to demonstrate that SDNs outperform much larger CNN architectures. This suggests that SDNs truly provide a useful inductive bias toward modeling dependencies between dimensions, analogous to how ResNet architectures typically outperform deeper non-ResNet architectures.\n\nThe results on latent factorization are also rigorous. The authors evaluate performance with multiple $\\beta$ values and report latent factorization using two metrics. Results are reported using 10 random seeds, helping to confirm that the improvement in disentanglement is significant.\n\nThe results generally seem to be reproducible, as algorithmic and architectural details are present in the main paper and the appendix. The main results (SDN-VAE) are demonstrated using an existing model architecture (IAF), so the majority of other modeling design choices are already publicly available.\n\nThe paper is formatted well, with clear titles for sections, figures, tables, etc. This will be helpful for unfamiliar readers.\n\nThe approach is not entirely novel (see below), but admittedly, many current VAEs exclusively use convolutions in the generative and inference models. This paper, while also using convolutions, introduces autoregressive transforms throughout the conditional mappings. This is potentially a worthwhile contribution, moving the generative modeling community toward other forms of functions (beyond feedforward and convolutional) in the mappings.\n\n**Weak Points**: While I agree with the approach, I felt as though it was poorly motivated. The authors attempt to motivate SDN by discussing the importance of modeling spatial dependencies between pixels in images, often appealing to notions of “coherence.”  For instance, the authors claim that SDNs “explicitly model image-level spatial coherence and dependencies.” However, from what I can tell, coherence is never defined. I’m assuming that the authors are referring to modeling the full joint distribution over dimensions, rather than assuming independence between dimensions. (Note that VAEs can already model such dependencies through their latent variables.) The closest that the authors come to explicitly motivating the difference from previous approaches is in the “Comparison to CNNs” in Section 2. However, this discussion incorrectly casts the network input/output as random variables, when they are, in fact, parameterized by deterministic functions.\n\nThe approach is somewhat lacking in novelty. Admittedly, the particular architecture itself is novel, and it is likely a useful improvement. However, it belongs to the larger class of neural autoregressive density estimators (NADE) and autoregressive models. Effectively, this is replacing a direct mapping with an autoregressive mapping, which has been used before, e.g. PixelCNN decoders in VAEs. The authors attempt to distance their work from autoregressive models in Section 2, saying that these are “conceptually very different,” yet autoregressive models can also contain multiple orderings of dependencies. This is the idea behind permutation/reversal operations found in autoregressive normalizing flows (which generalize Gaussian autoregressive models).\n\nGiven the similarity with previous autoregressive modeling approaches, I found it surprising that the authors chose to parameterize the conditional mappings, rather than the distributions, with SDNs. This is particularly surprising given that the authors chose the IAF architecture as their baseline, which includes autoregressive normalizing flows at each level of latent variables. While it’s unclear which form of model should be preferred, it seems like an obvious choice to at least evaluate using SDNs to parameterize the normalizing flows (e.g. IAF used MADE to parameterize the autoregressive flows).\n\nMore generally, I found it surprising that the authors chose one particular model class to demonstrate their network architecture. Convolutional networks are used widely in many classes of generative models, as well as discriminative models. If SDNs are a swap-in replacement for CNNs that are truly better at capturing spatial dependencies, then this should be apparent across multiple settings, not just latent variable models (VAEs). Experiments across multiple tasks and model families would help to improve the impact of the paper.\n\nThe main experimental results of the paper are 1) improved generative modeling on image datasets, and 2) improved factorization in single-level models. Given that SDNs are more powerful than CNNs, neither of these results are unexpected. Assuming one can avoid local optima, an improved conditional mapping should yield better density estimates, as well as map a standard Gaussian to data estimates (factorization). I do not expect every experimental result in a paper to be surprising, but it’s unclear what we learn from the results in this paper (other than the fact that SDNs work in practice).\n\n\n**Accept / Reject**: I found this paper somewhat difficult to assess. While the SDN-VAE model achieves state-of-the-art log-likelihood results on multiple image datasets, the method itself is somewhat lacking in novelty and does not seem to appreciably improve our understanding of these models. Autoregressive transforms are already heavily utilized in generative modeling, so it is generally unsurprising that including more autoregressive computations in the model will improve performance. Similarly, it seems fairly intuitively obvious that an improved conditional generative mapping should improve disentanglement, as it can more easily warp a (disentangled) Gaussian random variable into the data distribution.\n\nFurther, the authors present SDNs as a swap-in replacement for CNNs, however, experiments are only performed in one setting: explicit latent variable models of images. Spatial dependencies are relevant for any image dataset, and other data modalities more broadly. This paper would benefit from exploring these other settings (other generative models and discriminative models). Similarly, I would have liked to see the authors use their autoregressive transform within normalizing flows. This seems like a natural use-case.\n\nThe above points aside, developing improved network architectures is still a worthwhile endeavor. I’m hopeful that SDNs could become a standard network layer within the image modeling community. More broadly, it may help researchers to consider spatial dependencies more explicitly when constructing network architectures. For these reasons, I am slightly in favor of accepting this paper.\n\n**Additional Feedback**:\n\nAbstract:  \nThe first sentence is grammatically incorrect. It is missing a subject.\n\nIntroduction:  \nIt would help to clarify what is meant by the word “coherent.”  \nSimilarly, it would help to briefly describe why CNNs (supposedly) cannot explicitly model spatial dependencies and coherence.  \nFigure 1: should label the convolutional filters. Also, I understand that this is a generic overview of SDN-VAE, but it is somewhat too simplistic to be useful.  \n\nSDN Architecture:  \nI found some of the notation confusing. For example, if $\\mathbf{B}^{(1)}$ is a matrix, then why is being added to a vector in Eq. 1?  \nThe tanh on the input (in Algorithm 1) could be explained further. Is this for stability?  \nIt would be helpful to discuss an order-of-magnitude comparison of the number of parameters between SDNs and CNNs.  \nThe gated residual is referred to as a “highway” connection.  \nIt seems inaccurate to express the output of SDNs as a probability (though I understand the point).  \nIn related work, I would discuss (autoregressive affine) normalizing flows, e.g. IAF, MAF, etc., and autoregressive neural networks, e.g. NADE, MADE, EoNADE, etc. These seem highly related.  \n\nSDN-VAE:  \nShould also cite Rezende et al., 2014 for the reparameterization trick.  \nShould mention that the approximate posterior in top-down inference is structured, i.e. it models dependencies between levels of latent variables.  \n\nImage Density Modeling:  \nI would not necessarily consider density estimation and image synthesis as different “tasks.”  \nIn the maximum likelihood objective, missing a $1 / N$ factor.  \n\nLearning Disentangled Representations:  \nI would disagree that non-hierarchical models are more suitable for representation learning. Hierarchical models generally extract increasingly abstract features, which can be quite useful.  \n\nConclusions:  \nSomewhat misleading to compare SDN-VAE with GANs. There is nothing intrinsic to SDNs that allows them to calculate explicit density estimates. Indeed, SDNs could also be used in GANs.  ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Interesting approach that seems to improve performance in VAEs (at the cost of longer training/inference time)",
            "review": "This paper introduces a new neural network layer architecture (spatial dependency network - SDN) that can be used in place or in conjunction with traditional convolutional layers. The SDN layers' main difference to convolutional layers is that in SDN layers extracted features are dependent and influenced by nearby features. SDNs are evaluated with VAEs on density modeling and learning of disentangled representation and show better performance than purely convolutional baseline architectures.\n\nStrengths:\n- The new layer architecture can be used as a drop-in replacement for convolutional layers and shows promising results on the evaluted tasks\n- Evaluation is done on two different tasks and shows that SDNs perform better on both tasks than convolutional VAEs\n- Using SDNs seems to support sampling from VAEs at much higher temperatures than usually possible\n\nQuestions/remarks:\n- What is the relationship (similarities/differences) between SDN and normal (self-)attention? Why would SDN be better than attention (except maybe faster runtime)? What can SDN do that attention can't (in principal or in practice)?\n- Section 2, \"project-in\" stage: why are the project-in and project-out states needed? Can't the corrections stage be performed on the input features directly?\n- Figure 2 (b): \"Solid arrows represent direct dependencies, and dashed indirect ones\" -> what exactly do you mean with \"indirect dependencies\"?\n- Avoiding vanishing gradients: do you actually ever observe vanishing gradients? have you tried training with/without any normalization methods to see if vanishing gradients become a problem?\n- SDN-VAE architecture: why do you only use SDN in the decoder? Have you tried using them also in the encoder?\n- How much overhead does SDN in the SDN-VAE actually introduce in practice - regarding model size/VRAM and training time? How much bigger is the SDN-VAE compared to the baseline and how much longer does it take to train/per iteration? I see some of these answers are given in the tables in the appendix but I think it would be beneficial to be open about this and mention the trade-off in the main part of the paper.\n-Learning disentangled representations: why do you think SDN helps with learning disentangled representations?\n\nOverall I think this is a good paper. It would be interesting to see further results with SDN, e.g. on a baseline DCGAN architecture, to see if the improvements transfer to other architectures.\nI also believe the paper would benefit from a discussion comparing SDNs to traditional attention and when SDNs might be a better choice than attention. It would also be interesting to know a comparison of attention  vs SDN in terms of training and run-time. Attention has a higher complexity in O notation but it is not clear what exactly this means in practice, especially when attention/SDN are only used on \"small\" feature layers early on in the decoder.\n\n###\n###\n\nUpdate after revision: I have looked over the revised paper and believe the authors have addressed most issues that were raised by the reviewers, especially by describing in more depth the relation of their approach with autoregressive models and self-attention and mentioning the runtime differences of their model compared to a normal CNN.\nI have, therefore, raised my score to 7.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good results; discussion/comparison to related work could be improved",
            "review": "Summary: The authors propose a new building block called Spatial Dependency Networks (SDN) that increases the expressiveness of a CNN at the cost of some sequential computation. SDN propagates information across a 2D feature map using LSTM-like updates across the rows (or columns). They use SDN blocks to improve the decoder architecture of IAF-VAE [1], a SOTA VAE-based method, and show a substantial performance boost on generative model tasks. Their method performs much better than other VAE methods, and almost as well as SOTA autoregressive models.\n\nStrengths:\n\nSolid empirical results & thorough experiments\nPaper is mostly well-written & many experimental details are included.\nWeaknesses:\n\nThe motivation seems a bit fuzzy to me: the primary contribution of this work seems to be the SDN building block, but it seems like SDN isn't necessarily limited to generative modeling? The authors clearly discuss the advantages of SDN over CNN, but the experiments focus on generative modeling VAEs, but not any of the other tasks where CNNs are used & more expressive architectures might be helpful.\nThe discussion of related work is quite limited, in particular with respect to autoregressive models. The SDN block is quite reminiscent of the Row LSTM from PixelRNN [2], a pioneering autoregressive model. Autoregressive models have been known to perform the best density estimation (compared to other families of likelihood-based generative models), at the expense of slow sampling speed (since each element must be sampled sequentially). Since SDN introduces some sequential dependencies, it trades off computation during sampling time vs during likelihood evaluation (autoregressive models are slow at the former but fast at the latter; the proposed method is somewhere in the middle but the same speed for both). I would like to see some more discussion along these directions to better place the authors' method into context with related work.\nOn a similar note, the authors allude to the extra computational overhead incurred by adding SDN to VAEs, but I don't think they explicitly state what that is? This would also help frame the contributions of their work.\nNit: the description of SDN in Algorithm 1 is a bit confusing -- it doesn't seem to convey the sequential nature of SDN? From reading Algorithm 1 only, it sounds like SDN could be implemented via masked convolutions + gating, but this seems to disagree with the rest of the paper? Also, the output variable I_+ doesn't seem to be used?\nOverall, I would slightly lean towards acceptance (conditioned on improving the discussion around the points from above), but am curious to see what the other reviewers think.\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A nice idea with computing time drawback",
            "review": "1. Summary. This work is in line with the latest attempts[1,2] to improve the VAE quality by employing more powerful decoding architectures. The authors propose a new block that handles (non-local) spatial dependence of pixels at the expense of increased computation time - O(scale). The presented experiments demonstrate that usage of the suggested layer is beneficial for the tasks of density estimation and image generation.\n1. Decision. The described idea is novel, clear, and supported by experiment results. To my mind, the provided evaluation is thorough enough, the ablation study is quite comprehensive. Therefore, I tend to vote for acceptance. However, major issues of this approach are the increased number of parameters and computational time. This can make the applicability to other datasets limited.\n1. Questions. \n    1. Is it sufficient that the sweep from Algorithm 1 is GRU-like, or, more generally, gated? Could it be replaced with a masked-convolutional sweep with the output depending only on the \"past\" (row number <= i) and the pixel (i, j)'s value itself?\n    1. Am I right you employed Monte-Carlo integration to provide NLL in Table 1 for VAE-based models? How many samples did you use?\n1. Remarks. I believe the headings of Tables 2 & 4 are a bit misleading: ELBO is a lower bound (the higher score - the better), however in your notation the less - the better.\n\n[1] https://arxiv.org/abs/2007.03898\n[2] https://openreview.net/forum?id=RLRXCV6DbEJ\n\n#############\n\nAfter reading the rebuttal I confirm the initial rating.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}