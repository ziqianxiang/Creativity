{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper proposes an unsupervised representation (embedding) learning method for time-series. Overall, the paper is well-motivated, well-written and easy to follow. As agreed by all reviewers, the idea is interesting. To further improve the paper, the authors are encouraged to justify the choice of encoder architectures and window size, and describe more clearly how the statistical test is incorporated."
    },
    "Reviews": [
        {
            "title": "Has potential to be a good paper with edits",
            "review": "Summary\n\nThe paper proposes an unsupervised representation (embedding) learning method for time-series. While unsupervised representation learning has been extensively studied and shown good performance in fields like NLP and vision, it is relatively new to the time-series community. This paper, in contrast to recent work (CPC and Triplet-Loss), has the following differences:\n1. It estimates stationary temporal windows using statistical testing for stationarity/non-stationarity. \n2. It learns embedding using contrastive learning as in CPC and Triplet-loss, but additionally takes into account that fact that naive negative sampling may include false negatives and hurt embedding learning on time-series that have strong seasonality. Instead, it adopts the framework Positive Unlabeled learning to address this issue.\n\nQuality and clarity\n\nI believe the work itself is of good quality (or has the potential to be), but the presentation of the work is not very clear to me:\n1. The neural network architecture adopted is not well explained: if the architecture varies across problems then the authors should clearly state this and provide more explanation on the general framework using Figure 1; currently, the framework in Figure 1 is also not well discussed. \n2. The way how the statistical test (Augmented Dickey Fuller) is incorporated for window-detecting is not clear. According to the explanation in Section 2, \"For every W_t, we gradually increase the neighborhood size and measure the p-value from the test\", it seems the windows will have varying sizes according to test p-values; but in all experiments, the window sizes are fixed across the entire time-series. The authors should provide some explanation to this.\n\n\nOriginality\n\nThe paper builds on existing framework: contrastive learning based unsupervised representation learning, but proposes a new method to cater to the peculiarity of long-term time-series (non-stationarity and seasonality).\n\nSignificance\n\nGiven that exploration of unsupervised representation learning in time-series is at early stage, I believe this work will inspire future works along this direction.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "An advanced HMM",
            "review": "The authors defines the notion of temporal neighborhood to segment time series.\nIt is a location in real vector space where neighboring segments in the original time serie are projected into as well as similar but distant (in time) segments.\nThe model is trained using a discriminative loss that pushes away (in the feature space) windows distant in time while caring for the possible periodicity [Elkan & Noto 2018] \n\nOverall the paper is well written and easy to follow. \nThe idea is very interesting and I see several potential consequences and use-cases. \nNevertheless, I have several concerns.\n\n1/ I think the paper lacks classic baselines. \nI consider that HMM is part of unsupervised representation learning. \nIn its original form it does not include NN but, if needed several works have studied variants, eg.:\nFiroiu, L., & Cohen, P. R. (2002, July). Segmenting time series with a hybrid neural networks-hidden Markov model.\nThe authors rightfully raised the issue of how large (in time) a neighborhood is (\\eta). For that, Hidden semi-Markov models constitute a nice \"dynamic\" approach.\nAs well, HMMs are used to create the synthetic data-set.\n* Ok I see, interesting results. Thanks.\n\n2/ Regarding the PU methods, why choosing solution 2 over 1?\n* Thanks for the answer. My question was suggesting that it should better justified in the text.\n\n3/ \"For an ideal encoder that preserves the neighborhood properties in the encoding space, p(Z_l ∈ N_t ) should\nbe close to p(W_l ∈ N_t ), where Z_l is the representation of W_l\"\nI understood that N_t is in the encoding space. So the same letter can not be used for W_l.\nI don't see how the training steers the encoder to satisfy this property.\nOn the other hand, this constrain contradicts the PU discussion.\n* Ok I understand better. thanks for the answer.\n\n4/ Figure 1 is not very clear. The encoder outputs Z_t and P(Z_t)\n\n5/ The same network is used for all the baseline. However, Franceschi et al.'s work clearly relies on convolutional layers. Hence not using such is a bit unfair toward their method (synthetic data and HAR). \n* I understand your point. Nevertheless some models are very depend on the architecture, since the depend on some feature that not all architecture can provide. And I think your results show that. \n\n6/ Evaluation: Trajectory. Figure 3. Very interesting plot that clearly shows the benefit of this work in terms of interpretability. What happens for a 3 dimensional encoding? \n* Here it is my fault, I meant 4 dimensional encoding: as much as the number of hidden states. The point was to see if the model is would learn clearly different representation for each state.\n\n7/ The notion of  temporal neighborhood depends on \\eta and is based on a Gaussian distribution around the current timestamp. How does it behave for small and large eta? \nA model with a small eta might detect too many segments. \nIf large, I guess, the models smooths too much the variations and fails to cluster different behavior. Especially it will fail to detect abrupt changes.\nIn any case, such an analysis is missing.\n* Thank you for the explanations.\n\nI have spotted two typo in the last paragraph of page 3:\nNote that the Discrimin[a]tor...\n...the latent space, since [it] allows...\n\n\n* I update my score.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Leveraging the concept of a temporal neighborhood seems beneficial for capturing non-stationary properties of multivariate time series and may have applications in a spectrum of domains (including healthcare). Nevertheless, I feel there are some points (marked as “Improvement points”) which require additional attention. I hope that the authors can address these remarks in the rebuttal period.",
            "review": "Paper Summary:\n\nThis paper proposes a self-supervised encoder-discriminator based framework for embedding the multivariate time series into a compact fixed dimensional representation. The approach dubbed Temporal Neighborhood Coding (TNC) leverages the concept of a neighborhood in time (with stationary properties), and learns time series representations by ensuring the distribution of neighboring signals is distinguishable from the distribution of non-neighboring signals, in the encoding space. Empirical evidence is provided that such embedding of time series results in clusters of higher quality, as well that use of such obtained representations for supervised tasks outperforms few competitor (unsupervised) approaches. \n\n##########################################################################\n\nStrong points:\n1. The neighborhood-based unsupervised learning framework accounts for dynamic changes that occur among samples, i.e. non-stationarity of signals.\n\n2. The introduction of the concept of a temporal neighborhood as the distribution of similar windows in time, whose range can be automatically estimated by analyzing the stationarity of time series with statistical tools such as the Augmented Dickey Fuller (ADF) test.\n\n3. Leveraging the sample weight adjustment, a concept from Positive Unlabeled (PU) learning, to account for the potential bias introduced by sampling negative examples which is a common issue most contrastive approaches suffer from.\n\n4. The proposed TNC outperforms Contrastive Predictive Coding (CPC) and Triplet-Loss (T-Loss), both being state-of-the-art unsupervised representation learning approaches.\n\n5. The paper is well structured and written in a fairly clear and comprehensive manner. \n\n##########################################################################\n\nImprovement points:\n1. Efficiency and scalability to high dimensions are mentioned as merits of TNC in the introduction, but are never discussed in further detail. Could the authors elaborate a bit on the components of TNC that allow for its claimed efficiency and scalability and how does TNC compare on these two fronts to other unsupervised representation learning approaches?\n\n2. A bidirectional, single-layer RNN encoder is used in the experiments on the Simulated and HAR datasets, whereas a 2-channel, 1-dimensional strided CNN encoder was applied on the ECG Waveform dataset. The reason behind this decision is not completely clear to me. I would encourage the authors to clearly justify the choice of different encoder architectures in the two cases.\n\n3. In the cases where an RNN architecture is considered for the encoder, the authors could have considered time-aware RNN/LSTM autoencoders as additional baselines. Some of these methods include:\n- Baytas, I. M., et al. Patient subtyping via time-aware LSTM networks. In Proceedings of the 23rd ACM SIGKDD (2017).\n- Wenjie Pei and David MJ Tax. Unsupervised Learning of Sequence Representations by Autoencoders. arXiv preprint arXiv:1804.00946 (2018).\n\n4. Although the neighborhood range $\\eta$ can be automatically determined, choosing the window size $\\delta$ still remains unclear, as the performance of TNC under different window sizes was not analysed. Moreover, the selected window sizes seem to be different for different datasets (for instance, a window size of 50 is used for the Simulated data, while a windows size of 4 is used in the case of the HAR dataset). I would like to ask the authors to provide some guidelines on how the window size is (or can be) selected?\n\n5. Some statements appear to be vague and leading away from the main point. This includes usage of phrases such as:\n- “Time series are often *complex*” - as in having multiple interacting parts? (I don’t feel that aspect was crucial for paper topic)\n- “self-supervised framework for learning *robust* and generalizable representations” - how was the robustness defined and assessed? (no empirical evidence regarding the ‘robustness’ of the learned representations is presented).\n- “the representations are general, *transferable*, etc.” - transferable in what sense? (I believe that “applicable across multiple supervised tasks” would be a more appropriate choice of words in this context).\n\n6. There are several minor textual and format typos throughout the paper that can be easily addressed. Some of them are summarized as follows:\n- In the second sentence in section 3, consider replacing “state of the art” with “state-of-the-art”.\n- In the last sentence of the introductory part of section 3, replace “:” by “.”.\n- The term “GPs” is used in section 3.1, but is not defined earlier in the text.\n- In section 3.3, “concatenate activity samples” should be replaced by “concatenate the activity samples”.\n- In the last couple of sentences in section 4.1, replace “original time series” and “coherent cluster” with “the original time series” and “coherent clusters”, respectively.\n- There are certain instances of informal language use, such as “doesn’t” and “don’t”. Please consider replacing them with “does not” and “do not”, respectively.\n\n##########################################################################\n\nQuestions during rebuttal period:\nPlease address the aforementioned remarks/questions.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Unsupervised Time Series Embeddings for Clustering and Classification",
            "review": "The authors propose a novel unsupervised encoding scheme for time series. Utilizing a statistical test for non-stationarity, the authors derive a Temporal Neighborhood Coding (TNC) scheme and combine it with ideas from Positive-Unlabeled (PU) learning to learn informative hidden representations of time series windows. The representations are evaluated in terms of how well they can be clustered and how much they influence classification performance on three data sets. The supreme performance was demonstrated when comparing to the state of the art methods and a $k$NN (for classification) baseline. Furthermore, the authors illustrate how the learnt representations remain interpretable as long as the encoding network is reasonably small. \n\nThe presented work is very well motivated and described. The presentation of the ideas is clear and I think the authors did a great job making the manuscript accessible. I like the simplicity of the figures which do not suffer from too little information content. Starting with the analysis of a synthetic data set followed by two real-world data sets is a commendable route to choose for this kind of work. I particularly like the analysis of low sample size-high length ECG data set. I would say job well done! One aspect that is not quite clear to me is how do you combine the ADF test with the fixed-length window sizes that you mention in your experiments?\n\nSome minor comments/questions:\n\n•\tI am somewhat confused about the window size information you give in the experiments and for example in Figure 3. Here you say $\\delta=50$. I thought the window size is changed dynamically based on the ADF test which leads me to the second question\n\n•\tIs the process of determining the window size a preprocessing step or is it part of an end-to-end framework? \n\n•\tHow do you solve the classification task when you have an encoding for each window? I assume you do not have one specific label for each window. If every single measurement received a ground truth label, isn’t it possible to extract a window annotated with two different labels? Or do you take care that the windows you extract are always centred around the QRS-complex or always starts with the P-wave? \n\n•\tIt would also be interesting to evaluate how you could perform patient-wise classification (e.g. diagnosis prediction). Maybe it would be an impactful extension to find a good aggregation scheme for the embeddings to summarize single patients. \n\n\n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}