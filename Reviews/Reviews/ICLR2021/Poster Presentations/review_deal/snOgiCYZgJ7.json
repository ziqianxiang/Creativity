{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "Four knowledgeable referees support acceptance for the contributions, and I also recommend acceptance. There is agreement among all reviewers that this paper is about  a highly relevant topic, that the model presented is technically sound and has significantly novel aspects, and that the experimental results are convincing. There were several points of criticism raised by the reviewers, concerning, for instance, further comparison experiments,  the heuristic nature of masking rules, or the treatment of homologous sequences. In my opinion, however, most of these points have been addressed in a rather convincing way during the rebuttal phase.  "
    },
    "Reviews": [
        {
            "title": "Interesting work; needs more clarification",
            "review": "**Summary**\nThe work studies of the problem of jointly modelling RNA sequence and secondary structure using the framework of variational autoencoders. Several encoder and decoder architectures with increasing inductive bias for RNA structures are proposed. These are compared on two novel RNA sequence datasets and supervised and unsupervised RNA generation tasks.\n\n**Score justification**\nThe work presents a comparison of several interesting approaches to the problem of joint modelling of RNA structure and sequence using VAEs. Encoders and decoders that make use of (i) RNA sequence and structure in a string representation; (ii) in a graph representation; and (iii) in a junction tree representation are considered. This works particularly well on the encoder side, where the appropriate deep learning architectures are used; but becomes considerably less elegant on the decoder side, where the authors have to make use of sample masking with complex rules to prevent generation of invalid sequence-structure combinations. Furthermore it is not immediately clear from the empirical evaluation that the added complexity of the HierVAE, or that using VAEs are an optimal choice for the supervised generation task.\n\n\n**Major comments**\n* As noted by the authors, their decoders (linearized sequence and structure, as well as the hierarchical decoder) may generate invalide sequence-structure combinations. To eliminate this possibility the authors restrict the autoregressive decoder from sampling invalid sequence-structures by masking out some samples using a number of heuristic rules. First of all, it is unclear from the text whether this is also done during training and how the probabilities at masked stages are treated (are they re-normalized after the masking is applied?)\n* I appreciate that the authors include a comparison between decoding with and without the heuristic masking rules. Could the authors explain why the FE DEV in Table 1 is lower for samples produced without the masking rules (unconstrained generation)? This result seems counter-intuitive.\n* From the results in Table 1 it is still difficult to tease apart the relative contributions of (i) model training; (ii) inductive bias of decoder architecture; and (iii) decoding constraints. I would appreciate it if the authors also included decoding results for untrained decoders with and without constraints.\n* Comparing the AUC ROC in Table 2 to the results in Tables S2 and S3 it is not obvious what the benefit of using a VAE setup for the supervised problem is. The purely supervised approach is conceptually and technically easier.\n* I am a bit skeptical about the results presentable in Table 3 and section \"Targeted RNA design\" - judging improvement of a sequence using a different classifier trained on the same data is not very convincing. Ideally, empirical wet lab validation should be carried out for the improved designs; but absent that it would be great to see a more independent/less correlated evaluation. Perhaps the authors could demonstrate a reduction of edit distance of an improved negative test set sequence to a positive (test set / train set) sequence?\n* When generating the dataset for the unsupervised RNA modelling task the authors take human transcripts and draw random short (32 to 512 nucleotide) sequences from them for their dataset. Since the authors introduce a new dataset it would great to see more information about its composition - which kind of RNAs compose the dataset (e.g. mRNA, tRNA, lincRNA, shRNA, rRNA, etc); does this include alternatively spliced RNA (i.e. could there be leakage between the training and holdout sets)? What is the reasoning between random slicing of the RNAs into shorter sequences? Are the smaller RNA slices biologically plausible/relevant?\n* Could the authors provide additional justification / ablations of the choice of $\\beta \\ll 1$? How do the models behave if KL annealing is not used?\n\n\n**Minor comments**\n* The abstract states \"the design of large scale and complex biological structures *requires* dedicated graph-based deep generative modelling techniques\", which I believe is a too strong statement. These techniques work better right now, but does not mean that they are required (and the only appropriate way forward).\n* In \"Task Description\" section: \"functional properties *or* RNA\" -> \"functional properties of RNA\"\n* Is Figure 1 using some domain-specific terminology? It refers to cliques, but the nucleotides put into boxes are not all fully connected.\n* Why doesn't constrained generation (Table 2) achieve 100% validity? It seems to me that it should be possible at least for the linearized sequence-structure decoder.\n* Apologies if I missed it in the text, but what does \"RECON ACC \" in Table 2 refer to?\n* Incorrect opening quotes used for \"ground truth\" in section \"Targeted RNA design\"\n* In Appendix A there is \"$j > j$\"\n* In Appendix B: \"in-vitro\" -> \"*in vitro*\" and probably in italic\n* Why is FE DEV defined as the absolute value? Should not the MFE structure always have smaller Free Energy than any structure output by the model?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "New direction",
            "review": "##########################################################################\n\nSummary:\n\n \nThis paper sheds light into an impactful problem of neural representation and generation for RNA secondary structures. Authors presented benchmark tasks in unsupervised, semi-supervised and targeted generation setting and presented deep generative models to solve this tasks. They presented three different generative models using variational auto encoders based on sequence, graph and hierarchical representation. \n\n\n##########################################################################\n\nPros:\n \n1. The paper deals with a very important problem of neural representation and generation for RNA secondary structures. I think this area needs a lot more work to help solving real-world biological problems.\n\n \n2. As far as I can tell their method is novel in terms of idea. Generation of RNAs are not trivial. It requires extra attention to detail in terms of checking for validity and stability in folding. There are also diverse families of RNA which are very different from each other. In this work, the validity and stability constraints are implemented inside a depth first search traversal of building the RNA. Without these constraints its much harder to learn the structures of the RNA from embedding only as it is evident from the result on the unsupervised setting as well. In supervised setting, the authors showed their models efficacy in RBP datasets. Related works also contains recent works in RNA.\n \n3. Overall the paper is well written. I liked the illustrations for explaining the methods. Result section is also well structured. It clearly shows the effectiveness of the generative model used.\n \n##########################################################################\n\nCons: \n\n \n1. Is it possible to use this generation technique to use in solving problems like RNA folding or targeted RNA design? Specially the authors mentioned RL based method from (Runge et al., 2019). Is it possible to use this generative model and combine it with the existing RL techniques for RNA design or ML techniques for folding to show improvement in their result? That would definitely increase the impact of this work.\n\n2. Is it also feasible to learn the structural constraints from the data? Can we get rid of the dependency on constraints with more automated learning?\n\n3. I am also interested to know if we can compare it against any discriminative learning based baseline for any task. For example a recent work from (Yan et. al 2020) for RNA protein interaction.\n\n##########################################################################\n\nQuestions during rebuttal period: \n\n \nPlease address and clarify the cons above \n\n \n#########################################################################\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting biologically-inspired generative models for RNA",
            "review": "Brief summary: The authors describe a generative model constrained to model the primary and secondary structure of RNA.\n\nPros:\n\n- I applaud the authors for working with RNA data. This is less characterization of modeling RNA data.\n\n- I think the constraints for the RNA generation structure are reasonable, and both the manual and ML constrained generative process are well thought out.\n\n- I think the comparison between constrained and unconstrained generation of molecules was well done.\n\nCons:\n\n- This paper falls into the common pitfall of not controlling for homologous sequences between the training and test set. For each sequence in the training set, what is the sequence with the largest sequence ID in the test set by alignment?\n\n- It would be great to have a null distribution to characterize your model to. What is FE DEV, Normed, and Diversity for randomized, but kmer controlled, RNA sequences?\n\n- The definition of \"valid structures\" is poorly defined. Could generated sequences still fold if optimized in a traditional biophysical simulation?\n\n- I think a comparison with an algorithm like INFERNAL (http://eddylab.org/infernal/Userguide.pdf) is necessary. This is the most simple context-free grammar generative model.\n\nNeutral:\n\n- It is difficult to assess the range of \"good\" values in Table 1. To aid in interpretability to the reader, I would advise more explicitly denoting whether diversity, FE DEV, and Normed should be metrics that go up or down with improved performance.\n\n- I think it would be wise to train this sort of model on more data, such as RFAM. This is an already curated database of RNAs by sequence family from a large number organisms, much more than just human sequences.\n\n- Is there any way to evaluate valid tertiary contacts? These would be sequence motifs that are preserved across organisms with constrained sequence and function that may not necessarily follow Watson-Crick basepairing rules.\n\n- It is slightly concerning to me that in Table 1, \"Validity\" is the same for LSTMVAE and GraphVAE in *Constrained & Posterior Decoding* and \"Diversity\" is the same for GraphVAE and HierVAE *Constrained & Prior Decoding*. Why is this the case?\n\n- Page 2, typo: “benchmhark datasets”\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Evaluating generative models for RNA secondary structure is a challenging problem",
            "review": "Summary: This paper proposes 3 deep generative models based on VAEs (with different encoding schemes for RNA secondary structure) for the generation of RNA secondary structures. They test each model on 3 benchmark tasks: unsupervised generation, semi-supervised learning and targeted generation.  This paper has many interesting contributions — a comparison of VAE models that use different RNA secondary structure encoding schemes, including traditional dot-bracket notation and a more complex hierarchical encoding, and they also introduce various decoding schemes to encourage valid secondary structures. \n\nThis is an important problem for drug discovery and basic biology, but the evaluation is very tricky because there are not that many solved RNA structures (unlike proteins). The tasks proposed in this paper are not comprehensive enough to entice a (comp) biologist to be convinced one way or another, but it does provide an introduction to the problem for the ML field. \n\nComments:\n* While the trends in their results are clear, it’s difficult to know which values of Diversity, FE DEV, and Validity are good enough to say that they have made significant progress relative to the field. The main issue is that all model comparisons are only with their own VAEs. It would be nice to show how their VAEs stack up against other established methods. For unsupervised generation, perhaps Infernal (Nawrocki and Eddy. Bioinformatics, 2017), which uses a stochastic context-free grammar built up from sequences belonging to a single RNA family, but could still form the basis for a comparison. Other models include CONTRAfold (Do et al. Bioinformatics, 2006) and many other thermodynamic models (Lorenz et al. Algorithms for Mol Biol, 2011). These other approaches may lack the scalability of a VAE, but an effort should be made for at least a limited comparison.\n* It’s difficult to assess the quality of the generated secondary structures shown in various plots throughout the paper and appendices. This paper should plot a side by side comparison with the ground state structure that they are comparing the FE DEV scores against to see how well the structure corresponds. For instance, the generated (but invalid) structure may visually look similar as they walk through latent space, but the key mismatches can fundamentally change the “ground truth\" structure of the RNA (which is not shown in their generated structure). The ground truth structure is given by RNAfold, which itself is not perfect.  \n* Evaluation is very tricky and it’s unclear how to navigate this.  Careful wording can help differentiate true objectives (functional RNA) from difficult evaluations (FE DEV scores). If minimizing FE DEV is the objective, then I worry that models will simply become function approximators for RNAfold. \n* A good comparison (though limited) would be to compare the predicted structures with solved structures deposited in the PDB. Another comparison could be to sample about the posterior near well known non-coding RNAs, such as tRNA or 5S ribosomal RNA. Many more consensus RNA structures can be found in RFAM (Kalvari et al. NAR, 2018).  \n* On a side note, the comparison on RNAcompete-S in Table S3 shows that a sequence model is comparable to models that incorporate RNA secondary structures — there is only a small gain by including secondary structures. This is not that surprising as sequence-based models have demonstrated learning secondary structures when trained only on sequences (for the RNAcompete dataset — not RNAcompete-S yet).\n* One lingering question I had is, is the latent space of the VAE meaningful? For instance, does it capture well-established non-coding RNA families? \n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}