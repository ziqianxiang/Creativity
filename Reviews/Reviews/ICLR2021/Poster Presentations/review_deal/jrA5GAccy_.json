{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper considers learning settings with distributional change. It makes a lot of assumptions to obtain sample complexities that justify the use of empirical invariant risk minimization, and falls a bit short by not giving a formal converse for the inadequacy of plan empirical risk minimization, despite making the claim. Nevertheless, the contributions are insightful, and the paper may be worth sharing with the community.\nThe grading were overall positive from the reviewers, though particularly critical, and I doubt the whole paper could be fully double-checked: one could question the ability of the reviewers to perform a deep analysis on a 48-pages theoretical paper in the time constraints imposed by a conference model... "
    },
    "Reviews": [
        {
            "title": "Sample complexities for empirical invariant risk minimization that may show an edge over ERM in certain regimes of distributional change. Overall good paper.",
            "review": "### Summary\n\nThis paper considers a learning scenario where training data $(X,Y)$ comes from a mixture, where membership in each mixture component (“environment”) is clearly labeled. Generalization is required not just under the same mixture, but potentially under changing mixing distributions. This is captured via several alternative formulations. The key underlying assumption that makes the extrapolation possible is the existence of a a representation $\\Phi(X)$ that leads to an environment-invariant statistics (by focusing on quadratic loss and linear predictors, the paper asks for invariant $E[Y|\\Phi(X)]$ and constant $var[Y|\\Phi(X)]$). A range of other assumptions are made to enable analysis, such as sufficiency of the representation in at least one environment and boundedness of the loss and its gradient.\n\nThe main object of study is empirical invariant risk minimization (EIRM), which minimizes the empirical risk under the further constraint that the predictor is (approximately) optimal in each environment. The main result is that ERM and EIRM have the same sample complexity when the environments are related via a covariate shift, but that under other situations (namely confounding variables and anti-causal relationships), EIRM continues to enjoy guarantees, while informal arguments are offered as to why ERM doesn’t. Experiments are given to illustrate this disparity, and thus potentially justify the use of EIRM.\n\n### Strengths\n+ The paper provides a very methodical and detailed reasoning to try and identify the benefits of invariant risk minimization and provides several compelling arguments.\n+ The main strength of the paper is in placing the finger on the situation where ERM and IRM appear to differ. It doesn’t complete the story here, but at least identifies the reasons behind prior accounts that seemed to conflict on the surface.\n+ The math is laid out very clearly and the gradual introduction of detail from the main text to the supplements keeps the presentation of the ideas manageable.\n+ The new colored MNIST datasets proposed in this paper is a very useful contribution, which more realistically captures the kinds of shifts and dependences that may arise in realistic ML applications. \n\n### Weaknesses\n- In the quest for detail, some of the high-level perspective is lost. The first place where this happens is when the paper claims that Assumption 1 is equivalent to covariate shift when $\\Phi$ is the identity map. But the constant-variance requirement of A1 does not follow from covariate shift, so in fact A1 is a stronger assumption in that case.  And this is not mild: it is the glue that binds together the proof of Proposition 1. (Incidentally, the argument for the latter can be given more simply as: in the sufficient environment, the optimal estimator enjoys the constant variance as MSE; if the objective is higher then, in all other environments, we can reduce it by reverting to the conditional expectation; thus the latter must be the optimal predictor.)\n- The second place where the high-level perspective is lost is when tying together the ideal OOD predictor (1) and the IRM formulation (3). There are two facts that distinguish (1) and (3). First, (1) does not require the predictor to be optimal in each environment unlike (3). In fact, in the manner of most minimax results, the solution might sacrifice even in a “good” environment by being worse than optimal, so that some of the “bad” environments fare better. Second, (3)’s average objective may result in a choice of $\\Phi$ far from the ideal representation. To see this, imagine two environments that need a more elaborate representation and a third that doesn’t but is very noisy. (1) would favor the latter and give a simple representation that would perform well in the worst case, while (3) may ignore the third environment and focus on the first two and thus optimize the average case.\n- This blurred high-level perspective makes it so that the rest of the paper analyzes the procedure in (3) (or rather its gradient-constrained formulation) while striving to recover the solution of (1) (as given by Proposition 2). It’s a conceptual disconnected which unfortunately only becomes evident upon reading through, and is not adequately highlighted by the paper. It’s also possible that the bulk of additional assumptions introduced, as well as the loss and model structures adopted, are really a result of enforcing the reconciliation between these two different paths.\n- The entire methodology of EIRM hinges on a perfect awareness of the environment (the constraint cannot be calculated without it). In that sense, it is quite an unfair comparison to plain ERM. In fact, the awareness can be thought of as adding a particular inductive bias to ERM, so it is not much of a surprise that the analysis presented is not more novel than regularized ERM. \n- ~~What the paper claims a lot but does *not* deliver is a lower bound for ERM under the cases where new upper bounds for EIRM are given. In Table 1 there is a “No” under OOD, on page 7 we read “[t]his shows that EIRM works in more settings than ERM”, and in the conclusion we read “we proved that […] ERM can be asymptotically biased”. But apart from informal arguments and experiments, this is not in fact shown.~~ In particular, considering the great number of assumptions leading to Proposition 5, it is not at all clear how ERM behaves under the very same assumptions (note that the experiments do not satisfy all these assumptions, e.g. $\\pi^\\min=0$, so we cannot treat them as counterexamples).\n\n### Comments\n* The $1/\\kappa^2$ dependence on $\\kappa$ is a bit weird. $\\kappa$ is the smallest $|R’(\\Phi)-\\epsilon|$ over all $\\Phi$. So in principle there maybe a $\\Phi$ that makes $R’(\\Phi)=\\epsilon$ and Proposition 2 vacuous. I think the way it is used is the same as the reason behind why $\\kappa<\\epsilon$: it’s because we assume a feasible solution of the optimization exists, so $0 < R’(\\hat \\Phi) < \\epsilon$, thus $|R’(\\hat\\Phi)-\\epsilon|<\\epsilon$. But $\\kappa$ is *not* defined as $|R’(\\hat\\Phi)-\\epsilon|$, perhaps it should be?\n* After Assumption 5, the paper should make it clear that the initial focus is on $S$ being linear.\n* Some things should be moved up from the supplements for completeness. For example, the definition of your constraint, the $\\theta$ for Figure 1d. \n* Typos: Assumption 7, *Inductive* bias. $[\\psi^1,\\psi^e,\\psi^3]$: change $e$ to $2$. Figure 1, CMNIST for all.\n\n### Overall\nThe paper considers interest settings of distributional change and corresponding learning formulations. It makes a lot of assumptions to obtain sample complexities that justify the use of empirical invariant risk minimization, and falls a bit short by not giving a formal converse for the inadequacy of plan empirical risk minimization, despite making the claim. Nevertheless, the contributions are insightful, and the paper may be worth sharing with the community.\n\n_[Edit: I'm glad the author clarified some of the tensions between the different definitions. The ERM bias proof seems also okay, but it's still not clear how ERM performs with all the additional assumptions of the IRM result. I'm still not on the same page regarding $\\kappa$, but I'm willing to take a leap of faith on the bits that I'm uneasy about. I would suggest this paper is shared with the community, since I believe it has enough good insights.]_\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting theoretical insights about the good performance of IRM in the presence of distributional shifts",
            "review": "SUMMARY\n#######\n\nThe present paper proposes a sample complexity study to compare Empirical Risk Minimization (ERM) and Invariant Risk Minimization (IRM).\n\nIRM is a framework developed by Arjovsky et al. 2019 to avoid learning a predictor based on spurious correlations (e.g. color background of a photo).\n\nData are assumed to be sampled from different environments, and it is assumed that there exists a transformation $\\Phi^*$ of the input data $X$ such that the expected value (and the variance) of the output $Y$ given $\\Phi^*(X)$ is the same among all environments. If $\\Phi^*$ is the identity, this assumption encompasses covariate shift.\n\nUnder the additional assumption that $\\Phi^*(X)$ is sufficient on at least one environment, authors show that for the square loss $x \\mapsto \\mathbb{E}[Y \\mid \\Phi^*(X) = \\Phi^*(x)]$ is a solution to the Out-of-Distribution (OOD) generalization problem (prop. 1), that aims at minimizing the maximum environment risk.\n\nIRM is obtained by restricting the search space to invariant predictors, i.e. predictors that write as $w \\circ \\Phi$ such that $w$ minimizes over all $u$ the environment risk of $u \\circ \\Phi$ for all environments. This condition is further replaced with a gradient constraint. Empirical IRM (EIRM) is achieved by replacing expectations with empirical averages.\n\nAuthors first show the concentration of the solution to EIRM around that of IRM (prop. 2).\n\nThen, under additional assumptions, authors study how EIRM approximates the OOD solution by exhibiting sample complexities under both covariate shift (prop. 4) and more involved distributional shifts (prop. 5). Results suggest the superiority of EIRM over ERM in the later setting.\n\nExperiments on variants of the colored MNIST dataset are presented, matching the theoretical findings. Code is provided.\n\n\n\nCOMMENTS\n########\n\nThe paper is globally well written and understandable (despite notation getting heavier and heavier with the assumptions). IRM is clearly introduced and the motivations well exposed.\n\nThe subject tackled is of great interest, and the theoretical angle adopted is novel and insightful. Experiments are eloquent.\n\nAbout Asm. 1, how hard is it to check in practice? Does someone have to explicitly exhibit $\\Phi^*$? Maybe an example where $\\Phi^* \\ne \\mathsf{I}$ could be useful at this point.\n\nAbout eq. (1) and (3), the maximum over all environments has been replaced by the sum. This can be seen as replacing a $\\| \\cdot \\|_\\infty$ with a $\\| \\cdot \\|_1$, which has a very different behavior (tends to cancel components, while the infinite norm makes all components small). How can this change be justified and impact proofs? The same question could apply to $R'(\\Phi)$ that is the sum instead of the max (but the $\\epsilon$-relaxation makes this change more natural).\n\nAbout eq. (3), how to be sure that invariant predictors exist?\n\nIs the restriction to real-valued $\\Phi$ necessary? I feel it makes the compositional architecture $m \\circ \\Phi$ a bit obsolete. It rather appears now as a trick to use the differential constraint (the $\\nabla$ could be replaced by standard derivative since $w$ is one-dimensional). Also, is there a way to consider sub-differentiable, or non-differentiable loss functions here?\n\nAbout Prop. 2, can authors elaborate a bit on the $\\kappa$ constant, that seems to relate to $\\epsilon$ in a complex way, and is likely to be very small?\n\nAbout Prop. 4, as far as I understood, one of the main changes is that $\\epsilon$ now replaces $\\kappa$ (that I think is better to get an idea of dependences). Otherwise, the bound seems almost identical to that of Prop. 2, suggesting that the difference between EIRM and OOD solutions is \"the same\" as that between EIRM and IRM solutions?\n\n\n\nMINOR COMMENTS\n##############\n\np.3 Asm. 1, $\\exists$ in the beginning of an english (i.e. non mathematic) sentence is a bit weird\nP.3 we define *the/a* invariant map\np.4 Prop. 1, If $\\ell$ is *the* square loss\np.4 for *the* cross-entropy loss\np.4 the parentheses two lines after seem unnecessary\np.4 the space*s* of representation and\np.4 eq. (4) *s.t.* omitted\np.5 the statement of Prop. 3 is a bit misleading, putting the reference in the title would be clearer (although authors clearly cite the reference)\np.6 Asm. 7, induct*i*ve bias and & symbol quite unusual\n\n\n\nOVERALL EVALUATION\n##################\n\nThis paper proposes a very interesting theoretical study to elucidate which one of ERM or IRM is better and when. Authors also present conclusive experiments matching the theoretical findings. For all these reasons I think this contribution deserves acceptance.\n\n\n--- EDIT POST REBUTTAL ---\n\nI thank the authors for their detailed answer(s) and the efforts they put in editing the revision (in particular I agree with other reviewers that Prop. 17 adds clarity). Overall my stance on the paper has not changed and I still recommend acceptance.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "interesting yet very technical insights on ERM/IRM",
            "review": "Summary: The paper investigates the choice of learning paradigms to reach out-of-distribution generalization, namely IRM vs ERM under different scenarios of domain generalization. Technically, generalization bounds and rates are calculated to be able to compare theoretically how each paradigm fares in the different scenarios. Proposed analysis shows that IRM generalizes better than ERM in a number of important cases: presence of confounders and/or anti--causal variables. Numerical experiments on variants of the Colored MNIST benchmark (for each scenario) verify the theoretical findings.\n\nGood points:\n- focus on an important problem, and answering elements for recent debate in the scientific community e.g. https://arxiv.org/abs/2010.05761 [scientific importance]\n- general insights clear and easy to grasp (Table 1) [clarity of results]\n- remarkable effort to explain technical results (e.g. paragraph \"implications of Proposition 4\") [readability effort]\n\nQuestions:\n- it seems section 3.3.2 and generally results on the confounder and anti-causal scenarios are restricted to polynomial mechanisms ($g$) ?\n- in experiments with the confounder case CF-MNIST (Fig. 1b) it seesm IRM only achieves chance-level performance (=~ .45 test error) ? \n\nPoints limiting the relevance of the paper:\n- overly technical paper: it is hard to follow for readers interested in general (or deep) ML but not literate in generalization theory\n- density of the paper: most of the space is dedicated to technical results which albeit interesting are seemingly \"shoehorned\" into 8 pages. I wonder if the conference format makes justice to such a deeply technical work... 47 pages total with appendix\n- difficult to check the proofs: all are in appendix (no sketch) but there are so much of them. As a reviewer I find it discouraging as the time needed to check correctness would be much more than what I can afford in a short cycle for a conference such as ICLR\n- result a little over-sold in abstract/conclusion: it seems that the theoretical results are somewhat limited (see question on sec. 3.3.2) and that is omitted in both abstract and conclusion, maybe giving a false sense of generality to the results. Maybe the studied case - which is already impressive - is more general than I think but I'd like a comment of the authors about it.\n\nPoints that would improve the paper (and my score):\n- clarify the meaning of limitation of result in section 3.3.2 + add it in abstract/conclusion\n- comment the CF-MNIST (Fig. 1b) experiment\n- add/rewrite if possible \"interpretation for the layman\" paragraphs (e.g. last paragraph of sec. 3)\n\nFinally, I'd like the authors to think about the best venue to make justice to their work. It seems to me that a general ML journal would be a good option that would 1) allow more space to make the text more self-contained 2) allow more time for proof checking and discussion/optimization 3) allow more interpretative comments to be included in the work and make it accessible to a wider audience. In the mean time a preprint could also circulate to share the current research results.\n\nalso: small typo on title of Assumption 7",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "Summary:\n\nThe authors provide sample complexity study of the Invariant Risk Minimization (IRM) in the case of scalar predictions and one dimensional representations. Instead of considering the penalized version of IRMv1 proposed by **Arjovsky et al, 2019**, the authors study its constrained counterpart introducing a relaxation into the requirement of zero gradient. The main conclusion that I understood from the work is: the performance of IRM (provided the assumptions are satisfied) will never be much worse than that of ERM and will be superior to that of ERM in some particular cases. The authors support their theoretical discoveries empirically.\n\nReview:\n\nThe paper is rather dense and the exposition could be improved. I had some troubles in Section 3.3.2 since the spaces of the variables and matrices are not introduced (but it *might* be standard in causality literature, which I am not familiar with).\nIt is difficult to judge the significance of the work. On the one hand, the proofs of most of the results follow fairly standard tools of empirical process theory and I am not sure how much more insights this work provides compared to that of Arjovsky et al, 2019. On the other hand,  the statements themselves seem to be novel and they fit well in the big picture.\n\n\nPros: The theoretical results on IRM are novel and the authors introduce new variants of colored MNIST.\nCons:  The proofs follow the same pattern as that for ERM. The fact that IRM is constrained does not bring much novelty in the proof. See for instance: **Woodworth et al, Learning Non-Discriminatory Predictors, 2017** and **Agarwal et al, A Reductions Approach to Fair Classification, 2018** for similar proof techniques. It is also unfortunate that the main body refers too often to supplementary material for some details that in my opinion are very relevant to the main body. For instance, empirical version of the gradient, additional discussion on why ERM is biased in Section 3.3.2, and the setup of the experiments are all postponed to appendix.\n\nQuestions:\n1. On feasibility: I wonder whether there is an issue of feasibility for the problem in Eq. (5) and Eq. (6)? Is it obvious that if the set $\\mathcal{H}_{\\Phi}$ is finite then we can always find a feasible representation?\n2. Slack variables: what is the reason to introduce slack variable $\\epsilon$ in the formulation of Eq. (5)? It seems to me that it is sufficient to introduce it only for Eq. (6), since the authors only need to show that $\\Phi^*$ is feasible (with high probability) for Eq. (6) in order to prove their upper bound. I also do not see why $\\kappa$, introduced on p.5, cannot be zero.\n\nSuggestion: \nThe phrase **''``For ease of exposition, we use the standard setting of finite hypothesis class and extend all the results to infinite hypothesis classes in the supplement''** is not convincing. Finite class is not that realistic in practice (correct me if I am wrong). If all the results of this work can be stated for infinite classes of bounded capacity (e.g. VC-class or totally bounded spaces), I suggest to completely erase the case of finite classes. \n\n\nAfter rebuttal: I thank the authors for comprehensive rebuttal, which addressed most of my concerns. I update the score accordingly. \n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        }
    ]
}