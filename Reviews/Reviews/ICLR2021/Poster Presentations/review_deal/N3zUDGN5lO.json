{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper shows that using graph neural networks to address multi-task control problems with incompatible environments does not provide benefits to the learning process. The authors instead propose to use Transformers as a simpler mechanism to be able to train and discover the helpful morphological distinctions between agents in order to better solve multitask reinforcement learning problems.\n\nThe paper is well written and the analysis of the literature has been appreciated.  The contribution is original and relevant to the community.\n\nAll the reviewers agree that this paper deserves acceptance. We invite the authors to modify the paper by following the suggestions provided by the reviewers. In particular:\n- improve the analysis of the empirical results\n- update the plots\n- add the suggested references"
    },
    "Reviews": [
        {
            "title": "Broken body in healthy mind - an interesting apporach with potential",
            "review": "The manuscript studies the usefulness of Graph Neural Networks (GNN) in incomplete environments for Multitask Reinforcement Learning (MTRL). First, authors explore to what extend morphology information improves performance in GNN. By use of the Shared Modular Policies and the NerveNet methods, authors find that restricting morphology information does not improve performances. Based on this finding, authors apply GNN to fully connected graphs with memory/attention, i.e. the use transformers. Simulation results in different environments show that the proposed approach outperforms conventional methods.\n\nThe paper is well written, and methods and analysis approach used are clear. The selected approach and the authors findings are meaningful in a neuroscientific context, as the transformers approach better resembles the function of a human brain. The contribution is original and relevant for the community.\n\nThe introduction does not clearly explain why the assumption that restricting the model and encoding morphological information may be beneficial. A weakness of the manuscript is the assessment of the results. No statistical tests were calculated to support the authors claims, nor is it clear how the performance of the different methods was compared. For example, the comparison f the curves depicted in Figure 3. It is also not clear to the reviewer what specifically the highlighted areas in the figures represents? Confidence intervals or standard deviation? Moreover, a few statements are vague. For example, what is a “good MTRL policy” (page 6).\n\nOverall an interesting approach that is expected to improve performance in MTRL and needs further exploring.\n",
            "rating": "7: Good paper, accept",
            "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"
        },
        {
            "title": "Interesting paper, needs a bit more analysis",
            "review": "This paper proposes that recent methods that used graphical neural networks to help solve the multitask reinforcement learning problem and assume that there's an advantage from being able to encode the agent's morphology using a graphical neural network do not provide additional generalization and benefits for learning. Instead, they claim that the benefits from being able to encode this morphology are counteracted by the difficulty in having to train the graphical neural network using the message passing system. This paper instead proposes to use Transformers as a simpler mechanism to be able to train and discover the helpful morphological distinctions between agents in order to better solve multitask reinforcement learning problems.\n\nThe motivation that graphical neural networks are bogged down by their message passing framework is not necessarily a motivation for using transformers. There needs to be a separate motivation for why you want to use transformers and why they should perform better than GNNs or normal networks.\n\nThe author claims that the SMP paper does not work better due to the morphology encoding and then they point out that it instead works because of the encoding of the subtrees and some specific detail related to message passing. This could be correct the explanation but the paper so far hasn't gone into enough detail for the reader to understand the importance of this message passing and how it works and how it is not improving training for GNNs.\n\nFigure one does provide some information related to how the transformer is used with respect to some morphology it would be far more helpful if this figure method was described well enough so that anyone can understand how to apply this to a different morphology. One of the challenges with reading and understanding this paper is the lack of information on how graphical neural networks are used and designed to understand later comments in the paper.\n\nThere needs to be an ablation with respect to the residual connections added to the Transformer based network to make sure the improvement for amorphous is not working well just because of these residual connections.\n\nWhile I do agree that training a graphical neural network to be able to produce a quality policy for a number of control tasks from the opening item environment is difficult the author of the paper might be missing at least one of the key points from the previous work in that you can learn a stronger modularization of policy. And that a goal of the SMP work was to understand how more modular policies or policies with modular components could be learned.\n\nIt is stated in the paper that amorphous does better for state of the art incompatible continuous control? What is meant by incompatible continuous control? This term has not been defined anywhere in the paper and without this definition, it's difficult to understand the contribution this paper is making.\n\n----- Post Discussion ----\nI have updated my rating for the paper after the authors have provided additional discussion and experiments to address my concerns.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Strong performance against baselines; architecture/experiment details less clear",
            "review": "The paper focuses on the problem of multi-task control with a shared policy in the continuous action setting. Unlike current assumption of compatible state-action spaces, the proposed architecture is transferable across different morphologies. The paper includes ablation experiments that clearly show that current works that use the body morphology structure to constrain the graph structure of graph neural network based approaches do not actually improve the performance. The paper instead forgoes trying to input the body structure and uses a transformer based architecture that is capable of learning the appropriate (even dynamic) graph structure actually useful for control.\n\nI enjoyed the relatively simple experiments showing how the specific graph struture based on body morphology wasn't important at all. Although would be useful to know how many runs were performed given the noisy nature of RL.\nSimilarly the cyclical structure noticed in Fig. 6 definitely points towards the powerful nature of transformer architectures at learning this relations. The strong performances (none of which seem to have converged yet) compared to baselines speak for themselves. Although, again not clear about the number of seeds the experiments were repeated.\n\nThe architecture description is somewhat unclear. Both actor and critic seem to have three parts to their architecture. But for a critic you need to output value information which might be scalar unlike decoder MLPs for independent node action. If there is some sort of aggregation going on, it needs to be clarified as to specifically how.\nAlthough Fig. 5 shows changing attention patterns, it doesn't warrant the confirmation that the proposed architecture benefits from \"state-dependent message passing of transformers\" which itself consists of two things. One can do state-dependent message passing in such architectures without the transformers (see DICG [1] for an example with attention and graph convolutions). Second, there could be a static graph structure which is better than the dynamic masks: the paper didn't actually perform the experiments to rule that out. Maybe the obvious morphology is the wrong graph structure but there is something else which would work better.\nAgain, the paper's claim is probably true, but the causal language is not justified from Fig 5.\n\n[1] https://arxiv.org/abs/2006.11438\n\nEdit: Updated score to reflect the changes from the revision.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Clear contributions to expanding the capabilities of RL agents through GNNs, and transformers in particular.",
            "review": "This work considers continuous control environments in which each agent limb (actuator) is associated with one action and a set of observation factors. As in prior work, the proposed policy class is modular, where each module is mapped to one limb (or the root), and the modules share information through some GNN message-passing schedule. The experimental results indicate that fully connected, transformer-style message passing is more effective in this setting than message passing restricted to directly connected pairs of limbs.\n\nPros\n- Good framing of the problem and choice of experiments.\n- Insightful discussion of related work.\n\nCons\n- There is no discussion of the additional computational requirements of transformers over SMP.\n- The results would be stronger if hyperparameters had been systematically tuned.\n\nQuestions\n- Why were no results provided for the Walker-Hopper or Walker-Hopper-Humanoid combinations tested by Huang et al?\n\nSuggestions\n- The paper mentions in passing that this work involves agents “with each non-torso node having an action output”. This limitation probably deserves to be highlighted more prominently. \n\n- As the paper explains, “transformers can be seen as GNNs operating on fully connected graphs”. In other places, the paper contrasts transformers with GNN-based methods (“substantially outperforms GNN-based methods”), as if transformers were not GNNs. To avoid confusing readers, it would help to explain that GNNs are a broad class that includes both transformers and SMP, which differ in their message passing schedules, etc. \n\n- It would help to have a more detailed description of the limb torques and observation factors, so that readers don’t have to look in Huang et al (section 4) for these details.\n\n- The following phrases are important but unclear for readers who are not very familiar with GNNs or transformers:  “Having an implicit structure that is state dependent is one of the benefits of AMORPHEUS.”  and  “the implicit state-dependent message-passing schema learnt by AMORPHEUS can be better”\n\n- The paper says “We use entity to denote both vertices and edges.” But the term “entity” appears nowhere else in the paper.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}