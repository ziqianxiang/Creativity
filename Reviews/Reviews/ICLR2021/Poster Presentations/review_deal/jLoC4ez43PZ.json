{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes a simple extension to BERT-like pre-training for source code models, which allows incorporation of data flow information. This is a new way of incorporating code structural information into models, and it appears practical and effective. Reviewers are all in favor of accepting the paper."
    },
    "Reviews": [
        {
            "title": "Solid work on introducing structure aware tasks and data flow representation to pre-training on code",
            "review": "This work address the pretraining over code and text. It proposes to leverage data flow as additional inputs, and add two structure aware pre-training tasks besides the masked token prediction task. The pretrained model is evaluated on four different tasks and outperforms the CodeBERT baselines as well as other pretrained models. Further analysis confirmed the benefits from the additional tasks and data flow input. \n\nStrength:\n\n1. The use of data flow as additional input and the proposal of structure aware tasks are well motivated adaptations of pre-training to code. \n\n2. The extensive experiments and comparisons supported the claim and additional ablation studies and analysis confirmed the benefits from the additional structure-aware tasks and the data flow representation. \n\n3. The comparison with syntactic structures (AST) is interesting and demonstrated the higher potential of semantic structures like data flow. This could be beneficial to other code related tasks. \n\n4. The paper is well written and easy to follow. \n\nWeakness:\n\n1. Since the proposed approach is adding extra inputs, and given the O(n^2) complexity of Transformer, it would help to add some information regarding the increase in compute cost, for example, additional flops or latency due to additional data flow inputs. \n\nRelated to the point above, although linearizing the data flow into sequence seems the simplest approach, it might be computationally inefficient given that the data flow graph is probably sparse, thus large number of attention position needs to masked. But maybe the data flow input is much smaller than the code input so that it is not adding too much overhead.  \n\nIt would also help to add more statistics about the inputs, for example, the average length of the code, the length of the linearized data flow, the length of the linearized AST, etc. \n\n2. Not so much a weakness, but it would help to show some examples, in appendix if the space is limited, where GraphCodeBERT improves over CodeBERT or other models without semantic structures, to give some more intuition. ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review of Graph Code BERT",
            "review": "\nOverview:\nThe authors present Graph Code BERT, the first language model that leverages data flow to learn code representation. They use three objective functions: Masked Language Modeling, Edge Prediction, and Node Alignment. They claim their structure-aware pre-training can help improving performance on code-related downstream tasks, including code search, clone detection, code translation, and code refinement.\n\n\nReasons to accept:\n* The paper is well-written and easy-to-follow.\n\n* It is the first pre-trained model that leverages the data flow structure of code to learn code representation.\n\n* They propose two pre-training tasks for learning representation from source code and data flow. Edge Prediction is more interesting in my opinion.\n\n* They show improvement on four downstream tasks, even though some of them are marginal in my opinion compared to CodeBERT.\n\nReasons to reject:\n* This work is not the first work to conduct language model pretraining for code understanding applications. I will expect more comprehensive comparisons with related work. For example, Svyatkovskiy et al. (2020) propose GPT-C, which is actually a transformer architecture that can also use for both generation and classification tasks. Missing C-BERT (Buratti et al.) as well.\n\n* The 2.4M functions used for \"pre-training\" is not very persuasive. I think it is more reasonable to crawl existing code repositories for Pre-training. I do not see a particular reason because pre-training without natural language descriptions still makes sense.\n\nQuestions & Suggestions:\n* Do you find natural language descriptions useful in your experiments? Why if we discard it from pre-training?\n\n* What is the ratio of edge connection? 10%? 20%? The reason I am asking this is that when you do the randomly sampling 20% of nodes Vs in the data flow, how many node pairs you used to calculate the loss? Is the positive-negative ratio balanced? Please give more details about the training objectives.\n\n* Please give more prediction examples or qualitative analysis for each downstream tasks in the Appendix.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Simple extension over CodeBERT with small performance improvement for a series of tasks",
            "review": "This paper extends CodeBERT to include elements of dataflow in\naddition to the comments and sequence of code tokens. Also, the\npretraining includes two tasks that are structure-aware: predicting\ndataflow edges and aligning dataflow nodes to code. The paper is well\nwritten, well motivated and the empirical evaluation is quite\nthorough. The performance compared to CodeBERT is around 1-2%\nadditional accuracy for a series of diverse tasks.\n\nDetailed comments:\n- In general the paper is well written. There were a few parts that\n  required further clarification for me. For example, it is not clear\n  how the variable names are represented textually (e.g. x1 - is it\n  really x1 or a different way of expressing this in the sequence of\n  tokens that is sent to the model). The architecture seems to me to\n  be pretty identical to BERT with the exception of the attention\n  matrix for the dataflow portion. Is this matrix kept constant\n  throughout training? Not clear to me if the dataflow graph is a\n  traditional dataflow (as in the one used in compiler analysis). It\n  appears to me that it's a slightly simplified version based on AST\n  and some idea of data flow. I would contrast and compare with\n  \"traditional\" dataflow graphs (that use SSA - single static\n  assignment form - for example). Not exactly clear what you mean by\n  \"neat\". I would find a different word. Neat sounds like \"pretty\" in\n  this context; I don't find it scientific. Do you mean less complex?\n  The Noe alignment task was not clear to me at all. Perhaps an\n  example would be helpful showing the actual task and what is\n  provided as input (I'm guessing the same thing as usual) and how the\n  prediction looks like.\n\n- I'm not sure there is a discussion on the experimental methodology\n  and hyperparameter tuning. Are the results presented some average\n  over several runs? If they are single runs, the difference in\n  accuracy for most task is within noise, I would think. Also, I\n  particularly find the third decimal a bit much for \"stretching\"\n  result improvements. I think the idea is simple, it gives some\n  limited performance improvement over CodeBERT but the model\n  architecture is overall similar so it is probably worth it",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "AnonReviewer4",
            "review": "This paper proposes GraphCodeBERT as a Transformer-based pretrained model for programming language that incorporates data flow information in the graph representation of variables in the code. The data flow graph encodes the structure of variables based on “where-the-value-comes-from” from the AST parse. The pretrained model is jointly trained on the code, the natural language comment of the code, and the data flow graph of the code. In addition to the Masked Language Modeling objective, two new pretraining objectives are proposed including predicting the edge of the data flow graph and predicting the alignment of variables between data flow graph and code. The graph-guided masked attention is used such that the attention can only occur if two variables have an edge in the data flow graph or there is an alignment between data flow graph and code. The experiments show that GraphCodeBERT can deliver improvements on Natural Language Code Search, Code Clone Detection, Code Translation, and Code Refinement.\n\nStrengths\n\n- The approach is well motivated, and I totally agree that we should consider the code structure for a pretrained code language model.\n\n- There are consistent, though small, improvements on all four tasks.\n\n- The case study and ablation study are very helpful to understand why GraphCodeSearch is better than other models with better representation of code semantics on the Code Search task.\n\nWeakness\n\n- This paper uses the data flow graph and claims this is better than AST because AST has an \"unnecessarily deep hierarchy\". However, as the data flow graph is extracted from AST, data flow contains only partial information of AST, especially the information of “where-the-value-comes-from”, and throws out other useful information such as function name, condition control, operator. It is not clear to me if this design choice is a good one. \n\n- The experiments should have a comprehensive comparison using AST and the data flow graph to answer the questions such as: What is the performance of using AST instead of the data flow graph? What information in AST is useful for which tasks? In addition to “where-the-value-comes-from” information, what other information shall we incorporate into a pretrained language model for code? Right now, there is only analysis on the code search task and there is not enough discussion about AST approaches.\n\n- The improvement of GraphCodeBERT over previous SOTA, especially CodeBERT, is marginal. There is no significance test in the results. On Code Clone Detection, the improvement is 0.6% F1. On code translation, the improvement is less than 1% for accuracy. The most improvement is on 5.1 Natural Language Code Search, but the data flow graph and the two newly introduced objectives are not relevant to natural language comment.\n\n- I don't think BLEU is a meaningful metric for code translation and code refinement, and it can be misleading. For example, in Table 4, BLEU is 90+ while the accuracy is less than 10%. The author can consider not reporting BLEU.\n\nQuestions\n-  Can you please give more details about how to use AST Preorder Traversal and AST Subtree Masking for code search? I am wondering if this is a good usage of AST on this task. Thanks!\n\nMinor\n- Notation W is used to denote both parameters in self-attention (Eq (1)) and the code comment. Consider using two different notations.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}