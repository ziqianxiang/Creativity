{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes and investigates an approach for audiovisual synthesis based on the so-called exemplar autoencoders.  The proposed approach is shown to be able to convert an audio input to audiovisual outputs using only very small amount of training data.  All reviewers consider the paper interesting with a lot of potentials in a variety of applications and appreciate the novelty of the work in this domain.  But there are also concerns on the technical presentation and the quality of the samples in the demo.  The authors addressed most of the concerns in the rebuttal but agreed that the quality of the results still had room for further improvements.  Overall, the work presented is interesting. The paper can be accepted. "
    },
    "Reviews": [
        {
            "title": "Well written paper with interesting results. Worth publibication.",
            "review": "In this paper, the authors propose a generic system for performing one-shot audiovisual synthesis from only one small sample. The results are impressive for in-the-wild speech synthesis and their approach could have a broader impact in the community. \n\nStrengths:\n + One shot audiovisual synthesis for a target speaker.\n + The publication of a new dataset for AV synthesis evaluation.\n + Comprehensive analysis\n\nWeaknesses: \n - No theoretical novelty. It seems much of the benefits of the approach comes from the extra data and training procedure. \n\nOther comments:\n\nIn Sec 3.2. Autoencoders as projection operators, the authors here make it sound that they are the first ones that noted that autoencoders can capture the data-generating distribution. ",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting paper covering a lot of ground; exposition could be tightened/clarified?",
            "review": "Interesting paper covering a lot of ground; exposition could be tightened/clarified?\n\nSuper interesting topic; strong references to prior work (I add a reference at the very end that I think you'd be interested in, Hani Yehia et al. (2002).)\n\nI found the paper slightly verbose -- I'm not sure i fully \"got\" the central concept of Exemplar Autoencoders. Looking at Section 3, with that in the title, I was waiting to find the canonical definition of E.A.'s, but don't quite see it -- it seems somewhat unclear given the many variants of auto-encoders discussed. The discussion in Section 2, and e.g. Figure 2c, help clarify what the author's mean by EA's. What I'm not fully sure about is, are EA's basically what I would call Auto-encoders? Fig 3c is labeled \"Exemplar Autoencoder\", but then the caption for 3c mentions \"Non-linear autoencoder\", which is confusing. The caption starts with \"Figure 3: Our insights for Exemplar Autoencoders, \" leading me to think that actually all three of these sub-figures are variants of Exemplar Autoencoders. Maybe they could clarify their terminology or re-read the draft from the perspective of someone who is not as \"close\" to the work as they are.\n\nThe contrast with zero-shot conversion is very interesting -- but here too I feel I am somewhat missing the explanation of the essence of these zero-shot methods -- though it is possible most readers do not need any additional explanation. In particular, when I see Fig 2a, I'm wondering how the content and speaker embedding vectors are trained (and on what data), and the text doesn't quite clarify that for me.\n\nMore specific comments follow:\n\n\"(a) Zero-shot conversion that learns a generic low-dimensional embedding from a training set that _are_ designed ...\": \"are\" --> \"is\".\n\n\" In supp material, ...\" --> \"In supplementary material, ...\"\n\nDefine \"retargeting\"?\n\nTable 2: Use up & down errors, as you did in a later table, to indicate whether higher/lower is better.\n\nNo conclusion? It seems you have one, but it's \"Discussion\", as a subsection of the last experiments section.\n\nThe references seem very comprehensive, but I urge you to look at, and cite, Yehia et al. 2002, \n\nYehia, Hani Camille & Kuratate, Takaaki & Vatikiotis-Bateson, Eric. (2002). Linking facial animation, head motion and speech acoustics. Journal of Phonetics. 30. 555-568. 10.1006/jpho.2002.0165. \n\nas a seminal study in this area.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Recommendation Natural ",
            "review": "This paper covers a very interesting topic and method to convert any input speech to many audiovisual syntheses via exemplar autoencoders. The manuscript is well written and presented. It is easy to follow the concept. However, there are a few major concerns. \n\nPros: This approach is novel. The presented approach is unsupervised, which makes it more practical. This approach required only a small amount of data to train the exemplar autoencoders when learning specialized models tailored to particular target data.\n\ncons: \n1- After listening to the provided sample demo files. I think this approach is still in an immature status. The generated output speech is highly distorted, without knowing the input speech, it is hard to understand the generated speech. Hence, more work is required to improve on the audio decoder part. \n2- It is not mentioned if the data used for training is all speech of native English speakers or not.\n3- Any reasons authors did not use the x-vector or i-vectors, which are proper for the speaker characterization, and instead used mel features? \n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}