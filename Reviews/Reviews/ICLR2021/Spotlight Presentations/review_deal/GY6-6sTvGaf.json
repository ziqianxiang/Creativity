{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Spotlight)",
        "comment": "The paper describes a new data augmentation approach for image based RL.  The approach is both simple and effective.  It improves significantly the performance of several algorithms across a number of tasks.  The reviewers were unanimous about the benefits of the proposed technique.  This represents an important advance for RL."
    },
    "Reviews": [
        {
            "title": "Review",
            "review": "Summary: To enable robust policy learning with image observations, the paper proposes a simple data augmentation technique that can be used with existing model-free reinforcement learning algorithms. It defines a notion of optimality invariant state transformation which preserves the Q function. An example of such transformations can be random image translations. It uses these transformations to (i) transform the input images, (ii) average the target Q values, and (iii) average the Q function themselves. Using this simple technique, they are able to get SOTA on DM control tasks and Atari 100k benchmark. On DM control tasks, it’s able to outperform SAC trained on state representations. Additionally, the paper provides ablation studies on different image transformations and robustness analysis with respect to hyperparameter settings.\n\nNovelty: While data augmentation techniques are common in computer vision, this was the first work (concurrently with RAD) to apply the technique in the context of reinforcement learning.\n\nReasons for score: Overall, I vote for accepting. The reasons are as follows. (i) It’s a simple technique which can be used with any RL algorithm to improve the performance of the algorithm (ii) Good and detailed evaluation  (iii) additional ablation studies and robustness analysis present\n\nQuestions: Will using different random image transformations in sequence help? (My hunch is it won’t as shown in RAD but still wanted your view given this method also does the average of Q function and target Q values)",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Official Blind Review #1",
            "review": " ##########################################################################\n\nSummary:\n \nThis paper investigates data augmentation in the context of RL and proposes a novel augmentation algorithm to enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The authors propose to average both the Q function and its target over multiple image transformations. The experiments on DeepMind control suite and Atari 100k benchmark show that their method outperforms previous model-free, model-based and contrastive learning approaches.\n\n##########################################################################\n\nPros: \n \n1. This paper tackles a valuable problem of improving RL by data augmentation. It will have a broad impact on the area of both representation learning and reinforcement learning.  \n\n2. The idea of averaging both the Q function and its target over multiple image transformations is interesting and promising. This approach is easy to use and can be combined with any model-free RL algorithm.\n \n3. The paper is well written and the results section is well structured. They outperform baseline methods on two popular benchmarks and conduct ablation studies to verify the contribution of each component.\n \n##########################################################################\n\nCons: \n \n1. The proposed idea is very similar to RAD, a concurrent work by Laskin et al. The performance is also similar between these two approaches. More discussions and comparisons will help the readers better understand the difference.\n\n2. They claim data augmentation is all you need. To support this strong claim. I think the authors should conduct more experiments on more base algorithms. Can we get the same conclusion if we add the data augmentation techniques to other model-free RL algorithms (e.g., PPO or TD3) or a model-based RL algorithm (e.g., PlaNet or Dreamer)?\n\n##########################################################################\n\nTaken both pros and cons in to consideration, I vote for an acceptance because of the novelty of the proposed idea and large-scale comparisons to previous model-free, model-based and contrastive approaches. However, the experiments in the paper are not sufficient to support their claims “data augmentation is all you need” and I do suggest the authors to include more experiments to make it clear.\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "interesting paper on tackling RL with data augmentation",
            "review": "This paper tackle the effectiveness of data augmentation in reinforcement learning. Authors have introduced a regularization technique, based on image shifts and Q-function augmentation,(DrQ) that significantly improves the performance of model-free RL algorithms trained directly from images.\n\nHere are two positive points in the paper:\n1. The authors have combined a model-free RL and regularized method, which shows a promising direction with a training strategy with augmented data.\n2. The authors have solid improve in continuous control by image pixel.\n\nThe negative point is that their DrQ method is very rough and very intuitive, authors can tackle more on the transformations of the images.  \n\nGenerally the paper is acceptable and they have shown a promising direction. ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The paper proposes a new approach in RL by showing effectiveness of image augmentation in DQN. They compares against benchmark available in DeepMind control suite on several environments. ",
            "review": "PROS\n-------------------------------------------\n- Finding that image augmentation helps in learning a good policy is indeed a substantial contributions. \n- Other two aspects to regularize Q-values are also helping to learn a better policy. \n\nCONS\n--------------------------------------------\nThe paper presents many experiments but there are a few crucial ones which are missing. \nFor example, \n- what is the impact on training time of DrQ as compared conventional DQN? \n- How much augmentation is good? etc.\n\nQuestion that needs justification:\n---------------------------------------------\n- Overall, the paper is well written and explain various things but the algo-1 needs explanation of all parameters. For example, \\math{D} (reader has to think that it's a replay buffer), \\math{u(T)} is not at all clear. Many notation are not explained even in text and needs a clear explanation for reader from RL and non-RL domain. \n\n- The primary claim of the paper is that image augmentation improves the performance. Sec5.1 shows significant improvement when image augmentation is used with different methods but it is very strange to see the improvement is just by adding 4 pixels on image boundary. Is there implication when we do more augmentation by increasing the size of random shift? How does the augmented image compare visually to the original image? The figure-6 in appendix shows all the results but which one is the random shift? \n\n- In figure4, the SAC state is significantly better than the DrQ and no explanation is provided. Why one should not used SAC compared to DrQ?\n\n- Paper claims that the proposed method can work with any model-free RL algorithm. Any justification or experiments to support the claim? If not, the contribution needs a re-writing. \n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}