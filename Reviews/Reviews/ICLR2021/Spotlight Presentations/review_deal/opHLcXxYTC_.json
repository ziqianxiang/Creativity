{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper advances the idea that recent “influence estimation” methods for supervised learning cannot be trivially applied to GANs. Based on Hara et al.’s method, the authors propose a novel influence estimation for GANs, and an evaluation scheme based on popular GAN evaluation methods, exploiting the fact that they are differentiable with respect to their input data. The paper demonstrates empirically that the proposed influence estimation method correlates to true influence. It also shows that removing “harmful” instances using the average log-likelihood, Inception Score, and Frechet Inception Distance versions of the proposed metric improves the quality of generated examples.\n\nAll reviewers were positive about the paper. R2 pointed out that it was well-written and appreciated the detailed analysis. They thought it thoroughly explained the similarities between it and the most closely-related recent work (Hara et al. and Koh & Liang). Concerns expressed by the reviewer were: the amount of samples needed to be removed to obtain a statistically significant result, lack of qualitative results, and an outdated baseline for anomaly detection. The reviewer also stated that they had some concerns with practical applicability and would like to see more GAN metrics, like Precision & Recall. The authors added qualitative results to the paper which partially satisfied the reviewer.\n\nR1 also thought that the paper was well-written and contributed to the interpretability of GAN training. Like R2, they pointed out the lack of visual examples (addressed in rebuttal), and asked for more insight into what kind of characteristics make a data point influential. They also requested that the authors add a metric that trades fidelity and diversity like P&R. The reviewer originally felt that the paper was below the bar, because it was “like a story without a satisfying conclusion”. However, the authors responded with additional analysis which satisfied the reviewer, and they upgraded their score by two points.\n\nR3 also found the paper well-written and interesting, like the other reviewers. The reviewer raised some similar concerns as the other reviewers (e.g. qualitative results), as well as the scalability of the method to relevant architectures, which I thought was surprising that the other reviewers didn’t mention. The authors responded that they believe their method succeeded in improving diversity of the generated samples but not their visual quality. This is an important point.\nThe additions in Appendix D have addressed the main concerns of R1 and R2, as well as R3’s concern about lack of visual analysis. R1 seems quite convinced now, and R2, though not changing their score, was already in favour of acceptance. It is an interesting finding that “harmful” instances seem to come from regions of distributional mismatch.\n\nI would like to see a fidelity-diversity tradeoff like P&R added to a paper, and a discussion of this work in relation to DeVries et. al “Instance Selection” that appears to be similarly motivated though executed differently. I think one major thing holding back this paper is the scale of the experimental analysis (Gaussians & MNIST); I hope the authors can scale the method in future work."
    },
    "Reviews": [
        {
            "title": "Paper is interesting to read and think about, but there are no meaningful results",
            "review": "The paper proposes a technique to identify \"harmful\" data samples in the training dataset of a GAN. The research question is if it would be possible to find training samples that can be removed to improve the GAN training.\n\nI found the paper generally well written and interesting to read. I have some experience with GANs, but with so many papers being published in general I am not very confident in my evaluation that something like this hasn't done before. The authors argue that this idea has been applied in other settings and that they are the first to bring this idea to GANs and adapt the overall technical idea to this new context. I agree with the authors that this is technically interesting and a worthwhile topic to explore. It is also helpful to have some papers on this topic and someone has to write the first paper. I am supportive of having some research in this direction and I believe other similar work can follow that builds on this initial idea.\n\nOn the downside, the paper fails to establish that the proposed idea can actually contribute to creating a GAN that samples images of better visual quality. I would argue that this should be the main goal. The GAN research is somewhat split. This paper  uses small datasets and builds on a simple architecture (DC GAN) that is very far from a state-of-the-art GAN (e.g. StyleGAN2).\n\nThe first question is how can we know if the presented work really transfers to a GAN that is currently relevant. The paper does not offer any insights here and this is a part that could be improved. It would be preferable if such tests would already be performed by the authors so that others can know if it is worth following this direction of research.\n\nAnother question is why no images are shown in the paper. All results are using GAN evaluation metrics: FID score, Inception, and ALL. From these metrics, I doubt that ALL is as meaningful as the authors make it out to be and I do not think the Inception score is that great either. FID has been quite useful to compare GANs. \nHowever, I am skeptical that improving the FID score directly is a meaningful endeavor. It's easily possible that an attempt to directly influence a metric such as FID score doesn't lead to better results for GANs. The metric itself has many problems and it is easy to exploit a weakness in the metric when directly optimizing for it. Similarly, an important GAN component, truncation, significantly improves quality while significantly worsening the FID. This is also an indication that the FID score is not a great metric that can be directly optimized for.\nTargeting the FID directly also has other technical problems. Removing samples from the training set changes the target mean and covariance statistics of the inception v3 network. The idea of the FID score is to compute a 2048 dimensional mean vector and covariance matrix summarizing a set of samples. Typically, the statistics of the training set and the statistics of generated GAN samples are compared.\nIn this setup, removing training images changes the problem statement. Training a GAN with removed images can no longer be compared to training a GAN with all the images. What do you do here. Do you use all the original training dataset samples to compute the FID? That doesn't make sense in my view. Then the FID is computed with respect to a different dataset.\nOr do you only use the new training dataset to compute the FID? Maybe the FID is better, but you also might have just simplified the problem.\nHow can we judge the visual results here? I would guess the results are either not distinguishable and all look mediocre (because the baseline network is outdated) or maybe the new results may even look a bit worse. Do we really want to have GAN papers without any images?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good, but missing a key piece",
            "review": "Summary:  \nThis paper introduces influence estimation for GANs. The influence estimates approximate how helpful or harmful each training sample is with respect to some evaluation metric or loss function, such as Inception Score or FID. Removing harmful instances is shown to improve GAN performance.\n\nStrengths:  \n-Paper is well written.  \n-Influence estimations adds an interpretability tool to GAN training. This is very welcome, because the training dynamics of GANs are still not very well understand.  \n-Proposed technique does a good job of estimating the true influence.  \n-Predicted influences can be used to remove harmful data points, which is shown to improve GAN performance.  \n\nWeaknesses:  \n-No visual examples of the data points that were highly influential.  \n-No analysis into what kinds of characteristics might make a data point highly influential, either in the helpful or harmful sense.   \n-None of the evaluation metrics used differentiate between fidelity and diversity. This is particularly important because removing instances from the training set inevitably reduces diversity, but this is not reflected in the current metrics. Including metrics such as Precision and Recall [1] or Density and Coverage [2] would allow us to see the trade-off between fidelity and diversity.  \n-Computationally expensive, as model parameters need to be saved at every iteration (not a major issue for this paper, but worth noting)  \n-Proposed method is only applied to very simple datasets and GAN models (likely due to the aforementioned compute issue)  \n\nRecommendation and Justification:  \nI really like the direction that this paper is headed in, but I think there is one thing holding it back. My greatest disappointment with this paper was that it does not include any visual examples of highly influential data points, nor any analysis or discussion about what kinds of characteristics helpful or harmful data points might have. These insights are half of the reason why interpretability methods such as influence estimation are useful, and without them it feels like I have read a story without a satisfying conclusion. In its current state I would rate this paper as being marginally below the acceptable threshold, but if the above concerns were adequately addressed I would very gladly increase my score.\n\nClarifying Questions:  \n-In the paper it is stated that the generator and discriminator are simultaneously updated in one step for simplicity. However, the majority of GANs trained in practice use separate update steps for the generator and discriminator. Does this method also work for separate update steps?  \n-How consistent is the selection of harmful instances? For example, does the set of the most harmful instances change throughout training (at the beginning vs towards the end)? What about across different runs with different random seeds?  \n-Recently it has been shown that removing data points that lie in low density regions of the data manifold can result in improved GAN performance [3]. I would be curious to know if you think there is any correlation between the harmful instances identified by influence estimation and the density of the data manifold?  \n\n[1] Kynkäänniemi, Tuomas, et al. \"Improved precision and recall metric for assessing generative models.\" Advances in Neural Information Processing Systems. 2019.\n[2] Naeem, Muhammad Ferjad, et al. \"Reliable Fidelity and Diversity Metrics for Generative Models.\" arXiv preprint arXiv:2002.09797 (2020).\n[3] DeVries, Terrance, Michal Drozdzal, and Graham W. Taylor. \"Instance Selection for GANs.\" arXiv preprint arXiv:2007.15255 (2020).\n\n------------------------------------------------------\n\nEdit after author response:\nThe authors have sufficiently addressed my concerns with the additions that have been added in Appendix D. As such, I will increase my score from a 5 to a 7.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Provides a detailed theoretical and experimental analysis of the proposed approach on influence estimation for GANs, but there are concerns regarding its practical applicability. ",
            "review": "Summary: \n\nThe paper presents an influence estimation method for GANs. It discusses why previous approaches on influence estimation cannot be easily extended to GANs. It proposes to use Jacobian of the gradient of discriminator’s loss with respect to the generator’s parameters to learn how absence of an instance in the discriminator’s training affects the generator’s parameters. The authors evaluate whether an instance is harmful based on its influence on GAN evaluation metrics. They show that removing these harmful instances improves performance of GANs on MNIST with respect to three metrics: Inception Score, FID and Average Log Likelihood (ALL). \n\n#################################################################\n\nStrengths:\n\n1. The paper is well-written, motivates the problem well and provides a detailed analysis of the proposed algorithm. It distinguishes its approach with other works on influence estimation such as [Hara et al.] and [Koh & Liang]. \n2. Several experiments are performed to validate the authors’ claims. They verify that estimated and true influence on GAN evaluation metrics have statistically significant correlation, and show how removing harmful instances can improve evaluation scores.  \n\n#################################################################\n\nWeaknesses:\n\n1. For the improvements on evaluation metrics to be significant, the authors need to remove a large number of samples from the dataset. For example, considering the Inception Scores in Table 7, they need to remove 15k instances out of 60k MNIST samples to get statistically significant improvements. Even after removing those instances, improvements are modest (e.g. +0.11/5.8 = 1.9%). It seems that this approach eliminates modes of the distribution that the GAN is not able to cover well. While this can improve the evaluation metrics, this improvement is artificial and does not correspond to a better GAN model. In general, removing underrepresented and rare samples can increase evaluation scores, but a strong GAN model should be able to generate those samples as well. The authors can discuss the nature of harmful samples and whether removing them actually leads to better generation.     \n2. There are no qualitative results to demonstrate which instances are harmful in training GANs. [Koh & Liang] provide qualitative results about which samples help (Figure 4) and harm (Figure 1) the model’s performance. Considering the large number of samples that need to be removed for improvements in evaluation scores, the authors can examine those samples, visualize them and try to find common characteristics among them. \n3. The authors use Isolation Forests [Liu et al. 2008] as the baseline for anomaly detection. It would be better to consider more recent approaches for anomaly detection such as those outlined in [B].  \n\n#################################################################\n\nReason for Rating: \n\nThe authors motivate the problem well and provide a detailed theoretical and experimental analysis. However, there are concerns regarding practical applicability of the proposed approach and whether it can indeed help with performance of GANs or it artificially boosts the evaluation scores. There are no qualitative evaluation on which samples harm the model’s performance.   \n\n#################################################################\n\nAdditional Comments: \n\nThere are other metrics for evaluating performance of GAN models such as Precision and Recall [A]. While I think the current metrics considered in the paper are adequate, the authors can also mention other metrics. \n\n##################################################################\n\nReferences: \n\n[A] Are GANs Created Equal? A Large-Scale Study; Lucic et al.; NeurIPS 2018 \n\n[B] Deep Learning for Anomaly Detection: A Survey; Chalapathy et al.; arXiv 2019 \n\n##################################################################\n\nAfter author response: I thank the authors for the additional experiments. It partially addresses my concerns. However, the qualitative results do not always show improvements in image quality, and most of the low-quality samples are still generated after cleansing. It’s also hard to clearly notice differences before and after cleansing in Tables 9 and 11 (2D-Normal). It would be better to use a multi-modal Gaussian with some modes being more likely. Overall, I am still concerned about practical applicability of the proposed approach. I keep my score of 7.  ",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}