{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Spotlight)",
        "comment": "The paper gives a learning-augmented algorithm for estimating the support size of a discrete distribution. The proposed algorithm is evaluated experimentally, showing significant improvements in the estimation accuracy. The reviewers unanimously agreed that the contributions are strong and relevant. I recommend accept."
    },
    "Reviews": [
        {
            "title": "Interesting learning approach for a classical statistical problem",
            "review": "The paper considers the problem of estimating the support of a discrete distribution, when provided access to samples and an oracle that approximately predicts the probability of the observed sample.\n\nThey propose an algorithm based on Chebyshev polynomials  and also show that the proposed algorithm is optimal. They evaluate the algorithm on two public datasets and a synthetic dataset and show that the algorithm performs reasonably well. The results are interesting and I recommend acceptance.\n\nThe main technical contribution is to use the approximate probability of the sample to divide the interval [0, log n /n] into exponential bins and use the best Chebyshev approximation within each interval.\nI strongly encourage authors to add technical comparisons between their work and that of Canonne and Rubinfeld 2014 and other relevant papers e.g.,\n1. Optimal Bounds for Estimating Entropy with PMF Queries\n2. Probability–Revealing Samples\n\nI am also curious to know if similar results hold for unseen species estimation (e.g., https://www.pnas.org/content/113/47/13283.short)?\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Nice blend of theory and practice",
            "review": "Estimation of the size of the support of a distribution over a discreet domain is a fundamental problem. In the standard setting, this problem is theoretically well-understood with matching upper and lower bounds. The authors assume additional access to a constant approximation of the density function at each point, and then show that this can provably reduces the sample complexity. In particular, they offer matching upper and lower bounds in this setting. While the upper bound is a twist on the existing state-of-the-art method, the lower bound seem to deviate from that. \n\nOne may say that the problem of density estimation is harder than estimating the size of the support of a distribution, and therefore assuming access to such oracle is not natural. However, the authors provide practical evidence that in some cases this is reasonable. In particular, the authors use a learning-based method for estimation of the density function and plug it in as the oracle for their support estimation method. The results are promising, and seem to be more robust than a previous method that assumed access to an accurate pdf oracle.\n\nIt will be quite interesting to discuss the kind of structure that is present in the data sets that allows to improve over the WY method. For example, is it the case that WY method performs poorly compared to the proposed method for light-tails distributions (and not so much for heavy tailed distributions)?\n\nThe paper is well written and the background work is adequately discussed. \n\nFor some reason the submission format does not allow selecting/highlighting text in the pdf file. Please check this.\n  ",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Insightful discussion on the new assumption",
            "review": "The paper studies the following problem: suppose there is a distribution $P$ over $n$ class {$0, 1, \\dots n-1$},  and one has access to a set of samples drawn from $P$. The goal is to estimate $k$, the number of classes whose probability mass is non-zero.\n\nFirst of all, it seems the literature addresses the problem by assuming each $p_i \\geq 1/n$, where $p_i$ is the mass for class $i$. Thus, by Chernoff bound it is not surprising to see that taking $O(n)$ samples suffices. However, recent progress showed that a bound of $O(n / \\log n)$ can be achieved.\n\nIn this paper, by assuming that there exists some fairly good estimate of $p_i$, i.e. we have $\\Pi_i$ on hand with $1 \\leq \\Pi_i / p_i \\leq b$ for some known constant $b$, authors show that the above sample complexity can be further improved to $\\log(1/\\epsilon) \\cdot \\frac{n}{n^{\\log(1/\\epsilon)}}$.\n\nMy major concern is such assumptions might be too strong.\n- Authors simply cite couple of related works and say: (A1) $p_i > 1/n$ is mild condition. I am not convinced by such argument. If we drop such assumption, is there any fundamental challenge to well define the problem?\n- On top of it, authors further argue that: (A2) there are good estimates of $p_i$ available to us, and the approximation parameter $b$ is also known. It looks fairly artificial for such two-sided approximation condition to hold, and it is not even of theoretical interest.\n\nOn the algorithmic end, it looks interesting to draw a more careful analysis on top of [WY19]. However, the unrealistic assumption (A2) does diminish the contribution.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of \"Learning-based Support Estimation in Sublinear Time\"",
            "review": "This paper considers the support size estimation problem using a random sample from the unknown distribution and access to some predictor of the element frequency. Under that setting, the paper improves the estimator by Wu & Yang (2019) by refining the approximation interval promised by the predicted frequency. A theoretical upper bound on the sample complexity is proved in Theorem 1 using the proposed algorithm, and it is nearly optimal as shown by the lower bound in Theorem 2. The algorithm is empirically evaluated by both real and synthetic datasets. The empirical performance improves existing algorithms of WY and CR in most cases. \n\nThe algorithmic design and analysis are based on the Chebyshev polynomials by Wu & Yang (2019). Given access to the estimated element frequency, the proposed algorithm uses a smaller approximation interval that yields a smaller approximation error and thus a smaller bias. The performance improves WY which has no access to any frequency predictor, and also improves CR that requires perfect frequency information. Overall I think this paper is theoretically sound and empirically convincing. I will recommend for acceptance after the concerns below are addressed.\n\nOne major concern is the claim of sublinear time in the title. This seems to be not justified in the paper. From my understanding, given a dataset, the computation time consists of the following: train a frequency predictor, incrementally test the base parameter, construct estimators in each interval, collecting samples, call the frequency predictor, compute each estimator and report the sum. I can see some steps are easy to compute (such as computing a linear estimator), but the overall time complexity should be justified.\n\nAnother concern is the theoretical guarantee of the frequency predictor. Estimating element frequency seems not an easy problem by itself given such a small amount of samples relative to $n$. The theorem requires $\\Pi(i)\\le p_i \\le b\\Pi(i)$ for a constant $b$. Please comment on how realistic is this assumption. I understand this is not the main concern of the current paper, and empirically predictors indeed help. But I think some proper introduction or literature review on the frequency predictor should be included, as this is the major side information for improvement. Minor comment: it might be better to write something like $\\Pi(i)/\\sqrt{b}\\le p_i \\le \\sqrt{b}\\Pi(i)$ which tolerates error in both upper and lower bounds.\n\nIn the theory part, some statements should be more rigorous. In Theorem 1, it is unclear what is a precise meaning of \"with high probability\", and what is the o(1) term with respect to. In Section 2.1, for $N_i> \\Omega(\\log n)$ with high probability, it seems to require $p_i > \\Omega(\\log n/N)$. I think by \"random sample\" this paper refers to a random sample with replacement, so in the last line on page 1, one needs $O(n\\log n)$ samples to see all elements (coupon collector problem).\n\nIn the experiment part, it seems the base parameter $b$ is pretty large for some datasets (like more than 400 for IP). It seems that the proposed algorithm reduces to WY once $b$ is moderately large such that one interval suffices. Please explain the large values of $b$. Also, please specify how do you increment the base parameter $b$ when the interval fails the sanity check. The improvement over WY is not surprising as WY doesn't have access to the extra predictor, but CR seems to completely fail in some cases which deserves further discussions.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}