{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper describes a method for adapting an RL policy in a deployment environment that does not provide a reward signal.  This concern arises commonly when a task reward is available in a robot simulator but not on the physical robot where the policy is eventually deployed.  The proposed solution is to learn an inverse dynamics model as an auxiliary prediction task on an internal state embedding that is shared with the policy.  The policy is adapted during deployment by modifying the state embedding using this auxiliary task (with the assumption that the main objective remains unchanged).  The proposed method is tested with transfer between simulated domains and also on transfer from a simulator to a physical robot.  The experiments showed the method had consistently higher performance than alternatives.\n\nThe reviewers found many positive contributions in the presented paper. These include the problem's importance (R1, R2,R4), extensive experiments (R1, R2, R3), clear writing (R1,R4), simplicity and effectiveness in comparison to ablations (R3, R4).  The reviewers saw a weakness in the method's limitation to perceptual adaptation instead of dynamics adaptation (R1-4) and the lack of novelty (R4).  The author response addressed both concerns.  They stated that the method is novel for adapting to continuously changing environments in a self-supervised fashion without rewards.  The authors modified the paper to clarify how the method demonstrates robustness to changes in the system dynamics.  The reviewers found the author response addressed their major concerns.\n\nFour reviewers indicate accept for the contributions stated above and expressed no remaining concerns.  The paper is therefore accepted. "
    },
    "Reviews": [
        {
            "title": "A high quality contribution with a couple easy-to-address blind spots",
            "review": "## Summary of the Work\nThis work presents a method for test-time adaptation of pre-trained visual reinforcement learning policies, with a particular emphasis on robotics applications. The method uses both a self-supervised representation learning objective and an RL objective during training, then at test-time (when RL supervision may not be available) adapts the agent to a new environment using only the self-supervision objective. The self-supervision objective is to learn an inverse dynamics model $p(s_t|a_{t-1}, s_{t-1})$ using an IDM network which shares its convolutional layers with the policy network $p(a_t|s_t)$. The authors provide experiments using a variety of simulation environments augmented with adaptation target tasks, then demonstrate the effectiveness of the method using real robot experiments. Overall, the method appears to be a simple and effective idea for quick test-time adaptation to appearance changes in visual RL, though the lack of a fine-tuning baseline makes it unclear how it compares to the \"null\" hypothesis of doing naive RL updates with no auxiliary losses\n\n## Pros and Cons\n\n### Pros\n* Addresses an important problem in the field (fast test-time adaptation)\n* Extensive experiments supporting claims, including real robot experiments\n\n### Cons\n* Likely limited only to appearance changes in the environment (not dynamics nor reward functions)\n* Comparisons to more \"naive\" fine-tuning baseline missing -- this compares favorably to other self-supervision systems, but how does it compare to the most basic adaptation method available to an RL agent?\n\n## Evaluation\n### Quality\n3/5\nThe overall presentation quality of the work is very high, but I take issue with some of the scientific aspects. While the experiments are all high-quality and well thought-out, this work has some issues with data analysis which are easy to address, and would make the statistical significance of the results much more clear. Why go to the trouble of collecting 10 seeds for many experiments, then only calculate a mean and standard deviation? I also believe a couple obvious baselines (fine-tuning and from-scratch training) were unnecessarily omitted and would help readers better-position the performance of this work as compared to the \"naive\" alternatives.\n\n### Clarity\n5/5\nEvery part of the text and supplemental were very well-presented. The tables and figures are helpful for understanding the method and results, and I found the mathematical portions of the text increase the clarity of presentation rather than simply providing a \"window dressing\" of formalism intended to make the work seem more principled than it really is.\n\n### Originality\n4/5\nThis work's originality stems primarily from its simplicity. I commend the authors for avoiding overly-complicated methods when simpler ones will do.\n\n### Significance\n4/5\nAddressing test-time adaptation without costly pre-train methods (e.g. meta-learning) is an important challenge for RL in the real world, and this work provides a step in this direction.  I would also appreciate if the work spent more time discussing the limitations of the method, and avoided statements which border on over-stating its potential -- for instance, the introduction suggests that adapting to new reward functions at test time is important (it is!) but this text contains no evidence that the proposed method could achieve that kind of adaptation. \n\n\n### Misc Editorial Comments and Reviewer's Notes\n\n#### Claims\n* Adapts a pre-trained policy to an unknown environment without any reward\n\n#### Mechanisms\n* Introduces self-supervision supervised with an auxiliary task\n* Auxiliary tasks\n  - inverse dynamics prediction\n  - rotation prediction (classify rotation of image)\n* Auxiliary prediction networks share parameters with the policy (particularly, CNN layers)\n* Trains both RL and auxiliary task during training, at test time uses IDM or rotation prediction auxiliary updates to adapt (but not RL)\n\n\n#### 1. Introduction\n* The reward functions in these experiments don't seem to change, so I don't think the motivation about \"craft[ing] a dense reward function...during deployment is impractical\" really applies? This motivation would be more compelling if the work proposed adaptation to new reward functions rather than only visual changes to the environment. Adaptation to new reward functions and dynamics likely requires a different family of methods than those addressed by this work. Again, see [1].\n\n\n#### 2. Related Work\n* Please see [1] for a similar work with a similar experimental setting. In particular, this calls into question the conventional wisdom that fine-tuning is too naive for test-time adaptation and can be immediately discarded with a citation to Rusu, et al. Please consider including a baseline (at least in simulation) comparing the performance of PAD to \"naive\" fine-tuning or NSL-style fine-tuning (oversampling).\n\n#### 4. Experiments\n* \" as we find that learning a model of the motors works well for motor control.\" I didn't understand this comment. Please make this more clear -- are you introducing a modeling prior which attempts to explicitly model the actuators and joints of these models?\n* Table 2, 3, 4: I find it misleading to highlight elements of the table which don't exceed the performance of the other elements *including their confidence intervals*. Statistically, these entries might be equivalent, with some probability. However, the simple standard deviation from the mean is a poor confidence interval for comparisons-- please instead include a CI calculated with a bootstrap method and a known p-value (e.g. 95% interval) to enable better comparison. seaborn can do this for you. See [2] for a guide.\n* Table 5: Please include a confidence interval\n* Would prefer to see an ablation of the test-time performance which includes only the RL loss and RL loss+IDM(PAD), in addition to the existing \"no RL loss + IDM(PAD)\". Additionally, it would be helpful to show the reader the performance of a scratch-trained policy on the test environments, to establish an \"oracle\" or maximum expected performance for each environment given your base algorithms (A2C and SAC).\n\n#### Appendix\n* Figure 5: Please calculate a confidence interval for these curves with a known p-value (e.g. 95%) rather than simple standard deviation. This makes it much easier for readers to assess the statistical significance of your results.\n\n\n[1] https://arxiv.org/abs/2004.10190\n[2] https://arxiv.org/abs/1904.06979",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Pixel variations in sim2real can be compensated for with auxiliary tasks.",
            "review": "This paper presents the Policy Adaptation during Deployment (PAD) method, which allows a policy trained on a particular visual input distribution to be transferred to a new visual input distribution, as long as the underlying system dynamics remain the same.\n\nPAD works by structuring the policy into two parts, a state embedding network pi_e and an action selection network pi_a.  Additionally, PAD learns an auxiliary prediction task with a network pi_s that predicts the inverse dynamics (necessary action to transition from s->s').  Once on the real system, pi_a is kept frozen, and pi_e is updated to minimize the inverse dynamics loss.  In practice it seems that also updating pi_s provides negligible difference although I find only the case when pi_s is kept frozen to make sense (question on this below).  An alternative loss based on predicting scene rotation is also used and shown to work generally less well than inverse dynamics.\n\nExperiments show that PAD consistently outperforms other methods, either naive baselines, simplified domain randomisations, or ablations of PAD.  \n\nPros: I enjoyed reading the paper and believe it provides an interesting approach to used IDM as a consistent aspect of the task in the face of inconsistent visual representations of state.\n\nCons: This approach is very specific to pixels and assumes consistent inverse dynamics, which might not always be true esp. in sim2real tasks.    I think the more general question is how to adapt in MDPs when part of the MDP changes but another part stays constant.  The 'trick' in my opinion in this paper is that the dynamics are consistent across task variations, so building a predictor for this consistent dimension works.  It would have been interesting to look more generally at other aspects that could change, for example what if dynamics change but pixel representations are consistent, could a similar approach be used?  Is there a 'bigger' insight here?  I am overall accepting the paper, but I feel a bit frustrated we're not getting more insights and I feel like there's something more here.\n\nQuestions: \nI am clearly being dense here, but I wasn't able to understand why the constant zero feature would be a trivial solution to a next-state prediction problem (c.f.\n\" Note that we predict the inverse dynamics instead of the forward dynamics, because\nwhen operating on the feature space, the latter can produce trivial solutions such as the constant zero\nfeature for every state.\"), could you clarify?\n\nAlthough you mention that updating both pi_s and pi_e or only pi_e seems to provide equivalent performance, intuitively it would seem that allowing pi_s to move removes the common semantic grounding of the latent observation space that is necessary for pi_a to still be able to interpret it.  Do you have any insights here?\n\nPlease comment on my 'con', and provide some insights on how you see this approach working in potentially other dimensions than the observation space.\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Overall strong submission but missing some discussion",
            "review": "#### Summary\nThe authors present a method for online policy adaptation during domain transfer in the case no reward is available in the target domain. They achieve this by adding an auxiliary self-supervised task, such as inverse dynamics prediction, that helps shape a set of features shared with the policy during training. At test time, gradient updates are then performed on these shared features based on only the self-supervised loss. The authors evaluate their method on a number of visual continuous control tasks and discrete navigation tasks and show a significant improvement over direct transfer, domain randomisation, and using self-supervision only during training.\n\n#### Pros\n- The authors present a novel, intuitive method for domain transfer in the case of absence of reward signal. This is very relevant for e.g. sim2real transfer, where reward computation often require privileged information that is not easily measurable in a real-world setup.\n- The authors perform a large sweep of experiments on a variety of different tasks, incl. sim2real transfer on a robot arm, and provide significant detail of the experimental setup.\n- The results are largely in favour of the proposed method in comparison to fair baselines.\n- The paper is very well and clearly written, with significant attention to related work.\n\n#### Cons\n- My main concern with this submission is the lack of any discussion about the assumptions and resulting limitations of the proposed method. It is only in Section 3 that it becomes clear that this method is aimed at _perceptual_ adaptation, but otherwise assumes the transition dynamics of the source and target environment, and hence the resulting optimal action, to be the same. It's unclear how this method fairs in the case of a change in dynamics. This should definitely be more emphasised in the introduction, abstract and perhaps even title. This also puts the proposed method closer to methods such as RCAN (https://arxiv.org/abs/1812.07252), which would potentially be a strong baseline to compare to.\n- While the authors acknowledge that drift in feature space may occur by also updating the self-supervision-only weights at test time, they default to this setting but do not show any quantitative results and only mention that empirically this indeed does not significantly impact performance. Perhaps related, but the IDM+PAD setting seems to be is significantly detrimental in the CRLMaze task (see Table 4, Table 7), it would good to add a comment about this failure mode.\n\n#### Conclusion\nWhile there could be some additional discussion added, I think this paper is strong enough to be accepted.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review",
            "review": "This paper studies an important problem in vision-based RL: how to adapt a pre-trained policy to an unseen environment in a self-supervised manner. To do this, the authors introduce an auxiliary task branch that can be used to tune the intermediate representation of the policy network on the fly in a self-supervised manner(e.g. inverse dynamic prediction). The experiments in the DeepMind Control suite, CRLMaze, and robot manipulation tasks show the generalization and effectiveness of the proposed method in various vision-based RL problems.\n\nStrength:\n- The paper is well-written and easy-to-follow. The demo video is impressive.\n- The problem they studied is important to the RL community, especially for virtual2real adaption. \n- The proposed method is simple yet effective. The author conducts experiments in various environments and provides a comprehensive analysis of the results. The implementation details are also detailed for reproduction.\n\nWeakness:\n- Lack of novelty. The proposed method can be viewed as a simple application of the self-supervised auxiliary task in the domain adaptation.  Every component used in the proposed method is already introduced in previous work. It is trivial to select the auxiliary task for different environments.\n-  The setting. It seems that the training environment and the test environment only differ in appearance. However, in real-world robot settings, the test environment would be of unseen transition dynamic, e.g. additional noise in action, delay in the control loop. How does this method perform in this situation?",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}