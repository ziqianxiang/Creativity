{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Spotlight)",
        "comment": "In this paper, the authors propose a new max-sliced Wasserstein distance. Specifically, the proposed method is a multiple sliced variants of the existing max-sliced Wasserstein distance. Compared to the subspace Robust Wasserstein distance, the proposed method can be efficiently computed.\n\nOverall, the proposed method is a good extension of the max-sliced Wasserstein and can be used in various applications. All authors agree to accept the paper, so, I also vote for acceptance."
    },
    "Reviews": [
        {
            "title": "Distributional Sliced-Wasserstein and Applications to Generative Modeling",
            "review": "This is a well written paper with some interesting results.  This paper is to propose a distributional sliced-Wasserstein distance \nto address the limitations of standard SW and Max-SW. The proposed method finds an optimal distribution over projections that can balance between exploring distinctive projecting directions and the informativeness of projections. Some theoretical results are presented in both the main paper and its supplementary document. This reviewer personally enjoys reading this paper. \nHere are a few additional comments. \n\n1. How to select $\\lambda_C$ in practice? The authors need to discuss it in details. \n\n",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Slicing from a tuned distribution of slices is better than one best slice or uniform slicing",
            "review": "Summary:\nThe paper describes a family of sliced Wasserstein divergences that maximize the distribution over slices subject to constraints on the concentration of slices. Extremes of the family are the sliced Wasserstein and max-sliced Wasserstein distance. In between these the divergence is sensitive to informative discrepancies in multiple subspaces, while still leveraging the relatively fast computation the Wasserstein distance in one dimension for empirical distributions. A dual formulation provides a variational approximation using a (possibly deep) neural network to instantiate the slicing distribution through a pushforward approach. Basic theory prove the divergence is a distance metric between measures. Extensive experiments show improvement over sliced and max-sliced Wasserstein distances and related projection based approaches. \n\nStrengths:\nOverall the paper is a clear and original contribution to the field of sliced Wasserstein distances. The paper's appendix shows multiple applications of the methodology where it outperforms existing divergences. \n\nWeaknesses:\nThe specific parameterization of the mapping $f$ is not clear (it is not given in the main body and it is poorly described in Appendix F as a \"single multi layer perceptron (MLP) layer with normalized output\"). In the main body it is called a deep neural network. \n\nAlso I am a bit wary of interpreting the wall-clock timing differences due to differences in software implementation and hyper-parameter choices for optimization and number of gradient steps (Figure 2(a,c) and Figure 3). Learning curves are more fair, but even these use the best value of  $\\lambda_C$ versus default settings for other methods. As the paper recognizes the max sliced generalized Wasserstein with a deep neural network is the closest competitor and also requires solving a max-min saddle point optimization. It is shown in the appendix as Figure 6(a–e) (green trace). Even in this case the differences in learning curve and timing could possibly be due to software implementation and hyper-parameter selection in the optimization.  \n\nOverall this is a clear accept as it a meaningful improvement to sliced Wasserstein approaches. There a few points that need to be clarified, and I would argue for less emphasis on the wall-clock timing of methods. \n\nHow many parameters are used in the neural network defining $f_\\phi$? Would an empirical distribution optimized over a corresponding number of fixed slices (originally randomly drawn) perform as well? \n\nMinor comments (main body):\nP.1 use Author (year) instead of Author (Author year). \n\nP.1  The case of empirical sample has not been described so what is meant by the term \"atoms\" to describe $k$ is not apparent to the reader.  Then on page 6 \"$k$ is the number of support points\". The terminology could be more consistent. \n\nP.2  Double parentheses \"))\"  after Deshpande et al., 2019. \n\nReferences. Capitalization of conference names and book titles is not consistent. Sinkhorn is a proper noun and should be capitalized. \n\nMinor comments (appendix):\n\nThe appendix doesn't appear to be carefully proofread. \n\nSometimes the notation uses $.$ instead of $\\cdot$ for the arguments of multivariate functions, this is harder to read as in  $\\mathrm{DSW}_p(.,.;C)$ versus $\\mathrm{DSW}_p(\\cdot,\\cdot;C)$ \n\nThere are a number of missing definite articles for example \"that length of side of right triangle\" -> \"that the length of the side of the right triangle\".\n\nThroughout, \"close-form\" -> \"closed-form\".\n\nP.18 \"subspcaae\" and full stop \".\" beginning new line. \n\nP.19 \"as the al time increases considerably\" \n\nP.19 \" DSW and DGSW are slower than Max-SW, Max-GSW (50 gradient updates to find the max direction), and Max-GSW-NN (50 update times for the defining neural net function).\"  Is there any meaning to the subtle variation between \"gradient updates\" and \"update times\".\n\nP.23 \"single multi layer perceptron (MLP) layer\" ? This is not clear. \n\nP.23  \"as the f function in the dual empirical forms of DSW and DGSW for the dual empirical forms of these distances)\" Redundant wording and extra parentheses. \n\n\n\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Interesting novel variant of Sliced Wasserstein, but somehow incremental",
            "review": "\nThe paper presents a novel variant of the Sliced Wasserstein (SW) distance. Wasserstein distances have been used recently in lot of machine learning problems. One of the major problem is that, in its primal form, it is computationally expensive. In order to alleviate this problem, a class of methods, called sliced, leverage on the fact that Wasserstein has a closed form expression in 1D (which amount to sort the samples). It replaces the original Wasserstein distance by an expectation of 1D sub-problems over directions drawn uniformly on the unit hypersphere (akin to a Radon transform). Observing that not all the directions are meaningful, the authors propose a variant of SW where the expectation over all the directions is replaced by an expectation over a distribution of directions. The ‘extent’ of this distribution is controlled by an extra parameter. Interestingly, the authors show that this formulation is computationally tractable if one parametrizes this distribution by a measurable function, expressed as a neural network. This result is obtained by deriving the Lagrangian dual of the original problem. Comparisons with previous works are then given in two GAN scenarii: one on MNIST to explore the importance of the different parameters, and another on larger and more complicated datasets, where the FID score is exposed. \n\nThe paper is well written, very clear and easy to follow. Related works are correctly cited. There is one missing work that should be cited though: \n\nMeng, Cheng, et al. \"Large-scale optimal transport map estimation using projection pursuit.\" Advances in Neural Information Processing Systems. 2019.\n\nWhere authors choose the most relevant directions in a way similar to projection pursuit. I really believe this paper could also lead to interesting comparisons. \n\nAll in all, the presented method is a variant of SW that builds on several previous works exploring a similar idea (how to better sample the directions), such as max-SW or subspace robust Wasserstein distances. As such, it can be considered relatively incremental, but this should not totally prevent publications if the computational benefits/performances are very good. Regarding this point, I have some questions about the experimental section:\n - it seems that Figures 2.a (W_2 score) and 3.a (FID) are obtained at convergence. From appendix F, it seems that the number of training epochs is fixed for all methods. I wonder if this setting is fair for comparing max-SW, as far as only one direction is contained (the max direction), the gradient might gather less information. I suspect that more iterations might be needed, why not going until the full convergence  of the model ?\n - when comparing SW and DSW, are the directions drawn randomly for every batches ? (I guess this the common practice when using SW). As such, I do not understand what is the meaning of fixing the number of directions in advance.\n - finally it seems that DSW is computed on mini-batches of samples. While I acknowledge there is a common practice to do so, I think computing a 1D Wasserstein on a mini batch is not the same as computing the 1D W on the full dataset. As such, this mini batch version of SW is not the same as computing the true SW. In the end, i) the size of the mini-batch might have an impact on the estimation quality, that should be discussed ii) if computing 1D W on mini batches, why not computing and comparing with the mini batch version of the original version of W ? This has been done in several papers and has shown to give good results (See the recent study on this theme\nFatras, Kilian, et al. \"Learning with minibatch Wasserstein: asymptotic and gradient properties.\" the 23nd International Conference on Artificial Intelligence and Statistics. Vol. 108. 2020.\n) , plus it does not have the limitation/artefacts of the sliced Wasserstein.\n\nFinally, I would have liked to see other types of experiments than a GAN to really assess the power of this new methodology (SW has been used for classification, domain adaptation, computing gradient flows, etc.). Why limiting the applications to generative modeling ? If generative modeling is the target, then I would expect to see comparisons with other types of metrics than SW (wether it be dual version of W_1 as in WGAN, MMD, or any other divergence from the model zoo).\n\nFor all these reasons my recommendation will only be ‘above acceptance threshold’.\n\n## after author response\nI thank the authors for providing sensible answers to my questions, and analysis of their method compared to the minibatch version. I am still not sure about the fact that the rate of convergence of  SW is indeed indepedent of the dimension, as the dimension is indeed hiddent in the scalar product. Nevertheless, I believe this question is out of the scope of the paper, and I changed my final rating accordingly to the new version of the manuscript.\n",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}