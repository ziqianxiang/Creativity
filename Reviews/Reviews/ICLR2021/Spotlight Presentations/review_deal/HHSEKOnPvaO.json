{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper presents an interesting idea for task-free continual learning, which makes use of random graphs to represent relational structures among contextual and target samples. The reviewers agreed that the technical idea is novel, the experiments are extensive and the presentation is good. The authors addressed the reviewers' concerns in the rebuttal. I recommend to accept."
    },
    "Reviews": [
        {
            "title": "The paper provides a new perspective for replay-based continual learning. Unlike the traditional replay method, this paper uses a learnable random graph to expand this memory. Use graphs to capture pairwise similarities between samples. The model can not only prevent forgetting, but also help the learning of new tasks. Good results have been achieved through the given experimental results.",
            "review": "Strengths:\n - the combination of components is novel, Especially, this paper consider the relevance of instance in memory. \n- The model can not only prevent forgetting, but also help the learning of new tasks. The promotion of current tasks is also crucial in continual learning.\nWeaknesses: \n- the empirical validation is weak. Therefore, more new models need to be compared. For more details, please refer to “Reasons for reject”\n\nReasons for accept:\n1.\tThe structure of this paper is clear and easy to read. Specifically, the motivation of this paper is clear and the structure is well organized; the related work is elaborated in detail; the experimental setup is complete. \n2.\tBased on the use of replay to solve catastrophic forgetting, the current popular graph structure is introduced to capture the similarities between samples. Combined with the proposed Graph Regularization, this paper provides a new perspective for solving catastrophic forgetting. \n3.\tThe experimental results given in the paper can basically show that the proposed method is effective. The ablation study also verified the effectiveness of each component. \n\nReasons for reject:\n1.\tThe lack of comparison of experimental effects after replacing Graph Regularization with other regularization methods mentioned in this paper, or other distance measurement methods, eg., L2.\n\n2.\tThis paper compares relatively few baselines, especially recent studies. I hope to see the comparison results of some papers in the list below. The latest papers on the three types of methods (regularization, expansion, and rehearsal) for solving catastrophic forgetting are included. Therefore, if it can be compared with some of these models, it will be beneficial to the evaluation of GCL.\n\n[1] Ostapenko O , Puscas M , Klein T , et al. Learning to Remember: A Synaptic Plasticity Driven Framework for Continual Learning. ICML 2019\n[2] Y Wu, Y Chen, et al. Large Scale Incremental Learning. CVPR 2019\n[3] Liu Y , Liu A A , Su Y , et al. Mnemonics training: Multi-class incremental learning without forgetting. CVPR 2020\n[4] Zhang J , Zhang J , Ghosh S , et al. Class-incremental learning via deep model consolidation. 2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\n[5] Guanxiong Zeng, Yang Chen, Bo Cui, and Shan Yu. Continuous learning of context-dependent processing in neural networks. Nature Machine Intelligence, 2019.\n[6] Wenpeng Hu, Zhou Lin, et al. Overcoming catastrophic forgetting for continual learning via model adaptation. ICLR 2019\n[7] Rao D , Visin F , Rusu A A , et al. Continual Unsupervised Representation Learning. NeurIPS 2019\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "an interesting integration of continual learning, random graph, and attention network",
            "review": "This paper presented a memory-based continual learning model where relationships between training samples are represented with a random graph that is defined from the non-linear embedding of the input data. Catastrophic forgetting between tasks is partially (1) alleviated with a graph regularization that penalizes changes of random graph statistics, and (1) memory replay and reservoir sampling to update memory. The performance of this model is evaluated against several state-of-the-art models to handle the catastrophic loss. \n\nI tend to rate this paper as a good paper for acceptance because the presentation is clear, the idea is novel and the experiment is convincing.\n\nPros:\n\n+ clear presentation in Section 3, Graph-Based Continual Learning, plus Figure 1.\n+ Good literature survey in Section 4, Related Work.\n+ A good idea to related graph network and attention mechanism.\n\nCons: \n\n- Are all edges equally important in the regularisation term in Eq. 7? I feel more theory could be dug out by using, for example, tangent distance.\n- What does a graph representation really give us, since as authors mentioned, this is in a sense an attention mechanism? Can higher-order graph relationships be utilized if we replace a random graph with a ergm containing more structures? \n\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Interesting paper",
            "review": "This paper presents an interesting idea of using random graphs to represent relational structures amongst contextual samples and between contextual samples and target samples. Besides, the authors propose a regularization objective to alleviate catastrophic forgetting. \n\nThe novelty mainly comes from the use of graph structure to preserve the memory of previous tasks. The results on public datasets are encouraging. Also, the authors show some correspondence between the learned graphs and underlying (clustering) structure of data. It would be great if the authors can also extend this algorithms to other real-world datasets, e.g., capturing the climate or environmental changes over time, and provide more interpretation of learned graph structrure. \n",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Novel idea of using random graphs for continual learning",
            "review": "Summary:\n\nThe paper proposes a novel way of using random graphs to improve task-free continual learning method. It builds to random graphs, G and A, based on the similarity of images stored in the memory and those of the current tasks, and utilize the relative information to build representation of the images and predict. The idea is well-formulated, and carried out in a sound way. The graph regularization term resembles the knowledge-distillation, as the authors also mentioned, but it serves different purpose of preserving the covariance structure of the outputs of the image encoders. \n\nThe experimental results look quite strong and the ablation study also looks sound. Overall, I find the paper quite strong. \n\nCon & Questions:\n\n- It seems like there are 4 hyperparameters: \"temperature\" for the Gumbel-Softmax relaxation and 3 lambdas. Fig 6 shows the effects of $\\lambda_G$, but what about others? How sensitive is the performance with respect to other hyperparamters? Since the problem setting is a single-pass setting, how are the hyperparameters selected?\n\n- The inference time is a bit longer (not too much than others) since the algorithm has to sample graphs 30 times. What happens if the number of samples is less than 30 so that the inference time becomes similar to ER? Would the performance degrade significantly? How critical is the sample size 30?",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}