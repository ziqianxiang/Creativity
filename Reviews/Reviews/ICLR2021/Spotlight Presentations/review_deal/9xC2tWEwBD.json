{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Spotlight)",
        "comment": "The paper discusses a new threat model for multi-exit DNNs: attacks against efficiency of inference. The proposed attack increases the inference time of such networks by the factor of 1.5-5, while at the same reducing the accuracy of attacked networks. Unlike classical adversarial examples, the new type of attack cannot be thwarted by adversarial training.\n\nOverall, the paper exhibits a novel contribution, is well written and methodically sound. Its practical motivation is somewhat weak, as it is currently unclear for which applications such attacks may be feasible. However, the novelty of the threat model addressed by this paper makes it an interesting methodical contribution. "
    },
    "Reviews": [
        {
            "title": "Interesting problem, simple yet effective method, good results.",
            "review": "This paper studies adversarial attack and defense for adaptive multi-exit network. Adaptive multi-exit network is, by itself, a pretty new and under-studied topic, let alone the adversarial study on top of it. This paper proposes a simple-yet-effective DeepSloth attack based on layerwise loss function. It also proposed an efficacy metric for better evaluation. The experiments are conducted on four multi-exit networks and two datasets: cifar10 and tiny imagenet. In the appendix, there is also evaluation on different norms of attacks. The results on white-box and black-box attack demonstrate the effectiveness: DeepSloth not only hurts the accuracy (also achieved by baselines), but also hurts the efficacy (only achieved by DeepSloth). To reduce the computation burden of performing the attack, the authors did two things: 1) model partitioning in scenario of IOT, and 2) universal attack across a dataset. At the end, the authors adapt AT to multi-exit networks and demonstrate that AT is effective in general and DeepSloth is further helping AT for better robustness.\n\nOverall, I think this paper is pretty complete, containing both attack side and defense side. The proposed method is simple and effective, and the empirical results look good.\n\nMy main concern is about the usefulness. The main gain over baseline attack is that DeepSloth also hurts efficacy. In practical use case, two factors lead to bad results to users: 1) inference timeout; 2) wrong prediction. So if we already got wrong prediction, why we care efficacy (which may cause inference timeout)?\n\nSome other minor points.\n\n1. Section 3: metrics. This is novelty of this paper, so I think should be emphasized somewhat. \n\n2. From Appendix, why DeepSloth is half time versus PGD-(avg)?\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Detailed investigation of a new threat model",
            "review": "\nSummary of the paper\n=================\n\nThis paper studies a new category of adversarial attacks, i.e., attackers that try to slow-down multi-exit DNNs using adversarial examples. The paper extended adversarial attacks to perform the slow-down attack and showed that the attacks could slow-down multi-exit DNNs by 1.5x - 5.0x. Additionally, the paper experimentally answers many questions such as (1) the effectiveness of adversarial training against the attack, (2) input-agnostic attack, and (3) cross-architecture/domain transferability.\n\nStrongness\n=========\n\n- The paper suggests a new threat model.\n- The paper studies the problem from multiple aspects such as new-attacks, the effectiveness of existing and new defenses, and transferability.\n\nOverall, I think the paper suggested a new and interesting problem and also performed sufficient experiments and analyses for the new threat.\n\nWeakness\n========\n\nSome of the motivation of the attack is unclear. I am not convinced that attackers are motivated to use the slow-down attack instead of performing DDoS on cloud applications. And I do not think multi-exit models are used on IoT applications if the target is a real-time system (The reviewer is not a specialist in real-time systems, and please correct it if it is wrong). The reviewer believes that more discussion and motivational examples of the new thread model will increase this paper's impact.\n\nClarity\n=====\n\nThis paper is well organized and clear.\n\nRelation to prior work\n================\n\nTo the best of the reviewer's knowledge, the paper is correctly placed in the literature. However, it is not likely but possible that the reviewer is missing some important prior work.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Lacking motivation and novelty",
            "review": "This paper aims to indicate a new direction to conduct an adversarial attack: to delay the inference time/raise the computational costs of a ''multi-exit'' model.\n\nHowever, both the motivation and novelty are lacking, I will argue for the rejection if other reviewers wish to accept it.\n\nFrom the perspective of the industry:\n1. We do not actually use the so-called ''multi-exit'' architectures, because they could be difficult to optimize, quantize, assemble, and so on. For mobile devices, we deploy different ''single-exit'' architectures to meet their computing capacity and balance the performance. And it is even trivial to maintenance these architectures because we have developed automation tools.\n2. On the cloud servers, the deployment is redundant, such that the service will still work stably, even all the queries require the maximal flops. \n3. After all, it is easy to limit one's Queries Per Second (QPS), and nothing need to be worried about.\n\nFrom the perspective of academia:\n1. This paper only employs the PGD attack and the adversarial training on a specific task, the novelty is lacking.\n2. In section 5.2, if one knows the architecture and the training set and the task, how could this configuration being called ''black-box''.\n\nUPDATE:\n\nIn the rebuttal, the authors emphasize two facts:\n1. There exist such IoT scenarios and real-time systems that employ multi-exit architectures (including those that employ cloud computation.).\n2. The slowdown attack is effective in these scenarios.\n\nHowever, to prove this method works in practice, it is not a simple \"1 then 2\", you need to show us \"1 and 2\". That is, you do actually deploy any system described in [1,2,3,4,5,6,7,8], and provide a feasible approach to attack with your method, and report the actual damage caused by your method, and convince the readers the damage is significantly severe compared to the efforts spent for causing the damage. \nOtherwise, it is only an application of PGD with a different loss function.\n",
            "rating": "3: Clear rejection",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Good paper with an interesting new adversarial threat model",
            "review": "This paper defines a new problem named slowdown adversarial attack to decrease inference efficiency of early multi-exit networks.\n\nPros:\n1. Slowdown attack on efficient-inference networks in an innovative new problem with wide real-world applications.\n2. The proposed method, basically pushing the model prediction to uniform distribution, is straightforward and intuitively correct.\n3. The universal/class-universal variances of slowdown attack provide more flexibility of the proposed method, and the blackbox experiments show good generalization ability of slowdown attack across models and datasets.\n4. Last but not least, the authors show that traditional adversarial training is not a cure for the newly proposed slowdown attack, showing the challenging character of this new threat model.\n\nComments:\n1. Limitation in application scenario: From the prospective of an attacker, one may want to jointly attack inference efficacy and accuracy. Is it possible to design a more generalized setting, where the adversarial attacker can generate images that both slowdown inference and leads to wrong prediction? The current method pushes model predictions to uniform distribution, so it may be ineffective to generate adversarial images to fool classifiers. This is validated by the results, where the slowdown adversarial images have an 5% or 15% lower accuracy compared with clean images. \n2. Any discussions on how to conduct slowdown attacks on other efficient-inference models (e.g., SkipNet [1])?\n3. Please consider releasing your codes.\n\n[1] SkipNet: Learning Dynamic Routing in Convolutional Networks.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}