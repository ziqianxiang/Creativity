{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Spotlight)",
        "comment": "The rebuttal (revisions, and released code) very successfully addressed all the major concerns the reviewers had.\n\nPros: The dynamics distance function is a very neat, simple (which is good in this case) idea that is theoretically sound, has proven to perform well in thorough experimental results, and that can be broadly applied.\n\nCons: None"
    },
    "Reviews": [
        {
            "title": "Review for Model-Based Visual Planning with Self-Supervised Functional Distances",
            "review": "#### Summary:\n\nThis paper proposes to learn functional distances to varying goals concurrently with a latent dynamics model from images. A network is trained to predict the Q-value of state action pairs for a sparse reward at the goal. This way the Q-function represents the shortest path distance to the goal. The paper also proposes a specific learning scheme for training the model from random rollouts in the environment. The learned distance function is used with the dynamics model for planning from start to goal images using model-predictive control (CEM). The approach is evaluated on simulated reaching and object pushing tasks. It is compared with state-of-the-art methods and demonstrates improved performance in some of the tasks.\n\n#### Strengths:\n* The proposed approach for learning functional distance via Q-learning seems novel and effective.\n* The approach is evaluated on simulation benchmark tasks and compared with state-of-the-art method. The proposed approach outperforms previous methods in some of the tasks.\n\n#### Weaknesses:\n* Training data in the environment is collected using random policies, however, the paper does not provide sufficient detail about the design of these policies. Does data collection require random initial states in the environment and is this feasible in real systems?\n* The data collection scheme seems to be important for the performance of the method. Can this be applied to the other state-of-the-art approaches as well, and what is the performance gain for those?\n* The paper states that \"in theory\" MBOLD could use arbitrary image-based forward models. Please detail why and which theory.\n* p. 5, the explanation on how state transitions with positive goals from the sample trajectories are generated is not comprehensive (\"To sample positive goals...\"). Why should it make sense to use goals by images with a \\Delta time in between a transition (t -> t+1) and how to obtain this from the data with a given sampling rate? Please clarify.\n* The main paper does not give details on how the Q function network is implemented. It only appears in the supplementary material which suggests that the implementation significantly differs from the presented method: The Q function as in eq 3 is actually not implemented by the method, but an actor network directly predicts the best action in a state. Please clarify.\n* What is the importance of choice of the prediction horizon h in eq 3 on model performance?\n* The training data and goal selection scheme should also be used to train and evaluate the other state-of-the-art approaches to analyze the contributions of training data generation vs. distance learning. \n* The training data selection and goal specification encode the tasks in the evaluated simulation environments. How does the method perform with random trajectories and goal states which require less domain knowledge? The claims in the conclusion appear misleading (\"By leveraging offline data collected without a specific goal in mind,...\"). Please clarify the claim.\n* p. 8, Table I, please compare performance with different planning horizons. Does \"Q-function only\" correspond to a planning horizon of 1 ?\n\n#### Recommendation:\nThe paper proposes an interesting new approach that learns distances to goal states for model-based visual planning. \nThe paper should address the points raised in sec \"Weaknesses\".\n\n#### Question for Rebuttal:\nPlease address points raised on weaknesses above in the rebuttal.\n\n#### Post-rebuttal comments:\nThe author response has well addressed most of my concerns on technical details and experiments.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Official Blind Review R2",
            "review": "###########################################################################################\nSummary:\n\nThis paper proposes a novel distance function that considers task dynamics for visual MPC applications.\n\n###########################################################################################\n\nPros:\n1.\tThe dynamics distance function idea is useful and it can be useful for many existing tasks or visual foresight frameworks.\n2.\tThis work builds on previous works on model-based reinforcement learning and visual foresight. The simple idea of dynamics distance function works pretty well from the results.\n3.\tThis work leverages offline datasets obtained from a random policy instead of online interaction.\n4.\tThe realization of the dynamic distance function idea is interesting by referring to the robot proprioceptive state information and the arm movements. From the experiment, this seems to be a simple idea but works quite well.\n\n###########################################################################################\n\nCons:\nMy concern about the generality of this work actually comes from my last point in the Pros section. Will this assumption always true? Or is it just true when the robot takes more pixels in the observation image? Since there is no such analysis or a more detailed explanation about why this helps, it is hard to tell whether this assumption can generalize to other tasks.\n\n###########################################################################################\n\nRecommendation and explanation:\n\nOverall, I feel like this paper presents a simple idea that works quite well for the presented task. I thus recommend acceptance. Please also refer to my Pros section for more details.\n",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Great method, hard to understand",
            "review": "# ICLR 2021 Review - Model-based Visual Planning\n\n## Summary\n\nThe work introduces a method that allows learning a forward model and a distance function (for visual similarity) from offline data to later use this in planning. The work compares to sensible baselines and achieves competitive results.\n\n## Strengths & Weaknesses\n\n#### Strengths:\n\n- The method is surprisingly simple (in a good way)\n- The method can leverage random motion without any task-specific reward assignment to learn a model that's suitable for planning solely based on a goal image.\n\n#### Weaknesses:\n\n- It took me the longest time to figure out what's happening in this method because the paper isn't written terribly clearly. I think this can be resolved by reformulating the \"distance learning\" part of section 4. The high-level idea is clear from the beginning but I was struggling to see how exactly the distance function is learned. What you're saying is (a) that you assume that for all recorded image observations, you're also storing the robot's proprioceptive joint states, (b) that you're randomly sampling goals in the recorded trajectories, and that (c.) you're learning the distance function based on how similar the joints are, not how similar the images are. Maybe just add to the formula $r(s_t,a_t,s_{t+1},g) = 1_{s_{t+1}=g}$ that this is true if $x_{t+1} \\approx g \\pm \\epsilon$, where $x_{t+1}$ are the joint states of the robot in state $s_{t+1}$ and $\\epsilon$ is a small tolerance value. Maybe another diagram to illustrate this would be nice. Figure 2 is nice as a high-level overview but the crucial part is definitely the distance function and I felt that wasn't well-explained.\n- I think another important weakness of this method is the scope, i.e. data generation wrt. hard-to-reach areas of the state space. Imagine that your robot arm can't reach an object directly but has to pick up a tool to reach the target object. A random policy may touch the tool but is surely not going to take the tool and figure out that it can be used to reach the target object. This problem can be solved with dense reward (i.e. distance to tool, then distance tool-object) but in this case, this would be incredibly tricky. This is a fundamental problem of using offline data without any annotations and I acknowledge that it wasn't the intention of the method to overcome this exact problem but I'm saying this limits the usefulness of this method in any practical setting where certain areas of the state space are harder to reach.\n- No code was included, making this harder to reproduce.\n\n**TL;DR to improve my rating,** it'd be great if you could rewrite the method section to be a bit more clear and maybe add a sentence to the discussion section on how you'd deal with hard-to-reach areas of the state-space. Also please add an anonymized Github repo containing the project code or link a ZIP file - this has become a common practice in many ML conferences.\n\n## Impact & Recommendation\n\nThere's a handful of works that deal with offline policy learning but none to my knowledge that learn an image-based distance function that can be used to reach a target configuration. There's VICE-RAQ [1], but they need a lot of positive/negative human labels and there's PlaNet/Dreamer [2] but they do need the reward during training to shape their latent representation. One of my students experimented in the past where they removed the reward prediction loss from PlaNet and the performance of the forward model dropped significantly, indicating that the reward-conditioning is very important there.\n\nI'd recommend accepting this paper and I have high faith that the authors can incorporate my suggestions above. \n\n\n## Nitpicks and Comments\n\n- Intro paragraph 2 can be shortened significantly. I don't think a lot in there is relevant to set the stage for the paper.\n- Make fonts in Fig. 1 bigger. Also in Fig. 1, it looks like the image on the top left is one step before the image at the bottom center. Slightly confusing.\n- Fig. 3 fonts on the left are terribly small, also please remove the grey background on the right. Plots don't need to have backgrounds (same for Fig. 4 and 5).\n- Please specify the number of seeds for all experiments in the main paper.\n- The second paragraph in section 5-\"Qualitative Analysis\" is also a bit ambiguous and could use some rephrasing.\n- Fig. 6 is good.\n\n## References\n\n[1] https://arxiv.org/abs/1904.07854\n[2] https://arxiv.org/abs/1912.01603",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The paper combines optimal control and reinforcement learning for the execution of robotic manipulation tasks with variable goals. I believe the work may represent a nice contribution to the robotics community. ",
            "review": "The paper combines optimal control and reinforcement learning (RL) for the execution of robotic manipulation tasks with variable goals. The approach learns, on the one hand, an image-based predictive model using a deep neural network and, on the other hand, a distance cost function using Q-Learning. These model and cost function are used to define the next action to execute using model predictive control (MPC). The cost function is learned from task-agnostic data by randomly generating goal states from the state space and including these goals as input variables in a Q-learning approach. This permits defining cost functions for MPC for variable goals. \n\nThe approach is appealing since it exploits the synergies between MPC for planning over short horizons using approximated predictive models and RL for learning longer horizon goals reflected in the cost function.\n \nThe approach is an extension of already existing methods that use approximate dynamic programming for distance estimation. One of the claimed contributions with respect to them is the offline nature of the proposed method, in contrast with the online one of these other approaches. This offline nature is highlighted across the paper. In my opinion, going from online to offline is more a downgrade than an upgrade since offline approaches rely on limited and possibly biased data that might be insufficient to guarantee optimality an adaptability to variable scenarios. I would suggest the authors to be more conservative with this particular claim.\n\nThe distance learning is based on adding the goal as input variables in a Q-learning setting, where these goals are randomly selected. I am wondering what would be the result if they use a single RL method to learn the policy considering the goal as part of the input space. The authors indeed provide some evidence in the Experiment section (Table I) in this vein by comparing the results of their approach with that of the policy learned in the distance learning method. However, this comparison is still not conclusive. Experiments with more advanced RL approaches (e.g. deep RL) should be provided to fully assess the benefits of combining MPC with RL.\n\nThe authors suggest in the Abstract that the approach is suitable for applications where distances in the observation space are not meaningful. However, in the experiments, they use the final distance to the goal of each relevant object to evaluate success and, in distance learning, they use a notion of distance to the goal to define the reward. I would rephrase the statement in the Abstract for consistency.\n\nProvided the method uses limited data in an offline approach, how can the authors guarantee optimality and how can they quantify how different is the learned behavior with respect to the optimal one?\n\nI am also missing an analysis of the computational effort required to learn the predictive model and the distance cost function as well as to define the next action to execute using MPC. I suspect that the computational effort would significantly surpass the one needed by using a single RL approach. This establishes a trade-off between performance and computational effort that needs to be discussed.\n\nThe motivation of the proposed approach is very similar to that of inverse reinforcement learning (Fu et al., 2017). The authors should explain the advantages of using their approach with respect to it.\n\nIn the second page, the authors refer to their approach as a model-based RL one. I disagree, their approach uses RL only to learn the distance function while the decision-making is carried out using MPC without explicitly representing a policy. \n\nMinor comments:\n- The difference between positive and negative goals is not clearly explained (Page 5).\n- The bullet points in Page 7 are not properly introduced.\n\n\nFu, J., Luo, K., & Levine, S. (2017). Learning robust rewards with adversarial inverse reinforcement learning. arXiv preprint arXiv:1710.11248.",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}