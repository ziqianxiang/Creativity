{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Spotlight)",
        "comment": "All reviewers are for accepting the paper: in particular, R1 and R3 found the rebuttal sufficiently convincing to increase their scores from their initial assessment leaning towards rejection. \n\nStrengths:\n+ Clarity \n+ Simplicity of the proposed approach \n+ Convincing experiments outperforming reasonable baselines across all problem instances\n\nWeaknesses:\n+ Scale (as noted by R2 and R3) to larger problem sizes, beyond the setting of less than a dozen.\n\nI agree with some hesitation that the paper is narrow in scope (both in interest from the community and scale---and ultimately whether it would interest the overall quantum computing audience). However, I think the paper makes significant advances toward the area of adiabatic quantum computation."
    },
    "Reviews": [
        {
            "title": "Promising for designing quantum computers, although some potential scaling issues",
            "review": "Summary:\nDeep neural networks are used for supervised learning of optimal optimization schedule for various problems in adiabatic quantum computing (AQC). The usefulness of the method is assessed on three families of optimization problems.\n\nStrengths:\nThe supervised learning method is straightforward and general, yet is the first such method to be employed for adiabatic scheduling.\n\nThe method show impressive performance on all of the problem instances, outperforming both simple baselines and recently proposed methods using reinforcement learning.\n\nIn contrast to previous ML methods, the proposed method doesn't require simulations of the adiabatic evolution during training, with all such work being done upfront, in preparation of the training set.\n\nCritiques:\nIn order to apply the proposed supervised learning method, the ability to compute the optimal adiabatic schedule with reasonable accuracy is already required. To ensure this method has relevance for nontrivial problems in AQC, I would request that the authors give more evidence for the generalization capabilities of their method, for example by assessing the performance of the model in one of the following settings: (1) With different training set sizes, or (2) Using only QUBO instances or only randomized Hamiltonians (as described in Section 3.4) for the training data, rather than the mixture of both which is currently employed. I understand that the performance on Grover search and 3-SAT gives some evidence for this generalization ability, but some more clear-cut evidence would be desirable here.\n\nUsing the literal Hamiltonian as an input to the supervised problem scales quite poorly with increasing problem sizes. Could the authors comment on how to deal with this scaling issue? In particular, do the authors see any means of giving an efficient parameterization for the Grover search and 3-SAT problems in lieu of the Hamiltonian diagonal elements, along the same lines as how QUBO problems can be specified via the instance matrix Q.\n\nCan the authors comment on why the \"optimal\" path in Figure 1 sometime gives a worse final probability of success than the supervised learning method proposed?\n\nRecommendation:\nWhile the paper leaves open some questions about the generality and scalability of the proposed method, the novelty and performance of the method is promising. I would recommend acceptance.\n\n### UPDATE AFTER THE REBUTTAL\n\nMany thanks to the authors for revising the paper, the new material is comprehensive and does a lot to address my questions about the feasibility of scaling up the model. I would know the authors are short on space, but I would request that the current Figure 2 be tweaked in some manner to makes the legend in Figure 2c bigger. The current legend is nearly illegible, and the difference between the model trained on size 10 vs 16 is important. Independently of this, I have raised my score in light of these new additions.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "original and convincing work on learning interpolation schedules for adiabatic quantum computing",
            "review": "The authors consider the problem of providing optimal scheduling schemes for adiabatic quantum computing (AQC), i.e. with a way of interpolating / evolving a Hamiltonian from its initial form to its final form such that it is nwither too slow nor too rapid and thus can harness the speedup that AQC offers for solving combinatorial optimization problems (or, to be specific: QUBOs).\nIndeed, optimal scheduling of is a problem of theoretical as well of practtical concern as it makes or breaks the success of AQC but general, closed form solutions are hard to come by (or simply unknown at this point in time). \nAddressing drawback of previouslt proposed reinforcement learning approaches to this problem, the authors propose to train deep neural networks. To this end, they introduce a novel loss function loss that maximizes the fidelity based on an approximation of the success probability. In practical experiments, they find that schedules learned this way outperform those from previous approaches and, in addition, generalize from one type of problem to different instances of problems. Their experiments also reveal the approach to outperform previous.\n\nThis paper presents, original, convincing, and interesting work on a problem of considerable practical importance in adiabatic quantum computing. The idea of using neural networks in order to learn good (optimal) schedules for AQC is elegant and apparanetly leads to very good results. Moreover, to those with a background in AQC and deep learning, the paper is easy to read and clearly describes basic ideas and required technical details. ",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "interesting approach to learning annealing schedules, thorough investigation of the proposed models is lacking",
            "review": "**summary** \nthe paper proposes to learn parametric form of optimal quantum annealing schedule. Authors construct 2 versions of neural network parameterizations mapping problem data onto an optimal schedule. They train these networks on artifically generated sets of problem of different size and test final models on the Grover search problem as well as 3SAT. Experiments demonstrate improved performance in comparison to existing approaches.\n\n**pros**\n* the approach is straightforward to implement \n* experiments show the ability to do zero-shot generalization to different problem classes\n* empirically proposed models perform better then baselines\n\n\n**cons**\n* the mechanism of observed generalization to unseen problem instances is unclear. Training dataset size is chosen to be fixed 10000, it is unclear whether the learned model isn't just memorizing the inputs but rather providing useful generalization. Experiments with varying the dataset size say as 2^m could help clarify this. In addition to that providing training curves with train and validation errors as function of step would be also very useful.\n* The authors don't provide any intuition for why MSE objective performs worse then  success probability objective. Is it the case that training the latter is easier for some reason?\n\n\n**questions**\n* it is not clear how the architecture is varied for different model sizes. Since the input size changes with problem size, neural networks must vary at least in the input layer. Do they share the rest of the layers between different problem sizes?\n\n**comments**\n* it would be useful to add comparison b/w QUBO model and Hamiltonain model for the set of QUBO problems. Is it true that QUBO models performs better in this case?",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Review of \"Fidelity-based Deep Adiabatic Scheduling\"",
            "review": "I would like to thank the authors for the rebuttal. The added experiments have made this work better.\n\nWhile the collection of training data still seems hard (require exponential computation), I like the idea and the new experiments on generalizing from small training size to large ones. I still think the application domain of the proposed work is limited. The application to quantum computing is definitely interesting, but adiabatic scheduling is a more restricted domain and the contribution in this work is not applicable to gate-based quantum computers explored by Google, IBM, Rigetti, etc.\n\nBecause the revised manuscript is stronger now, I have modified my rating accordingly.\n\n##########################################################################\n\nSummary:\n\nThis paper concerns a specific application of machine learning in quantum computing: training classical machine learning models to predict the optimal schedule in adiabatic quantum computation. Adiabatic quantum computation depends greatly on the schedule to achieve good quantum speed-up. While in various cases, the optimal schedule is known. There will be instances where the optimal schedule is unknown, and the work proposes predicting those optimal schedules using neural networks. The main contribution comes from designing a new loss function for training such a model.\n\n##########################################################################\n\nReasons for score: \n\nI think the application is relatively narrow, even within the field of machine learning/quantum physics. The proposed neural architectures are very standard. The novelty comes from how to define a good loss function. The loss function is well-motivated and the empirical performances are convincing. However, the contribution may be too specific to be used in other machine learning/quantum physics applications, so I think most audiences attending ICLR may not be very interested in this result.\n \n##########################################################################Pros: \n\nPros:\n \n1. The proposed loss function is well-motivated.\n\n2. The proposed method works well on a wide range of tasks (QUBO, Grover search, 3-SAT).\n \n \n##########################################################################\n\nCons: \n\n 1. The application of the contribution of this work is relatively narrow (producing schedules for adiabatic quantum computation).\n\n\n2. The problem size tested by this work is small (6, 8, and 10). The QUBO problems that can be solved by current quantum devices (such as D-Wave machines) seems to be much larger (bigger than 1000).\n\n\n3. The proposed machine learning approach can not be used for larger n (e.g., n > 100) because the input to the classical neural network is a 2^n vector. If this scalability issue can not be addressed, then the proposed method would not be useful in practice (any form of quantum advantage would become vacuous if we need an exponential preprocessing time to find the schedule).\n\n \n##########################################################################\n\nQuestions during the rebuttal period: \n\n\n1. I have some concerns about the scalability of the proposed technique as discussed in the Cons section. The input to the neural network is a 2^n dimension vector. Do the authors have ideas and supporting evidence for how one could scale to a large system when n is like 50~100?\n\n2. Are there other non-ML alternatives to the linear schedule that people employ in practice? If the importance of scheduling is as claimed, I believe there should be extensive literature studying heuristics scheduling for QUBO and 3SAT. How does the proposed ML method compare to these existing heuristics?\n\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}