{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper introduces LambdaNetworks, a new method to capture long range interactions in data (such as global context in images). The method is novel and simple, and the experimental results are strong, especially in terms of speed/accuracy tradeoffs. The paper is well written and easy to follow. For these reasons, I recommend to accept the paper."
    },
    "Reviews": [
        {
            "title": "A promising new attention-based layer studied on vision tasks",
            "review": "This paper presents a local attention + relative positional encoding type of network suited for image classification and detection tasks. \n\nThe first focus of the approach is to make attention scale to (2D) images. Vanilla (global) self-attention with an input of size $n$ (e.g. n=224x224 resized ImageNet) and a context size of $m$ (m=n if global) costs $nm$ memory. They decompose their approximation in two parts, the content attention and the positional embedding (which requires global attention). For the (dense) content part, in the same vein as multiple \"linear attention\" approximations (e.g. Linformer, Wang et al. 2020) they make this attention $nk$ with $k$ independent of $m$ and much smaller. For the (relative, this translation equivariant) positional embedding, the space cost is still $nm$, but doesn't depend on the image, so this factorization into content + position is beneficial for larger batch sizes.\n\nAnother contribution of this paper is to study the convolutional variant, so called \"lambda convolutions\" (strictly local relative position embedding) by setting the weights of the convolution dynamically based on the relative positional embeddings, and which can effectively reuse optimized [T|G]PU convolution kernels\n\nThey also break down the query in \"multiquery lambdas\" (followed by concatenation) to reduce the computational cost (as in grouped convolutions).\nFinally, they construct LambdaResNets by hybridation with vanilla ResNets where they replace any (see Table 12 for full results) of the convolution layers by lambdas for a parameters/throughput/accuracy trade-off.\n\nThey perform experiments on ImageNet (classification) and COCO (detection, with Mask-RCNN), which show competitive results: beating ResNets and relevant variants on size and accuracy, but not speed. LambdaResNets also beat EfficientNets on speed-accuracy on ImageNet accross the board.\n\n\nSome limitations of the paper and/or method include:\n- (minor) The related work (which is only really included in Appendix) does not discuss Zhao et al. 2020 (which is in Table 3).\n- A lot of the good/important content is in the Appendix (e.g. Appendix E.1 / Table 8 showing that **the positional embedding is absolutely necessary for good performance while the content part is quite optional**; or experiments on the scope size from E.2 / Table 9).\n- Their model can be seen (and indeed that is how they propose to implement it) as a kind of ConvNet with dynamically computed filters. Still, there is hope to recover long(-er than context size) range attention with multiple layers (as in ConvNets) and this is not studied/discussed.\n- (minor) Speed is problematic in the current implementation (see Table 12 and 13 in Appendix), how much of it is due to [optimized kernels for ResNets vs. einsum implementaion] vs. necessary computational cost? There is a bit of a FLOPS comparison in Tables 6, 8, 9 and accompanying text, but practice (e.g. Table 4, 12, 13 throughput) vs. theoretical complexity is not discussed.\n- A small caveat is that the space cost vs. global (self-) attention is only really advantageous for large batch sizes.\n\n\nOverall, this is a good, readable, well studied (if one considers the appendix) paper on a promising new hybrid conv/attention layer that yields small and accurate models for computer vision core tasks (classification and detection). \n\n\nTypo:\n\"=This section\" in 3.2",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Explanation can be further improved.",
            "review": "This paper presents an efficient method to model long-range interaction. The proposed lambda layer removes the nonlinearity of the original attention operation and makes the matrix multiplication independent of the context, hence skipping expensive computation and storage of large attention maps. Two kinds of lambda functions in lambda layer, i.e., content lambda and position lambda,  allows the model to capture both dense content and long-range interaction.  In addition, the lambda layer can be further extended to working with local context and to being more efficient by docomposing a query into multiple short ones. Its effectivess has been demonstrated on extensive experiments on different backbone network architectures and tasks. Its speed-accuracy tradeoff perform very favorably against SOTA methods.\n\nHowever, there are still several issues to be addressed.\n1. This paper is not easy to follow. There are too many symbols and several of them are not explained. Besides, the organization of the paper can also be further improved.\n\n2. Some typos with the paper. E.g., In Table 1, tensor Q should have shape of nxkxu instead of mxkxu; lambda_n in eq. (2) should be transposed;\n\n3. In section 3.2, it is not clear how symbol u comes and what it means. \n\n4. Why are d and h removed from complexity analaysis in Table 4?\n\n5. Although authors explain its value to long sequences and high-resolution images, there is no experiment on the corresponding tasks such as long sequence language translation or high-resolution pixel-level prediction tasks.\n",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Official Blind Review #3",
            "review": "This paper proposes a novel lambda layer to capture long-range interactions by transforming available contexts into linear functions, termed lambdas and applying these linear functions to each input separately. The proposed Lambda Network achieves good performances on ImageNet Classification, COCO object detection and instance segmentation tasks. The proposed lambda convolution is much more dense than the attention-based layer thus reducing parameters and complexity. However there are still several weaknesses in this paper. 1) Generalization of the proposed lambda convolution layer. For example, how about the performance of the lambda layer when combined with the lighter convolutional networks, e.g. mobilenet ? How about the performance when much deeper networks for the highest performance?  2)The source code is suggested to be released for more details. 3) Check the typos in the paper. ",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "A good module but incremental technical novelty compared with self-attention.",
            "review": "This work proposes a lambda layer to capture long-range context. To address the issue of heavy memory-cost in self-attention, the proposed lambda layers transform context aggregation in self-attention into individual linear functions. The system-level performance is good on classification, detection and instance segmentation.\n\nStrengths:\n\n->Less memory and parameters, lower complexity than self-attention.\n\n->The performance is consistently good on classification, detection and instance segmentation.\n\n\nWeaknesses:\n\n->The main concern is technical novelty. Though interesting decomposition of query-context is introduced by linear functions, the overall design is still follow self-attention, the new part is incremental.\n\n->Some notations are missed in Table 1. Better to involve all the used notations in the Table.\n\n->The Lambda convolution performs local context, how about non-local context based on the proposed lambda function\n\n->What is SE in Table 7? The full name should be used before using its acronym.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "The ideas and results are interesting, but it lacks some important discussion and results.",
            "review": "Summary\n1. This paper present a new method, the lambda layer, for capturing long-term dependency.\n2. lambda layer summarizes the context into the fixed vector to reduce the computation burden\n3. This paper validates the performance on image classification and object detection tasks.\n\nStrengths\n1. The lambda layer is a simple and effective method.\n2. The idea of context summarization is interesting.\n3. Lambda layer shows the meaningful performance gain (including memory and time efficiency).\n\nWeaknesses\n1. This paper says that the lambda layer is a general framework, but the results only include vision tasks. Furthermore, there lacks discussion or results for the auto-regressive tasks. \n2. The comparison between the lambda layer and linear attention is not well discussed.\n\nQuestions and Additional Feedback\n1. Why Q=XW_{Q} is |m|\\times|k|\\times|u| sized tensor in table 1? \n2. The idea of reducing the complexity in the lambda layer seems similar to the “Transformers are rnns:” papers. This paper says that “We argue that such approaches may be overly restrictive and unnecessarily complex in trying to closely approximate an attention similarity kernel”. It is hard to agree with the sentence. What is the reason behind the sentence? Are there any empirical or theoretical results?\n3. This paper says that the lambda layer is “a general framework” and “versatile”. However, all of the experiments are related to the vision tasks. Are there results that related to natural language or time-series?\n4. I wonder how lambda layers work for auto-regressive tasks. There is some description of auto-regressive training in the appendix, but it is not enough.\n5. What is the advantage of lambda layers over attention? According to this paper, the only shortage of attention is an expensive computation. However, there are several linear time attention research. I suggest that this paper includes additional qualitative and quantitative analysis to compare the attention and linear time attention.\n\nTypos\n1. signifcantly => significantly\n2. as the the queries => as the queries\n3. formluations => formulations\n\nComments after the rebuttal\nThe author's response resolves my concern in part, and I will keep my positive score.",
            "rating": "6: Marginally above acceptance threshold",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}