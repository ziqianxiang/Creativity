{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Oral)",
        "comment": "A novel second order nonlinear oscillator RNN architecture is proposed, analyzed, and evaluated in this paper. The results are solid and impactful. Authors and expert reviewers showed exemplary interactions with each other, improving the manuscript in significant ways. All four reviewers overwhelmingly recommended accept. I recommend that this paper be selected as an oral presentation."
    },
    "Reviews": [
        {
            "title": "Prevention of exploding/vanishing gradients in novel RNN architecture CorNN",
            "review": "The paper proposes a novel RNN architecture (CorNN) to tackle the infamous problem of vanishing and exploding gradients in RNNs. The novel CorNN architecture is based on time-discretized forced coupled damped nonlinear oscillators. For the gradient norm of CorNN analytical lower and upper bounds are calculated implying that CorNN avoids vanishing and exploding gradients.\nThis is accompanied by numerical results (including code) that demonstrate the improved trainability of CorNN in permuted sequential MNIST, and adding task and noise-padded CIFAR10 compared to some other RNN architectures (GRU, LSTM, antisymmetricRNN, IMDB sentiment analysis, and a human activity recognition task.\n\nIn summary, the paper proposes a useful and mathematically transparent way of tackling the challenge of training RNN on tasks with long-time dependencies. \n\nWeak points of the paper:\n* In the mathematical analysis the paper claims to \"rigorously prove precise bounds on [...] gradients, enabling the solution of the exploding and vanishing gradient problem\". If I am not misunderstanding, the mathematical bounds are actually only shown for the initial gradient norm (plus some number of steps afterward in section C4 of the Supplementary Material).\n\n* A more in-depth comparison of numerical gradient norms and their respective upper/lower analytical bounds would be desirable, as the lower bound on the gradient in (16) is only given in as O(∆t^(3/2)) without any prefactors. \n\n* While the numerical results demonstrate improved trainability and performance on a number of tasks, the underlying reasons remain mostly unclear. Supplementary Material B gives some heuristics on how a superposition of forced coupled damped nonlinear oscillator can generate complex output, but it doesn't explain the avoidance of exploding and vanishing gradients and the superiority to other solutions (e.g. gated units like LSTM or GRU).\n\n* Missing: What are the limitations of CorNN, when would you expect them to fail?\n\n* The link to biological networks is not yet very convincing. While there exist without doubt many oscillations on different scales in the brain, it is not clear how the insights gained here could be applied to the brain.\n\nSome smaller comments:\n* How could expressivity be quantified?\n* figure 1: Plotting MSE with a logarithmic or semilogarithmic axis would help to distinguish small errors.\n* figure 3: It would help to also plot this for other tasks.\n* figure 3 lines for other tasks would be helpful",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Review",
            "review": "The paper introduces a novel recurrent neural network architecture which approximately preserves the norm of the gradient irrespective of the number of unroll steps. This complements a rapidly growing line of research that aims to better understand dynamical properties of RNNs and their gradients, thus potentially enabling training of models that capture long term dependencies while avoiding exploding and vanishing gradients.\n\nThe submission has the following strengths to it:\n\n1. It offers a clear and succinct proof of the gradient stability as well as the stability of the forward dynamics.\n1. It provides convincing experiments on relevant datasets, all while showing competitive results.\n1. It's exceedingly well written, and readily understandable even without prior knowledge.\n\n##### I firmly believe that based on those, it should be accepted. It has all the hallmarks of a good, paper with potentially wide application.\n\nThat being said, I do have some minor objections:\n\n1. When presenting Eq. (1), the 'intuitive' interpretation of $\\gamma, \\epsilon$ should given right away, rather than deferred to later sections of the paper, especially the appendix.\n1. The motivation for the use of non-linear oscillators is well-written but perhaps should be de-emphasized. I would like to put-forward the following argument for it. It appears that the choice of the dynamics only constitutes half of the 'puzzle'. \nThe choice of IMEX is mentioned *en passant*, but seems to be rather crucial to obtaining the theoretical guarantees viz. Proposition 3.2 and 3.3. I did not have time to check the derivation against other schemes, but I presume that the choice of IMEX was highly non-trivial in designing the new architecture. If that is indeed the case, then the role of the solver ought to be emphasized.\n1. I appreciate the authors comments regarding the expressivity of the proposed architecture, as well as the demonstrations in Appendix B.\nHowever, I would also appreciate a simple example of the kind of dynamics where the coRNN cell ought to break down -- given the corollary of Proposition 3.1, it would be interesting to show the potential break-down of the network on task that involves approximating a chaotic dynamical systems. In particular, it would be very interesting how coRNN fares against similarly sized gated RNN (LSTM or GRU).\n1. For my own understanding, is the exponent $r$ in Eq. 8 there only to conveniently related $\\eta$ to $\\Delta t$? If not, does it admit some more intuitive interpretation?\n1. Since, the proposed architecture requires a sufficiently small step-size, are the resultant equivalent (in some suitable sense be it topologically, having the same invariant set) to the continuous time dynamics?\n1. Lastly, the authors report the hidden unit dimensionality, but from the main text it is entire unclear whether that's the dimensionality of $y$ or $ \\left[y^\\top, z^\\top \\right]^\\top$. Having looked at the code, it appears to be the former.\n\n#### Edit:\nOut of curiosity I ran the submitted code for the permuted sequential MNIST task, and noticed that the following:\n1. The numbers that the authors report in the paper seem to be result of a single network realization. While granted, this is somewhat consistent with practices common in the community, it makes one question how representative they are of different initial seeds. In the current setting it's hard to disambiguate whether the random seed was chosen coincidentally or rather specifically because of the purported state-of-the-art outcome.\n1. For this reason I suggest the authors compute additional iterates of the model and report some distributional information about the best loss/accuracy for all the tasks covered in the submission.\n1. Attaining \"state-of-the-art\" results is notable, but is by no means pre-requisite for this to be considered a good submission.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "A dynamial systems inspired CT-RNN that mitigates exploding/vanishing gradients",
            "review": "This paper proposes a new continuous-time formulation for modeling recurrent units. The particular form of the recurrent unit is motivated by a system of coupled oscillators. These systems are well studied and widely used in the physical, engineering and biological sciences. Establishing this connection has the potential to motivate interesting future works. The performance of the proposed recurrent unit is state of the art.\n\n\nReasons for my score: Overall, I vote for marginally above acceptance threshold. I like very much the proposed approach for modeling recurrent units. Further, the presented results are intriguing, and the paper is well written. However, I have some concerns (see below). I am happy to increase my score if the authors can address my concerns in the rebuttal period. \n\n\nPros: \n-------\n+ Second-order systems of ODEs seem to be a promising approach for modeling recurrent units, and this approach has not received much attention for this task before. Indeed, this paper impressively demonstrates that a unit motivated by a system of coupled oscillators is able to achieve state of the art performance on a range of benchmark tasks.\n\n+ The analysis shows that the particular form of the proposed continuous-time unit mitigates the vanishing and exploding gradients problem by design, which is very appealing. The analysis is mathematically sound.\n\n+ Code is provided!\n\n\nCons: \n-------\n- In Eq. (3) the authors advocate the IMEX scheme to obtain a discretization of (2). I was very curious to see how the authors implement this scheme in practice, however, the provided implementation revealed that the authors use an explicit scheme in practice. Please, comment why you chose the IMEX scheme here. Does the analysis also hold if you use an explicit discretization in (3), and if, why do you mask the fact that you are using an explicit scheme in practice. I feel, it would be relevant to discuss how you train the unit in practice.   \n\n- Section 3 is no pleasure to read. It is not clear to me what the value of the sketch of the proofs are, since the proofs are pretty standard. Instead, the space could be better used for an extended qualitative discussion of the analysis and the nice properties of the proposed recurrent unit. For instance, you can extend the discussion around proposition 3.1 and provide some context on why you want to rule out chaotic behavior (this might not be obvious for everyone); further it would be nice to see a better discussion on the effect of dampening and forcing on the performance of the recurrent unit. Also, I would like to suggest to move parts of Appendix B into the main text, since this discussion actually helps to build some intuition for the proposed unit.\n\n- The extremely good performance on the sMNIST task is slightly surprising, since my intuition would not suggest that the particular form of the unit has the ability to substantially improve the expressivity as compared to some other recently proposed units. I used the provided code to evaluate the coRNN (N=256 and 128) on the sMNIST task and the highest accuracy that I was able to obtain (out of 8 runs on 4 different GPUs) was 99.2% on the test set. These results are still very good, but they do not match the reported results. (Note, that the code is printing out the accuracy for a smaller validation set which indicates a higher accuracy than is actually obtained on the test set.) This said, I would like to ask the authors to double check the experiments on sMNIST. (Also, I assume that the model can be trained in less time if the learning rate is decayed much earlier, e.g., around epoch 30 and a second time around epoch 60.)\n\n- It is not clear to me how sensitive the RNN is to the particular choices of \\gamma and \\epsilon. It would be good to provide some form of ablation study that studies how the performance varies for different values of \\gamma (and \\epsilon) while keeping all other tuning parameters fixed (I assume that you have all these results handy since you have performed an extensive hyperparamter search). This would help to gain some better intuition for how difficult it is to tune the proposed unit. In other words, I would like to see how sharp the performance drop is if you perturb the tuning parameters slightly (i.e., plot the test accuracy as a function of \\gamma and \\epsilon).\n\n\nMinor comments: \n-------\n* Given additional space, it would be nice to see an extended related work section.\n\n* It would be nice to so results for a language modeling task.\n\n* In Table 2, the citation for the Fast RNN is incorrect.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "This paper proposes the coupled oscillatory recurrent neural networks (coRNN) for solving the exploding and vanishing gradient problem by proving precise bounds on the gradients of the hidden states. ",
            "review": "Firstly, this paper conducts the rigorous analysis of the coRNN via the formula deduction to verify the bound. Then the coRNN is proved to mitigate the exploding and vanishing gradient problem and this is also validated in a series of experiments. Also, the performance of the coRNN is comparable or better compared to state-of-the-art models. This paper provides a new idea to address the exploding and vanishing gradient problem, which hinders the development of deeper neural networks tremendously. In my opinion, this coRNN model is meaningful for practical application, especially for the extension of more complicated neural networks. \n\nBesides, for the biomedical signals with high temporal resolution (e.g., electroencephalogram, electromyogram), the coRNN model can be a good alternative in future work. Furthermore, the efficiency of the proposed model also be proved by the mathematic formulation and experiments ranging from pure synthetic tasks designed to learn long-term dependencies to more realistic tasks rigorously. Considering the whole structure of this paper, I argue that the clarity is clear and logical. Different from the recently published literature, this paper has explicit use of networks of oscillators with the underlying biological motivation, so this paper expresses the originality in some extent. To sum up, the quality of this paper is suitable for the publication in ICLR2021. \n\nThere are two main pros in this paper: \n1.\tThe theoretical verification is clear and rigorous, readers can easily catch good understanding of the bounds this paper proves following the formula deduction. Specifically, this paper demonstrates how to avoid the exploding and vanishing gradient problem for the RNN in theory.  \n2.\tThe experiments are quite abundant, experimental results show that the coRNN can not only avoid the exploding and vanishing gradient problem, but also achieve better performance with fewer parameters compared to recent studies.  \n\nBut some cons should also be noticed. Firstly, the illustration of proposed coRNN should be presented in the paper, which is more comprehensible. Secondly, the related work part, when mentioning the similar works, it will be better to describe the main differences and correction with this paper more specifically. Lastly, in the part of discussion, the practical significance of proposed coRNN should be emphasized with more words.\n",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        }
    ]
}