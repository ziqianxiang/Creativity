{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Oral)",
        "comment": "+ Interesting method for binaural synthesis from moving mono-audio\n+ Nice insight into why l2 isn't the best loss for binaural reconstructions. \n+ Interesting architectural choice with nice results.\n+ Nicely motivated and clearly presented idea -- especially after addressing the reviewers comments.\n\nI agree with the idea of a title change. While I think its implied that the source is probably single source, making it explicit would make it clearer for those not working in a closely related topic. Hence, \"Neural Synthesis of Binaural Speech from Mono Audio\" as suggested in the review process sounds quite reasonable.\n"
    },
    "Reviews": [
        {
            "title": "Review for \"Neural synthesis of binaural audio\"",
            "review": "The paper is about a method for synthesizing binaural audio from a mono recording of a single speaker's speech.\n\nFirst, I think the title is too general. The paper does not attempt to convert all possible sounds, but it tries to convert a single speaker's monaural speech signal to binaural audio where the speaker is moving. I think this inherent assumption is important since the method will probably not work for multiple overlapping audio sources. I suggest changing the title to \"Neural synthesis of binaural speech of a single moving speaker.\"\n\nThe first part of the network \"neural time warping\" is an interesting component that is capable of adjusting the delays conditioned on the location and orientations of the source and microphone such that a location dependent binaural audio is formed by estimating time-varying delays of the original mono recording separately for two channels. It is believable that such a module would be helpful for a single moving speaker. However, such a model would not help or work when there are more than two active audio sources. A separation module would be required for that scenario. Neural time warping is an autoregressive model which can work online. \n\nThe second stage convolutional network which uses conditioned hyper convolutions is also an interesting architecture that takes the warped signals and applies time-convolutions with kernels obtained from the conditioning input which has the time-varying locations and orientations of the source and the microphone.\n\nThe section about the loss function is also interesting in that, the time domain l2 loss is shown to not work well for accurate phase estimation, so the authors propose to add a separate phase loss term to compensate for that. I think it would be better if Figure 2 is replaced with a plot of epsilon/|yhat| versus amplitude error divided by |yhat| in (a) and versus the phase error in (b). It could be clearer than the current 2D figure which is hard to interpret.\n\nThe use of \"sine activation\" is not well justified. \"sine\" activation is useful in the first layer of a \"signal representation network\" which is different from a signal prediction network. I do not see how and why that could be helpful here.\n\nIn terms of comparisons, 2.5D method uses visual information as conditional information to generate complex masks to produce binaural audio. In this paper, visual information is replaced with the spatial conditioning information. It would help to get more information about the window size and hop size used in 2.5D since they may be an important factor that relates to the amount of delays they can introduce.  For wavenet comparison, it was not clear how the wavenet model was trained to generate binaural data. Did it use any conditioning information? If so , how? Was it applied in an auto-regressive way with randomized sampling? The wavenet audio example sounded noisy which is not typical of wavenet generated audio. It looks like the DSP method can utilize a listener specific HRTF which may be difficult to incorporate for the proposed neural model. Is it an important issue?\n\nHow does the model generalize to unseen speakers and rooms? The training and testing strategy uses the same room and the same speaker(s).  Would we have any problem when the monaural audio is recorded in some other room with some other speaker?\n\nIn Figure 8, maybe it is OK not to draw the original binaural signal for every method.\n\nIn general, I liked the neural warping and conditional convolutional components which are interesting and I liked the analysis about the loss function. The approach is an interesting way to obtain binaural version of a monaural mono speaker recording in a room. The dataset produced for the paper would also be useful for research.\n\n**Update after revision**\n\nThe revision improved the paper. Thanks for taking care of my comments.\n\nJustification of sine activations, generalization to unseen speakers experiment are nice additions.\n\nThe new title is a bit better and I think it may be OK since the goal is to perform a moving source simulation for single speech sources. Multiple speech sources can be simulated separately and added together as mentioned. \nThe authors may consider possibly a better name: \"Neural binaural synthesis from mono speech\" which emphasizes that the synthesized target is \"binaural speech\" from a single speech recording.\n\nJust a few more points.\n\n1. I think it is essential in wavenet to apply the model in an auto-regressive fashion over samples. Just using the network architecture and the loss function from wavenet is not equivalent to \"using a wavenet model\" since an essential part of the model is the autoregressive sampling which makes sure the samples are dependent and coherent. Without auto-regressive sampling, the resulting sound is poor as observed by the authors. So, I suggest to emphasize that \"autoregressive sampling\" is not performed in the paper to avoid misleading the readers.\n\n2. More explanation of 2.5D is appropriate. One wonders if using a larger STFT window size would improve its results.",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "An interesting paper with novel ideas and strong results",
            "review": "Strengths:\n1. The paper is well written. It includes clear math notations and figures. Readers can easily follow the thought process of the authors. For example, Figure 2 shows the relation of l2 loss and phase loss with respect to target energy, indicating the importance of penalizing phase loss in the end to end system. The same observation is supported by Figure 3.\n\n2. Strong results. The proposed end2end model significantly outperforms previous SOTA in terms of objective measures and subject tests. The video demo is very convincing. The model improved spatialization and sound quality.\n\n3. High novelty. This paper proposes to impose monotonicity and causality to the learned warping function, which incorporates the physics of sound propagation. I am excited to another example of applying domain knowledge to an end-to-end model. The model includes two novel components: the neural warp network compensates the errors from geometry warp, the temporal convolution works as a post processing module to account for reverberation and other effects. Ablation study shows both components are critical.\n \n\nTo be improved:\n1. The caption for Figure 4(a) seems to be incomplete.\n\n2. It would be good to include a table to compare the proposed model with baselines in terms of model size and inference speed.",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        },
        {
            "title": "Model, data, and evaluations for binaural audio generation from single-channel audio",
            "review": "This paper presents a neural network-based model to generate binaural audio given a single-channel audio and positions of source/listener & their angles.  The authors developed a dataset of binaural audio, which will be released on acceptance.\n\nTechnical details and model architecture are available in the body of the paper, whereas additional details such as baseline DSP-based approach, proof, and dataset are available in the appendix. The model was evaluated using the dataset developed in this work.  A demo video demonstrating the capability of the model is also provided as a supplementary material.\n\nThere are a few parts need to be addressed. (1) it is unclear why DTW-based warping is required.  IIRC the warpfield here can represent not only a shift but also other monotonic & causal such as repeating.  If there is only delay between left and right, just having a shift is enough isn't it?  It is great if the authors can explain the motivation to use warpfield more clearly.  (2) The use of hyperconvolution is an interesting idea.  The equation 5 uses conditional temporal convolution.  However, audio generative models such as WaveNet uses a different architecture; gated convolution.  The gating mechanism can give additional non-linearity and so I'm wondering if you can evaluate the performance of hyperconvolution against gated convolution.  (3) too large confidence intervals in Table 4.  Although there were many evaluations, the confidence intervals were pretty large and there were overlaps among them (e.g., small overlap between DSP and \"ours\" in cleanliness, large overlaps between spatialization and realism between DSP and ours).  With this result it is difficult to claim that there was a significant improvement over the baseline system.  Please check your results and design the experiment more carefully to figure out whether there is any significant difference between them. \n Conducting side-by-side comparision is one possiblity.\n\nComments:\n-  This paper claims that it works in real time but no information about speed such as real-time factor & hardware specification are provided.\n- Sampling rate information is not explicitly provided in the experiment section.\n- 0.6 MOS difference is large, not \"a bit\".\n- Modern WaveNet models often use mixture-of-logistics (refer to Parallel WaveNet paper for details) as output rather than mu-law to achieve better quality. ",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        }
    ]
}