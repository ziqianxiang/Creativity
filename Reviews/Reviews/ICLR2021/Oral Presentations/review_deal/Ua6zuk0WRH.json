{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Oral)",
        "comment": "This is a solid paper that proposes a new method for approximating softmax attention in transformer architectures that scales linearly with the size of the sequence. Even though linear architectures have been proposed before using a similar idea (Katharopoulos et al 2020), this paper provides a better solution along with theoretical analysis and makes a rigorous empirical comparison against other methods. All reviewers agree that this is a strong paper that should be accepted. I suggest citing the recent paper https://arxiv.org/abs/2011.04006 (Long Range Arena, mentioned in the discussion) which provides further comparisons on long-range benchmarks, including the method presented in this paper and Katharopoulos et al 2020, along with a detailed discussion of the differences between the two methods."
    },
    "Reviews": [
        {
            "title": "Another step forward towards efficient transformers",
            "review": "This paper proposes a set of techniques called Fast Attention Via positive Orthogonal Random features (FAVOR+) to approximate softmax self attention in Transformers and achieve better space and time complexity when the sequence length is much higher than feature dimensions ($L\\gg d$). The resulting architecture, Performers, is provably and practically accurate in estimating regular full-rank attention without relying on any priors such as sparsity or low-rankness. It can also be applied to efficiently model other kernalizable attention mechanisms beyond softmax, achieving better empirical results than regular Transformers on some datasets with such strong representation power. Performers are tested on a rich set of tasks including pixel-prediction, language modeling and protein sequence modeling, and demonstrated competitive results with other examined efficient sparse and dense attention models. I think this paper is a solid step towards more efficient Transformers for practical long-sequence data, and should be accepted. Below I raise two potential questions and look forward to the solutions in the future. \n\n1. For Performers, speedup can only be achieved when $L\\gg d$, which means Performers might have less advantages on wider Transformers. The time complexity of Performers for approximating the attention matrix is $O(Ld^2\\log(d))$, while a regular Transformer has a time complexity of $O(L^2d)$ to compute the attention matrix. From Figure 3, it looks like for a model similar to BERT-Base, Performers is only faster when the sequence length is larger than 512. I wonder how wide Performers can be while  preserving faster speed than Transformers with the same size on the current datasets, and how their test scores compare. \n\n2. It looks like Performers do not converge as fast as Transformers on larger language modeling datasets like PG-19. I wonder whether similar phenomenon will happen in other domains when the dataset is larger, and feel quite curious about the cause.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "A solid paper that will likely affect the way Transformers are estimated",
            "review": "This is a solid paper that presents a computationally less expensive, unbiased, low-variance estimator of the Transformer architecture. \n\n**Strengths**:\n1. The authors provide mathematical guarantees for the suggested estimator.\n2. The estimator (called FAVOR+) seems a more scalable replacement for regular attention.\n3. Theory is empirically verified in a variety of experimental settings.\n\n**Question**: \nIn Introduction, you mention that your method does not rely on low-rankness, but doesn't your equation (4) as well as Fig. 1 assume that the attention matrix $\\mathbf{A}$ allows low-rank decomposition? (I suppose $r<d$)\n\n**Suggestion**:\nWill it be possible to compare a pre-trained Performer-based encoder (like BERT) against its competitors (Linformer, Reformer) on a [GLUE benchmark](https://openreview.net/pdf?id=rJ4km2R5t7) or on [probing tasks](https://openreview.net/forum?id=SJzSgnRcKX)? I see that you compare them (Per-/Lin-/Re-former) against the original Transformer on the pre-training task itself (Fig. 5), but I am wondering how well the pre-training performance transfers to downstream tasks in each case.\n\n**Minor issues**:\n1. In the proof of Lemma 1 you first prove that $\\exp(\\left\\lVert\\mathbf{x}+\\mathbf{y}\\right\\rVert^2/2)=\\mathbb{E}_{\\omega\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})}[\\exp(\\omega^\\top\\mathbf{x})\\cdot\\exp(\\omega^\\top\\mathbf{y})]$, however this is simply an m.g.f. of $\\omega$ at $\\mathbf{x}+\\mathbf{y}$, and thus IMHO does not need to be redirived. (I assume that the formula for the m.g.f. of a multivariate Gaussian random vector is well-known).\n2. Similarly, eq. (16) is the m.g.f. of $\\omega\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$, it does not have to \"immediately follow from\" some other fact.\n\n**Typos**:\n1. In eq. (18), a norm is missing in the last term: $\\mathbf{z}^2$ -> $\\left\\lVert\\mathbf{z}\\right\\rVert^2$.\n2. In the text after eq. (18): inequality -> equality\n\n**Update after the author's response**: The authors have answered my questions during the rebuttal period and I am satisfied with the response. Hence updating my score: 7 $\\to$ 8.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Theoretically grounded O(N) approximation of the softmax attention",
            "review": "##########################################################################\n\nSummary:\n \n\nThe paper proposed a theoretically grounded O(N) approximation of the softmax attention. The key idea is to interpret attention as a kernel function and construct the random feature projection that can reproduce this kernel. It is highly non-trivial to derive a feature mapping that can accurately approximate the softmax kernel. To better approximate the softmax kernel, the author proposed some important design choices, all of which are supported by theoretical and empirical evidences. The author showed that 1) adopting non-negative random features is very essential to the approximation and the proposed Positive Random Features (PRF) can effectively reduce the variance when the attention values are small, 2) drawing orthogonal random matrices can further reduce the variance of the approximation, 3) the final proposed Performer model runs faster, takes less memory, and has better performance than other O(N) and O(N logN) attention methods.\n\n\n##########################################################################\n\nReasons for score: \n \n\nThe paper is very well-written and should be accepted. This is an important landmark in the research about O(N) attention. The design of the random feature mapping is reasonable and theoretical analysis is convincing. Experiments show that Performer is better than the other O(N) attention methods and also other efficient attention methods.\n \n\n##########################################################################\n\nPros: \n \n\n1. The paper gives a provable O(N) approximation of the softmax attention. The method works without assumptions on the structure of the attention map (like sparsity). The theoretical proofs provide good insights on how to design a good O(N)-complexity approximation to the  attention mechanism.\n\n2. Apart from approximating the softmax attention, the proposed FAVOR+ method can be utilized to approximate other attention kernels. In fact, the author has experimented with Performer-ReLU, which outperforms Performer in some experiments. This provides the insight that softmax attention may not be the best choice.\n\n3. The author conducted very comprehensive ablation studies on different components of the proposed method. This includes: 1) effectiveness of using the positive features, 2) drawing orthogonal random samples, 3) redrawing the random samples\n \n\n##########################################################################\n\nCons: \n \n\n1. From Figure 5 and also Figure 15, periodic redrawing is quite essential. However, the author has not mentioned about the implementation details on how they redraw the random samples and how to choose the period. For me, I feel that this hyper-parameter should be important because the model may need to ensure that each group of samples has been trained for a sufficient amount of time.\n\n2. Performer-ReLU has replaced the attention kernel and can sometimes be better than the softmax attention. Thus, I feel the author may also want to compare with the linear attention method in ((Katharopoulos et al., 2020) \"Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention\".\n\n\n##########################################################################\n\nQuestions during rebuttal period: \n \n\nPlease address and clarify the cons above \n \n\n#########################################################################\n\nTypos: \n\n(1) Page 5, after \"than those from SM_{2m}(x, y)\", there is an additional right bracket.",
            "rating": "8: Top 50% of accepted papers, clear accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Great theoretical and experimental results but missing time equalized comparisons with simple baselines ",
            "review": "### Summary\n\nThe authors propose to use the kernel feature map self-attention formulation introduced in [1] to efficiently approximate the softmax attention. The main contribution of the paper lies in the proposed _positive random features_ that can approximate softmax with a strictly positive feature map without which the training is unstable. The authors also show that an approximation of softmax is not necessary for good performance and actually use ReLU random features to achieve their best results when training from scratch.\n\n### Strengths\n\n- The paper deals with a very pressing and important issue, that of the scalability of self-attention.\n- The positive random features are also useful outside of the context of self-attention for efficiently approximating softmax.\n- The experimental results provide strong evidence about the performance of training transformers with kernel feature-maps.\n\n### Weaknesses\n\nThe biggest weakness of the paper in my opinion is the lack of comparison with a simple feature-map as proposed in [1]. Since the authors also use the ReLU random features, we establish that approximating softmax is not necessary for good performance.\n\n1. What would the performance be if a simple deterministic feature map was used?\n2. What would it be if the computational cost was equalized either by adding more layers or by increasing the dimensionality of the queries and keys?\n\nThe second weakness of the paper concerns the evaluation of the practical softmax approximation capabilities. I find the theoretical results interesting and important but I would like more experimental evidence. Without fine-tuning, the authors provide evidence that the approximation does not work (Fig 5).\n\n1. What would happen, for instance, in a toy task where the Lipschitz constant of the transformer layers was kept low? How big would the feature map need to be in order for the approximation to work in such a simple case?\n2. What is being approximated in Fig 4? Is it a randomly initialized attention? What is the rank of the attention matrix?\n3. How good would the approximation be for an attention matrix that is almost full rank and how many features would we need then?\n\n### Reasons for my recommendation\n\nI am recommending acceptance because I believe that the positive random features is an important contribution both for transformers and for kernel approximation. In addition, the experimental results are impressive and show that fast kernelized attention indeed works in practice. My only reservation for a higher score is, as mentioned in the weaknesses section, the lack of comparison with simpler feature maps under equalized computation time.\n\n[1]: Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention",
            "rating": "7: Good paper, accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}