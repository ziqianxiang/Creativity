{
    "Decision": {
        "title": "Final Decision",
        "decision": "Accept (Oral)",
        "comment": "This paper proposed using the state bisimulation metric to learn invariant representations for reinforcement learning.  The method is generic, effective, and is supported by both theoretical and experimental results.  All reviewers and I think this is a strong contribution to the area."
    },
    "Reviews": [
        {
            "title": "The paper is well written and presents a clear approach to a well motivated problem with strong evaluation results. ",
            "review": "The authors propose an approach to robust representation learning of observations for reinforcement learning by training a model to align the euclidean distance between two observations with bisimulation metrics that quantify how similar the states that generated the observations are in terms of the control problem.  This reduces the effect of irrelevant features in the observations on the representations.\n\nThe paper is well written, the problem is clearly motivated and the approach and technical contribution is easy to follow.\n\nThe approach to use the state bisimulation metric to supervise observation representation is intuitive and clearly motivated.  Theoretical analysis is provided with generalization guarantees.\n\n\"As an example, in the context of autonomous driving, an intervention can be a change in weather, or a\nchange from day to night which affects the observation space but not the dynamics or reward.\"  I do not agree with the example as weather can directly alter the dynamics and desired behavior of an AV system.  The point in this paragraph is still clear but I would suggest a different example.\n\nThe evaluations are strong and run on a number of different experiment settings against multiple strong SOTA models.\n\nIn Figure 4 the proposed approach is outperformed by contrastive learning in the default setting.  I understand that the goal is to learn robust representations for the natural setting, but can the authors comment on why it fails to beat the contrastive approach here and provide insight on how this may be addressed.  Some problems have only a few distractors and may fall between the natural and the default setting.\n\nrecommendation and reasoning\n\nThe paper is well written and presents a clear approach to a well motivated problem with strong evaluation results.  I recommend acceptance.",
            "rating": "7: Good paper, accept",
            "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"
        },
        {
            "title": "Promising approach to learn task relevant representations for control",
            "review": "**Summary**\nThe paper focuses on how learning state-representations that encode information relevant to the task can improve reinforcement learning from pixels. Often, observations in an MDP can contain information that are irrelevant (“distractors”) to the task at hand and can likely “distract” the downstream RL algorithm used. Unlike existing reconstruction based approaches (which don’t explicitly incentivize ignoring task-irrelevant information), the authors propose Deep Bisimulation Control (DBC) that relies on bi-simulation metrics (as the task-aware criterion) that encode behavioral similarity b/w states with respect to the reward structure. Instead of explicitly learning a bi-similarity distance function, authors enforce the representations which under L1 distances correspond to bi-simulation metrics. DBC demonstrates learning these representations in conjunction with the control policy, reward and a dynamics model. Furthermore, the authors highlight connections to causal inference which can hopefully further provide insights into which “new” reward structures can the learned representations generalize to (since bi-similarity metrics themselves are heavily dependent on the reward structure). Results obtained by the authors demonstrate that DBC can learn task specific representations and a control policy in a robust manner in the presence of distractors on the Deepmind Control Suite and the CARLA simulator. Additionally, the authors also demonstrate how DBC can generalize to new reward functions on Mujoco.\n\n**Strengths**\n\n- The paper is generally well-written and easy to follow for the most part. The authors do a good job of walking the reader through the preliminaries, highlighting distinctions with prior usage of bisimilarity metrics and how DBC ties in with slight modifications to the Soft Actor-Critic algorithm.\n- The results obtained on DM control suite demonstrate that while DBC is competitive or slightly worse compared to other approaches in the absence of distractors, it performs better compared the set of baselines when distractors are introduced — significantly outperforming the reconstruction and the contrastive approaches. Furthermore, compared to a VAE, DBC places observations that are similar in terms of task information closer in the learnt embedding space.\n- The authors also show that compared to an adaptation of prior usage of bisimulation metrics, DBC is more sample efficient. Additionally, within the considered family of reward functions to generalize to “walker_run” and “walker_stand”, DBC shows stronger generalization compared to DeepMDP. Furthermore, learned embeddings on CARLA demonstrate qualitatively that DBC learns to group behaviorally similar distractor states (manifesting as obstacles).\n\n**Weaknesses**\nI don’t have any major weaknesses to point out. I will highlight minor comments / weaknesses which, I think if addressed, would definitely make the paper stronger.\n- While the authors state that DBC in practice can be combined with any model-free of model-based algorithm, the paper would definitely benefit if this claim was backed up with results demonstrating DBC combined with another model-free or model-based algorithm since it might be unclear off-the-shelf if an algorithm requires major or minor changes to work in conjunction with DBC.\n- Task generalization (or generalizing over different reward functions) relies on the assumption that the new reward function depends on contributing factors that likely influence the earlier “seen” reward function as well. While empirically validating this claim from walker_walk -> walker_run / walker_stand is a reasonable starting point, demonstrating another instance where inferring the source and target reward functions is not immediately obvious would definitely benefit the paper.",
            "rating": "7: Good paper, accept",
            "confidence": "3: The reviewer is fairly confident that the evaluation is correct"
        },
        {
            "title": "Deep twin network loss implementation of bisimulation-based state abstraction is robust to extraneous details in realistic visual environments ",
            "review": "# Paper Summary \n\nThe paper presents a new method for embedding visual images into a state space suitable for effective control by an actor-critic style RL algorithm. They show how a previously explored idea of using a bisimulation between state abstractions and reward sequences to group states that are similar from a decision theoretic perspective can be extended to a continuous deep embedded representation using twin network style learning through standard gradient descent optimization. They call the approach Deep Bisimulation for Control (DBC).  The paper also argues for the correctness of their approach using a contraction proof and a theoretical argument for generalization to new problem domains. The approach contrasts directly with algorithms that use an autoencoder to find a compact representation by reconstructing input frames or predicting future input frames. These approaches necessarily represent enough information to reconstruct both task relevant and incidental details. Experimentally, the paper shows that performance of reconstruction-based approaches degrades by a large and significant amount when extraneous background detail is present in image frames, while the proposed method is immune. The evaluation is done on widely respected benchmark of Deep Mind MuJoCo based articulated figure simulations and a CARLA realistic image simulation of car driving which shows that focusing on task specific detail is important on realistic tasks. \n\n# Pros and Cons \n\nThe work addresses a key problem in reinforcement learning, which is learning effective policies from unlabeled visual images.  The ability to find compact, task relevant representations of high-dimensional inputs is central to the ICLR community. \n\nThe paper covers relevant background in state abstraction in RL and it is clear how it relates to the paper's contribution.  \n\nThe paper clearly explains the background of bisimulation and how the idea of grouping states according to their probable reward sequences can be practically realized by grouping states by their immediate reward and future state distributions. It is also clear in principle that a twin network can be used to induce a latent space with these properties.  \n\nThe paper shows that the policy will converge and that the error will be bounded and that the representation will generalize to any problem domain in which causal dependencies are a subset of the problem domain the representation was trained on. Which is nice.  \n\nThere is a nice evaluation on both MuJoCo articulated figure problems from the Deep Mind Control suite as well as the CARLA simulated driving application that uses large realistically rendered images. Comparison against state-of-the-art RL algorthims makes results convincing. I think the CARLA example is nice as it shows that practical problems without artificial augmentations have the property that the input details can be distracting if fully reconstructed. The superiority of DBC in this case really makes this point well. \n\nThe paper does not report computational load associated with their approach. Presumably due to the closed form Wasserstein approximations, we are probably looking at only a fractional increase in time for the encoder network above the policy, dynamics and reward models. So roughly 30% more?? \n\nKey figures for central results are too small to get meaningful interpretations unless enlarged by a factor of 4 or 5. Somehow this seems to go against the spirit of page limits to me. \n\nFigure 6 and the description don't seem to be aligned, but I can imagine that it can be readily fixed.\n \n\n# Recommendation  \n\nI recommend acceptance. The paper shows how to extend bisimulation principle for grouping states to continuous deep actor critic methods and provides convincing evidence on standard benchmarks that it is effective in extracting a task-relevant abstraction that is robust to noise in observations and focuses on task specific detail. \n\n \n\n# Questions  \n\nFigure 6: it seems that the “simple_distractors” environment is different than “setting 2” natural video setting. This caption could use some reworking to get parallelism clear. The ‘ideal gas’ is the same as simple_distractors?  Also the graphs seems to be about different experimental types. Does the first experiment also use frozen encoder?  The graphs do not seem to relate to the text which talks about walker_stand, walker_run reward functions. I am confused. \n\nCan the paper have any insight into the relative performance of their algorithm versus benchmarks algorithms across different tasks (finger spin, cheetah, walker) given that they are all stick figures? \n\n# Feedback  \n\nI was able to work through Definition 1 and assure myself it made sense. In the end I made a small picture that helped. \n\nThere seems to be some ambiguity in notation between observations, underlying state and latent variable spaces.  For instance, in section 3, script S is defined as a state space. In section 4, the function d is defined on script S x script S which is described as an observation space.  Admittedly, the work seems to be situated in an approximately fully observable world in which states and observations are somewhat equivalent. I suspect that is why the 5-camera 300-degree suround view was necessary in the CARLA experiments. \n\nInteresting that the paper employ stop gradients on the latent representation terms when they appear in the reward and Wasserstein terms in the loss function. This is to enforce the separateness of the optmizations in algorithm 1? \n\nDefinition 2, the bisimulation metric contains a max … so this is worst case discrepancy between the futures between state space actions and empirical actions. Was average discrepancy considered? This might be relevant later in discussion about intractability in previous approaches (section 4 paragraph 3). \n\nEquation 4 specifies an L1 metric for the distance in abstraction space ||z_i – Z_j ||. Is there a motivation for this choice? Again trying to bound the worst error? \n\nTheorem 4 references Theorem 5 which is not in the main text. \n\nIn section 5, ideally epsilon would be briefly described before it appears in a bound. \n\nFigure 3 is very small... the labels – particularly the subscripts are unreadable. It may not be making a point important enough to include it. The causal variables section could be shorted in general. It is a good point but not completely surprising. \n\nFigure 4 is a key figure to support the paper's hypothesis that deep bisimulation is effective for state abstraction in noisy images. This figure really needs to be bigger. In particular, I had to strain to read the legends to understand if the axes were different between the uncluttered video of articulated figures and the cluttered video of articulated figures with a background movie in them.  It was also hard to make out which line was which.  In particular, the “cheetah” column does not show DBC improving on cluttered video scenario --- it tops out at around 250 in both top and bottom graphs, but the scales are very different. It confuses the message a bit.  \n\nThere is a statement “a single loss function would be less stable and require balancing the components”. Is this speculation or based on experience? Separate optimizations implicitly define a balance between these terms wouldn’t they? Namely equal balance? \n\nTo some degree, figure 9 is making the same argument as figure 5. Could leave this out if you were tight for space. \n\nFigure 5: What is the first column of figures to the left? It doesn’t seem to be relevant? Otherwise, I think the figure is very effective in conveying the structured embedding space does a better job of grouping similar states. It is also very small. Drawing a white border between the figure pairs would underscore visually that there are two states in each “image”  \n\nFigure 9: I could not make sense of the images on the sides of the figure. Particularly, the images on the left side.  They seemed to be abstract geometric shapes and I could not get an intuition about what the driving scenario was. \n\nSection 6.4 “reward highway progression an penalizes collisions”  an => and \n\nFuture work – another likely avenue for future work would be to introduce some sort of memory to handle partially observable worlds. For instance, can the agent drive a car with only a forward view if given memory? Does this break down if it does not have memory? This could either be an explicit neural memory or implicit memory such as an LSTM …  Estimating uncertainty could also be important to produce agents that can work in the real world and assess when they know what they are doing and when they do not. Another future work area is in modeling of transition distributions as something more complex than Gaussians … what if there are distinct possible futures that are equally valid: it is ok for the robot to turn left or right as long as it avoids the object straight ahead? ",
            "rating": "9: Top 15% of accepted papers, strong accept",
            "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"
        }
    ]
}