{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper studies the problem of learning a graphical model given observational and experimental data. The main novelty is the use of interventions to avoid the acyclicity constraint that plagues existing methods. Although this idea is quite standard and well-known, the generality of the approach merits consideration. After the discussion, there was a consensus among the reviewers to accept this paper. Some valid concerns have been raised and we expect that the authors will take into account all of the suggestions raised by the reviewers."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors propose a differentiable causal discovery method from interventional data. Each edge is learned separately using a score-based approach that relies on Monte-Carlo sampling. The main difference is that - since edges are learned separately - acyclicity need not be enforced. ",
            "main_review": "\"Unfortunately, the solution space of DAGs grows super-exponentially with the number of variables. Current methods are thus typically limited to a few dozens of variables\"\nI am not sure if this reasoning is accurate since exhaustive search is not used.\n\n\"Yet, we show that under certain assumptions like being able to intervene on all variables, the proposed optimization is guaranteed to converge to the correct, acyclic graph\"\nThis is not an insignificant assumption so it should be front and center in the introduction instead of being mentioned in passing. \n\n\"Thirdly, under mild conditions including interventions on all variables, we show that ENCO is guaranteed to converge to the correct causal graph\"\nSimilarly, it is hard to justify this claim that this is a mild condition. I think it would be much more transparent to call the paper causal discovery from observational and interventional data - which describes the setting - rather than trying to present this as just another assumption.\n\n\"A DAG G and a joint distribution P are faithful to ea􏰍ch other\"\nDistribution is said to be faithful to a graph, not the other way around.\n\nImportant recent works that learn from obs+int data are missing from literature review under constraint-based methods:\n- Mooij et al. \"Joint causal inference from multiple contexts\"\n- Kocaoglu et al. \"Characterization and Learning of Causal Graphs with Latent Variables from Soft Interventions\" \n- Jaber et al. \"Causal Discovery from Soft Interventions with Unknown Targets: Characterization and Learning\"\n\n\"Since the search space of DAGs is super-exponential in the number of nodes, many score-based approaches rely on a greedy search\"\nAs far as I am aware of, there are consistency guarantees that these methods will output graphs in the true equivalence class. So it would be more accurate to mention this. \n\n\"We do not assume faithfulness\"\nThis is a great addition.\n\n\"perfect\" intervention is also called a \"hard\" intervention. \n\nIt would be nice to list all the assumptions in 3.1 under a numbered list. Specifically, authors assume that we have n interventional datasets for n variables where in each intervention a different node is intervened on (intervention size 1).\n\nThe discussion under \"Graph fitting\" on page 4 mentions that the method requires intervention on a node that is adjacent to the edge (although others might eventually be sufficient as well). However, if such an abundance of interventional data is available, one can simply learn each edge separately by contrasting two distributions and hypothesis testing without the need for a continuous optimization framework. Say X-Y and p_1 = p(y|x), p_2=p(y|do(x)) is available. Then we can check if p_1 and p_2 are the same or different to infer this edge. So essentially I am not sure if there is a need to leverage the power of continuous optimization or neural networks in this setting. More on this later.\n\nThe conditions of Theorem 3.1. seems to have been developed by essentially reverse-engineering the proof. In that sense, they are far from being intuitive about the data-generating conditions or system settings. Furthermore, due to this, it is not possible to assess whether these assumptions are realistic. This renders the theorem not really useful in my opinion and the authors can simply remove this unless the assumptions can be converted to intuitive ones descriptive of the setting irrespective of the proof. \n\nThe authors propose an interesting method to handle latent confounders. They have to assume that latent confounders cannot exist between adjacent variables. Even then though I am not sure if there is any guarantee because authors say \"which is maximized by latent confounders\" but I don't see a proof of this claim. It would help to clarify the exact claim here.\n\nIn the experiments, I am not sure why authors only compare with DCDI but not with the other mentioned methods.\n\nCould you comment on whether the improvements seen in Section 4.3 are due to the new edge-wise detection framework (not having to enforce acyclicity constraint) or due to the better-variance estimator described in Section 3.3? \n\nI really would like to see the performance of a very simple baseline: \n- Learn the skeleton somehow (e.g. w/ GES)\n- Orient each edge X-Y by learning the skeleton on the interventional distribution, say under an intervention on X. If X-Y still then X->Y else X<-Y. This is the basic idea that has been used in several constrained-based methods in the past when hard-interventional data is available. If only soft-interventional data is available one can check p_obs(y|x)==p_x(y|x) and orient X->Y if so and X<-Y otherwise. Of course, these require some sort of faithfulness but in settings with faithfulness, I think it is important to have these comparisons to clarify the contribution of the method. Perhaps your method is more sample-efficient and it would be great to demonstrate this empirically. \n\nFinally, I am not absolutely sure if your method can avoid faithfulness. Suppose X->Y is the true graph but we have X\\indep Y. Your (or any method really) will remove the edge between the two but that won't give the true graph. Any comments on this as well will be appreciated. \n\nAfter the rebuttal:   \n--------------------  \nI would like to thank the authors for the additional experiments they conducted and the further explanations about their assumptions. Accordingly, I will increase my score. However, due to the shortcomings mentioned in my original review and the theoretical assumptions still being non-intuitive, I will not be able to recommend a strong acceptance. In case the paper goes through, please implement all the discussed changes, and thank you again for keeping an open mind and enabling a productive rebuttal period!",
            "summary_of_the_review": "A new approach for causal discovery and some interesting ideas but the utility should be demonstrated by comparing with simple baselines since interventional data is assumed to be available. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper introduces a method, called Efficient Neural Causal Discovery (ENCO), to determine the structure of a causal graphical model of a set of random variables, which uses both observational and interventional data.\n\nThe training procedure of the method consists of two steps: a distribution of one random variable given the others is fitted through a neural network and by performing interventions the graph is fitted using the approximate distributions. By alternating the two steps the causal graph can be recovered. The authors state the conditions when this procedure converges and prove the respective convergence theorem. Furthermore, they show how latent confounders can be detected within their framework.\n\nThe authors apply their method to several synthetic datasets, among them real-world inspired data form the Bayesian Network Repository (BnLearn), to show that they can reconstruct the causal graph for various numbers of random variables as well as find latent confounders if they are present. Based on various performance metrics they outperform competing methods, such as Structural Discovery from Interventions (SDI).",
            "main_review": "Overall, the paper is well written and clearly structured. I appreciate Figure 1 which is a concise graphical overview of the method.\n\nThe method itself and how to train it is clearly explained in the sections 3.1-3.3. Regarding the convergence guarantees made in section 3.4, I am not an expert in this domain so I cannot confirm with high certainty that the proof is correct but the logical reasoning seems to be fine. I acknowledge that the authors have an example of how to check the conditions of Theorem 3.1 in appendix B.2.1. However, as far as I understand the paper, these steps can only be applied if the probabilistic model is known but this is not the case in an real world scenario. Especially condition 3 seems problematic as it sets the scale of the regularisation parameter $\\lambda_{sparse}$. For large graphs, it seems to be very complicated to estimate $\\lambda_{sparse}$ and, hence, there will be no guarantee that we recover the full causal graph.\n\nThe experiments are extensive and the authors proved that their method is clearly better than several baselines on different datasets. What seems strange to me is that they did only compare their method to SDI in section 4.6. They argue that it is in general the strongest baseline but in fact it is outperformed by DCDI on the sachs and child dataset according to Table 9. These results should be reported in the main paper as well, especially since they do do not need a lot of space. \nFor the paper to be truly outstanding, it would have been nice to see the method being applied to a real-word dataset. Although the true causal graph is not known in this setting, the plausibility of the result could be discussed in this case to evaluate the performance of the method.",
            "summary_of_the_review": "Overall, the paper is a significant contribution to the area of learning causal graphs. Besides several minor points, the article is well written and should be accepted.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper works on causal discovery on both observational and interventional data.\nIt proposes a method for enforcing the acyclicity of DAGs which avoids imposing constraints on the optimization problem.\nIt proves that under some assumptions, when all the variables have interventional data, the resulting graph converges to the ground-truth one. The experimental results show the properties and performance of the proposed method, including the cases with latent confounders and interventions on fewer variables.",
            "main_review": "The paper is in general well-written. The discussion on the latent confounder case and few interventions are appreciated. The strength of the work is its originality. It introduces a certain parameterization method for enforcing the acyclicity of the model without imposing acyclic constraints on the optimization problem.\n\nNevertheless, I do have some concerns about the work:\n1. The sparsity penalty term in (2).\nThe graph with the maximum likelihood may not be the one with the minimum loss value of (2) because of the penalty term. The graph fitting procedure is actually not picking up the graph with the maximum likelihood but selecting a graph by jointly considering likelihood and the penalty term. So why would this be reasonable?\n\n2. The convergence guarantee.\n* It requires intervention on all variables. It would be necessary to justify the significancy of the convergence result. According to the citation (Eberhardt et al., 2005) in the paper, roughly speaking, in the worst case, it requires log2(N) + 1 experiments to determine all causal relations, where N is the number of variables. \n\n* Moreover, in the case with intervention on fewer variables, it remains unclear that what would be the results in such case. Are they in the same Markov equivalent class as the ground-truth graph, or there can be arbitrary differences from the ground-truth graph?\n\n* It would also be necessary to justify condition 3 in Thm. 3.1. Especially, how realistic is the condition? When can it be satisfied? And what will happen if it is not satisfied? \nIt would be better to make it clear whether the improper sparsity parameter can only lead to slow convergence or it can even lead to wrong results.\n\n3. A minor suggestion. It would be better if the paper distinguishes causal discovery on observational from causal discovery on both observational and interventional data, and then introduce more about the later setting which is more related to the work.\nBecause when mentioning causal discovery, in common, it refers to only on the observational data, it can be misleading sometimes.\nMoreover, instead of only generally mentioning the works based on observational and interventional data, it would be helpful for having an overview and a better judgment and understanding of the paper if including the important theoretical results and the related methods/conclusions in such setting.",
            "summary_of_the_review": "The strength of the paper is the proposed way for enforcing acyclicity and the good properties of the proposed model. \nHowever, the convergence guarantee may require justification for its significancy and Condition 3. Moreover, the loss function in the graph fitting procedure may need a justification and discussion for the penalty term.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The problem of score-based structure learning using observational data as well as a set of interventional datasets is increasingly relevant in domains where experiments are readily available. The authors propose a new parameterization of the DAG underlying the data and score showing, provided that experiments are available on all observed variables and the system be Markovian, that the true causal graph can be recovered. ",
            "main_review": "I find the proposal very interesting, especially in light of strong empirical results. The paper is well written but in my opinion, many of the claims were not sufficiently justified.\n\n1. A starting question I have is why acyclicity is necessary to enforce if interventions are available on all variables. An approach such as ICP [Peters et al, 2016] applied to all variables separately should recover the underlying graph and does not require acyclicity. \n\n2. It is not clear that the authors ensure that the recovered graph is acyclic. It is mentioned before section 3.3 that \"... acyclicity can be enforced as a post-processing step\". What is this step? Where is it discussed?\n\n3. Interventions on all variables will obviously never be the case in practice, how do the authors think the proposed approach can be used in practice. What does the method converge to if interventions are not available on all variables? Can we characterize equivalence classes?\n\n4. Theorem 3.1 assumes access to conditional distributions if my reading is correct, is it plausible to expect that there will be no local optima with finite samples? With overparameterized models, this seems unlikely.\n\n5. After Theorem 3.1 the authors say \"we can guarantee to converge to the true causal graph if, for every edge Xi → Xj in the ground truth, the variables are not conditionally independent in the interventional data of Xi\". Isn't this a faithfulness condition, i.e. that a conditional independence implies a corresponding separation in the graph?\n\nMinor.\nScore-based methods are defined by the optimization of a score rather than by a search strategy in the space of DAGs which makes me uneasy about the new category of continuous-optimization methods.",
            "summary_of_the_review": "Interesting proposal if some of the claims can be better justified and if we can bring the algorithm closer to practice. In its current formulation, both interventions on all variables and unlimited sample sizes to guarantee convergence are not plausible in my opinion.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new gradient-based method to learn causal DAGs from observational and interventional data. It models a probability for every possible directed edge between pairs of variables and searches for the graph which generalizes best from observational to interventional data.",
            "main_review": "This paper is well-written. The claims seem sound and the experimental results are convincing. The assumptions are clearly described, and the limitations are also discussed. Also, the authors extend their method to detect hidden confounders, which is interesting and important.\n\nMy only question is about the distribution fitting stage. The authors build a neural network for each variable to model the conditional distributions of that variable, conditioning on any variable set. That is, given a variable $X_i$ and a conditioning set ${\\\\bf X}_I$, where the indexing set $I\\\\subset \\\\{1, 2, \\\\cdots, N\\\\}\\\\setminus\\\\{i\\\\}$, the authors define a mask ${\\\\bf M}\\\\in \\\\{0, 1\\\\}^{N}$ such that ${\\\\bf M}(j)=1$ if and only if $j=i$ or $j\\\\in I$, and model $P(X_i \\\\mid {\\\\bf X}_I)$ by the NN $f({\\\\bf M}\\\\circ {\\\\bf X}; {\\\\phi_i})$. I am wondering how this objective relates to the loss function given in Equation (1).\n",
            "summary_of_the_review": "Overall, this paper is well-written. The claims are supported by rigorous theoretical analysis and extensive empirical studies. It seems that there is only a minor issue in Section 3.2. I think this is a good submission if the authors could clarify this point.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}