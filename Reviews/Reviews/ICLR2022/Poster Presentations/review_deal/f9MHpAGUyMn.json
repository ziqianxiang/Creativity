{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "A new method for dynamic token normalization in ViTs (both within and across tokens) is introduced in the paper. As noted by the reviewers, the proposed method is technically sound, with a clear and solid motivation. The main raised concerns included the lack of experiments using larger models, unclear reason for the accuracy gains, and lack of experiments on other tasks beyond classification, such as detection and segmentation. The authors’ response was strong, clarifying other questions and providing additional experiments, for example, showing the effectiveness of the method on object detection, and when applied to larger models or architectures that explicitly model local context. Two reviewers recommend borderline rejection, but they did not participate in the discussion nor updated their reviews after the author response. The AC considers that their concerns were adequately addressed by the rebuttal, and agrees with the other two reviewers that the paper passes the acceptance bar of ICLR. The authors should carefully proofread the paper for the final version."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper introduces a dynamic normalization, named Dynamic Token Normalization (DTN), to replace the vanilla layer norm in ViT. It learns to normalize tokens in both intra-token and inter-token manners, enabling Transformers to capture both the global contextual information and the local positional context. Experimental results show that DTN can improve some Vision Transformers in ImageNet Classification and Long ListOps tasks. ",
            "main_review": "Strengths:\n\n* The motivation of this work is clear and solid. Normalization plays an essential role in both CNN and Transformer-based models, but it does not attract too much attention for current ViT models. This work analyses some properties of token normalization. In Fig 2 and Fig3 (b), it shows us the importance of token statistics diversity. \n\n* The proposed method is technically sound. Adoptingrelative positional embedding to significantly reduce the extra learnable parameters is quite interesting and effective. \n\n* The paper is clearly written and easy to follow.\n\nWeakness:\n\n* The novelty of dynamic normalization is limited. The technology of dynamic networks has been widely adopted for architecture, activation functions, and normalization, especially for CNN models. The authors should discuss some related works [1].\n\n* The experimental results are not sufficient to support this work. One key motivation of this work is to easily induce inductive bias such as local context, which is more important in some downstream vision tasks such as detection and segmentation. So I think the single result in imagenet classification task is not enough. The author also claims that DTN is easy to be plugged into Swin and PVT, so it would be appreciated to see the experiments on more downstream tasks. \n\n* Minors: in Eq (3), the sigma sum over $\\mu_c^{in}$ should be T, not C.\n\n[1] Luo P, Zhanglin P, Wenqi S, et al. Differentiable dynamic normalization for learning deep representation[C]//International Conference on Machine Learning. PMLR, 2019: 4203-4211.",
            "summary_of_the_review": "Overall, I like the motivation and some technology in this work, but my major concern is the insufficient experiments. Thus I stand in \" marginally below the acceptance threshold\" now and waiting for the author's response.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies properties in layer normalization and instance normalization, and states that the two normalization operations have their own drawbacks in vision transformers: LN suffers from lacking inductive bias while IN may be affected by the different semantics within the tokens. Accordingly, this paper proposes a new normalization method called DTN and considers both inter- and intra- token normalization into vision transformers. According to the experiments in the paper, the proposed DTN normalization boosts the performance of various vision transformer models on classification with ImageNet dataset as well as robustness with IMAGENET-C and IMAGENET-R.",
            "main_review": "Pros:\n1. The paper is well written and organized.\n2. The performance of popular vision transformer models gain with the proposed DTN, no matter with the ViT model or with the stage-wise model PVT and Swin Transformer;\n3. The proposed DTN can also be combined with other sparse attention modules such as BigBird or Reformer to boost the performance;\n4. The visualization results in Figure 2 confirm that DTN can boost the model to have a relatively small attention distance in early layers.\n\nCons:\n1. As shown in Table 3, the performance of ViT with BN drops a lot. Can you give some more explanations? As BN has proven to be more effective than LN as shown in many CNN networks, I am wondering why it is less effective than LN in vision transformers. The reason that tokens represent different semantics seems not convincing, since contextual information is also very important in vision tasks.\n2. It seems that the initialization of DTN depends on the image size, as it uses position embeddings. As demonstrated in CPVT, such learned or fixed position embedding may not be so effective in testing on variant image sizes, i.e., training with image size 224 and test with image size 384, which is necessary for downstream tasks. As the proposed DTN is also integrated into Swin and PVT, can the authors provide the performance comparison between the origin Swin/PVT and the DTN integrated versions on downstream tasks, such as detection/segmentation?\n3. All experiments are conducted upon models with performance less than 82.5 top-1 accuracy on ImageNet, which makes the utilization of DTN on more powerful models unclear. The authors should consider more experimental results of DTN on other models like Swin-B or PVTv2-B3.\n\n\nAdditional:\n\n1. The notion of \\mu \\ sigma in Figure 4 seems to be put at the wrong place, which makes readers confusing.\n\n2. Just for curiosity, if we initialize the DTN modules with the $lambda$ learned in Figure 4, fix them and train the model from scratch, would that lead to better performance?\n",
            "summary_of_the_review": "The paper's analysis seems reasonable on some given models. However, it is not clear in the paper whether this finding is still valid for larger models, or whether it can handle inputs with different sizes. I am willing to raise my score if the authors can address my concerns in their response.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper first analyzes the limitation of LN in Transformers and then proposes DTN to capture both long-range dependencies and local positional context. DTN is a unified version that balances LN and IN. Extensive experiments show the effectiveness of the proposed DTN with some small/middle-scale Transformers on the ImageNet.",
            "main_review": "+1) Both the motivation and idea are clear. The analysis is reasonable.\n\n+2) The ablation study shows that DTN can improve the performance on some small/middle-scale models.\n\n\n\n- Q1: Can authors evaluate the training or inference throughput? Flops sometimes cannot reflect model speed.\n\n- Q2: DTN only is evaluated with small models (e.g. swin-tiny, vit-small, vit-base, etc.). It may be more robust if DTN can improve performance on some larger models (e.g. ViT-L, Swin-B, Swin-L, PVT-L, etc) or down-stream tasks (detection, segmentation, recognition, etc.).\n\n- Q3: I notice that the relative positional embedding is used in DTN. However, vanilla ViTs use the learnable positional embedding. Does the positional embedding keep same in ablation studies?\n\n- Q4: The motivation is to balance LN and IN in token normalization. It is straightforward to integrate output of LN and output of IN with a balance weight. However, P^h = softmax(R*a^h) is closely related to positional offsets. Therefore I am afraid the improvements mainly come from positional information instead of normalization paradigms. In other words, ViTs can introduce a better positional embedding but does not change LN to do the same thing.",
            "summary_of_the_review": "The paper clearly presents the problem and methods. The experiments can be improved to further support the conclusions. Therefore, I rate to 'marginally above the acceptance threshold' now.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work presents a token normalization method in replacement of Layer Norm (LN) and Instance Norm (IN) for vision transformers (ViTs). The motivation is that the authors find that the common normalization used in most existing ViTs, LN, will reduce the difference in token magnitude, and this may lead to failure of capturing positional context and other inductive bias. Hence, they propose a Dynamic Token Normalization (DTN) component by combining LN and relative positional embedding based transformation. DTN can be plug in varying ViTs e.g., ViT, Swin, PVT. The experiments are done on ImageNet, ImageNet-C, ImageNet-R, and ListOps in supervised learning setting, as well as self-supervised pre-training. ",
            "main_review": "**Strengths**\n1. The norm component is some important ingredient of ViTs and it is worth further investigation and bringing in our attentions\n2. An extensive study of different token norm methods for ViTs\n3. The writing is clear and easy to understand \n4. The results of DTN is generally good across different benchmarks\n\n**Weaknesses**\n1. Motivation is not clear and strong sufficiently:\n> + Why is the token magnitude so critical for model performance? The objective of norm itself is to restrict the training data (e.g., tokens in ViTs) to some specific range which has been shown to be helpful for model optimization. For learning capacity, the normalised vectors of high-dimension should be well expressive, and hence there is no high necessity to further explore the magnitude dimension (a single dimension).\n>+ In Figure 1, I have these concerns:\n>>- If one considers that (a) LN reduces the token magnitude difference for the red box case, then (b) IN and (c) DTN would increase the difference, rather than *preserve*. \n>>- From (a) it is seen that different heads actually present different patterns. There is no interpretation and insights that why all heads need to be similar in maintaining the token magnitude, and the current bahaviour of LN is not as good or desired. \n>>- Using norm to impose inductive bias (e.g. local context) is some unusual and implicit. How does this compare with existing local window attention e.g. [a], [c], [d], [e].\n\n2. Unclear and inaccurate content\n>+ Global vs. Local attention: It is unreasonble that when the model can model global context (attention), it will be hard to capture the local context. Local context is just part of global context. At least this is not accurate statement.\n>+ With conventional wisdom, norm is usually not considered as the decisive component for choosing between global context and local context. Instead, it is the scope of tokens in computing the attention scores for given a token, i.e., what tokens are used for pairwise attention learning. So this authors need to be further justified this.\n>+ Figure 2: how is the mean attention distance computed? In particular, what is the center of attention?\n\n\n3. Experiments\n>+ What positional embedding is used for baseline such as ViTs, PVTs, and Swin-T? Given that DTN uses the relative positional embedding (RPE) at each block, for a fair comparison, RPE should be applied to baseline as well. This also helps to separate the effect of RPE from the proposed DTN in analysis. Besides, RPE is shown to be beneficial for ViTs in some works [b].\n>+ How many iterations run for each experiment? In some cases, the margin of DTN over LN/BN is not big, so the variation of different runs may become more important.\n>+ As the authors consider DTN as a component that is able to better learn local context, except comparing different normalization designs, more other alternatives (e.g. [a], [c], [d], [e]) that help with local context learning should be considered in comparison. While some of these works are very new, but the authors should at least include some necessary evaluations on this aspect.\n\n**References** \n\n[a] Stand-Alone Self-Attention in Vision Models. NeurIPS 2019.\n\n[b] LeViT: a Vision Transformer in ConvNet’s Clothing for Faster Inference. arXiv 2021. \n\n[c] Twins: Revisiting the Design of Spatial Attention in Vision Transformers. NeurIPS 2021\n\n[d] Transformer in Transformer. NeurIPS 2021\n\n[e] Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet. ICCV 2021\n\n",
            "summary_of_the_review": "It is an interesting point for research in ViTs. The motivation is not sufficiently clear and solid. Some of the claims are not precise as stated above. In the experiments some important details and comparisons are missing, while a variety of experiments have been included including self-supervised learning. \n\nOverall, in the current form this submission is not strong enough for acceptance. I pretty much vote a rejection rate. However, more information from the authors is needed to make a final recommendation. \n\n\n******* Post-rebuttal Update *********\n\nThe authors have well resolved all of my concerns with additional interpretation and experiments.\nOverall, this work is novel in taking the normalization aspect for resolving the limitations of existing ViTs in exploring local context, and showing consistent gain for SOTA ViTs, based on interesting observations and solid design. Therefore, I recommend acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}