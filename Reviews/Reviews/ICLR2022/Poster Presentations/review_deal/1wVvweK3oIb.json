{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This is a borderline paper. The most enthusiastic reviewer does not have much confidence in the score. The other reviewers think the paper has some value after the rebuttal, but also feel there is little technical novelty. The proposed applications of the approach are interesting.\n\nAfter reading the reviews, rebuttal, and the paper, I agree that there is little technical novelty. The idea of adding node-label noise to a GNN to improve GNN expressiveness dates back to (Murphy et al., 2019) and has been also explored by (Dasoulas et al., 2019), (Vignac et al., 2020), (Loukas, 2020) among others [one of which is suggested by a reviewer] (this literature is entirely missing from the paper). The paper has some novelty in proposing a regularization method for tackling the node-level noise by augmenting the loss function with a denoising term. The oversmoothing justification is not properly investigated (whether the proposed solution really solves the issue in practice).\n\nIf there is space in the borderline decision boundary, this paper could be a worthwhile inclusion.\n\nDasoulas, G., Santos, L.D., Scaman, K. and Virmaux, A., 2019. Coloring graph neural networks for node disambiguation. arXiv preprint arXiv:1912.06058.\nLoukas, A., 2020. How hard is to distinguish graphs with graph neural networks?. arXiv preprint arXiv:2005.06649.\nVignac, C., Loukas, A. and Frossard, P., 2020. Building powerful and equivariant graph neural networks with structural message-passing. arXiv preprint arXiv:2006.15107.\nMurphy, R., Srinivasan, B., Rao, V. and Ribeiro, B., 2019, May. Relational pooling for graph representations. In International Conference on Machine Learning (pp. 4663-4673). PMLR."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "To overcome oversmoothing in GNNs, this paper proposes node noise with noise-correction training, similar to denoising autoencoder, to make node representation distinguishable. On 3D molecular property prediction tasks, it improves GNS significantly, achieving state-of-the-art. It also improves GCNs on non-spatial tasks.",
            "main_review": "* Pros:\n\n1. The node noise method is simple and easy-to-use with significant improvement on 3D molecular prediction tasks and non-spatail tasks, which may make it a standard trick in the future.\n2. The abalation study is comprehensive.\n\n* Concerns:\n\n1. I agree that it would be interesting to incorporate the methods into small data regime.\n2. Such an autoencoder structure may guide the learning process in GNNs into some bias. For example, it would be intriguing to see how to use it to analyze whether GNNs can keep focusing on local information after stacking many layers. (This question is a little bit derivating from the original motivation of this paper.)",
            "summary_of_the_review": "I recommend to accept it for its straightforward intuition, simple methods and strong experiment results.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper presents a simple regularization method based on denoising, called Noisy Nodes (NN) for Graph Neural Networks to reduce oversmoothing. The NN method adds a small noise to the input node representation (and optionally edge representations) during training. An auxiliary loss is added for predicting the uncorrupted node representation. The paper presents experiments using this method for a few popular GNNs (GNS, GCN etc) on different datasets and show improved performance, especially for very deep GNNs.\n",
            "main_review": "Strengths\n1. The NN method shows improved performance on a wide variety of tasks for different GNN architectures. The paper includes experiments with both graph level prediction tasks as well as node level prediction tasks and NN is shown to improve for both kinds of tasks.\n2. It has generally been hard to train very deep GNNs for many problems because of oversmoothing problem. The NN method is shown to work well with upto a depth of 50 in the paper.\n3. The paper clearly demonstrates that a model trained with NN is able to overcome oversmoothing by showing the MAD statistic at each layer (fig 2).\n\nWeaknesses\n1. NN adds additional hyper parameters like noise standard deviation and weight of the auxiliary loss. From reading the paper, it is unclear how sensitive the final performance is to these hyper-params. It would be good to shed some light on this because the practical utility of a method like this depends on the sensitivity to such hyper-params.\n2. In table 7, the 16 layer model is only shown with NN. Please add a column to show performance without NN.\n3. While the current results are very impressive, some of the best models for molecular prediction use higher order terms (e.g. Senet, Dimenet++, Gemnet). It is unclear if the NN method can work with such models.\n\nA few more things that could be added to the paper to improve it:\n1. The paper shows benefits of scaling the depth up to 50 layers. It would be good to show what happens beyond this depth. For example, does performance keep improving and plateau at some point, or does it deteriorate beyond a limit?\n2. NN can be seen as both a regularization method (which is most helpful in the small data setting), and a way to reduce oversmoothing to enable training very deep models (which is most helpful in the large data setting). It is, therefore, unclear how the performance of NN would vary with the size of a dataset. It would be good if the authors can shed some light on this.\n",
            "summary_of_the_review": "The paper presents a simple new method for improving GNNs. The proposed method is based on existing ideas used in other domains, but their application to GNNs is novel. Through extensive experiments, this method was shown to benefit a variety of GNN architectures across different datasets and tasks. The simplicity and generality of this method is likely to have a big impact on the field of GNNs, which prompts me to lean towards accepting the paper.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a new regularization method for tackling both over-smoothing and overfitting in GNNs. The main idea is to add node-level noise to input graphs and augment the loss function with a denoising term. The authors run experiments on three benchmarks for 3D molecular property prediction and three datasets (with no 3D features) from Open Graph Benchmark. Overall,  GNS (Graph Network Simulator) and vanilla GCNs are used as base GNNs. The proposed method outperforms the existing approaches. \n",
            "main_review": "**Strengths**\n- The simplicity of the proposal.  \n- Solid empirical results.\n\n**Weaknesses**\n- It is not entirely clear where the gain is coming from (W1);\n- No comparison against other ways to achieve deep GNNs, i.e., ways to tackle over-smoothing (W2);\n- Limited technical novelty with non-concrete claims (W3);\n- Experimental setup: not clear why the authors focus on molecules (W4).\n\n**Detailed comments**\n- (W1) The authors fail to show that GNS (same setup but without noisy nodes and additional loss) actually suffers from over-smoothing and overfitting (the paper's initial motivation). Figure 3 is not enough to demonstrate overfitting as it only exhibits validation curves and no training ones. Also, Figure 3A shows that the performance of 3MP and 50MP are very close --- can we say over-smoothing is an issue here? Additionally, it seems the authors already apply other regularization strategies (dropout, early stopping, etc.) --- aren't they enough to avoid overfitting? \n\n- (W2) Since \"Noisy Nodes\" is introduced as a new simple regularization technique, I believe it is crucial to assess how the proposal compares with alternatives to handle over-smoothing in GNNs, such as PairNorm [1], long-range residual connections [2], and DropEdge [3].\n\n- (W3) The authors provide little support to their claims in Section 4, which are supposed to explain why the method works. For instance, the authors say that \"the GNN learns to go from low probability graphs to high probability graphs\". However, it is hard to guarantee that the noisy graphs lie in low-density regions, especially if we consider high-dimensional spaces.  \n\n- (W4) Is there any aspect of the method that makes it particularly tailored to molecular data or regression tasks? The gains on Arxiv seem to be less significant compared to those in other datasets.\n\n**Additional observations**\n* Noise has also been used to increase the expressive power of GNNs [4]. It should probably be mentioned in the paper. \n* Is there any particular reason for picking base GNNs with virtual nodes in Section 6? \n* It is weird to see tables full of numerical results with 0 std (Tables 1 and 3). \n* Typos: \"..adversarial noise during to input..\" (page 2); \" range of of \" (page 4); \"which report which report\" (page 9). \n* Reference Zonghan Wu et al. is duplicated.\n\n[1] Lingxiao Zhao, Leman Akoglu. PairNorm: Tackling Oversmoothing in GNNs, ICLR, 2020.\n\n[2] Ming Chen et al., Simple and Deep Graph Convolutional Networks. ICML, 2020.\n\n[3] Yu Rong et al. Dropedge: Towards deep graph convolutional networks on node classification. ICLR, 2020.\n\n[4] Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Random features strengthen graph neural networks. arXiv preprint, 2020.\n\n",
            "summary_of_the_review": "Overall, I like the simplicity of the proposal and the fact the paper tackles relevant issues in GNNs. Also, the paper shows good empirical results. However, my main concerns revolve around limited technical novelty, lack of experiments to validate some intuitions behind the method, and comparison against related methods. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a noise-injected training mechanism for graph neural network. The idea is to perturb the node attributes and add an auxiliary loss for the noise-correction task. The authors. Implementing this strategy for the GNS model And observed improvements on several 3D molecular prediction tasks. \n",
            "main_review": "\nStrength:\n- The method is pretty simple, but the results are very positive on several benchmarks, which has achieved improvements over several baselines. \n\nWeakness:\n\nThe writing needs to be improved, especially for the method section and the experiment section. I find many important details are missing. For example. How is the noise-enhanced auxiliary loss incorporated into the 3D prediction task? How are the two aspects balanced?\n\nThe claimed relative improvements need to be broken down in Table 1-4. For\nexample, in table 1, there are many different factors that may contribute to\nthe claimed improvements over existing baselines: is the improvement due to\nthe advantage of GNS over baselines, or the changes made to GNS, or the core\nidea of noise injection?\n\nSome important baselines are missing. Two categories of baselines need to be\nadded: (1) There are several existing works on injecting adversarial losses\ninto graph neural networks, which can be also combined with GNS for 3d\nmolecular prediction. How does this proposed method compare with those\nexisting methods? (2) For 3D molecular modeling, there are other more powerful\nequivariant models, such as Tensor Field Network, Se(3)-Transformers, GemNet.\nWithout including those baselines, it is hard to conclude how this method\nadvances the SOTA for 3d molecular property prediction.\n\nIt's better to perform sensitivity analysis to the injected noise. How are the\nnoises chosen for different datasets, and how sensitive is the model to such\nnoise?\n\n",
            "summary_of_the_review": "The proposed idea is intuitive and sound, and the results are positive. However,  important baselines and detailed experiments needs to be added to support the claims and justify the contributions. The writing also needs to be improved.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}