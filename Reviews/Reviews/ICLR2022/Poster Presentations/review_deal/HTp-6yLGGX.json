{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper is proposed to deeply investigate the hot-refresh model upgrades of image retrieval systems. The hot-refresh model is very useful since the model can be quickly updated after the gallery is backfilled. To address the model regression with negative flips, this paper introduces a Regression-Alleviating Compatible Training (RACT) method by reducing negative flips. The proposed method has been verified on the large-scale image retrieval benchmark such as Google Landmark. The key contribution of this paper is the new setting targeting an important application in real-world image retrieval systems. However, some of the technical details are not fully explained. Despite these minor concerns, the AC will rate this paper as a poster acceptance based on the overall contributions."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper addresses the task of hot-refresh model upgrade (ie. the task of deploying a new model into production while the gallery features from an older are still being re-indexed) in retrieval systems and studies the problem of model regression that happens when the similarity of new-feature-to-old-feature positive pairs is lower than new-feature-to-new-feature negative pairs. To solve this the authors propose a compatible training method that takes these flips into account, encouraging the new-to-old pair positives to be more similar than both new-to-old and new-to-new negative pairs (this is nicely illustrated in Figure 2). The paper also introduces an uncertainty-based backfilling (ie. the process of re-indexing the old gallery features with the new model) strategy that tries to follow a poor-first policy using the probability distribution of the training classes yielded by the new classifier.",
            "main_review": "_Strengths_\n\n- The presentation of the paper is very good. The paper is well written and the method has been properly motivated. The quality of the figures is also very good and they help a lot to understand the problem, the method, and the experiments.\n- It addresses an interesting problem that has real-world application in industrial image retrieval applications \n\n---\n_Weaknesses_\n\n- Although it addresses a novel problem in image retrieval, I find the novelty of the method itself somehow limited. It's very similar to other methods that sample pairs of features coming from the new and the old model, being the main contribution in this case that new-to-new negative pairs are also considered.\n- I have a couple of comments related to the fact that the authors decided not to use new-to-new positive pairs in the \"regression-free compatible loss\" (Lcomp and Lrf-comp). First of all, what I understand from the main paper is that they decided not to do it because \"it is hard to make sure intra-class instances exist in the same mini-batch without a pre-sampler\", however, adding this pre-sampler is actually easy and trivial, hence it is doable and not a reason why not to consider it in the first place. Secondly, the authors actually include an experiment in the appendix where they introduce a positive sampler and they show that similar results can be obtained by either using it or not. My suggestion here it is two fold: first I would suggest the authors to reference this experiment in the main paper to strength the decision of not including positive pairs. Second, I wondered if the reason of not seeing an improvement when sampling positives pairs in the batch it has to do with the fact that they're seen as negatives (?) in the second term of the denominator in Lrf-comp (k \\in B/x). Could other losses be more suitable for this scenario, eg. a triplet loss where several triplets combining positives and negatives from \"new\" and \"old\" are considered at the same time?\n- I found Figure 6 a bit difficult to \"read\" and to draw conclusions from it. Maybe showing an average per dataset or protocol would be better? Not sure about this though.",
            "summary_of_the_review": "Even though I consider the technical contribution a bit limited I positively acknowledge the fact that they're the fist to study the problem of hot-refresh model upgrade in image retrieval in particular and the problem of model regression.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper presents a method to learn compatible features with low negative flips rate. The paper also introduces a resampling strategy to gradually replace the gallery to ameliorate the performance. Resampling is based on feature uncertainty. The less discriminative a feature is, the sooner it is replaced.",
            "main_review": "STRENGTHS:\n1) The problem is very interesting and is not limited to social network applications. Compatible representations basically decouple learning from matching so that up-to-date features can always be used to match to the gallery-set. This may have several advantages especially in all those applications in which back-propagation is considered too slow to provide up to date features.\n2) Hot-refresh is a novel problem.\n\nWEAKNESSES:\n1) Contrastive loss seems to be a distillation loss because $\\phi_{\\rm old}$ is a fixed network. This should be better discussed.\n2) The paper seems to be divided into two independent parts. The first one is related to the combination of compatible learning with the reduction of negative flips, the second one is about refreshing the gallery. Their dependency seems to be only related because of the final application. This should be discussed.\n3) Contrastive learning usage is not motivated. For example, feature learning through a surrogate classification task is typically performing better than contrastive learning. This is also shown in the landmark dataset paper (Weyand et al., 2020), in which the cosface method is used. Moreover the contrastive effect of the loss seems to be limited being $\\phi_{\\rm old}$ a fixed network. This part is less developed with respect to the rest of the paper. Compatibility is also less evaluated.\n4) Sequential multi-model compatibility in upgrading to novel fresh data is not evaluated. This would have given the paper a lot more value. Even if sequential multimodal compatibility did not perform well, it would have been a great baseline to compare with hot-refresh model update. For example a five step sequential-upgrades with 20%, 40%, 60%, 80% and 100% of the data should be evaluated.\n5) A practical disadvantage of hot-refreshing through resampling is that the original image gallery has to be stored. Compatible learning typically does not require storing the original images. In many applications original images cannot be stored for privacy issues. \n6) The resampling strategy based on discriminative uncertainty seems to require multiple instances of the same classes in the gallery. This could be a strong assumption since the gallery is not a statistically meaningful set like for example the test-set in classification tasks.",
            "summary_of_the_review": "The paper introduces a very nice problem with some technical details to be clarified. The reviewer's opinion is that paper is not \"uniformly\" developed, many technical parts would have required more attention.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "The reviewer has no concerns.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper tackles the problem of model upgrades in large-scale training of image retrieval models. It proposes a new framework of “hot-refresh” updates with two main techniques: \n1) by alleviating “negative flip” by a regression-free regularization term in the loss and \n2) by prioritizing gallery images with the highest uncertainties during the backfilling process.\n\nThe results suggest improvement across three major retrieval benchmarks compared against baseline model-upgrade methods.",
            "main_review": "## Strengths\n\nFirst of all, this paper is mostly well-written and well-motivated, even though there are some parts where the readability could be further improved, especially in terms of definition and references on key terms/ideas such as “backfilling” “cold/hot-refresh”, “negative flip” etc. However, this does not impact the overall readability of the paper too much in my opinion.\n\nFor the results, I think they are clearly presented and link to the contributions well, as demonstrated in Figures 3-7 which shows that the new losses and uncertainty-driven backfilling indeed helping the hot-refresh model update process significantly. I particularly appreciate the small figures in Figures 3-5, which shows the progress in NFT v.s. vanilla, as it ties back to the observation of negative flips causing the model regression problem as a main motivation for this work.\n\n## Weaknesses\nFirstly, in terms of novelty, I think this work is ok as neither of these techniques have been used for model updates in image retrieval to the best of my knowledge. However, for uncertainty based backfilling, the authors have also mentioned that the techniques are based on Settles, 2009, which is on active learning. \n\nThe other main contribution is in the Regression-Free Compatibility Regularization (RFCT) in Eq. 6, as mentioned above, alleviates negative flips by diluting the significance of both new-to-old and new-to-new negative pairs in the contrastive loss. This is where I find the contribution could have much more potential, as this simple addition to the loss could have a lot more variety resulting in much more in-depth analysis of the impact of this term - e.g. changing the ratio between the two terms in the loss? Or perhaps different temperatures in each of them? Or even some dynamic tuning of these hyperparameters according to the uncertainty in the backfilling process, which ties the two novelties together. I think there is so much more that can be done to strengthen this novelty, but unfortunately is not presented in this work.\n\nAnother comment I have is the phrase “Regression-free”. If I understand correctly from the authors’ definition, the results in Figure 7 shows that this work is only regression free on the GLDv2 dataset (which the vanilla baseline is already close to regression free), and there is still significant model-regression observed in Oxford5k and Paris6k. I think in this case, claiming the main contribution of this work as “regression-free” is slightly misleading and I would suggest toning down the name to something less strong, e.g. “regression-alleviating”.\n\nLast but not least, not really a weakness here but just a general question, as the authors also took inspiration from active learning, I am wondering how does “hot-fresh” model upgrades compare against other techniques such as continual learning? For example, *Continual Learning for Image-Based Camera Localization* (Wang et. al 2021) also tackles the problem of learning gradually on unseen data, but on the problem of visual localization (which is still quite related to image retrieval). As I am not an expert on active learning / model upgrades, it would be great if the authors could clarify this.\n",
            "summary_of_the_review": "Overall, this paper is a good read and does provide useful insights to the proposed problem and potential remedies. \nHowever, I think this works is just so slightly short of passing my threshold for acceptance due to the lack of exploring more on its main contribution, which is the RFCT loss, and potentially giving more valuable information to the community on the synergies between old and new features in the model upgrade process. \nMoreover, the slight overclaim in the \"Regression-Free\" naming requires some extra attention. \nTherefore, I am recommending \"5: marginally below the acceptance threshold\" now, but this decision is marginal and I am willing to consider boosting the score if the authors address my concerns well in the rebuttal.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a new technique for updating image retrieval models while not requiring full backfill of the index in order to allow deployment. This has the potential to make model upgrades more practical for real-world large-scale applications. The paper is the first to study the impact of such “hot-refresh” techniques to the image retrieval domain. The authors make observations about the reason behind negative flips, which motivate their newly-proposed loss function / training technique. The paper also proposes an uncertainty-based backfilling method to decide the ordering in which images in the index are backfilled. Experiments are conducted on standard image retrieval datasets.",
            "main_review": "Strengths:\nS1) First to study the impact of such “hot-refresh” techniques to the image retrieval domain, relying on standard datasets in this area.\nS2) The observation about the reason behind negative flips is novel and interesting – it had not been discussed in previous work. The proposed training method is well-motivated from this observation.\nS3) The proposed training method achieves improved performance in the hot-refresh application compared to previous work, reducing the penalty incurred by negative flips.\n\nWeaknesses:\nW1) The uncertainty-based backfilling approach does not consistently show performance improvements. I am not convinced by this part. In some cases, random backfilling even performs better. The proposed uncertainty-based backfilling method overall feels simplistic.\nW2) Writing could be improved. For example:\nPage 4, “We figure out the reason for the negative flips (...)” – the writing here is a little informal. This part could also be rewritten for more clarity / better explanation. It took me some time to parse why this was the only reason for negative flips – this should be explained better.\nPage 8, in section 5.2. The paragraph mentions the figures, but does not discuss the results shown in them at all.\nW3) The method still shows room for improvement. For example, the lack of pre-sampler to include positive image pairs in the same batch probably hinders performance (the authors justify the lack of that due to it being “hard”, although this is common in deep metric learning, eg for the triplet loss).\n\nMinor details:\nM1) Page 5, below Eq 7: “denotes the cross-entropy” — I think the authors here mean softmax + cross-entropy, right?\nM2) The Oxford and Paris dataset versions used are more commonly referred to as Revisited Oxford (ROxford) and Revisited Paris (RParis). The renaming would make the dataset usage more clear for the readers.\n",
            "summary_of_the_review": "This is an interesting paper, targeting an important application, proposing a novel observation and method to improve hot-refresh image retrieval. However, some aspects of the paper are not well-developed (see weaknesses listed above). Due to the latter, my rating is “marginally above the acceptance threshold”.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}