{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The proposed method, Differentiable Symbolic Execution (DSE), addresses the safety of learned navigation and control programs. The approach samples code paths using a softened probabilistic version of symbolic execution,  constructing gradients of a \"safety loss\" along these paths, and then backpropagating these gradients through program operations using RL. \n\nPros\n - The paper is well-written and sound\n - The issue of safety is underexplored\n - The method improves over a strong baseline on benchmarks\n\nCons\n - The benchmarks are relatively small-scale and artificial"
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents an approach to learn worst-case safe parameters for neurosymbolic programs (programs with neural networks + symbolic portions). The key contribution is an algorithm to compute gradients for the worst-case safety loss by symbolically sampling control paths in the programs and using a modification of the Reinforce estimate to approximate the gradients. The approach is compared with DiffAI on several synthetic tasks. ",
            "main_review": "**Strengths**\n\nThe paper is well written and proposes a mathematically rigorous algorithm to compute gradients in the presence of discontinuous conditionals. The formulation looks sound to me. The need to take gradients through symbolic components arises in many other scenarios, so I am excited to see this idea explored beyond the context of this work.\n\n\n**Weaknesses**\n\nI have several concerns regarding the current evaluation setup. \n\t\n— In my opinion, all of the tasks (even the thermostat, ac, racetrack) are synthetic. In particular, I am calling them synthetic because the data to train these models are artificially generated. Thus, the lack of a real-world application of this technique is one of my main concerns. \n\n— I am not convinced about generating the training data from a manually written “safe” program. In these safety-critical domains, one main challenge is the tension between achieving good loss and achieving safety. That seems to be missing in this paper (except probably for the racetrack benchmark).\n\n— These benchmarks actually seem like reinforcement learning problems rather than supervised learning problems. It is not clear to me how the current approach would work in a reinforcement learning setting. Moreover, there are several safe RL works [1, 2, 3] that might form as good baselines to compare this approach to. And these safe RL approaches don’t need to take gradients through the environment code. \n \n— Another potential issue with optimizing through symbolic code is the presence of numerous minima. This issue is a much bigger issue than just being able to take the gradient. I guess that the authors didn’t encounter this local minima issue because of the supervised data (which is collected from a safe program). More realistic benchmarks might reveal this problem. \n\n**Other comments**\n\nWhy is safety not measured on concrete trajectories during the evaluation? From figure 4, most of the concrete trajectories from the baselines look safe, but the symbolic trajectories are not safe. \n",
            "summary_of_the_review": "The evaluation is not satisfactory as it is very synthetic and seems to be tailored to benefit the presented approach. Therefore, I lean towards rejecting this paper. \n\nUpdate after the author's response: I am happy with the additional experiments and increasing my score to a 6. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes DSE, an approach to optimizing parameters of neurosymbolic programs (programs with mixed neural and symbolic components) while satisfying a safety constraint. To accomplish this, DSE defines a *safety loss*, a nonnegative term in the loss function which is nonzero when the function does not satisfy the safety constraint. This safety loss is constructed by symbolically executing the program, executing discrete transitions probabilistically according to a uniform distribution over concrete states. The paper evaluates DSE on a set of small-scale case studies, showing that in programs with discrete control flow, DSE can often train neural networks that soundly lead to safe behavior, with similar quality of learned results as those of a neurosymbolic training approach which does not take into account safety.",
            "main_review": "## Novelty and Significance\n\nThe proposed approach is both novel and significant. There is an emerging field of approaches for developing and verifying such neurosymbolic programs with safety constraints. DSE represents an advance in that literature which, depending on how well the technique scales to other programs, could emerge as an influential approach in this literature.\n\n## Correctness and Clarity\n\nTo the best of my knowledge, the approach presented in the paper is technically sound. My main qualm with the correctness of the approach is in the lack of discussion of the implications of sampling based on volume. Specifically:\n- Sampling based on volume:\n  - The definition of $V_\\theta$ near the top of page 4 is relatively loose. Specifically, it seems $V_\\theta$ must consist of shapes with differentiable (w.r.t. $\\theta$) and non-zero volume. The choice of intersection of intervals seems to be a sufficient choice (it doesn't seem to allow for the full generality, but that's fine), though it does leave open the possibility of zero volume (e.g., if the shape is lower dimensional than the full space [such as a plane](https://math.stackexchange.com/questions/1697067/lebesgue-n-dimensional-measure-of-a-hyperplane)).\n  - In addition to clarifying these points (or correcting me if my understanding of this is wrong), the paper should discuss the case of identically zero volume symbolic states, since this would cause the probability to become undefined.\n  - It would also be helpful to see some explicit analysis of the volume and sampling based approach, especially as volumes can behave unintuitively in high dimensions. The paragraph in the middle of page 5 (\"Note that a low value ... employed during learning\") addresses some of these questions, but the evaluation does not show the probabilities along trajectories induced by this volume-based approach.\n\nI am less convinced of the correctness of the empirical evaluation (corresponding to the final contribution in Section 1). This is due to a combination of the both the benchmarks chosen as well as the evaluation of the results.\n- The approach is only validated on programs with control flow, on which DiffAI was not evaluated. DiffAI+ is not a published system, so it's hard to reason about what to expect its performance to be. The paper would be significantly stronger if either the authors compared against a system with well-understood behavior on the given task (i.e., some other neurosymbolic network training system with safety constraints) or the authors additionally evaluate DSE in a setting that DiffAI is validated to perform well in (i.e., on programs without branches).\n- The approach is only validated on small-scale programs, using a manual heuristic of splitting the input space into 100 subregions. How was this heuristic decided upon? Does the approach still work when not splitting, or splitting at a finer granularity? Does the approach work on longer programs (with more than 20 lines of code) or when using different neural network architectures? Though of course I don't expect the technique to scale perfectly, it would help to understand where the technique breaks down rather than showing only benchmarks on which it gets perfect or near-perfect results.\n- There is also little discussion of the actual results. For instance, there is no analysis of what characteristics of the benchmarks allow DSE to achieve perfect of less-than-perfect results. As a minor point here, the paragraph at the bottom of page 7 also claims that DSE is able to successfully learn pattern2 while DiffAI finds pattern4 hard; however, actual gap in results between the two benchmarks is relatively small (0.58 v.s. 0.78) which makes the claims of success and failure feel arbitrary.\n- The paper doesn't show C^# for many problems, meaning there is something of a disconnect between the main body of the paper and the results. Figure 9 shows this over the course of training, though for a tiny dataset size. I would be interested in seeing this for all data points (potentially just the final value). I'm also somewhat confused by DSE's behavior on the racetrack and the thermostat. Is DSE's loss conservative? My understanding was that it is not (that C^# > 0 means that the program is not safe) but both the Thermostat and Racetrack show some variance still, implying to me that the program is unsafe.\n- The paper also doesn't show Q^# over the course of training. This especially hampers the understanding of the baseline results -- for instance, why is it that Ablation has consistently high test data loss when it's trained with no constraints? Is this just an issue with generalization?\n\n\nMoreover, the paper (both approach and evaluation) is not sufficiently clear, such that it would likely not be possible to reproduce the results (or even implement the algorithm) based on the description in the paper.\n- Clarity of approach:\n  - I'm not super familiar with the prior work, but why convexify the program space? Algorithm 1 shows that this is essentially an iterative algorithm that often will repeat prior steps (with a different value for lambda). Please correct me if I'm misunderstanding something about this.\n  - Top of page 5: it would be helpful to define volume here, since it's not immediately clear that the formulae describe shapes (on first pass, I thought that this meant something like the count of satisfying assignments).\n  - This is a minor point, but it would be great if the entire set of terms, types, and definitions in the formalism were collected in one place (e.g., as a table in the appendix). I found myself continually having to scroll back and forth and search to find definitions.\n\n- Clarity of evaluation:\n  - Many details of the evaluation are left out. Specifically these include the details of the \"ground-truth program\", the specific loss functions (i.e., the definition of Q^# and C^# for each approach), the meaning of the error bars in Figure 3, the training procedure for the neural networks (i.e., learning rate, convergence criteria, etc.), the approach for interleaving the training of the networks, and the actual architecture of the neural networks.\n  - It is also very hard to understand the plots in Figure 4 -- my understanding of the symbolic trajectories is that they should encode intervals of state, but the trajectories seem to be individual lines. It also seems like the symoblic trajectories for the Thermostat allow for the possibility of failure, while Figure 3a seems to show that DSE is verifiably correct.\n\n\n# Response to author response\n\nThanks to the authors for the very detailed replies. Based on these clarifications and new results, I plan to raise my score to an accept (under the assumption that these clarifications are integrated into the paper).\n\n## Volume calculation\n\nThanks to the authors for the clarification here. To confirm my understanding, the description in the paper:\n\n> let Vol($V_\\theta$) denote the volume of the assignments to X that satisfy $V_\\theta$\n\nis incorrect: for instance in Example_volume, the volume of the assignments to X that satisfy $V_\\theta$ is zero, because res=0. However, the example shows that the volume calculation is not actually performed over the entire program state X, and instead is only calculated as the fraction of the range of the singular variable in an if condition.\n\nIf this is the case, the authors should correct this description for the final version of the paper. Regardless, the clarification has convinced me that the authors have thought this through and that the approach is sound.\n\n\n> Could you please clarify further what you mean by “volumes can behave unintuitively in high dimensions”? We are not confident that we fully understand the question.\n\nSorry for including such a vague statement without an example/citation, I should have been much more clear in the original review. I meant the examples along the lines of the majority of the volume being near the surface of high-dimensional shapes (so e.g. if a vector `x` of length 100 had all elements $\\in [0, 1]$, then a branch that checked that all elements were less than 0.9 would likely never be sampled) and that the volume of the unit n-ball tends towards zero. I was curious to see the actual volumes computed by the technique, to understand whether the technique was falling prey to any of these degenerate behaviors.\n\n## Scalability\n\nThanks for these results -- I would still be interested in seeing results where DSE falls apart even with more data (10,000 does not seem like a particularly large dataset by ML standards, especially considering that this data is synthetically generated, so the fact that DSE still works in this domain gives the impression that these are still small-scale problems), but I'm overall satisfied by this presentation which shows the dimensions on which scalability can be a concern and the knobs to turn to address them.\n\n## Additional experimental details and results\n\nThanks to the authors for the additional details on the programs, Q, C^#, and the additional analysis. This additional data and analysis seems to be sufficient for readers to much more completely understand the experiments performed, their results, and the comparisons between DSE and the baselines.\n",
            "summary_of_the_review": "Weak reject. Though the approach is novel and significant and seems correct, the clarity of the description of the approach and clarity and correctness of its evaluation are lacking. I would be willing to raise my score if the authors provided a much more clear description of the algorithm and its evaluation such that it would be possible to reproduce the majority of the results based on the paper alone. I would be willing to raise my score further if the authors provided a more thorough evaluation of the scalability of the approach beyond just neural networks of different parameters (i.e., programs of different static and dynamic lengths, more or less branches, different input sizes and splits, and different neural network architectures).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper targets the problem of learning parameters of programs that involve both neural and symbolic components, with the objective that the program is guaranteed to be safe. This objective is challenging because it is not differentiable. The proposed method uses symbolic execution to finitize the execution trajectories and uses sampling with REINFORCE-styled algorithm to optimize for an approximated objective. Results show that although the proposed method is not guaranteed to be sound, it in practice produces programs that are safe on the example benchmarks.\n",
            "main_review": "This paper is well-motivated and targets an important, forward-looking problem: how to learn neurosymbolic programs that are worst-case-safe. The proposed technique of combining symbolic execution with sampling  and REINFORCE-styled learning is interesting. And the experiments show that the proposed method is effective on the benchmarks considered in the paper. I have a few questions regarding the method, which I put below.\n\nStrength\n- This paper is well-motivated, targeting an important, forward-looking problem\n- The proposed technique is interesting\n- Experiments show that the method works well on the benchmarks considered, and also significantly outperforms a prior approach \n\nWeakness/questions\n- How are the symbolic states $\\sigma$ generated, in particular, the logical formula V’s? Are they human-specified? Seems that they are the key to reduce the trajectory space to a finite set of symbolic trajectories, but I did not find how they are generated in the paper.\n- It seems that the proposed REINFORCE-based algorithm essentially works by pushing the parameters of the neural models so that they will only trigger the safe symbolic trajectories. If that is the case, the following simpler method seems to work as well: starting with the set of symbolic trajectories, first determine the subset that are safe, convert the subset into constraints on input/output of the neural model, then train the model directly to satisfy this constraint (by penalization). I would like to see the authors compare to this baseline, or discuss why this would not work.\n- In page 3, it was mentioned that the proposed method assumes that the program contains only one learnable neural network. Why does the method require this constraint? This seems to limit the applicability of the proposed method. \n",
            "summary_of_the_review": "This paper is well-motivated and targets an important problem. The proposed method is novel and interesting. However, I have some questions regarding the justification of the proposed method and the limitations on its applicability. Overall, I am slightly leaning towards acceptance, but would like the authors to address my questions before I can solidify my recommendation.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}