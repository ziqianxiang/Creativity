{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper introduces a novel quality-diversity algorithm, \"Evolutionary Diversity Optimization with Clustering-based Selection (EDO-CS)\", and applies it to reinforcement learning. A bandit approach (UCB) is used to select which cluster to sample parents from. The QD algorithm can be evaluated on its own, outside of the RL context, and if so it should be compared to the several approaches to niching and other standard diversity preservation approaches in evolutionary computation that rely on clustering. (And the authors should make an effort to connect to the niching literature in particular.) However, the use of the algorithm for RL makes it possible to use behavioral features as the space in which to cluster, separating it from standard diversity preservation methods. The resulting algorithm is relatively simple and the empirical results are good.\n\nSome of the main concerns for reviewers included the bibliography, which the authors promptly acted on by citing several suggested papers and comparing their approach where relevant. There was also discussion about the exact novelty of the paper, for example as compared to the CVT-MAP-Elites algorithm, but this was clarified by the authors. Reviewers agree that the paper is easily to follow and well-written.\n\nBased on this, it seems that the paper makes a clear contribution to QD methods for RL, and is worth accepting."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors present a new method for finding robust policies in reinforcement learning, those policies which give high rewards but also come from a diversity of behaviours. The authors accomplish this by using a clustering algorithm to first divide policies into relevant clusters and then use evolutionary algorithms to optimise policy choice. The result is an algorithm that performs well over several representative benchmarks, achieving SOTA performance with high convergence rates. ",
            "main_review": "The paper gives clear motivation to the problem of optimising for quality and diversity in reinforcement learning and clearly defines relevant terminology throughout. It is easy to follow and the conclusion are well supported by the results. \n\nMore specifically to the method presented, the authors investigate the applicability to clustering methods for selection of policies in evolutionary strategies. This is an interesting area of study and the authors present a convincing method that performs well on several toy problems. There has been some application of evolutionary strategies for selection of policies in the field but this is the first application of K-means clustering to my knowledge. One of my initial criticisms when reading was the impact of hyper-parameters (cluster size, clustering algorithm, archive size, etc) but was pleased to see that the authors have considered this in detail and presented relevant results on this within the appendices. \n\nThe central comment I have on this paper is it would be interesting to see how their method navigates between clusters and how representative these clusters are of behavioural strategies. Evolutionary algorithms are well known for falling into local minima given poor selection of mutation rates and population size. As I understand the paper, the clustering aspect addresses this by ensuring a wider range of policy-space is sampled. Is it possible to quantify/comment on whether the authors method is achieving this? Similarly, I naively assume that cluster separation will be arbitrary where the number of clusters is larger than the number of distinct behaviours. I would be interested to see how well the clustering algorithm performs at identifying behaviours – for example, a demonstration that policies sampled from the clusters do indeed lead to diverse behaviours when examining those policies that were ranked highly but distinct. \n\nFinally, there are some minor recommended typographic edits. In several places, method initials are given without prior statement (for example, ME-NS and ES). The colour and size of points in Figure 1 is also not extremely clear, especially when trying to identify Pareto/Clustering/Both/None in Fig.1b. \n",
            "summary_of_the_review": "Overall, an interesting and well thought through investigation into the role of clustering when selecting in evolutionary strategies for reinforcement learning. The results give a clear indication that clustering can greatly improve results under certain settings, with good attention given to the impact on changes method parameters such as clustering algorithm and size. However, more needs to be done to show explicitly that those selected policies do end up being more diverse in behaviours. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors present EDO-CS an approach to multi-objective optimisation that ensures the high quality and diversity of generated RL policies. Instead of uniformly sampling solutions from the Pareto front, sampling policies from learned clusters is introduced, as a selection mechanism within the ES optimisation. The quality and diversity are further achieved by modifying the objective function of the ES algorithm so it includes both the fitness and behaviour-diversity terms, balanced with a hyperparameter $\\lambda$. This hyperparameter is set using a multi-arm bandit approach, which is adapted during the training. \nThe performance of the proposed approach is evaluated on several MuJoCo continuous control tasks, as well as compared to state-of-the-art QD benchmarks.\n",
            "main_review": "\nThe paper is well-written with a clear goal developing a method that optimises for diversity in addition to fitness. The main contributions include sampling policies and a novel approach to adapting the optimisation function.\n\nHowever, I have several concerns:\n\n* The definition of a behaviour is not clear from the beginning and should be defined earlier. There seems to be a misalignment between the use of behaviour diversity in the proposed approach (mostly for exploration) w.r.t. the approaches in the literature (to maintain a diverse collection of policy behaviours). It would be useful to discuss how the proposed approach relates/differs from the two main approaches in QD:\n\n  - Joel Lehman and Kenneth O Stanley. Evolving a diversity of virtual creatures through novelty search and\nlocal competition, GECCO, 2011 \n  - Jean-Baptiste Mouret and Jeff Clune. 2015. Illuminating search spaces by mapping elites. ArXiv, 2015\n  - Antoine Cully, Jeff Clune, Danesh Tarapore, and Jean-Baptiste Mouret. Robots that can adapt like animals. Nature, 2015\n\n* If I understand correctly, the clustering is done in the policy parameter space rather than in the behaviour space. This leads to the diversity in policy parameterisation but not necessarily to behaviour diversity. This then also raises a question what is the point of maintaining these clusters as opposed to sampling based on distance. Another drawback I see is that there is no explicit maintenance of diversity within the archive, and thus no way of sampling diverse behaviours.\nOn this note, it would be beneficial to provide some additional reasoning behind this. Moreover, there are several recent works that discuss using learned representations or manifolds in order to obtain a more grounded notion of distance in the policy parameter space, which might be useful in this work:\n  - Nemanja Rakicevic, Antoine Cully, and Petar Kormushev. Policy manifold search: Exploring the manifold hypothesis for diversity-based neuroevolution. GECCO, 2021\n  - Adam Gaier, Alexander Asteroth, and Jean-Baptiste Mouret. Discovering Representations for Black-box Optimization. GECCO, 2020\n  - Vassilis Vassiliades and Jean-Baptiste Mouret. Discovering the elite hypervolume by leveraging interspecies\ncorrelation. GECCO, 2018\n\n* I think an ablation study to show the contribution of the MAB selection of  $\\lambda$ would be beneficial, as this seems to be an important contribution. This ablation would further strengthen the point made about optimising for diversity.\n\n\n### Some questions:\n* What are the results that are reported, of the best performing policy in each iteration?\n\n* The total number of iterations in the deceptive task is 600, does this mean the total number of policies evaluated in the environment?\nSection 4.3 distinguishes number of interactions with the environment vs number of iterations. This is not clear from Algorithm 1, and would be useful to clarify. The presented number of evaluations is significantly less than what is usually needed for ES-based approaches.\n\n\n### Minor comments:\n\n* It is not clear what does \"EAs type\" in Table 1 mean exactly. In the text it says “how many parents and offspring the algorithm will maintain in the population” but I find it not clear what this means.\n\n",
            "summary_of_the_review": "- Interesting approach to maintaining the policy clusters and adapting the objective function\n- Needs to improve the context in the literature, w.r.t. the use of diversity of behaviours\n- An ablation study to show the contribution of the MAB for objective function adaptation, would be beneficial\n\nEDIT: I have increased my Empirical Novelty And Significance score, and conditioned on the additional results added to the camera ready I would be willing to increase my recommendation.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In the paper \"Evolutionary Diversity Optimization With Clustering-Based Selection For Reinforcement Learning\", the authors introduce a new selection mechanism for Quality-Diversity based algorithms. This new selection mechanism is based on the K-Means algorithm, to cluster the behaviour space into cells and then select only the best policies within each cell. The paper also uses a linear combination of the reward and novelty score when performing the policy updates. The weights the of the linear combination are automatically adjusted using a bandit algorithm so that the best reward is obtained. ",
            "main_review": "The experimental evaluation demonstrates the benefits of the proposed methods on a variety of domains showing different numbers of modalities and complexity of the behavioural space. Multiple existing algorithms are used as baselines, which is great. \n\nOverall the paper is very clear and easy to read and understand. It is well structured and nicely illustrated. \n\nUnfortunately, while it is a great paper, it misses some key references in the literature which are making the novelty claims less obvious and inviting additional experiments.\n\nFirst of all, the concept of dividing the behaviour space into cells and only selecting the best solutions is exactly the purpose of the grid-based archive in MAP-Elites algorithms[1], probably the most famous QD algorithm in the literature. Combining MAP-Elites with a K-mean to automatically partition the behaviour space was firstly introduced in CVT-MAP-Elites [2] and its extension Cluster-Elites [3]. Biasing the cell/cluster selection based on the reward/fitness was first studied in [4] (without really demonstrating improvement over a uniform selection, when quality and diversity of the archive is considered). \nMoreover, the comparison between selection from the archive, or from a separate population or a Pareto-front, was also studied in [4] reaching the same conclusion as in the proposed paper: archive-based selection outperforms population or Pareto-front ones.\n\nIt is important to come back on the fact that the grid of MAP-Elites already performs a form of clustering and filtering to keep only the best policies in each cell of the grid. Given that ME-ES (which stand for MAP-Elites-ES) uses such a grid, I found the statement \"only selecting one policy in each iteration [used by ME-ES], may cause only a specific behaviour to be well optimized and can not return a set of high-quality policies with diverse behaviours.\" confusing as the grid is designed to maintain a diversity of behaviour and improving them in parallel. \n\nIn my opinion, the proposed selection mechanism is not novel and directly linked to the archive management introduced in MAP-Elites and its extensions. \n\nThe paper should also consider PGA-MAP-Elites[5], which combines CVT-MAP-Elites with policy gradient (TD3) for the evolution of NN policies and outperforming the best baseline (QD-RL) presented in this paper. Given that PGA-MAP-Elites uses a selection mechanism that is linked to the one proposed in this paper and shows better performance than QD-RL and other baselines, I think it is an important baseline to consider. \n\nAdditionally, it seems that the authors of the QD-RL paper are still trying to publish the paper and made several updates to the paper. https://openreview.net/pdf?id=8FRw857AYba\nOne of them is the use of the grid from MAP-Elites. This change was most likely made because the authors found similar findings to the one presented here. I do not think it is fair to request the authors of this paper to be aware and to consider a paper that is still under review somewhere else. Therefore, I am not considering this part in my assessment of the paper. Yet, I thought it is important to point the authors to this new version of the paper so that the claims made here will not be outdated in a few months. \n\nOther, smaller comments: \nThe use of the best reward makes sense from a deep-RL point of view. However, the sum of the reward of all the policies in the collection (also, called QD-score) is more frequently adopted in the QD literature. It would be good to add a few sentences justifying this decision and how it can affect the baselines' performance. \nIn the multi-modal environment, how is computed the Mean rewards? Is it the mean reward of the two best policies (one per modality), or something else?\n\"Figure 6: Archive size\" does not seem to report archive size. \n\n[1] Mouret, J. B., & Clune, J. (2015). Illuminating search spaces by mapping elites. arXiv preprint arXiv:1504.04909.\n\n[2] Vassiliades, V., Chatzilygeroudis, K., & Mouret, J. B. (2017). Using centroidal voronoi tessellations to scale up the multidimensional archive of phenotypic elites algorithm. IEEE Transactions on Evolutionary Computation, 22(4), 623-630.\n\n[3] Vassiliades, V., Chatzilygeroudis, K., & Mouret, J. B. (2017, July). A comparison of illumination algorithms in unbounded spaces. In Proceedings of the Genetic and Evolutionary Computation Conference Companion (pp. 1578-1581).\n\n[4] Cully, A., & Demiris, Y. (2017). Quality and diversity optimization: A unifying modular framework. IEEE Transactions on Evolutionary Computation, 22(2), 245-259.\n\n[5] Nilsson, O., & Cully, A. (2021). Policy gradient assisted MAP-Elites. In Proceedings of the Genetic and Evolutionary Computation Conference Companion.",
            "summary_of_the_review": "Overall the paper is very clear and easy to read and understand. It is well structured and nicely illustrated. \nUnfortunately, while it is a great paper, it misses some key references in the literature which are making the novelty claims less obvious and inviting additional experiments.\n\n---- \nUpdated score after discussion below. \n----",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper performs research in the area of diversity/novelty-seeking agents under evolutionary optimization approaches. The core idea in this area is to optimize a weighted combination between raw environment reward and diversity during training (even though evaluation is still on the raw environment reward only). The diversity acts as a \"regularizer\" in order to achieve better exploration, which can lead to higher rewards.\n\nThe specific approach (EDO-CS) by the paper suggests using a pool/\"archive\" $A$ of previous policies, which are then clustered via K-means according to the distance metric defined by the behavior function $b(\\cdot)$. Each cluster then produces a new policy (and thus there are a total of $K$ new policies), which are then updated via ES over a regularized reward (via diversity metric, along with an adaptive weighting $\\lambda$), and put back into the archive. \n\nExperiments are performed over various continuous control benchmarks for testing exploration, and results show that EDO-CS achieves SOTA.",
            "main_review": "\nStrengths:\n* Experiments are quite comprehensive, having performed comparisons over many different diversity algorithms. The results show that EDO-CS achieves better performance over all other baselines. The environments used are also quite diverse, ranging from AntWall, to multi-modal environments, and finally single-modal environments.\n* Multiple ablation studies are performed over the method, including varying components such as the clustering algorithm used, number of ES update iterations, population size, and archive size. This should give practitioners better idea of how to tune the method.\n\n\nWeaknesses:\n\n* Some presentation issues throughout the paper. \n  * In Figure 1(a), the dots should be made bigger and colors should be more visible and contrasting, or there should be a way to denote the region highlighted by e.g. blue or red dots. This will allow the conclusion to be much more obvious that Pareto/Clustering-selected points allow more multiobjective dominance / higher rewards, as currently it is fairly straining to the eyes. Similar issue occurs in Figure 1(b), where I had trouble understanding what is going on. What does \"quality\" (the heatmap metric) mean here? What should I be looking at? It may help to define these terms like \"quality\" first, which means it might be best to move Figure 1 after notation has been established in Section 2. \n  * Could you explain why we're using a bandit approach for $\\lambda$? If I'm not mistaken, normally multi-arm bandit scenarios involve a stochastic reward function defined by a probability distribution. Can you explain, what is the \"reward function\" for $\\lambda$? If it's $J(\\theta)$, isn't $\\theta$ moving and thus \"$\\lambda$'s reward is nonstationary?\" I'm fairly confused about the involvement of bandits in this scenario. \n  * Slightly awkward English that could also be cleaner and more precise with the use of mathematical notation. For instance, Section 3.2: \"Then, we select one specific policy from each cluster. More specifically, from the cluster where the best quality policy locates, we just select this policy.\" This sentence could be made more precise by using the notation defined previously (e.g. cluster $C_{i}$, policy $\\pi_{\\theta_{i}}$). This was an issue throughout the paper, where there were too many ambiguous references. \n  * A less important issue is in formatting, where there is a gap in space at the bottom of each page that should be fixed. Grammar, spacing, and \\citep vs \\cite issues should be fixed throughout.\n\n* Somewhat shaky motivation for the clustering heuristic. \n  * The real end goal of the clustering procedure, is to really find $K$ policies which are behaviorally different from each other. The rest of the clustering doesn't ever actually get used (please correct me if I'm wrong). If this is the case, why is clustering the best way to find $K$ diverse policies? Normally, clustering algorithms use an entire cluster for some purpose (e.g. see applications in [1]). Why not optimize an explicit diversity metric that measures the behavioral difference between the $K$ policies? I admit, I have seen a variant of the clustering approach (termed speciation) in NEAT [2], but it would significantly strengthen this paper if there was a better motivation + explanation for the use of the clustering heuristic. \n  * Another way to explain this better is to perhaps visualize the clusters in terms of behaviors, throughout optimization, on a 2-d plot (since some of the Mujoco benchmarks conveniently define $b(\\cdot)$ in terms of xy-locations).\n\n\n[1] https://en.wikipedia.org/wiki/K-means_clustering\n\n[2] Evolving Neural Networks Through Augmenting Topologies (2002)",
            "summary_of_the_review": "While the paper produces experimentally strong results, it lacks motivation (especially for the clustering heuristic) as well as certain components such as the use of multi-armed bandits for adaptively tuning $\\lambda$. One danger to this lack of motivation is the introduction to potential confounding factors (e.g. what if the clustering heuristic is implicitly optimizing an ideal diversity objective, and it's better to optimize this ideal diversity objective instead?). \n\nThere is also a nontrivial amount of presentation issues (not just small grammar mistakes or typos) that affect the paper's scientific quality as well.\n\nI am willing to increase my score if the authors fixed the core issues outlined above.\n\nEDIT: I have increased my score to a 6 as well as my confidence scores. The authors have addressed my questions and concerns.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}