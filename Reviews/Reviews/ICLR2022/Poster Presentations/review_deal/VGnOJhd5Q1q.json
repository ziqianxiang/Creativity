{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper adds to the literature of efficient sparse attention for long-range transformer architectures: a learned hash function is proposed by building successfully upon contributions from previous work. A similar idea appears in contemporary work, but with clear and complementary differences.\n\nThe reviewers are convinced of the importance of attention complexity, bucket imbalance issues, and agree that learning-to-hash is is a promising solution. The authors have clarified almost all outstanding concerns, in some cases adding valuable new results (e.g. timing experiments.)\n\nI echo the reviewers' concern and stress to the authors to clarify the precise meaning of \"plug-and-play\", as it may be misinterpreted (e.g., no fine-tuning needed? or just no change to model but still fine-tuning is needed.)\n\nSome of the cited papers are accepted at conferences, please update your bib file with the correct information for credit attribution."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper introduces a new method based on learnable hash functions to reduce the O(N^2) cost of self-attention in transformers to O(N^1.5). As previously known for bucket-based approaches, this cost is only achieved when buckets are balanced. The paper investigates the effectiveness of related approaches regarding bucket imbalance issues by showing statistics for several attention heads of a pre-trained transformer.  A precise metric is designed to quantify this notion of efficiency (\"attention utility\"), and later this metric is optimized by learning separate parameterized hash functions for queries and keys. To be able to optimize this metric, the authors use the unbiased approximation to the softmax function proposed by Performer (Choromanski et al., 2020) as a regularization term. Experiments on several NLP and CV tasks show that the proposed method achieves better results than previous fast-transformers while being faster than a standard transformer.\n",
            "main_review": "Pros:\n\nI really enjoyed reading this paper. It starts by departing itself from related works by investigating the critical issue of not having balanced buckets, showing that this characteristic is a common pitfall of previous popular works. Therefore, the foundation is clear and has practical implications.\n\nMoreover, quantifying the degree to which an approximate self-attention can imitate the full attention is very informative and might guide future work on fast transformers. The authors acknowledge that computing this number is an NP-hard problem, but they go further and try to optimize this metric during training. \n\nThe idea of having a learnable function prior to bucketing is not entirely novel (see a contemporary idea by Treviso et al., 2021), but the formulation is concise and clear, enabling generalizations to previous approaches.\n\nThe experiments span a large set of tasks in NLP and CV with increasingly large input lengths. LM experiments show that the proposed method (trainable LSH) outperforms Reformer (untrainable LSH) on multiple settings. Results on the GLUE benchmark show that the proposed method achieves results on par with the original RoBERTa model, which is a good sanity check. The experiments on Long Range Arena give the final flavor to the paper, with the proposed method outperforming related works on all considered tasks while being cheaper (higher throughput) to train than a standard transformer.\n\n\nCons:\n\nHowever, I was also able to spot some concerns in this paper. Concretely:\n\n- The core of the proposed method relies on optimizing the attention utility, which relies on the Performer's softmax kernel to regularize the training regime. The paper opts to use a KL between the softmax kernel and LHA to that end. It would be interesting to see how this KL term evolves during training.\n\n- The choice of 10 heads from the 3rd layer in section 3.2 looks arbitrary. In fact, it is widely known that different heads at different layers focus on distinct tokens. Thus, what is the motivation for that choice? Moreover, from which pre-trained model were the results extracted? This is not clear in the paper.\n\n- Figure 1 shows the bucket size of queries and keys for each attention head using LSH attention. Since LSH attention was originally designed to use tied queries and keys (i.e., queries = keys), how can queries and keys have different bucket sizes? Also, it is unclear if queries and keys were tied for all experiments using LSH attention in the rest of the paper.\n\n- Which setting regarding the number of attention heads and layers was used for experiments on the GLUE benchmark and LRA? That is, how many layers and heads were set to use LHA?\n\n- In section 3.2, it is said that \"queries should attend to enough keys to get a good approximation of the full-attention.\". Is there a precise notion of what is \"enough\" in this context? That is, for recovering the true softmax distribution, queries should attend to ALL keys. In contrast, if we aim to recover an entmax distribution, queries can attend to only a subset of keys (see the sparse-consistency property in Treviso et al., 2021). So, what would change in LHA if we want to approximate entmax instead of softmax?\n\n\nMinor comments: \n\n- While Equation 16 is easy to follow, there is a big jump in Equation 17. \n- In section 4.3, \"Since h_K(K_j) does change for the queries\". Perhaps \"Since h_K(K_j) does NOT change for the queries\"?\n\n\nRefs:\n\nTreviso, M., Góis, A., Fernandes, P., Fonseca, E., & Martins, A. F. (2021). Predicting Attention Sparsity in Transformers. arXiv preprint arXiv:2109.12188.\n\n\nUpdate: \n\nI thank the authors for their responses and efforts to improve the paper. The newly added experiments alongside the discussion addressed my concerns. I believe this paper provides a new angle to study the efficiency of bucket-based methods, proposing a method that overcomes issues found in previous approaches. Therefore, the paper presents a step forward for this field, and I recommend it to be accepted.",
            "summary_of_the_review": "Overall, I think this paper is well-written, has a clear and practical motivation, and provides an elegant solution to the quadratic bottleneck issue in transformers. Most of my concerns are about the lack of correctness in some parts of the paper, such as choosing hyperparameters or evaluating LSH attention. However, the authors can easily address these concerns in the rebuttal period. Therefore, I am learning towards acceptance. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a learning to hash attention (LHA) to learn sparse attention for Transformer.\nThe proposed LHA addresses the limitation of ANN-based sparse attention method by separate learnable hash functions for queries and keys and utilizes kernelized techniques for efficient approximation of attention utilities. \nExperiments on several applications validate the effectiveness of the proposed LHA.",
            "main_review": "### Strengths:\n* The attention sparsification is important to reduce the complexity of Transformer when applied to long sequence.\n\n* The imbalance of bucket size and query-key ratios are studied, which is closely related to efficiently reducing the complexity and performance on down-stream tasks.\n\n* The attention utility is proposed as a metric to measure how well the attention weights are preserved.\n\n* The attention utility is used to train the learnable hash functions since the argmax operation makes it impossible to train the hash functions from the down-stream tasks.\n\n* An approximation of the attention utilities is proposed with random Fourier features to reduce the complexity.\n\n### Weaknesses:\n\n* What is the statistics of the bucket size and query-key ratios for the proposed LHA? Is it significantly better than LSH?\n\n* It is not clear how to formulate the final training objective and balance the loss terms.\n\n* It is not clear how to apply LHA as a plug-and-play replacement for dense attention layers in pre-trained Transformer models. Since the LHA method introduces the learnable hash functions h_k and h_Q, the hash functions should be trained on the target dataset.\n\n* The implementation is inconsistent with the analyze. By using the token sorting method in Roy et al. 2021, the validation of attention bi-clustering is no longer guaranteed. However, the attention utility is meaningful only if the  attention bi-clustering is guaranteed. \n\n\n### Minor:\n\n* The notation of key and query is confused. Q_i should be the query, and a key should use K_i (before (14)). \n\n* To make the paper self-contained, it is necessary to include a brief introduction to the compared baselines in Table 3. Adding a column to show what type of attention sparsification would help the reader to understand the comparison.\n\n* Below(14): is not longer --> is no longer\n\n* Below (16): h_K(K_j) does change --> does not change",
            "summary_of_the_review": "This paper propose to learn separate hash functions for keys and queries with the guidance of attention utilization, which is the metric proposed in this paper. However, the implementation is inconsistent with the analyze and important method details are missing. At this point, I would tend to vote this paper slightly below the bar. If the authors can address my concerns, I would be happy to increase my score.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work studies the content-based sparse attention in transformer and how to improve the self-attention part, i.e., efficiency and effectiveness.",
            "main_review": "The authors figure the bucket imbalance problem in ANN-derived content-based sparse attention and analyze its weakness on the imbalance problem. Attention Biclusteringis introduced to find the optimal attention utility. The learning to hash-based attention model is proposed to improve the effectiveness of sparse attention. Experiments on different applications support their claims.\n\nConcerns:\n1. It is well-known that LSH is data-independent hashing, which is formulated by random projection with some pre-defined metrics. Notably, using learning to hash is better than the LSH and its variants, in most cases, for representation learning. Therefore, the authors need to clarify why replacing LSH with learning to hash models is useful in sparse attention.\n2. Such a simple replacement could fully support your work in the present form. The authors should give full reasons for your contributions.\n3. The authors fail to convince the reviewer what are the connections between Theorem 1 and your proposed method. To me, you can directly claim there are some limitations on LSH-based methods, and implementing learning to hash can improve its representation capabilities on feature learning.\n4. Efficiency is one of the proposed methods, and more analysis on the efficiency and efficacy is necessary.\n",
            "summary_of_the_review": "The authors try to improve some limitaions on the content-based sparse attention when using LSH-produced sparse attention patterns. A Learning-to-Hash Attention is formulated to enhance its model expressiveness. Experiments show its usefulness of the proposed methods. However, there are questionable points that should be clarified.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "See the above Main Review Section.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}