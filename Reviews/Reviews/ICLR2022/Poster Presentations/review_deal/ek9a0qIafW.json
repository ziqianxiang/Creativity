{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper presents a prompt learning method for few-shot learning in NLP.  In particular, they proposed DART, a new soft prompt tuning method, to optimize the label representations and template. \n\nOverall, the paper is well-written and well-motivated. The proposed approach is interesting. The experiments were well justified and sufficient experimental analyses are provided. All reviewers support the paper. \n\nThere are a few remaining critics of the papers. \n\n- The major one is the positioning of the paper raised by the reviewers. I agree with the reviewers that it is a bit misleading to emphasize the approach requires no external architecture. Although the approach can reuse the same transformer architecture (rather than additional LSTM) so that it enjoys the beauty of simplicity, it is still required additional parameters. I would suggest better clarifying this point in the final version. \n\n- There is also a critic that the paper is related to ADAPET. However, the key ideas in this paper are sufficiently different from ADAPET. Also, ADAPET is published at EMNLP 21 after the paper is submitted, although it was in Arxiv earlier. It's fair to say this work is concurrent with ADAPET."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This work reduces the need for prompt engineering for few-shot tasks by optimizing only the word-embeddings of unused tokens in the LLM. Results on multiple datasets showed reasonable results which IMO seems worth sharing to the broader audience. The exploration on what exactly are learned in the prompt was also interesting.",
            "main_review": "Strengths\n- Very clearly written, the key ideas were well explained and simple.\n- The results seemed convincing.\n- As far as I can tell, all related works have been covered.\n\nWeakness\n- I am bit unconvinced about claiming that this work requires no external architecture in Table 1. There are new parameters that are getting trained/fine-tuned before we can do inference. The main difference I see between this and another approach like p-tuning is that the most of the existing architecture is getting reused vs in p-tuning there is explicitly an external LSTM.\n- It would've been nice to see what happens if we constrain the additional tokens to be a part of the same vocabulary. This is more or less the approach taken in AutoPrompt, except with the current training methodology.\n\nQuestions\n- I don't think I quite understood Sec 4.3, based on this section, it looks like there is an explicit embedding of output classes into an embedding? How are these learnt without fine-tuning? If they are learnt from scratch, then how is this different from regular fine-tuning?\n\n- While the work claims that it reduces the need to fine-tune, I am seeing this work as a point on the spectrum between effort-to-tune vs zero-shot. Engineering prompts is truly zero-shot, P-tuning is somewhere in middle that it requires an external LSTM, whereas this approach requires retraining the big model (modulo all parameters except T1, T2.. held constant). Is this fair to say? How computationally easy is it to actually fine-tune these unused embeddings vs learning an external model\n\n- How do we decide the number of unused tokens T1, T2..  to use? \n\n- How much does this work translate to a completely novel task beyond classification? e.g. let's say constituency parsing or dependency parsing task, for which it is possible to engineer a novel prompt.\n\n\n\n\n\n",
            "summary_of_the_review": "Clearly written paper with key insights nicely presented. I had some comments/questions on the key insights, but I don't think it should affect acceptance, unless I misunderstood.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new soft prompt tuning method called DART. The authors claim that training label representations for prompt tuning is important. With trainable label representations as well as prompt ones, auxiliary training task called fluency constraint object. Without the addition of a task-specific architecture, DART outperforms previous state-of-the-art prompt tuning in few-shot performance on 9/14 NLP tasks.",
            "main_review": "Strengths of the paper:\n- Clearly written and easy to follow.\n- A simple, scalable, and effective approach of prompt tuning has been proposed.\n- Importance of differentiable label has been well demonstrated.\n\nWeaknesses of the paper:\n- Missing information: Because the proposed Fluency Constraint Object is based on MLM, GPT-2-medium (in section 5.5 and figure 2-b) cannot use this objective.\n\nComments:\n- In section 4.5, the authors said that their method requires no external parameters. But this claim could be controversial since adding new tokens (for prompts and labels) on the pre-trained language models' token embedding seems more efficient than pre-training language models that already have unused or special tokens in their token embedding. I know many PLMs already have unused tokens in practice, but I would like to treat this claim theoretically.\n- For GLUE tasks, I'd like to know the performance of DART with full training set.\n- Comparing with Prefix-tuning and WARP in experiments would be great. But this is not mandatory.\n- In figure 3 and 4, training iteration or epoch should be presented.",
            "summary_of_the_review": "Recently, there has been heavy interests in prompt tuning such as P-tuning, Prefix-tuning, WARP due to its efficiency in use of large pre-trained language models for downstream tasks. The proposed DART has similar motivation. However, DART is more efficient than previous prompt tuning methods and is on par with them. Especially, focusing on differentiable label would be important observation for the community.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "-",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a new approach called DART (Differentiable Prompt) which can perform few-shot fine-tuning without any prompt engineering (main difference w.r.t. previous works). This is achieved by optimizing the prompt template and the target label with backpropagation. Since the proposed approach doesn’t use any extra parameters, it can be easily used for any pre-trained language models. Further, the empirical evaluations suggest that it does better few-shot learning than previous works. ",
            "main_review": "Strengths:\n\n1) The paper proposes an interesting framework for few-shot fine-tuning without any prompt engineering. This approach can be easily used with other pre-trained models, which makes it a more generalizable framework.\n\n2) The problem space is well defined (and also well contrasted with previous work), and the paper is clearly written and easy to follow. \n\nWeaknesses: \n\n1) Even though the results in Table 3 for relation extraction and event extraction datasets suggest that the proposed methods perform significantly better than LM-BFF, the results on the 10 popular datasets seems to be very close to LM-BFF. From the results, only two datasets (SST-2 and MR) seem to have significant improvement over LM-BFF. It would be great to further discuss these results in detail. \n",
            "summary_of_the_review": "Overall, the approach is technically sound and is also very easy to apply to other pre-trained language models. The empirical results on the 10 popular datasets seem closer to LM-BFF and it’s hard to interpret if the approach empirically performs better than static/fixed prompts.  On the other hand, the proposed method performs much better on relation extraction and event extraction datasets, and it is not clearly discussed in the paper on these variations in the results. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new few-shot learning method for NLP problems by incorporating a simple,effective framework. This method is extensively validation and shows compelling performance.",
            "main_review": "Strengths:\n - The paper is generally well-written, with excellent motivation and empirical setup/analysis\n - The overall strategy of differentiable prompt optimized to maintain fluency is reasonable and novel.\n - The ablation experiments and optimized prompt analysis are insightful.\n\nWeaknesses:\n - The notation/description of section 4 is not immediately intuitive. I had to read it a couple of times before I could fully follow the method. The authors could add more content to figure 1, which may resolve this issue.\n - The choice in deciding how many template tokens are used is unclear. An additional ablation experiment trying different number of tokens to optimize could be illuminating (even for just 1 dataset).\n - The ablation study would be better if specific numbers were provided.\n - I know GPT3 access is hard to get, but I wish experiments with k=8 prompts could be compared against\n\nPost rebuttal: I think another pass for clarity over the paper would be good for the final version, but otherwise I'm happy with the paper updates and I'm happy to see the ablation numbers aren't too sensitive to token count. I have updated my score to reflect this.",
            "summary_of_the_review": "This paper provides a simple, yet effective approach to the few-shot learning problem. While it has a couple minor issues, I think the broader community would find this paper interesting.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}