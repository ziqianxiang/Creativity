{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The reviewers are all weakly positive. The author response clarified important aspects of the paper. The new human evaluation was critical. However, the human evaluation result presentation is flawed: presenting Likert scores as means does not reflect them well. The authors should use something similar to a Gantt chart to fully reflect the distribution across Likert categories. Another detail in the human evaluation that are troubling: it does not reflect interaction with the system, but judgements through observation. Therefore, the human evaluation does not reflect the ability of the learned dialogue system to interact with users. Overall, the paper makes a nice, original contribution, but despite author improvement there are evaluation flaws (even if they are common in papers using these benchmarks)."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes an offline RL method applied to an end-to-end task-oriented dialogue model, where the proposed GPT-Critic is built on GPT-2 and fine-tuned on the self-generated sentences for policy updating. \nThe paper claims that it is free from the issue of diverging from human language (a common issue in standard RL), because it learns from the sentences directly sampled from the pre-trained language model.\nThe conducted experiments show that the proposed model achieves better performance compared to other task-oriented end-to-end dialogue models in both offline and online settings (MultiWOZ and ConvLab respectively).",
            "main_review": "This paper focuses on improving the dialogue policy together with the responses by utilizing a pre-trained language model and offline RL.\nThe claimed contributions include:\n1) The proposed method is free from the common issue of diverging from human language, because it learns from the sentences sampled from the pre-trained LM.\n2) The proposed method outperforms other SOTA models in offline and interactive online settings, MultiWOZ and ConvLab respectively.\n\nThe proposed method is reasonable and moderately novel. The experimental results are promising for both settings.\nHowever, there are unclear parts to be addressed or clarified.\n\n- Because the policy learning procedure utilizes the additionally generated dialogue acts and corresponding responses, it is easy to think that naively fine-tuning the GPT-2 model on the additional generated data may also improve the dialogue model performance in terms of its policy and responses (similar to a data augmentation method). Did the authors try this as another compared baseline? This method should be included in the experiments in order to justify the proposed RL approach is necessary.\n\n- The paper mentioned that the standard RL methods easily fail and generate responses diverging from human language, even when fine-tuning a pre-trained LM. Hence, it will be better to additionally include the results of other standard RL algorithms for better justifying this claim. (Current experiments only include the results of models that are free from this issue.)\n\n- In the experiments in MultiWOZ, this paper only evaluates the response generation results. However, evaluating dialogue policy is also important to justify the learned policy is suitable. It is unclear why the authors only show the response generation results.\n\n- The experiments contain two setups, one is offline response evaluation via MultiWOZ, and another is interactive simulation via ConvLab. Both settings show the better performance of the proposed method. The paper can be better if adding the real-user interactions, because the performance may be different between the simulation environment and the real-user interactions reported by prior results (DSTC in ConvLab). Conducting real-human interactions can better justify the effectiveness of the proposed RL method in practical scenarios.\n\nIn sum, the proposed method is relatively novel and the idea is reasonable. The performance seems promising in both settings.\nHowever, the paper does not include detailed descriptions about the proposed method, making readers not easy to understand.\nAlso, some additional experiments need to be added in order to better justify its claims.",
            "summary_of_the_review": "The idea is reasonable and moderately novel. The conducted experiments demonstrate the better performance of the proposed model in two different settings.\nThe paper misses some details when describing the proposed method, making readers difficult to fully understand its idea.\nAlthough the experiments already include a lot of baselines, there are still some results to be included for fair comparison (simple data augmentation method).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a reinforcement learning-based approach to building a task-oriented dialogue agent. Given a dialogue dataset annotated with rewards, the state-action value function is first trained by minimizing temporal differences. Then, a new training dataset is created by using the best actions selected among the candidates generated by the current policy (i.e., the language model). The policy is then updated by behavior cloning using the created dataset, and the whole process is repeated. The authors have conducted experiments using MultiWOZ and ConvLab and shown that this iterative process improves the performance of the agent.",
            "main_review": "The proposed approach is very simple and should be easy to implement. The experimental results seem promising. However, I do have a couple of concerns.\n\nFirstly, I think that some important details are missing in the paper. For example, what is the action space of the agent? Do you treat the conjunction of a dialogue act and a system response as a single action? If so, how exactly are the candidate actions generated? Is some kind of beam search employed? I think some actual examples of actions (and rewards) would help the reader understand the proposed method more clearly.\n\nThe paper does not really describe how the rewards are computed, either. In particular, I am wondering how the reward for the newly selected action is computed. Is it given to the agent by an external program?  Then, it seems to me that the whole training procedure is more like (a somewhat restricted version of) actor-critic-based reinforcement learning than offline reinforcement learning (in which the agent cannot interact with the environment). If that is the case, what is the novelty of the proposed method?\n\nThe authors claim in Table 3 that their proposed approach gives much better results than UBAR, but the original paper of UBAR (Yang et al., 2021) reports much better results (e.g., Inform score of 95.4). Why is there such a big difference?\n\nAlgorithm 1 states that the policy is updated by behavior cloning until \"convergence\". I am wondering if it causes any overfitting problem. Is overfitting the reason why the whole training process is stopped at the fourth iteration (Table 2)?\n\nMinor comments:\n\np. 1: outperforms the state-of-the-art -> outperforms the state of the art;\np. 1: not trained to for -> not trained for?\np. 2: fine-turning the GPT-2 -> fine-turning GPT-2? fine-turning the GPT-2 model?\np. 2: generates strategically -> generates a strategically;\np. 2: Pr(O_{t+1} ... ) -> should not be italic?\np. 3: by training action-value -> by training the action-value;\np. 3: of critic network -> of the critic network;\np. 4: for $i$-th -> for the $i$-th;\np. 4: in task-oriented -> in the task-oriented?\np. 4: generated system response -> generated system responses?\np. 4: from (Zhao et al., 2019) -> from Zhao et al. (2019);\np. 4: prohibitory -> prohibitively?\np. 4: over response candidates -> over the response candidates?\np. 4: updated policy by above -> the updated policy by the above?\np. 5: on MultiWOZ domain -> on the MultiWOZ domain;\np. 5: ConvLab framework -> the ConvLab framework;\np. 5: HuggingFace Transforms library -> the HuggingFace …;\np. 7: prices -> priced?\np. 8: user goal -> the user goal?\np. 8: with following -> with the following;\np. 8: the all -> all the;\np. 8: whereas original -> whereas the original?\np. 9: straightforward -> straightforwardly?\np. 8: large scale -> large-scale?\n",
            "summary_of_the_review": "The proposed algorithm is simple and seems effective. However, some important details are missing and the novelty is not very clear.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "To tackle the response generation diverging issue from human language, this paper introduces a critic on the top of the pretrained GPT-2 task-oriented dialogue agent, and demonstrates promising empirical results on two datasets (MultiWoz and ConvLab).  ",
            "main_review": "Strengths:\nThis paper introduces a critic value function on the top of the pretrained GPT-2 task-oriented dialogue agent, to guide the response generation. \n\nWeakness:\n1. The novelty may be limited by only adding a critic value function on the top of the existing work.\n\n2. Is it possible to add human evaluation for at least of the datasets? (fixed in the rebuttal)\n",
            "summary_of_the_review": "The motivation (tackling the response generation diverging issue from human language) of this work is clear, and the solution is also good, by adding a critic on the top of existing pre-trained GPT-2 dialogue agent, and shows promising empirical results on the automatic evaluation of two datasets. However, the main concern for this work, is is quite incremental with limited novelty, and also lacks human evaluation to verify its effectiveness, may recommend for a workshop paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper works on offline reinforcement learning for natural language action space setting, particularly for task-oriented dialogue management. The paper nicely incorporate the policy network (to sample agent response) and the q network (to evaluate the agent response) into a single GPT-2 network and propose a policy interation algorithm to optimize both the q and policy network.\nDuring policy evaluation, the q network is updated with sampled system actions and responses. During policy improvement, the sampled system actions and responses with maximum q values are used as labels to update the policy network. The model achieves SoTA performance on MultiWOZ dataset.",
            "main_review": "The paper proposes a nice way to handle large scale natural language action space for RL. An action in this model is a system action plus response (a sequence of tokens) rather than pre-specified variables in discrete latent models for dialogue (LaRL, etc.), so the proposed model can be easily incorporated into a transformer-based language model (e.g. GPT-2).\n\nMy biggest concern is the q-value estimation. Estimating the Q value for an offline dataset is hard due to the problem of overestimation, which is already pointed out by the authors in the paper. However, the authors didn't describe in detail how they mitigate this problem. The only sentence in the paper about this is \"However, we avoid this OOD problem by xxx ... revised by generated system response and evaluated reward using offline automatic evaluation\". More details are needed, and ablation study about this is needed. Otherwise, I don't know if the proposed method can generalize to other natural language action space applications where the ground-truth reward function is unknown for offline automatic evaluation.\n\nMy second concern is the exploration perspective of self-generation. The paper mentioned that {a_k}^N is a set of N response candidates generated from the current policy \\pi. What number is N set to? What is the impact of N to the final performance? Moreover, is{a_k}^N sampled by beam search or vanilla sampling? I'd assume that we want to sample {a_k}^N that is diverse enough, rather than similar system responses with only one or two different words. More discussion about the generation is needed.\n\n",
            "summary_of_the_review": "1. Good method to incorporate policy network and q network into a GPT-2 model to handle large scale natural language action space.\n2. Have concern on offline q-value estimation and sampling of response candidates. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}