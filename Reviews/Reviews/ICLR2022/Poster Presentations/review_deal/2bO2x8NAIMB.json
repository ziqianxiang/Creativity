{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The authors argue in favor of task-aware continued pretraining and demonstrate through experiments that using objectives based on the end-task during continued pretraining help in improving downstream performance. \n\nThe reviewers generally appreciated the motivation, the formal treatment of the topic and the thoroughness in the experiments. There were some concerns about (i) positioning of the paper (pretraining as opposed to continued pre-training) (ii) thorough comparison with other MTL frameworks (iii) evaluating on more datasets (iv) cost of continued pretraining for each task v) the benefit of META-TARTAN over MT-TARTAN only in specific  settings and (vi) lack of surprise/novelty in the results. \n\nIMO, the authors have adequately addressed ALL the above concerns raised by the reviewers. Further, despite the above concerns, all reviewers agree that the problem is well motivated and of interest to the community and most aspects of this work are thorough. The findings will be useful and may spawn other work in this area."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper considers the common setup of pretraining and finetuning paradigm and argues that an end-task aware setting, either by multitasking or an additional meta learning that learns the weights between various auxiliary tasks and the end task, is superior than end-task agnostic pretraining.\n\nThe paper provides formal formulations of the various set-ups. Propose an interesting strategy for the meta-learning setup (Section 3.3).",
            "main_review": "Strengths:\n1. Formally formularize different configurations of pre-training, finetuning, end-task-aware multitasking or meta learning setups. These formulations are clean and make the difference between the various setup clear.The paper is well-written and easy to understand.\n2. The set-up is clean and the experiments are done thoroughly. There are several interesting observations. For example, the observation of task weighting strategies over time from the meta-learning setup is interesting. It shows that in the beginning the end-task is included but with a smaller weight while in the later phase the end-task is upweighted but the auxiliary tasks still receives small weights, instead of being all zeros.\n\nWeaknesses:\n1. Intuitively, it is not surprising that the proposed method performs better than other end-task agnostic pretraining. While the paper argues that the end-task aware setting, when done correctly, will save computational power, it does not consider the comparisons when there are multiple end-tasks involved. It is understood that a shared pretrained model has the advantage of quick iterations on various end-tasks. When there are a large number of end-tasks involved, it become daunting to train/pre-train large models for each end-task.\n\nThough I also acknowledge that the end-task aware approach proposed by the paper is a good middle ground when there is a specific end task and medium amount of resources are available, which makes training useful large models more accessible, since the end-task aware method are more data-efficient (resource-efficient.)",
            "summary_of_the_review": "The paper provides supporting evidence that end-task aware training can achieve a better performance while being more data-efficient. The paper is well-written and easy to follow. It provides an alternative to the pretraining + finetuning setup and their observations that end-task aware approaches are more data-efficient might make training large models more accessible to entities with slightly less computational resources.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The author argues that direct training on both pre-training task and fine-tuning task would lead to better end-task performance. The key is to incorporate the end task objective function to the pre-training stage.\n",
            "main_review": "The paper focuses on the continued pre-training setting and it proposes a multi-task end-task aware training method (MT-TARTAN) and a meta-learning variant to achieve better performance than the pre-training + fine-tuning paradigm. \n\nThe idea to compare co-training with pre-training-then-fine-tuning is interesting and a promising direction. The proposed method is well motivated to adapt weights during training for the auxiliary tasks and the domain task. The analysis about the tasks weight is interesting in figure 3 and 4, which shows some guidance on how to choose training tasks and dynamically changing the weights during MTL.\n\nThere’re a few minor concerns about the paper:\n\n1. Lacking comparisons with other multi-task learning (MTL) work about weight selection\nOne of the main contributions is to use meta-learning for choosing the weights of each pre-training task. There's a line of work regarding weight selection for MTL. How is meta learning compared to these methods in terms of performance, training speed, etc?\n\n[1] Guo, Michelle et al. “Dynamic Task Prioritization for Multitask Learning.” ECCV (2018).\n\n[2] Kendall, Alex et al. “Multi-task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics.” 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (2018): 7482-7491.\n\n2. The setting studied here is kind of limited. Only focusing on low-resource classification tasks. The experiments cover three datasets and show mixed results. \nThe META-TARTAN doesn’t show much improvement without OOD auxiliary data.\n- According to Table 1, the META learning version seems only slightly on two tasks while worse on the other for TAPT. Is there any explanation here?\n- It’s more convincing to add more fine-tuning datasets and analyze the effect of dataset size and final performance for the proposed method.\n\n3. One merit about DAPT is that for similar tasks belonging to the same domain, you only need to train DAPT once and then do TAPT. On the other hand, the proposed method always needs to mix in pre-training data to fine-tuning tasks. Given this, TARTAN also introduces overhead and it’s not 100% true that DAPT is more data efficient given that you might only need to go through it once. Not to mention many domains actually have plenty of unlabeled corpus.\n\nA few more questions and comments:\n1. What are the number of auxiliary tasks? Seems like for DAPT, TAPT and DAPT+TA, there’s only one auxiliary task? The MLM on either the domain corpus or the target corpus? If that’s the case, then it seems to be too few tasks to be worth selecting from.\n2. How hard is it to extend the proposed method to generation tasks instead of discriminative tasks?\n3. It’s until the experimental section that I realize the paper focuses on continued pre-training rather than pre-training from scratch. I guess it requires some revision to make that clearer at the beginning of the paper.\n\n",
            "summary_of_the_review": "The MTL f design to combine pre-training tasks and fine-tuning tasks in continued pre-training stage is well motivated. However the experimental setting is a bit limited and the results are also mixed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In the paper the authors propose TARTAN, methods to enable end-task aware pre-training. MT-TARTAN simply combines the pre-training objectives and the end-task objective as multi-task learning. META-TARTAN learns a set of weights for the pre-training objective and the end-task objective, using meta-learning. MT-TARTAN and META-TARTAN shows improved performance and data efficiency in a set of three low-resource text classification tasks. ",
            "main_review": "Strengths:\n* The authors advocate for / introduce end-task aware (continue) pre-training, which is a new and practical problem setting for NLP practitioners.\n* The authors overcomes the pathological solution problem in their META-TARTAN methods by introducing a separate classification head $\\phi^{*}$. Given that meta-learning is known to be unstable and have optimization challenges, the authors' observation and solution is helpful.\n* Improved performance (with significance test) and data efficiency compared to DAPT and TAPT proposed in Gururangan et al. 2020.\n\nWeakness:\n* The description of the META-TARTAN method is not clear enough. \n  * A figure illustrating the data, parameters and optimization steps for META-TARTAN would be very helpful. (e.g., illustrating the relation of $\\theta_{body}, \\phi^1, \\phi^2, ..., \\phi^{'}$, how meta-objective is computed using weights $w$ and parameters $\\theta$, where does the separate classification head come in, ...)\n  * I get to understand the method better (e.g., whether $\\phi^*$ is re-initialized at every time stamp $t$, whether $\\phi^{'}$ is still being optimized when $\\phi^{*}$ appears) only after seeing Algorithm 1 in the appendix. If space allows, please enrich the Sec 3.2-3.3 or move the algorithm to the main text.\n* Though the authors put lots of efforts in META-TARTAN, it appears that META-TARTAN is comparable with MT-TARTAN in most cases. META-TARTAN seems to be better in the DAPT-only setting (Sec 5.3); however since we're doing _task-aware_ pre-training, TAPT can be easily achieved in this case. I wonder what is the practical utility of META-TARTAN.\n* The paper needs more thorough discussion on computation costs. Computation savings is claimed to be one major advantage of TARTAN. However the comparison is made according to training iterations/steps, while one step in META-TARTAN is much more computationally-expensive than one step in DAPT or TAPT. Please take this into consideration when discussion computation savings.\n\nQuestions and further discussion:\n* I'm not sure if involving the validation set $D_{T^*}^{val}$ in computing and optimizing the meta-objective is a fair practice. DAPT/TAPT only access it for model selection or early stopping; while META-TARTAN can use it to update meta-parameters $w$ and indirectly influence the training for model parameters $\\theta$. Therefore META-TARTAN has more advantage in terms of data usage; this should be discussed in the paper.\n* The authors mentioned \"an optional fine-tuning step\" on the end task in the introduction. Does the results in the paper include this step or not? \n* I wonder why in Figure 3 DAPT-MLM is gradually down-weighted, but in Figure 4 DAPT-MLM is up-weighted. Seems to be contradicting. What does Figure 3 look like if we have more iterations?\n\n---\nThanks the authors for their hard work!",
            "summary_of_the_review": "Strength: problem setting is practical and interesting; good performance and improved data-efficiency.\nWeakness: lack of visualization and clear description for the method; comparison in computation cost and data usage is questionable.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper makes the argument that generic pre-training (on auxiliary tasks) is inferior to task-specific pre-training. The authors argue that the final task should be learned together with the auxiliary tasks in a multi-task setup, which they call MT-TARTAN. They also propose a meta-learning algorithm META-TARTAN that uses meta-learning to mitigate the potential impact of updates from auxiliary tasks that detract from the main task.\n\nThe authors back this up through experiments. They consider three tasks on CS and Biomed papers, where they train 1) the main task on top of a RoBERTa checkpoint 2) the main task on top of a pre-trained model from Gururangan et al. (2020) and 3) the main task mixed-in with auxiliary tasks from (2) on top of a RoBERTa checkpoint. They observe that their approaches (3) outperform (1) and (2), both in terms of model accuracy and data efficiency. In a setting where the auxiliary tasks are potentially more noisy, they also observe an advantage of META-TARTAN over MT-TARTAN.\n\nOverall their work heavily references Gururangan et al. (2020), who show that adapting a generic model to the task domain through more task-specific pre-training can improve model performance. They make it directly comparable by using their models and a subset of tasks considered in that paper.",
            "main_review": "The paper is clearly written and well structured.\n\nI believe the question that the authors actually address, whether auxiliary tasks should be used separately or in conjunction with the main task, is important, and their results should be of interest to the community.\n\nHowever, I think the general framing of their paper in abstract / introduction is misleading. At no point do they train a model from scratch (i.e. without pre-training) with their proposed methods. They do justify this with the high cost of pre-training and the convenient availability of pre-trained models, which ironically would be my main criticisms of actually foregoing generic pre-training. So although they raise the question whether pre-training is necessary, they then don’t actually compare against a model that is not pre-trained. Rather, they show that after pre-training it might not be necessary to further pre-train on large amounts of data just for domain adaptation.\n\nI think the paper would be much stronger if they did not defer their main question (“Should we be pre-training?”) to future work, but rather tested their method with the typical MLM auxiliary task on a newly initialized Transformer model.\n",
            "summary_of_the_review": "I think the paper raises an interesting question, but then only addresses a different, but related question. The results should still be of interest to the community. I think the paper would be much stronger if the authors actually ran experiments that forego pre-training entirely and instead use their mixed-training approach.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}