{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "*Summary:* Study gradient flow dynamics of empirical and population square risk in kernel learning. \n\n*Strengths:* \n- Empirical results studying several cases in MSE curves.\n- Explaining / solving certain phenomena in DL using kernels. \n\n*Weaknesses:* \n- More motivations would be appreciated. \n- Technical innovation not so high. \n\n*Discussion:* \n\nUd7D found that the main strength of this paper is the take-home message rather than innovations. They concluded 7 might be appropriate for the evaluation. This opinion was seconded by WyHh who considered 7 the most appropriate rating. 5uQz also found that 7 would be the most appropriate rating. qXRH maintained concerns about the novelty of the work and rating 5. Nonetheless, they agreed the study is valuable and would not oppose acceptance. \n\n*Conclusion:* \n\nThree reviewers found this paper is definitely above the acceptance threshold (suggesting rating 7) and one more reviewer found it marginally below the acceptance threshold however not opposing acceptance. I found the general impressions from the discussion well described in a comment from Ud7D, who indicates that although this is not a breakthrough paper, it is a nice paper showing that a lot of DL phenomena are can be explained by Kernels. I conclude that the paper makes a sufficiently valuable contribution and hence I am recommending accept. I suggest the authors take the reviewers’ comments carefully into account when preparing the final version of the manuscript."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "Studied the evolution of generalization error of the kernel gradient flow trajectory with respect to the training (empirical world) and population (ideal world) MSE loss.\nThe analysis builds upon [Mei et al. 2021], which relates kernel ridge regression to projection onto low-degree polynomials. The authors showed that the estimator optimizing the empirical risk achieves vanishing training error, but the test error plateaus at certain value depending on the training set size, whereas the online (population) estimator can learn increasingly complex components of the target function as training proceeds.",
            "main_review": "In my opinion this is an interesting paper. \nPrior works have already shown that under appropriate conditions, kernel models can learn at most an $\\alpha$-degree polynomial, where $\\alpha$ depends on the relation between the sample size and input dimensionality, as well as certain invariance structure. \nWhile it is not very surprising that the training time of kernel gradient flow (up to the sample size bottleneck) plays a similar role as the sample size in kernel regression, to my knowledge this is the first work that analyzes this correspondence in the high-dimensional asymptotic limit. Such result may be interpreted as a \"scaling law\" for optimization, i.e., a certain sample size and training time is required to learn a target function with certain complexity. I believe the ICLR community will find this message relevant. \n\nA few comments and questions.\n\n1. My impression is that the theoretical analysis does not consider SGD. If this is the case, then mentioning SGD multiple times in the abstract and introduction can be misleading. \n\n2. Related to the previous point, the population dynamics (2) requires an integration over the input distribution, which is not very realistic. If we optimize the model using one-pass SGD instead, would the number of training steps be analogous to the training time in gradient flow?\n\n3. If we consider gradient descent on random features models (as in Section 4.2), then following the results in [Mei et al. 2021], do we expect the model width to play a similar role (in terms of limiting the complexity of the learned function) as the training set size or training time? \n\n4. It might be a good idea to elaborate on how results in this submission differ from the classical nonparametric rates for kernel regression, in which the generalization error rate is typically specified by the source and capacity condition. What are the benefits of working with the asymptotic setting? For example, the depicted three-stage phenomenon is not really that surprising, and I don't think it is only present in high dimensions. \n\n5. The listed assumptions in Appendix A.2 are rather opaque. Can the authors comment on whether it is straightforward to verify these conditions for more general input distributions (beyond unit sphere or hypercube), such as a Gaussian mixture?",
            "summary_of_the_review": "I believe the contributions in this paper are solid, and the take-home message is of interest to the ICLR community. I will consider adjusting my score if the authors can address some of the aforementioned concerns.  \n\n----------------------Post-rebuttal update----------------------  \nThank you for the detailed reply, which addressed some of my concerns. In my opinion this submission is above the acceptance bar (I think 7 would be more appropriate for my evaluation, but that is not an option...)",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors discuss three phases of the MSE loss curve: in the first phase the training, test, and the oracle errors remain close together; in the second phase, the training error goes to zero; and in the third case, the oracle loss converges at the approximation level whereas the test loss is higher. There are two theorems on dot product kernels (including some NTKs) and invariant kernels where the functions through linear dynamics with two kernels exhibit three phases. ",
            "main_review": "Strengths: Nice empirical results studying several cases around the three phases in the MSE curves. \n\nWeaknesses: Although the connection to deep learning is very weak, the paper mentions deep bootstrap and some other connections with deep learning. \n\nIn my opinion, the paper would benefit from more motivation for comparing the oracle risk together with the usual train and test errors. As it stands, the paper's attraction is rather limited.\n\nSome issues:\n- Is \\beta_t on page 1 a scalar or a vector? Either way, there seems to be a problem with the linear dynamics on the eq. at the bottom of the page. \n- In theorem 1, I could not find where w_d(1) is introduced. \n\nTypos:\n- In Figure 1, the oracle world legend should write R(f_t^{or})\n- At the top of page 2, f is not defined in the definition of K_N, I guess it is f_N. Normalization with N here does not make sense to me as N was only introduced as an index of growing width networks on page 1 (instead of precisely the width of the network)\n\n\n",
            "summary_of_the_review": "It is not clear why the main message of the paper around the three stages of learning is interesting for the community. The paper does study several cases including random features. Nice experiments are included supporting the main message of the paper. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper studies the training/learning dynamics of (inner product) kernel gradient descent in the high-dimensional setting. The main contribution is proving a three-stage learning dynamics which were observed in neural networks in existing work. \n(1) Stage 1: train loss ~ test loss ~ oracle test loss (training set == whole (input, label) distribution) \n(2) Stage 2: test loss ~ oracle loss; train loss ->0; \n(3) Stage 3: test loss unchanged; oracle loss decays; \n\nI think this is a nice application of the results developed in [1], showing that many seemingly surprising deep learning (DL) phenomena are indeed not unique in DL -- most of them are provable / observable in the kernel setting. \n\n[1]Generalization error of random features and kernel methods: hypercontractivity and kernel matrix concentration. ",
            "main_review": "This is a good paper that applies the machinery developed in [1] to solve / explain certain DL phenomena. In what follows, I briefly summarize the strategies / contributions. \n\nIn the kernel gradient descent setting, the dynamics can be characterized by the gradient flow (solution) operator $G_{t, X}$, where $t$ and $X$ are the time and training dataset, resp. The $t=\\infty$ (i.e. kernel regression) is solved in [1, 2]. The current paper analyzes the setting of $t<< |X|$ (stage 1), $t\\sim |X|$ (stage 2) and $t>> |X|$ stage 3. The paper shows that, in the high dimensional setting, $G_{t, X}\\approx P_{\\min (t, |X|)}$, where $P_r$ denotes the projection operator onto certain low-frequency eigenspace (depends on $r$). The main technical difference between the current paper and [1] seems to be: in [1] $G_{t=\\infty, |X|}$ is essentially a matrix inversion and in here $t<\\infty$, $G_{t, |X|}$ is a matrix exponentially. This difference requires certain technical treatment and other than this, the overall strategies seem quite similar. \n\nIn sum, the strength and weakness of the paper are \n\n(Strength) Explaining/ solving certain phenomena that were previous observed in DL setting using \"Kernels\".\n\n(Weakness) Technical innovation is not very high and largely depends on the framework of [1]. As mentioned in (Strength) above, the proposed problem itself is mainly about \"Kernels\" rather than something unique in DL. \n\n\nMinor Comments: \n(1). The gradient flow dynamics in the current paper seems to be the limit of GD dynamics rather than SGD. \n\n\n[1] Generalization error of random features and kernel methods: hypercontractivity and kernel matrix concentration. \n\n[2] Learning with invariances in random features and kernel models ",
            "summary_of_the_review": "Although this is not a breakthrough paper and the technical contributions is not very high, I believe this is another nice paper showing that a lot of DL phenomena are can be explained by Kernels. The analysis also offers some basic intuition of neural networks' learning/training dynamics. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies the learning dynamics of gradient flow for kernel ridge\nregression. The authors contrast to different setups: the \"empirical world\",\nwhere the model is trained on a finite data set, and the \"oracle world\", where\nthe model is instead trained directly on the population loss, or in other words,\nby minimising the test error. Nakkiran et al. (2020) recently reported that the\nerrors in both setups stay close to each throughout training for a range of deep\nneural networks. By studying the relation between empirical and oracle world in\nthe setup of kernel ridge regression, the authors investigate this phenomenon in\na setup under precise theoretical control.\n\nThe authors perform such a study by leveraging the analysis of Ghorbani et\nal. (2021) and Mei et al. (2021), who gave a detailed and precise analysis of\nthe implicit bias of learning in random feature models, and in particular the\nlearning of approximations of increasing complexity. \n\nContrasting the learning in the two worlds, the authors find that learning\ngenerally, but not always, proceeds in three stages. In the case of polynomial\nkernels, models in both the empirical and the oracle learn first the leading\n$\\ell$-components of the target function, defined as those projections of the\ntarget function along the polynomials of small degree. After the training error\nof the empirical model reaches zero, a second phase ensues where the test errors\nof both models remain close, but the gap between training and test error in the\nempirical world can be large. Finally, either there is enough training data to\nlearn the target function perfectly, or learning enters a third stage where the\noracle model learns the target function perfectly, which the empirical model\nwon't achieve.\n\nThe authors confirm this picture also for invariant kernels, say\ntranslation-invariant ones, using technical tools from Mei et\nal. (2021). Quantitatively, the presence of an invariance shows up in the\ntraining speed, which is slower than for the product kernel (Remark\n2). Numerical experiments for kernel least-square confirm the theory, while the\nauthors also report experiments for SGD for random feature regression.",
            "main_review": "This is a timely paper: the deep bootstrap paper (Nakkiran et al. 2020) reported\nan interesting observation, which the authors investigate here in a more\ncontrolled setting. While the authors don't seem to go significantly beyond the\ntechnical tools developed by Mei et al. and Ghorbani et al., which the present\nauthors acknowledge, they use them to provide theoretical insight on a question\nnot directly investigated by Mei and Ghorbani. \n\nThis paper is also well-written: the structure is clear, previous work is\nacknowledged, the schematic plots in Fig. 1+2 are helpful. I was only unsure\nabout how to interpret Fig. 3: is it a purely schematic plot, i.e. a drawing? Is\nit obtained from simulations, or from theory? Also, the target function is\nnoisy, so how can even the oracle world reach zero test error in Fig. 3? Or is\nthe test error simply very small?\n\nMy only qualm with the presentation regards Fig. 5 and the section on SGD. If I\nunderstand correctly, this plot only shows that the difference between gradient\nflow and the discrete updates of SGD are very small, is that correct? If so, I\ndon't find the agreement between gradient flow predictions and discrete SGD\nupdates so surprising - or are there specific reasons why it would be? Instead,\nit might be worth relegating these plots to the appendix and showing what\nhappens if you know train two-layer ReLU networks end-to-end on the same\ndataset. While I understand that you focus on kernel ridge regression here,\nbridging the gap to a setup with feature learning, if only numerically, would be\nanother interesting contribution imho. I would understand if the authors prefer\nto keep the paper in the RF setting though.\n",
            "summary_of_the_review": "I see the contribution of this paper in providing a theoretically sound\ntreatment of the observations presented by Nakkiran et al. (2020) in a\ncontrolled setting. I think this paper would be a valuable contribution to ICLR,\nand recommend acceptance.\n\n*Edit 14.11.* After the first round of responses, and after reading the other reviews, I have increased my confidence score from 4 to 5.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}