{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper presents miniF2F, a dataset of 488 highschool and college level math problems. The problems are fully formalized and include proofs in the Metamath, Lean and Isabelle theorem provers (as the reviewers pointed out, the support for Isabelle is limited, and that should be made clearer in the abstract). This multi-platform support is the main selling point of the benchmark, because it will make it possible to make direct comparisons among systems targeting different theorem provers. \n\nThe paper also does a good job discussing the benchmark selection and formalization process. This is important since some of the problems were translated from word problems. \n\nAs part of the rebuttal, the authors added extra information on the performance of the baselines and some qualitative details on how they fail. \n\nOverall, there is agreement among the reviewers that this is a valuable benchmark that will enable comparisons among systems that today are very hard to compare."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors present miniF2F, a dataset of formalized mathematical problems drawn from diverse sources including IMO, AIME, AMC, undergraduate, and high school problems. The focus is on algebra, inequalities, and number theory as those problems are easier to formalize than for example, geometry or combinatorial problems. The formalization is done in Metamath, Lean, with efforts for Isabelle ongoing.\n\nThe authors run GPT-f on Metamath and Lean, and the tidy baseline (from the PACT paper) on the dataset and present results. They find that proving in Lean is vastly better for performance than Metamath which they conjecture is due to access to higher level tactics in Lean compared to Metamath.",
            "main_review": "Deep learning applied to theorem proving is I think one of its most exciting applications. The multiple different frameworks and datasets are a barrier to making progress in this area as a community and to that end this dataset is a significant step. \n\nThe methods the authors apply on the dataset are fairly state of the art and serve as a good baseline for someone wanting to make further progress. I do however think that some more analysis would be worthwhile. In particular, I think the authors should add the following\n\n(1) Breakdown of the performance on the problems sourced from the MATH dataset by level of difficulty.\n(2) A qualitative analysis of what kinds of problems the baseline models fail on and whether they fail on similar problems.",
            "summary_of_the_review": "Highly relevant new dataset with recent baselines run on it to get an idea of SOTA performance. However detailed analysis of the baselines on the dataset is lacking",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents miniF2F, a test suite of Olympiad-level problems of theorem proving that is implemented in Metamath, Lean and Isabelle. MiniF2F contains 488 individual theorem statements that are formalized from Olympiad math contests. GPT-f models trained on Metamath and Lean are evaluated on this test suite.",
            "main_review": "Strengths: (1) Since previous benchmarks of ATP mainly focus on basic math theorems, miniF2F fills the vacancy of the contest-level test suite for verifying the performance of theorem provers. I think this is an important step towards the goal of the grand-IMO challenge. (2) The cross-system design of miniF2F provides a good way to compare different formalizations and proving systems. (3) The experiment results demonstrate the importance of expert knowledge for theorem proving. Built with high-level tactics, GPT-f/Lean achieves better results than GPT-f/MM. The formal theorem provers also work better than the natural language-based problem solver.\n\nQuestions: (1) What are the meanings of \"CUSTOM\" and \"Induction\" in Table. 1. (2) What is the distribution of the number of theorems proved across different difficulty levels? (3) Personally, I am quite curious about your experience of formalizing these problems. What is the average time spent on one problem? Except for geometry and combinatorial problems, how large portion of problems could be formalized, and what would be the ultimate size of miniF2F in your expectation?\n\n\n\n",
            "summary_of_the_review": "Overall, I think miniF2F is an important benchmark that could help the community advance the research of theorem proving. I recommend accepting this paper.\n\n===========================\nI would like to maintain my old score of this paper after reading the authors' responses and other reviewers' comments.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper presents a new formal mathematics benchmark consisting of 488 statements expressed in three prominent theorem-proving/verification systems. Baseline ATP systems, notably GPT-f/PACT in Lean, are evaluated on this benchmark.",
            "main_review": "Strengths:\n- The advantages of this benchmark are that it is cross-system and that it covers a variety of mathematical topics at the Olympiad level.\n- The motivation for the particular assemblage of mathematical topics is solid: miniF2F is intended as an intermediate step toward the IMO as an ATP task, which is out of reach for current systems. This is the first effort to unify various Olympiad topics in one dataset, and the problems cover a wide scope of tactics and difficulties.\n\nWeaknesses / questions:\n- The benchmark is not really as cross-system as claimed in the abstract. Only 12% of the training statements are available in Isabelle.\n- How were the Olympiad and Custom problems chosen?\n- The way in which multiple-choice problems are formalized gives additional information to the solver:\n   - In Table 4, the AMC problem asks which value of an expression is *possible* (quantifier on a and b), but the formalization drops the quantifiers and asks to prove equality. This would be incorrect under minor changes in the conditions. It would seem more appropriate to formalize this with \"x = -2 or x = 1/2 or ...\" as a hypothesis.\n   - Problem 22 of AMC 12B 2020 asked to find the maximum value of a certain function, out of five choices. Yet, the theorem amc12b_2020_p22 (Lean) asks to prove that for all values of the argument the function is smaller than the correct maximum. This is clearly insufficient without the prior knowledge of the correct answer (we can imagine that the solver could prove a weaker bound, but exceeded its timeout trying to prove correct bound).\n   - (AIME problems are multiple-choice as well, but it is perhaps forgivable not to formalize them as such.)\n\nSuggestion to the authors: Code such as Table 4 and the theorem on the circle and hyperbola (p.6) would be more readable with simple natural language annotations describing the meaning of each line, for the benefit of readers who are not familiar with all three systems or do not see the solution.",
            "summary_of_the_review": "The main value of this work is in the set of formal Olympiad problem statements, which have not appeared in other datasets. There is little technical novelty in the ML algorithms and analysis of their performance, especially in comparison to [Han et al. 2021], on which this paper heavily relies.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a benchmark of formal Olympiad-level math problems focusing on algebra, number theory and inequalities with cross-system support on Metamath, Lean and Isabelle (in development). The paper evaluates performance of $\\textit{GPT-f}$ on Metamath and Lean, and a custom baseline $\\textit{tidy}$ on Lean as well.",
            "main_review": "$\\textbf{strengths}$\n- The paper formalizes a decent amount of cross-system Olympiad-level benchmark of 488 problems. Cross-system support on Metamath, Lean and Isabelle provides benefit on comparing automation and tactics of systems. Olympiad-level problems are also interesting to both researchers and general public.\n- The inclusion of formalization of a subset of MATH benchmark also enables comparing provers in formal and informal format.\n- The paper is well-written with good literature review on theorem proving benchmarks.\n\n$\\textbf{weaknesses}$\n- The paper could justify more on what type of problems were selected in $\\textbf{miniF2F}$. The benchmark mostly focuses on algebra, number theory and inequalities. Will this benchmark in some way skew the research direction of the community to only focus on developing algorithms particularly suitable for solving these types of problems that may or may not generalize well to other types of problems such as geometry problems?\n- It would be interesting to see if the gap on pass rates between Metamath and Lean could be reduced when the models are trained or fine-tuned on a subset of $\\textbf{miniF2F}$ in addition to pre-training. This could provide more evidence if the gap is mostly due to access to high-level tactics.",
            "summary_of_the_review": "The paper makes a solid step forward on creating cross-system Olympiad-level formal math benchmark and should have profound benefit on the community with its continual development.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}