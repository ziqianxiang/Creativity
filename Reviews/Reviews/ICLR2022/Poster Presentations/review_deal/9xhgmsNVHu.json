{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The reviewers unanimously appreciated the quality of the experiments. The main point raised was about the related work by Wang et al. but that was addressed by the authors in the rebuttal. I thus encourage the authors to make sure that discussion is reflected in the final version of their work."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper studies the variance of the DDPG algorithm on continuous control tasks. They identify that the variance in rewards arises in part due to the failure of some runs to learn. They argue that the failure to learn is due to saturating tanh nonlinearities, and suggest two methods to address this problem. ",
            "main_review": "As pointed out in the paper, variance in RL is one of the main challenges to efficient and relevant research today, and this paper is a well-written exploration of a possible cause of this variance from the DRQv2 algorithm. \nThe paper studies the performance of the DRQv2 algorithm on several continuous control tasks, arguing that the variance in RL performance over several runs is primarily driven from a bimodal distribution of training runs. \nSome of the runs learn well, but some get stuck early on and don't learn. Several possible reasons are proposed for this, including poor initialization, feature learning, and unforgiving environments. The authors then argue that saturating tanh nonlinearities are to blame. Two remedies are proposed: a penalty on the squared norm of the action, and a capacity reduction of the model where the features of the penultimate layer are normalized and a simple linear layer is used to produce the action. \n\nOverall I think the paper identifies an interesting area and an interesting idea, but there is currently a lack of rigour in a lot of the arguments and seems to have some incorrect statements. Furthermore, there is a lack of references to some existing work with quite similar approaches. These points are elaborated below\n\nComments:\n\n1. In equation 2 and 3, it is claimed that the action $a$ is produced as $a_\\theta(s) = \\tanh (\\mu_\\theta(s) + \\epsilon \\cdot \\sigma_t), \\epsilon \\sim {\\cal N}(0, 1)$. However, in DDPG, the action is generated from a clipped truncated normal which is centered at $\\mu = \\tanh(f(s))$ for some learned \\(f\\) mapping states to $\\mathbb{R}$. Indeed, in the authors' code this seems to be exactly the procedure followed in the `Actor` method in the drqv2.py file, unlike as described in eqs 2, 3. It's possible I may have misunderstood this, so could the authors comment on this discrepancy?\n\n2. The authors claim that the failure to learn is primarily due to runs that get 'stuck' early on, with figure 1 as evidence. However, it's quite hard to read figure 1 due to the large number of overlapping lines making it hard to tell the relative number of runs. Could the authors add a third row to this figure showing a histogram of the final return achieved by each run? The authors' claim can then be reinforced if the histogram is bimodal with many at zero return and the 'good' return. Continuing on figure 1, it is unclear to me how well this supports the argument that the final returns are bimodal, in particular with reference to the `hopper` environment where it appears the majority of the runs are still improving after 1,000 episodes. If we run for longer, do they all continue to improve?\n\n3. Section 4 argues that the cause of the stuck runs is saturation with the tanh nonlinearity. However, I think this section seems to leave out some important details which could help show convincingly that the 'stuck run' cause is the tanhs. Firstly, it would be useful to have a version of fig 3 that is done on a population level, like figure 1. This would allow the reader to compare the runs in figure 1 and 3, and verify that this behaviour happens consistently. (It may be too busy to show these all runs, but e.g. a large high-resolution plot showing 20 randomly-selected runs from the dataset of 40 seeds would be a great figure to eventually have in an appendix). Secondly, it would be useful to focus on the size and gradient of the final layer specifically. As it is, it's hard to judge what's happening when looking at the norm of l and w since they are taken over the whole actor's weights. Plotting the pre-tanh $a_\\theta^{\\text pre}$ would also be more useful than a flat saturated 1 value for the norm of the action. A logarithmic scale would also be useful for the gradient norm.\n\n4. In section 4, did the authors try initializing the last layer's weights to be much smaller than the other layers? It has been suggested previously that the last policy layer should be initialized to 100x the weight of the other layers [1], and it seems that this may combat the saturating nonlinearities.\n\n5. It is surprising to me that the $\\lambda$ value 1e-6 is sufficient, as this seems very small. Could the authors elaborate on why such a small value is sufficient to change the training dramatically?\n\n6. A similar idea, that of normalizing the features extracted for the agent in an off-policy algorithm, has been explored in quite a bit of detail for the DQN. In particular, [2] investigates weight normalization for DQN, and in [3] spectral normalization is applied in the layer before the action outputs, which seems quite similar in spirit to the normalization in this paper. [3] Also shows that this increases robustness with respect to hyperparameters. Can the authors comment on this work?\n\n7. At the moment the technique presented is only evaluated on DDPG. However, it seems that this saturating tanh problem could be present any other RL algorithm with bounded action range. If this method could be shown to consistently help with other algorithms, this would significantly increase the significance of this result. I appreciate that computation resources are not free, but if the authors could show even preliminary results that this method works with e.g. SAC, PPO, etc, this would be very significant.\n\nI am happy to reconsider my recommendation based on feedback from the authors and other reviewers.\n\nSummarizing:\n\nStrengths:\n+ Convincing argument about the importance of the tanh saturating in DDPG\n+ Thorough evaluation of a few different ways of addressing this problem\n\nWeaknesses:\n+ Missing some relevant related work on normalizing image features in off-policy RL\n+ Some possible incorrect statements about the action distribution\n+ Plots that could be made easier to read and interpret\n+ Limited experimental evaluation only on DDPG\n\n\n[1] What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale Study (Andrychowicz et al, ICLR 2021)\n\n[2] Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks (Salimans and Kingma, 2016)\n\n[3] Spectral Normalisation for Deep Reinforcement Learning: An Optimisation Perspective (Gogianu et al 2021)\n\n----------------------------------------------------------\nUpdate 2021-11-21\nBased on the authors' additional results, correction of some inaccuracies and comparison to previous work on normalization, I have raised my score to 6.",
            "summary_of_the_review": "The paper identifies an interesting phenomenon and has good proposed fixes, but there is currently a lack of rigour/convincing arguments and seems to have some incorrect statements. There is a lack of references to some existing work with similar approaches. The method has somewhat limited significance as it is only demonstrated to be useful with DDPG.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper is an empirical study of the issue of high variance in RL. Learning continuous control from pixels is adopted as the setup. The authors demonstrated that the failure of a few runs is responsible for much of the variance. To look for the source of the variance, they separately studied the impact of the initial and training-time randomization as well as variation in representation learning between runs, none of which were found to be majorly responsible for the variance. The authors hypothesized and isolated the exploding activation of the actions to be the main source of the variance, which cause some runs to saturate beyond salvation and fail at performing. The hypothesis is tested by employing several measures to counter this phenomenon. Employing normalization of the penultimate features turned out to be particularly helpful.\n",
            "main_review": "Strength:\nThe main strength of the paper is its focus on an important problem and studying it effectively. Being able to pinpoint the problem of variance to exploding activation of actions is definitely a success. Various experiments are conducted based on well-formed hypotheses trying to rule out other suspected sources.\n\nThe main weakness is not thoroughly discussing the result in table 2. Which approaches really worked? It is claimed that “Normalizing activations seems to stably improve performance and decrease rewards”. If I look at it closely, actor normalization achieves the second largest variance on finger turn. Actor+critic normalization does not achieve a low variance on that task either.\n\nAnother weakness is that the choice for combining different strategies isn’t justified. The penalty got mixed results from table 2 and so did the layer norm. Early contrastive learning, which is added in the combined fix, came out of nowhere. To me, it seems like different combinations were tried and this one somehow worked. Can you shed some light on it? How does Figure 4 look like when we only try normalization or penalty or a combination of just those two? What would the mean and the standard deviation for the combination of just these two be?\n \n",
            "summary_of_the_review": "It is a paper on an empirical study of an important question in RL with satisfactory claims and fixes. In the latter part of the paper, some thorough discussions are missing, and the choices made around the combined fixes are not well motivated, for which I would like to hear from the authors.\n\n*** Updated ***\n\nThe authors addressed all my concerns satisfactorily. They even addressed a last-minute question about a highly-related work that I didn't notice before. The authors clearly described the difference between that work and theirs. \n\nMore elaborately, the authors promptly and elaborately discussed a comparison to this newly discovered work. They not only showed that they can discuss this work in their submission fairly, but they can also utilize it to improve their submission. Moreover, their new results showed that the existing work was inferior to their proposed solution. The authors' explanation also made sense that the existing approach normalized the pre-tanh action, which limited the action space too restrictively.\n\nHence, I increased my score from 6 to 10. Thank you!",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper investigates sources of poor outlier runs in a DDPG-based agent on 5 image-based DM control tasks. The empirical analysis suggests that such outlier runs result from saturating tanh nonlinearities due to large-valued activations in the policy network. To prevent saturation of tanh, the paper proposes and evaluates fixes and shows that combining a bunch of them results in better performance with lower variance.",
            "main_review": "Strengths: \n -  While existence of outlier runs is well known in deep RL, for example, see evaluation of 100 seeds on continuous control tasks in [1] (Figure 3)  and 100 seeds on on Atari 100k in [2] (Appendix), the empirical study on continuous control tasks to find what causes such runs is interesting. For example, it seems to rule out network initialization and feature learning. \n- The paper suggests a number of simple fixes for control tasks, such as normalizing feature activations in actor and critic, which are easy to implement and often lead to smaller variance on the 5 tasks in the paper.\n- The paper is well-written and easy to read.\n\nWeaknesses:\n\n- One of the main limitations of empirical study is that its conclusions are specifically tied to network architectures that use a tanh squashing function in the policy network. For example, policy gradient methods (such as PPO/TRPO) or DQN-style methods typically use `relu` activations and are unlikely to suffer from saturating nonlinearities. The same limitation also applies to the proposed fixes which try to prevent tanh from saturation. This limits the possible impact of this work.\n\n- The visualizations on walker walk seems somewhat misleading as this is the only task where the proposed fixes results in ~60x lower variance. This doesn’t seem to be true for other tasks such as finger turn where the improvements are significantly smaller. Same is true for the experiments for learning stability (can the authors possibly repeat it on other tasks too?) It is unclear if this is the case of showing evidence only on the tasks which support the hypothesis. \n\n- The fix which works well is combining feature normalization for both actor and critic but the rationale for normalizing features in the critic network is not explored in the paper (I believe that the critic network uses ReLU activations only). This makes me wonder if we actually understand why the proposed fix improves performance or reason behind poor outlier runs on tasks other than walker walk?\n\n- The use of a single benchmark with only a small subset of tasks with the high-variance issue casts doubt about the generality of the findings and proposed fixes. I feel lack of compute is not a solid argument to evaluate on only a small number of environments. To give an analogous example from deep RL, [3] show that methods proposed for tackling hard-exploration methods hurt performance on easy-exploration tasks. So, do the proposed fixes actually result in any performance improvement on the remaining tasks used by Yarats et. al (2021b) in the medium tasks. Furthermore, do the combined fixes in Section 4.2 generalize to harder tasks?\n\n- **(Missing result)** The paper introduction mentions reducing variance on a humanoid locomotion task by two orders of magnitude, however I couldn’t find this experiment anywhere.\n\nOther comments (Minor):\n\n- There is a distinction between self-supervised methods like BYOL  and contrastive methods like SimCLR. The authors interchange them throughout the paper which is technically incorrect. Furthermore,  CURL actually hurt performance compared to the baseline method on Atari and their reported improvement resulted from mismatch in evaluation protocols [2] and seems to be a poor example to cite.\n- Properly warm-starting / reusing features probably require lowering the learning rate for feature layers so that learning may not lose this initialization quickly after a few updates. Thus, it is unclear if feature learning experiments actually capture this or not. \n- Reporting mean and std is a specific kind of interval estimate, so the statement in related work about being different than [2] didn’t compile for me. \n- The results in Table 2 were somewhat overwhelming to read because of the plethora of numbers  across multiple rows and columns, maybe reporting aggregate statistics such as mean/median/ interquartile mean with CIs might be better visualization.\n- More detailed discussion about outlier runs in RL and discussing this in related work.\n- Spelling: intractable → intractable\n- While I appreciate the use of a large number of seeds, there is a trade-off between generalizable findings and statistically robust findings. Using a single task with 200 seeds is an extreme example of this.  As such, my opinion is that the conclusions in this work would be more generalizable if the authors used a large number of tasks but maybe slightly smaller number of seeds (maybe 30?) as that provides us with more diverse data points to validate our findings. \n\n[1] Mania, Horia, Aurelia Guy, and Benjamin Recht. \"Simple random search of static linear policies is competitive for reinforcement learning.\"  NeurIPS (2018).\n\n[2] Agarwal, Rishabh, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G. Bellemare. \"Deep reinforcement learning at the edge of the statistical precipice.\"  NeurIPS (2021).\n\n[3] Taiga, A.A., Fedus, W., Machado, M.C., Courville, A. and Bellemare, M.G., 2021. On Bonus-Based Exploration Methods in the Arcade Learning Environment. ICLR (2020).",
            "summary_of_the_review": "Although I liked the empirical study, the analysis and the proposed findings may not generalize beyond the setup used by the paper (tanh nonlinearities) and lack of thorough evaluation on a large set of environments. Furthermore, there are other weakness such as missing results and fixes without much motivation with possibility of visualizing results on a cherry-picked environment. Overall, I feel the paper can be significantly improved by addressing some of these weaknesses. \n\n\n-------------------------------- Update after rebuttal -----------------------------------------------\n\nBased on the discussion with the authors, I feel that the paper can be accepted at the conference. I'd like to update my score from a 5 (weak reject) --> 7 (accept) but the ICLR review scale only allows me to pick a score of 6 or 8. Since a very related work (highlighted by reviewer cHfn is not discussed), I decided to go with a score of 6.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper gives a systematic evaluation on different factors that may cause the high-variance of actor-critic algorithm on various robotic control problems. They found that the main source of variance is from the numerical instability. The authors then propose four training techniques to mitigate the instability, which significantly reduced the variance.",
            "main_review": "I believe the problem this paper concerns is very important to the area. High instability in the early training process is a serious issue that stops RL from being more widely applied. I like the authors eliminating possible factors that might cause high variance before finally identifying the main cause. The improvement of the combined agent seems solid. However, I do have the following concerns.\n\n1. It is not clear whether the similar variance-reducing techniques could be applied to other environments. For example, penalizing saturating actions only applies when the actions are continuous variables, which is not the case in Atari games or other environments with discrete action space.\n\n2. I don't fully understand the logic of testing environment sensitivity. The variance is guaranteed to reduce since the mean reward reduces. I think one should compare the variance of the original algorithm to the algorithm with a pre-trained good policy that may automatically eliminate the possibilities of reaching bad states, which may reduce the variance.\n\n3. Some techniques that are used to reduce variance are not new and are commonly used in previous literature like penalizing actions and learning rate warmup.\n\n-------------------------------- Update after rebuttal -----------------------------------------------\nI have read the insightful discussion on the newly-discovered work and I appreciate that the authors' responded quickly with new results and pointed out the novelty compared to the new paper. I support accepting the paper.",
            "summary_of_the_review": "I work mostly on the theoretical side of RL so I am not sure how significant the experimental results are in this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}