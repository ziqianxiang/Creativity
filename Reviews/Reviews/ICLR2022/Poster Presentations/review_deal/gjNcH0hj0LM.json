{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The authors design a framework for active learning on time-series data. The framework, called Temporal Coherence-based Label Propagation (TCLP) leverages temporal coherence to propagate expert labels to nearby points by a plateau model. In addition to describing the framework clearly with simple pseudocode, several experiments are carried out with careful analysis to validate the effectiveness of the framework.\n\nThe reviewers are mostly positive on the simple algorithm with strong empirical performance as well as the solid analysis of experimental results. They are also satisfied with most of the rebuttal feedback from the authors. Somehow there are joint concerns on the weaker theoretical results, especially in terms of their correctness. In particular, the unrealistic assumptions and over-simplification make it hard to connect the theoretical results with the actual algorithm. Several reviewers suggest the authors to move theoretical analysis section to a supplementary section as a hypothesis, and the authors are also encouraged to clearly discuss what the theory can and cannot cover."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper develops a framework for time series active learning called Temporal Coherence-based Label Propagation (TCLP), that uses the temporal coherence within time series to choose as few samples as possible for domain expert labeling and assigns those labels to temporally close samples. In addition, unlike the most similar methods in the literature, the proposed method does not require estimates of the approximate locations and true classes of the segments of the time series.",
            "main_review": "The strengths of this paper are in addressing time series active learning in a way that truly respects the temporal coherence of time series and addressing issues with identifying the time series segments. These are presented in a coherent algorithm with pseudocode that is relatively easy to follow. Additionally the empirical results are quite strong. There is also some analysis of the differences in performance between the proposed algorithm and other competitors, as a function of the characteristics of the dataset---this, along with pseudocode, are unfortunately becoming quite rare in machine learning research papers.\n\nI have one issue that would be nice for the authors to address in the paper. Near the end of the experiment section, the authors mention how performance varies as a function of the parameter T in temperature scaling, and show performance increasing as T increases from 1 to 1.5 to 2. Please describe what happens as T is increased further until the point at which performance decreases. It would be good to see how that breaking point varies depending on the dataset, and why.\n\n",
            "summary_of_the_review": "This paper, as far as this reviewer knows, is truly addressing time series active learning for the first time, and does so in a novel and coherent way algorithmically and experimentally.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents an active learning algorithm for classifying and segmenting timeseries data. When instances are labeled by users, these labels are propagated to [temporally] neighboring instances using a plateau model (if possible) so that the information is shared more efficiently across multiple instances.",
            "main_review": "Pros:\n1. Good approach to use limited user feedback efficiently.\n2. Satisfactory set of experiments\n\nCons:\n1. Need some more elaboration on the theory\n2. Computational/time complexity is missing\n\n\nMain Comments:\n\n1. Equation 3: It seems like c, w, s are all estimated independently for each plateau. However, we know that very likely, these will be similar for a particular y_l (class label). Could we use such a constraint/model? E.g., we might have a [parameterized] model for (c_l, w_l, s_l) which relates to y_l.\n\n2. Equation 5: This expression needs to be elaborated and also needs to be shown that the sum of all Pr(t_e - t_s = i) where i = 1, ..., L is 1 so that we can claim Pr is a valid probability. This is not obvious.\n\n3. Algorithm 1: Do we always have r*b plateau models at the r-th round of active learning? What is the computational complexity and/or time for each round? It seems that computation time would increase with each round. Is there a way to lower the complexity by taking a sample of the most relevant plateau models?\n\n4. It is unclear why 'b' is 85 for mHealth (85 for mHealth vs 200 for others) -- no reasoning has been provided. To make sure that the settings were not selected after-the-fact just to get the best results (in table 2), the same settings should be used across all datasets unless there is a really good justification. I assume w_0 is diferent for GTEA (5 for GTEA vs 15 for others) since it has the smallest segments. It would be better if these parameters could be auto-selected or the paper explicitly stated some reasoning/rule to select appropriate values for them.",
            "summary_of_the_review": "This paper proposes a technique to improve accuracy in an important field of application where data volumes are high and efficient use of limited user feedback is of urgent need. The plateau model makes intuitive sense in the context employed here and looks like something we would want to add to the toolbox.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper addresses the label propagation segment estimation problem in time-series active learning, and apply plateau function to model temporal coherence. The experimental results show their effectiveness compare with baseline label propagation approaches.",
            "main_review": "Strengths: Applying plateau model (Equation 2) into time-series active learning for pseudo labeling is a good idea for promoting the performance. The paper is easy to follow, the idea is simple and the experimental results show significant improvement (considering both performance gain and label reducing) compare with other label propagation methods.\n\nRemaining questions: \n1) In Table 1, why b and w0 are inconsistent in the experiments across different datasets? Does it make any difference to have a larger b? That is, does the performance of TCLP decrease as the query size increases? Since in active learning, enlarging the batch is an effective way to reduce the number of rounds.\n2) For temperature scaling parameter T, why the larger of T, the better the performance are? It's better to include more settings of T, i.e., T < 1, T > 2 and more analysis about the influence of T, since the value of T seems to have a significant impact on performance.\n3) Besides UTILITY, it's better to add the evaluation results that trained on whole datasets as comparison.",
            "summary_of_the_review": "I vote for a weak ac of the current version since the idea of using plateau model for pseudo labeling in time-series AL is interesting. Although there is no rigorous theory to tell us why TCLP improves performance (at least in this paper), that is, why plateau model could provide a better pseudo labeling, and what's the upper limit of its promotion. The experimental results are fairly good, they demonstrate TCLP's effectiveness in their experiments.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The goal of the paper is to improve active learning for time-series.\n\nThe goal is to learn a function from x to y, with data coming from a time series (x_t) and (y_t). It is assumed that (y_t) is piecewise constant.\nAt the beginning no (y_t) are known, but query can be made in a active learning framework.\n\nThis paper tries to find the pieces where (y_t) is constant based on the features (x_t). It is done using the labeled timestamps (y_t_q) at (t_q) and the current learnt model f from X to Y with a thresholding rule based on f(x_t) around the (t_q). This gives segments on which (y_t) is supposed constant egal to the (y_t_q). Those labels are added as if they were true labels. Then the model for f is updated, and some active query are made based on any model. The loop is repeated for a given number of rounds.",
            "main_review": "Strength:\n- The empirical result shows that this methods is a really good method.\n- The paper is a good ICLR paper, it is based on a simple idea, and it does work well in practice.\n\nWeakness:\n- Updating rules based on a learnt model is proned to overconfidence loophole. While the authors briefly discussed temperature scaling usually used to avoid overconfidence in neural networks, I would appreciate a more thorough discussion on the matter, as well as some example of failure of TCLP.\n- I was not able to open the dropbox files and to read the code, but I believe the authors have reused licensed files that are not mentionned in the paper.\n\nThere is a lot of small corrections that I will detail now:\n\npage 3: \n1/ I believe D_u \\cup D_l = D, but this is not mentioned clearly.\n\n2/ \"More formally, the estimated segment for t_q is Eq. (1)\"\nThis is a choice made by the authors, and should be more clearly stated as a choice, by using “we estimate the segment for t_q”. If this choice is standard, than it would be nice to refer to litterature that introduce this, or similar, objective.\n\n3/ What happen when the points in [t_s, t_e] are not all in D_u? For example, if segments [t_s, t_e], overlap for t_q1 and t_q2? \n\npage 4:\n1/ Eq.(4), what is t? I believe c, w, and s should be updated according to the gradient of the quantity appearing in Eq. (3) where there is a summation over t that make it disappears.\n\npage 5:\n1/ we assume that the estimated class […] is an indenpendent random variable: independent of what, where does the randomness comes from?\n\n2/ Eq.(5) is really weird, and is based on assumptions that seems highly unrealistic. I would like to see derivations for such an equality. \nIt seems to me that the authors are assuming that $f_\\theta(x_t)[y]$ is a random variable that is independent of t, for each label y.\n\n3/ \"Without loss of generality”, why don’t we loose generality?\n\nOverall I believe Section 3.3 is too poor scientifically speaking to be put inside an ICLR paper.\n\npage 6:\n1/ It would be nice to describe temperature scaling in more details.\nSimilarly, the labels propagation from H_r is not that clear, especially if there is overlaps.\n\n2/ In Algorithm 1, the loss ell and the query strategy should be added as input of the algorithms, as well as the stepsize lambda.\n\npage 7:\n1/ Is TCLP working better than other methods for active learning simply because it is working better for semi-supervised learning?\nWhat happen if we start with 1% of labeled datapoints rather then nearly 0%?\n\n2/ Can we have standard deviation on Figure 4?\n\n3/ Is the same neural network architecture used for each methods?\n\npage 10:\n1/ Error regarding the github url.",
            "summary_of_the_review": "The experiments speak for themselves.\nOverall while the idea is simple, and the math are not really satisfying, the experimental part erases most of my concerns.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "=",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}