{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This manuscript proposes and analyzes a distillation approach to address heterogeneity in distributed learning. The main paper focuses on a relatively simple two-agent kernel regression setting, and the insights developed are extended (and partially analyzed) for a multiagent setting. \n\nThere are four reviewers, all of whom agree that the method addresses an interesting and timely issue. However, reviewers are mixed on the paper score. While all reviewers agree that the setting is somewhat stylized, a subset of reviewers highlights that the results give some deep insight that might drive future analysis and implementation in the area. Other concerns raised include potential issues with the communication overhead and the simplicity of the kernel regression setting vs real-world deep learning. There are initial concerns about whether the failure case is realistic, which the authors address. Extensions to the multi-agent setting and a partial analysis are also addressed by the authors and partially satisfy the reviewers. Nevertheless, after reviews and discussion, the reviewers are mixed at the end of the discussion. \n\nThe area chair finds, first, that the paper is much improved, and much more applicable in the updated form than in the original version, and indeed, the insights from the simple model may be informative for practice. However, the concerns raised about the distance between theory and practice are valid. The final opinion remains borderline. Authors are encouraged to address the highlighted technical concerns in any future submission of this work. In particular, the muti0agent setting should probably be central in the discussion of this work. More ambitious empirical evaluation showing that the theory translates to practice )even if there is a gap) would also help."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper focuses on the setting of federated learning where the two agents are attempting to perform kernel regression using different kernels (and hence have different models). Their study yields an algorithm of using alternating knowledge distillation (AKD) imposes overly strong regularization and may lead to severe under-fitting. Their theory also shows an interesting connection between AKD and the alternating projection algorithm for finding the intersection of sets. Leveraging this connection, they propose an algorithm that improves upon AKD. ",
            "main_review": "Strengths:\n* This paper introduced the federated kernel regression framework where they formalized notions of both model and data heterogeneity, which can be useful for developing new algorithms for model agnostic federated learning.\n* The theoretical analysis for knowledge distillation, including AKD, AvgKD, and EKD is very rigorous. \nWeakness:\n* The application of the proposed method is limited. Based on the problem setting, the proposed method is only applicable for the case of two agents, while the problem of federated learning is usually for multi-agents (more than two). Can this proposed method be extended to a more agents case?\n* The analysis of the experimental section is not clear enough or even missing:\nEx1: For Figure 3, is the loss of AKD convergent or not? This result cannot be directly observed from the figure. This paper can give an analysis on this and also for the reason why the loss of AKD increase.\nEx2: The analysis for Figure 4, 5, 6 is missing, which can make the readers confused about these results. \n* Minor problem:\nSome of the notations in the paper are a bit confusing to me, especially those seem to have similar meanings. I would suggest that if have the same meaning, they can be represented by the same notation， if not, more explanations for their differences can be offered.\nEx: It seems that $h(t)$ in Algorithm AKD have similar meaning as $g_t^2$ in Algorithm AvgKD (because these two algorithms have similar schemes). ",
            "summary_of_the_review": "* Although this paper introduced the federated kernel regression framework which might be useful for developing new algorithms for model agnostic federated learning, however, the application of the proposed method is limited.\n* Although the theoretical analysis part is rigorous, the analysis of the experimental section is not clear enough or even missing.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper analyze knowledge distillation based model agnostic federated learning. They consider simple two agent kernel regression scenario, where each agent has its own dataset and predicting function. They propose to train agent 1 model on dataset 1, and then use agent 1's model to make predictions on dataset 2, and agent 2 will train model using these predictions. They analyze the dynamics of agent 1's model, and show that Alternating Knowledge distillation will degrade the model prediction to 0. They provide another algorithm ensemble AvgKD which can actually avoid this issue and converge to optimal solution. The experiments also show that AvgKD can converge to zero loss while other approaches do not work.",
            "main_review": "Pros:\n1. This work gives the first theory analysis for model agnostic FL, even though the setting is very simple. They also use the negative result to motivate the improved algorithm ensemble AvgKD. The whole story is complete and clear.\n\n2. The experiments are sufficient, and support their analysis.\n\n\nCons:\n1. Lin et al also have similar KD idea for model agnostic FL. What is difference between AvgKD and their work?\nLin, Tao, et al. \"Ensemble distillation for robust model fusion in federated learning.\" arXiv preprint arXiv:2006.07242 (2020).\n\n2. As I mentioned before, the setting is somewhat toy. In practice, there are multiple agents, and the machine learning model is way complicated, that may not have closed form solution.\n\n3. In addition, the paper only consider optimization perspective, but I am curious about the generalization ability of AvgKD. Does it provably generalize better than purely local training?",
            "summary_of_the_review": "Overall, model agnostic FL area is lack of theory results, so this work has its own novelty. However, the setting is somewhat toy, so I am afraid it may not be very helpful towards understanding model agnostic FL in practice (e.g., multiple clients and complicated models that do not have closed form solution.) ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper analyzes the dynamics of optimizing two kernel regression models via co-distillation in a distributed setup where local models may differ in the kernel used. In each round, each local client uses the other client's model to produce novel labels for its local dataset and then retrains its local model using these novel labels. The paper analyzes three variants of this approach, the vanilla variant, a variant where novel labels are an average of the actual label and the one predicted by the other client's model, and an ensembling approach that uses all model iterations to produce predictions. The approaches are analyzed theoretically and empirically.",
            "main_review": "The paper thoroughly analyzes the dynamics of co-distillation and the proposed variants in a limited setting with two clients. The local models may differ in their kernel function so that model averaging is not possible. While the constrained setup limits practical insights, it allows to clearly analyze the behavior both theoretically and empirically. To that end, the paper details the dynamics of all three approaches and shows that the straight-forward approach may degenerate, and that the ensemble approach is optimal in the limit. Moreover, it gives conditions under which the averaging approach degenerates. These theoretical results are interesting and insightful. The empirical evaluation confirms the theoretical findings and in addition shows that the behavior is similar when using neural networks instead of kernel regression. This hints at the generality of this analysis. The paper is well-written and clear, the theoretical results are sound and, to the best of my knowledge, correct.\n\nI really appreciate the theoretical contributions, but I feel that at least the empirical evaluation is too limited. While there is good reason to analyze only two clients theoretically, it would be straight-forward to apply the proposed methods on larger numbers of clients empirically. Using only two clients and only linear regression and MNIST as experiments limits the contribution. The paper would benefit greatly from a broader empirical evaluation. \n\nDetailed comments:\n- Why is only linear regression used for the kernel experiments? \n- Which kernels are used for the experiments? I guess, a linear kernel would be most suitable to the task, but then both kernel functions would be the same.\n- How does co-distillation compare to averaging kernel models [1]?\n- The approach in [2] similarly uses co-distillation. Even though they use an unlabeled reference dataset, it seems worthwhile to compare to it.\n\n[1] Kamp, Michael, et al. \"Communication-efficient distributed online learning with kernels.\" Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, Cham, 2016.\n[2] Bistritz, Ilai, Ariana Mann, and Nicholas Bambos. \"Distributed distillation for on-device learning.\" Advances in Neural Information Processing Systems 33 (2020).\n\n\n******** after rebuttal **********\n\nThe authors have addressed my questions to my satisfaction. While I agree with my fellow reviewers that this work is limited in its scope, I do enjoy this novel theoretical take on distributed learning with different model types via knowledge distillation. Thus, I vote for acceptance. I have updated my score accordingly.  ",
            "summary_of_the_review": "The paper presents an interesting theoretical analysis of co-distillation for two different kernel regression models. The empirical evaluation confirms the theoretical findings. The constrained setting on the one hand allows for strong theoretical results, but on the other limits the practical insights that can be drawn from them. Here, a broader empirical analysis would have improved the significance of the contributions.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduce an interesting perspective on federated learning via knowlegde distillation, which allows participating clients to have their own choices of models. In particular, the paper develops theoretical results for a two-client federated regression scenario, which demonstrates (a) the degeneration of an alternating knowledge distillation where iterative distillations (i.e., a client model at each iteration is re-trained based on unlabeled input + prediction of the other client's model of the previous iteration) gradually lose information over the iterations and eventually converge towards a vacuous model; and (b) a new ensembling technique that aggregates intermediate models produced by both clients over the iterations such that in the limit (i.e., when the no. of iterations tends to infinity), the aggregated model is the same as the (oracle) centralized model. This developed intuition on ensembling intermediate models is then applied to more realistic 2-client federated classification scenarios on MNIST and CIFAR-10 datasets.",
            "main_review": "NOVELTY & SIGNIFICANCE\n\nThis paper uses a simple 2-client federated regression scenario with simplified modeling choices (i.e., kernelized linear regression) with closed-form solutions to develop novel insights which can potentially be translated into new model-agnostic FL framework with strong asymptotic guarantees. The idea (as I summarized above) is novel to me. However, in terms of its practical significance, I am not convinced because of there is too much gap between the theoretic results and real-world setting, which is elaborated below.\n\n1. The theoretic setup started with 2-client setting but has not been extended to multi-agent setting. This is problematic because the result derived in 2-client setting is very specific to the iteration between the two clients and how they average between the local data & the distilled prediction of the shared models. If there are more than one shared models, how would the client set up their training output for the next iteration? Furthermore, the authors assume implicitly that all clients have the same amount of data, which is clearly not true in practical setting -- how will it impact the averaging scheme in such cases?\n\n2. Another peculiar restriction is that the presented result is tied to an obvious flaw in the way AKD is set up in this paper. From the 4 steps mentioned at the beginning of Section 4, client 2 will never get to see true output. I suspect this is a main cause that leads to AKD's degeneration. Given this, the theoretic result developed in 4.2 is kind of moot. Can the authors re-visit this result in case models at iteration t + 1 of any clients are built on distillation of models from iteration t instead -- this is so client 2 can see its true training output at iteration 1.\n\n3. The authors also proposed a data re-incorporation scheme (i.e., AvgKD) which performs better than AKD but given the 2nd point above, it is not clear if this mechanism is necessary if the peculiar flaw in AKD is fixed. My point is if this new mechanism ends up fixing only the flaw above and not anything else, then it is somewhat unnecessary because the above flaw can be fixed by allowing both client to share models concurrently rather than iteratively. Could the authors elaborate more on this?   \n\n4. The fix in 3. is later abandoned and replaced with a better EKD fix via ensembling intermediate AKD models. But again, this result is correct given the flawed setup of AKD as pointed out in 2. above. If this flaw is corrected, can the authors revisit the asymptotic convergence result for EKD?\n\n5. The result is largely based on a formulation of regression model and the result is certainly tied to this specific regression form. I am not sure if it is reasonable to anyhow impose the implication of the derived result on a very distant classification setting. In the same vein of thought, another minor restriction is that if we view this regression formulation from a probabilistic perspective then it appears the authors impose the same Gaussian likelihood across all clients and that kind of clashes with the model-agnostic motivation. \n\n6. Have the authors considered the communication cost beyond the 2-client setting? As the distillation requires access to local data, every client would have to send models to every other clients. The total communication cost is therefore N times for than the normal cost of FL and in addition, there will also be extra distillation expense, which is a lot more costly than aggregating model weights.\n\nSOUNDNESS\n\nThe results appear correct to me. But I have reservation about the theoretic implication of both AvgKD and EKD as I pointed out above: I am not sure if these proposals were meant to fix anything more than the obvious flaw in the presented AKD setup (e.g. client 2 never gets to see true training output). Furthermore, if the AKD setup were to be fixed by letting client 2 sees its true training output, would the convergence result of EKD still hold?\n\nCLARITY\n\nThe paper is well-organized with a nice narrative flow that is easy to follow. But on a minor note, there are a number of grammatical errors all over the paper.\n\nEXPERIMENT\n\nI find the experiment somewhat strange. It implies the initial local model of client 1 is already on the same level with the centralized model which means distillation does not help at all. In all 3 experiment settings, not a single one shows that distillation is being helpful. Am I missing something here? \n\nOn another note, the flaw setup on AKD is observable from the reported results. In all 3 settings, client 2 always perform worse than client 1. In fact, on the same model, same data setting, it is noticeable that client 2 is much worse than client 1 at the initial round. This is clearly because client 2 never sees its true training output and this seems to be the case that with the flaw, client 2 is initiating and leading the distillation degradation.\n",
            "summary_of_the_review": "This paper presents interesting thoughts that were substantiated via a theoretical exercise on a simplified setting. But unfortunately, in both theory & practice, this is still pretty much a work in progress. Given the huge gap between the theoretic and real-world setup, it is not clear whether one can apply the insight derived from the theoretic setting to another distant real-world setting -- please see my points in 1-5 above. Thus, while this could be the beginning of an interesting theory, more work is needed to complete the idea. As a matter of fact, the current experiment implies the proposed distillation is not working. In addition, I believe the authors have not considered the compute/communication cost of this distillation scheme.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}