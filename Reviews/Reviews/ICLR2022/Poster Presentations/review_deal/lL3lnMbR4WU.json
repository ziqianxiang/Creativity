{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "the aim of this work is to produce an open-vocabulary detector.  The approach is via knowledge distillation from existing large-scale V+L models, and the evaluation is based on novel classes with LVIS.  The reviewers were generally happy with the work (approach and results), but there were substantial points of clarification during discussion that need to be properly integrated into the final manuscript."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper aims to propose a novel approach for open-vocabulary or zero-shot object detection by leveraging models pre-trained on internet-scale data which might include many of the desired open-vocabulary categories. The proposed approach, called ViLD, uses the information learned by models like CLIP, and ALIGN to detect \"novel\" classes. The paper shows that the proposed approach can achieve better performance than supervised methods on rare classes in the LVIS dataset. However, the performance is much lower than state-of-the-art. Also, the performance on frequent classes in LVIS, and the overall performance is significantly lower than prior works. The authors also show that the proposed approach can achieve good performance improvements on the zero-shot COCO dataset - both for \"unseen\" and seen classes.  ",
            "main_review": "The paper presents strong results for some cases like rare classes in LVIS dataset. I would like to comment the authors on a well-written paper. It is mostly easy to understand. The authors have provided clear motivations and intuitions behind most of their design decisions. The diagrams and figures also assist in understanding the paper better. \n\nHowever, there are still several problems with the paper which need to resolved before it can be considered ready for acceptance. The performance on non-rare classes and the complete test sets is significantly lower than prior works even though the performance on rare classes is slightly better in many cases. This raises questions about the utility of the proposed approach. Further, using models like CLIP which have probably already seen the rare classes muddies the definition of a \"zero-shot\" setting as used in the paper. \n\nIn particular, I have the following concerns/comments/questions/suggestions about the current version of the paper. \n\n1. Do the authors know what classes the pre-trained image encoder (CLIP) is trained on? Has it seen any of the novel categories? If yes, then I would hesitate in calling it a zero-shot setting. I think the authors should just call it an open-vocabulary setting which can be weaker than a real zero-shot framework. Further, when comparing against prior zero-shot works, the authors need to clearly specify that the settings are slightly different because many of the prior works have ensured that no visual data from unseen classes has been used during pre-training or training. \n\n2. It's not clear to be why we need N and M proposals separately? This is what I understand currently: The N proposals come from the RPN being trained along with the other trainable weights and might change from one epoch to the next. On the other hand, the M proposals are obtained from the RPN at the start of training and are then fixed i.e. they do not change throughout training. Is this correct? If yes, why not just use the N proposals in both cases? In section 3.4, the authors mention that using the same proposals causes contentions. I am not sure how or why. Further, if my interpretation mentioned above is incorrect, then how are the M \"pre-computed proposals\" obtained during training? And how are they obtained during inference? Similarly, for N. \n\n3. In section 3.2, the authors mention that they ensemble 1x and 1.5x embeddings. Are there any emperical studies to show this helps? What if we just used 1x or 1.5x? What if we used 1x, 1.5x, and 2x? What if we used 0.5x and 1x? etc. \n\n4. In section 3.4, the authors mention that \"ViLD-image learns to approximate the predictions.....may improve performance\". This seems very odd to me. If ViLD-image already approximates the predictions of the teacher model, why do we need to add the teacher model directly?\n\n5. I am not sure I understand how ViLD-ensemble is different from ViLD. Is the diagram shown in Figure 3d ViLD or ViLD-ensemble? Can the authors please list all of the differences between ViLD and ViLD-ensemble?\n\n6. In the caption of Table 3, the authors mention that the SOTA model uses additional tricks. What additional tricks? Are they not applicable to ViLD? What is the performance of ViLD using those tricks? The performance on all r, c, f, and overall is very low compared to SOTA. Why is this? And what is the utility of the proposed approach if this is the case?\n Further, I think not making the SOTA performance bold in Table 3 might be disingenuous. Some reasons I can think for not doing this are 1. if the SOTA performance achieved in a supervised setting, 2. the SOTA model uses the rare classes during training, 3. the SOTA model uses different/larger data for training. Are any of these true? If not, the authors should highlight the SOTA performance and discuss why the proposed approach does not meet SOTA and what can be done to achieve similar/better performance. \n\n7. For table 4, what is the performance of supervised methods for COCO novel classes? The authors can do a similar comparison as done for LVIS. Further, the authors should clarify that the CLIP model in ViLD-image and the ViLD-text models have seen instances of $C_N$ during the large-scale pre-training which makes it a weaker zero-shot setting than prior works. \n\n\n\nTypo:\nOn page 6, in the 5th line from the bottom, there is an extra \"has\". ",
            "summary_of_the_review": "The paper proposes an interesting direction for open-vocabulary open detection. The authors have described most things about the proposed approach clearly and have written an easy-to-understand paper. \n\nBut the paper, in its current form, is not ready to be accepted. There are several concerns about the validity of the experimental set-up and comparisons against prior works. Further, there are several decisions taken by the authors which need clarification or more empirical evidence.  \n\n\n\nEdit: \nAfter reading the other reviews and the authors' responses to all the reviews, I have increased my rating to 6.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper addresses the task of open-vocabulary detection, where an object detector is trained from two forms of annotations: (a) bounding box annotations for a set of base categories and (b) open-vocabulary image-caption pairs.  This detector is then evaluated on a set of novel categories.  Compared to prior work, this paper tries to distill knowledge from strong pre-trained vision & language models (like CLIP) into an open-vocabulary detector. The authors propose multiple ways to do this (RCNN+CLIP, ViLD-text, ViLD-image, ViLD), including some ensemble variants (ViLD-ensemble). The paper also introduces a novel experimental setup based on the LVIS dataset (with >1200 categories) as compared to prior works that used COCO with only 80 categories.",
            "main_review": "### Strengths\n\n- The addressed problem is interesting and relevant for many practical applications\n- The authors did a good job exploring different variants of how to leverage a pre-trained vision & language model (i.e., RPN+CLIP, ViLD-image/text/ensemble)\n- The empirical results are very encouraging\n- The proposed setting of using LVIS for open-vocabulary detection is good\n- The paper (incl. appendix) provides several additional interesting experiments (transfer to other datasets, expanding of label space with attributes, etc.)\n\n\n### Weaknesses\n\n#### Clarity\n\n- Region proposal networks (Sec 3.1 and 3.3): It is unclear to me how the authors implemented the region proposal network. Only after reading the whole paper, it seems that there are two different variants: The first one is a two-stage detector as described in Sec 3.1. The other is the standard RPN network used in Mask-RCNN, mentioned in Sec 3.3. This aspect is still unclear to me.  In any case, the two-stage objectness detector (Sec 3.1) needs more explanation. First, one should add a reference to the appendix already in Sec 3.1. But even then, some training details may be missing because in Mask-RCNN, the details of how to assign boxes to anchors/proposals is different region proposal vs. region classification networks.\n\n- Related to the previous point, in Sec. 3.3, the authors mention the \"two-stage detector\", which was unclear to me. Does this refer to a standard Mask-RCNN with RPN or is this in reality a \"three-stage\" detector?\n\n- Sec. 3 - Notations: It might be worthwhile spending one more sentence or a small figure on explaining the typical architecture of the pre-trained open-vocabulary image classification model.\n\n\n#### Related work\n\n- One could add a reference to the related problem of \"open world object detection\". Specifically, a recent work [A] introduces an \"unknown-aware RPN\", which might be relevant for this work as well.\n\n- There have been a few recent works that try to expand the label space of object detectors by unifying the heterogeneous label spaces from multiple datasets into one model [B,C]. This seems like related work for the aspect of increasing vocabulary size.\n\n#### Experiments\n\n- Sec 4.5: It is not clear if the model used to compare on the COCO dataset uses FPN or not. If yes, I believe the comparison is unfair because the main competitor (Zareian et al.) seems to only use a ResNet-50-C4 backbone.\n\n\n#### Minors and suggestions\n\n- Sec 1: \"exponentially more data is needed (Gupta et al., 2019), which makes it expensive to scale up detection vocabularies.\". This statement is true for the images, but maybe not for the annotations, i.e., the human cost, because of the federated annotation technique that is used by LVIS (and also OpenImages >= v4). Please relate this statement to this type of dataset annotation.\n\n- Please add a citation for the Average Recall (AR) metric, which is computed over multiple IoU thresholds. This may be unclear for readers not familiar with the details of object detection evaluation, and the numbers may then seem surprisingly low for a metric that measures recall.\n\n- Sec 4.3: Typo: \"... CLIP on cropped regions already outperforms supervised baselines ...\"\n\n- Implementation details: Are the input images first resized and then cropped to 1024x1024?\n\n- Implementation details: What version of the CLIP model was used for distillation? CLIP provides pre-trained models for different image backbones, e.g., R-50 or ViT.\n\n- Why was the R-50 backbone not initialized with the weights from the CLIP-R50 model? That could further add knowledge \"transfer\".\n\n- Last paragraph in introduction: \"... that uses additional tricks\" is anecdotal. Please clarify. Also, I assume the challenge winners used full supervision for rare categories during training, which should be highlighted.\n\n- Sec 4.6: The statement \"... it is the first time we can directly transfer a trained detector to different datasets\" may be inaccurate. The multi-dataset object detection paper [C] evaluates in a \"zero-shot cross-dataset\" setting (see Table 5). It is a different setting, but it's related, which should be discussed.\n\n#### References mentioned above\n- [A] Towards Open World Object Detection. Joseph et al. CVPR'21\n- [B] Object Detection with a Unified Label Space from Multiple Datasets. Zhao et al. ECCV'20\n- [C] Simple multi-dataset detection. Zhou et al. arXiv'21\n",
            "summary_of_the_review": "Overall, I think this is a solid and interesting work. There are several aspects of the paper that need improvement, though, see comments above. I encourage the authors to consider those comments to further improve the manuscript.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a new method for training object detection models that can generalize to novel object categories without bounding box annotations. Their key idea is to use an existing vision-language joint embedding model (teacher), and distill its knowledge into a typical object detection model (student). This way, the detection model learns to project each object proposal into an embedding space, where it can be compared to the text embedding of various object categories, and the closest one can be chosen. The teacher model is an existing work (CLIP), which is trained on millions of image-caption pairs, and hence relies on weak supervision. The proposed method achieves impressive results on novel object detection, outperforming not only the best open-vocabulary detection method, but also fully supervised detectors. ",
            "main_review": "Strengths:\n\n1. The paper studies an important research area with many potential applications. It is a step towards more scalable and efficient training without a massive amount of supervision.\n2. The quantitative results are impressive, and provide valuable insights. Every row of Table 2 and Table 3 conveys a novel and interesting insight. Especially, the authors show that self-supervised embedding models trained on general-purpose image-caption data have the potential to outperform fully supervised models on real-world, long-tailed detection tasks.\n3. The proposed method can also be used for open-vocabulary instance segmentation, which is novel and may inspire future work in this area.\n4. Unlike prior work that relies on specialized pretraining, the proposed method decouples pretraining and training by using two separate models. This results in flexibility, because for instance, a large and complex model can be used for pretraining, while a fast, lightweight model can be used for distillation.\n\nWeaknesses:\n1. The proposed method has limited technical novelty. the authors distill an existing classification model on an existing detection model via an existing distillation method. Note that despite limited technical novelty, the empirical contribution is significant.\n\n2. The paper is at some parts vague and skips some details. For instance:\n\n    * The architecture of the text encoder is not explained anywhere.\n\n    * In Section 3.1, it seems the output of the second stage of Mask R-CNN is used as proposal, instead of the output of the first stage (RPN), which is normally called proposal. This is confusing because the output of the second stage is also the detected objects.\n\n    * There is an L2 normalization in Figure 3, which is not elaborated in the paper, and seems to be redundant as the authors use cosine similarity, which already normalizes the embeddings.\n\n    * The authors do not mention how many base and novel classes they have in the LVIS experiments.\n\n3. In VILD-text, pushing background proposals into a single point may cause them to collapse and not correctly map to novel class embeddings at test time. How do the authors address this problem?\n\n4. The authors use most of the categories as base classes, and particularly choose categories that have frequent annotations. It is not clear how the model would perform with a smaller amount of supervised data (both fewer categories and fewer samples per category), and how smaller data affects each component (i.e. recognition and proposal)\n\n5. The authors mention the CLIP baseline is slow, but do not provide quantitative speed metrics.\n\n",
            "summary_of_the_review": "The paper presents a new solution for an important, yet unexplored, research problem. It achieves impressive SOTA performance and provides valuable empirical insights. Hence, I believe this paper would significantly contribute to the field.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}