{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "All reviewers appreciate the suggested EM approach to goal-conditioned long-horizon reinforcement learning, and the technical contributions of the paper. While there is a mix in ratings, even the most critical reviewers feels that the paper has clear merits and is acceptable, and there are two solid acceptance recommendations. Overall, the papers significantly meets the standards of an ICLR paper acceptance."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes C-Planning, a method that generates subgoals for distant goal-reaching tasks. The method is built upon the C-learning method, which learns a goal-condition policy together with a function that can estimate the connectivity between two states. The method then leverages an EM-like algorithm to propose waypoints by selecting a subgoal from replay buffer according to the distribution between the sum of state-to-waypoint distance and waypoint-to-goal distance. The authors apply this approach to both maze domain and challenging meta-world tasks and show improved performance.",
            "main_review": "Strength:\n\n1. The method nicely combines the C-learning with SoRB-like search methods. If my understanding is correct, we can regard this approach as a single-step waypoint search based on the learned universal value functions. Such an approach benefits from the fruitful goal-conditioned value function learning and can make progress in sparse-reward settings. At the same time, waypoint selection allows the agent to do structural exploration as suggested by \"Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?\" and get a better value estimation by expanding the expectation of values. According to my understanding, it is still hard to know if or why only searching waypoints during exploration should be better than searching the whole path (like in SoRB) and if we need to use the sampled waypoints to help us accelerate the bellman-equation update. However, I think the idea is already enough to serve as a strong competitor for goal-reaching tasks.\n2. The method provides theoretical insights by modeling waypoint selection as an EM algorithm. At the same time, it shows good experiment results are good. I agree that the Obstacle-Drawer-Close task is pretty challenging. Success in such an environment is appealing. \n\nWeakness:\n\n1. My major concern is about the evaluation and the lack of link to the hierarchical RL methods. Many hierarchical RL methods exploit the way of combining a subgoal proposal network with a goal-condition function as C-Planning does. For example, the HIRO and HAC algorithms. In fact, we can also view LEAP, SoRB as kind of hierarchical RL algorithms in which high-level policies are model-based search methods. C-Planning employs a replay-buffer search based on the learned value function to propose subgoals. Though it is much simpler than previous approaches, the authors should at least provide a discussion about the relationship and the differences. I would also suggest authors conduct experiments on some classical environments such as AntMaze and AntPush to help researchers to position this work in literature.\n\nMinor:\n\n1. In Alg 1. line 7, there should be small bugs. There is no reason to concatenate the trajectory with itself.\n2. I am a little confused by the performance of SoRB. Its performance decrease when it runs longer in Spiral 11x11. What's the reason behind it? I also do not understand why it can not solve this task. The simple maze environment should be very friendly to the graph-based search method.\n3. The word CURRICULUM in the title is confusing. Do I miss any part that shows the proposed way-points are curriculum? I think curriculum refers to a process from the simple one to the hard one, but I don't understand why C-planning is a curriculum.",
            "summary_of_the_review": "This paper proposes a nice framework to combine goal-conditioned RL and waypoint selection. The method is neat and effective. Though the connection with the HRL is unclear from the paper, the advantages outweigh the flaws. I tend to accept the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "no",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents an goal conditioned algorithm that uses an Expectation Maximization  framework. The E step is a \"graph search\", and the M step is a previous developed goal reaching RL algorithm. There is an automatic curriculum learning mechanism that comes from using variational inference. The goal-conditioned policy optimization and the method for sampling waypoints are jointly optimized using the same objective.",
            "main_review": "Strength:\n\n Robust, solid and strong theoretical development of an EM algorithm to learn long-horizon goal conditional policies that learn in a curriculum manner.\n Clear experiments.\n Clear language and well explained paper.\n\nSome questions :\n\nWhy the negative binomial distribution assume a n = 2 (sentence before  to equation 1) Why the number of failures is 2? Or in other terms, why the planning horizon is always 2? Because of intermediate goal and goal?  Its assuming that need to do 2 steps at least? \nThe criticism about other papers using a Geometric distribution seems not adequate. The interpretation of Geometric distribution in this other papers seems similar to a NegBinomail(1- lambda, n). Sg should be passed to the Algorithm 2 - C-Planning.\n\nWeakness:\n \nExperiments:\n    1. Why experiments do not contained the same as RIS? Just for completeness to show that you do better than RIS on their experiments.\n     2. Why you don't consider to compare to the Skew-fit algorithm? Even if its different there some similarity on the curriculum learning with distribution that keep being modified?",
            "summary_of_the_review": "\n\nThis is valuable progress on the field of learning goal conditioned policies. curriculum learning have been a challenge and this work shows a clear advance and solid theoretical connection.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper presents a method for goal-conditioned RL, tailored to solving tasks with distant goals (compared to the current SotA in goal-conditioned RL). For that, a prior approach called C-Learning [1] (a VI-flavored reformulation of approximate control for goal-reaching tasks) is extended with a scheme for experience collection (collecting on-policy rollouts), which is akin to search / planning in the sense that the overall start-to-goal reaching task is broken up into collecting start-to-waypoint and waypoint-to-goal trajectories. It is argued that breaking up the problem, and respectively introducing a posterior distribution over waypoints, enables solving long-distance reaching tasks & makes for more efficient and stable goal-conditioned RL (intermediate tasks are easier to learn from, better quality of the data, waypoints easier to reach, etc.). An additional benefit is that there is no need for search during test time (faster evaluation of the policy). The claims are empirically validated on a number of relatively complex, simulated tasks (2D maze navigation, Metaworld robot arm manipulation). The method compares favorably to the considered baselines.\n\n**Technical details**\n\nIn technical terms, the scheme starts with the assumptions in C-learning, casting the goal-reaching objective as maximizing the prob. of the goal state under the distribution over reachable future states under policy (aligned with the VI view on optimal control). From there, a latent variable for intermediate waypoints is introduced, which leads to the formulation of an ELBO for the goal-reaching objective of C-Learning. Optimizing the ELBO is done via EM: in the E step an approximate posterior distribution over the waypoint $q(s_w)$ is implicitly defined via importance-sampling (proposal is the previously-collected empirical over states), cleverly utilizing the outputs of the classifier of C-learning for the importance weights. The waypoints sampled from $q(s_w)$ then guide the sampling of on-policy rollouts (start-to-waypoint, waypoint-to-goal) that reflect the two relevant terms in the ELBO (waypoint-to-goal corresponds to the \"likelihood / reconstruction\" term, start-to-waypoint to the \"prior\" part of the KL term). Conveniently, the two terms in the ELBO can be equated to two C-learning objectives, so the C-learning algorithm can be used as a black-box on the collected rollouts (this represents the M step of EM, maximizing w.r.t. the policy & classifier parameters in expectation over the rollouts from the E step).\n",
            "main_review": "## Strengths\n- The presented method steps on C-Learning, but I also find it manages to introduce sufficient theoretical and practical novelty. This is primarily expressed in the waypoint idea (coined C-planning), which is very nicely motivated through the probabilistic framework. The proposed ELBO reformulation of the objective in Lemma 1 appears natural and sound to me.\n- Using the classifier of C-learning to define the approximate posterior over waypoints through importance sampling is a nice conceptual trick. The outputs of the classifier are a proxy for the future state distribution, needed to evaluate the \"likelihood\" (waypoint-to-goal) and \"prior\" (start-to-waypoint) for the reweighting, so I find the usage very fitting, assuming the classifier really captures the distributions in question.\n- The considered environments are sufficiently complex, albeit simulated. The proposed algorithm seems to perform well in all of them and outperforms the baselines, including C-learning and a baseline that incorporates planning during test time, which is nice to see.\n- The E step of the developed EM algorithm (MC-estimation of the ELBO w.r.t. samples from the waypoint distribution) seems rather generic, and can probably be combined with approximate control algorithms other than C-learning (the M step), as long as they subscribe to the VI / probabilistic view on control.\n\n## Weaknesses\n- The ELBO (and respective PGM) in Lemma 1 is derived under the goal-conditioned policy, but later in Lemma 2, when the approximate posterior over waypoints is defined, and for pretty much all later derivations in the paper we see a goal-conditioned policy used for the waypoint-to-goal term (\"likelihood\" in the PGM for the ELBO), and a waypoint-conditioned policy for the start-to-waypoint term (\"prior\" in the PGM for the ELBO). This seems discrepant with the ELBO derivation. I checked Appendix A.1, and it hints at that, promising a proof of why this would be OK, but I did not manage to find it. I am setting my correctness score to 3 because of this issue, until it is clarified.\n- I think further clarity about the exact conditional inputs fed into the C-learning classifier are necessary, preferably in the main body of the paper. E.g. appendix E.1 mentions a secondary classifier is needed, leaving the control input out (which I imagine is necessary for e.g. the weights in Lemma 3, etc.). Given that the classifier (critic) introduced in C-Learning is a proxy for the assumed distribution over future reachable states, I think this is important. In general, spelling out exactly which distributions are approximated through the classifier (and for which policies) would make the paper more self-contained.\n- It was hard for me to glean from the paper why exactly RIS [2] (which adds planning to the objective directly) fails in comparison to C-planning (in which planning is expressed through the waypoint selection when collecting experience, i.e. it's reflected in the empirical of collected data). Section 2 mentions this is because \"(C-planning) avoids favoring the learned policy\", can you please elaborate? I think this is an important distinction.\n\n## Further remarks & questions\n- In Algorithm 1, line 7 currently says that two rollouts are added to the data set, both start-to-waypoint under the current policy. Shouldn't one of them be waypoint-to-goal, to match the second term in the ELBO, or am I misinterpreting? Also, it might be good to specify the conditioning of the policy at this point, for clarity.\n- Looking at Algorithm 1, the same policy and classifier appear in every loop iteration, for different episodes. Are their parameters kept in-between episodes, or do they get reset? Also, does C-learning run to convergence for every episode? Would be good if this is specified somewhere.\n- The assumed NegBinom model on the assumed distribution of episode lengths seems like a choice out of convenience. Judging by the appendix, it's equivalent to concatenating two rollouts with geometric-distributed length, which then fit the C-Learning assumption. Was there any other motivation behind it, and did you consider alternatives?\n- The link to code advertised in the paper did not work for me (I only see the paper title on the page, no videos either).\n- The title of section 4 is currently at the bottom of page 3.\n- I think the caption of Figure 4 does not reflect the order of the figures.\n- In section 5, Results, it says C-planning performs worse on Push, but if I am reading figure 4 correctly it should be Reach.\n\n## References\n[1] Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. C-learning: Learning to achieve goals via recursive classification. ICLR, 2020.\n\n[2] Elliot Chane-Sane, Cordelia Schmid, and Ivan Laptev. Goal-conditioned reinforcement learning with imagined subgoals. ICML, pp. 1430–1440. PMLR, 2021.\n",
            "summary_of_the_review": "I find the proposed method to be a nice step-up from existing prior work, both theoretically and in terms of performance. The proposed incorporation of planning (in terms of waypoints) appears well-justified, and seems to enable solutions to a variety of goal-reaching tasks that are not trivial. Heuristics also seem to be mostly avoided, which is appreciated.\n\nI would recommend acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work extends the previous work, C-learning, by using search at training time to generate a curriculum of intermediate states enroute to the goal state. The main idea is to decompose the goal reaching problem into a sequence of easier tasks of reaching the reachable intermediate states. Authors prove that under the negative binomial prior with $n=2$, the goal-reaching RL objective is lower-bounded by the sum of two goal-reaching objectives: reaching waypoint $s_w$ from the initial state $s_0$ and reaching the goal state $s_g$ from the way point $s_w$. In practice, authors proposed to uniformly sample the way points from the replay buffer and apply importance sampling afterwards to avoid the challenge of sampling high-dimensional waypoint states. The experiment was conducted on various goal-conditioned RL tasks on 2D mazes and the robotics manipulation tasks. The C-planning method outperforms the compared baselines on challenging tasks. Authors also conducted various ablation studies justifying several design choices made in the proposed method and demonstrating the light deployment-time computation cost of the proposed method.",
            "main_review": "Main concerns\n1. Missing important baselines\\\nThe experiment is missing an important baseline: [Huang et al., 2019]. In section 2, authors claim that [Huang et al., 2019] does not perform planning in train time and does not improve the underlying goal-conditioned policy. However, this is not true. [Huang et al., 2019] also improves the quality of the data used to train the policy by improving the exploration in training. Specifically, [Huang et al., 2019] periodically populates the landmarks using the farthest-point-sampling to explore the frontier of the explored state space.\nAnother important baseline to consider is the goal-relabeling methods because they also provide additional learning signals from the states visited during training for GCRL. It would be very helpful to demonstrate that the proposed method is a better way to provide additional learning signals to GCRL than the alternatives.\n\n2. More intuition on “iterative” waypoint sampling\\\nTo my understanding, the optimal way point $s_w$ that minimizes the proposed objective is the midpoint between current $s$ and the goal state $s_g$ regardless of the choice of $n_g$. However, this is not really the optimal choice for the GCRL. The better waypoints would be (arguably) the equally-spaced $n_g$ points on the route from initial state to the goal state. If authors can share the philosophy behind this specific design choice, it would be very helpful for the reader.\n\n3. The choice of prior\\\nAuthors proposed to choose a specific prior model: negative binomial distribution with $p=1-\\gamma$ and $n=2$. It seems this assumption is necessary for the decomposition of Eq.(2) into Eq.(4), but the specific design choice could be justified/explained better. Also, it is counter-intuitive in that the pdf of negative binomial distribution is bell-shaped. It means the agent receives a *smaller reward* if it achieves the goal “too early”. \n\nMinor concerns\n\n4. Searching at training time\\\nIt may be controversial whether C-planning performs (implicit) searching in train time. The C-planning queries the distance between several pairs of states, but it does not perform any path finding\n\n5. RIS is not learning at all\\\nIn figure 4, the performance of RIS is consistently the worst among the compared methods and seems RIS is not learning at all even in the easiest task. Is this expected? It is unclear whether the RIS is correctly reproduced and the hyperparameters are appropriately tuned.\n\n6. Performing the test-time search\\\nIt seems natural to me to consider also performing the waypoint sampling in test-time similar to training. In figure 5, authors instead implemented the C-Planning + SoRB. Is there a reason that C-planning cannot perform test-time waypoint sampling? If possible, does performing test-time search improve the performance?\n\nQuestions/suggestions\n\n- Q: in line 7 of Algorithm 1: What’s the difference between $\\tau_1$ and $\\tau_2$? Why do you need those two?\n- Gradient analysis: it would be great if authors can provide at least an intuitive explanation of why increasing  the norm of critic norm and decreasing the variance of the actor gradient norm is helpful for learning in the paper for the readers\n\n[1] Mapping state space using landmarks for universal goal reaching. Huang et al., 2019.\n\n[2] Hindsight experience replay. Andrychowicz et al., 2017\n",
            "summary_of_the_review": "The paper presents a non-trivial extension of the previous work and presents an interesting viewpoint of formulating implicit train-time searching as variational inference. The paper is clearly written and easy to follow. However, I found that the experiment section is relatively weak and the presented results are less convincing. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}