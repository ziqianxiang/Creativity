{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "Implicit neural representations are a new and promising method to represent images and scenes. Implicit neural representations enable good performance on task like view synthesis. Those networks generate an image of scene pixel-by-pixel and are therefore computationally expensive. The paper proposes a method to accelerate inference with an MLP by learning each dimension of the input (e.g., x, y, and z coordinates) separately. The paper reports speedups by a factor of up to three.\n\nThe four reviewers all agree that this paper should be accepted. The reviewer's highlight that the speedup is a solid technical contribution, and agree that the idea of splitting the coordinates is well motivated and well evaluated, and leads to the advertised speedups. \nDuring the review process, the reviewers also raised some issues, such as a slight performance loss at that cost of a slight increase in efficiency, but were convinced by the response of the reviewers. \n\nI recommend accepting the paper. Like the reviewers, I think that the proposed idea is interesting and technically sound, however, I'm a bit concerned about the slight drop in performance: If a slight drop of performance is allowed, then a speedup is possible by simply making the networks smaller, reducing the layers, or through other more immediate means than the proposed one. The contribution would be even more convincing if the paper also compares the speedup in a fair setup where the performance is kept constant."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "Summary: This paper proposes a modification to INR models on multidimensional coordinate grids where a subset of the earlier layers operate on the decomposed coordinate grid. In this setup, the grid (which is assumed to be regularly sampled to permit this decomposition) is broken into its constituent components, (e.g. x and y instead of (x,y)), passed through a single linear layer unique to that component, then through a stack of shared layers, followed by an outer product to return to the joint (x,y) space, and at least one layer that operates on the joint space. This approach lightly reduces parametric efficiency but strongly improves compute efficiency (both in terms of FLOPS, memory usage, and actual observed runtime) for common implicit neural representation tasks, including fitting images, videos, shapes, and volumetric rendering via radiance fields.\n",
            "main_review": "My take:\nWhen I first looked over this paper, I thought to myself, “That’s crazy! Does that actually work?” When I read it in detail, I thought, “Okay, the authors have definitely done their due diligence, and thought through a lot of the caveats needed to get this idea to work and to appropriately reap the benefits.” I then took to implementing it myself in a range of benchmark tasks involving implicit neural representations and in other architectures. As it turns out, this absolutely works as advertised, is extremely easy to implement, and provides the advertised compute speedups, albeit at a slight cost in parametric efficiency. What’s more, some digging turns up that there’s actually a reasonably solid theoretical justification for this approach: as noted by Hinton in arxiv.org/abs/2102.12627, “The Kolmogorov-Arnold superposition theorem states that every multivariate continuous function can be represented as a superposition of continuous functions of one variable.” I think this paper needs a bit of cleanup on the presentation and clarity side, but is clearly in accept territory. I am willing to champion this paper: while I don’t necessarily think its impact potential is massive, I think it does have the potential to have a reasonable degree of impact, and I especially believe that we should highlight solid work which teaches us something *surprising*.\n\n\nDetailed notes:\n- The authors mention that they require more parameters to reach a given performance level a few times, but I think they should be more upfront in specifying that this is a drawback of the method. I have experimented extensively with this and it is quite clear that, if one keeps a fixed reconstruction accuracy target, CoordX models require slightly more parameters than a non-CoordX model to reach that target regardless of architecture; this can also be seen in lines 1 and 3 of Table 5. As most readers/practitioners likely won’t care about a reduction in parametric efficiency due to the improvements in compute efficiency, I don’t think this will make the paper seem any weaker (and is more honest anyway). Specifically I would like to see mentioned in the second bullet point under contributions that the parameter count is *comparable* but is almost necessarily *slightly larger.* This also retains the specific design of the weight-sharing layers as a visible contribution. \n\n\n-One “drawback” of this model is that it does not improve things in the unidimensional coordinate case. This is not actually a drawback, but rather a lack of a benefit, but that’s perfectly fine. It might be worth mentioning this as e.g. audio representation would not benefit here.\n\nMinor:\n-The very first sentence of the introduction appears to be some sort of typo or accidentally left-in mistake (“recently coordx…”). Please fix this.\n",
            "summary_of_the_review": "This paper presents a technique of general interest to the subfield of implicit neural representations, has solid empirical results, and is in my opinion quite surprising. The paper is clearly in accept territory.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes an interesting tweak to the network architecture to accelerate CoortMLP. The idea is to split the input coordinates along the dimensions and then share weights before fusion.  The authors analyze the theoretical upper bound (as far as the MAC ops are concerned) and show about 2X speedup on actual machines.",
            "main_review": "I have no major issue with the paper. The idea is clearly presented with nice qualitative and quantitate support. The matrix decomposition discussion in Section 3.3 is pretty interesting to read. The algorithmic novelty, however, is rather limited. It's incremental, but delivers good results.",
            "summary_of_the_review": "Novelty is limited. Practical engineering efforts with wide applicability and good results.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a novel coordinate-based network architecture which proposes to process each of the input coordinates independently in the first layer instead of together in a fully connected layer. This input style results in a speed-up in terms of evaluation of the network, and thus faster training and inference in tasks where coordinate-based MLPs are used, without incurring a significant degradation in terms of the quality of the signal fit. These benefits are demonstrated for the tasks of image representation, video representation, and 3D shape representation.",
            "main_review": "The strengths of the paper are as follows:\n1. The quantitative results are communicated clearly, and the benefits of the method are well justified and ablated. \n    - The benefits in speed and lack of degradation in quality are shown across three signal fitting tasks (image, video, shape representation). It is also demonstrated that this idea can work for local implicit methods (using latent codes) and still provide a speed-up with limited degradation in quality. \n    - The degree of splitting versus number of layers comparison is insightful and shows that the method proposed is in fact more powerful than simply using smaller MLPs (i.e. the individual processing FC blocks for each coordinate are highly useful).\n2. The contribution of the paper is quite clear. I think that the simple idea of splitting coordinate processing, ablating it well and showing that it doesn’t degrade quality but speeds up the training and evaluation is quite clear. This makes it much more likely to be more widely adopted in practice, as faster training of coordinate-based representations of signals is a very important problem with lots of recent work.\n\nIn my opinion, the weaknesses of the paper are as follows:\n1. The exposition of the significance of the results, and the necessary contributions to obtain them. \n    - While fitting a single image, video, or 3D signal with a coordinate-based network is intellectually interesting as a toy problem, the main use of coordinate-based networks is in neural rendering. This means that it is essential for CoordX to be able to be applied to the volume rendering of NeRF-like methods, or the ray-tracing applied in neural surface based methods. However, to find any mention of volumetric methods using CoordX, I had to get to section C in the appendix. I think that this should absolutely be included and emphasized in the main paper, and ideally more experiments should be dedicated to this instead of only overfitting on the Blender lego scene. Additionally, this section should include more ablation on the properties of the modified volumetric rendering proposed. Without this, the magnitude of the contribution is severely limited, as simply being able to overfit a few types of signals faster isn’t that impactful to many applications.\n    - Similarly, section A of the appendix should be mentioned in a bit more detail in the main paper. The input point sampling is crucial for actually obtaining the speedup benefits of the CoordX architecture (just doing the same minibatch training as standard coordinate-based networks would not yield any speedup), and thus the contribution of CoordX should be both the modified architecture and the sampling method.\n2. Following weakness 1, first bullet point, I think that while the evaluation on the lego NeRF model is very crucial to include in the paper, I think that this should be studied more and more results should be demonstrated on more varied datasets. This would make the paper significantly stronger, because then CoordX architecture could be applied in many neural volume rendering systems, which have had a significant amount of work in recent years.\n\nMinor comments:\n- I think that the introduction could be reordered to set a better context for the contribution so as to not make it immediately confusing. For example, starting the first sentence with what CoordX does doesn’t make sense since I haven’t read about CoordX yet. Starting by introducing coordinate-based MLPs and their applications makes it more clear in what area the contribution of the paper will come from.\n- The notation is a bit confusing in equation (2). Shoudn’t $W_j$ also have an $i$ index, since this is an independent weight matrix for each branch?\n- It would be nice to have an ablation on the feature fusion method, as concatenation versus multiplicative modulation versus additive modulation has been a topic in generalizing over coordinate-based networks. This could certainly be interesting and enlightening on whether or not this is a crucial part of the CoordX architecture.\n",
            "summary_of_the_review": "Overall, the paper proposes an architecture modification which improves the evaluation time of coordinate-based MLPs without significantly hurting the quality of the signal fit. This contribution is demonstrated ablated clearly for signal memorization, and I believe that the proposed architecture has value for direct signal fitting tasks in coordinate-based networks. However, the effectiveness of the method is not immediately clear for applications in neural rendering, where coordinate-based networks are mainly used. A novel formulation of volume rendering which is compatible with the architecture change is proposed, and evaluated on one scene, which provides promising results that this can speed up the training of NeRF-like models in neural rendering, but this section of the appendix could be expanded upon significantly to create a significantly stronger paper. Based on the current state of the paper, I believe that the architecture shows promise and is ablated well for signal memorization, but may not be as significant as it could otherwise be. Thus, I believe it is marginally above the acceptance threshold.\n\n**Post Rebuttal Update**\n\nThe authors have done a good job of addressing my concerns in my original review. I think that while the paper does propose a small algorithmic change and relies on more complex sampling methods to work for general coordinate-based network applications (as noted by other reviewers), the results and contributions are ablated well and the simplicity of the idea could likely have a larger impact upon practical usage of coordinate-based networks. Thus, I have updated my score, and think that the paper in its current state should be accepted.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new architecture for implicit neural representations, called CoordX, which splits each dimension of the input signal into separate branches (e.g. the x and y coordinates of pixel locations in an image) and processes each of these separately before fusing them. The authors achieve this by projecting each of these branches into a hidden feature and then using shared fully connected layers to process these. Each branch is then fused by an outer product which then reconstructs the entire input grid (e.g. for an image of size H x W, 2 branches take in H locations and W locations respectively which are then combined into H x W features by the fusion operation). The fused layers are processed by a few more MLP layers to output the predicted features. In addition, the authors propose a method for effectively subsampling the grid during training as well as different splitting strategies for the branches.\n\nThe authors perform experiments on various data modalities, including images, videos, 3D shapes and NerF scenes.\n\nThe main contributions of the paper in my eyes are then:\n- Introducing a new architecture for implicit neural representations that in certain cases can improve training/inference speed without incurring a decrease in reconstruction\n- Experiments on various data modalities showing the strengths/weaknesses of the method\n",
            "main_review": "While I think the motivation of this paper is interesting and I appreciate the many experiments the authors have run, I still believe there are quite a few issues with the paper which need to be addressed. The main strengths and weaknesses (in my eyes) are described below.\n\n**Strengths**:\n- The motivation of the paper is good and improving the speed of implicit reps is an important topic. The observation that building methods that leverage the locality between coordinate points could help implicit reps (as opposed to regular implicit reps that consider points as independent elements of a batch) is interesting\n- The authors perform experiments on a wide variety of data modalities (images, videos, 3D shapes, NeRF scenes) and make some interesting ablations\n\n**Weaknesses**:\n- In general, the experimental results are not very convincing. For example, on the image experiments, the training time can be halved but at the cost of a 3dB drop in PSNR. The authors refer to this as “a slight drop in quality” but 3dB is a very significant drop (PSNR is measured on a log scale). For videos and the superresolution experiments the reconstruction results are on par with the baselines but are faster, which is good. However, comparisons to e.g. [1] or other methods aiming to accelerate implicit reps are missing. Methods such as these would very likely both be faster and have better reconstruction accuracy than the proposed method. Adding ablations against more than just vanilla implicit reps models would definitely help strengthen the paper.\n- One of the main drawbacks of the proposed method is that the decomposition operation requires a regular grid. A huge part of the motivation for using implicit representations is exactly to be able to model data that does not lie on a regular grid, e.g NeRF scenes, meshes and so on. So this does seem to be a significant drawback. While the authors briefly discuss this and propose a fix for NeRF scenes, this fix still relies on the existence of some underlying grid (256^3) which then limits the resolution of the signal.\n- The related work section disregards some very closely related work in accelerating training + inference of implicit representations [1]. While the authors cite this paper among others, they then mention that competing methods either are only applicable to a single task (not the case for [1]), have high memory overheads (not the case with [1], in fact memory is reduced) and they are not effective for dense/rich signals like images (not the case for [1], which can model gigapixel images). While [1] can arguably be considered concurrent work, I believe there should be a more fair discussion of this.\n- The paper writing is not very clear. The authors introduce a lot of notation which can make the paper heavy to read. In addition, the authors seem to use very non-standard notation. For example, in equation (1) and (2) it appears like the authors use ; to denote matrix multiplication which I’ve never seen before. In equation (3) the outer product is denoted with a typical product symbol, it would be much clearer to just write this as an outer product.\n- The paper seems rushed and there are many typos. For example, it looks like the first sentence of the paper is misplaced.\n- Comparisons on e.g. NeRF scenes do not seem fair as there is a lot of work accelerating the rendering of NeRF scenes which considerably outperforms the proposed method (including the papers [2, 3] that the authors already cite). It would be nice to add comparisons to these too.\n\nThe authors also mention in the conclusion that decomposing signals into smaller signals that can be used to reconstruct the original signal could potentially be applied to data compression. This was one of the first thoughts I had when initially reading the paper and I think it would be interesting to expand more on this, e.g. in the context of [4].\n\n[1] ACORN: Adaptive Coordinate Networks for Neural Scene Representation, Martel et al.\n\n[2] FastNeRF: High-Fidelity Neural Rendering at 200FPS, Garbin et al.\n\n[3] PlenOctrees for Real-time Rendering of Neural Radiance Fields, Yu et al.\n\n[4] COIN: COmpression with Implicit Neural representations, Dupont et al.\n",
            "summary_of_the_review": "This paper introduces a new architecture for implicit neural representations. While there are some interesting ideas in the paper, overall I still believe more work needs to be done and better comparisons to baselines need to be made before this paper is ready for publication.\n\n--- Post rebuttal update ---\n\nI appreciate the large efforts the authors have put in to address the issues put forward in this review and by the other reviewers. With these changes I believe the paper is stronger and have updated my score accordingly. In particular I appreciate the experiments showing that CoordX can also help approaches such as ACORN (even though these results still seem preliminary, particularly for Tokyo). Including these results in the updated paper will definitely help give a clearer picture of the contributions. The extended discussion and experiments around NeRF scenes are also helpful and appreciated. \n\nWhile I still believe there are some drawbacks with the proposed approach (in particular that using the method for non grid-based signals requires slightly hacky workarounds and that the gains by the proposed method are quite small compared to other methods aiming to accelerate implict reps e.g. ACORN), I am happy for this paper to be accepted in its current updated state.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}