{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper presents a new graph neural network layer that is sensitive to topological structure in the graph. Reviewers all believe the work is technically sound, and the experiments (particularly after author revisions) show clear benefits in cases where topological structure is important. The main questions are about whether the experimental evaluation is sufficient. While there are always more experiments that could be run, I tend to agree with the authors that the chosen experiments support the key claims in the paper, so it seems ok. The other question about the experiments is if they sufficiently convince the reader that topological structure is useful in practice. This seems more mixed. The paper would certainly be improved if there was a motivating application where there was a clear win. For example, molecular structures are used as motivation in the intro, but the best performing method on proteins doesn’t use the topological layer. All-in-all, though, there does appear to be clear improvements on carefully constructed cases, and there appear to be some benefits in real-world datasets."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors present a topology analysis improvement to GCN, using persistent homology, to capture global information regarding the topology of the graph. \nThe authors conduct several experiments, from graph to node classification, and also introduce two novel data sets to exemplify the importance of topology. \nIn most cases, the proposed method outperforms other baseline methods, as well as other topology aware methods.\n\n",
            "main_review": "The paper is well motivated and mostly clear and easy to follow.\nThe method is well formulated and the experiments details are clear.\n\nIn terms of weaknesses of the paper, I have two main concerns:\n\n1. Indeed, as discussed, the aspect of oversmoothing is important in the case of GNNs. However, for a better verification of the existence/absence of oversmoothing with the proposed method, much layers need to be stacked, and preferably more datasets (like Cora, Citeseer and Pubmed) ,should be compared against to other recent methods like GCNII [Chen et al. 2020] .\n\n2.In the context of topology, which is the main contribution and scope of this work, it would be interesting to see more geometrically oriented data sets like ABC or Thingi10k (this is just an example), where objects with different topologies are given. Also, it would be interesting if the authors can discuss a recent ICCV paper that deals with geometrical data using persistent homology :\n\"Persistent Homology based Graph Convolution Network for Fine-grained 3D\nShape Segmentation\"\n",
            "summary_of_the_review": "The paper is clearly written and adds a contribution to the GNNs society. Also, the numerical experiments suggest improvement over other data sets. Some of the experiments can be improved, and since topology is of high interest geometrical datasets and application, I think that some discussion should be added, and will strengthen the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Motivated from the field of topological data analysis (TDA), the paper proposed a Topological Graph Layer (TOGL) plugin for Graph Neural Networks to boost the ability of topological structure detection.",
            "main_review": "The paper provides new insights into topological aware GNN and I summarize the pros as follows:\n1. The work is well motivated. First of all, Figure 1 gives a motivational toy example to demonstrate the necessity to capture the topological structure. I really like the example, which is clear and easy to understand. Besides, I appreciate that the authors provide a comprehensive introduction and research roadmap in the related work section.\n\n2. The work is technically sound and relatively novel as it introduces persistent homology, which is more powerful than the WL-test from the view of topological awareness (Theorem 2), and provides a reasonable framework to integrate it. \n\nComments:\n1. I am confused about the topological structure awareness power of GCN illustrated in Figure 1. From both two subfigures, Vanilla GCN overperforms WL-test. The results are a conflict with the analysis of GIN~[1], where GCN should be less powerful. From my perspective, GCN first aggregates the neighbourhood features and then pool features with a mean operator. Such a message passing mechanism should be less powerful than WL-test.\n\n2. There may be a gap between the Betti numbers and the learned graph signal from the vertex-based filtrations, as no specific training loss was used to guarantee the alignment between the two terms.\n\n3. The author used the DeepSets embedding function as in section 4.1 to embed different graph signals into high-dimension space. However, the paper contains no empirical study to validate the choice.\n\n4. Though TOGL is demonstrated strictly powerful than WL-test, the experiment results from Table 2 showing that WL-test overperforms in several datasets.\n\n[1] Xu, K., Hu, W., Leskovec, J. & Jegelka, S. How Powerful are Graph Neural Networks? Arxiv (2018).",
            "summary_of_the_review": "Motivated by topological data analysis, the paper proposed the TOGL plugin for GNN to boost the ability of topological structure detection. The idea is insightful and is analyzed theoretically and empirically. Though I have a few concerns as listed in Main Review, I would like to weakly accept the work. Surely, I will appreciate it if any of my concerns can be addressed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Authors propose a topology-aware layer that is compatible with GNNs and can encode connected components and cycles to let the GNN learn better representations based on such global topological features.",
            "main_review": "The paper is well-motivated, well-written, and builds on a good theoretical framework. The authors also explicitly mention the limitations that they had to impose due to computational constraints (such as l1 constraints). They also show that in practice even using limited topological features such as connected components and cycles, helps GNNs perform better. There are three questions that I would like to ask the authors:\n\n1- Diffusion-based models also capture some global topological information about the graphs and it has been shown that using them in both supervised and contrastive manner significantly help GNNs. How diffusion-based models are compared to your model and what if there are any theoretical links between them and your work.\n\n2- I am curious to see how your model can contribute to contrastive learning with linear evaluation protocol as it seems it can introduce a good amount of signal. \n\n3- I was hoping to see this method to be applied to large-scale benchmarks on OGB rather than small (and almost overfitted) TUdataets. Benchmarking the model on those datasets can show the scalability and the performance of the proposed model.\n\n4- Finally, expanding more on the theoretical aspects of the work and maybe providing examples would make the paper more eadible.",
            "summary_of_the_review": "The paper roots in a nice theoretical framework but shorts fall on experimental side.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper introduces TOGL, a new layer for Graph Neural Networks (GNN), making the GNN \"aware\" of topological information during this training phase. It differs from the closely related work *Graph Filtration Learning (GFL)* (although taking inspiration from it) as GFL is mostly a readout function (roughly, final layer in a GNN) while TOGL is a more general type of GNN-layer. Numerical experiments show how, when topological information is relevant, TOGL helps to leverage it.",
            "main_review": "# Strengths \n- TOGL may be a useful tool for the GNN community, while also opening the door for some further research in Topological Data Analysis (TDA). \n- The paper is well-written. \n\n# Weaknesses\n- The flow of the paper may be hard to follow for the non-GNN-expert ready (i.e. TDA-side reader) due to the lack of a short background section dedicated to the topic. \n- Numerical results, though fairly solids, are not striking either. \n\n# Minor comments : \n- I think that writing $f_\\theta$ instead of $f$ would improve clarity of some claims throughout the paper (it feels weird to me to write say \"the map $\\theta \\mapsto \\Psi(\\mathrm{ph}(G,f))$ is differentiable\", as the right-hand-side does not (visually) depends on $\\theta$).",
            "summary_of_the_review": "This is a competent paper that builds upon existing works and provides an interesting application case of TDA and may be a good addition to GNN toolboxes.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}