{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper studies the problem of how to construct orthogonal convolutional layers. It is known that a convolution layer is orthogonal if and only if its filters are obtained by certain Fourier operations on an orthogonal matrix. Previous work proposes to learn this orthogonal matrix, parameterized either through Cayley transform, or the exponential of a skew-symmetric matrix. This requires spectral computations with large matrices. The idea of this submission is to reduce the computational cost associated with this construction by letting this “core” matrix P be a periodic extension of a smaller orthogonal matrix P_0. Because of cancelations in the inverse DFT, this leads to sparse filters which can be implemented by dilated convolution. \n\nThe review process generated a very detailed discussion between authors and reviewers, with several important clarifications. Reviewers generally found that the paper contributes a novel construction of orthogonal convolution layers, with better efficiency at test time. Remaining concerns held by some reviewers include the limitations vis previous constructions of orthogonal convolution layers, questions about the efficacy of use of a Taylor expansion, certain minor limitations of the experiments. After detailed interaction with the authors, the reviewers converged to a decision to accept, motivated by the novelties of the construction and its advantages for test-time efficiency."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "Summarizing the paper\n-------------------------------\n    \nThe paper proposes a method to construct a  convolutional layer with the orthogonal Jacobian matrix. Such a layer is 1-Lipschits: this property is an important one for building robust neural networks.\nThe authors conduct experiments on classification tasks using CIFAR10/CIFAR100 datasets. They report standard and robust accuracies, training, and evaluation time. They claim the proposed method outperforms existing state-of-the-art approaches.\n    \n",
            "main_review": "Benefits\n-----------\n    \n- They show the link between singular vectors of the Jacobian of the convolutional layer and the convolutional kernel structure.\n- The evaluation time of the proposed convolutional layer coincides with the evaluation time of the standard layer.\n- They clearly describe how existing relative approaches differ from the proposed one.\n- They obtain analytical formulae that yield a nice three-step approach to construct orthogonal convolution.\n    \nDrawbacks\n--------------- \n- Longer training time for proposed ECO (Table 2)\n- Simple datasets \n- No black-box attacks\n    \n    \nSuggestions\n-----------------\n- Add schematic visualization for forward and backward pass.\n- Demonstrate results for more complex datasets (e.g. ImageNet)\n- Add loss curves to see if there are effects aka standard&robust accuracy trade-off (similar to what is happening during adversarial learning)",
            "summary_of_the_review": "Recommendation \n------------------------\nI recommend accepting the paper because the proposed method is new, addresses an important problem and empirically demonstrates its benefits.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes an economical method to construct orthogonal convolutions using dilated convolutions.",
            "main_review": "Overall, I think the idea in the paper is novel and elegant. However, there are a few aspects in the current draft that are not carefully developed/explained.\n\n1) The limitations of the proposed method. a) In previous approaches (BCOP, Cayley, SOC), the convolution kernel is orthogonal for ANY resolution. In contrast, the convolution kernel is orthogonal ONLY IF the resolution equals the product of filter size and dilation. b) As a result, the method limits the flexibilities of orthogonal convolutions --- for example, the method can not construct orthogonal convolutions for features with a prime-number resolution. While orthogonal convolutions are rarely used for different resolutions or prime-number resolution, I think the authors shall discuss these two limitations explicitly.\n\n2) \bReal frequency domain. The matrix P is the Fourier transform of the convolution kernel, which generally is complex-valued. Restricting to the real-valued matrix has two drawbacks. a) The filters must be even, i.e., the method cannot construct orthogonal convolutions that are asymmetric. b) The space of real orthogonal matrices is disconnected, and the matrix exponential can only cover one component SO(c). I don't see why the authors do not adopt unitary matrices, which do not suffer from these two drawbacks.\n\n3) Computation of matrix exponential. The paper uses truncated Taylor expansion instead of other more computation/memory efficient methods --- The SOC paper adopts truncated Taylor series because there is no efficient algorithm when A is a convolution instead of a matrix. Indeed, a cited article (Casado & Martínez-Rubio 2019) provides an exact and efficient algorithm to parameterize orthogonal/unitary matrices. However, using the current method, the authors will have to analyze how truncation error affects the exact orthogonality of the convolution kernel.\n\n4) Other minor issues. a) At the end of the second paragraph of the Introduction (Section 1), the authors write \"Straightforwardly, we can ...\". This is not correct. Even one can expand the kernel to a Jacobian matrix, it is not straightforward to construct a matrix that is both block-Toeplitz and orthogonal. b) In the second paragraph of the Implementation (Section 5), I think the padding will be proportional to the product of filter size and dilation instead of addition. c) The experiments only repeat the ones in SOC paper, which do not fully demonstrate the proposed method in other settings (e.g., Wasserstein estimation).",
            "summary_of_the_review": "In summary, I believe the idea in the paper is novel and promising. However, the authors have not thoroughly/carefully developed the picture (as explained above). Therefore, I suggest rejecting the paper for now --- but I look forward to upcoming revisions.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper studies how to construct orthogonal convolutional networks in an efficient way. To this end, this paper builds the connection between the DFT-transformed kernel with the common dilated convolution. During training, the forward pass can be done by a sequence of inverse DFT and dilated convolution. During testing, all the convolution kernels only need to be transformed once so that evaluation time is significantly reduced.",
            "main_review": "Pro:\nThe authors propose a theoretically-motivated way of constructing orthogonal convolutions that achieves good robust performance on several benchmarks.\n\nIt has a big advantage on the testing speed over the previous state-of-the-art, SOC. I think this is an important step in this research direction.\n\nCons:\nThe proposed method is still limited to the 1-Lipschitz architecture. What is the challenge to extend the proposed method to more advanced architectures?\n\nComments:\n\nIn my opinion, it will be helpful to have another entry showing how ordinary convolution performs in table 2 and table 4. It may happen that the ordinary convolution will have higher standard accuracy, but by showing they are not robust at all, the advantage of the proposed method will be further highlighted.\n\nAnother line of enforcing orthogonal constraint in convolutional neural networks should also be discussed:\n[1] orthogonal convolutional neural networks. CVPR 2020\n[2] deep isometric learning for visual recognition. ICML 2020.",
            "summary_of_the_review": "I think this paper contributes a new idea in this field.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a method for enforcing strict orthogonality for convolutional layers. The method is based on considering the spectral domain, where the orthogonality of the 4D conv kernel (in spatial domain) is characterized as the orthogonality of 2D matrices, for which orthogonality can be enforced by existing techniques such as Carley transform. It is shown in experiments that this method is significantly faster than the recent skew orthogonal convolution (at ICML'21) method.",
            "main_review": "Technical comments:\n\n- It is not clear to me why one needs to consider the *expanded* convolution kernel. In principle, the orthogonality of a convolutional kernel is independent of the size of the feature map that it is applied to. However, the construction of orthogonal convolution in the paper requires one to first expand the kernel to the size of the feature map that it is applied to, by padding zeros. My understanding that this expansion causes a lot of trouble that requires some intricate fixes, such as Eq. (12), (13), and (14), that greatly complicates the method.\n\n- Can the authors comment on the completeness of their parameterization for orthogonal convolution? Namely, does the parameterization contain the set of all orthogonal convolution, or just a subset? \n\n- In Eq. (3), it seems that $F_k$ should be $F_n$ since $\\mathcal{W}$ is of size $n \\times n$.\n\nExperiments:\n\n- Table 2: It could be good to add a baseline where no orthogonality is enforced. \n\n- Can the authors comment on why ECO produces significantly better performance than SOC, in Table 2? If both of them enforce strict orthogonality of convolution then shouldn't they be giving exactly the same accuracy (in principle)? BCOP is inferior probably because it only parameterizes a subset of all orthogonal convolution (or maybe the authors have other explanations?)\n\n- The caption of Table 3 says that it reports evaluation time, while the text says that it is the training time. Am I missing anything here?\n\n- The experiments only contain results with LipConvnet which are perhaps non-standard and have far worse performance than standard networks, such as ResNet. This makes it less convincing as to how important the method is for practice. Are there any results on standard networks? \n\nClarity:\n\n- \"simply constructing an orthogonal convolution kernel cannot achieve an orthogonal convolution\": Not sure what this mean. How is \"orthogonal convolution kernel\" different from \"orthogonal convolution\"? Actually, what do those two terms mean, exactly? \n\nOther comments:\n\n- While the paper argues that previous methods for enforcing orthogonality in convolutions, such as Trockman & Kolter '21 and Singla & Feizi '21, have computational issues, it fails to mention that there are earlier work [a] as well as more recent work [b] that are computationally more efficient. \n\n[a] Deep Isometric Learning for Visual Recognition, ICML 2020\n[b] Scaling-up Diverse Orthogonal Convolutional Networks with a Paraunitary Framework, 2021",
            "summary_of_the_review": "This is a nicely written paper for enforcing orthogonality of convolutional kernels with a computationally efficient method. However, I find the experiments to be shallow relative to related works, and cannot fully justify the practical value of the proposed method. I also have some minor concerns on the technical approach, that hopefully can be clarified during rebuttal. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}