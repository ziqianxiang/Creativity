{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper is focussed on proposing a new evaluation metric for evaluating untrained, randomly initialized neural network architectures towards predicting their accuracy/performance after training. The metric they propose is based on evaluating the gradient sign. The method shown to outperform existing approaches on NAS benchmarks.\n\nThe reviewers found the paper's idea simple but effective. The experimental evaluation and efficacy of the proposed method were the main strong points of the paper. The paper was also significantly improved during the discussion period both in terms of presentation and the scope of experiments/comparisons was enlarged. \n\nWhile the metric is theoretically motivated, I personally found some of the theoretical statements weak in terms of assumptions/clarity. I would request the authors to consider taking this and other suggestions made by reviewers into account \n\nOverall I recommend acceptance based on the strong and thorough experimental results shown by the paper on a problem of clear interest to the community."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "Model performance inference is a key challenge in neural architecture search. This paper introduces GradSign, an accurate, simple, and flexible metric for model performance inference. GradSign approximately analyzes the optimization landscape of different networks at the granularity of individual training samples using the gradients evaluated at a random initialization state.\nExperimental results show that GradSign can generalize well to real-world networks and outperform state-of-the-art gradient-based methods for model performance inference.",
            "main_review": "Strengths:\n1. The proposed metric is efficient to compute. It only requires the gradient information of a minibatch at a random initialization point.\n2. The underlying theory is sound. The proposed metric is backed by theoretical insights.\n\nWeaknesses:\n1. In the evaluation, the paper only compares GradSign with gradient-based approaches. I would like to see a comparison against other sample-based, learning-based approaches as well. You can compare the prediction accuracy and time cost of each method and show the trade-off. For example, a sample-based approach can be more accurate but takes 10x more time than GradSign.\n\nAdditional questions:\n1. How does the hyperparameter of training (e.g., learning rate) affect the prediction accuracy of GradSign?",
            "summary_of_the_review": "This paper introduces a simple yet effective way for model performance inference. It outperforms existing gradient-based approaches in almost all experiments. I recommend accepting this paper.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a new evaluation metric for scoring untrained, randomly initialized neural network architectures (zero-shot NAS) in order to predict their accuracy/performance after training. The score is based on evaluating the gradient signs and is shown to outperform existing approaches on NAS benchmarks (NAS-Bench 101, 201, NDS).",
            "main_review": "Overall, I am quite intrigued by the method. It's elegant and simple and seems to perform well. I do have some reservations around the execution and presentation of the work as listed below. That's why it's not a clear accept for me at this point. \n\n### Strengths\n\n* Well-written and natural to follow\n* Interesting theoretical results and derivation. I don't enjoy reading through many theoretical results as they are often inaccessible. So I do appreciate the very clean and insightful presentation of the results. \n* Good experimental performance (although missing a few comparisons potentially).\n\n### Weaknesses\n\n1. **NAS vs Pruning**: I think the authors mix weight pruning and NAS throughout the paper. This is really confusing and while pruning and NAS have lots of overlap I don't think it's appropriate for this paper. Their main contribution is a Model Performance Inference (MPI) score and as such much more suitable for the NAS framework (evaluating architectures for their performance on some target task). \n\n2. **Pruning at initialization vs MPI scores**: (related to 1.) How can you compare against `SynFlow` and similar metrics that are designed for *weight pruning* at initialization? This seems like a strange comparison. I understand that the scores are compatible but could you provide some more context on why this is a useful comparison.\n\n3. **Related work**: I think the authors need to include a few additional citations that they are missing. For example, most of recent advances in NAS are missing such as OFA ([Cai et al., 2019](https://arxiv.org/abs/1908.09791)), ProxylessNAS ([Cai et al., 2018](https://arxiv.org/abs/1812.00332)), MobileNetV3 ([Tan et al., 2019](http://proceedings.mlr.press/v97/tan19a.html)), EfficientNet ([Howard et al., 2019](http://openaccess.thecvf.com/content_ICCV_2019/html/Howard_Searching_for_MobileNetV3_ICCV_2019_paper.html)). [He et al., 2021](https://arxiv.org/pdf/1908.00709.pdf) also provide a useful survey for recent advances. \n\n4. **Comparisons**: Two really important related papers that I believe merit a in-depth discussion:\n   1. _[Deconstructing Lottery Tickets](https://arxiv.org/abs/1905.01067)_. They have some interesting results around the significance of the sign of the weights in the context of lottery tickets. \n   2. _Zen-Nas ([Lin et al., 2021](http://openaccess.thecvf.com/content/ICCV2021/html/Lin_Zen-NAS_A_Zero-Shot_NAS_for_High-Performance_Image_Recognition_ICCV_2021_paper.html))_: Another really important related work to cite and compare against is Zen-NAS. It's an almost identical setup to your problem formulation.\n\n5. **Experiments**: I think there are potentially two more interesting experiments for the authors to run. \n    1. _[NATS-Bench](https://github.com/D-X-Y/NATS-Bench)_: The latest iteration of the NAS-Bench used by the authors. \n    2. _ImageNet_: I think having one full-scale ImageNet experiment will be very powerful. The authors could even follow the setup of the Zen-NAS paper in order to potentially save on compute cost and recycle the results reported in that paper. \n\n### Ways to improve my score\n\nI do hope the authors decide to engage during the rebuttal period. I think the method has potential and hopefully, the authors can address the raised issues during that time. \n\n--- \n\n### Update after the rebuttal \n\nThe authors engaged very productively with my feedback during the rebuttal and have addressed my concerns. The paper clearly meets the acceptance threshold now in my opinion. I am raising my overall score from 6 to 8 and \"technical novelty\" from 2 to 3. ",
            "summary_of_the_review": "The paper proposes an interesting and sound idea but currently falls short of properly contextualizing their work within existing work and consequently comparing against recent work experimentally as well.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work discusses the problem of neural architecture search. While existing gradient methods are based on heuristics, this work proposes a metric called Gradsign for model performance inference, which provides some theoretical guarantees and performs well in practise. \n\nAuthors compare gradsign to a number of state of the art methods using 3 benchmarks involving CIFAR10, CIFAR100, and ImageNet16-120 datasets. Gradsign shows better performance compared to these methods. ",
            "main_review": "Paper is clearly written and experimental results are convincing. The idea of Gradsign is novel and integrating it to existing methods seems to yield improved performance. \n\nI have some questions: \n\n1) Does eq (2) have an intuitive meaning? Is so, can you pls explain it. \n2) Table 1 shows that gradsign is task independent. Do the results generalise to other modalities such as text? Are there any benchmarks based on text datasets. ",
            "summary_of_the_review": "\nThe problem of automatically discovering efficient neural architectures is the foundation of work on AutoAI. From that perspective this is a nice and important piece of work that seems to have theoretical foundation and good practical performance. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}