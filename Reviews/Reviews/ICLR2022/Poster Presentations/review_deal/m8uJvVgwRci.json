{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "Scores ultimately point to accept. The one negative review is borderline and doesn't raise any red flags. Weaknesses in other reviews mainly point to minor improvements in the paper and are largely supportive. Rebuttal points are uncontroversial and seem to clarify several issues."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper addresses a novel research problem of using \"indirect\" label sources in the weak supervision framework to create labelled datasets. The indirect label sources are similar to the labeling functions in prior works in weak supervision ( data programming) with one caveat that these sources produce labels from different space than that of the original ( target) label spaces. It can be useful, when there are good indirect LFs and there is some relationships between the labels in the LF's label space and target label space. This paper, gives a probabilistic label model (PRML) which utilizes these indirect LFs and label relationships to produce desired labels for the given unlabeled data. The methodology is backed by theoretical analysis and real-world experiments. In analysis, a generalization error bound (for the end model learned using estimated labels) is provided which turns out to be similar to the work in Data programming (Ratner et al. 2016). One needs to be careful with the issue of indistinguishability between labels, in such setup. This issue has been studied in detail and they provide definition, conditions for distinguishability. Experiments on real-world data shows that the proposed method works well in comparison to several competing baselines.    ",
            "main_review": "Strengths: \nThis paper studies an important and novel research problem in the weak supervision(WS) setting. In WS, one wants to use as many sources as possible and in many settings indirect LFs can be easily available. However existing WS frameworks cannot accommodate such indirect LFs. Proposed method is technically sound and backed by analysis of generalization error of the end model and distinguishability of unseen labels. Moreover, the experiments on real-world data are promising as well. Overall, the paper is well written and easy to follow. Examples provided help in understanding the setup better. \n\nWeaknesses:\nI can't see any major issues with this work. The generalization error analysis is similar to prior work (Ratner 2016) but I think it is a good sanity check. There are some grammar and spelling mistakes which are listed below. If some details/intuition for Theorem 2 can be provided in the main paper, that would be helpful. \n\nGrammar and spelling errors:\nin abstract \"....an generalization bound.\"\nin section 6 first para \"Husy\" is used several times, I think the authors meant \"Husky\" ??\nin section 5.1 \"... Specifically, for a ILF λj and one ...\"\n",
            "summary_of_the_review": "I think, overall its a good paper which solves an important problem that in turn broadens the applicability of weak supervision. The work is novel and technically sound. In my opinion, it can be a clear accept.   ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces a probabilistic framework that is designed to aggregate multiple weak supervision to a single strong proxy guidance to train a model with insufficient supervision from data. Authors claim the proposed framework differs from the previous attempts since it can also model a non-overlapping set of tasks.",
            "main_review": "Strength\n* Framing the “Weak Indirect Supervision” problem is quite interesting since it is not too uncommon that we have noisy hierarchical labels.\n* Overall the paper is clean and make a reasonable contribution by proposing the probabilistic label relation model along with theoretical analysis.\n\nWeakness\n* Label relation should be one of pre-set relationships. In other words, the labels should be “comparable” in some sense and the proposed method (and also probably for the several similar label graph based methods) cannot model properly when there are a few labels that are not in the similar semantic space. For example, {red, green} and {dog, bird} are not comparable and some relations cannot be expressed by the label graph. Hence, it is natural to apply the current technique in hierarchical labels, but not for heterogeneous (or multi-modal) labels. I wanted to ask the authors to clarify and discuss on this topic in the main text.\n* Datasets used in the paper is all for the hierarchical label interactions. It feels it's somewhat limited than what the authors claims.\n* If I understand correctly, PLRM models the relation from ILF to an label with accuracy levels, which are independent from the actual value of ILF outputs. Hence, the noisy level or the certainty from ILFs for each individual data points are ignored, which seems to be a bit limiting.\n\n",
            "summary_of_the_review": "I feel the paper is really at borderline (slightly leaning negative) considering 1) the contribution is a bit limited because the framework is only applicable for hierarchical label space 2) modeling ILF to label space with a graphical model is not too novel 3), but the paper still made reasonable contribution in this limited scope.\n\n-------------\n\nPost-rebuttal comment:\n\nI thank authors' response and the clarification on the \"comparable\" assumption in their paper. I also agree on other reviewer's points for the contributions of the paper and increased my score. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors summarized their contributions quite well in the introduction section -- In this paper, the authors (1) propose a new problem setting called Weak Indirect Supervision (WIS), which extends from the problem setting of Weak Supervision; (2) develop Probabilistic Label Relation Model (PLRM) for WIS; (3) introduce the concept of distinguishability in WIS; (4) conduct empirical evaluation of PLRM on image/text classification and an advertising application.",
            "main_review": "Strength:\n* The paper proposes an interesting problem setting, weak indirect supervision, which is a practical setting and helpful for reducing annotation costs in certain applications.\n* Comprehensive theoretical analysis on error bound and distinguishability. However I'm not an expert in PGMs, so I didn't check the proof in the appendix thoroughly.\n* Comprehensive experiments with both synthetic datasets and real-world applications. The proposed PLRM method outperforms baselines by a large margin.\n\nWeakness:\nI don't see major weaknesses; here are some suggestions and questions.\n* I feel the following two paper may be relevant to the WIS problem setting, please consider adding them to the related work section and provide further discussion.\n  * Few-shot Relation Extraction via Bayesian Meta-learning on Task Graphs. ICML 2020. (It studies few-shot/zero-shot transfer to new relations, by leveraging relation graphs, similar to the label graph in this paper.)\n  * Co-Tuning for Transfer Learning. NeurIPS 2020. (It studies transferring a model trained on source categories to target categories. Problem setting is quite different as Co-Tuning assumes target training data, however I feel it's still quite relevant.)\n* Seems the datasets used all have _hierarchical_ label relations. Does that mean the labels are forming a tree structure? I wonder how often \"overlap\" relation exists in the sampled relation graphs.\n* The sampled label graphs have 8 classes. I wonder how the proposed method scales with more classes.\n\nSuggestions for paper presentation:\n* Please consider providing some more examples of the sampled label graphs. I see one in the Appendix E.5, but more examples (perhaps with different structure of graphs) will help me get a sense how hard this task is for humans.\n* The notations are a little complicated; it would be great if the authors could mention that there is a glossary in the Appendix A in the beginning.\n",
            "summary_of_the_review": "Strength: interesting new problem setting; comprehensive theoretical analysis; the proposed method has good performance on three datasets.\n\nWeakness: some potential missing references; paper presentation may be further improved; some more discussions on the questions I raised may be helpful.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studied a weakly supervised classification problem, called **weak indirect supervision**, where the supervision signals are from labels that are different from but still informative of the classes.\nThe author proposed a new two-step method that first creates probabilistic labels using an **exponential family graphical model** based on (1) a set of **indirect labeling functions** (pretrained classifier, heuristic rules, etc.) that output deterministic labels and (2) a given **label relation graph** (ontology graph, knowledge base, etc.) that captures the relations between the observed labels and the target classes, then uses the generated labels to train a classifier for the target classes.\nThe author provided a theoretical analysis on the requirements of the labeling functions and derived a generalization error bound.\nThe proposed method was evaluated on semi-synthetic datasets based on the ImageNet dataset for image classification and the LSHTC dataset for text classification.",
            "main_review": "## Strengths\n\nAlthough similar problem settings (e.g., zero-shot learning, hierarchical classification, or distant supervision) have been studied in the literature, the proposed task seems novel and interesting.\nIn this problem, the given labels are not necessarily the target classes, but they still provide useful information for the classification task.\nThis work takes a data-centric perspective and provides a way to make use of these supervision sources.\n\nTechnically, the proposed method adopts a two-step method that creates pseudo-labels first then trains a classification model, which makes it compatible with many training techniques in the second step.\nThe use of an exponential family graph model leads to some theoretical guarantees.\nThe proposed testable condition can be used for evaluating if a specific supervision source can be helpful for a certain classification task.\n\nExperimentally, the proposed method was evaluated on large-scale image and text classification datasets equipped with label hierarchies and was tested in two real-world scenarios.\nThe author provided empirical evidence that the proposed method outperforms four baselines on these datasets.\n\n---\n\n## Weaknesses\n\nThis paper can benefit significantly from improved writing and organization.\nI may have missed something, but a few symbols are difficult to understand and some concepts seem unnecessary.\n\nFor example, is it necessary to formulate this problem with \"indirect labeling functions\"?\nIt seems that those functions are only queried once for \"unlabeled training set\", but doesn't it make it just a multi-labeled training set?\nThis makes the difference between ZSL and WIS less clear.\nAlthough I understand that the data distribution would be different, e.g., to classify instances from a label set $0$, ZSL uses paired data $(X_1, Y_1)$ from the label set $1$, possibly $X_0$, and label relation between label sets $0$ and $1$; while WIS uses tuples $(X_0, Y_1, Y_2, ...)$ with instances from the label set $0$ and their labels from the label sets $1, 2, \\dots$\n\nA writing issue is that the formulation with \"one-hot vectors\", in my opinion, makes the notation unnecessarily complex and less clear.\nFor example, currently we have $Y \\in \\\\{0, 1\\\\}^k$, $y \\in \\mathcal{Y}$, $Y[a] \\in \\\\{0, 1\\\\}$, $\\mathcal{Y}_{\\lambda_j}$, $\\hat{\\mathcal{Y}}$, $\\hat{\\mathcal{Y}}(y)$, $\\mathbf{y}^a \\in \\\\{0, 1\\\\}$ (not a vector), $\\hat{Y} \\in \\\\{0, 1\\\\}^\\hat{k}$, and $\\tilde{Y}$.\nSo it is easy to forget their meanings and their value ranges.\nAlso, some symbols are indexed by subscripts, some by superscripts, and some by $[]$.\nAccompanied with one-hot vector-valued labeling functions, they make this paper hard to follow.\nAlternatively, I think \"one-hot vector\" can be an implementation detail and it might be more understandable to define the label to be an element in a set and define the relationship with the indicator function/Iverson bracket.\n\n---\n\n## Confirmation\n\nBefore I proceed to ask more questions about the method itself, please allow me to confirm my understanding of the notation and the problem setting.\n\nTask:\n\n- $X \\in \\mathcal{X}$: input feature, e.g., an image of a dog\n- $Y \\in \\\\{0, 1\\\\}^k$: one-hot vector of class, e.g., $[1, 0, 0, 0]$\n- $y \\in \\mathcal{Y}$: class name, e.g., \"dog\" (is $y$ indexed by a number $a$?)\n- $Y_i[a]$: I can somehow understand it but it's a bit confusing. There are too many layers of indirection. Please define \"corresponding to $y_a$\" more clearly.\n\nSupervision:\n\n- $\\lambda_j: \\mathcal{X} \\to \\\\{0, 1\\\\}^{k_{\\lambda_j}}: X_i \\mapsto \\Lambda_{ij}$: labeling function\n- $\\Lambda_{ij} \\in \\\\{0, 1\\\\}^{k_{\\lambda_j}}$: one-hot vector of indirect label, e.g., $[1, 0]$\n- $? \\in \\mathcal{Y}_{\\lambda_j}$: label name, e.g., \"caninae\"\n- $\\mathcal{T}$: could you explain the meaning or give some examples of label relations _exclusive_, _overlapping_, _subsuming_, and _subsumed_ in Section 4? (Or move Definition 2 and Figure 4 here?)\n\nPlease confirm the items listed above and improve the readability if possible.\n\n---\n\n## Questions and comments\n\n### In general\n\n- Besides more flexible training, could you clarify the advantages of the two-step approach?\n- Is the exponential family distribution needed for the theoretical guarantees?\n- Could you clarify the difference between distinguishability and identifiability with a concrete example?\n\n### Section 5.1\n\n- Which prior work? Is \"accuracy dependency\" defined elsewhere?\n- $y_d$ does not appear in the first equation? (cf. the notation issue mentioned above)\n- A very common misuse of the indicator function: It is defined for a set, not for a proposition (Iverson bracket)\n\n### Section 5.2\n\n- How should we interpret Figure 2?\n- \"Training an End Model\" notation issue: should be $\\to$, not $\\mapsto$\n- Are there alternatives to SGD? Is SGD needed for Theorem 1 to hold?\n- Eq. (3): What does $\\tilde{Y} \\sim \\hat{\\Theta} | \\hat{\\Lambda}\\_i$ mean?\n  Do you mean $\\tilde{Y} \\sim p\\_{\\hat{\\Theta}}(\\tilde{Y} | \\hat{\\Lambda})$?\n\n### Section 6\n\n- Very minor: inconsistent capitalization, and typo \"Husy\"\n- $\\forall \\Theta > 0$: Isn't $\\Theta$ a vector? If so, the order $>$ is not defined\n- If it's \"a.e.\", then such $\\Theta$ and $\\tilde{\\Theta}$ may still exist but have measure zero. Is this enough?\n- What's the difference between $\\mathbb{P}$ and $p$?\n\n### Extensions\n\n- The current label relation graph is only deterministic. Is it easy to extend it to be probabilistic?\n- Is classification with rejection applicable here?\n  For example, if we use a dog/cat-classifier on a desk image, the output may be very random and non-informative.\n  Is out-of-distribution detection needed in this problem?\n\n---\n\n## Post-rebuttal\n\nI've read other reviews, the author's responses, and the updated paper. The author addressed most of my concerns and answered my questions well. The presentation of the paper has been greatly improved. Hence I would like to increase my rating.",
            "summary_of_the_review": "This paper studied an interesting problem in weakly supervised learning and proposed a novel method based on the indirect labeling functions and label relation graph.\nThe proposed method was validated theoretically and empirically.\nOverall, this paper provided a data-centric approach for improving machine learning systems, which might be useful to the community.\nHowever, there are still a few concerns mentioned above, and the writing and organization can be further improved.\nHence I recommend acceptance of this paper.\nIf the author can address my concerns well, I would like to raise my rating.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}