{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The authors made substantial  improvements to the originally submitted manuscript; however, reviewers initially remained reluctant to support the paper for acceptance based on the degree to which they were confident in the underlying arguments / position taken by the authors and the evidence provided to support their position and arguments. There are also concerns about the significance of the gains in performance afforded by the proposed approach.\n\nDuring the author response period two reviewers became satisfied with the additions and modifications leading to an increase in the final score. It will be critical for the authors to try to add ImageNet results if possible in addition to other promises made to reviewers.\n\nThe AC recommends accepting this paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "Based on empirical observations and the neural tangent kernel theory, the authors present an explanation of why Knowledge Distillation (KD) and Early Stopping KD (ESKD) are better than using just the one-hot labels as well as how hard samples have high variance curve in the output space. Based on these observations, they propose a novel method called Filter-KD which yields better performance over using just one-hot labels as well as EKSD for standard vision datasets.  ",
            "main_review": "Strengths:\n\n1. The theoretical explanation and the illustrative toy experiments are very suggestive of what may actually be happening with ESKD. These contributions are very novel and useful in my opinion. They show that for harder examples, compared to easier examples, the learning path initially takes the network output to the true $\\mathbf{p}^*$  before making a turn for the one-hot target. \n\n2. The experiments when performed more real-world datasets using a proxy called \"integral difficulty\" instead of the true $\\mathbf{p}^*$ (as it is unknown) still show very similar patterns, at least for the easy examples, compared to the toy Gaussian dataset where $\\mathbf{p}^*$ can be calculated exactly. This is intellectually satisfying.\n\n3. The experiments show how hard examples, as measured in terms of integral difficulty, show a very different -- highly erratic -- learning path compared to easier examples. More importantly, the authors show that by low-pass filtering, the path actually looks quite similar to the  case of medium hardness. This is very important as it allows the authors to propose an intuitive novel KD framework called FilterKD where the targets are smoothed teacher outputs. \n\n4. The FilterKD algorithm is novel, intuitive relatively easy to implement for smaller datasets.\n\n5. The experiments on CIFAR-10 and CIFAR-100 show that FilterKD is indeed better than using ESKD. \n\n6. Experiments with noisy labels are well-performed and provide good evidence for the theoretical ideas as well as FilterKD.\n\n\nQuestions and suggestions for improvement:\n\nExperimental results need to be improved. I would suggest the following additions\n\n1. Results of regular KD in addition to ESKD. \n\n2. There are results only with one teacher-student architecture pair. A wider set of teacher-student pairs is required before concluding that the proposed method is better.\n\n3. In Table1, both the teacher and student have the same architecture. One of the main reasons to do KD is for model compression. So having a larger teacher and smaller student would be quite useful for a larger audience. I would suggest looking at the benchmark and code provided by Tian et al. in Contrastive Representation Distillation, ICLR 2020 as an easy starting point.\n\n4. The memory requirements for storing smoothed targets in memory may be prohibitive. But why can't we load them from permanent memory as and when required, like we do with a dataloader for images and videos?\n\n5. Experiments on a dataset like ImageNet or at least TinyImageNet would strengthen the paper more. ",
            "summary_of_the_review": "Overall, I believe there are solid contributions in this submission which can be further demonstrated with the suggested experiments. Hopefully, the authors will be able to address the weaknesses I have outlined. \n\nFor now, I am giving this a 6: marginal acceptance. I am happy to increase the rating based on the authors' response to the next level.\n\nUPDATE AFTER AUTHOR RESPONSE:\n\nI thank the authors for a clear response to my suggestions and the additional experiments. I am satisfied with the additions and modifications and I am increasing my score. Please try to add ImageNet results if possible, as it may help others in the field who use it as a common benchmark.\n\nI have also read the other reviews and the authors have also addressed them well too. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes an explanation for the success of distillation. It first experiments with synthetic Gaussian data. On synthetic data, it shows that distillation works better from an early-stopped model. It probes why, and finds that, when the one-hot label is far from the true conditional distribution, the early-stopped model’s distribution tends to be closer to the true distribution than either the one-hot label or the converged model. The early-stopped model is better because, over the course of training, the model tends to produce a distribution close to the true one before finally overfitting the one-hot label. Some examples show similar patterns for real-world networks/datasets, provided the predictions over training are smoothed. Motivated by these results, the paper proposes to use an averaged distribution of labels from different steps of training rather than the distribution at the end of training, and shows that this improves performance by a little bit.",
            "main_review": "In terms of novelty, clarity, and significance, I think this paper is good. The question of why distillation improves performance is an important one. Analyzing the learning paths of individual examples is an interesting idea. To my knowledge, the visualization of learning paths is novel, as is the Filter-KD method. I enjoyed reading the paper, and I think the ideas described have the potential to inspire interesting future work.\n\nWith that said, I’m not entirely convinced that the explanation for the success of KD provided here is accurate for real-world KD use cases.\n\n1. A major claim is that KD’s success is related to the zig-zag pattern, but this claim does not seem to be sufficiently supported by the experiments involving real data. It is shown that, on some examples, there is a zig-zag in the learning path, but there is no direct demonstration that this zig-zag is related to the efficacy of KD. One idea is to measure the degree of the zig-zag for each example and then verify that it suffices to use KD only for these examples and one-hot training for others. This seems like the bare minimum thing to show, but the entropy of the model distribution at the early stopping point might be correlated with the degree of zig-zag, in which case it seems somewhat trivial that the low-entropy outputs could be replaced with one-hot labels. I am not sure exactly what a good empirical test of the “zig-zag” hypothesis would look like, but I don’t find the evidence provided compelling enough.\n\n2. It’s not clear to me whether distillation is performed at a temperature of 1 or a temperature that is tuned somehow. Assuming the magnitude of the final logits provides some indication of learning difficulty, I could see distillation at a temperature >1 providing some of the benefit of early stopping. This also seems like a relevant baseline for FilterKD, since both increasing the temperature and averaging predictions will have the effect of increasing the entropy of the target distribution.\n\n3. The numbers in Table 1 seem uncompelling. The caption does not say whether the +/- is standard error, standard deviation, or something else, but assuming it is standard error, I don’t think the improvements on CIFAR-10 or AG News would be statistically significant. The improvements from KD on these datasets are small to begin with, so they may not be particularly good benchmarks for this work. The performance with noisy labels is more compelling, but doesn’t help to validate that the success of KD on clean data is attributable to the phenomenon described.\n\n4. I’m suspicious of the idea of “base difficulty.” “Base difficulty” is defined as the L2 distance between the one-hot label and the true conditional distribution. A nice aspect of base difficulty is that it depends only on the true data distribution. However, it seems to capture only two kinds of “difficult” examples: those that are ambiguous (if the conditional distribution has high entropy) or incorrectly labeled (if the conditional distribution has low entropy). The proposed “integral difficulty” measure further captures the difficulty of _learning_ particular examples, and is somewhat similar to previously proposed metrics such as [C-score](https://arxiv.org/abs/2002.03206) (see also the learning speed based proxies in that paper, one of which is equivalent to integral difficulty) or [variance of gradients](https://arxiv.org/pdf/2008.11600.pdf). Intuitively, data points with low base difficulty could have high integral difficulty if they are located in regions of the distribution with low density. The labels in CIFAR-10/100 are relatively clean and the images are relatively unambiguous, so I suspect that most of the high integral difficulty examples do not actually have high base difficulty. \n\nMinor:\n- “Early-stopped knowledge distillation” is introduced without any further description. Only from the appendix is it clear that it means knowledge distillation from an early-stopped teacher, as opposed to early stopping during knowledge distillation.\n- IIUC, the intuition from Section 3.3 is that the reason for the “zig-zagging” in Figure 3 is that the label assigned to a given point changes not only based on the gradient of the loss for that point, but also on the gradients of the losses on other similar points. This point seems intuitive even without bringing up the $\\mathcal{A}^t$ or the empirical NTK, and I am not sure that Proposition 1 helps to understand it. Further analysis of the dynamics of $\\mathcal{A}^t$ and the stationarity of the empirical NTK might make this decomposition more interesting, but the analysis in Appendix E doesn’t seem conclusive.\n- Missing paren in definition of integral difficulty on p. 5.\n- The idea of taking a moving average of network weights, as explored in Appendix G, is much older than MoCo and it would be good to cite relevant literature, e.g., [Ruppert](https://ecommons.cornell.edu/bitstream/handle/1813/8664/TR000781.pdf?sequence=1), [Polyak & Juditsky](https://epubs.siam.org/doi/abs/10.1137/0330046), and the [Inception paper](https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html), which AFAIK was the first or among the first papers to apply this method in deep neural network training.",
            "summary_of_the_review": "This paper is well-written, novel, tackles an important question, and contains potentially interesting ideas. However, the evidence provided is not quite sufficient to validate the claims made. The suggestion that zig-zag in the learning trajectory is important to the success of distillation on real (non-noisy) data is not well-demonstrated, and the benefits of the proposed FilterKD technique seem marginal on real-world data.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper analyzes the learning process in knowledge distillation and proposes a method called Filter-KD. It was evaluated by using CIFAR10, CIFAR100, and AG News.",
            "main_review": "The observed \"zig-zag\" pattern is interesting and gives one reason that early stopping is effective in knowledge distillation. The improvement obtained by early stopping and the proposed FilterKD is rather minor. ",
            "summary_of_the_review": "I am still not sure the proposed method is justified well in theory. It is based on NTK theory but it is applicable only for a sufficiently wide network. Also, several \"zig-zag\" patterns are shown in figures, but we are not sure of their definition and how often it appears in the real dataset. The improvement in accuracy is rather minor.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper introduces a method for doing knowledge distillation with noisy labels. The contribution consists on representing distilled labels as a weighted moving average of the predicted labels from a teacher as it trains (using noisy one-hot encoded labels). Finally, a student is trained using the distilled labels.",
            "main_review": "**This review has been updated since it was originally posted. See thread below.**\n\n## Strengths:\n- The paper is, for the most part, well-structured and easy to read. Argument and ideas are developed step-by-step, formulated and substantiated formally, based on previous work.\n- Establishing hypothesis up front.\n- Starting from a toy example, building up the contribution from initial observations, and finally moving on to more complex/realistic scenarios after.\n- Tackles a relevant problems for the ML community, namely knowledge distillation and noisy labels.\n\n## Weaknesses:\n- The concept of noise is not well-defined, to the point where the problem definition becomes confusing: is the noise big enough to flip the target label? More precisely, does `arg max_i p*(y|x)_i = arg max_i e_yi` always hold? If it does, the hard sample from Figure 3 does not seem to comply. If the equality doesn't hold, the problem definition is more about data with wrongly labeled samples; in this case, the insights from section 3 will be contingent on the number of samples for which the equality mentioned above doesn't hold (i.e., mislabeled samples). This aspect has not been addressed in the paper. Moreover, the concept of easy, medium and hard samples does not explicitly compensate for mislabeled samples, and hard samples *could* trivially correspond to the mislabeled samples.\n- KD, ESKD and LS depend substantially on a temperature hyper-parameter which the paper does not address. Having the wrong temperature can drastically affect the performance of these baselines.\n- The labels from ESKD can be seen as a special case of the proposed Filter-KD where alpha = 1. Therefore, one could expect the labels from an ESKD teacher to be similar to those of Filter-KD if the teacher were trained using less epochs (in other words, if the early-stopping criterion forces the teacher to stop sooner). Even though ESKD is a valid baseline, is it the most compelling one to justify the introduction of Filter-KD?\n- Does not use a real dataset (i.e., not a toy dataset) for which `p*` is available like CIFAR10H **[1]** to evaluate Hypothesis 1.\n\n### Detailed comments:\n- The abstract is not a clear description of the paper, lacks context and coherence. It is not clear what the contributions are (is it a criterion to measure generalization, a function to measure difficulty or a novel way to do knowledge distillation).\n\n- Increase text size in all figures (at least as big as the figure's caption).\n\n> In this paper, we first clarify what makes supervision good for a classification problem, and then explain two existing label refining methods, label smoothing and knowledge distillation (KD), in terms of our proposed criterion.\n\n*The paper clarifies or proposes a new criterion to quantify generalization performance?*\n\n> we look deeper into the learning path of the network’s predicted distribution for training samples with different difficulty.\n\n*What is the 'learning path'? How is difficulty defined? Make it clear that these are definitions that will be introduced later on.*\n\n- Evaluate labels produced by teachers trained on KD, ESKD, LS and Filter-KD wrt. human annotated labels from CIFAR10H **[1]**.\n\n- Proposition 3 from Menon et al.: Lacks context. I recommend explaining further where it comes from to make the argument more self-contained.\n\n- Figure 1b-c: The range on the y-axis is rather small (coverage of 0.7% for accuracy and 0.02 on ECE), exaggerating correlation between accuracy and similarity between `p*` and `p_tar`. Also, KD and LS depend on a temperature parameter, which according to the supplementary material, has only been tested with with one setup.\n\n> ...leads to better generalization performance, as measured either by accuracy (ACC) or expected calibration error (ECE)\n\n*It is critical to define the term \"generalization\" here. If these are the two criteria that are going to define generalization, or are distribution shifts also being considered?*\n\n- Figure 2: It would be interesting to see the [distribution of the entire dataset for each axis](https://matplotlib.org/stable/_images/sphx_glr_scatter_hist_001.png). The scatterplot can hide how dense the distribution is (and how thin the tails are).\n\n- Figure 3 (hard sample): How can the `p*()` label for the hard sample be so close to the bottom right corner (say [0.1, 0.8, 0.1]), and yet the one-hot encoding be all the way across, on the bottom left corner ([1, 0, 0])? It looks as if the samples were mislabeled. Refer to the first point under 'Weaknesses'.\n\n- Figure 3: How representative is this behavior across the dataset? How many easy/medium/hard samples are there?\n\n- Figure 3: The paths on the right are close to impossible to see and interpret.\n\n- Proposition 1: `x_o` has not been defined within the proposition.\n\n> however, for a harder sample (middle), it is very difficult to observe any patterns\n\n*Capitalization*\n\n- Figure 4: What does the black dashed line mean?\n\n- Section 5.1: What labels are used for the students' test set? Is it the original OHT or a label set by the corresponding teacher?\n\n- Section 5 & Figure 5: How does Filter-KD compare to models trained using LS and KD (with optimum temperature)? Any reason to leave them out?\n\n### References\n[1] Peterson, Joshua C., et al. \"Human uncertainty makes classification more robust.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.",
            "summary_of_the_review": "The paper is easy to follow and has contributions that are sufficiently motivated. Empirical evaluation shows that the proposed method is more robust to noisy labels. However, the problem definition (therefore the scope of the paper) is not clear, and the core contribution Filter-KD has not been evaluated to an extent that justifies its simplicity.\n\n**UPDATE: After discussing with the authors (see thread below) I have updated my review to reflect my view of the paper.**",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}