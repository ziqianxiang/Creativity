{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "Nice paper, providing a thorough investigation of a simple idea that may be useful to a wide range of practitioners. All reviewers are positive, and the discussion has led to significant improvements in exposition and overall in the quality of the submission."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper introduces ensembles as an option the reduce the amount of FLOPs while increasing or keeping the accuracy.",
            "main_review": "## Strong points\nThe paper presents many experimental results and offers an interesting view on ensembling and cascades. \nThe paper is well written and understandable.\n\n\n## Weak points\nGiven that efficiency is of core concern it would be good to at least discuss the concept and define what is referred to in the paper as efficient.\nE.g. is it the amount of electric power used in training and inference? (Which it is not).\n\nGiven that ensembling is well-known (as correctly pointed out by the authors) and wanting to establish a new interpretation of a known concept I would expect more intuition on why the presented interpretation seems fit.\n\nI am lacking concrete guidelines for a researcher or practitioner that go beyond existing knowledge.\n\nWhile the performed experiments are rather broad in scope, the evaluation protocol is not.\n\nOther ensembling approaches could be easily added, such as majority or variation ratio. How to set the threshold is also not explored in similar detail.\n\nEnsembling is to me rather orthogonal to the approaches it is compared with in this paper (i.e. model scaling).\n\n\n## Detailed comments\nEnsembling for classification can be done in multiple different ways, see e.g. Beluch, William H., et al. \"The power of ensembles for active learning in image classification.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.\n\nUsing a confidence threshold for generating the cascade is justified with the simplicity, i would argue the approach referenced from Streeter (2018) - which is \"Margin sampling\"\nsee T. Scheffer, C. Decomain, and S. Wrobel. Active hidden Markov models for information extraction. In Proceedings of the International Conference on Advances\nin Intelligent Data Analysis (CAIDA), pages 309–318. Springer-Verlag, 2001. - is also rather simple. Given the amount of experiments conducted i wonder why such a worthy comparison was skipped.\n\nThe cascades approach seems interesting, but is in my view rather is an approach how to generate an ensemble, whichs size is determined by an uncertainty measure of its first members.\nHere the question arises which uncertainty measure is to be chosen, as pointed out before.\n\nWhy do you report average FLOPs to compare cascade and ensemble? How about total FLOPs over the images in the test set?\n\nWould be good to compare the number of parameters as well.\n\nI feel it would help the paper to strip down a bit on the amount of results included in the main text in favor of more details on why specific choices were made and conclusions taken from the results.\nSome of this (also some points I raised) are hidden in the appendix, which feels suboptimal to me.\n\n\n## Questions\n* How was the speed-up in the tables calculated?\n* How were the confidence thresholds t_i tuned?\n* Sem. seg. why average confindence score? Did you try others?\n* Could be interesting to plot the size of the resulting cascade over threshold\n\n## Minor comments:\n\n* Would be easier to read if r specifying the grid would be a different letter than r_1 and r_2 for the resolution of the self-cascade.\n* Caption figure 2: typo: \"cost-effectiev\"",
            "summary_of_the_review": "This paper offers a new and interesting view on the well-known ensembling of DNNs. Some details are not entirely clear to me, details see above. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper investigates the effectiveness of model cascades in computation/accuracy tradeoff improvement.  A straightforward procedure is used, where all combination-permutations of a handful of models are evaluated in a cascade, and exit thresholds are determined by choosing best computation work within accuracy degradation constraint (or, best accuracy given computation constraint).  The resulting cascades perform significantly better than larger single models (more accurate or fewer flops depending on comparison).  Most significantly, the paper provides extensive evaluations on the degree of these gains for three model families (EfficientNet, Resnet, and MobileNetV2).\n",
            "main_review": "Using early-exit cascades to improve average efficiency, equipped with a large handful of models and means to evaluate their combinations, is a simple and practical approach.  This paper clearly demonstrates their effectiveness and profiles the degree of their effect in several scenarios, including comparisons with larger single models, NAS, and differences between small and large models.\n\nWhile this paper does a good job in profiling results of this approach, I would have liked to see more on variation in the search and search space.  The exhaustive search in cascade selection appears important, but I don't see any evaluation on how important it is.  What is the performance variation among cascades in the search (particularly same-size cascades)?  What if one starts with a smaller model pool --- will results be similar or worse, and by how much, depending on the number of models trained?  Or, if one trains just 2 same-size models and uses them in a cascade with searched exit threshold, what is the expected performance and its possible variation?  How many model instances are needed in the search pool for obtaining good results with high enough probability?\n\nThe results from this paper also appear to conflict with those in Huang et al 2018 \"Multi-scale dense networks for resource efficient image classification.\":  On p.8 and Fig 7, that paper says \"We also evaluate an ensemble of the five ResNets that uses exactly the same dynamic-evaluation procedure as MSDNets at test time ....\", describing a similar cascade to this paper, and see no improvement in the average case, though the models used appear to be smaller than those investigated in this work (which this work says is a factor in effectiveness of non-cascade ensembles).  Are there any other causes for this difference?\n\nOverall, the measurements provided here demonstrating the effectiveness of this simple and sometimes overlooked approach, are welcome evaluations.  I would have liked to see more on the importance of the search and variation between model configurations.  However, I think there is significant practical interest to see measurements that establish the effects of cascades from a candidate model pool, in the context of current classification models.\n\n\n\nMinor questions:\n\n\nsec 3 ensembles:  How many models were trained in the pool for each architecture, and how many combinations were evaluated?\n\nsec 3 ensembles:  Were evaluations of the multiple combinations are performed on the test set, and pareto-optimal points chosen and shown as final performance for the same test set?  This would result in potentially better perf than if a separate validation set were used.  (Though in the context of the overall paper, seems a relatively minor point, since a validation set was indeed used for the cascade).\n\n\nsec 7.2 semenatic segmentation:  With grid subdivisions, it's a little unclear what is the image resolution for each model, and if any models (just the first or any beyond the first) take the entire image as input to be able to incorporate larger context regions.  If 128x128 does not have enough context, for example, it seems possible the second stage of a cascade might only degrade performance compared to the initial model that used the full image.  Were there any effects like this?\n\n",
            "summary_of_the_review": "Overall, the measurements provided here demonstrating the effectiveness of this simple and sometimes overlooked approach, are welcome evaluations.  I would have liked to see more on the importance of the search and variation between model configurations.  However, I think there is significant practical interest to see measurements that establish the effects of cascades from a candidate model pool, in the context of current classification models.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The work provides an empirical accuracy-efficiency comparison of model ensembles and cascades of shallow models against single deeper models. The main finding, which supports previous results, is rather interesting: compositions of shallow models tend to provide better efficiency-accuracy trade-off than single deep models. This finding has been extended here to the ImageNet classification task with three architecture families (ResNet, EfficientNet and MobileNet), with further examples on video recognition and semantic segmentation.",
            "main_review": "**Pros:**\n\n+ the study of the efficiency-accuracy trade-off in shallow network compositions is a relevant topic;\n+ the empirical study on ImageNet is thorough and convincing;\n+ nice extensions to NAS-based models and related tasks of video recognition and semantic segmentation;\n+ generally very thoughtful experimental setups.\n\n**Cons:**\n\n> Significance: While I appreciate the thoroughness of the study, I have difficulties recognising the significance of the new insight gauged w.r.t. prior work. Compared to (Kondratyuk et al., 2020; Lobacheva et al., 2020), this work simply augments those studies to larger-scale classification, video recognition and semantic segmentation, with ImageNet experiments being by far the dominant content (7 out of 8 pages).\n\n> Reproducibility: This work would be likely very hard to reproduce. The text does not provide any training details (e.g. the training duration, the learning rate and schedule, regularisation, etc.).\nPerhaps more importantly, how were the ensemble models trained? Was there any bootstrapping used (e.g. see Ilg et al., (2018)), diversity-encouraging loss terms (e.g., Lee et al., (2016)) or snapshot ensembling? Why or why not? \nRelated to both the significance and reproducibility: I wonder how much deviation is there on the evaluation metrics (e.g., of the top-1 accuracy) in ensemble/cascade models?\n\n> Choice of the baseline and metrics: 1) I wonder how cascades/ensembles fare against the more efficient types of temporal ensembling (e.g. SWA from Izmailov et al., (2019))? 2) I’m curious what happens to uncertainty estimates. The choice of the max-based confidence function is interesting, but why is this a reasonable choice? The observed improvement in accuracy that it provides may come at a cost of deteriorated calibration; this needs to be verified. \n\n> Clarity. I find it rather unusual to seek ensemble configurations satisfying a particular value of accuracy or the number of FLOPs. It seems only to introduce additional experimental setups that deliver little new insight; there is unnecessary complexity of constraining the ensemble/cascade set, and it’s not always clear which models end up in the composition. I may be overlooking something, but the simple accuracy-efficiency plots (like Fig. 1 or Fig. 2) are already sufficient to draw the same conclusions and are simpler to interpret.\n\nMinor:\n- E1 the missing x30 factor in Tab. 8 is important, even though it does not change the ranking.\n- Semantic segmentation: Why is it reasonable to change 7x7 convolution to 3x3?\n- A number of typos (e.g. “cost-effectiev” on p. 4; “This methods does not reqire” on p. 2).\n- “we only show those Pareto optimal ensembles in the figure” needs more explanation. Which models/configurations were evaluated but were not Pareto optimal?\n\n\n**Post-rebuttal comment**\\\nI thank the authors for the clarifications. I recognise that the work provides significant empirical evidence (indicated in some previous findings) that compositions of shallower models may be both more accurate and more efficient. I also acknowledge that the revision now includes important details for reproducibility. However, a few things could be improved:\n\n- I agree with the other reviewers that the clarity of presentation should be improved significantly. For example, I find it bizarre to refer to Fig. 2 without introducing the architectures (and justifying their choice), how they were trained and selected for composition (i.e. the cascades are introduced only much later), how the hyperparameters are chosen (e.g. the thresholds), etc. These points are discussed in the paper, but either much later or in different contexts.\n- given the available computational resources and that most of the empirical advantages come from ImageNet, it would be interesting to verify these results on another classification dataset (e.g. Open Images, Pascal 2012 for multi-label classification, or at least CIFAR-100).\n- model/ensemble calibration needs more analysis. As highlighted in the paper (and in the rebuttal), the calibration of the models used contradicts the view on the networks being overconfident. However, the work makes no effort to explain this finding. Is it an artefact of the dataset? The training schedule? How does it affect the calibration of the cascades/ensembles?\n- minor: there are still quite a number of typos.\n\nNevertheless, I upgrade my score, since the remaining points can be addressed in the camera-ready.",
            "summary_of_the_review": "I find that this work provides a thorough study of the efficiency-accuracy trade-off offered by ensembles of shallow models w.r.t. single deeper models. However, I have difficulties recognising the significance of the new insights this study delivers in light of the previous work (e.g. Lobacheva et al., (2020))",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}