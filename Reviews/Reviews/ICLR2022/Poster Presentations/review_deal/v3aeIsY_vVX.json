{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This work proposes a hybrid autoregressive and adversarial model for sound synthesis (including but not limited to speech), conditioned on various types of control signals. Although recent adversarial approaches have gained favor over previously popular autoregressive approaches in this domain, because of their ability to produce audio signals much more quickly, the authors argue that these models tend to introduce certain types of artifacts which stem from an inability to learn accurate pitch and periodicity. They propose to address this by reintroducing some degree of autoregression, without compromising too much on inference speed.\n\nReviewers praised the presentation of this work, the thoroughness of the experimental evaluation, and the audio examples provided. A few concerns were also raised regarding related work and the clarity of some parts of the paper, which the authors have taken the time to address. After the discussion phase, all reviewers chose to recommend acceptance, and I will follow their recommendation."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper introduces a Chunked Autoregressive GAN (CARGAN) method for conditional synthesis which is autoregressive over chunks of audio but uses Hifi-GAN like parallel generation within a chunk. The method is motivated by the periodicity and pitch errors shown by existing parallel (non-autoregressive) GAN generators. The authors argue that the periodicity and pitch errors are caused by the parallel GAN generators which may disregard continuity of the periodicity and pitch when generating audio. The auto-regression makes sure that the generated audio is less error-prone in regards to continuity and accuracy of the periodicity and the pitch. The discriminator takes in some contextual samples as well as the chunk that is generated to make sure there is less boundary artifacts.",
            "main_review": "Strengths:\n1. The paper is well presented and the reasoning is well discussed.\n2. Literature review is satisfactory, but some more references can be added.\n3. The artifacts caused by parallel generation are highlighted in the web page with extensive audio examples.\n4. The method is evaluated by looking at pitch, periodicity errors and F1 score of voiced/unvoiced classification as well as A/B comparison subjective listening tests with other conditional synthesizer methods.\n\nWeaknesses:\n1. An example of hybrid methods would be: \"Wave-tacotron\" which is very similar to this paper but uses flow instead of GANs. I think it needs to be cited.\n2. The boundary errors are quite audible in music examples even with context inclusion. Maybe the hyper-parameters such as chunk size and context size need to be tuned for music.",
            "summary_of_the_review": "The paper reads well and has a very extensive demo page. I believe it furthers the state of the art in this area.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors present a chunk-wise auto-regressive generative model for audio with adversarial loss.\nIn particular, the authors note the limitations of purely convolutional adversarial audio generation model for text to speech as those fail to provide a consistent pitch for an extended duration. The authors provide a number of audio samples to illustrate the issue.\nThey evaluate the proposed method on standard datasets (VCTK) and perform subjective evaluation. they show that their method achieve better pitch correctness, and can improve perceptual evaluations, in some cases.\nThey also evaluate the speed of evaluation and training of their method. Unsurprisingly, the method proposed is slower than non auto-regressive generation but the chunking makes it theoretically faster than purely auto-regressive methods.\n",
            "main_review": "Strengths:\n- the pitch motivation is well presented and natural.\n- the solution proposed is a good trade-off between accuracy and speed, and is straightforward to put in place.\n- the authors provide extensive evaluations, including MOS.\n- the authors provide extensive examples to highlight different types of artifacts\n\nWeaknesses:\n- Some remarks (see after) should be addressed.\n- a single baseline (HiFi GAN), while their exist a considerable number of vocoders (auto-regressive, flow etc).\n- the phase prediction issue is intuitive and could be more compactly treated.\n\n\nQuestions and Remarks:\n- In [1], the authors condition on a melody synthetize from the pitch (i.e. a cosine waveform with consistent phase and the right pitch). This means the generator can use the phase from the waveform as a base for its generation. This is much more efficient than giving the pitch directly for the reasons the authors state. This is related to your work, and it could be interesting to see how HiFi-GAN performs with that simple fix. Of course the advantage of the method proposed here is that there is no need to extract pitch information for generation.\n- for the toy task, the results for HiFi GAN should be the same pattern repeated as the input to the model is constant and so each output frame should be the same no?\n- can you comment and add speed comparison with other methods, in particular waveglow, wavernn etc.\n- do the authors know if Parallel WaveNet would suffer from pitch artifacts ? because it is no longer autoregressive and only conditioned on random noise. intuitively one might think that this would be problematic. Same question for flow based methods like waveglow.\n\nThere a number of typos:\n- p2. 'deniosing'\n- p2 'hybird'\n- p 4: 'Autoregression is an sensible' -> 'a sensible'\n\n\n[1]: Unsupervised Cross-Domain Singing Voice Conversion, Polyak et al 2020.",
            "summary_of_the_review": "Simple method suggested by the author to fix some known artifacts from GAN audio vocoders.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "no concerns",
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a conditional waveform synthesis (CWS) model called Chunked Autoregressive GAN (CARGAN). By combining the advantages of the AR and non-AR generative models, CARGAN achieves better pitch accuracy with faster training speed and less memory usage without much decline in generation speed.\n\nFirst, it shows that the previous GAN-based non-AR CWS models do not accurately preserve the pitch and periodicity of the audio signal. Then, this paper demonstrates that there is a close relationship between pitch and phase, and the inductive bias of AR model is good at learning the relationship. Especially, this paper supports the claim by conducting an experiment that compares the ability of  AR and non-AR models to learn the cumulative sum operation, because the instantaneous frequency and instantaneous phase have the cumulative sum relationship. Lastly, by comparing HIFI-GAN and CARGAN based on spectrogram-to-waveform conversion task, this paper shows that CARGAN reconstructs the original waveform more precisely. Furthermore, CARGAN shows faster training speed and less memory usage than those of the HIFI-GAN.",
            "main_review": "This paper proposes a novel conditional waveform generative model, CARGAN, combining the advantages of AR and non-AR models. \nFurthermore, it not only improves the performance of the CWS, but it also conducts the side-by-side listening comparison and pitch analysis, which makes it easier for readers to understand what is the problem of the previous non-AR CWS model. Also, I think that the consideration about choosing the optimum chunk size depending on the size of the receptive field also can give good insights to future works.\n\nHowever, I have several concerns about this work.  \n\nFirst, I am not sure that the comparison of the ability to learn the cumulative sum operator is enough to explain the performance gap. This is because, unlike the cumulative sum operator, there is a range for the phase values ([0~2pi]), so it might be possible for HIFI-GAN to cover the phase if it has enough receptive field to cover the phase. If it is not, I think it would be better if there is an experiment showing the change of the performance when the length of the input gets longer, because the loss for learning cumsum gets larger when generating full sequence.\n\nSecond, although it conducts diverse analyses and can give many insights about designing the architecture of the model, I think there is a slight lack of novelty. As mentioned in Section 1, there have been several models that generate audio autoregressively in the chunk level [1]. Also, in text-to-speech, there is already a technique of using a reduction factor that controls the trade-off between the sampling speed and the speech quality [2].\n\nThird, there are several questions about the paper that I am confused about.\n* In Section 2, it is a little difficult for me to understand how penalizing the L1 distance between activations leads to requiring a model to produce a waveform that is close to the ground truth waveform.\n* According to Figure 2, it seems that a part of the generated speech is fed back to the model. Is it also true during training or the ground-truth segment is used? (i.e. teacher-forcing)\n* If the CARGAN and HIFI-GAN have the same generator and discriminator architecture, how the training speed and memory usage can be improved? I think CARGAN would rather be slightly inferior because of the additional previous waveform information. If this is because of the difference in the sequence length used in training (8192 vs. 2048), what happens when training the HIFI-GAN with 2048 segment size?\n\nLastly, there are some personal opinions about this paper.\n* As far as I know, there is a method to implement the causal convolution with the non-causal convolution layer based on padding operation [3]. Therefore, I think the maximum length of a learnable cumulative sum is not [(m+1)/2].\n* I think it would be better if the loss is described more in detail to understand the loss compensation technique.\n* I wondered about which chunk size 'k' is used in the experiment of training the cumulative sum operation, in order to understand the relationship between receptive field, chunk size, and the sequence length.\n* There is no subsubsection 5.2.2, so I think the subsubsection 5.2.1 can be combined with subsection 5.2\n* I think that it would be better if the transcripts for the audio samples in the demo page are written with the words to focus emphasized.\n\n\n[1] Weiss, R. J., Skerry-Ryan, R., Battenberg, E., Mariooryad, S., and Kingma, D. P. Wave-Tacotron: SpectrogramFree End-to-End Text-to-Speech Synthesis. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021.\n\n[2] Wang, Y., Skerry-Ryan, R., Stanton, D., Wu, Y., Weiss, R. J., Jaitly, N., Yang, Z., Xiao, Y., Chen, Z., Bengio, S., Le, Q., Agiomyrgiannakis, Y., Clark, R., and Saurous, R. A. Tacotron: Towards end-to-end speech synthesis. In Interspeech, 2017.\n\n[3] https://theblog.github.io/post/convolution-in-autoregressive-neural-networks/",
            "summary_of_the_review": "Overall, I think this paper gives several insights to design a conditional waveform synthesis model by combining AR and non-AR TTS model.\nHowever, I think it slightly lacks novelty in that there have been several models designing an autoregressive model with chunk-level output. Also, I think it is a little weak in supporting its claim and there is room for improving the writing. As a result, I will give a score of 5 for this work.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}