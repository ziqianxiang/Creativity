{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The reviewers appreciated the treatment of the topic of certifiable robustness done in this work and although they had a number of concerns, I feel they were adequately addressed by the authors."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies certified robustness of a policy in a reinforcement learning setting. The authors propose a method, similar to randomized smoothing in supervised learning to get certified robustness for a policy. The authors provide theoretical evidence and experimental results.",
            "main_review": "Strengths:\nThis paper tries to tackle an important problem: how to get certified robustness in reinforcement learning (RL) problems. The method that the authors propose brings the gap between the randomized smoothing technique and RL.\n\nWeakness:\nI think this paper needs to be better polished in order to be published. I found several places that are quite hard to follow.\nI also find that the idea is quite incremental given that randomized smoothing has been widely studied in supervised learning and the key idea in this paper is quite similar. The authors mentioned that the Adaptive Neyman-Pearson Lemma is a non-trivial result, but I found that the discussion about this lemma is quite confusing.\n\nBelow are some detailed comments.\n\nI found Figure 1 quite hard to understand. It would be helpful to explain this figure better.\n\nI found that the problem formulation is a bit inconsistent throughout the paper. In Section 1, it seems to me that the adversary can only add $\\epsilon_i$ to the observation. Then I found that in Section 3, it is mentioned that the adversary can also choose a smoothing noise $\\delta_i$. It would be useful to clarify and make the formulation consistent. Is the distribution of $\\delta_i$ chosen before the algorithm starts? Does the adversary have to add $\\delta_i$ in the perturbation? For example, if the adversary believes that the $\\epsilon_i$ perturbation is strong enough, maybe it can skip adding $\\delta_i$?\n\nLemma 1: the symbol S has already been used to denote the state space.\n\nI am a bit confused about the construction of $Y^{st}$. Does it mean that the best strategy for the adversary is to put all the perturbation budget to the first time step? Is this phenomenon observed in the experiments or is it only for theoretical purpose?\n\nSection 4.3: can the authors provide more specific expressions for \"underline F\" and \"overline F\"?\n\n===After response===\n\nThanks to the authors for the response. Now I understand that system B is not the adversary; instead it is a virtually constructed system for the purpose of the proof. This makes more sense and helped me understand the paper better. However, it should be more explicitly explained in the paper what is the actual attack system (which part the adversary is allowed to perturb) and what is the virtual system used for analysis. Currently in Figure 2 of the paper, it still seems to me that system B is just the attacker.\n\nI increased my score but overall I still feel that there is a lot of room for improvement in terms of the clarify of the paper.\n\nI also feel that I am not familiar enough with the existing proof technique for randomized smoothing so I may not be able to evaluate the correctness very well. Thus I decided to lower my confidence score as well.",
            "summary_of_the_review": "Overall, I found this paper a bit hard to follow and I don't think the key messages are clearly conveyed. I recommend the authors to carefully revise this paper in order to be published.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposed a method for RL agents' robustness certification for using randomized smoothing. An adaptive Neyman-Pearson Lemma is developed and a robustness guarantee in terms of the cumulative reward is obtained. This paper also gives a worst-case setting to prove the tightness of the aforementioned certification and also evaluate their certificates in two environments under adversarial attacks.",
            "main_review": "> In general, I think this paper is clearly written and easy to follow. The proposed method is intuitive and the experimental results are convincing. Studies on RL robustness are still limited and many questions remain open. \n\n> A major competitor of this paper is Zhang et al, NeurIPS 2020, which reduced the significance of contribution. But I agree that the authors adequately discussed the connection and difference between their work and the related works. Also, though the philosophy is borrowed from Cohen et al, randomized policy smoothing is still a completely new approach for defending RL models.\n\n> I would also point out that the formulation of $\\bar{\\pi}$ in Section 4.3 is very similar to a POMDP and I guess that the authors did not realize this. I think it would be better if the authors can provide some discussions on this matter. Similar connections are also mentioned in Zhang, et al. \"Robust reinforcement learning on state observations with learned optimal adversary.\" ICLR 2021, which I think the authors may also need to include in the reference.",
            "summary_of_the_review": "This paper studies an important and novel problem with limited works in the literature: RL agents' robustness certification. The authors made strong assumptions and borrow ideas (randomized smoothing) from existing works in similar fields. The contribution is obvious, though not very significant. I am positive about this paper and recommend a weak acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper presents theoretical lower bounds (i.e., certified robustness guarantees) on the performance of RL policies with randomized smoothing against norm-limited adversaries. The proposal of the paper is an extension of the Neyman-Pearson Lemma to the adaptive settings. Authors present formal proofs for their proposal, and evaluate their claims in 4 RL benchmarks.",
            "main_review": "Pros:\nRegularization via augmentation with noisy samples has been a major focus of research in defending against inference-time adversarial perturbations. This paper advances the state of the art by presenting tight lower-bounds on the return of policies that are trained on randomly perturbed observations, when subjected to norm-limited adversarial perturbations of observations. This work also overcomes the limitations of prior work by Zhang et al. via extending the certification to the entire trajectory, and not just step-wise experiences of the agent.\nThe theoretical treatment of the problem is sound, and clearly stated.\nCons:\nIn multiple sections of the manuscript, authors claim that the proposal of policy smoothing is a novel contribution of their work. This does not seem to be the case, as numerous works in the literature have already proposed and investigated the regularization of RL policies via training on randomly and adversarially perturbed observations. \nIn the entirety of the presented analysis, there seems to be an implicit assumption that the adversary is aware of the observations of the target agent. This assumption is expressed more clearly in Algorithms 1 and 2. Do the authors assume that in the absence of the adversary, the target agent’s observation would be the same as the state? If so, this needs to be clearly stated in both the problem setup and the threat model.\nThe empirical results seem lacking. If policy smoothing is a focus of this study, the results should also report the effect of policy smoothing at both training and test-time under normal (i.e., non-adversarial) settings. This requires more information to be presented than just the single dot on the performance curves under adversarial attacks with 0 budget. \nIn the full Pong game, the certified lower-bound seems to be very loose compared to the presented results. While the interpretation presented in the appendix seems valid, it may be best to rethink the attack methodology to show that the lower-bound is meaningful.\nMinor comments:\n\nPage 2 - below equation 1: “...end after t time steps” - perhaps would be better to use upper-case T to avoid confusion in symbols. \n\nPage 8: “an attack tailored to the threat model defined in 1”\n",
            "summary_of_the_review": "I find this paper to be a valuable contribution to the field, and believe that the theoretical treatment and methodology of this paper can inform future investigations in not only test-time adversarial robustness, but also the general analyses of sequential perturbations in RL.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper discusses the extension of randomized smoothing (RS) to sequential interactive settings, on the example of reinforcement learning (RL).\nThe authors show, that in order to certify a sequence of steps in this setting it the theory from RS can not be directly utilized as it does not account for the adaptiveness of the adversary/environment.\nTo enable the application of RS, the authors propose an adaptive version of the Neyman-Pearson Lemma, which can be used to instantiate RS to this setting.\nMathematically, this works by first showing that the impact of a potentially stochastic adversary can be bounded by a deterministic adversary, which in turn can be bounded by a deterministic adversary with special structure.\n\nEquipped with this theory, the authors introduce policy smoothing, an algorithm that lower-bounds on the reward (or other functions of the trace) that an agent will obtain (with high probability) in the presence of an adversary.\nThe authors showcase the effectiveness of the approach on the Cartpole, Mountain Car, Pong and Freeway for different deep RL agents.",
            "main_review": "I am glad to see RL-verification approach that considers traces, rather than just applying static verification techniques to individual time-steps.\nWhile the resulting algorithm is a straight-forward adaption of RS, I think the conceptual and mathematical arguments on how to apply it in this setting are the key strength of the paper.\nFurther, the main paper and the technical arguments in appendices B - D are well written and mostly easy to follow. The key mathematical results seem to be correct.\n\nThat said, the results indicate that the presented approach does not provide a usable defense for sequential-decision-making yet, as none achieved score-vs-budget trade-offs seems really desirable. While not ideal, I think the paper can still benefit the community by shifting focus on trace-based certificates in sequential decision making and is further valuable for its theoretical contributions.\n\nTo conclude I have a few questions:\n- In the tightness proof (section 4.2) you suddenly assume another policy $\\pi'$. Can you elaborate as to why the argument isn't over the initial $\\pi$ and why this change is possible?\n- Do you think it would be possible to utilize multiple temporally overlapping certificates to improve upon the budget-score trade-off?\n- [1] seems to provide similar guarantees to the proposed approach. Can you briefly outline the difference in approach?\n\n[1] CROP: Certifying Robust Policies for Reinforcement Learning through Functional Smoothing; Fan Wu et al.; arXiv 2021\n",
            "summary_of_the_review": "The paper presents a novel way to show robustness to perturbations in sequential decision making in the presence of an adversary.\nWhile empirical results are not strong, the paper appears to be formally correct and would benefit the community.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper extends the smoothing technique to the RL setting as a defense to test-time adversarial attacks. An adaptive version of the Neyman-Pearson Lemma is proposed. Empirical evaluations showcase the effectiveness of policy smoothing.",
            "main_review": "1. In the adversarial objective defined above equation (1), it appears as if the perturbation sequence $\\epsilon_t$ is fixed regardless of what actual state $s_t$ is observed, whereas from later sections it's obvious that the paper actually allows $\\epsilon_t$ to depend on $s_t$.\n\n2. What is \\Phi in Theorem 1?\n\n3. In the experiments, what exactly is the undefended agent? It is stated that the defense applies policy smoothing during both training and testing phases. So for the construction of undefended agents, are they constructed just via standard training, i.e. with no smoothing during training either?\n\nThe result in Figure 5 seems unintuitive because I would anticipate the undefended agent to perform somewhat better than the smoothed policy when there is no adversarial perturbation, as intuitively techniques such as smoothing must deteriorate the performance in a clean environment. \n\nI'm not very familiar with the Neyman-Pearson Lemma, but it seems unintuitive to me that in Lemma 3, one can provide a robustness guarantee against an adaptive adversary from the construction of a much weaker adversary. Some remarks and background regarding the Neyman-Pearson Lemma would be helpful.\n",
            "summary_of_the_review": "Experiment results seem convincing, except for some minor issues. I'm not familiar with the prior literature to judge the significance of the new Neyman-Pearson Lemma. The exposition can certainly be improved to make the paper more self-contained and readable. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}