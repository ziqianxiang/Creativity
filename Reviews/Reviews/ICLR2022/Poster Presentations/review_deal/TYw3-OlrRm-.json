{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper studies the problem of training tiny networks, by proposing a new training method called Network Augmentation (NetAug). The main challenge for training tiny networks lies in underfitting, which data augmentation and dropout etc. regularizations may suffer from for tiny networks. To overcome this hurdle, the proposed method first embeds or augments the tiny network as a subnet into a larger network, mostly by enlarging the width; then the gradients from the larger network are used as additional or auxiliary supervision. With this training strategy, the tiny model can perform better than the conventional training scheme on ImageNet and several downstream tasks. The proposed method is simple to implement and complementary with other techniques such as knowledge distillation and pruning. While there are lots of works studying how to improve the accuracy of large models, there are relatively fewer works focusing on the tiny network training. Despite that there are existing works sharing a similar idea of NetAug for large model training, which slightly hurts the novelty of this work, the majority of reviewers still like the idea and suggest to accept the paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "To improve the performance of compact neural networks with limited model capacity, the paper proposed Network Augmentation (NetAug) which addresses the under-fitting problem in small neural networks. This is accomplished by incorporating the target tiny neural network into bigger neural networks for additional training supervision. The paper demonstrates the effectiveness of\nNetAug on improving the effectiveness and the performance of tiny models image classification and object detection on several image datasets, achieving up to 2.1% accuracy improvement on ImageNet, and 4.3% on Cars. On Pascal VOC, NetAug provides 2.96% mAP improvement with the same computational cost.",
            "main_review": "The paper introduced a training mechanism that includes auxiliary forward flow and supervision from the very training of the very large model into the training of the tiny nets, this training mechanism improves the classification and object detection performance of tiny nets on several large datasets. The paper does not provide evidence that if a small sub-network of the larger network which is equivalent to the tiny network in network sizes will provide the same results as shown in the paper, in other words, does the auxiliary training necessary?",
            "summary_of_the_review": "As I stated in my main review, I suspect that auxiliary training is necessary for getting the better performance of the tiny network. I suspect that an equivalent sub-network of the larger net will have similar results in comparing to the performance of the tiny network solely training on the large datasets (ImageNet, Pascal VOC etc).",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work proposes a novel method to train tiny networks. The proposed method first augments the width of the tiny network to make the tiny network a bit larger. Then the gradients from the larger network are used as additional supervision. The authors show that the proposed method significantly improves the generalization performance of tiny models, is complementary to knowledge distillation and pruning. In addition, the authors show that data augmentation and dropout or similar regularization methods actually hurt the tiny models in training.",
            "main_review": "[Strengths]\n\n1) The authors show that large models benefit from data augmentation and dropout while tiny models suffer from these regularizations/augmentations.\n\n2) Augmenting the network width to improve the trainability of tiny models is an interesting approach.\n\n3) Extensive experiments on ImageNet, Pascal VOC and several down-stream tasks show the superiority of the proposed method.\n\n4) Ablation studies show that the proposed method can be combined with KD and pruning to get better results.\n\n[Weaknesses]\n\n1) The proposed method is similar to progressive shrinking used in OFANet in principle.\n\n2) Augmenting the width of the network in training  has been proposed before, such as \"Go Wide, Then Narrow: Efficient Training of Deep Thin Networks. ICML 2020.\". It would be better to discuss the key differences.\n",
            "summary_of_the_review": "While there are lots of works studying how to improve the accuracy of large models, there are relatively fewer works focusing on the tiny network training. This work demonstrates that the tiny models suffer from under-fitting rather than over-fitting therefore requires different training strategies. The proposed method, NetAug, is effective and simple to implement. It also works well with other techniques such as knowledge distillation and pruning. There are existing works sharing a similar idea of NetAug for large model training, which slightly hurts the novelty of this work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed to train and augment a tiny network by incorporating it into the larger networks with weight sharing training mechanism. The tiny neural network is learned with the auxiliary/additional supervisions from the larger models that wrap it. With this training strategy, the tiny model can perform better than the conventional training scheme on ImageNet and several downstream tasks.",
            "main_review": "Generally, I like the idea proposed in this paper. It is clear, somewhat interesting and looks reasonable. While, it is essentially similar to the one-shot NAS with weight sharing method, even the authors emphasized that they are different.\n\nIn the weight sharing training, the sub-network can also be augmented by auxiliary supervision across different iterations, it seems this proposed method just aggregates the gradients from auxiliary supervisions (from larger models) in a single iteration training, more like a special case of the former.\n\n**Some of my other concerns are as follows:**\n\n***About novelty:***\n\nAs I mentioned above, the overall idea and implementation strategy are both similar to the weight sharing mechanism.\n\nAlso, only considering width dimension without depth in this paper is too limited for the method to be deployed on real-world applications.\n\n***About experiments:***\n\nIn the paper, the authors claimed MobileNetV2-Tiny is a tiny model so the method is effective and ResNet-50 is a large one so the method is harmful, this may be true from the perspective when compared to some compact networks. But if we consider efficientnet_l2, vit_large, GPT3, etc., ResNet-50 is also a *tiny* model in this case, it seems the proposed method only works well on the extremely low-level performance families, basically, these extremely tiny models may not have fully converged so they are relatively easy for improvement. I suggest the authors can try to train the tiny models with more epochs like 600 and better optimizers like AdamW to make sure the model has completely converged (a strong baseline), then check whether the proposed method is still effective in this case.\n\n***About KD:***\n\nFirst, I personally don’t agree with the claims in Sec. 2 of the paper that “Recent studies (Furlanello et al., 2018; Cho & Hariharan, 2019; Yuan et al., 2020) show that the teacher model does not necessarily need to be larger than the student model in KD.” These papers claimed the teacher scale is not necessary but there are also a large number of literatures stating that larger models can provide more precise supervision and are crucial for better distillation, such as: A good teacher is patient and consistent [1], Is label smoothing truly incompatible with knowledge distillation [2] ReLabel ImageNet [3], etc.\n\nBasically, this is not a well-explored problem and in different scenarios the requirements for the teachers in KD are definitely different, so it seems the claims in this paper are too arbitrary and not well motivated. I just went through all the mentioned related papers, it seems the performance of the models in these papers is still at a very low level and they mainly tested on small datasets like CIFAR, which means the models may not yet fully converge. I don’t think the derived conclusions from these papers are more solid than [1] which has true state-of-the-art accuracy on the large-scale ImageNet.\n\n[1] Lucas Beyer, Xiaohua Zhai, Am ́elie Royer, Larisa Markeeva, Rohan Anil, and Alexander Kolesnikov. Knowledge distillation: A good teacher is patient and consistent. arXiv preprintarXiv: 2106.05237, 2021.\n\n[2] Zhiqiang Shen, Zechun Liu, Dejia Xu, Zitian Chen, Kwang-Ting  Cheng, and Marios Savvides. Is label smoothing truly incompatible with knowledge distillation: An empirical study. In ICLR, 2021.\n\n[3] Sangdoo Yun, Seong Joon Oh,  Byeongho Heo,  DongyoonHan, Junsuk Choe, and Sanghyuk  Chun. Re-labeling imagenet: from single to multi-labels, from global to localized labels. In CVPR, 2021.\n\nFurthermore, KD seems not related to the proposed method, I’m a little bit confused why the authors spend a lot of space to introduce KD in the related work and further involve it into all the experiments. To my perspective, some statements are even not accurate and also not yet verified.\n\nKD brings the significant contribution of improvement, but it will also involve much uncertainty and vagueness of the improvement. I think it's better for the authors to focus on the proposed method itself in the experiments. Considering the not very accurate statements for the KD introduction, I’m concerned about the solidness of the experiments in this paper.\n",
            "summary_of_the_review": "I think this is an interesting paper, but I still have a few concerns stated above, especially on the experimental part and some statements on knowledge distillation. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "When training tiny neural networks, a major issue is under-fitting. This paper proposes Network Augmentation (NetAug), which randomly augments (enlarges) neural networks during training to solve the under-fitting issue. NetAug shows consistent improvements when training tiny neural networks in various settings, including ImageNet classification, fine-grained classification, and object detection.\n",
            "main_review": "### Strengths\nS1. This paper has a clear motivation. \\\nS2. This paper is easy-to-read and well-organized. \\\nS3. The proposed method is simple, so it has broad applicability. \\\nS4. The proposed method shows consistent improvements in a variety of settings and high compatibility with other frameworks, including knowledge distillation and network pruning.\n\n### Weaknesses\nW1. How does the proposed method solve the under-fitting issue? \\\nI agree with the under-fitting issue in training tiny neural networks. However, even though augmented networks are larger than the target tiny networks, the capacity of the target network is fixed (i.e., constant). Hence, the under-fitting issue of the target network still exists and might remain unsolved. I wonder why and how the proposed method can find a better optimum.\n\nW2. Missing baselines that can avoid under-fitting. \\\nThere are several simple techniques to avoid under-fitting. For example, one can use a smaller weight decay (e.g., 1e-5, 1e-6) or weaker data augmentations. Also, using a small batch size can be another option since randomness may prevent stuck in the local optimum.\n\n### Minor comments\n- The citation formats in tables are not consistent with that in the main body.\n",
            "summary_of_the_review": "I think this paper's strengths are clear, but some important parts (explanation and baselines) are missing. Hence, my initial rating is weak reject. I will raise my rating if all the issues are resolved with a strong rebuttal in the discussion period.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}