{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes an extension to learning a representation: it motivates, proposes and evaluates a new regularizer term that promotes smoothness via enforcing the representation to be geometry-preserving (isometry, conformal mapping of degree k). Comparisons with a standard VAE and FMVAE (Chen et al. 2020) are shown and experiments are provided on CelebA with several different attributes as target classification tasks. \n\nThe paper has received extensive reviews and the authors have successfully answered most of the concerns raised, mostly regarding comparisons to other techniques that try to introduce a regularization based on the properties of the Jacobian of the decoder network. \nThe appendix has been extended as a result of the rebuttal and the paper could be accepted.\n\nNotes: \nI find the formulation based on the notion of the isometric decoder somewhat surprising as the encoder is a key object of interest that controls the nature of the representation. The authors should clarify the assumption 3 in 3.3 better by the consideration of potentially $dim(z) << dim(x)$, how the isometry of the decoder effects the encoder, \n\nAdditionally, for the latent space flattening an ablation using SVD (merely a linear mapping for $i(\\cdot)$) could be considered.\n\nReviewer ZGHS has noted that they raise their grade to 6 in their comment, but this is still not currently reflected."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies the aspect of preserving geometry on the learned latent space representations. In particular, the paper looks at a hierarchy of geometry-preserving mapping (isometry, conformal mapping of degree k, area preserving mapping, etc.). Existing popular methods such as SimCLR pays limited attention to preserving geometric relationships. The paper shows that the mapping that preserves angles and relative distances is better than the ones that preserve angles and absolute distances. The main contribution of this paper is to propose a representational learning technique that uses a reconstruction loss and an isometric regularization term. This allows us to learn embeddings that satisfy isometric properties while not suffering from any reconstruction loss or just marginal reconstruction loss. The comparison is done with VAE (Kingma & Welling, 2014) and FMVAE (Chen et al. 2020). Experiments are shown on CelebA with 40 annotations.",
            "main_review": "Pros:\n\nThe idea of studying geoemetry-preserving regularization while learning representations is a challenging and an important topic with a large potential impact.\n\nCons:\n\n1) In section 4.2 the paper talks about using a seperate invertible mapping from R^m -> R^m that essentially looks at not affecting the reconstruction accuracy and only improves the isometric representation. I am not sure why this can not be already captured by the original decoder that takes an input point in R^n-> R^m\n\n2) The experiments show that the isometric regularization and FM leads to marginal or no loss in reconstruction accuracy. In the implementation H(x) is assumed to be identity and this is a strong assumption. As the authors acknowledge, it would be more useful to use domain-specific knowledge. However, by assuming identity for H(x) the regularization gets closer to Jacobian regularization that is already a well studied problem:\n\n  Rifai, S., Dauphin, Y. N., Vincent, P., Bengio, Y., and Muller, X. The manifold tangent classifier. In Advances in Neural Information Processing Systems, pp. 2294–2302, 2011a. \n\n   Rifai, S., Mesnil, G., Vincent, P., Muller, X., Bengio, Y., Dauphin, Y., and Glorot, X. Higher order contractive auto-encoder. In \nECML-PKDD, pp. 645–660. Springer, 2011b.\n\n   Hoffman et al. Robust Learning with Jacobian Regularization, 2019.\n\n3) The visualization in Fig. 1 does not clearly show the improvement using the isometric regularization approach shown in the paper. \n\n4) There should be some ablation on the mixup regularization that is used in this paper. It is not clear whether the improvement is mainly from mixup or the proposed regularization.\n\n5)  The paper does not look at sufficient strong baselines for representational learning methods. It only uses VAE and FMVAE. ",
            "summary_of_the_review": "Overall, the paper addresses an important problem but there are many concerns with the experiments and the formulation. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a new regularization approach to introduce geometric constraint to the latent space of an autoencoder. A hierarchy for geometry-preserving mappings is formulated to clarify how strong this constraint can be defined. In particular, authors focused on scaled isometries, i.e., maps that preserve angles and distances up to some scale factor. This scale is learnt together with the manifold and the latent space representation during the training of the autoencoder. This approach was proposed also in (Chen et al., 2020) for the so called FMVAE, but here a new coordinate-invariant regularization term is introduced that measure how close the decoder\nis to being a scaled isometry. Finally, a post-processing flattening procedure is introduced to further improve the geometry properties of the latent space. \nSeveral results have been reported on different applicative scenarios showing a clear improvement of the proposed approach in comparison with other methods of the state of the art .      \n",
            "main_review": "The paper is clear and well-motivated. The problem of introducing geometry properties to the latent space is challenging and not much explored. The proposed regularization method is convincing and well justified. The idea of coordinate invariant functional is novel (for the best of my knowledge) and theoretically sound. \nThe paper is well organized. The state of the art is complete. The description of method is clear. It would be nice to see a graphical scheme of the method architecture to give a visual explanation to the meaning of the involved functions and their role to move from one space to the other.\nExperiments are convincing. The experiments on the most simpler scenarios like MNIST and CMU motion capture are useful to understand the behavior of the proposed approach. The experiment on the most challenging dataset, i.e., unsupervised human face retrieval is important to appreciate the improvement of the proposed approach against other methods.    \nThere are some important points that authors should clarify:\n-In the introduction, authors claimed that using a less stringent regularization term on isometry is more helpful, but than is not clear to identify other parts of the paper that support this claim. Please clarify with more examples and details. \n-It is not clear the role of the stabilizing latent space flattening described in Sec 4.3. If this term is relevant it should be described in Eq. (11).\n-Although it is already clear that the proposed method is better than FMVAE, authors should explain more details on that, especially in highlighting the importance of a coordinate-invariant strategy.\n-h in Eq. (7) is different than h in Sec. 4.2, perhaps it would be better to change notation…,\n-It would be nice to see the benefit of the proposed method for data generation. Usually to evaluated the regularity of the latent space,  the generation of data in between two samples is visually shown. Is it possible to show something similar to appreciate the benefit of the geometric constraints?\n-Is the code available? More details on the used software and performance in terms of speed of method will be appreciated. \n",
            "summary_of_the_review": "\nThe proposed method is novel and convincing.\nThe paper is well organized and clear.\nExperiments shown that the proposed approach clearly overcome other methods.  \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper introduces a regularization scheme that constrains autoencoders to produce latent spaces which are isometrically closer to the input space, up to a certain scale factor, that is, a latent space whose pullback metric is closer to a scaled identity matrix. The paper further shows both (i) the tradeoff between two surrogate metrics for isometry and reconstruction accuracy, showing how much reconstruction error is affected by increasingly stricter regularization and (ii) that the representations learned by a variational autoencoder with such a regularizer results in better performance on a retrieval task when compared to existing baselines.",
            "main_review": "**Strengths:**\n\nI appreciate the theoretical motivation and foundation for the method proposed. Its justification and development were clear enough to follow. The experimental results in an unsupervised image retrieval task show a noticeable improvement in performance measured by multiple metrics when compared to a strong baseline. Furthermore, the tradeoffs between achieving a scaled isometry and balancing the reconstruction error are on the side of the proposed method, as evaluated on image and motion capture data.\n\n**Weaknesses:**\n\nWhile I mentioned most experimental results are convincing, I still think the paper has some considerable flaws to be addressed.\n\nFirst, the experimental results do not seem averaged over multiple random seeds. Additional information such as standard deviation, would be helpful in assessing how stable the training is and how reliable are those results for someone attempting to reproduce the experiments. The same applies to the curves shown in Figures 1 and 2, where the difference to the baseline is marginal. I see this as critical for the paper to be accepted.\n\nSecond, there seems to be a conflict between the KL-divergence regularization of a VAE w.r.t. changes introduced to the latent space caused by the newly introduced regularization scheme. I believe there should be a discussion on this, since knowing $P_Z$ seems to be necessary and without the KL term it is hard to say it follows a standard Gaussian.\n\nThird, I missed a discussion with other perspectives, such as from [1], where instead of changing the latent space that can be learned, one considers different ways of \"navigating\" it as is.\n\n**Questions for the authors:**\n\n- I found the first listed contribution to be problematic, which says the authors define a family of coordinate-invariant regularization terms. It strikes me as straightforward since it seems to be a by-product of the necessary mathematical foundation for the method. The family of functions itself was not really explored, just one specific case. Could you make it clearer how novel is that family of regularization terms w.r.t. existing work on this?\n- Regarding the generative performance of the models in question. While I understand this might not be the main goal of the paper, the methods used as starting point (VAE) do present the characteristic of being able to generate new data points. For instance, because we change the structure of the latent space, how does this affect the generative aspect of the model?\n- Why is it the case that IRVAE outperforms IRVAE+FM w.r.t. to P@1 by so much, but this does not happen in any other case?\n- What are the actual impacts of the additional computational costs of the regularization procedure, in terms of total training time compared to not using them?\n\n**Additional minor comments:**\n- Section 2 seemed more like a subsection, as it only deals with introducing some concepts and relating them to each other, without carrying any major point on its own.\n- There are some problems with notation, specifically the function h which seems to have multiple meanings (c.f. Eq. (6) and Eq. (11))\n- It is also usual to also credit [2] for VAEs.\n- Section 5.1: what is considered a sufficiently large number of samples for computing MCN and VoR?\nFigures 1 and 2 show categorical information (classes of data points) using a continuous color scale. While this does not affect interpretation, I believe a categorical color scale would improve readability.\n- References seem to need review. For instance, the first and fifth are from ICML 2020 and ICLR 2017, respectively, but this information is missing.\n\n[1] Arvanitidis, Georgios, Lars Kai Hansen, and Søren Hauberg. \"Latent Space Oddity: on the Curvature of Deep Generative Models.\" International Conference on Learning Representations. 2018.\n\n[2] Rezende, Danilo, and Shakir Mohamed. \"Variational inference with normalizing flows.\" International conference on machine learning. PMLR, 2015.\n",
            "summary_of_the_review": "The paper follows a solid theoretical foundation using (scaled) isometry between input and latent space as a principle for designing a regularizer to learn meaningful representations with autoencoders. Experimental results show improvements over relevant baseline methods in an unsupervised retrieval task and as tradeoffs between reconstruction accuracy and surrogate metrics for how close the learned latent space is to an (scaled) isometry. These positive aspects, however, are overshadowed by key major issues: (i) results are not averaged over multiple random seeds, which makes it difficult to assess the stability of the regularizer in training; (ii) and missing discussion of conflicting requirements of the KL divergence term with the one introduced, making the method analysis seem too superficial.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a new type of regularized autoencoder (actually variational autoencoder) that is designed to preserve the geometry of the data. They demonstrate that preserving angles and relative distances lead to an improved representation of the data. Specifically, they add a regularization and explore the tradeoff between reconstruction and geometry preservation. They further propose a scheme to flatten the latent representation in a postprocessing fashion.",
            "main_review": "The authors address an important problem in unsupervised learning, namely, manifold learning using a NN. The method proposed in the paper extends two recent works by Chen et al. and Jang et al. They propose an Isometric Regularization that is realized based on Hutchinson’s stochastic trace estimator to enable efficient estimation of the gradients. The English level is good, but the description of the method is hard to follow. It would be helpful if the authors could add a pseudocode of the proposed methodology. The authors demonstrate that the proposed method leads to several benefits compared to the existing method. However, I have several concerns about the proposed method. Specifically, my major comments are:\n-The method proposed here is a variational autoencoder; why is it presented in the introduction as an autoencoder?\n-On the same point, the variational part is not well justified; I had to read Chen et al. to understand why the variational part is included.\n-I am not convinced that the method leads to any benefit on MNIST. The MSE gain compared to FMVAE is really marginal and could result from non-optimal hyperparameter tuning. \n-How are the hyperparameters of all methods tuned?\n-If I understand correctly, the latent dimension used in the paper is 2; can you use a higher dimension? I would expect that for MNIST and other image datasets, a higher latent dimension is required.\n-The equidistance ellipses are not well defined, are these computed in the ambient space and plotted in the latent space? If this is the case, I am not sure that preserving large Euclidean distances is “good”. In high dimensional space, we should mostly trust small distances and not large ones. Therefore, these ellipses should be related to a small distance in the ambient space. The scale of these ellipses is not provided.\n-The improvements in the retrieval task do seem substantial; this supports the effectiveness of the method. However, the way the hyperparameters of all methods are tuned is not explained.\nMinor comments:\n-”Learning the data manifold”- this term is not well defined in the paper. Perhaps the authors mean “preserve the data manifold”. This is not clear; learning the manifold suggests learning its underlying parametric representation. \n -Before the words “We introduce” on page 4, a period or comma is missing; also, the sentence does not flow.\n-Figures 1 and 2, the MSE vs. MCN and MSE vs. VoR from both of these plots are redundant; you could only leave the ones in figure 2; the information is redundant.\n-What are the colors in the embedding? The labels? If so, the separation is not improved between classes.\n-Please mention in the caption of figure 2 on what data is this experiment evaluated.\n-The condition number calculation for figure 4 is not clearly explained; please add some intuition.\nThe following works have very similar ideas and should be cited:\n\n\n[1] McQueen, J., et al.  (2016). Nearly isometric embedding by relaxation. Advances in Neural Information Processing Systems, 29, 2631-2639.‏\n[2] Peterfreund, Erez, et al. Local conformal autoencoder for standardized data coordinates. Proceedings of the National Academy of Sciences, 2020, 117.49: 30918-30927.‏",
            "summary_of_the_review": "To summarize, the authors address an important problem and propose a new method for learning a low-dimensional representation from high-dimensional data. They rely on the manifold assumption and propose a new regularized to a variational autoencoder. The results demonstrate that the method leads to improvement in retrieval tasks. However, the method has some limitations (namely the low dimensional embedding dimension), and the experiments are not backed up by a rigorous scheme for hyperparameter tuning. For these reasons and the marginal improvements demonstrated in figures 1-3, I recommend a weak rejection of the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper makes two main contributions. The first is the introduction of a new regularizer term for the VAE loss, ensuring a (scaled) isometry between the learned latent space and the (typically unknown) data space. The second is a post-processing \"flattening\" step to improve the isometry constraint by directly operating on the latent space, while leaving the reconstruction error untouched. The resulting pipeline seems effective, as showcased on a selection of experiments over standard datasets.",
            "main_review": "- The paper shares an insight that whenever the mapping between latent space and data space in a generative model (e.g. VAE) approximates an isometry (or a scaled isometry, as proposed), one observes better results in a range of experiments. I am missing the key motivation behind this, perhaps also due to the paper itself mentioning (beginning of page 2) that this kind of requirement is too much to ensure good reconstruction. What is the rationale behind this? One unclear aspect to me springs from the fact that, in general, an isometry does not even exist between arbitrary manifolds, for example, one can not isometrically embed a sphere into a Euclidean space of any dimension. Why should a quasi-isometry lead to better generative models? Why isn't a homeomorphism enough?\n\n- What I think is really missing, is a so called \"killer application\" that would convincingly show the benefits of the proposed regularization. In particular, it is important to demonstrate that isometric representation learning is relevant, and that it can lead to significant improvements in many contexts. By contrast, the current experiments show some good results on simple data. Without a strong motivation, or a strong argument in support of the proposed method, it is hard to judge how potentially impactful it can be within the broader research field.\n\n- I would expect the choice of dimensions D and m to be important, since the distortion measure can vary greatly among different embeddings. How were these parameters chosen? I suggest to include some sensitivity experiments showing the influence of these dimensions on the final results, together with a discussion on their effect. Figure 6 in the supplementary goes in this direction, but I am missing an interpretation of these plots. Similarly, I could not find a sensitivity experiment showing the influence of parameter k (degree of the conformal mapping).\n\n- What is the consequence of choosing the identity metric for the data space?\n\n- The proposed measure (eq.7) is novel to my knowledge and is well described.\n\n- The manuscript is very pleasant to read, it is fluent, solid, and clear from start to end. The mathematics are sound and accessible. The writing is sharp.\n\n- The idea behind the paper is novel to my knowledge, but I found a few related works in recent conferences, see e.g. \"LIMP: Learning Latent Shape Representations with Metric Preservation Priors\" (ECCV 2020), it might be worth checking these out.\n\nA few side-comments:\n\n- It would be nice, although not strictly necessary, to visually illustrate the difference between scaled isometries of different order, perhaps by texture mapping a checkerboard pattern between 2D surfaces embedded in 3D if at all possible.\n\n- How costly is the flattening step (solving eq.11)?\n\n- How should the equidistance plot be read? The more isotropic, the better?\n\n- The paper mentions a few time some \"popular\" choices for the h() function. To what literature does the popularity refer to?\n\n- There is an incomplete sentence in page 5 \"inherent tradeoff We introduce\".\n\n- At the end of page 5, \"the the\".",
            "summary_of_the_review": "A well-written manuscript with a practical implementation of an isometric representation learning pipeline, which is nonetheless counterbalanced by unclear motivations and not very impressive results. Might still have an impact on a certain community, but a potentially broader impact is not clear at this stage.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}