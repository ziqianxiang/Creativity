{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The proposed method for set representation learning with an application to mete learning is well-motivated and reasonable. Reviewers' original concerns about novelty and technical presentation have been well explained and addressed in the revision. If some theoretical analysis can be provided regarding the proposed method, it would make this work stronger.\n\nIn summary, a positive recommendation is given here."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper provides an optimal transport (OT) based algorithm for improving existing summary networks for learning from set-structured data. The proposed approach views each set as a distribution over a set of global prototypes. To learn the distribution over the global prototypes, the proposed approach minimizes the OT distance to the set's empirical distribution over data points. Empirical results demonstrate that the proposed framework improves upon the existing summary network approaches as well as metric-based few-shot classification and generative modeling applications.\n",
            "main_review": "‌‌ **‌Originality**\n\nThe paper is original and proposes an Optimal Transport (OT) based algorithm for improving existing summary networks for learning from set-structured data.\n\n**Quality**\n\nThe paper is technically sound, however, the novelty is a little bit limited. The proposed approach doesn't improve upon the pre-existing summary networks architecture.\n\n**Clarity**\n\nThe paper is clear and easy to follow. Algorithm 1 should be moved to the main body rather than the appendix.\n\n**Significance**\n\nThe work is significant, but novelty is a bit limited.\n\n**Limitations**\n\n- The paper lacks theoretical analysis for the proposed approach.\n- Error bars are missing and not reported in the results.\n\n\n**Questions to Authors**",
            "summary_of_the_review": "‌‌ The paper provides an optimal transport (OT) based algorithm for improving existing summary networks for learning from set-structured data. The paper is original, and  technically sound, however, the novelty is a little bit limited. The proposed approach doesn't improve beyond the pre-existing summary networks architecture. Empirical results demonstrate that the proposed framework improves upon the existing summary network approaches as well as metric-based few-shot classification and generative modeling applications. The paper lacks theoretical analysis for the proposed approach, and the error bars are not reported for the empirical results.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "‌‌",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work introduces a straightforward development for set representation learning in the meta-learning context based on the intuition that the sets encountered in real-world meta-learning tasks tend to have common attributes, as illustrated in Figure 1. The idea is to jointly learn these common attributes $\\beta_{[1:K]}$, referred to as 'global centres' or 'global prototypes', and the parameters of a summary network using an Optimal Transport derived loss function that compares the empirical set distribution $P_j$ with a set summary distribution over the global centres/prototypes, $Q_j=\\sum_{k=1}^K h_{jk}\\delta_{\\beta_k}$. The idea is elegant in its simplicity and effectiveness.",
            "main_review": "**Strengths**\n\nThe intuition of the work is clear and the approach is reasonable a priori. This makes it easy to follow the motivations and illustrations, and lowers the barrier to the community adopting the presented techniques.\n\nThe proposed method is simple and does not require complicated architectural or training adaptations to improve the models to which it is applied.\n\nThe experiments are extensive, varied and compelling.\n\nThe related work section is mostly good and does a good job of comparing the contained approach with existing work (although see the minor comments.)\n\n**Weaknesses**\n\nThe main weakness is in the presentation of the manuscript. I think these problem can be addressed by the authors within the rebuttal period and my score is based on the expectation that this happens; if the authors do not improve the technical aspects of the presentation I will lower my score.\n\n1. (Technical) All the tables should include errors. Currently the tables 1 and 3 do not include errors. The table captions should also include details of the number of repetitions used to calculate the mean and the error that is being reported (standard deviation or standard error on the mean). An indication should be made when results are within error.\n\n2. A meta-statistic would be useful for Table 2. In many cases the original model and the +OT model performances are within error but overall 14 of the 16 head-to-head comparisons results in the OT model having the higher mean. It is unlikely that the addition of the OT loss does not improve performance given such a high number of head-to-head (e.g. in a simple binomial model with $p=0.5$ and 16 trials, the probability of 14 or greater successes is <0.03).\n\n3. (Non-technical) Figure 2 would be improved by using a better colour scheme and different markers for each model to make it easier to distinguish the different models in black and white or for a colour blind person. This is probably an access issue which is why I have included it in the weaknesses rather than the minor comments, and it is in the authors' interest to improve the presentation of their results in any case.\n\n4. (Non-technical) The last sentence of page 2 reads _‘Despite their effectiveness, there is no clear evidence that the output of these summary networks could describe a set’s summary statistics well, which have been proven crucial for the set-structured inference problems (Chen et al., 2021)’_. If X is necessary for Y, and Z has Y, then Z has X. If describing a set’s summary statistics well is crucial for set-structured inference problems, and summary networks are effective at set-structured inference problems, then the output of summary networks describe a set’s summary statistics well. Either effectiveness is clear evidence or describing a set’s summary statistics well is not crucial for these problems. This sentence should be revised.\n\n5. There is no supplementary code provided which means I cannot verify the experimental claims, this has resulted in a lower confidence score in my review.\n\n**Minor comments and suggestions**\n\nThe following comments are minor and potentially subjective so feel free to ignore them. My score does not depend on these comments being acted on.\n- Infinite Mixture Prototypes for Few-Shot Learning Allen et al ICML 2019 seems like a relevant reference that could be discussed in the related work section on developments to metric based approaches. The approaches differ in fundamental ways, and may be complimentary, but both have the concept of learning multiple centres and it may help to position this work to discuss the ideas presented here in contrast with those presented by Allen et al.\n- Algorithm 1 is in the Appendix and that should be stated in the text when it is referenced (e.g. page 4)\n- Algorithm 1 should be included in the main text.\n- 'as good as possible' on page 5 should be 'as well as possible'\n- The related work section has many sentences that should be revised:\n  - 'Different from them that focus on...'\n  - 'where most related work to ours is...'\n  - Different from these studies usually compute class' prototypes...'\n- Figure 3 is not particularly compelling and takes up a lot of space that could be better used by presenting Algorithm 1.",
            "summary_of_the_review": "In summary, I think this is a well motivated and elegant proposal that has been shown to be effective in a variety of experimental settings. I found the authors' claims to be well supported and I cannot find technical fault with their work that would support rejection, so I will recommend acceptance. The lack of supplementary code reduces my confidence in the review as I can only assess the claims as they are presented. The main issue with the work is its presentation which may be easily improved within the rebuttal.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": " This work proposes a method to improve set representation, and applies it in the context of meta-learning. More precisely, the method consists in jointly learning a summary network and prototypes using an optimal transport loss: the prototypes and summary network output should minimize the sum of the optimal transport costs for all the sets of the meta dataset. Experiments are conducted in the context of few-shot learning and other tasks used to evaluate summary networks.",
            "main_review": "My main concern is on the novelty of this work. Indeed, it seems to me that some ideas presented here can be found in previous papers on set representation learning that are not mentioned. In [1], prototypes against which OT is computed are learned (potentially jointly with an element-wise embedding) with or without supervision, although the outputs used in practice seem to differ. [2] presents a similar method in the context of graph representation. It is therefore difficult to me to understand the real contributions of this paper: it would be great if the authors could comment on the similarities and differences between their method and these ones.\n\nPros:\n- The paper is well-motivated and mostly clear.\n- Experiments are varied and seems sound.\n- In particular, the proposed framework seems to improve existing methods for set representations (DeepSets and Set Transformer).\n\nCons:\n- I have concerns on the novelty of this work.\n\nQuestions and remarks:\n- Equation (6) seems to be an instance of Wasserstein barycenter: could you comment on this?\n- It could be worth to look at this other paper on set representation in the related work [3] which has a different approach to the current and previous work yet also similar ideas.\n- Could the authors elaborate on the computational complexity of computing the transport plans? How many Sinkhorn iterations do you use?\n- How sensitive are you results with respect to the entropic regularization parameter $\\epsilon$ ?\n\n------------------\n\n[1]  A Trainable Optimal Transport Embedding for Feature Aggregation and its Relationship to Attention (Grégoire Mialon, Dexiong Chen, Alexandre d'Aspremont, Julien Mairal)\n\n[2] Wasserstein Embedding for Graph Learning (Soheil Kolouri, Navid Naderializadeh, Gustavo K. Rohde, Heiko Hoffmann)\n\n[3] Rep the Set: Neural Networks for Learning Set Representations (Konstantinos Skianis, Giannis Nikolentzos, Stratis Limnios, Michalis Vazirgiannis)",
            "summary_of_the_review": "The paper seems sound but I have concerns on the novelty: I am willing to raise my score if the authors clarify their contribution w.r.t [1] and [2].",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposed an improved training of summary network using optimal transport based auxiliary loss. The summary network with negligibly increased parameters show much better performance on amortized clustering, point-cloud classification, sum of digits, few-shot classification, and few-shot generation tasks.",
            "main_review": "Strengths\n1) This paper provides a promising research direction in using set-structured data on various downstream tasks. Especially, the paradigm using well-learnt set-representations as an conditional input deserves more explorations to tackle the limitation of few-shot generation problems.\n\n2) The proposed algorithm is well-motivated, and is effective to be implemented for real-world applications. In particular, it is easy to combine the summary network with the majority of existing metric-based meta-learning models and generative models.\n\n3) The experimental study is extensively conducted on various tasks related to set-structured data, and the improvement is consistent over different tasks. This shows the effectiveness of the proposed plug-and-play framework.\n\nWeakness\n1) The main idea of this paper that uses optimal transport in learning representation, has been widely studied such as https://openreview.net/pdf?id=ZK6vTvb84s, https://arxiv.org/pdf/2007.05840.pdf. It would be better to clarify what the contribution of this paper is and what the major difference is in contrast to the existing literature of optimal transport for representation learning.",
            "summary_of_the_review": "This paper is well-motivated and definitely effective for real-world meta-learning applications, however, the authors should discuss the literature of representation learning using optimal transport.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}