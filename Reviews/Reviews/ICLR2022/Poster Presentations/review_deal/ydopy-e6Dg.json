{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper is interesting, and its focus is timely and important, given the continuing rapid rise of transformers (and their dependence of tokenization of images). All three reviewers recommend acceptance, to varying degree. The paper will be a valuable contribution to the program at ICLR."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This is one of the first papers doing BERT-style pretraining for vision transformers, in particular the first to not use a pre-trained tokenizer (here the (part of) the network that generates the targets for the masked tokens). This has a similarity with BEiT, although BEiT uses an externally procured tokenizer (CLIP, so trained with labels). Instead, here, the idea is to use a teacher (EMA from student) to propose the targets for the masked tokens, yielding a masked image modeling task (L_MIM). The whole model and approach works with 2 augmented views of the same image (as in BYOL or DINO) and also has a self-distillation loss (L_CLS). L = L_MIM + L_CLS. The paper presents state of the art results in kNN (retrieval), image classification with linear probing and fine-tuning (ImageNet), image detection, instance segmentation (both on COCO), and semantic segmentation (ADE20K), within the classes of models that the authors studied.\n",
            "main_review": "The paper is clearly written, I believe I could reimplement the method from the paper (using the appendix). \nIt contains plenty of ablations that answer most of my questions about variants and details of the model.\nThe experimental results are (very) competitive, the comparisons are relevant, the litterature is adequately discussed. \n\nMaybe a missing interesting comparison would have been BYOL with a VIT model. \nThe visualizations are convincing (in the main body but also in the appendix), although the weaker point (which is not a major flaw) of the paper is probably section 4.3.1, where it is not very clear how the learned patterns differ really from BEiT or DINO, and more to the point, how the help from this training procedure in image recognition is of any different kind than from the competition.\n",
            "summary_of_the_review": "This is a well presented paper that shows convincing progress in masked image modeling pretraining for vision transformers. It should be accepted for publication at ICLR.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a new strategy for self-supervised training of vision transformer models by combining the self-distillation-based method DINO with mask image modeling. The momentum teacher model in DINO is taken as an online tokenizer. A mask image modeling loss that computes self-distillation loss between the reconstructed tokens and the output tokens of the momentum teacher is proposed. Extensive experiments are conducted to show the effectiveness of the proposed self-supervised training framework. The result on downstream tasks including classification, object detection, and segmentation also shows the benefit of the mask image modeling loss.",
            "main_review": "### Strength\n- The MIM for self-supervised feature learning is well motivated and technically sound. \n- Noticeable improvement on downstream tasks like detection and segmentation over the strong baseline DINO with the proposed MIM loss.\n- Extensive experimental results and visualization are given to show the effectiveness of the proposed mask image modeling objective and iBoT model. The authors also ablate different tokenization methods and different loss components to validate the contribution. \n- Detailed results and training settings are listed which can be helpful for future research.\n\n### Weaknesses\n- The improvement of the fine-tuned model in Tab 2 is limited. It would be better if the authors could explain more on this part.\n\n- The experimental results on the downstream tasks look good and the authors also provide some insights, but the technical contribution is limited. \n\n- The main difference between DINO and proposed iBoT is 1) masked image as input for the student model; 2) MIM loss for self-distillation on reconstructed patch tokens. Table 8 and section E in the appendix only provide ablation on the tokenization method, loss function, and dense self-distillation. To be fully convinced by the MIM loss, I think the author should also provide the result of the iBoT model without MIM loss (i.e. use the masked image as input for the student model of DINO). Since applying the mask on the input image is similar to Cutout augmentation, it can also improve the robustness and the performance of the trained model. \n\n- It would be better if the authors provide more comparisons on the proposed online tokenizer and pre-trained tokenizer (as in BEiT). It's not clear to me why the proposed online tokenizer can better capture the semantically meaningful pattern.\n\n- As multi-crop augmentation is important for both Swav and DINO, I think it’s also important to explain and analyze more on how to apply multi-crop augmentation with mask image modeling in the main paper. Also, the training costs (actual training time) are not listed. Is there any additional training cost for applying masked image modeling?\n",
            "summary_of_the_review": "Overall, I think this is a practical method with good experimental results. However, the proposed mask image modeling loss may not be clearly verified. As its current state, I would like to rate this paper as marginally below the acceptance threshold.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes the iBOT method. This approach is inspired by the contrastive self-supervised learning approach like DINO and the mask modelling approach like BeiT. The idea is to use an online tokenizer instead of a pretrained tokenizer like Beit. The iBoT approach combines a loss at the patch level like Beit and a global loss at the image level like DINO. IBOT is evaluated in image classification, detection and segmentation.",
            "main_review": "Strengths:\n- **Writting:** The paper is well written\n- **Simplicity:** The method is not a significant change with the previous approaches, but the method is simple and give good results. \n- **Ablations and Analysis** Provide elements to understand the impact of IBoT modifications compared to a more traditional SSL approach like DINO[1].\n\nWeakness:\n- **Comparison with others approaches**: Table 1 and 2, It would be interesting to have a cost comparison of pre-training approaches. Indeed, the number of parameters and the number of epochs used is not very useful. Indeed, using for example multi-crop increases the number of FLOPs and the memory used. Moreover, contrastive approaches using 2 or more different data-augmentation per image in each batch have longer epochs than approaches like BeiT. It would be interesting to try to make a more accurate comparison.\n\n-  **Comparison with supervised baselines for full fine-tuning setting**:  The number of epochs used for the supervised baseline and for the self-supervised approaches are not the same which makes the comparison quite unfair. It would be interesting to compare the performance by adapting the number of epochs in order to better measure the gain compared to the supervised model. \n\n[1] Caron et al., Emerging Properties in Self-Supervised Vision Transformers, ICCV 2021",
            "summary_of_the_review": "The paper is quite well written, the method is simple and gives better performance than DINO. Ablations and analysis are quite interesting. Nevertheless, the comparison between the different approaches does not seem to be complete enough. So, it is difficult to see if the iBoT approach brings a significant gain.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}