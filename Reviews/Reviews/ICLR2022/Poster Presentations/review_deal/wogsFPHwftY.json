{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper proposes a model for large-scale image retrieval. Unlike previous work that rely on local features, the proposed method aggregates local features into the so-called Super-features to improve their discriminability and expressiveness. To do so, the method proposes an iterative attention module (Local Feature Integracion Transformer, LIT), that outputs an ordered set of such features. By exploiting the fact that features are ordered, the paper proposes a contrastive loss on Super-features that match across images. The paper presents a thorough empirical evaluation on several publicly available datasets including relevant baselines.\n\nOverall the paper is well written and the empirical results are strong (including detailed ablations that motivate the design of the method). All reviewers and the AC appreciate the idea of applying the contrastive training at local feature level while only requiring image-level labels.\n\nReviewer hp4Y points out that the proposed LIT is not particularly novel, but previous work are properly cited. Also this is not a major issue given that the motivation is very clear, it is well executed and the empirical results are strong. \n\nReviewer uoYN had initial concerns regarding inconsistencies in the mathematical formulation of the method, which were resolved in a detail (and constructive) discussion with the authors.\n\nAll reviewers recommend accepting the paper, three of which consider the contribution to be strong. The AC agrees with this assessment and recommends accepting the paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes Local Feature Integration Transformer (LIT), a new architecture to extract mid-level local features from images that are used for image retrieval. The resulting features are called SuperFeatures and are shown to yield state-of-the-art performance on two standard image retrieval benchmarks. The paper further proposes a learning framework, called Feature Integration-based Retrieval (FIRe), to train LIT using a contrastive loss with only image-level supervision. In contrast to other deep learning based image features that apply the loss on the image embedding or aggregated local features, FIRe applies the loss on the local features directly. An interesting property of SuperFeatures is that they are ordered, meaning a SuperFeature with the same index i in two matching images will be localized on similar structures.",
            "main_review": "=== Strengths ===\n\n- LIT is an interesting new method for local feature extraction from images using a transformer-like architecture.\n- While supervision is only needed on the image level, the loss is applied on the local feature level. In contrast, previous work on deep learning based local features applied the loss on aggregated local features.\n- The ordered property of SuperFeatures helps interpret what each feature represents semantically.\n- Rigorous ablation experiments clearly motivate the design and parameter choices.\n- SuperFeatures consistently outperform the state of the art on the ROxford and RParis datasets.\n- A good selection of relevant related work is cited.\n- The paper is well-written, well-organized and easy to follow.\n\n=== Weaknesses ===\n\n- Some inconsistencies in the mathematical formulation of the method make it difficult to understand its details and would also make it difficult to reimplement the method. Details on this in the “Questions and suggestions on Sec. 3.1” section below.\n- The computational cost of extracting SuperFeatures from an image is not discussed. This would be important for real-time image retrieval systems where features of the query image have to be extracted on-the-fly. It would be interesting to compare the speed of SuperFeature extraction to, e.g., DELG or HOW.\n\n=== General questions and suggestions ===\n\n- In the paper, image retrieval is only performed using ASMK. Have the authors tried other methods too? In particular, do SuperFeatures lend themselves to traditional spatial verification using RANSAC? If yes, it would be interesting to see results on that. If not, it would be helpful to clarify in the paper why this is not possible.\n- Have the authors performed an ablation where the loss is applied on an aggregation of all local features? This could further motivate why applying the loss on the local feature level is beneficial.\n- How sensitive is the performance of SuperFeatures to the initial set of templates? Are templates specific to a certain domain, e.g., landmarks or can they are transferred to, e.g., products or artwork?\n- In Tab. 1, it would be interesting to also have the following rows: ”reciprocal+same”, “ratio only” and “same+ratio”.\n- What is the value of N (number of SuperFeatures) in the experiments?\n- The ablations show that matching only SuperFeatures with the same ID during training increases performance. However, in retrieval with ASMK, the ordered property of SuperFeatures is not taken advantage of. Have the authors tried an image retrieval method that matches only identical SuperFeature IDs?\n- In Fig. 5, what are the numbers (1000, 2000) next to the curves?\n- In the Related Work section, consider renaming the first subsection to “Image descriptors”\n- In Related Work on image retrieval, consider citing GeM (Radenovic2018b), since it’s a popular method for feature aggregation that SuperFeatures are also compared against.\n- In the second to last row of page 4, it seems a word is missing. Maybe it should be “the attention maps *tend* to be similar”\n\n=== Questions and suggestions on Sec. 3.1 ===\n\n- Are attention maps (\\alpha_l) the rows or the columns of the matrix \\alpha? Put differently, does an attention map tell me how well a single SuperFeature responds to each image location (like in Fig. 1), or do they tell me how much a single image location excites each SuperFeature?\n- In the 1st equation in Eq. 4, should the final index be \\alpha_N instead of \\alpha_L? This seems to be inconsistent with the row above that says that there are N attention maps.\n- It would help if the paper could specify the linear projection dimensions d_k, d_v and d_q.\n- What are the dimensions of the three matrices in the second equation of Eq. 5? As far as I understand: \\alpha is an (L x N) matrix as per Eq. 4, V(U) is an (N x L) matrix because it needs to have as many rows as \\alpha has columns, Q is a (d x N) matrix since it has N templates of dimension d. However, if these are true, then the addition would not work since an (L x L) matrix would be added to a (d x N) matrix. If I understand correctly, the output of \\psi(U, Q) needs to be a (d x N) matrix, meaning \\alpha would need to have d rows and V(U) would need to have N columns. I’d appreciate it if the authors could point out where I’m wrong here.\n- In the same equation, V(U) is a linear projection function applied to the set U. I think it is implied here that the elements of the set are concatenated into a matrix before applying the function. It would help if this were clarified in the text.\n- The 2nd equation of Eq. 4 seems to divide vectors by vectors. I think there is a 2nd index missing in the divisor, i.e. it should be \\alpha_{l,i}\nEquations 4 and 5 skip the “t” superscript, as explained in footnote 2. I personally think that having the superscript in these equations would help with clarity.\n- What is the motivation for applying softmax and L1 normalization to \\alpha?\n- In the 1st equation of Eq. 5, I’d suggest adding “Q^{t+1} = \\phi(...)“\n- I’d suggest using n instead of j as the running index for the templates, which would be consistent with using l as the running index for the local features.\n- I’d suggest putting footnote 3 in the main text since it is an important detail.",
            "summary_of_the_review": "I am confident to suggest the paper for acceptance since it introduces a novel and interesting way of using transformers to train local image features that outperform the state of the art. The method is easy to train since only image-level labels are required, however the loss is applied in the local feature level, which is an interesting novelty.  Detailed ablations motivate the design of the method. My main concern are inconsistencies in the mathematical formulation of the method, which I am sure can be resolved for the camera-ready version.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper introduces the concept of super features into image retrieval. The whole framework is named FIRe. The idea is to aggregate a set of features on the feature maps at different scales according to a set of query vectors representing different latent concepts. Hence, the proposed model somehow implements a transformer, which is called LIT in this paper.\n\nTo train this model, the authors define two losses. The first one is a triplet-like one executed on the super-feature granular. Some empirical criteria are established to select positive and negative super features. The second one decorrelates the super features from a single image.\n\nASMK becomes the measurement to compare two images when testing, which is a conventional option in comparing two groups of local features.\n\nWe see experiments on ROxford and RParis. The results are quite promising.",
            "main_review": "## Pros\n1. The paper has a clear motivation. The concept of super features is very intuitive.\n2. The majority of the design is reasonable. A transformer might be the best bet currently if we want to aggregate local features.\n3. Some parts of the experiments are interesting. Fig 6 clearly show how FIRe differs from HOW in selecting local features from all scales. The major results in Tab. 3 are quite promising.\n\n## Cons/Questions\n1. In Eq. (4), the attention maps are normalized twice alongside the axes of both L and N. In conventional attention, normalizing the attention along the key entries (counting to L) would be enough. So why should we do it again along the query entries?\n2. I think LIT implements a set transformer [a], at least a part of the set transformer, in the way of reducing and aggregating features according to a set of dummy queries in a QKV manner. I haven't seen any discussion on this. So maybe the authors can elaborate more on this and find more related articles. They should be very careful when claiming the novelty of LIT.\n3. The proposed model follows HOW to measure image similarity using ASMK. Does it have to be ASMK? As the authors mentioned in Sec. 3.1, S is ordered (which is quite different from the case of HOW). Simply comparing two super features from two sets on the same entries is reasonable as they reflect the same latent topic defined by Q. Aggregating the similarities on all entries only requires a time complexity of $O(Ld)$. I believe ASMK undergoes a higher time complexity of $O(1000^2d)$ according to the implementation. Why should we go for a more complex solution besides 'a fair comparison'?\n\n## Optional Questions\n4. $L_{attn}$ seems to be the solver in this paper regarding the oversmoothness problem. This design is just too empirical. Do we have a better option here?\n5. Do we have a better option to get multi-scale features than rendering the resized images multiple times? It seems this solution requires too much memory.\n\n------\n[a] Lee, Juho, et al. \"Set transformer: A framework for attention-based permutation-invariant neural networks.\" International Conference on Machine Learning. PMLR, 2019.",
            "summary_of_the_review": "I conservatively give a borderline before the discussion phase. The motivation and intuition of this paper are quite nice, but some parts of the details are questionable. I think the overall quality of this paper is about ok, but it doesn't hit. Some parts of FIRe are not absolutely novel. The authors need to defend these designs to convince the readers.\n\n------------------------------------------------------------------------\n\nThe authors' response satisfies me. Hence, I have changed my final rating.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a novel model, denoted as super-features, for image retrieval, with the following specific contributions: 1) features are extracted using an iterative attention module (the Local Feature Integracion (LIT) Transformer). Although similar modules have been proposed before in the literature [Locatello 2020], the authors reference them properly and their proposal have several modifications (decorrelation loss, learning the initial templates, residual connections). 2) The model is trained using contrastive losses on the local features, opposed to the more spread training procedure using constraints on the global embeddings. The paper presents solid experimental results in public datasets and compare them against reasonably chosen baselines. ",
            "main_review": "+ The experimental results are solid and relevant. The authors' approach outperform the baselines by a large margin, not only in mAP but also in its memory requirements. The ablation studies are complete and contribute to understanding the role of each part of the approach. This reviewer finds very relevant the fact that a global loss is irrelevant, as it is the dominant approach in most papers in the literature.\n+ Although it is not its strongest point, several details on the architecture and the losses are novel. This might be a critical point in a machine learning conference as ICLR: there are a few small contributions but not groundbreaking advances in this paper. In the opinion of this reviewer, the novel contributions plus the significantly improved performance is enough for acceptance.\n+ The research is excellently written and presented.\n- I miss Patch-NetVLAD [Hausler et al. CVPR21] in the list of references and the baselines. I am not expecting it to outperform the authors' approach, as Hausler et al. report worse performance than DELG, that is outperformed by this research. But I find it very related to this research, as it also refers to local features.\n- I cannot understand why the authors refer to their approach as \"mid-level features\". I could not find anywhere in the paper an explanation on why the features after the transformer should be called like that. My understanding is that the LIT will leverage **all** the local features to produce the super-features. The super-features will incorporate then global context, and I wouldn't call a feature that incorporates information from all the image as mid-level... The authors should explain why this naming or remove it.",
            "summary_of_the_review": "Summing up, although the paper does not have a very strong novelty, its several contributions together with the solid experimental results in a relevant application domain are sufficient for acceptance. I found two minor issues that I think the authors should fix, but the rest is excellently written and presented, which is also valuable.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Authors describe an architecture for deep image retrieval. For this, they use mid-level features that they call super-features (SF). They construct thes SF by an iterative attention module. They build them as an ordered set in which each element focuses on a localized yet discriminant image pattern. Auhtors show that for training, only image labels suffice. Authors present a set of experiments on common landmark retrieval benchmarks to validate that their SF substantially outperform state-of-the-art methods when using the same number of features. They also show that this requires a significantly smaller memory footprint to match their performance.",
            "main_review": "Strengths:\n\n1) A new method called Feature Integration-based Retrieval or FIRe for short. \n2) Paper is well written.\n3) A comparisson with other state-of-the-art methods. Authors show that their methods permers better on selected databases.\n4) Reference section is updated.\n\nWeaknesses:\n\nNot particularly found.",
            "summary_of_the_review": "Authors describe an architecture for deep image retrieval. For this, they use mid-level features that they call super-features (SF). They construct thes SF by an iterative attention module. They build them as an ordered set in which each element focuses on a localized yet discriminant image pattern. Auhtors show that for training, only image labels suffice. Authors present a set of experiments on common landmark retrieval benchmarks to validate that their SF substantially outperform state-of-the-art methods when using the same number of features. They also show that this requires a significantly smaller memory footprint to match their performance.\n\nMain contribution: A method called: Feature Integration-based Retrieval or FIRe for short. It can decomposed into 3 main parts:\n\n1) an image representation based on mid-level features (super-features) and an iterative module to extract them.\n2) a framework to learn such representations. It is based on a loss applied directly on SF yet only requiring image-level labels.\n3) a set of extensive evaluations that show significant performance gains over the state of the art for landmark image retrieval. \n\nSome possible works that could be added to reference section:\n\nIntellige: A User-Facing Model Explainer for Narrative Explanations\nJ Yang, D Negoescu, P Ahammad - arXiv preprint arXiv:2105.12941, 2021 - arxiv.org\n\nNext generation 3D pharmacophore modeling\nD Schaller, D Šribar, T Noonan, L Deng… - Wiley …, 2020 - Wiley Online Library\n\nAutomated cyberbullying detection in social media using an svm activated stacked convolution lstm network\nTA Buan, R Ramachandra - Proceedings of the 2020 the 4th …, 2020 - dl.acm.org\n\nAUTOMATIC MODULATION CLASSIFIER\nHA Rasool - Iraqi Journal of Information & Communications …, 2020 - ijict.edu.iq\n\nNext generation 3D pharmacophore modeling\nD Šribar, T Noonan, L Deng… - WIREs …, 2020 - refubium.fu-berlin.de\n\nSynergistic Target Combinations Against Obesity: Focus on MCHR1/H3R Modulation\nDA Schaller - 2020 - refubium.fu-berlin.de\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No comments.",
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}