{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper studies the problem of producing distribution-free prediction sets using conformal prediction that are robust to test-time adversarial perturbations of the input data. The authors point out that these perturbations could be label and covariate dependent, and hence different from covariate-shift handled in Tibshirani et al 19, the label-shift handled in Podkopaev and Ramdas 21, and the f-divergence shifts of Cauchois et al 2021. \n\nThe authors propose a relatively simple idea that has appeared in other literatures like optimization but appears to be new to the conformal literature: (i) use a smoothed (using Gaussian noise on X, and inverse Gaussian CDF) nonconformity score function, in order to control its Lipschitz constant, (ii) utilize a larger score cutoff than the standard 1-alpha quantile of calibration scores employed in conformal prediction. The observation that point (i) alone lends some robustness to adversarial perturbations of the data is interesting. As several experiments in the paper and responses to reviewers show, this comes at the (apparently necessary) price of larger prediction sets. \n\nI read through all the comments and also the supplement. The authors have responded very well to all the reviewers questions/concerns, adding significant sections to their supplement as a result. Three reviewers are convinced, but one remaining reviewer requested additional experiments to compare with Cauchois et al (in addition to all the others already produced by the authors originally and in response to reviewers). However, the authors point out that the code in the aforementioned paper was not public, but they were able to privately get the code from the authors during the rebuttal period. At this point, I recommend acceptance of the paper even without those additional experiments, since it is not the authors' fault that the original code was not public. Nevertheless, I suggest to the authors that, if possible, they could add some comparisons to the camera-ready since they now have the code.\n\nI congratulate the authors on a nice work, a very solid rebuttal, and also the astute reviewers on pointing out various aspects that could be improved. \n\nMinor point for the authors (for the camera-ready): I would like to comment on the Rebuttal point 4.4 in the supplement, which then got further discussed in the thread. The reviewer points out four references [R1-R4]. I will add one more to the list [R5] https://arxiv.org/pdf/1905.10634.pdf (Kivaranovic et al, appeared in 2019, published in 2020). I think the literature reviews in this area are starting to be messy, and all authors need to do a better job. Clearly, the original paper of Vovk et al already establishes various types of conditional validity (and calls it PAC-style guarantee), produces guarantees that others in this area produce, and it appears that much recreation of the wheel is occurring. For eg, [R2, R4] do not cite [R5], despite [R5] appearing earlier and being published earlier, and having PAC-style guarantees and experiments with neural nets, etc. However, in turn, [R5] do not cite Vovk [R1], but [R2, R4] do cite [R1]. (And [R3] does not seem to be relevant to this discussion of conditional validity?) In any case, I am not sure any of these papers need citing since the current paper does not deal with conditional validity. If at all, just one sentence like \"Conditional validity guarantees, of the styles suggested by Vovk [2012], would be an interesting avenue for future work\"."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a generic method to construct conformal prediction sets in an adversarial setting. Since standard conformal prediction method assumes an i.i.d. assumption for training and testing input, its generated prediction sets for adversarial examples will not satisfy the coverage guarantee. Build upon on randomized smoothing, the paper then proposes a new non-conformity score and raise the threshold to account for the adversarial transformations. It then proves that the prediction sets constructed by the proposed method satisfy the coverage guarantee for worst-case scenarios. Empirical evaluations are also performed on benchmark datasets, which justify the effectiveness of their method.\n",
            "main_review": "The paper considers the conformal prediction problem in an adversarial setting, which is new and important in my perspective. It is very well-written, which I enjoyed reading. The idea of using randomized smoothing to construct a robust conformity score is quite novel and theoretically sound for conformal prediction, despite the techniques it used are well-known in the field of adversarial robustness. The empirical studies clearly demonstrate the vulnerability of standard conformal prediction method in the presence of adversarial examples, which support the motivation of the paper. Nevertheless, I have the following general questions for the authors:\n\n1. In Figure 5, you show the marginal coverage against adversarially-perturbed inputs for different methods. How about the marginal coverage for normal examples? \n\n2. The method “CP + SS” is not able to satisfy the desired coverage. Is this because the base classifier is trained to be robust against Gaussian noise instead of against worst-case adversarial perturbations? I am wondering whether you could achieve the coverage by using the conformity scores outputted by an adversarially-robust classifier?\n\n3. One of the evaluation criteria for conformal prediction you used is average set size. Based on Figure 5, it seems that HPS based method outperforms APS based method by a large margin. But you mentioned in Section 2 that “APS reflect better the underlying uncertainty across sub-populations”. Do you have any empirical results supporting this? In general, how to decide which method to use if we want to deploy the conformal predictors you proposed.\n",
            "summary_of_the_review": "Overall, I think the paper is well-written and reach the acceptance bar of ICLR. Both theoretical and empirical results support the main claim of the paper. Therefore, I vote an accept for this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Quantifying predictive uncertainty is critical for various real-world applications of ML models. Post-hoc ``wrapper-style'' procedures for uncertainty quantification, which can be built on top of any black-box model and thus do not require modifications of training algorithms, are of great value due to a, typically, high number of engineering tweaks used during the model development stage. For example, (split) conformal prediction modifies an underlying point prediction model, which outputs the top-ranked label only, into a set-valued predictor that instead outputs a set of labels. Under the i.i.d. (or more generally, exchangeability) assumption, the resulting sets are provably valid (in terms of coverage) with guarantees being marginal over calibration and test data. \n\nViolation of the i.i.d. assumption invalidates the inference, and adaptations to some structured distribution shifts have been proposed recently in the literature. The current work focuses on a setting where adversarial examples might be present at the test stage. Vanilla conformal is based on considering a collection of candidate prediction sets, parameterized by a single parameter, which is tuned/calibrated using a held-out set for performing set-valued predictions on test data. The authors propose correction of vanilla conformal, which essentially boils down to inflating the threshold in a way that guarantees robustness against adversarial examples with bounded $\\ell_2$-norm (using randomized smoothing), where inflation results in outputting larger the prediction sets. ",
            "main_review": "The framework proposed in the work considers an important direction in the study of the applicability of conformal prediction in settings beyond the i.i.d. scenario. For safety-critical applications, it is important to build robust ways of quantifying uncertainty as certain failures could have disastrous consequences. The authors consider a popular setting where test data contains adversarial examples with bounded $\\ell_2$-norm. For a modified procedure to work, the non-conformity scores have to satisfy a certain property, and the authors propose a way to incorporate any ``base'' score into the framework via randomized smoothing. I was wondering whether the authors could comment a bit more on the following questions:\n\n--- Q1: While it is intuitive that getting robust prediction sets comes at the cost of larger prediction sets, I couldn't find a simulation that considers a null case. That is, a comparison of vanilla conformal against a robustified version proposed in this work in the setting where adversarial examples are not present, and thus i.i.d. assumption is sensible (i.e. vanilla conformal would have reasonable marginal coverage). Is it indeed the case that such simulation was not present in the paper? The reason is that one reasonable requirement for the prediction sets is their ``actionability'', i.e., it is hard to assess how actionable prediction sets of size 30 are in 100-classes classification problem.\n\n--- Q2: Regarding computational complexity. It is clear that at the inference stage, for constructing a prediction set, it is necessary to perform sampling of perturbed input $n_S$ times for each class (as the authors point out in the Appendix S7). Are there any specific settings (i.e., pair of model architectures, datasets) that the authors have numbers for? It is interesting how computationally feasible the framework is.\n  \n--- Q3: I was wondering whether the authors can comment a bit more on the sensitivity of the smoothed scores to using approximations obtained via sampling. A partial answer is presented in S5.2 where some figures are presented for average set sizes and coverage. But it seems that if the original non-conformity scores are bounded, then it is possible to look at the sizes of the confidence intervals for the quantities that are being approximated.\n\n--- A side note: equation 8 might contain a typo. it seems that $\\tilde X_{n+1}$ (an observed feature vector) should be stated in place of $X_{n+1}$ (an unobserved one).\n\nIn general, the work is well-written equipped with important parts of (relevant) literature review and well-designed simulation studies.",
            "summary_of_the_review": "The current work represents a solid piece of work that takes a step forward towards robust conformal prediction. Within the context of CP under distribution shifts, prior works studied settings where structured/constrained shifts are present. This work focused on a different setting which hasn't been covered in conformal literature before where at the test stage adversarial examples invalidate the vanilla CP. The reasons for lowering the score include: limited theoretical contributions, a limited study of the types of adversarial examples, unaddressed questions stated in the main review.\n\n**Update after rebuttal**\n\nI would like to thank the authors for the detailed responses. Taking into account the general contribution of this work and points mentioned in this and other reviews, I tend to keep the current score.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper generalized a data-splitting conformal prediction approach to the adversarial attack setting by combining conformal prediction with randomized smoothing, which yields a prediction set with finite sample coverage guarantee under an l2-norm bounded adversarial noise. The effectiveness of the proposed methods was demonstrated on the CIFAR10, CIFAR100, and ImageNet datasets, showing a significant increase in the coverage size compared to the vanilla conformal prediction method.",
            "main_review": "I like the proposed idea of using randomized smoothing to construct a non-conformity score that forms a prediction set with a finite-sample coverage guarantee under adversarial noise on the features. The paper is also written in a clear way which gives enough background knowledge to readers who are not familiar with the field of conformal prediction. The motivation of introducing randomized smoothing into the score function was very well illustrated, and the theorems 1 and 2 provided convincing results of the effectiveness of the proposed approach. \n\nI have one major concern of the experimental section. Since the comparison was only done with the non-robust conformal prediction approach, I am wondering if it is possible to compare to other robust CP methods such as those mentioned in Section 4.1. In particular, the authors mentioned that this work \"handles a full distributional shift induced by the adversarial perturbation\". But my understanding is that this work considers only the perturbation on the feature X while P(Y|X) remains intact. It would be helpful if the authors can clarify on this point. \n\nA minor point, there is a typo in Eq. (8), where it should be \\tilde{S}(\\tilde{X}_{n+1}, y).",
            "summary_of_the_review": "Overall I like the idea proposed in this paper. Their theoretical and experimental results demonstrated the effectiveness of the proposed approach. My major concern is about comparing with other robust conformal prediction methods in the experimental section. It would be helpful if the authors can provide insights on this.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper tackles the conformal prediction problem under adversarial perturbations, where the conventional exchangeability is violated. The proposed approach combines the non-conformity score with randomized smoothing and the standard conformal prediction. The proposed approach is evaluated over three different image benchmarks (i.e., CIFAR10, CIFAR100, and ImageNet) and demonstrated its efficacy by comparing the naive conformal prediction and the naive conformal prediction with the randomly smoothed non-conformity score. ",
            "main_review": "**Strengths**:\n1. This paper considers an under-explored problem, i.e., conformal prediction under adversarial perturbations.\n2. The paper is easy to follow.\n\n**Concerns**:\n1. (novelty on theoretical guarantee) The novelty on the theoretical result is less significant; the correctness of the main theorem is heavily dependent on the condition of a non-conformity score in (7). In particular, finding the Lipschitz constant M_\\delta is the challenging problem and actively researching area (as the paper pointed out), especially the score function is a highly non-linear neural network and an input is high-dimensional. However, given the score function that satisfies (7), constructing a conformal prediction set is relatively straightforward (even though I agree that it's a new result). \n\n2. (preciseness on theoretical guarantee) The statement of Corollary 1 needs to be described more precisely, considering the fact that the literatures of conformal prediction are trying to be rigorous on making the theoretical guarantee; Corollary 1 requires randomly smoothed score function, but we cannot evaluate this function due to the expectation without approximation (as described in the paper). In this case, it would be more appropriate to say \"the prediction set is asymptotically valid\" or consider the samples required for approximating the expectation as a part of sample complexity analysis for the proposed prediction set. I think the latter making this paper theoretically more interesting and novel. \n\n3. (comparison) The most closely related work is Cauchois et al. (2020), but the comparison is missing. The related work section points out the limitation of this work, i.e., \"the f-divergence measure is notoriously difficult to estimate in practice\"); however the proposed approach of Cauchois et al. (2020) is actually evaluated on CIFAR10 and ImageNet; in particular, the core part of this approach is convex optimization, so could be computationally not expensive. Finally, the results of Cauchois et al. (2020) is similar to the results of this paper, so the comparison looks necessary.\n\n4. (CP+SS v.s. RSCP) I think it's not easy to say that RSCP is better than CP+SS; I understand that RSCP satisfies the coverage criterion, but that criterion is satisfied only for the next single example in a usual setup---the probability in (1) is taken over all calibration examples and (X_{n+1} , Y_{n+1}). If the constructed prediction set is conditioned on one fixed calibration set and evaluated over multiple testing examples, the empirical coverage probability of the standard conformal prediction is around the nominal level \\alpha (see Figure 2 in Tibshirani et al. (2019)). In this sense, it's unclear whether RSCP is better than CP+SS. If the paper wants to claim that RSCP is better than CP+SS, I think PAC-style prediction set definition is required (e.g., [R1], [R2], [R3], or [R4]), which construct a prediction set conditioned on a fixed calibration set for correctness guarantee. \n\n* [R1] https://arxiv.org/abs/1209.2673\n* [R2] https://arxiv.org/abs/2001.00106\n* [R3] https://arxiv.org/abs/2101.02703\n* [R4] https://arxiv.org/abs/2106.09848\n\n\n5. Other comments\n- The description on conformal prediction is not precise; the paper said it requires i.i.d. but exchangeability is enough. Moreover, it can be applied to regression.\n- Related to the above, is the i.i.d. assumption in Theorem 1 necessary? Is the proposed approach only applicable to classification? \n- \"however, none of them addresses the adversarial setting specifically\" and \"Both approaches differ from ours, which handles a full distributional shift induced by the adversarial perturbation\": I think they are strong or incorrect since the adversarial setting is a special case of covariate shifts. \n\n",
            "summary_of_the_review": "Overall, this paper handles an interesting and timely problem, but the theoretical and empirical results are less significant compared to the known literatures as described in the main review; I lean to reject, but willing to discuss to adjust my understanding.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}