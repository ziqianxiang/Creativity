{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper introduces Noisy Feature Mixup: an extension of input mixup and manifold mixup to all layers of a neural net, for the purpose of improving robustness and generalization in supervised learning. Experimental validation supports the increased robustness to attacks on the input data. The reviewers find the paper well written and they appreciate the theoretical analysis as well as the empirical results. The reviewers did not identify any big problems, and their minor concerns were sufficiently addressed in the author reponse. I'm therefore happy to recommend accepting this paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a new and inexpensive mixup method named Noisy Feature Mixup (NFM) to mitigate over-fitting and improve generalization. NFM combines mixup and noise injection, which inherits the benefits of these methods. Due to its conciseness, it is convenient to apply NFM in model training. More importantly, the authors prove NFM's regularization effect on model optimization and robustness improvement with mathematical derivation. They build the implicit connection between the NFM empirical loss and the original loss. Then they identify the regularizing effects of the proposed NFM. To demonstrate that NFM helps robustness, the authors relate the NFM loss to the one used for adversarial training, which can be regarded as an example belonging to distributionally robust optimization. With integrated experiments, NFM shows its superiority on various models and datasets. The further discussion shows the interesting tradeoff between predictive accuracy on clean and perturbed test sets. The Supplementary Material provides detailed proof and additional experimental results to show NFM's firmed ground.",
            "main_review": "Theorem 1 is one of the most important contributions of this paper. It shows that minimizing the proposed NFM loss is equivalent to minimizing the sum of the original loss and feature-dependent regularizers. The form of the regularizers is very complicated. I have checked part of them and seems to be correct. Too much work to follow the rest part. This theorem shows that the regularizers actually reduce the Jacobians and Hessians, which is reasonable and fit everyone's perception. In my opinion, although there is no surprising conclusion, this contribution provides evidence for everyone’s understanding of the mixup type methods.\n\nThe experiment part provides the necessary experiments to support the argument. But I am curious about the different behaviours of the proposed methods for features of different levels. According to conjecture, when the feature level is different, the meaning of adding noise and mixup may become completely different. It seems that this part of the discussion is missing.",
            "summary_of_the_review": "It is difficult to rate this paper. Theorem 1 has theoretical value. The proposed method is relatively simple (both pros and cons). But I would vote positively for theorem 1.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies data augmentation methods for improving the robustness of supervised learning. The main contribution is presenting Noisy Feature Mixup, extending input mixup and manifold mixup to all layers of a neural net. The experimental results show that the proposed approach improves the robustness of supervised learning under several noise attacks on the input data set. The theoretical results derive the Taylor's expansion of the Noisy Feature Mixup optimization objective.",
            "main_review": "## Strengths:\n\n- The main contribution of this paper is extending input mixup (Zhang et al. (ICLR'18)) and manifold mixup (Varma et al. (ICML'19)) to Noisy Feature Mixup, which applies mixup randomly to *all layers of the neural network* and *adding random noise to the mixup examples*.\n- Through this extension, the authors find that Noisy Feature Mixup improves the robustness to several kinds of noise models comp\u0010ared to previous mixup variants.\n- Additionally, a theoretical analysis of the regularization effects of Noisy Feature Mixup is presented, building on top of the results by Zhang et al. (ICLR'21).\n\n**Related works**\n\nThe related works are comprehensive in this area.\n\n## Weakness:\n\nThe main weakness of this paper is the accessibility of the technical contribution. Thus, I think several revisions towards mitigating this weakness could improve the presentation, including, for example\n\n- Illustrative examples and figures to convey Theorem 1 and Theorem 2, for example, similar to Figure 2 in Zhang et al. (ICLR'2021).\n- The proofs are in line with recent works including Zhang et al. (ICLR'2021) which seem correct to me. The main idea is using the Taylor's expansion to the Noisy Feature Mixup objective. Then, the authors note that the random noise injection can be simplified on top of the regularization terms from Zhang et al. (ICLR'2021). However, the steps are difficult to verify; in particular, more detailed derivations should be included in the appendix.\n    - For example, above equation (30), it's stated that \"we compute, using linearity and chain rule.\" However, it's not clear to me how this is computed.\n    - Then after equation (33), it's stated that \"The equation in the theorem follows upon setting...\" It's not immediately clear to me how this step is carried out either.\n    - The rest of the appendix has similar issues as above. They should be expanded with more clear explanations and derivations.\n\n**Missing ablation studies**\n\n- It's unclear why the authors set the hyper parameters of the additive noise $\\sigma_{add}=0.4$ and the multiplicative noise $\\sigma_{mult}=0.2$. This should be justified with an ablation study that varies each hyper parameter.\n- Since there are two components in the proposed approach (all-layer mixup and random noise injection), It's also unclear which part is contributing more or less. Again there should be an ablation study to justify the role of each component in the proposed approach.\n\n**Missing baselines**\n\n- The authors mainly consider Mixup and Manifold Mixup as baselines in the experiments. This is marginally acceptable. To make the experiments stronger, I think the authors could consider other related methods for improving robustness such as *label smoothing*.",
            "summary_of_the_review": "This is a technically solid and empirically strong paper. The theoretical results follow from recent works for mixup (Zhang et al. (ICLR'21)). The experimental results are conducted on three popular image datasets, with three input perturbation models, and do support the validity of the proposed approach. The main weakness of this paper is the accessibility of the technical contribution and the lack of ablation studies. I think several revisions could help mitigate these weakness.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes noise added version of the manifold mixup. The authors verify the theoretical properties of the method in terms of regularization and robustness. The paper verifies the effectiveness of the proposed method on image classification tasks with noisy test environments. ",
            "main_review": "**Pros**\n- The paper is easy to read and the writing is very clear. \n- The paper provides thorough theoretical analyses, which the original manifold mixup paper lacks. \n- The proposed method is effective in noisy test environments with various models and datasets. \n\n**Cons**\n- The performance gain under the clean setting is marginal. \n- The experimental setting is limited. It will be valuable to evaluate methods in more general and natural corruption settings. (please refer https://github.com/hendrycks/robustness)\n- Baselines in experiments are not sufficient. It will be informative to compare some recent mixup techniques enhancing robustness (e.g., Puzzle Mix [1] performs mixup-aware adversarial training, and AugMix [2] enhances corruption robustness). \n\n**Additional comments**   \n- Does the theory can be generalized to the more general perturbations (such as natural data augmentation)? \n- For me, the statement in Theorem 2 is counter-intuitive (L^NFM upper bounds the worst-case). It will be informative to add a more intuitive explanation for the theorem in the paper. (e.g., when the statement is valid). \n- In appendix Figure 10, it is hard to interpret which one is better. If possible, the formal evaluation of the features will be also interesting. \n\n[1] Kim et. al, Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup, ICML 2020.  \n[2] Hendrycks et. al, AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty, ICLR 2020\n\n**Post rebuttal**   \nI appreciate the authors' rebuttal and believe the revision improves the paper. However, the robustness experiments are still not convincing and require stronger baselines to be convincing (e.g., AugMix or other methods with stronger augmentations). Overall, I believe this is a clear paper and I maintain my score. ",
            "summary_of_the_review": "This paper is well-written and provides thorough theoretical properties. However, the experiment settings are a bit limited and require more baselines to be compared. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new method for data augmentation, named Noisy Feature Mixup (NFM). This method combines the advantages of both the interpolation based training and noise injection schemes. In particular, this method is simple and easy to implement. Empirically, this paper shows that NFM achieves a favorable trade-off between the test accuracy on the clean set and the model robustness. Theoretically, it is shown that NFM enables smoother decision boundary, and amplifies the regularization effects of manifold mixup and noise injection. Additional theoretical analysis also shows that NFM training is approximately minimizing an upper bound related to the adversarial loss, thereby leading to more robust model.",
            "main_review": "Overall this paper is well written and easy to follow. The strengths of this paper include:\n1. This paper has a strong motivation, i.e., exploits the benefits of mixup and noise injection to further improve the generalization ability of a trained model.\n2. The proposed method, i.e., Noisy Feature Mixup (NFM), is simple in principle and easy to implement in practice.\n3. Theoretical analyses are offered to illustrate the underlying mechanisms of NFM. The theorems in the paper are built with reasonable assumptions, and the conclusions are sound and can explain the practical behaviors of NFM to some degree. Overall I find the theorems offer a good insight for NFM.\n4. The proposed NFM is evaluated on different datasets, with two representative neural network architectures. The experimental designs are reasonable, and the results can well demonstrate the advantages of NFM over different Mixup variants.\n\nFor this paper, I have the following concerns/questions:\n1. In NFM, the noisy mixup is performed on a random layer $k \\in [L]$ in each iteration. But is it really necessary to apply NFM to every layer? In other words, is it possible that for some layers, the noisy mixup would lead to negative (or neutral) effects and hurt the performance?\n2. From the experimental results, it is unclear if the stochasticity introduced by NFM during training would lead to a slow convergence of model training. Is it possible that such stochasticity would make the model difficult to converge, or require more time for the model to converge?\n3. In Theorem 2, it is assumed that $E_{r \\sim D_x}[g_k(r)] = 0$. However, I find this assumption could be too strong in practice.",
            "summary_of_the_review": "The problem this paper focuses on is important, i.e., improve the generalization performance of deep models. To this end, this paper proposes a new method that exploits the benefits of mixup and noise injection, and is empirically demonstrated to be more effective at improving the accuracy and robustness of trained models. The proposed method is simple and easy to implement. Notably, it has sound theoretical analyses to illustrate the underlying mechanisms. Although for this paper there are still some points which are unclear, currently I think it is overall a good paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}