{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes a framework for learning disentangled representations of content and style in an unsupervised way, using a permutation invariant network. It adopts VQ network for content encoding, and Cross-Attention for Style and Linking Attention at decoder. It is shown to be domain agonistic, working well in image and audio domain. Experiments are conducted on speech and image datasets.\n\nThe paper is recommended as an accept (weak) to ICLR. The reviewers have given detailed feedback and suggestions -- please address them in the next revision of the paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a novel idea of disentangling the content and style of structured data by treating style as permutation invariant information. It adopts VQ network for content encoding, and Cross-Attention for Style and Linking Attention at decoder. It is shown to be domain agonistic - worked well in image and audio domain.",
            "main_review": "Interesting idea of content and style disentanglement; validated on both image and audio domain.\n# Strength:\n1.\tThe paper proposes a new way to define style (for a given dataset) using permutation invariant (P.I) network. This is a good attempt to have a common definition of style.\n2.\tThe network architecture is novel although using existing components such as VQ network and attention network. The idea of disentangling and link attention are what make the paper interesting.\n3.\tThe model is domain-agonistic – demonstrated to work well in audio and image domain\n4.\tThe paper demonstrates various downstream task in each domain. The part style transfer is impressive although there are some artifacts. Nice that the authors provided audio sample to play in supplementary file.\n\n# Weakness/Suggestion/Queries \n\nThe paper proposes to separate out style representations. But it lacks visualization of how actually each individual style visually look like. It may be that these styles are actually relates to attribute of the dataset which for some datasets (e.g. DeepFashion, CUB birds) already defined. It would be interesting to cluster these style features and see the discovered styles from a given dataset.\n\nOverall, there seems to a lot of hyperparameters to tune and may be that one must have good understanding of dataset for proper hyperparameter settings. Details as follows:\n\n1.\tHow is $r_{\\theta}$ implemented? It seems this is the linking attention network. Could you please include some discussion with guiding equations at least in the supmat.\n2.\tContent Encoder: It is stated  the spatially adjacent tokens are forced to share the same VQ code. If so, how does this work when there are two different objects (parts) in the adjacent tokens (adjacent patches in images)?\n3.\t$\\mathcal{Z}_o \\in \\mathbb{R}^{m \\times d_s} $: What is a suitable value for m? Does the value of m relates to the number of styles the network can represent?\n4. The total loss is sum of three loss terms with $\\lambda$ weighting parameters. How sensitive is the model (training/convergence) to the hyper-parameters $\\lambda$ s? If the choice of these parameters slightly off, do the network perform still reasonably?\nAny training strategy used to deal with discrepancies brought by these parameters? How do you train the network in different stages e.g., adding more loss terms in subsequent stage?\n\n5.\tHow do you come up with G=2 and V=100 VQ is sufficient to capture all content information for audio dataset? Any heuristics for this, or perform rigorous parameter search?\n6.\tDo you do manual observation to associate content codes with parts in image domain? It is stated that V is dependent on image datasets. Does this mean that you set V to the number of parts you want to segment out from images?\n7. It is not clear how the authors generate: (a) Codebooks for content (b) Prototype vectors for style?\n\n",
            "summary_of_the_review": "The idea is interesting and seems novel, and the method is domain agonistic that may be applied to other domains. Experiement conducted are convincing, I am leaning to accept the paper, provide the minor concerns raised are resolved.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a framework for learning disentangled representations of content and style in an unsupervised way. The paper presents a slightly uncommon definition of content and style, which serves as a foundation for the proposed methodology and experiments. Experiments are conducted on speech and image datasets. ",
            "main_review": "*Strengths*\n- S1. The proposed method is multimodal and can serve multiple modalities with a broad definition of style and content. The paper provides results on speech and vision data.\n- S2. The proposed method is unsupervised, so it does not need annotation on style/content for the training data. This is very useful, as the boundaries on style and content are not straightforward and challenging to annotate.\n- S3. The paper is clear and easy to read. \n- S4. Experiments are technically sounds.\n\n*Weaknesses*\n- W1. The definition of style as \"the information that is not affected by the permutation of tokens\" is a bit counterintuitive — no justification or examples of why the paper chooses this, and no other definitions are provided. \n- W2. The definition used in the paper does not correspond to the traditional definition of style and content in computer vision, which has been (and still is) a widely studied problem. This creates some confusion, especially in the image domain section.\n- W3. After multiple examples referring to text as a potential target modality for this framework, no experiments are conducted on NLP datasets.\n- W4. Additionally, for the image domain experiments, it would have been interesting to see results on art datasets, a field in which the problem of style and content disentanglement has been severally studied.\n- W5. In the image domain experiments, there is no comparison to other methods other than for the proxy task of landmark regression. As visual results on style transfer are provided, it is expected to compare against other methods to see the potential of the proposed model against current technology.",
            "summary_of_the_review": "The idea of the paper is interesting and potentially useful for the community. However, the experimental results seem limited for now.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes an unsupervised framework to learn content and style representations of structured data, such as texts, images and speeches. The task is interesting to me and decomposition representation learning is important. In addition, a bipartite graph is proposed to model the link between content tokens and style tokens, making the model more interpretable. The proposed model performs relatively well on both speech and image domains.",
            "main_review": "Strengths:\n\nS1: This paper addresses an important problem -- content and style decomposition representation learning and proposes an unsupervised framework to learn content-style representations.\n\nS2: The proposed model performs relatively well on both speech and image domains.\n\nWeaknesses:\n\nW1: It is difficult to follow the paper some notations lack details. For example, what are the prototypes and link keys in Figure 3 and how to obtain them? For speech and image domains, how to obtain the input of the model?\n\nW2: Lack of ablation study on hyper-parameters in Eq. (1). What are the roles of the three items in Eq(1)?\n\nW3: It is unclear how the model avoids or mitigates information leaking. It seems that a perfect model should avoid the style leak while achieving perfect reconstruction (shown in Figure 2 (c)).\n",
            "summary_of_the_review": "This paper addresses an important problem and proposed a framework to learn content and style representations. The experiments show that the performance of the proposed model is relatively well. But some details are missing, so it is difficult to follow the paper and it is unclear the roles of the loss functions and how the model avoids information leaking.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}