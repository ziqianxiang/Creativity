{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper considers the empirical distribution of layer/channel in CNN ,and proposes to use global null tests with Simes and Fisher statistics to aggregate the p-values. This method is competitive while computationally efficient. The underlying theoretical insights are discussed in detail.\n\nThe paper received mixed ratings, and the discussions weren't active. So, AC carefully read the paper and inspected all reviews. Reviewer a8KZ comments were factually inaccurate in listing references, and lack substantial feedback on the actual content of the paper. Hence, the review was down-weighted. \n\nThe other negative reviewer Ni17, as an OoD expert, unfortunately did not offer more feedback to author rebuttals. From what AC comprehends, the authors should have clarified their The theoretical guarantee and compared properly with Liu et. al. 2020 energy-score (ES).\n\nConsidering the above, AC feels that the study deserves to be published."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors formalize the out-of-distribution (OOD) detection problem in the framework of statistical hypothesis testing. The proposed method, called Max-Simes-Fisher (MaSF), is designed for deep neural networks and combines sample features extracted from multiple layers in the network. The method returns a $p$-value for every test sample at a specified significance level (_i.e._, probability of incorrectly predicting \"OOD\" for an actual in-distribution sample). Compared with existing methods for OOD detection, the proposed method achieves similar or better True Positive Rates (TPR, _i.e._, correct OOD prediction rate) with significantly lower computation cost.",
            "main_review": "### Strengths\n\nS1) The paper frames the out-of-distribution (OOD) detection problem in the framework of hypothesis testing, which more directly links the problem to existing statistical tools and methods.\n\nS2) The paper proposes a new, computationally efficient method (MaSF) for performing hypothesis testing which uses multiple layers from a deep neural network. The authors report many empirical experiments which supports their method.\n\nS3) The paper is generally well-written.\n\n\n### Weaknesses\n\n**W1) Lack of comparison / discussion about \"calibration\" literature**\n\nI am admittedly not very familiar with the OOD detection literature. However, the motivation for OOD detection, as described in the paper's introduction, is that deep neural networks output prediction scores that tend to be overconfident. This sounds like a problem of \"calibration,\" which the paper does not mention. For example, a canonical paper on calibration of deep neural networks is \"[On Calibration of Modern Neural Networks](http://proceedings.mlr.press/v70/guo17a.html)\" by Guo et al. 2017. I would appreciate a discussion of how OOD detection is related too and/or different from calibration techniques, given that they are motivated by the same underlying problem of overconfident neural network predictions.\n\n**W2) Applicability to regression settings?**\n\nThe method described seems to be specifically designed for classification problems. Is there a similar formulation for regression problems? If not, I think it should be emphasized (maybe even in the paper title, e.g., \"A Statistical Framework for Efficient Out of Distribution Detection in Deep Neural Network Classifiers\") that the method is specific for classification.\n\n**W3) Needs more background / justification for Fisher and Simes tests**\n\nPerhaps this is due to my unfamiliarity with the Fisher and Simes tests, but I found the paper lacking in justification for why these were the two tests that authors use. What about other spatial reduction and channel reduction functions, such as simple mean/max reductions? What if you used a neural network for the reduction functions?\n\nOne way that the authors could help with this issue is to describe the Fisher and Simes tests in the body of the main text, instead of leaving it for the appendix.\n\n**W4) Implications for designing neural networks?**\n\nAre there any implications of the OOD detection method for designing better neural networks that are able to generalize better? For example, does this method present any natural mechanism for better calibrating neural network layers?\n\n**W5) Other clarity issues**\n\n- In Section 1.1 (Contributions), the definition of a Type I Error seems wrong, or at least poorly worded. The paper says that a Type I Error is _detecting OOD as in-distribution_, which to me is confusing. A better wording could be, for example, _incorrectly predicting \"OOD\" for an actual in-distribution sample_.\n- In Section 3.1 (Preliminaries), the definition for $N_{val}$ seems to be a typo. I think it should say $N_{train}$.\n- Why is the global null hypothesis denoted with an intersection $\\cap$ symbol? Is each $H_{0,i}$ a set? And if so, wouldn't it be a union instead of an intersection? Presumably this is notation that is standard in the literature, so pardon my ignorance. In any case, I think it would be helpful to clarify the notation in the text.",
            "summary_of_the_review": "I preface this summary with an admission that I am not the most familiar with the OOD detection literature and global null hypothesis testing. However, I believe that formulation of OOD detection as a hypothesis testing problem is well thought out and beneficial for the ML community by rooting the problem in a concrete statistical framework. Furthermore, the proposed method seems sound (I skimmed through the math, albeit not thoroughly) and has significant speed-ups compared to existing methods, such that the method is far more likely to be deployed in real-world applications and edge devices. For these reasons, I believe this to be a valuable contribution to the ML community.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper poses detecting out of distribution (OOD) samples as a hypothesis testing problem. Specifically, the authors propose a variant of the inductive conformal prediction framework where a 'p'-value is computed as a measure for OOD-ness. Finally, the OOD samples are detected by thresholding the 'p'-values.",
            "main_review": "Strengths:\n\n1. Considering OOD detection as a hypothesis test is an interesting formulation.\n\n2. Proposed approach considers an 'Observer'-style approach that does not require updating the underlying model. This is more efficient and easily accessible.\n\n3. Proposed framework achieves high efficiency while detecting OOD samples and can be deployed in real-time setups.\n\nWeaknesses:\n\n1. Formulating OOD detecting as a hypothesis test is previously explored in the following works. What are the novel contributions compared to these works. Authors are encouraged to add these in the reference section.\n\na. Haroush et al., \"Statistical Testing for Efficient Out of Distribution Detection in Deep Neural Networks\"\n\nb. Cai & Koutsoukos, \"Real-time Out-of-distribution Detection in Learning-Enabled Cyber-Physical Systems\" \n\nc. Bates et al., \"Testing for Outliers with Conformal p-values\"\n\n2. Experiments are limited in terms of comparisons with the recent state-of-the-art approaches and evaluating on benchmark datasets. Here are a few recent approaches on OOD detection and authors are encouraged to compare with these approaches.\n\na. Hendrycks et al., \"Deep anomaly detection with outlier exposure\"\n\nb. Liu et al., \"Energy-based out-of-distribution detection\"\n\nc. Lee eta al., \"Training confidence-calibrated classifiers for detecting out-of-distribution samples\"\n",
            "summary_of_the_review": "The approach lacks novelty and experiments are limited to justify the efficacy of the proposed approach. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper constructs OOD detection as a hypothesis testing problem and extracts p-values from each layer and channel in CNN from the empirical cumulative distribution. Max, Simes, and Fisher test statistics are adopted as spatial, channel, and layer reduction methods (MaSF). Empirical results show that MaSF achieves comparable or better results than GRAM (Sastry and Oore, 2019), ResFlow (Zisselman and Tamar, 2020), and Mahalanobis (Lee et al., 2018) while significantly reducing the computational cost.",
            "main_review": "Strengths:\n\n(1) Different from previous work, the paper considers the empirical distribution of each layer and channel in CNN and proposes to use global null tests with Simes and Fisher statistics to aggregate the p-values.\n\n(2) The proposed method is more computationally efficient than compared baselines while preserving comparable performances.\n\n(3) The effect of the proposed method on different layers and channels is discussed in detail. \n\n\nWeaknesses:\n\n(1) Proposition 1 and 2 are obtained by the underlying cumulative distribution without considering the gap between the estimated cumulative distribution and the real one. Equation (8) in the proof of proposition 1 is true only when q extracted is from the real distribution. It seems like it is considered as the real distribution in the following analysis, while the experiments consist of the estimated cumulative functions and corresponding p-values.\n\n(2) The proposed method uses p-values from every layer and channel in CNN, but intuition and arguments of Fisher and Simes tests as layer and channel reduction methods are not fully presented. As the core of the proposed algorithm,  it would be better to include the background of Fisher/Simes tests and why they are effective as layer and channel reduction respectively.  The current draft only includes a brief introduction with no discussion in the appendix.\n\n(3) It is great to see the computational cost is much lower than baselines such as Gram and Mahalanobis, but some baselines do not require retrain or expensive computation (e.g. Energy score (Liu et al., 2020)) are missing. It would be interesting to see the comparative performance, both computation-wise and detection accuracy-wise. \n\nSuggestions for improvement\n\n(1) The notation of t and T can be pre-defined before introducing equation (4) (rather than presented in the appendix) to facilitate better understanding.\n\n(2) Move the introduction of Fisher and Simes to the main paper.\n\n(3) Given the large OOD uncertainty space, it's desirable to also include more evaluation datasets as used in literature. \n\n(4) Add more recent baselines, especially the ones that are already computationally efficient. ",
            "summary_of_the_review": "Hypothesis testing using the empirical cumulative distribution is not novel enough as a \"new framework\": existing works already include such spirit of it by using FPR95 of the in-distribution samples. The theoretical part is also not strong enough without considering the gap between the true and estimated cumulative distribution. \n\nThe proposed method uses p-values from every layer and channel in CNN, but intuition and arguments of Fisher and Simes tests as layer and channel reduction methods are not fully presented. Lastly, the experimental evaluations can be further strengthened (by adding more recent baselines). ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes a new framework to detect out of distribution samples for deep neural networks without retraining or modifying the model. The method is based on statistical hypothesis testing which is applied to activations of all or multiple layers. \n",
            "main_review": "Their framework is interesting and solves a relevant problem. It also seems to work well as, based on results, it outperforms baseline methods in several cases being also much more computationally light. \n\nHowever, a problem of the manuscript is that it quite extensively uses appendices as extra space (the whole paper is 28 pages long). Their framework is not completely described or cannot be understood if appendices are left out. I agree that improving this is a very hard task, but e.g. moving Algorithm listing (page 21) to the main body would already provide better understanding of the framework. \n\nSpecific comments:\n\n- There are few instances in which authors are used statistical terminology that are unclear for wider audience. For instance, \".. maintaining the Type 1 Error ..\" gives impression that the aim is to keep T1E error level in certain level. \n\n- I might have missed this point, but how was the threshold for p value for final decision (Algorithm 1 output) chosen?\n\n- page 3, \"... the p-value is given by $q(t_{obs})=P(...)$ ...\", this would be less confusing if it would be mentioned that p-values are denoted by q\n\n- Eq (1): I is not explained, I guess it's indicator function\n\n- Section 3.2: the procedure of obtaining p-values was not unclear based in this description (as I do not have too background in hypothesis testing). Perhaps the content of footnote 3 and/or Algorithm description from appendix could be moved to this section? \n\n- Could \"Related work\" be earlier? The baseline methods are now described here, but already used in the previous section. \n\n- Second paragraph of discussion mentions that OoD detector cannot be constructed without any assumptions regarding the test distribution. I am bit puzzled that how this is reflecting to the proposed framework. Is it that there is assumption that training data for the framework comes from the same distribution as the testing and results are more or less arbitrary if not? But, in the experiments, the OOD detector was trained using the same training data as CNN (i.e. in-distribution data?) without seeing any out-of-distribution samples?\n\n- page 13, \"F convlutional neural network\"\n\n- page 18, figure 3: what red and blue colors represent? Font is also quite small\n\n- page 21, Algorithm 1, the second last line: $T^L$ is taking $t_1^c$s as input, but based on (6), the input of $T^L$ is p-values. Algorithm is missing t->q calculation in this point?\n",
            "summary_of_the_review": "As the framework is novel and results shows good performance both in accuracy and computational complexity, I feel that the study deserves to be published. But the length might be issue.\n\nI chose \"marginally above the acceptance threshold\" as my recommendation due to the length issue. If there would not be any limits, my recommendation would be \"accept, good paper\".",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}