{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The SAC wrote a very good meta review and I just copy and paste it here. I completely agree with the SAC that the contribution of the paper due to the similarity to MME and MCD. Hopefully adding data augmentation to MCD and providing empirical results on new tasks can shed some lights to the community.\n\n--------------------\nBased on a request from the authors, the SAC read the paper and the reviews and engaged two additional expert reviewers to provide an additional assessment.\n\nThe paper addresses semi-supervised learning (SSL) and makes two contributions: 1) a method called χ-model, which combines data augmentation with a two-headed network that has an adversarial loss between the heads and the feature backbone; 2) an empirical evaluation on classification and regression SSL tasks.\n\nAs pointed out by reviewer 6Dgh, the proposed model is very similar to two existing methods: MME and MCD. These related works were not mentioned in the submission. During the rebuttal the authors compared to MME but not MCD.\n\nSimilarity to MME: The core idea in MME (Min-Max Entropy) is to introduce a min-max game between the head and backbone to regularize the model. The authors point out that one difference is that X-model uses two heads and regression loss, while MME uses a single head and entropy loss. They also have other components like data consistency regularization, which are borrowed from prior work (FixMatch). They further point out that MME was evaluated and motivated for unsupervised domain adaptation whereas their paper evaluates on the same distribution (regular SSL). In general, there is a lot of overlap and borrowed techniques between UDA and SSL methods, but the experimental benchmarks tend to be different.\n\nSimilarity to MCD: A larger concern is that the proposed method is actually more similar to MCD than MME. In Eq.5, the data is augmented into two ways and two different output heads are applied. If the data is not augmented this way, the formulation is the same as MCD. The authors did not discuss this point, even though reviewers pointed out the similarity. From the perspective of the technical approach, the novelty wrt MCD appears to be mainly in adding the data augmentation. They do show in the paper that this type of min-max regularization is effective in various regression tasks, which is a good empirical contribution. In the ablation studies of the data augmentation part, the difference over MCD is not very large, but the augmentation provides good gains in some experiments (although MCD is not mentioned, in Table 2 and 5, the \"χ-model (w/o data aug.)\" is likely essentially MCD.)\n\nOverall, the strength of the paper appears to be in adding data augmentation to MCD and providing empirical results on new tasks showing that it works on same-distribution test sets and on regression tasks. The technical contribution seems somewhat limited, if accepted, the paper should include a clear discussion w.r.t MCD in the method and experiment sections. Furthermore, some baselines may potentially be missing, e.g. an MDD-based (similar to MCD) UDA method for regression can be used as a baseline (Regressive Domain Adaptation for Unsupervised Keypoint Detection, CVPR'21)."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "\nThe paper focuses on reducing data labeling efforts by improving data efficiency. In contrast to most existing approaches that address this problem only in the classification setup, the paper focuses on both classification and regression set ups. The proposed method primarily is built on leveraging invariance to data stochasticity and model stochasticity. Experiments are conducted on various tasks like age estimation, key point localization, and object recognition. ",
            "main_review": "Strengths\n\n* The paper is meticulously written, well structured, and tackles the really interesting and challenging problem learning from\n scarcely labeled data.\n* The paper has good flow, it builds from the existing literature, and first points out the drawbacks of the existing methods,\n which then motivates the proposed X-model to tackle those issues.\n* Extensive experiments on both classification and regression tasks show the ability of the proposed method’s ability to exploit\n strong data and model stochasticity, resulting in enhanced performance. It is shown that the proposed method is able to outperform all the methods compared in the paper.\n\nConcerns\n\n* One of the major concerns is regarding the technical aspect of the proposed approach. It seems to be very similar to another\n method proposed for semi-supervised domain adaptation [1], which is also proposed to improve the data efficiency of the model. While they don’t explore a task beyond classification and the problem beyond domain adaptation, the overall approach seems to be\n very similar. Furthermore, the overall idea to enhance model stochasticity has a lot of similarities with [1], [2], [3], [4] which are not mentioned in the paper at all. Hence, it is essential to have the method proposed in [1] for comparison and needs to have a discussion on the differences between the proposed method and [1]. Another experiment to consider would be Mean-Teacher + [1] which would be almost the same as the proposed idea.\n* The paper does provide an analysis with the UDA method, not sure why semi-supervised DA methods are not considered as it is\n more similar to the proposed problem setup than UDA, having such comparison would strengthen the claims provided in the experimental section. \n\n[1] Saito, Kuniaki, et al. \"Semi-supervised domain adaptation via minimax entropy.\" Proceedings of the IEEE/CVF\n International Conference on Computer Vision. 2019.\n\n[2] Saito, Kuniaki, et al. \"Adversarial Dropout Regularization.\" International Conference on Learning Representations.\n 2018.\n\n[3] Saito, Kuniaki, et al. \"Maximum classifier discrepancy for unsupervised domain adaptation.\" Proceedings\n of the IEEE conference on computer vision and pattern recognition. 2018.\n\n[4] Lee, Seungmin, et al. \"Drop to adapt: Learning discriminative features for unsupervised domain adaptation.\"\n Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.\n\n\n",
            "summary_of_the_review": "My concerns are primarily due to similarities to some of the works in semi-supervised DA techniques. Considering that the paper addresses the problem in a more general setting of classification and regression, I am leaning towards accept for now. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes the so called Chi-model, which combines data augmentation with a two-headed network architecture to include model stochasticity. It specifically addresses data efficient learning in regression settings.",
            "main_review": "## Strong points\nGood performance on a wide range of benchmark datasets.\nEasy to read, experiments clearly described.\nApproach is fit for regression as well.\n\n\n## Weak points\nTwo heads which i assume do not share weights are similar to an ensemble and can hence be compared with ensembling based approaches. Even more if comparing with a MC Dropout based ensemble, where the dropout might be applied in the final layers only, hence effectively creating an ensemble with a backbone with shared weights. [4]\nTo my understanding the proposed approach is the same as if applying random data augmentations and passing through a DNN with dropout sampling twice. Training one model on the label provided by another model is known as co-training [1] and has been applied for Semi-supervised learning (SSL) also recently [2] e.g. on CIFAR100 results, especially w/o data augmentation would aid in judging the contribution of individual components of the proposed approach.\nThere is a huge amount of SSL literature that would be needed to be taken into account, on guide could be [3] - please check for more recent ones as well.\n\n\n## Questions\nDo both heads of the chi-model have the same architecture and do not share weights?\nAre the heads only one-layer or multi-layered architectures?\n\n\n## Minor comments\nEvaluations w/ and w/o data augmentation especially when comparing with other methods would be helpful. At least stating which methods use the same data augmentations.\n\n[1] Blum, Avrim, and Tom Mitchell. \"Combining labeled and unlabeled data with co-training.\" Proceedings of the eleventh annual conference on Computational learning theory. 1998.  \n[2] Qiao, Siyuan, et al. \"Deep co-training for semi-supervised image recognition.\" Proceedings of the european conference on computer vision (eccv). 2018.  \n[3] Zhu, Xiaojin Jerry. \"Semi-supervised learning literature survey.\" (2005).  \n[4] Gal, Yarin, and Zoubin Ghahramani. \"Dropout as a bayesian approximation: Representing model uncertainty in deep learning.\" international conference on machine learning. PMLR, 2016.",
            "summary_of_the_review": "Approach seems promising, especially the empirical results. I am lacking a bit of information to set this into relation to previous works as outlined above. Without these details it is hard to judge how much can be attributed to the proposed approach. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents a g data-efficient approach that encourages invariance to both data and model stochasticity that works for both classification and regression tasks. Furthermore, the proposed minimax loss function can specifically enhance invariance to model stochasticity. The extensive experimental results verify that the proposed method is effective.",
            "main_review": "Strengths:\n*  Data-efficient regression is a very interesting direction. Most previous works on data-efficient methods focus only on classification setups. For example, pseudo-labels, which is proven to be one of the most effective methods for semi-supervised classification, cannot be exactly defined in a deep regression problem. This paper sheds some light on this new direction.\n* The proposed method seems to be simple but effective. \n\nWeaknesses:\n* One missing related work on pseudo labels [1]. It would be great to include some discussion or even add as a baseline in the classification experiments.\n* It is unclear whether the proposed method can generalize well to more challenging tasks (for example, depth estimation or larger dataset). Though, I'm satisfied with the current evaluation (from a single-value toy dataset to dense-value prediction datasets).\n\n[1] Pham, Hieu, et al. \"Meta pseudo labels.\" CVPR 2021.\n",
            "summary_of_the_review": "In general, the proposed method is well-motivated with sufficient experimental comparisons. Most importantly, the paper seems to be the first step toward data-efficient regression leveraging strong stochasticities from both data and model. To this end, I believe this paper has enough merit to be accepted.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}