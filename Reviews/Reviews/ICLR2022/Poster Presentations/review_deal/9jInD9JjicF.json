{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This work reduces the time and memory complexity of Transformer for long sequences by using multiscale pooling to reduce attention from quadratic to linear complexity. Theoretical and experimental results show good results and are very competitive with the state-of-the-art. The paper is well written and experiments are thorough. The additional results in the rebuttal also helped reduce some of the reviewers' concerns. Though the work is somewhat incremental and the experimental settings for the baselines are different, the thoroughness of the experiments and the good results make this a good addition to the conference. For the final version, the authors should provide code, provide error bars and details of the speedup of GLUE."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "PoNet addresses the quadratic time and memory complexity of Transformer with a new attention mechanism that has only linear complexity. Theoretical (Section 3.2) and experimental results (Table 2) bear this out. Experimental results on standard datasets like Long Range Arena and GLUE are very competitive with SOTA. Ablations were done on the 3 components of the attention mechanism, too.",
            "main_review": "The PoNet attention mechanism is very attractive as it intuitively makes sense with global, segment, and local inductive biases. PoNet also distinguishes itself with a low constant factor and performs well at even short sequence lengths. Experiments are very well done. Explanations and figures are clear.\n\nI would have liked to see more discussion of the inductive biases beyond the ablation experiments and visualization in the appendix. For instance, I'd be curious if the different granularities are more or less important at different layers. Based on the single sentence in the appendix, it's hard to get an idea what the attention mechanism is actually doing, too. Some tasks beyond classification that may better be able to use structure like question-answering, translation, or summarization may be interesting, too.",
            "summary_of_the_review": "My recommendation is to accept the paper. It builds upon much work about exploiting inductive biases to make a more efficient transformer. While not revolutionary, it is a well-written paper with good analysis and experiments. The idea is has good intuition behind it, and the fact that it is a drop-in replacement for self-attention encoders makes it attractive for ML practicioners, too.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed PoNet, which is an efficient architecture to replace self-attention in Transformer-based models. \nPoNet consists of three components, which are called multi-granularity pooling block.\n\nThe first pooling component is the global aggregation module, which is very similar to the additive attention in FastFormer. The different is that in FastFormer, the global query vector is computed by a weighted sum of the query vectors, while in PoNet it is the average of the query vectors. The segment max-pooling and local max-pooling modules are straight-forward, which are max-pooling operations on pre-defined segments and on local sliding windows, respectively.\n\nExperiments were conducted on Long-range Arena (LRA) and large-scale pretraining and fine-tuning.\nThe authors also conducted ablation experiments to analyze the contributions of these three multi-granularity pooling components.",
            "main_review": "The architecture of PoNet is simple and the paper is clearly written.\n\nHowever, there are some weaknesses:\n\n1. The proposed architecture of PoNet is not novel. The key component, which is the global aggregation module, is very similar to FastFormer. The other two components are also straight-forward and less important than GA.\n\n2. The experimental results are not entirely convincing, especially those on large-scale pre-training and fine-tuning. The BERT-base model in the second group of Table 3 is trained on the same data of the original BERT paper, with and advanced SSO loss and more update steps (125k in original BERT paper vs. 340k in this paper). However, the fine-tuning performance of the BERT-base in Table 3 on GLUE is even worse than that in the original paper. In the paper of Linformer (Wang et al., 2020, Table 2), they re-trained a BERT-base model with only the MLM loss and 250k update steps, but their scores on GLUE are also better than that in Table 3. \n\n3. The authors did not clarify if the PoNet can be used in auto-regressive attention.\n\n4. Personally speaking, some claims in this paper are not well-supported. For example, the last sentence in page 2 claims that the low-rank approximations used in Linformer and Nystromformer limit the ability of looking at the full sequence and hence degrade long-range modeling capabilities. I do not see why PoNet is better than these low-rank methods on modeling long-range modeling.\n\nQuestions:\n\n1. In the experiments of LRA, in Table 1 the authors said that they used the same setting as (Xiong et al., 2021). However, in Table 2 they said the setting is the same as (Tay et al., 2021). Are these two experiments using different settings?\n\n2. In table 2, Performer is 4.3 times faster than Transformer under 4K sequence length. However, in the original LRA paper (Tay et al., 2021), the number is 5.7. What is the difference?\n\n3. In the LRA paper, the authors clarified that the sequences in the text classification task are trunked to length 4K. Why in Table 2, there are two columns with 8K and 16K lengths?\n\nMissing references:\n\nThere are some relevant papers that are missed in the related work.\n\nZhu et al., Long-Short Transformer: Efficient Transformers for Language and Vision. NeurIPS 2021.\n\nMa et al., Luna: Linear Unified Nested Attention. NeurIPS 2021.\n",
            "summary_of_the_review": "To sum up, the idea in this paper is incremental and more empirical evidences are needed to demonstrate the effectiveness of the proposed PoNet architecture.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes PoNet, an efficient model with linear complexity for modeling long sequences. This model replaces the self-attention layer of the Transformer model with its pooling network. \n\nTo aggregate information from different levels, their pooling network consists of three pooling strategies: \na. Global aggregation that aggregates information from all tokens in a sequence. This method is a bit like BERT CLS, and the difference is that they use average representations rather than the representation of the first token.\nb. Segment Max-pooling that performs max-pooling in each sentence to capture sentence-level information. \nc. Local Max-pooling that performs max-pooling in each window to capture local information.\nTheir fusion method is the addition operation.\n\nAs to the training objective, they not only use the MLM, but also use the sentence structural objective as in StructBERT.\n\nThis paper contributes to a well-defined but highly-influential problem: Efficient modeling for long sequences. The main contribution of this paper is the introduction of a multi-granularity pooling that is more efficient than self-attention and brings higher accuracy than previous efficient methods.\n\nEmpirical studies are performed to show the superiority of PoNet over previous SOTA FNet. Their method outperforms FNet in terms of accuracy in their experiment settings but slightly lags regarding time efficiency.\nResults are shown on the task of (1) Long-range Arena (2) GLUE by fine-tuning pre-trained networks.\n",
            "main_review": "Strengths: \n\n1. This paper accelerates calculation and retains accuracy in common benchmarks. \n2. The paper is clear.\n3. Although the idea of each module in multi-granularity pooling is not entirely new, the combination is novel and comprehensive.\n\nWeaknesses: \n\n1. The improvements are not significant over the previous token-mixing method, the FNet. Moreover, their experimental settings are different, and the FNet gets lower performance in this setting. As to the originality, FNet does token-mixing, the contribution of Ponet is incremental.\n2.  The paper relies on empirical results but does not report error bars.\n3. The paper claims it designs a new pretraining objective in their abstract, but they do not validate this claim well.\n4. They do not report their speedup on the GLUE benchmark (e.g., reduces xx% GPU/TPU time) but only report that they lose 4% accuracy than the Transformer baseline. \n5. This paper lacks theoretical evidence or understandings.\n\nMinor: \nThe right subfigure of Figure 1 is a bit confusing.\n",
            "summary_of_the_review": "The paper is good but not enough. It empirically improves the accuracy of classification and language modeling. But the significance and the originality are not high, and they do not bring us much new knowledge. Their comparisons to baselines are not sufficient, e.g. they can be benefited from reporting GPU/TPU time. Since it does not meet the high standard of ICLR, I tend to reject this paper.\n\n---------------Post Rebuttal-------------\n\nThe authors have clarified my concerns about performance, I decide to raise the score from 5 to 6.  I still recommend authors to provide code, report error bars, speedup on GLUE. Moreover, the combination of StructBERT and MLM can not be a significant contribution. Finally, the challenge of combining these pooling methods and pertaining objectives should be discussed and well-evidenced.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors aim to resolve the quadratic time and memory complexity of the standard attention mechanism. They introduce a multi-granularity pooling and pooling fusion that captures contextual information from various levels (global, segment, and token).  They conduct experiments on long sequence tasks and large-scale pre-training and fine-tuning. As a result, their model achieves great performance with speed-up. ",
            "main_review": "Strengths:\n1. Their method is easy to understand and highly motivated. \n2. Extensive experiments have been conducted and reasonable performance has been obtained.\n3. Ablation studies show the contribution of these pooling mechanisms. \n\nWeakness:\n1. PoNet Is suitable for language representation learning but to my understanding, it is impossible for PoNet to build generation models such as machine translation.\n2. I do not find the number of parameters for PoNet. Will these pooling methods introduce a lot of extra parameters?\n3. Some relevant works should be discussed, such as Luna: Linear Unified Nested Attention.\n4. More importantly, the authors introduce the SSO during the pertaining. Is the FNet pertaining also using this objective? If not, I think the comparison is not fair.\n\nMinor comments:\nIn the abstract, authors claim that \"PoNet significantly outperforms Transformer\" which should be in terms of memory and speed, right? Maybe here could be clearer.",
            "summary_of_the_review": "The authors introduce a pooling-based module to replace the self-attention layer in the Transformer in order for fast training speed and low memory usage. Extensive experiments show the effectiveness of their methods. However, some details of their models should be further clarified in the paper (such as the number of parameters). ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}