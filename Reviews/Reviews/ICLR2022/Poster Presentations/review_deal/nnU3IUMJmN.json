{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "Reviewers were in agreement but borderline.  The paper has a nice hypothesis and develops the work using two realistic datasets, Wikipedia and Code.  One reviewer was initially more negative but changed their views based on the authors improvements to the paper.\nThe idea is fairly simple, but does require modellers come up with the structural features.  There was discussion that more down-stream tasks are needed to highlight the approach.  Moreover, more datasets should be experimented with.  In all, experiments are good but improvement is easily done."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This work concerns itself about utilizing structural locality inherent in real-world datasets in improving the effectiveness of non-parametric language models. It makes a claim that a) structural locality is not implicitly fully captured by the distance metric used in non-parametric language models and further that b) explicitly plugging in structural locality into non-parametric language models can improve their effectiveness. It validates this claim first by doing analysis of two datasets with the help of custom locality functions and then by plugging in the locality functions into a non-parametric language model with the help of learnable parameters. \n",
            "main_review": "This work concerns itself about utilizing structural locality inherent in real-world datasets in improving the effectiveness of non-parametric language models. It makes a claim that a) structural locality is not implicitly fully captured by the distance metric used in non-parametric language models and further that b) explicitly plugging in structural locality into non-parametric language models can improve their effectiveness. It validates this claim first by doing analysis of two datasets with the help of custom locality functions and then by plugging in the locality functions into a non-parametric language model with the help of learnable parameters. \n\n\nPositives:\n\t1. Well stated hypothesis and analysis that shows that structural locality is not implicitly fully captured by the distance metric used in K-nearest neighbour non-parametric language model of Khandelwal et al.\n\t2. Locality features for two datasets - wikipedia and Java projects\n\t3. Incorporation of locality features in non-parametric language models using simple learnable functions of the distance metric\n\t4. Analysis that shows that incorporating locality features leads to improved distance distribution among nearest neighbours\n\nNegatives:\n\t1. Structural locality inherent in datasets need to be captured by a set of custom locality feature functions which requires prior knowledge of the domain of the datasets. \n\t2. Marginal improvements in results\n\t3. No detailed discussion of learned parameters presented in Table 3.\n\t\ta. It appears that w doesn't matter so much for all non l_0 features. So it would be interesting to set w to 1 for these features and learn only b and for l_0 learn w. \n\nIn Eqn 4. y should be w_t?\n\n",
            "summary_of_the_review": "The paper makes an interesting hypothesis and goes about validating the hypothesis. However, the improvements due to the proposed method are marginal.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper is about modelling structural locality in non-parametric language models. The key hypothesis is in modelling not only the co-occurrence characteristics but also structural characteristics such as locality. The paper explains the key claims via case studies conducted on source code data and Wikipedia datasets.\n\nThe model paradigm is based on non-parametric language models. A key difference between the non-parametric model and the parametric counterpart is that in the non-parametric model the model parameters are not only determined by the model architecture but also the underlying data.\n\nStructural locality, which is different from just co-occurrence counts, models the structural relationships between pairs of items, e.g., whether they belong to the same or different directory in the case of source code.\n\nThe optimisation model is presented in Equation 7 where the authors need a small sample set emanating from the same domain to train the model. The authors then conduct experiments to demonstrate that the method improves upon existing works. Both qualitative and quantitative experimental results are shown.",
            "main_review": "The paper models structural information into the non-parametric language models. While the results demonstrate that the method improves upon the existing methods, there are some weaknesses too.\n\nStrengths:\n1. The model or the loss function developed by the authors that incorporate structural information is novel. The authors have also clearly explained the model.\n2. Results demonstrate that the method improves upon existing methods.\n\nWeaknesses:\n1. While there are clear strengths, one weakness is that one may need to define structural properties in different types of datasets that one might use. For instance, it is clear that the model works for source codes and Wikipedia because associated structural information can be mined from the data. It is unclear how does the method generalise across different tasks and datasets, i.e., beyond two datasets. While the authors have addressed these limitations towards the end of the paper, the question is will the work be useful only to a small set of audience, or people from different domains can manually or automatically build such prior knowledge and incorporate it in this model. The key advantages are clear from the paper, this seems to be the weakness that is hard to defend. One possible way to improve the argument so that we could obtain Wikidata-type structure for most datasets is to exploit entity detection and linking including automatically learning their relation (vector) information in a completely unsupervised way. The authors must note that I am simply giving ideas on how to strongly defend this weakness of the model.\n2. In terms of experiments, these can be further improved by conducting some downstream application tasks. Can the model be useful for document classification tasks? Currently, it is very difficult to gauge the usefulness of the model through the limited experiments presented in the paper which mainly revolves around perplexity analysis and table 5 in the appendix has additional token prediction results.\n",
            "summary_of_the_review": "Overall, the paper indeed has some merits. The paper can be made stronger by considering some comments mentioned above.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose an approach to complement context by adding 'locality' information in examples present in external stores of non-parametric language models. The locality information captures the hierarchical structure, and deems two contexts more similar (or having less distance) if they share common hierarchical structure/attributes. The authors conduct experiments on source code as well as natural language articles. They analyze the results to point out reasons for improvement, and also highlight differences between the domains.",
            "main_review": "+ The paper is well written and easy to read\n+ What the paper is suggesting is simple, yet useful. The example depicted in Figure 1 describes their motivation as well as what they are doing effectively\n+The concept of structural locality as used in their paper is defined clearly\n+Results are compared with other state of the art models (Table 2)\n\n- The only weakness that I can identify is that the authors have used one dataset of each domain. Since there is minor improvement in the results, it is difficult to gain confidence that the results will indeed improve on other datasets as well\n\n\n\n\n",
            "summary_of_the_review": "Same as above",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Authors propose a way to use structural locality in language models. Authors demonstrate that the structural locality information improves results for two domains they experiment with. ",
            "main_review": "Strengths:\n- Authors propose a way to include structural locality into kNN-LM models\n- Authors demonstrate improvement on retrieval tasks for Wikipedia and Java source code domains\n\nWeaknesses:\n- It seems that the structural locality and its value are known and not new. The paper's contribution then is adding the structural locality to the non-parametric language models. The way to add structural locality is straightforward. It seems to me that this contribution is not significantly novel.\n- There is no comparison or discussion of other ways to add structural locality to LMs. It seems authors tried just a single approach that they present in the paper.\n- Using locality info in authors' experiments leads to minor improvement. This is not a significant weakness since there might be other tasks where locality will contribute more. However, it would have been interesting to see tasks or domains where improvement is more significant.\n\nBased on discussion with authors and clarifications provided in comments and paper itself, I have increased my score to \"marginally above acceptance threshold\".",
            "summary_of_the_review": "In my opinion, the paper makes a minor contribution by proposing a straightforward way to add structural locality to kNN-LM models. The contribution is possibly useful, but not major. For tested tasks, structural locality information improves results but not significantly. I believe the paper is marginally below acceptance threshold.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}