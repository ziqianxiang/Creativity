{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper has potential impact in the theorem proving community, and demonstrated the possibility of using LMs for theorem proving in Lean, and is good enough to use \"in the real world\" through an interactive theorem proving tool. \nThe reviewers wish their data/models were public to address some concerns raised by the reviewers, but we think the community can benefit from this work."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "To deal with data scarcity in context of learning large Transformer language models for theorem proving, this work proposes a methodology (called PACT) for extracting auxiliary task data for joint training alongside the tactic prediction objective. The methodology has been applied to Lean proof assistant. PACT significantly improves the theorem proving success rate on held-out suite of test theorems.\n",
            "main_review": "* This work makes the following contributions.\n\n  1. The PACT methodology: The paper proposes a methodology for extracting auxiliary tasks that can be trained jointly along with the main task (tactic prediction task). The research shows how low-level proof artifact data may be used to significantly boost performance on high-level theorem proving by co-training auxiliary tasks.\n  The auxiliary tasks themselves will be useful for designing similar tasks for other theorem provers. More importantly the PACT methodology is more general; the idea of incorporating diverse auxiliary tasks as a language modeling task by introducing a distinct token for each task is novel. In my opinion, this idea has a potential to be applied to domains other than theorem proving.\n\n  2. The paper contributes LEANSTEP dataset and the Learning environment. This is the first such dataset for the Lean Theorem prover. It is nontrivial to gather the data as it involves hooking into the Lean's compilation process. The source for generating the data is a big contribution to the theorem prover and machine learning community.\n\n* The paper is well motivated and easy to understand and follow. The methodology is explained clearly and experiments are executed with a considerable amount of detail. I feel that the conclusions are in general well supported by the results.\n\n* It is hypothesized that PACT acts a regularizer while imparting useful knowledge to the model due to mutual information across tasks. An ablation study is carried out to rule out the possibility that the benefits from PACT come from simply regularizing the model.\n\n* It is interesting to know that WebMath pre-training is still helpful even in the presence of PACT. There are several such insights in the experiments section that will be helpful to the community.\n\n**Weakness/Suggestions/Questions**\n\n* Although The paper is well written overall, I think section 3.2 Proof Artifact Training can be improved by adding an example explaining the Lean terminology proof term, proof type, tactic, tactic state, etc. It is a bit difficult to understand the task definitions without first understanding the various constituents of the proof.\n\n* Description of refl and tidy-bfs baselines appears much late in the paper. It would be nice if these baselines are described before Fig. 2 is referred.\n\n* It is explained in the paper that the runtime environment ensures that the proofs are never circular. Is any care taken to handle this in training data?\n\n* I am wondering why the authors chose Lean for this this work (as opposed to say MetaMath). Is it just because of the popularity of Lean or the richness of Lean tactics. Can PACT be applied to simpler theorem provers like MetaMath where there are no tactics? I suppose it would be hard to define auxiliary tasks for simpler systems. Would like to know author's view on this.\n\n* page 3: s/\"It has also been previous observed\"/\"It has also been previously observed\"",
            "summary_of_the_review": "Overall, I think PACT and the LEANSTEP dataset are significant contributions. The claim that PACT significantly improves the theorem proving success rate has been well supported by well designed experiments and ablation studies. The system has already contributed theorems to Lean library and has a potential of significant impact on the Lean community in future. I recommend acceptance of the paper.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper describes a transformer-based tactical prover for Lean and its training methods.\nThe work is useful and the results are encouraging, however, there are also major issues.\n",
            "main_review": "Below is my (mildly edited) previous review for another conference that the authors have seen more than half a year ago.\n\nThere are no significant changes to the major issues of the paper and hence practically no changes to my review.\n\nThe major issues with this work remain to be: \n\n1. Poor data hygiene. Training on an unpublished WebMath corpus that may contain unknown amount of information about the testing data. This practically invalidates most of the experimental results. Training on (html-ized/isomorphic version of) the testing data is as much taboo in ML as circular proofs (proving T by T) in math. Unless there is full disclosure, there is no way to assess the results and compare them to other results obtained by established ML/ATP training/evaluation methods. If nothing else, the authors had more than half a year to publish their training dataset and let the readers and reviewers analyze it.\n\n2. No ATP evaluation of the system when trained without WebMath. Given (1) this should have been done long ago but is still missing.\n\n3. To re-iterate on (1) and the futility of not addressing the issues head-on, note that the checks for contamination (p. 14) added in this version of the paper are again poorly designed. A quick search of the html code of mathlib on the web shows pages such as https://leanprover-community.github.io/mathlib_docs/order/zorn.html where the rintro keyword is wrapped in the \">\" \"<\" tokens (likely to do syntax highlighting in html).\n\nOld review:\n\n%%%%%%%%%%%%%%%%%%%\n\nStrengths:\n\n- The largest contribution is that this nontrivial engineering work has\nbeen done for the first time for Lean, a relatively young but quickly\ndeveloping proof assistant. Making a working system such as this is\nnever easy, due to many technical issues. The system works and already\nassists Lean users.\n\n- The results seem encouraging, reaching between 32% and 48% depending\non what is used for pretraining.\n\n- The most honest evaluation on the temporarily held-out set gives 37%,\ncompared to 22% done by a rather naive search procedure. This is a\nreasonable improvement.\n\n- A number of training tasks are experimented with in the\nLean setting. Neither the tasks nor the results seem extremely novel\nor surprising compared with systems like TacticToe and its successors,\nbut this is certainly solid and useful work done for Lean.\n\n\nWeaknesses:\n\n- Similar to the previous work on Metamath, it is hard to understand\nwhat are the possible leaks between the pretraining \"WebMath\" corpus\nand the testing set. This seems quite negligent when training large\nmodels with almost a billion of parameters with a large capacity for\nmemorization. The evaluation should really include the success rate\nwithout the WebMath pretraining.\n\n- To expand on this, many ITP corpora are published on the web and in\npublic repos in many different forms, sometimes after various\nsyntactic translations (to latex, etc.), which are however often\neasy to recover by neural architectures\n(https://doi.org/10.1007/978-3-319-96812-4_22). It is completely\nunclear to me how big would be the effect of GPT pretraining on the\ntest dataset translated in various ways. See e.g. the work of\nGauthier for relatively simple statistical methods that quite\nreliably transfer the proof knowledge between syntactically\ndifferent corpora (https://doi.org/10.1016/j.jsc.2018.04.005 ,\nhttps://doi.org/10.1007/978-3-662-48899-7_26 ).\n\n- It is unclear what resources go into the test evaluation that\nremotely uses gptf. This makes an honest comparison with non-remote\nmethods using standard hardware difficult. This should be at least discussed.\n\n- I do not understand the argument that RL-based data synthesis is\nmore expensive. The RL-style proof data synthesis done in TacticToe,\nhttp://arxiv.org/abs/1805.07563 or\nhttps://doi.org/10.4230/LIPIcs.ITP.2019.34 is most likely orders of\nmagnitude cheaper than just the pretraining done on WebMath here.\n\n- While the contribution is solid and data augmentation methods are certainly useful in DL, the introductory claims about data scarcity being a newly encountered difficult problem in ML-for-TP are uninformed.\nIn particular, all of the larger ITP corpora (Isabelle, Mizar, HOl, Coq) are capable of easily exporting\nmillions of problems and proofs to start with and this has been done\nmany times since long ago. Already the 2003 version of MPTP and\nthe AI/TP experiments based on it\n(https://doi.org/10.1007/s10817-004-6245-1) allowed and announced a\nstraightforward generation of 630000 related proof tasks from\nMizar. Further millions/billions/zillions of proving and training\ntasks can be created easily by chasing the large derivation graphs\nof the ITP libraries. This has been to various extent used in works\nsuch as [1,2,3], where the benefit of learning from additional proof\ntasks was also demonstrated. Equally easy and cheap is chasing the\ngraphs of large ATP proofs and generating problems from them,\nrunning ATPs to generate terabytes of further data, etc. The theorem\nproving domain is really the antithesis of data scarcity, has been\nsuch for long time, and it is one of its great advantages over NLP\ndomains.\n\n- A number of related experiments have been done with argument,\nwitness, conjecture and proof (step) synthesis recently. See e.g. [4-7].\n\n[1] Cezary Kaliszyk, Josef Urban:\nLearning-assisted theorem proving with millions of lemmas. J. Symb. Comput. 69: 109-128 (2015)\n\n[2] Kaliszyk, C., Urban, J. & Vyskocil, J. Lemmatization for Stronger Reasoning in Large Theories in\nFroCoS 2015 9322 (Springer, 2015), 341–356.\n\n[3] Bartosz Piotrowski, Josef Urban:\nStateful Premise Selection by Recurrent Neural Networks. LPAR 2020: 409-422\n\n[4] Thibault Gauthier:\nDeep Reinforcement Learning for Synthesizing Functions in Higher-Order Logic. LPAR 2020: 230-248\n\n[5] Thibault Gauthier:\nDeep Reinforcement Learning in HOL4. CoRR abs/1910.11797 (2019)\n\n[6] Bartosz Piotrowski, Josef Urban:\nGuiding Inferences in Connection Tableau by Recurrent Neural Networks. CICM 2020: 309-314\n\n[7] Josef Urban, Jan Jakubuv:\nFirst Neural Conjecturing Datasets and Experiments. CICM 2020: 315-323\n\nUPDATE:\n\nI do not agree with the idea that the potential test set contamination would be a significant advantage due to its autoformalization potential:\n- The Wang et all 2018 paper I mentioned and their related 2020 paper show that RNNs and Transformers are very good at such translations. I don't agree this would be surprising today for GPT.\n- This is really an opposite of a \"significant advantage\". The test set evaluation is thus made incomparable to any other honest evaluation done on Lean in the future.\n- The WebMath dataset has not been published and is impossible to check by readers and researchers. I would expect at least its publication as a partial response to such concerns.\n- Not doing the test set ATP evaluation without the WebMath pretraining is a serious omission.\n\nI also do not see any improvement in explaining the hardware resources used for the evaluation. This again makes the numbers here hard to compare with for other researchers and methods.\n\nThere is also hardly any improvement of the overclaims (noted also by other reviews) about the technical novelty, claims about comparable slowness of RL setups, etc.\n\n%%%%%%%%%%%%%%%%%%%\n\n",
            "summary_of_the_review": "The work is an important practical step for Lean, but there are major evaluation and presentation issues.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to train large Transformers for proving theorems on Lean by using the auxiliary training objective built with proof terms. The main contribution of this paper is (1) building the theorem proving benchmark for Lean (2) building a sequence of auxiliary training tasks using proof terms and verifying the effectiveness (3) comparing the pre-training method with the co-training method.",
            "main_review": "The main idea of using proof terms for self-supervised training is reasonable. The paper is well-written and the main approach is easy to understand. The experiment details are thoroughly demonstrated and the results look solid.\n\nThe main drawback of this paper is the lack of technique novelty in terms of matching learning.\n\nQuestion: when extracting subterms from each proof term, do you parse a proof term according to the steps of human-written proofs, or do you parse the proof term according to its own grammar?",
            "summary_of_the_review": "Overall, this paper advances the techniques of training deep networks for theorem proving by co-training on proof terms. I recommend accepting this paper.\n\n==============================\nAfter reading the authors' responses and other reviewers' comments, I maintain my previous rating of this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper addresses the general setup of using transformer models for interactive theorem proving (ITP) tasks. The ITP engine considered here is Lean. The contribution of the paper is a data augmentation method. This is achieved by mining low level artifacts from a given dataset of Lean proofs. This includes data extracted from additional type information inferred by Lean while processing the given proofs. Several prediction tasks are formed for this additional data, to allow pre-training and co-training in combination with the existing WebMath dataset. Results show that the additional datasets extracted in this way aid substantively when used for pretraining and cotraining.\n",
            "main_review": "This paper is an interesting application of a data augmentation or self-supervised learning type of approach for tactic based theorem proving. The idea itself is not completely new as the authors readily explain in the paragraph MACHINE LEARNING WITH PROOF ARTIFACTS on page 2. The paper appraisal therefore rests on the clarity of presentation, how convincing the experiments are, and how reproducible. \n\nOverall the paper presentation is okay, although the clarity could be improved. One issue is that the main contribution is  mostly condensed into section 3.2 which is less than one page. In general the writing does not make the mechanisms by which proof artifacts may be extracted from Lean clear enough. It would be nice to include (possibly as an appendix) a focussed primer on Lean which allows to give more depth in the main paper while still being relatively self contained.\n\nThe experments are fairly convincing, although it is not entirely surprising that this approach works, and to repeat, the basic idea of extracting additional training data in this way is not entirely new.\n\nTacticZero by Wu et al 2021 is missing from the related work. This is relevant because, by training end to end, that work effectively generates arbitrary amounts of training data through interaction the the HOL4 ITP system (intermediate theorems which are proven give some reward in that work).\n\nTypos:\n- synthesis -> synthesise\n- DeepHOLZero -> DeepHOL\n- wrong bold number in Figure 3\n",
            "summary_of_the_review": "This paper gives a useful demonstration of how an existing idea (extract additional training data from the Lean engine) can be scaled up to give a tangible benefit in practice (improved theorem proving results).\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}