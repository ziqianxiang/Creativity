{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes a new dataset called ComPhy to evaluate the ability of models to infer physical properties of objects and to reason about their interactions given these physical properties. The paper also presents an oracle model (named oracle because it requires gold property labels at training time) that is modular and carefully hand designed, but shows considerable improvement over a series of baselines. The reviewers for this submission had several concerns including:\n(a) [VByS] \"concerns are about the complexity that the proposed method can handle\"\\\n(b) [VByS] \"the method is only demonstrated on a simple synthetic dataset\"\\\n(c) [8BUA] \"I am struggling to see any direct application\"\\\n(d) [8BUA] \"choosing 4-videos as reference\" -- why use ref videos, why use 4\\\n(e) [8BUA] \"Baselines showing results with ground-truth object properties should be reported\"\\\n(f) [3cQE] \"no innovation in the type or structure of questions asked\"\\\n(g) [3cQE] \"neither the CPL framework nor the implementation of any module is novel\"\\\n(h) [DJEq] \"The only difference is that this paper infers hidden properties instead of collisions\"\\\n(i) [DJEq] \"The dataset is not comprehensive enough\" -- only 2 properties and simplistic and synthetic videos\\\n\nThe authors have provided detailed responses to these concerns and I discuss these below.\n\nThe authors have addressed (c),(d) and (e) well in their rebuttal.\n\nI don't think (a) is concerning. The proposed model is not expected to solve the dataset entirely inspite of having access to gold properties at training time. As the authors mention, this indicates the complexity of the task at hand.\n\nThe authors also address (f) well. I dont think there is any need for innovation in the structure of questions asked. QA is merely a mechanism to probe the model, and using CLVERER style questions seems appropriate.\n\nI disagree with the sentiment behind (g). The proposed oracle model clearly inherits modules from past works and assembles them to suit the needs of the dataset. It is this assembly that differentiates it from past works. This is true of most papers in our field, including ones that are widely acknowledged to be important papers. The underlying modules in proposed networks are rarely novel, but their assembly can lead to improvements on benchmarks. Furthermore, the oracle model, isnt the central contribution of this work. The dataset is, and hence, the requirement for novelty is reduced. The oracle is meant to serve as a guideline to show what one may achieve given gold labels at training, and it serves that purpose well.\n\nRe (h), my takeaway is that inferring properties based on their dynamics and without any link to their appearance is an important step, and past datasets do not exhibit this characteristic. And thus, in spite of being a limited differentiation from CLEVERER, I think this is interesting.\n\nRe: (b) and (i) I do agree with some aspects of these, with the reviewers.\nI think its still valuable to have a dataset with synthetic videos, given that models today are unable to solve this dataset. Moving to more realistic videos is a next step.\nHowever, as the reviewer [DJEq] points out, it would be desirable to add more physical properties and add more complex scene elements like ramps. That would have added a lot more diversity to the dataset -- visually, with regards to physical properties and with regards to the types of reasoning required.\n\nHaving said that, I believe that the dataset in its present form is still valuable to the community, and hence I recommend acceptance.\nI think adding more physical properties and scene elements will have made this a much stronger submission."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a dataset and a method to study the hidden properties of objects present in videos. The method can extract object attributes and use them for predicting subsequent scenes, with the additional capability to handle counterfactual scenarios.",
            "main_review": "This paper is overall well written and executed. With a focus on inferring hidden attributes, this work tackles a crucial aspect of visual reasoning. Experiments are well-designed to illustrate the claimed contributions.\n\nMy main concerns are about the complexity that the proposed method can handle. Overall, the method is only demonstrated on a simple synthetic dataset with two attributes (mass and charge), while mass only has two values (heavy vs light). Despite it does demonstrate the claimed contributions, to what extent the proposed method would work and generalize is unknown.\n\nAdditionally, it seems that all objects in a video in the datasets are of interest; there are no irrelevant objects. Would this become a problem for the algorithm?\n\nFor a visual reasoning task, the proposed method seems to struggle in the following scenario: If A and B are attracted, and B and C are attracted, A and C should be repelled.",
            "summary_of_the_review": "This is a good work that fills the gap of visual reasoning of hidden attributes, but experiments could be more comprehensive.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a new task and corresponding dataset ComPhy, a video question-answering dataset for evaluating video reasoning capability. First, a model is given 4 reference videos (2-sec clips showing object interactions in isolation) which is used to deduce object properties. Then a new query video (which could have more than 2 objects) is given on which 3 types of questions are asked: factual (about what happens in the video), counterfactual (what would happen if some condition changed), and predictive (what would happen after), with later two being multiple-choice questions. A particular contribution is the focus is on latent properties of the objects (mass and charge) which needs to be deduced from the reference videos. Multiple video reasoning model baselines are reported along with a newly proposed oracle Composition Physics Learner (CPL) model which shows considerable improvement over the baselines.\n\n",
            "main_review": "### Strengths:\n\n1. The task definition, formulation, and the proposed dataset Comphy are novel and thoughtful. Models working on other physics-based (such as Phyre, Cleverer, Cophy) datasets may or may not work on this dataset (Comphy).\n\n2. Multiple baselines are tested and the proposed model CPL shows significant gain over these.\n\n### Weakness:\n\n1. On Motivation: The paper doesn't motivate the problem, and I am struggling to see any direct application. Could the authors provide any examples where such a model would be useful?\n\n2. (Minor): In Table 1., the authors suggest CoPhy and Cleverer do not support counterfactual, but to my understanding they do? I could be missing something here.\n\n3. On Task and dataset setup:\n\n(i) For intrinsic properties, the authors chose mass and charge suggesting that inferring the two is challenging. However, this is not explicitly shown in experiments. Authors should show that just finding mass or just finding charge from reference videos is doable, but having both in properties together makes the task difficult (I believe this to be the case intuitively, but an explicit expt would be helpful).\n\n(ii) The reason for choosing 4-videos as reference is unclear as compared to directly providing the values as input. For instance, the input could itself be the target video + properties of the objects, followed by multiple questions. \n\n(iii) How is number \"4\" for reference videos chosen? Does the task become simpler for a particular target video if more reference videos are added (say 10 or 1000)?\n\n(iv) In the current setup, each target video has 4-reference videos, followed by a number of questions. A simple extension would be to use the same 4-reference video, but use a different target (same objects, same properties but different target videos). This could suggest if the model is is able to answer some questions by chance or did it actually infer the physical properties.\n\n4. On Model and Experiments:\n\n(i) Intermediate results: Baselines showing results with ground-truth object properties should be reported. For instance, NS-DR and CPL in Table2., but  they are given the ground-truth heavy/light information as well as the corresponding charge. (This is related to point 3.(ii) ).\n\n(ii) The task of deriving object properties from the 4-reference video is independent of the questions asked on target video. As such, the authors should report separate metrics for just this part. This would show if future research should focus on deriving physical properties or given physical properties work on the counterfactual/predictive types of question.\n\n(iii) The physical property inference is (to the best of my understanding) is applied on 4 reference videos + target video. This setup of being dependent on target video seems confusing to me. As such a model which has seen 4 reference videos should be able to work on a completely new target video as well (see related point 3.(iv)) Could the authors show results without using target video?",
            "summary_of_the_review": "The authors provide a novel and well-thought task and corresponding dataset ComPhy which requires a model to use 4-reference videos to infer intrinsic properties of the objects and thereby apply these findings to a target video and answer corresponding factual/predictive and counterfactual questions. However, in its current form, the exact motivation of the paper is unclear, the reliance on using 4-reference videos compared to simply providing the information is not justified, and some dataset choice such as using only target video for given reference videos are not clear. More intermediate results such as finding object properties would make the paper stronger.\n\n---\nPost Rebuttal: The authors have substantially updated their paper with additional ablative studies and human evaluation results. In my opinion, this substantially improves the findings and takeaways of the paper. More experiments with physical properties in isolation, showing the effect of increasing the number of reference videos, and de-coupling the reference and target videos could further strengthen the paper.\n\nAs such, I am increasing my score to 6.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work extends prior datasets (chiefly, CLEVRER) for physical reasoning from visual input. It endows objects with two latent properties–mass and charge–thereby producing object dynamics like attraction or repulsion. The paper also presents VQA results on the dataset from a modular architecture (similar to NS-DR from CLEVRER) trained by supervision.",
            "main_review": "This paper is structured identically to CLEVRER (Yi et al., ICLR 2020): Figures 1 and 2 contains examples of scenes and questions from the dataset. Table 1 uses check marks to highlight features of existing datasets. Figure 3 shows the frequency of question types in a pie chart. Figure 4 presents sample programs representing the questions. Table 2 compares baseline methods (with almost the same column headers as the corresponding table in CLEVRER). And so on.\n\nBeyond the writing, the models presented (CPL versus NS-DR) are also broadly the same. The similarity of the two papers raises the concerning possibility of plagiarism, or more likely, a marginal contribution sold anew in recycled packaging. Nevertheless, I will attempt to assess the work on its own merits.\n\n**Strengths**:\n- the code is public\n- human baseline scores\n- some attempt to avoid mistakes in past datasets (e.g. the exclusion of counterfactual questions on objects which have no interaction with other objects)\n\n**Weaknesses**:\n- Dataset: there's no innovation in the type or structure of questions asked. The generation makes arbitrary choices e.g. a video can contain a pair of charged objects (or none), or up to one heavy object. The visuals themselves are highly similar to prior work (borrowing originally from CLEVR), contributing to collective overfitting over time.\n- The model: neither the CPL framework nor the implementation of any module is novel. It is entirely supervised, and exhibits little generalization.\n- Writing: \n  - there's several instances of overclaiming (e.g. \"we compare the mass and charge edge label prediction result with the ground-truth labels and find that they achieve an accuracy of 90.4% and 90.8%. This shows the effectiveness of CPL for property identification.\" Given everything is trained with supervision, these prediction accuracies are no surprise.) \n  - the paper is disorganized (e.g. human baselines are presented inline but not in Table 2, the generalization results in Figure 6 use a different method than in Table 3, etc). Several parts are unclear (e.g. \"To run NS-DR successfully in ComPhy, we provide NS-DR with extra ground-truth physical property labels. The variant, NS-DR+ uses the PropNet (Li et al., 2019b) for dynamics predictions, which does not consider the mass and charge information of the objects.\" This sounds contradictory to me.)\n  - it is also sparsely detailed (e.g. how exactly was the human data collected? Are the modules trained end to end or one by one?).\n- Empirical results: \n  - despite the numerous baselines in the paper, some important ones are missing. For instance: how would an oracle model perform if it understood all object properties but had no inkling of the latent properties (mass and charge)? That would help reveal the significance of latent properties over the remaining properties.\n  - there's no error bars for any of the results.\n\nMore philosophically: do we actually need models which can infer/understand physical properties like charge from visual observation? Or models which can figure out–to use the example in the paper–why \"apples float in water while bananas sink?\" It doesn't seem to me that this kind of knowledge emerges from direct observation or experience in humans.",
            "summary_of_the_review": "Both the presented dataset and model framework are minor tweaks of existing work. There's no novel insight. The dataset will not enable any new research directions. The paper also needs more work. I can't recommend accepting at all.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper focuses on inferring two hidden physical properties (mass and electricity charge). A synthetic dataset, called ComPhy is created. The dataset consists of simple shapes (such as cubes and sphere) that move on a flat plane. For each query video, there are four reference videos that the models can use to learn the properties of objects. The dataset also includes three types of questions: factual, counterfactual and predictive. The paper also presents a pipeline approach called CPL that leverages graphs to model the relationship of objects and solve the QA tasks.",
            "main_review": "**Strengths**\n\n- The paper explores a quite interesting direction. Reasoning about the physical properties of objects is a critical requirement for designing agents that interact with the world around them.\n\n- The proposed CPL approach outperforms a number of baseline approaches.\n\n**Weaknesses**\n\nThe paper has some shortcomings:\n\n(1) The work is incremental compared to Yi et al., 2020. The only difference is that this paper infers hidden properties instead of collisions. The questions and the way they are generated are very similar as well.\n\n(2) The dataset is not comprehensive enough. Only two properties are considered: mass and electricity charge. I don't think electricity charge is a useful property to infer since it can be used only in a limited set of applications. The dataset should include useful properties such as friction and material. Moreover, the synthetic videos are so simplistic (cubes and spheres moving on a plane). It is not clear if a system that solves this task can be useful in any scenario that is slightly more complex in terms of appearance and object interactions. I encourage the authors to create more realistic scenarios in simulators such as AI2-THOR and iGibson.\n\n(3) It is not clear how the train/val/test splits are created. It doesn't seem there is much variability in the videos so there might be a high degree of similarity between train and test scenarios. Some metrics should be reported to show train and test splits are different enough.\n\n**Questions and comments**\n\n- \"CNN-LSTM (Antol et al., 2015) is a basic video question-answering model.\": This paper is not a video question-answering model. It just uses single frames.\n\n- \"We think the reason is that these models are based on massive training videos and question-answer pairs and have difficulties adapting to the new scenario in ComPhy\": It might be simply due to the large networks and small amount of data.\n\n- There is an assumption that all objects are already known and (it seems) there is only one object per each category. It would be nice to see the results of generalization to unseen shapes. However, it is not clear how the graph can accommodate new objects. It seems it is a fixed graph.\n\n- Equation 2 is not clear. What is g_emb and g_enc? What does z represent?\n\n- It is not clear why CPL shows better performance on counterfactual questions. There is no component in the model to better handle that type of question.\n\n- Why is CPL called oracle?",
            "summary_of_the_review": "The reasons for my low rating are: (1) lack of novelty with respect to previous works, (2) dataset not being comprehensive enough, (3) simplicity of the dataset and lack of generalization to slightly richer scenarios in terms of appearance and object variability (4) lack of clarity on creation of the dataset. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}