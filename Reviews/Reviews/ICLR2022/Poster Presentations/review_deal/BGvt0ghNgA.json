{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper introduces unsupervised skill discovery using Lipschitz-constrained skills. It is well-written and demonstrates the advantages in a solid experimental section."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a new objective (LSD) for unsupervised skill discovery, building on prior skill discovery work using mutual information criteria. As in previous works, skills are parameterized by a latent variable. The main ingredients are changes to the MI-based reward that (1) maximize alignment of learned state representation and latent variable, and (2) ensuring that state space differences are larger in the actual state space than in the learned representation. These modifications act as a prior so that the skill variable contains semantics (as it encodes a direction in representation space) and skills are encouraged to significantly change the agent's state.",
            "main_review": "Strengths:\n- Clear write-up with good motivation that correctly points out a common failure mode of previous MI-based objectives.\n- Good performance on basic and challenging robots, including Humanoid. Overall, the empirical evaluation is quite exhaustive, and multiple relevant baselines are included\n- Alignment of representation and skill enables zero-shot goal reaching, which is nicely demonstrated.\n\nWeaknesses:\n- A lot of focus is put on skills resulting in directed locomotion. While this is a useful result and allows to easily measure coverage, for example, it's also a limited evaluation -- for directed locomotion, it's pretty straightforward to simply learn goal-directed policies and use them as skills. It would be nice to consider other tasks as well, e.g. letting the Cheetah jump over hurdles (as in DIAYN, for example). It's good that the authors also evaluate on robot manipulation tasks (Fetch* environments), but if I understood correctly, in these setting they consider the location of the target object as the only state features for reward and state representation?\n- The authors acknowledge that with continuous skills, their method mostly discovers locomotion skills. I'm left wondering to what degree this is caused by the skill dimension being set to 2. What happens if 3 or more dimensions are used?\n- The notion of \"dynamic skill\", which is used quite a bit in the abstract and motivation, is not really clear. It's finally explained in section 3.1., but I don't find it very intuitive.",
            "summary_of_the_review": "Overall I think this is a good paper, but also that the focus on the navigation tasks is rather narrow and that it would be nice to a few non-navigation downstream tasks with the standard MuJoCo agents.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper builds upon the DIAYN idea (Eysenback 2018) that an agent could develop skills in an unsupervised environment by finding a set of skills that collectively visits the whole state space but encourages each skill to cover a different subspace and later use one of these skills to simplify the learning of a downstream task. The paper points out that prior work maximizes the distribution of latent states, but does not necessarily maximize world state differences, because latent states are the result of a learned projection from world states. The paper introduces an explicit Lipschitz constraint that forces latent state differences to be larger than observed state differences and relaxes a prior constraint of equality so that the latent states only need to move in the same direction as observed states but not in a fixed ratio. The paper shows this can be implemented as an efficient per step intrinsic reward for unsupervised learning.  Since the latent state aligns with observed state, one can use simple planner on downstream tasks to choose the right z to achieve a goal in observed space without any additional learning or fine tuning (zero shot skill exploitation). The paper evaluates LSD on 2D skills in the widely used AI GYM benchmark. Qualitatively, the LSD model discovers challenging dynamic behaviors in the difficult humanoid environment that competing unsupervised models fail to discover. Quantitatively, expected reward is higher for LSD and LSD zero shot than competing models. Direct evaluation of state space coverage shows LSD has significantly larger coverage than competing models supporting the paper hypothesis. LSD is also demonstrated on robotic tasks and is able to learn to pick objects in using unsupervised learning. ",
            "main_review": "Figure 5 seems to show that LSD gets fundamentally better skills as competing methods look like they are asympoting to a lower level in at least AntMultiGoal, HumanoidGoal and HumanoidMultiGoal. The evaluation on classic AI Gym as well as robot pick and place tasks shows good scalability to a variety of interesting tasks. \n\nThe objective looks simpler to evaluate than a number of the competing approaches promising a more efficient algorithm.\n\nThe idea of aligning the direction of latent and observed states is nice for downstream planning but will this ultimately limit the flexibility of the latent states to express important properties of the space that may not be one-to-one? \n\nIn equation 6, is the <= sign backwards? Shouldn’t the projected states be larger || phi(x)-phi(y) || >= ||x-y|| according to the argument of the paper? \n\n“LSD” and “learning” including “machine learning” has a big literature already that might make it hard to find the “LSD” paper. Maybe something like Lipschitz-constrained Unsupervised Skill Acquisition (LUSA)? \n\nWhile LSD technically doesn’t make use of hand engineered features, the fact that the algorithm enforces a directionality consistent with the observed states pace kind of builds in a similar concept to maximizing travel in space … it is a bit subtler but not completely different. The case here seems overstated. ",
            "summary_of_the_review": "The paper proposed a new practically implementable objective for unsupervised learning that gives qualitatively and quantitatively better performance on classic AI Gym benchmarks and challenging robotic pick and place.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper introduces Lipschitz-constraint skill discovery (LSD), a method for unsupervised skill discovery. The method addresses one limitation of the line of unsupervised skill discovery works which use mutual information between states produced by a skill and latent skill representations as the training objective, which is that they often discover overly simple and static skills, and require some additional domain information to be provided to learn more dynamic skills. LSD addresses this problem by reformulating a Lipschitz-constrained objective to learn continuous or discrete skills, and then demonstrates the effectiveness of the learned skills for downstream tasks. \n",
            "main_review": "Strengths:\n\n\nThe paper is well written and motivated. The discussion in section 3.2 is intuitive and makes the decisions behind the LSD objective quite clear. \n\n\nThe experimental results are quite impressive, in Ant and Humanoid particularly, and it seems that LSD is quite effective at discovering more dynamic skills compared to prior work, without specific domain knowledge. \n\n\nThe downstream task performance of LSD (using a hierarchical controller which selects actions in the space of skills) is a significant improvement compared to a number of sensible prior works.\n\n\nThe method is relatively simple to implement (as an additional reward to a model-free RL method), and an implementation is provided for reproducibility.\n\nWeaknesses:\n\nThe related work by Choi et al also uses spectral normalization to improve learned skill quality, so it needs to be better clarified how this work stands with respect to that.\n\nAs the authors mention in the conclusion, LSD assumes that the Lipschitz constraint is meaningful ($ell_2$ norms in the state space are meaningful) which is not always true, for example when learning from pixels, or perhaps in some more complex manipulation settings. But I agree that representation learning would likely help to address this issue when learning from pixels.\n\nThe scale of figure 2 seems like it could be slightly misleading: while it makes sense that LSD would generate skills which cover the state space well (especially in the state magnitude), it’s unclear if the other discovered skills are also able to perform the same action (maybe the ant can still walk, but just not as far). For example, DIAYN-XYO on the Ant environment may learn some locomotion skills but its trajectories are almost invisible on the figure, which makes it seem like it doesn’t learn any skills.\n",
            "summary_of_the_review": "I think the paper can be accepted with some modifications. The method is well-motivated, tackling an important issue with mutual-information based skill discovery methods. It demonstrates impressive qualitative and quantitative results compared to prior works. The paper reads very well and is easy to follow. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "\nThe paper proposes a new skill discovery objective based on lipschitz-constrained dynamics skills. Specifically, for a state encoding \\phi, <\\phi_{t+1} - \\phi_t, z> is maximized with a DIAYN-style training, and spectral normalization on \\phi. The connection to DIAYN with Gaussian skills is shown, which is an unconstrained version of the proposed objective. A version of the method that defines a prior distribution of skills with finite support is also presented, which can be used similarly to discrete skill learning methods. Empirically, the method achieves significant improvements over prior work, being able to tackle ant and humanoid domains, where all prior methods fail. \n",
            "main_review": "\nThe paper proposes a sensible method with empirically very significant results. Further, interesting theoretical connections to existing work are provided and the paper is generally well written with high-quality evaluation. \n\nHowever, tzhe paper also suffers from several issues\n1. The proposed method is ad-hoc. See Sec 3.2, “Specifically, we suggest replacing the implicit regularizer of VIC (the second term in Equation (5)) with a 1-Lipschitz constraint on φ”, which is done without justification. The connection to mutual information skills is intriguing, but the proposed method is not shown to maximize mutual information. \n2. The “discrete” version of the method is even more ad-hoc. Indeed, it is impossible since a discrete variable cannot be Lipschitz-continuous. Instead, the continuous method is used while restricting the prior skill distribution to a finite number of skills. It is unclear what are the theoretical properties of this method since it is different from the standard methods that use discrete skill variables.\n3. The introduction is very confusing and does not state the actual contribution of the paper. Instead, the claimed contributions 1 and 2 in the intro are not novel. Both constraining the skill encoder to be Lipschitz continuous and using skill discovery for zero-shot evaluation was proposed by Choi’21 (or arguably by even earlier work). Why does the contribution paragraph not focus on differences with prior work?\n4. The paper claims that none of the prior work can perform zero-shot evaluation, while also citing Choi'21 that explains the opposite. Please provide zero-shot evaluation for the prior work that the paper compares to.\n\n\n---- Update -----\n\nThe updated paper resolves most of my initial concerns, therefore I increase my score to accept. However, I am not confident that this method will scale to visual environments as the method crucially relies on Eucledian distance in the observation space to be meaningful (via the Lipschitz constraint), as the authors themselves note in the limitations section. Further, it appears from Fig 9 that the method ignores some of the state dimensions entirely and therefore might not be able to learn diverse skills that involve all state dimensions such as reaching a desired joint pose. Therefore I am not able to give a strong accept.\n\n",
            "summary_of_the_review": "The paper proposes a novel method with significant performance improvement. However, the paper suffers from several issues with accuracy of the claims. Further, a version of the method dubbed \"discrete Lipschitz-constrained skill discovery\" is proposed, which is clearly a contradiction (discrete variables cannot be Lipschitz constrained). The paper needs to provide a clear explanation of what the discrete version of the method actually is. Without fixing these issues, specifically points 2-4 above, I believe the paper cannot be accepted.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}