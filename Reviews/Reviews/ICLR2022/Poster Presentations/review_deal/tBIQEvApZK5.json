{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This is a borderline paper. \nThis paper proposed feature kernel distillation (FKD), a new distillation framework, by matching the kernels obtained from the networks of student and the  teacher.  Theoretical justification is provided by  extending the results of Allen-Zhu and Li(2020)(ALi20 hereafter). Empirical results show superiority of FKD over vanilla KD on several datasets.\nThere is however concern that the technical novelty is limited and  incremental, an opinion shared by DKJu, and 68WG, compared to ALi20. Reviewer DKJu suggests that the authors could highlight those results which are not straightforward extensions of ALi20. Another important point of concern is that the paper may have some Overstated claims.  The authors clarified that the language of the claims be suitably edited. In this regard Reviewer h8ud have some specific suggestions which should be easy to incorporate. \n\nIn view of additional experiments conducted and detailed discussion during rebuttal addressed some of the concerns of the reviewers.\nIf accepted, the final version, should include most of the discussion and additional experiments."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposed a new knowledge distillation framework, feature kernel distillation (FKD), by matching the student network's feature kernel and that of the teacher network's feature kernel. The authors extended the theoretical results in Allen-Zhu & Li (2020) to demonstrate the advantages of their framework over standard NN training. The theoretical results are built upon a synthetic dataset consisting of multi-view and single-view data, where standard training will fail to generalize while FKD will generalize well. Empirically, the authors (loosely) showed the consistency between the empirical behavior of FKD and the theoretical predictions, though the empirical setting is fairly different from the theoretical setting. In addition, on several image datasets, FKD can outperform other KD baselines in terms of test accuracy.",
            "main_review": "Pros:\n\n- The proposed FKD is novel, to my knowledge. Especially, it can apply to the setting where the teacher's output dimension is different from that of the student, while the traditional KD is not applicable. \n- The authors provided theoretical justifications for their FKD under a carefully designed setup, which I believe intuitively makes sense.\n- The entire paper is well-organized and the authors provided detailed and intuitive explanations when they introduce the theorems and derivations, which makes it easier for me to follow the paper.\n\nCons:\n\n- The technical novelty is very limited. I roughly went through the appendix, and I found most of the derivations are based on the results in  Allen-Zhu & Li (2020). There were no novel proof techniques presented.\n\n- It's unclear for me, how does FKD compares to KD theoretically. If I understand correctly, FKD, KD, and ensembles are all able to deal with the multi-view setting, where standard training will fail to generalize. So, what's the advantage of FKD over KD in theory? I think some clarifications on this are essential.\n\n- Why there is an $\\epsilon$ in theorem 2? I don't understand where it comes from. Can you explain?\n\n\nMinors:\n\nIn Definition 2 in Appendix C.1, page 18 & the second equation in Appendix C.4, page 27, $\\mathcal{P}\\_{v, l}$ should be $\\mathcal{P}\\_{v_{c,l}}$. \n\n\nOverall, I believe the proposed FKD is a novel and (potentially) practically-useful algorithm. However, I think the theoretical results in the paper do not provide any new insights on KD. So, I would suggest the authors move the theoretical results into the appendix, and focus on providing more empirical results, e.g., NLP tasks. I believe this will better sell the paper, and also expose the paper to a broader audience. ",
            "summary_of_the_review": "I like the idea of FKD that matches the student network's feature kernel with that of the teacher network in KD. This can apply to situations where the teacher network and student network have different output dimensions. However, the theoretical novelty is limited, and also the empirical evaluations are not thorough (only on some very small datasets). In summary, I vote for a weak acceptance. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper the authors consider extending the knowledge distillation (KD) framework to cases where the student and the teacher do not share prediction spaces. To this end they propose Feature Kernel Distillation (FKD), where one views the last layer weights of a neural network as a data dependent feature kernel. Instead of regularising the distance between temperature scaled logits corresponding to each data point of the student and the teacher in vanilla Knowledge Distillation, the authors instead regularise based on the distance between feature kernels of a pair of data points (inner product of weights of the pre-final layer of the NN). \n\nUnder a simplified setting, viewing data as being multi-view the authors show a setting where classical knowledge distillation would not do well, but a FKD based knowledge distillation using an ensemble of teachers does well (the teacher kernel between two data points is the average teacher kernel across the ensemble). The authors then propose using a correlation kernel instead of raw kernels and feature regularisation to spread out the kernel values.\n\nThe authors proceed to empirically demonstrate  that this approach helps in dataset transfer between datasets of different number of classes. They also show that using an ensemble of teachers enables FKD to improve performance (but KD also improves performance).",
            "main_review": "The problem investigated in this paper is an interesting one. The approach of trying to regularise to match the kernels of the teacher rather than temperature scaled logits is also quite interesting. The experimental results shows that the method is competitive in various settings. The theory is interesting though it seems to be a fairly straightforward extension of Allen-Zhu and Li. \n\nThe one weakness of the paper is the lack of concordance between the theory and the practice. The experiments show that FKD is quite competitive even without ensembles. But the theory does not explained why. It was not clear to me how Feature Regularisation would effect the theory.",
            "summary_of_the_review": "The paper examines an interesting problem. The Feature Kernel Distillation is an interesting algorithm. The experiments seem to indicate that it is competitive in a variety of settings. The theory is reasonable interesting as well. The paper is well written as well. Hence I recommend acceptance. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "There are several approaches in knowledge distillation for neural networks. This paper studies kernel-based knowledge distillation for multi-view data set. The authors revealed that the assembled feature map obtained from the ensemble teacher network effectively improves students' generalization ability. Some numerical results are presented to show the practical effectiveness of the learning method based on theoretical findings. \n",
            "main_review": "This is an interesting paper. The paper is clearly written. Though I did not follow the detailed proof in the appendix, the brief description of the proof in the main paper was helpful to understand the idea behind the theorem. \n\nThe following are minor questions:\nIn eq(4), sum-pooling is used. In practice, however, max-pooling is also widely employed. Does the theoretical analysis also work for the max-pooling? \nIn numerical experiments, how was the regularization parameter lambda_KD determined?\nWhen the genuine ReLU is used instead of tilde{ReLU}, can one obtain almost the same numerical results? \n",
            "summary_of_the_review": "This is an interesting paper. The paper is clearly written. So my recommendation is to accept this paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper the authors consider neural networks as data-dependent kernel machines and propose applying a distillation method directly on the pairwise kernel matrix of the models. Authors extend their setting into ensemble settings, examining both some theoretical aspects of the process, building upon the work of Allen-Zhu & Li (2020), as well experimentally. More specifically, authors provide experiments on ensemble distillation on CIFAR-100, as well as on dataset knowledge transfer on CIFAR-100 to STL-10 and CIFAR-10.",
            "main_review": "Authors nicely motivate their work and draw connections with recent theoretical findings. However, I have several concerns regarding this paper:\n\n1) Authors claim that \"We further use our theory to motivate practical considerations for improving student generalisation when using distillation with feature kernels, which allows us to propose a novel approach: Feature Kernel Distillation (FKD).\" In other words, this work does not only provide a theoretical discussion on feature kernel distillation, but also it is claimed that a novel feature distillation method is proposed. However, there is a vast amount of works that employ similar kernels (even if not explicitly drawing connections with kernels) and employ divergence metrics between the kernels for performing distillation. More specifically, the angle-wise loss used in RKD as proposed in [1] is probably exactly the same (or very similar ) as the formulation used in (3), PKT [2] employs a KL divergence loss based on similar cosine-based kernels (ref to Section 3.1 on correlation kernel, which seems like a cosine-based formulation), while even the work in [3] is highly related, since they proposed MDS-based distillation.\n\nOnly [1] is briefly discussed and included in the experimental setup. However, I think authors should very carefully discuss similarities among the proposed method and the related ones.\n\n[1] Park, Wonpyo, et al. \"Relational knowledge distillation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.\n\n[2] Passalis, Nikolaos, and Anastasios Tefas. \"Learning deep representations with probabilistic knowledge transfer.\" Proceedings of the European Conference on Computer Vision (ECCV). 2018.\n\n[3] Yu, Lu, et al. \"Learning metrics from teachers: Compact networks for image embedding.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.\n\n2) Furthermore, authors seem to also ignore a very large portion of the recent literature on ensemble-based online distillation methods:\n\n[4] Zhang, Ying, et al. \"Deep mutual learning.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.\nLan, Xu, Xiatian Zhu, and Shaogang Gong. \"Knowledge distillation by on-the-fly native ensemble.\", NIPS2018\n\n[5] Chen, Defang, et al. \"Online knowledge distillation with diverse peers.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 04. 2020.\n\nAuthors heavily discuss topics related to ensemble distillation, so I think that recent works on the topics should be discussed and - most importantly - included in the experimental evaluation.\n\n3) On the theoretical side, I am not sure that similarities can indeed provide useful knowledge in datasets with many classes. For example, would the authors expect their method to scale when 1000 classes are used? What kind of kernel should be employed in this case? In my experience, such methods tend to degenerate when the number of the classes go up, since the discriminative power of most kernels goes down. Therefore authors should better define the application area of the proposed method and/or provide relevant experiments. \n\n4) It is unclear if the proposed method is expected to work when activation functions other than ReLU are used. \n\n5) Is the proposed method only applicable when performing distillation from the last fully connected layer?",
            "summary_of_the_review": "Overall, this is a well written paper, easy to follow and the proposed method is - mostly - sound and backed up with some theoretical evidence. However, given the amount of literature ignored, together with the high similarity of the proposed method with many existing ones, I am very skeptical of this paper at this current form. Also, authors should provide more in depth experiments to evaluate how the employed kernel behaves when more classes are used, as well as when there are architecture changes. In other works, the scope and application area of the method should be clearly defined and possible limitation clearly discussed. As a minor point, I feel that some claims are quite bold, e.g., \"This enables us to *prove* that KD using only pairwise feature kernel comparisons can improve NN performance in such settings\". ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}