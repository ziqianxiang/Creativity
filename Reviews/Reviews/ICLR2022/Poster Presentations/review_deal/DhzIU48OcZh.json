{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper introduces a prompting technique for eliciting factual knowledge from frozen pretained transformer LMs. The key idea is to modify the embeddings produced by the embedding layer before they are passed to the first attention layer and the paper investigates several different design choices. The Reviewers all agree that the paper tackles an important problem with interesting methods, that it is well written and has strong results. The main concerns, raised by Reviewer jddf, were about clarifying the connections to the robust optimization literature and evaluating on OOD relations. The former has been addressed in the revised version. While the latter point remains valid, I find that the paper in its current state has enough useful experiments and analysis to warrant publication. The authors have clarified most of the other points raised by the reviewers in their rebuttal."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper addresses the problem of robustness for extracting factual information from large language models. It first describes and motivates the problem of inconsistent predictions of large language models on fact-seeking prompts when these prompts are perturbed or rephrased.  It then proposes a few different methods for addressing this inconsistency that operate on the same portion of the language-model, namely, between the input token embeddings and the first hidden layer of the language model. The work evaluates the performance of the variants using a pooled collection of fact-seeking prompts (e.g., LAMA, LPAQA and ParaSel). The results employ a consistency metric and show that different interventions in the input embeddings cause large differences in inter-prompt consistency.\n",
            "main_review": "### Strengths of the paper:\n\n- The problem of extracting factual and consistent information from large language models is of high interest to the NLP community. Given how LLMs dominate NLP at the moment, making sure these models are robust and consistent is a timely problem,\nThe paper is overall well written, with only a couple of confusing parts (see below),\n\n- The proposed architecture for intervening between the input embeddings and the first hidden layer of the language model is quite comprehensive. I enjoyed seeing the different options, and in particular, thought the use of the MoE for relation classification to be quite insightful,\n\n- The experimental analysis of the work is well executed, and demonstrated convincingly which interventions were most useful in make predictions more accurate and consistent,\n\n- I liked the analysis in Figure 6, showing the importance of the subject entity on the precision of the fact extraction task,\n\n### Weaknesses of the paper:\n\n- The main weakness in this work is one that relates to the overall goal of fact extraction from language models. The “Oracle'' results from Table 1 are thought provoking: with perfect knowledge regarding the predicate/relation of test examples, and a subsequent 100% consistent response, the LLM is only able to obtain ~50% correct responses from T-Rex, which is an admittedly limited evaluation (41 “head” predicates, mostly of well known entities). While I understand that this work is clearly focused on the consistency issue, not necessarily correctness, it puts into question whether fact extraction from LMs is a worthwhile pursuit.\n\n- I would have liked for the paper to dig a little deeper into this headroom question from the previous point.  Would it be possible to conduct a sampled qualitative evaluation of errors of the Oracle model in the ID cases?  Are the errors due to unseen triples during training time (e.g., not in Wikipedia), or maybe there are issues with model capacity (maybe a 10x version of the LM would be able to recall the prompted fact)?\n\n- In terms of writing, the most confusing section in the paper is Section 4.1. After re-reading it twice, I was still not able to ascertain: (1) what data was used to train the models, and (2) what data was used to evaluate the models. The section makes reference to LAMA’s T-REX, LPAQA, ParaRel, as well as augmentations using BERT lexical replacements, as well as data from “Shin et al, 2020”. The section also talks about examples from these sources as well as templates (presumably filled in with WikiData triples?). I really think this section needs to be rewritten and the training, eval and test datasets should be much more precisely described.  I would also encourage authors to release the exact datasets and splits to allow others to reproduce/improve on this work. But even with a data release, a precise description of how this data was constructed is very important.\n\n- For the MoE and Oracle layers, the description in the paper is insufficient to determine the outputs presented to the first layer of the model. The depiction in Figure 2 hints that the entire sequence is rewritten using the fixed-length learned embeddings, and perhaps the subject or MASK embeddings are preserved? But actually sub-section 4.2 never formally describes how the embeddings are used to create the continuous prompts? Are they prepended/appended to the original inputs? Or do they rewrite the original inputs? Do either the MASK or subject tokens get copied?\n\n- The LAMA benchmarks have one unfortunate characteristic: since it was constructed for BERT-style single token prediction, it has stripped down the original datasets (see the original version of T-Rex, which contains over 600 unique predicates vs. the 41 from LAMA: https://hadyelsahar.github.io/t-rex/  and https://aclanthology.org/L18-1544.pdf ). I wonder if a more comprehensive version of this would be to evaluate on a larger sequence-to-sequence model like BART https://arxiv.org/abs/1910.13461  or T5 https://arxiv.org/abs/1910.10683 (both available as HuggingFace models). Given that this work leverages frozen LLMs, it seems that training and evaluation could be done relatively cheaply even for larger models with proper decoders.\n\n\n### Other comments:\n\n- With respect to the MoE solution, the paper claims that the model does not use a weighted combination and opts to use the top-1 predicted relation. I wonder if authors have tried using a weighted combination instead? If the relation classifier is trained with cross-entropy softmax loss, most of the weights will be close to one-hot (similar to top-1) except when the model is uncertain. Therefore combining prompt embeddings may yield some benefit over top-1. Does this make sense?\n\n- Note sure this is a good idea, but: given that the LLM is frozen, it seems plausible that the continuous prompt embeddings learned in some of the models resemble existing embeddings from the original vocabulary. As such, would it make sense to attempt to “decode” the continuous prompt embeddings into the existing vocabulary? One could use a greedy decoding strategy of extracting the nearest neighbor (via dot product or cosine distance) from each continuous prompt embedding to the vocabulary input embedding table. Have the authors tried  inspecting the continuous prompts in this way? I wonder if the output is informative or whether these prompts are modeling purely latent variables.\n\n- Typo in Figure 1 “Canada si” -> “Canada is”,\n\n- Typo in page 6: “Cannonical”  -> “canonical”\n",
            "summary_of_the_review": "The problem of extracting factual and consistent information from large language models is of high interest to the NLP community, and this work in particular should be of interest to the ICLR community. Overall, this work was well-written throughout (easy to follow in most places except for a few rough parts detailed above). The experimentation work was also of high quality, with interesting results. To highlight a few findings: (1) the use of a relation-classification MoE and its consistently high performance on consistency metric seems promising, (2) the analysis demonstrating the importance of the “subject” is correct fact prediction, and (3) analysis demonstrating the negatives effects of uniformizing objects in train/test sets, which is strong indication that LLMs still do not generalize well to unseen objects.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents an approach for extracting factual information from LLMs, where authors discuss a user oriented setting where they take in varied natural language prompts and return the objects with information needs. Authors proposed an approach, denoted P-Adapters, which is a model that is between the embedding layer and first attention layer of LLMs. It takes LLM embeddings as input and output prompts used to query the LLM. \n\nAuthors also presented a Mixture of Experts models that learn a set of prompts and select one to query the LLM. Experimenta results show that P-Adapters perform comparably to the more complex MoE models in extracting factual information from BERT and RoBERTa while eliminating the need for additional annotations.\n\nThe model and training procedure are well described and results are promising. It is also great that the data used is in the Github repositories. This said, the reader could benefit from better error analysis, as it is not clear how much hallucination and grounding the model is producing.\n",
            "main_review": "\nAuthors also presented a Mixture of Experts models that learn a set of prompts and select one to query the LLM. Experimenta results show that P-Adapters perform comparably to the more complex MoE models in extracting factual information from BERT and RoBERTa while eliminating the need for additional annotations.\n\nThe model and training procedure are well described and results are promising. It is also great that the data used is in the Github repositories. This said, the reader could benefit from better error analysis, as it is not clear how much hallucination and grounding the model is producing.\n",
            "summary_of_the_review": "The obtained results and model itself will benefit the reader and researchers working in this important research area.\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper explores methods for improving the consistency of prompt-based factual probing of pre-trained language models. The main contributions are (1) several methods for mapping natural language prompts to continuous prompts that empirically improve accuracy and consistency on a factual probing benchmark and (2) analysis comparing reasonable alternatives in terms of accuracy and consistency.",
            "main_review": "Strengths:\n* The problem setting--mapping natural language prompts to more optimal continuous prompts--is interesting. Most prior work on prompting attempts to find optimal prompt templates for each task (or in this setting, KB relation type), with the assumption that the task label is known at test time. Methods like P-Adapters for learning a transformation of natural language prompts could be an interesting alternative to standard prompting (with a fixed template per task) and fine-tuning, with possible applications to meta-learning/transfer learning.\n* The authors compare methods with different assumptions about the availability of annotations, and under different kinds of distribution shifts.\n* The paper is clearly written and the proposed methods are sensible and easy to understand.\n\nMajor comments:\n* The paper does not draw a connection to prior work on robust optimization (for example, see references in [1]), which offers a more principled framework for formulating the objective of invariance to a class of input perturbations. At the very least, it would be good to give a formal definition of robustness/consistency and cite this line of prior work.\n* The paper does not provide a clear justification for why P-Adapters would be expected to improve consistency, and nothing in the training objective encourages the model to be consistent. The argument might be more convincing if you could compare it to a training objective that does explicitly promote consistency--for example, by encouraging the hidden representations of different prompt paraphrases to be similar.\n* As this paper notes, the factual probing benchmark has class imbalances and represents a very particular use case, so while P-Adapters appear to improve consistency here it’s unclear if these results will generalize to other settings. Would it be possible to evaluate P-Adapters on a wider variety of tasks, following prior work on prompt optimization [2, 3]? For example, [4] provide 100 prompt templates for a wider variety of NLP tasks. The paper would be more compelling if you could show that P-Adapters improve consistency in other settings too.\n* Much of the motivation of this paper is based on assertions about user preferences, but there is no human evaluation to validate these claims. For example, would a typical user prefer to give a natural language input but get an inconsistent response? Or would they rather pick a pre-defined relation type with better promise of consistency? In particular, it’s not clear why user preferences would place any restrictions on training.\n* MoE model: How accurate is the relation classifier? There is a big performance drop in the “OOD Prompts” setting, which leads me to wonder if the relation classifier was adequately trained. Either way, relation classification results should be included and discussed in the main paper--a perfect relation classifier would lead to 100% consistency.\n* All of the models suffer a performance drop on the “OOD Objects” setting, which indicates that the models have over-fit to the (imbalanced) distribution of entities in Wikidata. It’s unclear how to interpret consistency in this setting, because the models are consistently producing the incorrect response. It might be more informative to report something like the “Consistent-Acc.” measurement from [5].\n* Table 2 does not compare results to any prior work. For example, can these results be compared to the “Consistency Improved PLM” results from [5]?\n\nMinor comments:\n* The metric called “precision@1” should be called “accuracy@1”\n* Table 2 does not explain how the results are aggregated (prior work takes either the micro- or macro-average over relation types).\n* The description of the methods and the illustration (Figure 2) are somewhat unclear. Is there one MLP applied at each position? Three MLPs? The meaning of the different-colored tokens could also be explained in the caption.\n* The paper is missing some implementation details, such as the size of the BiLSTM.\n* The results section (section 5) might be easier to read if it were divided into subsections or paragraphs.\n* It would be interesting to see the accuracy breakdown by relation type.\n\n[1] Sinha, Aman, Hongseok Namkoong, and John Duchi. \"Certifying Some Distributional Robustness with Principled Adversarial Training.\" International Conference on Learning Representations. 2018.\n\n[2] Shin, Taylor, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. \"Eliciting Knowledge from Language Models Using Automatically Generated Prompts.\" EMNLP. 2020.\n\n[3] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. GPT\nunderstands, too. arXiv preprint arXiv:2103.10385, 2021b.\n\n[4] Gao, Tianyu, Adam Fisch, and Danqi Chen. \"Making pre-trained language models better few-shot learners.\" ACL. 2021.\n\n[5] Elazar, Yanai, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schütze, and Yoav Goldberg. \"Measuring and improving consistency in pretrained language models.\" arXiv preprint arXiv:2102.01017. 2021.",
            "summary_of_the_review": "The paper proposes methods for mapping natural language prompts to continuous prompts, with the goal of improving accuracy and consistency on a factual probing benchmark. The problem is interesting and the method appears to work on this benchmark, but I there are three main changes I would like to see before recommending this paper for acceptance, possibly at a future conference:\n1. Drawing a connection to prior work on robust optimization, and providing a clearer formal justification for why this method will improve consistency.\n2. Providing more detailed empirical results and discussion (in particular relation classification accuracy). The current results are hard to interpret because models can be consistent but inaccurate, and because models can perform well by over-fitting the entity distribution.\n3. Ideally applying the method to a wider range of prompting tasks, to show whether the results will extend beyond the particular setting of factual probing.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper provides an interesting direction to extract factual information from large language models accurately and consistently. Since the quality of the factual information depends on the prompts used to query them, this paper tries to minimize the inconsistency by introducing an intermediate representation. To do this, this paper proposes an adaptor that uses the token embeddings of the natural language prompt as input and outputs continuous prompts that are used to query the large language model to extract factual information accurately and consistently.",
            "main_review": "Pros:\n+ This paper tackles one of the most important issues in the large language model: inconsistency results obtained from different prompts which have the same information needs. The problem itself is real and must be resolved.\n\n+ The proposed adaptor, P-adaptor is a simple but effective solution to alleviate the inconsistency.\n\n+ This paper is well-written and easy to follow. Furthermore, this paper provides comprehensive experiments including several qualitative analyses and discussions to show the effectiveness proposed method.\n\nConcerns:\n- Although the proposed method involves experiments in four different settings including OOD keyword error, it might be valuable to investigate an OOD syntax error setting. That is, in a real scenario, users write the natural language prompt which may have a grammatical error. Assessing the robustness of the proposed method in terms of grammatical errors can enhance the quality of the paper.\n\n- In Table 2, the performances of P-tuning in RoBERTa-large are better than the ones of Oracle. Without any explanation, it is not convincing. Instead of listing the result in the tables, it would be clearer if the authors provide more explanations about them although they are in the appendix.",
            "summary_of_the_review": "The proposed method is reasonable and analyses of experiments are well described.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The focus of the paper:\n\nThe paper proposes a way to increase the consistency of extracted information from the Large Language Models (LLMs) with respect to the different wordings of the queries that users use to prompt the models.\n\nResearch Directions:\n\nThere are two orthogonal directions that the authors explore. The first direction, is the use of adapters to tackle the aforementioned problem. The second direction is the reduction of supervision data that is needed to train and to test (query) the models. More specifically the authors advocate for the use of natural language prompts only. That is they do not require additional relation(s) that one can incorporate into the prompts to further specify the type of information that one can expect to be retrieved from the model. \n\nMotivation:\n\nThe first direction is motivated by two factors: 1) efficiency - the authors aim for a lightweight approach to increase the consistency and 2) to maintain all of factual knowledge learned during pretraining.  The second direction is motivated by user experience - the authors claim that the natural language prompts are more user friendly.\n\nContributions:\n\nPropose an adapter model - “P-Adapters” that rely only on natural language prompts during the training and testing. It has also been shown that the P-Adapters allows to increase consistency of the LLM to queries that are used to prompt the models.\n",
            "main_review": "+ves:\n+ Overall the paper is clear\n+ Use of LLM for extracting factual information is well motivated\n+ The motivation behind the use of natural language prompts to improve the user experience is generally clear. However, it can benefit from further citation of the literature in that domain. Having a few citations of works that compare the user experience with and without the additional information will make your argument in the paper stronger. \n+ Empirical results  suggest that P-Adapters improve the consistency\n\n\n-ves:\n- research questions are dispersed all over the paper. What I mean by this:\n\n    Abstract: you say that you tackle the inconsistency problem and you want to use a lightweight approach. Hence, I can see that there are two research questions: 1) inconsistency of LLM and 2) efficiency - how to tackle the inconsistency problem with a light weight model.\n\n    Introduction: you say that focus is the continuous prompts and user-friendly prompts. These are another two research questions.\n\n    Discussion: you hypothesising that the success of P-Adapter is due to not updating the LLM parameters. Such that it is likely to maintain all of factual knowledge learned during pretraining. Which to me is yet another research question - addressing the inconsistency of LLM with frozen weights vs fine-tuning. In Introduction you talk about the frozen weights of LLM but do not motivate why you decide to keep them frozen until the Discussion section.\n\n   Can you please state and motivate clearly your (full) research question somewhere in Introduction?\n\n- Lack of experiments:\n    1) (Minor) Why didn’t you train BERT fully with the proposed natural language prompts. You emphasise the importance of this experiment yourself in the Discussion section.\n\n    2) (Minor) Why didn’t you train (fine-tune) BERT with P-Adapters? This experiment will also allow you to answer your hypothesis in the Discussion section.\n\n    3) (Major) Did you try to freeze the parameters of LLM (or use an adapter) in the MoE model? You claim that MoE performs better than P-Adapters because of the additional supervision information (in form of relations)  the model gets during the training. With the present experiments it is hard to tell. Maybe MoE perform better because you also tune parameters of LLM?\n\n",
            "summary_of_the_review": "I cannot recommend the acceptance of the paper in its present state. There are two main reasons for this. The first reason is the lack of explicitly stated  (unified) research question. Without it, it is hard to tell if the authors have conducted enough experiments to support their research question. I did my best to identify these research questions in the paper and then try to link them to the experiment they conducted. However, I had a hard time doing this. Sometimes motivation given by the authors can be ambiguous. For example, do you use adapters just for efficiency or you are interested in exploring how one can address the inconsistency problem with the adaptors?   The second reason is the lack of experiments with the frozen LLM parameters of MoE. Depending on the outcome of the experiment the conclusions that are made in the paper may change. For example: “While MoE models usually perform better, the small increase comes at the cost of requiring relation annotations at training time and specifying the subject of the entity at inference time.”\n\nHowever, if those two issues are addressed, in my opinion, the paper has a good chance to be accepted. Reduction of  supervision data that is needed to tackle the inconsistency problem has both academic interest and also can be of the great interest to practitioners. Moreover, adapters are presently has high research interest in the NLP community - demonstrating how they can be used to address the inconsistency problem will also be valuable to the community.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}