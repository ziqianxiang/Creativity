{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper adopts ViT in the VQ-GAN framework replacing CNN, and achieves SOTA FID and IS scores. The empirical results are pretty impressive. It could benefit some practical applications. \n\nThe technical novelty is limited, but the tricks such as l2-normalization of codes are interesting."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper builds on top of VQGAN and DALL-E like models to achieve SOTA FID and IS scores for image modeling and unsupervised feature learning.",
            "main_review": "This paper builds on top of VQGAN and DALL-E like models to achieve SOTA FID and IS scores for image modeling and unsupervised feature learning. This paper is well-written and mostly clear, however a few clarifications would be helpful (see below).\n\nStrengths:\nthis paper achieves very good empirical results by combining various minor innovations based on first quantize the image representation followed by training a transformer based LM. The description of the architecture as well as the presentation of the results are very clear.\n\nCons:\nWhile the paper is clearly written, there do needs to be a few classifications. \n- For example, 'perceptual loss' is mentioned in several places in the paper but no definition or comment is made about what exactly is being used. It is better to have a footnote for the completeness of the paper. \n- In the introduction, it isn't super clear what are the main novelty of the paper over VQGAN and DALL-E, which also uses quantization and transformers. It would be better if contributions are outlined more clearly, especially w.r.t. the architecture in DALL-E.\n- Training time, hardware used are missing in section 5.1. How long did it take to train 500K steps, and what are the 128 accelerators? \n\nQuestions:\n- for Eq. 1 in section 3.2, is it not equivalent to get rid of the stop-gradient operators and just have a single term in the loss function, but multiply /beta to the weights of the encoder during training?\n- Are there implementation or code available accompanying this paper?\n",
            "summary_of_the_review": "This paper achieves very good empirical results and build on-top of several recent works on VQ + Transformers for image modeling. However the paper lacks some details and clarifications. Given the significant improvement of FID and IS, the paper could also do more to analyze which components of the proposed method was instrumental in achieving these gains.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work replaces the CNN encoder/decoder into ViT to minimize the distortion caused by quantization as minimal as possible. Experiments show that transformers learned on the discrete representation learned by ViT-VQGAN can generate high-quality samples, and provide discriminative features better than iGPT. ",
            "main_review": "**Strengths**\n* The manuscript is well written, and easy to follow. \n* Validating the discriminative power of autoregressive models on  ViT-VQGAN is a big plus. This shows the potential of generative models on classification tasks, just as in GPT. \n\n**Weaknesses**\n* In my opinion, comparing ViT-VQGAN of 32x32 latent dimension with VQGAN of 16x16 latent dimension in Table 6 is not fair at all, because the former is much slower than the latter in terms of sampling and training speed. This is my major concern in the experiments. \n* It seems that ViT-VQGAN marginally improves VQGAN (1.28 vs. 1.49) in terms of rFID. Can we say that this improvement is significant? In addition, I’m still struggling with the reason why ViT is better than CNN in VQ methods. Is capturing the global relationship between patches (as in the classification) significant in VQ methods? \n* It seems that the reproducibility is somewhat limited, because the detailed description on the computing environment is not presented. If the standard GPU machines having multiple V100 or A100 GPUs are used to train the 1.7B transformer with a sequence length of 1,024, it may not be possible to compute the gradient, even in the case of the single sample (batch size=1). I guess that the authors have tried many techniques, including mixed-precision training (obviously the standard technique), model sharding, gradient compression, storing Adam statistics in fp16 format, and so many other techniques. Unfortunately, I cannot find these details in the manuscript. \n\n**Additional comments**\n* The uncurated samples in Figure 2 and others in the supplementary are too smooth, which typically appears when we’re sampling with low top-k or top-p. Are these samples obtained by top-p=1.0 and top-k=8192?\n* The reference to Table 3 is missing. Considering the context, the sentence “Table 4 shows FID between reconstructed images and original images ...” should be revised to refer to Table 3.",
            "summary_of_the_review": "* I’m seeing the potential and practical benefits of the proposed method, even though it is a fairly straightforward approach by replacing CNN into ViT in the VQ-GAN framework. In addition, including the linear probing experiments is also a plus.\n* However, I have many concerns on the experiments and reproducibility. Without resolving these issues, my recommendation is borderline reject. \n\n== Updates after the rebuttal ==\n\nMy major concerns have been addressed by the response, thereby raising my score from 5 to 6. I really appreciate the authors reporting these additional results in such a short time.\n\nThough the empirical results of ViT-VQGAN are quite impressive, I still have a concern about the technical novelty. In the response to Reviewer 6eTH, the authors have argued that ViT-VQGAN is the first approach to consider vector quantization image modelling on both generation and recognition, while iGPT didn’t consider the vector quantization. However, according to the original iGPT paper (http://proceedings.mlr.press/v119/chen20s/chen20s.pdf) published in ICML’20 proceedings, they have reported the results of linear probing of iGPT on the top of discrete codes from VQ-VAE. Please revise the response to Reviewer 6eTH. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper mainly investigates how to further improve the image generation quality of previous work VQGAN, where several modifications are proposed to train a better quantized auto-encoder model. After modeling the discrete tokens with an auto-regressive transformer, we could observe that the generation results of the proposed ViT-VQGAN are really amazing and beat most of previous works in various benchmarks. Meanwhile, ViT-VQGAN also demonstrates its exceptional representation capabilities through unsupervised linear-probing.\n\nTo summarize, it seems there are several key factors to obtain a better reconstruction model:\n\n- Project the latent codes into lower dimension, which leads to a larger codebook size and less ‘dead’ codes.\n- StyleGAN Discriminator.\n- $\\ell_{2}$ normalization on latent variables.\n- Stronger backbone (ViT-like architecture or decoder with more parameters).\n\n",
            "main_review": "The performance gains, validated on different datasets, are very clear over state-of-the-art generative models, even compared with recent denoising diffusion methods. Meanwhile, the proposed modifications are simple and apparently effective. The paper is also well and clearly written.\n\nSome other questions:\n- Could you please give some explanations on ResNet-101 classifier-based rejection sampling? Is it the one used in VQVAE2 [1] and better than top-k sampling or nucleus sampling?\n- What is the intuitive understanding to explain why  $\\ell_{2}$ normalization is so vital?\n- Is the generation quality only bounded by reconstruction? If not, what are the underlying flaws of auto-regressive probability models?\n- Considering the fixed generation order of vector-quantized image modeling, I wonder if the new proposed reconstruction method  could combine with other transformer-based models like BERT, to further advance image inpainting tasks like [2]?\n\n[1] Generating Diverse High-Fidelity Images with VQ-VAE-2, NeurIPS 2019.\n\n[2] High-Fidelity Pluralistic Image Completion With Transformers, ICCV 2021. ",
            "summary_of_the_review": "Overall, the value of this paper is mostly experimental, or more like “bag of tricks”. The detailed analysis and explanation of why some components are essential are not stated very clearly, which may make the whole paper more interesting, but it indeed shines some light on the image generative model. Hence currently I tend to accept this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes vision transformer based VQGAN whose encoder and decoder are implemented as transformers rather than standard CNNs. It provides few improvements that demonstrates quantitatively better results in multiple datasets in unconditional / conditional image generation. It also shows how the proposed model can perform well as an unsupervised representation learning framework.\n",
            "main_review": "strengths\n========\n- VQVAE and VQGAN have shown impressive generative performances but they were still lagging behind CNN based GAN models such as styleGAN. This work reduces the gap between them and shows qualitatively good generations. \n- Although the model is a relatively straight-forward combination of ViT and VQGAN, its simplicity makes the model widely applicable\n- The tricks introduced to improve the model such as projection to low dimensional space for code index lookup and l2-normalization of codes are useful findings. \n- Strong quantitative results and ablation studies make the method convincing.\n- The paper is written clearly.\n\nweaknesses / questions\n==================\n- The claim for better codebook usage can be better supported by doing an analysis on their usage.\n- In Section 3.3 multiple losses are used for training ViT-VQGAN. It would be good to see some quantitative results for choosing such a weighting between different losses, and some intuition of why the logit-laplace loss contributes to better codebook usage. \n- For VQGAN results, is it using the same number of codebook entries as ViT-VQGAN? I could not find this information.\n- As ViT has shown better performance than CNN based encoder/decoder, a natural next question is using it for the discriminator also. Have you tried this, if not, why not?  \n- Regarding styleGAN discriminator, was the improvement in metrics due to the discriminator architecture or the training procedure of stylegan discriminator (e.g its regularization)?\n- Typo: the second last row in page 6, table 4 -> table 3",
            "summary_of_the_review": "I believe this paper introduces a simple and effective method for improving VQGAN using vision transformers. The findings in this paper and the model would be useful to the community.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}