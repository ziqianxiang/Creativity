{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper presents a method for inference in state-space models with non-linear dynamics and linear-Gaussian observations. Instead of parameterizing a generative model, the paper proposes to parameterize the conditional distribution of current latent states given previous latent states and observations using locally linear transitions, where the parameters of the linear mappings are given by neural networks. Under fairly standard conditionally-independence assumptions, the paper uses known Bayesian filtering/smoothing tricks to derive a recursive estimation algorithm and a parameter-estimation method based on a simple maximum likelihood objective.  \n\nOverall, the reviewers found the idea to be novel and interesting and I agree.  They also found the relation to the noise2noise objective worth highlighting. Several concerns were raised during the discussion period, which I believe the authors addressed satisfactorily. However, I think the authors should bring the assumed distinction between ‘supervised’, ‘self-supervised’ and ‘unsupervised’ upfront, as usually these types of models are trained using the noisy data (to which the authors refer to as unsupervised). \n\nGiven the large body of literature on dynamical systems, filters and smoothers, I believe the paper will benefit significantly from more comparisons across a wider range of (and more realistic) datasets."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors describe a parametrized inference approach for nonlinear dynamical models with linear observations. They borrow ideas from the well-known Kalman filter, and use Bayesian filtering and smoothing approaches to recursively estimate the states of the nonlinear model. The parameterization makes use of a locally linear transition between subsequent states using neural networks; thus the authors are still able to perform the Kalman-like filtering and smoothing.",
            "main_review": "Major:\n1) The authors keep calling the Kalman filter (and other variants such as the RKN) a 'supervised' method, but these methods do not need a 'ground truth' latent variables. They do, however, need a set of observations and a set of model parameters. It would be good for the authors to clarify what they mean and exactly what the differences are in their approach. \n2) Needs more validation on real datasets, and more visualization for how it performs on the one real dataset that the authors did use. The RKN is seen to perform better than the proposed approach- is this also true for forecasting? What is the comparison between the estimated states x from different models?\n3) The authors need to clarify the utility of their 'prior knowledge' - how much of the recursive filter results are due to this prior knowledge as opposed to the Bayesian filtering? Please separate out the contribution of the prior knowledge from the recursive filter.\n4) The figures could be made much better. Figure 2 could apply to any state space model; the novel ideas take a while to get to.\n\nMinor:\n1) Please detail the difference between 'recurrent' and 'recursive' earlier, when you first introduce the terms in Figure 1. I would also recommend changing the terms to terms that are not as similar.\n2) Please bold the values that perform the best in Table 2.",
            "summary_of_the_review": "The ideas in the manuscript are novel, but clarification is needed as stated in the Main Review. The figures need work. Moreover, additional empirical novelty, either further evaluation on the datasets already examined as a part of this manuscript, or on other datasets, is recommended.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes an approximate inference framework for state-space models with nonlinear latent dynamics. The approach uses the classical Bayesian update rules with neural network for estimating the parameters of local linear transitions.",
            "main_review": "I found the idea and its connection with the noise2noise approach interesting. However, I have concerns about the empirical evidence of the work and some comments on the structure of the paper:\n- Given that this area is an active area of research with a relatively large body of related work, I believe the related work deserves a separate sub/section. The current version of the paper has a list of related work in a long paragraph at the beginning of the paper. I suggest you move this to a separate section and provide a better overview of the relevant work (see “KalmanNet: Neural Network Aided Kalman Filtering for Partially Known Dynamics” for an example).\n-  Noise2noise method has been mentioned and used as a baseline, but no description has been provided for this related idea. Adding a few sentences or a subsection in the appendix can help the reader unfamiliar with noise2noise to connect the works better. \n- The “KalmanNet” paper, mentioned above seems to be closely related to the paper. As mentioned by the authors of KalmanNet, it can also be trained in an unsupervised fashion. Can you add the work to the \"related works\" section and describe the possible advantages of your recursive approach over theirs? \n- The experiments provide evidence for 1) model working in simple synthetic examples (passing sanity tests) and 2) model performing better than KF and EKF in both synthetic and real dataset. However, a more extensive comparison with other unsupervised methods is recommended. Given at least 9 other unsupervised methods in Table 1, adding at least one more unsupervised baseline to the audio denoting experiment can be helpful. If none of these methods is applicable to the experiment, justification can be provided instead. \n- The performance of SIN (Krishnan et al.) in figure 5 is a bit surprising to me. I’d expect a more gradual decline in MSE instead of a sharp drop. Can you provide some explanation for this behavior?  ",
            "summary_of_the_review": "I found the proposed approach interesting; however, I think the paper can be further improved in terms of format and empirical evidence. \n################### POST-REBUTTAL COMMENTS ########################\n\nI found the authors' response adequate; hence, I'm adjusting my score. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors proposed a new inference method for state-space model with nonlinear latent states. They showed that their inference model can be incorporated with domain knowledge and optimized in a self-supervised manner. They also demonstrated that their model can achieve competitive results through both simulated and real audio denoising tasks.",
            "main_review": "strengths:\n- The authors proposed a new inference model, and demonstrated the effectiveness of their method through solid quantitative comparisons in the experiment section. The paper is clear written.\n\nweakness:\n- Is it possible to show plots on the estimated latent trajectory for the real audio denoising data? (e.g. plots like fig.1 but for real data) I'm curious about what are the latent states look like for different noise groups? Are there any qualitative differences among the latent trajectory estimated for different methods besides the quantitative differences on MSE?\n\n",
            "summary_of_the_review": "The authors clearly state the contributions of their paper compared to the previous literature. The proposed inference method is novel, and the math derivations look solid. And the quantitative comparisons in the experiment results look competitive. Overall, I recommend this paper to be accepted.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "A Kalman filter typically requires specifying a state update equation, along with noise covariance matrices for state transition and observation. Determining these can be a challenge in certain scenarios. The manuscript proposes to replace these unknowns with trainable neural networks. The input to the neural networks are the observations, and the output are the unknown parameters (or their factorizations, to ensure positive definiteness). The manuscript validates the approach through numerical experiments.",
            "main_review": "The manuscript's proposal is reasonable and interesting, but I have concerns about novelty. Also, I found some of the discussion a bit confusing, and the overall organization of the paper can be further improved. \n\nBelow are more specific comments : \n\n1. Introduction : The opening paragraph suddenly loads a lot of references to the reader, without really putting them into a context. I would suggest selecting some of them and going into a bit of a more detail, if they are helpful in describing the approach put forth in the manuscript.\n\n2. Introduction : I was also expecting a more concrete description of the proposal in the introduction. You are proposing to replace some of the parameters/matrices that occur in the Kalman filter with neural networks, which is perfectly fine, and is a simple idea (simple=good!). It would help to show the Kalman filter formulation (i.e., what it's solving), and outline what you're doing differently, without having to go into heavy notation.\n\n3. Introduction : in the list of the contributions, for the first item : can you make that into a precise statement (like a proposition) and refer to it in the paper, so it's clear what you're showing? Are you referring to the development in Section 3? If so, I don't see what's novel with that development (other than replacing the said matrices with observation dependent estimates obtained through neural networks) -- can you elaborate?\n\n4. Eqn 6 : What's the difference between the first line and the second line? Are you just replacing $<k$ with $\\leq (k-1)$? If so, I'd suggest removing this equation. In fact, since $<$ and $\\leq$ appear as subscripts, it may be good to stick to either one of them, so as to avoid confusion.\n\n5. As I noted above, I think the proposal is simple, but I'd expect that training the neural networks might be a challenge in practice (if not, that's great). However, the idea reminded me of the paper titled \"AI-IMU Dead Reckoning\", by Brossard, Barrau, Bonnabel (https://arxiv.org/pdf/1904.06064.pdf) -- see for instance the comments right before section III.A. Can you point out the departure of your idea from what Brossard et al. do in their paper?\n\n6. Another reference, which might be good to place the proposal into context is \"Discriminative training of Kalman filters\", by Abbeel, Coates, Montemerlo, Ng, Thrun, 2005. That work is certainly different from what you're doing, but also aims to find a good fixed set of covariance matrices from data.\n\n7. Audio Denoising Experiment : Is it possible to train the network on a portion of the signal, and then stop training, so as to switch to \"test\" mode? More generally, I found the description of this experiment a bit terse. I'd welcome it if you can give a more detailed description, possibly using equations to mark what the input/output or the noise term etc. is.   ",
            "summary_of_the_review": "Overall, the manuscript discusses an interesting approach, and has wide application potential, given that it promises to facilitate the tuning of Kalman filters. However, I have concerns about novelty. I also think the paper could be better organized.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}