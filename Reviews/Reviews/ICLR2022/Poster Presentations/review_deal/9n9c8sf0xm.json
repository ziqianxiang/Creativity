{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper takes on (in my view) one of the most important questions in the lottery ticket literature today: how small are the smallest lottery tickets that exist in our neural networks? Many methods have been proposed for finding weak lottery tickets (those that require training to reach full accuracy) and strong lottery tickets (those that do not), but we have no idea how close they come to finding the smallest lottery tickets. Moreover, in many cases, we only know how to find lottery ticket subnetworks early in training rather than at initialization. Is this a fundamental limitation on the existence of lottery tickets, or is this simply a limitation of our methods for finding them? I am personally very involved in lottery ticket conversations in the literature, and I believe I can speak with some authority when I say that these are vital questions where any progress is important.\n\nMoreover, these are exceedingly difficult research questions, and (again, in my view) the authors should be commended for taking them on. A naive approach to these questions would involve brute force search over all possible subnetworks, which is infeasible even on the smallest of toy examples, let alone the meaningful computer vision tasks where lottery ticket work typically focuses.\n\nI am sharing all of this information to provide background for my confident recommendation to accept this paper over the many legitimate concerns expressed by reviewers and those that I saw when reading the paper in detail. Those include that:\n* This paper does not solve any of these research problems in their entirety.\n* It focuses on toy networks smaller than those traditionally studied in the lottery ticket literature, and it is well known that lottery ticket behavior changes in character at larger scales.\n* Planting good subnetworks may be an unrealistic proxy for the kinds of subnetworks that actually emerge naturally.\n* There may be multiple good subnetworks in a network, not just the one that was planted.\n* The graphs are a bit hard to read.\n* I find the mix of pruning methods studied, which were designed with very different goals (pruning after training, pruning before training, finding strong lottery tickets), a bit confusing.\n\n**The bottom line:** With all of that said, in my view, the paper asks good questions and provides an initial foothold that other researchers will be able to build on as we seek more general answers. This is similar to the contributions made by Zhou et al., which started the conversation on strong lottery tickets, and potentially even Frankle & Carbin, which kicked off the lottery ticket discussion but got many things wrong. Both papers were good first attempts at solving big problems, and both were highly influential despite their flaws. Similarly, even if this submission isn't perfect in every way, this is among the most important kinds of contributions that a paper can make. For that reason, I strongly recommend acceptance under the belief that this paper will help to foster a valuable conversation in the literature.\n\nP.S. I really, truly, strongly beg the authors to redo their graphs following the style of some of the more user-friendly lottery ticket or pruning papers they have cited (e.g., Frankle et al., 2021). The graphs in this paper were really hard to parse. Really really really hard to parse. They're too small, the y-axis is often squished, gridlines would be helpful, the lines are overlapping in ways that are difficult to distinguish because the colors blend, etc. etc. This is quite possibly the biggest impediment I see to this paper's ability to have broader influence."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors note a distinction between kinds of sparse networks in the literature. \"Weak tickets\" require training to perform comparably to the original network, while \"strong tickets\" do not.  The authors prove a lower bound for the probability that a strong ticket exists, and note that instead of training, this \"strong ticket\" need only be scaled by some constant to achieve similar performance to the larger network.  Using the insights of this proof, the authors propose three benchmark tasks to find a ground truth sparse network.  They evaluate common pruning methods to find both weak and strong tickets using these tasks, and find that most methods perform well on 2 out of the three weak ticket finding tasks.  They also note that the only method designed specifically for strong ticket finding performs well on these tasks.  For further evaluation, they additionally share weak and strong ticket finding results for VGG-16.\n\n\n",
            "main_review": "The authors provide an interesting new avenue for evaluating pruning methods, with the use of ground truth sparse networks.  This contribution relies on a theoretical result, which allows us to place a lower bound on the probability of a strong, sparse network.  These both are useful contributions.\n\nFrom a practical, empirical perspective, I do have concerns about the impact of this result, as the authors note that strong networks are currently still difficult to find.  Moreover, I also have some concerns that the outcome of using ground-truth strong networks largely confirms prior work (Frankle et al., 2021, Ramanujan et al. 2020) and yet relies on much smaller networks for evaluation than the networks evaluated in prior work.  The VGG-16 task is presented as a bit of an afterthought and is not central to the experiments, so I am uncertain how this planting algorithm can be scaled to similarly larger tasks. At the same time, I share the author's hope that using ground truth strong networks will spur new methods for the finding of strong, sparse networks.\n\nMy suggestions for the paper are mostly cosmetic in nature.  First, the inclusion of Zhou et al 2019, would provide a fuller picture of the history of research into strong lottery tickets, as Ramanujan et al. 2020 write that their work was inspired by the results shown in Zhou et al 2019.  Additionally, the description of \"confidence\" in the plots is not only vague, but not visible based on the format of the plot markers.  These markers are too large for the current \"confidence\" metric, which is not explicitly described in the paper.  Organizationally, information from A2, A3, and B1 were most helpful to me personally in understanding the paper.   In fact, I found section A3 and B1 more useful to understand the experiments than sections 2.2 and 2.3. I believe that content from these sections would better help the reader if they were presented in the main sections of the paper.  Whitespace used in the main paper could be reorganized, and figures 1 and 2, I believe were less helpful in understanding the paper and could therefore be moved to the appendix.\n\n----\n\nBased on the additional information provided by the authors in response to reviewer feedback, I would also like to increase my review score.\n",
            "summary_of_the_review": "Overall, this work I believe would be a useful contribution to the community.  I think however, further streamlining of the manuscript to improve clarity would strengthen the paper.  First, citation of Zhou et al https://arxiv.org/abs/1905.01067 would strengthen the related work, as  Ramanujan et al. note the influence of the paper on their work.  Second, re-plotting the figures so that \"confidence\" is both clearly defined (percentile value for the confidence is not defined) and visible within the plots, would allow the reader to better interpret the plots on their own.  Third, information from Appendix A2, A3, and B1 are useful experimental details and would help the reader if they were presented in the main section of the paper instead of the appendix.  In fact, I found section A3 and B1 more useful to understand the experiments than sections 2.2 and 2.3. Whitespace used in the main paper could be reorganized, and figures 1 and 2, I believe were less helpful in understanding the paper and could therefore be moved to the appendix.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "There are no ethics concerns.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper argues that one reason evaluating the strong Lottery Ticket Hypothesis is difficult is the lack of ground-truth tickets. They circumvent this by embedding a winning ticket in the weights at initialization and evaluating how well different methods can recover it. ",
            "main_review": "I think the idea of embedding hidden tickets inside a network to evaluate lottery ticket hypothesis is interesting, but this paper is let down by the mismatch between the proposed method and experiments section:\n\nThe derived lower bound applies to strong lottery ticket hypothesis but the experiments section mostly focuses on pruning-at-initializaton methods such as SNIP, GraSP, and SynFlow that were clearly designed for a different (e.g. weak LTH) setup. Given that one of the claimed aims of this paper is to analyze the ability of state-of-the-art pruning methods in finding tickets, the authors should elaborate on how the insights from their approach are transferable to these methods. The authors do note this gap but claim that the insights are similar to known trends in image classification and therefore the insights are transferable. However, I'm not sure how this setup can be used to make any new claims about pruning-at-init methods.\n\nI'm also not convinced about the three designed tasks in the experiments section. The authors claim they construct tickets for tasks that represent \"typical machine learning problems\" but these tasks are not typically used in the LTH literature. It would be great if authors discuss how these are relevant to the image classification task that LTH literature mainly targets. Some citations to where similar tasks are used would also be great.\n\n### Lower bound on existence probability\n\n- The authors prove the lower bound for MLPs but say it can also be applied to convolutions. Does this include architectures that include BatchNorm and/or residual connections. Given that these are the most commonly used architectures, the authors should discuss in more detail how the proof applies to these models. \n- I think it would be great if authors could provide plots that give better intuition for theorem 2 in terms of what kind sparsities one can expect for typical architectures. Perhaps as a function of in-degree and width of the parent network? Basically it's not immediately obvious to me how tight these bounds are.\n- The authors exclude the last layer from the strong lottery ticket hypothesis and mention it is common practice. Could you cite some of the references where this is common? I believe the original paper by Ramanujan did include the last layer (and BN layers). It would be great if the authors could also provide ablations to show how much this assumption skews the results?\n\n### Planting the ticket\n- This seems to be a core idea of the paper but I don't think it's well described in the paper. I think the authors should elaborate on the role of scaling factors in the method. I'm assuming the whole point behind them is to do the actual \"hiding\" part of the method but that aspect has been somewhat lost in the current version.\n- I think a diagram could be very useful in explaining the ticket planting here.",
            "summary_of_the_review": "Interesting idea but there's a mismatch between the proposed theorem and the experiments.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proves the existence of strong lottery tickets and further develops a framework to plant and hide winning lottery tickets with desirable properties in randomly initialized networks to help analyze the ability of state-of-the-art pruning methods for identifying tickets of extreme sparsity.",
            "main_review": "**Strength:**\n1. This paper provides a framework for benchmarking different pruning methods on their abilities to identify strong/weak lottery tickets, which can provide rich insights for the lottery ticket community.\n2. The proposed framework is driven by theoretical analysis.\n\n**Weakness:**\n1. One major concern is what's the practical usage of the proposed framework in real-world tasks like image classification, considering the optimal solution is no more available. Can the framework still plant near-optimal solutions and provide useful insights in these cases? Although the paper analyzes some general trends, it's not clear whether such observations can be consistently scaled up to large-scale datasets. \n2. The paper is not well-written and the logical flow can be improved. For example, it's not clear the \"lottery ticket\" in Sec. 2 denotes strong or weak lottery tickets, leading to many confusions. The wording in Sec. 2.1/2.2 can be improved with more structural logic. In addition, the title of Sec. 2 is not accurate as \"existence of lottery tickets\" has been discussed in the first lottery ticket work.\n3. Other questions: \n- For the problems proposed in Sec. 2.3, are there any references? It's not clear why they can \"reflect typical machine learning algorithms\" as described in the contributions.\n- It's a common practice of existing pruning methods to further fine-tune the pruned networks with inherited weights. In addition, a recent work [1] also shows that fine-tuning the identified tickets can achieve better results than retrained lottery tickets or rewinded lottery tickets, which also aligns with the common practice in network pruning. Can the proposed framework indicate what's the best pruning algorithm with the best final accuracy beyond the scope of lottery tickets?\n\n[1] \"Lottery Ticket Preserves Weight Correlation: Is it Desirable or Not?\", N. Liu et al., ICML'21.",
            "summary_of_the_review": "Given the concerns about the practical usage of the proposed framework, I tend to deem this paper marginally below the acceptance threshold. I'm willing to adjust my scores if the concerns are addressed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors identify that contemporary methods for finding very sparse subnetworks in deep neural networks (DNNs), including both methods for finding either \"weak tickets\" (after training) or \"strong tickets\" (before training), do not find very sparse (and good) solutions. They question if this is a fundamental limitation (i.e. these very sparse solutions don't exist), or if this is a limitation of current methods in finding such solutions. To answer this question, the authors propose to either \"plant\" known good very sparse subnetworks in DNNs, or to try and find solutions for toy problems for which the authors could identify good hand-made very sparse subnetworks - including a classification problem (Circle), function regression (ReLU), and manifold-learning problem (Helix) . The experimental results analyze the performance of strong and weak sparse subnetwork search methods, including GraSP, SNIP, SYNFLOW, alongside magnitude and random pruning in finding these planted tickets. None of the strong ticket methods are able to find the \"planted\" tickets for any sparsity level, while the weak methods are able to find tickets, but not at extreme sparsity. Further analysis reveals layer collapse in particular to be a problem. The authors conclude that rather than a fundamental limitation, current methods (in particular for strong tickets) are limited in finding very sparse tickets even if they are known to exist at that sparsity level.",
            "main_review": "Strengths:\n- Well written paper overall, with clear motivation, experimental method and interesting results/conclusions.\n- Proposed idea is, as far as I'm aware, novel when applied to sparse neural networks.\n- The experimental setup is overall very well thought out (save the potential issues below), and interesting. It is complementary to the many papers that explore this topic in real models/datasets empirically.\n- The paper presents a novel take on the ability to find good sparse subnetworks before and after training, with the results suggesting that the state-of-the-art methods for finding \"strong\" (before training) subnetworks do not perform as well as those for finding \"weak\" subnetworks even in for these toy problems — results matching the conclusion of other recent works in the field, but from a very different angle.\n- Common wisdom for the lack of good results at high sparsity is, I believe, that at extremely high sparsity, there are just not enough weights to solve the problem. However this work shows that in cases where we know there are extremely sparse solutions, and we have enough weights, these solutions are still not found; it seems in part due to layer collapse. This result is actionable, and provides an interesting research direction for improving sparsity in neural networks, can we find methods that better avoid mode collapse and are able to discover these \"planted\" tickets, and does this generalize to real-world problems?\n\nWeaknesses:\n- The abstract, unlike the rest of the paper, is relatively poorly written and does not do the paper justice as it is. For example, \"for which we lack ground truth information\" is very ambiguous, this should read, \"for which we lack knowledge of ground-truth solution subnetworks\" or something similar. I would suggest the authors replace this with an abstract based on the first paragraph of their conclusion, which presents a much better summary of the paper than the current abstract.\n- This work shares some similarity with \"Large Automatic Learning, rule extraction and generalization\", Denker et al., 1987. In that work small neural networks were trained on the toy binary problem of identifying binary sequences as either having \"one\" or \"two or more\" clumps/clusters. The solutions were compared with ground-truth human proposed solutions much like in this work. *However* that work showed that dense NNs do not necessarily identify weights/structure (i.e. subnetworks) that humans would identify/hand design, but they do find equally performant solutions (that weren't as compact) — an important finding that seems relevant to this work, and suggests an important, but missing issue with the author's experimental setup: **Can dense NN find these hand-designed tickets/solutions?** Perhaps this is not a restriction of sparse DNN search/training algorithms, but a problem with NN training in general?\n- I would have liked to have seen the result from dynamic sparse training methods for finding weak tickets (such as RiGL), which are state-of-the-art in (weak) post-training sparsity. In particular the ability of DST methods to potentially avoid layer collapse would make them an intestine avenue to being explored (just as the authors explored iterative methods). This is a major weakness in the evaluation in my opinion.\n- It's unclear how much the results on these toy problems have bearing on the issues in finding subnets for real problems/datasets, the authors try to address this concern, but not enough that the reader can discount the issue — I found the experimental setup for planting subnetworks found from real models into randomly initialized models less convincing however.\n- The authors should also cite previous work looking at training NNs on toy problems where the solutions are known (not necessarily the above reference, it's just the first that came to mind).\n- There is very little space in the main paper dedicated to the results, with the results only being presented in 3 figures that are relatively small, although there are a lot more results in the appendices.\n",
            "summary_of_the_review": "This is overall a very well written paper (strangely with the exception of the abstract), with a clear motivation and experimental method that I believe is novel in the sparse NN realm. The paper asks fundamental questions on the ability of contemporary methods for finding sparse subnetworks both before training, with an interesting experimental setup, and finds results (in the toy problem setting) that suggest these methods are lacking. Layer collapse specifically is identified as one of the problems leading to this issue, and the paper is somewhat convincing that the problem of finding very sparse solutions is algorithmic rather than a fundamental limitation. \n\nUnfortunately the paper loses some relevance in that it lacks any analysis of dynamic sparse training methods in addressing this problem, also it's not clear that the author's implicit assumption that a dense DNN could find these hand-designed solutions is true — perhaps this is not a restriction of sparse DNN methods but NNs in general. Nevertheless, overall it remains interesting and relevant overall, and I recommend its acceptance - although I encourage the authors to revise the abstract.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper develops a framework that allows planting and hiding winning tickets within a randomly initialized network and can be further used to assess the state-of-the-art pruning before training methods. Their empirical results of three common challenges are in line with the previous findings in Frankle2021. ",
            "main_review": "##########################################################################\n\nPros: \n\n(1) The paper gives a lower bound of strong winning tickets with the same depth as the target network, which reduces the dependence on the larger model depth.\n\n(2) The winning ticket planting is quite interesting and can provide ground truth for pruning before training. \n\n##########################################################################\n\nCons: \n\n(1) I miss the motivation of planting the winning ticket to the large network. How was the target network selected? Will this winning ticket still win after being planted into the larger network? A further experiment to validate the effectiveness of winning ticket planting is required. What's more, as shown in \"Stabilizing the Lottery Ticket Hypothesis\", the initialization might not be enough to guarantee the matching performance. If this planted ticket can not match the full accuracy, it is reasonable that various pruning methods e.g., SNIP, GraSP, achieve unsatisfied performance. \n\n(2) We are missing the experiments without ticket planting. Will the accuracy of various pruning methods remain similar or not? This will help us understand the role of ticket planting better.\n\n(3) It's not clear the novelty of this work to me. The empirical findings in the paper seem already be presented in the previous work Frankle2021, which includes larger architecture and datasets. Is the bound provided tighter than the existing works? \n\n(4) How are neurons matched in the lottery ticket planting? I suppose that the matching between the target ticket after training and the initial network is difficult.\n\nMinor typos:\n\n(5) Most quotation marks in the paper are used incorrectly, e.g., ’winning tickets’.\n\n## After rebuttal\n\nThanks a lot for the response! \n\nAfter reading the response, the motivation and contribution are clearer to me. However, I am not convinced about the effectiveness of ticket planting. I believe more elaborate sentences/figures/diagrams are necessary to help explain the whole process of ticket planting, as the core contribution of the paper. I notice that I am not the only one who is confused by this planting process. I encourage the authors to add the empirical results to support the effectiveness of ticket planting as well. Overall, I decided to increase my grading to 5.",
            "summary_of_the_review": "The motivation and the correctness of the lottery ticket planting are not clear to me. And the empirical findings in this paper are already presented in the previous paper Frankle2021.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}