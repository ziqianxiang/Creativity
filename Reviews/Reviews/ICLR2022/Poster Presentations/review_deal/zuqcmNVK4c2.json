{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper explores a classification approach based on labeling pairs of inputs concurrently using a single network, rather than singletons. The authors test the approach on adversarial robustness (towards norm bounded perturbations), OOD detection next to basic standard accuracy calculations.\n\nWhile the key idea is potentially interesting and the paper has received positive comments from the majority of reviewers, there were also some concerns that need to be addressed in a final manuscript:\n\n* The paper does not motivate or explain theoretically why the joint classification framework is superior, beyond verbose arguments. These\narguments need to be better clarified and linked with the experimental evaluation.\n\n* While the empirical results are perceived as positive by the reviewers, one reviewer has raised the concern about the comparisons. The adversarial robustness and OOD comparisons are indeed basic. The adversarial attack used here is quite a weak PGD attack with a small radius and low iteration budget. Possibly include stronger attacks. The OOD comparisons are with standard baselines only. Please include further comparisons.  \n\nIn its current form, the paper seems to be acceptable, and I strongly encourage the authors to improve both the theoretical justification, and empirical exploration in the final version."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "Paper propose a simple but effective method of learning. Feeding in a pair of data and learn the join conditional probabilities for predictions. The advantage is that the combination of data goes as the number of ways to partition the original data into pairs. This is a huge number.\n",
            "main_review": "Strengths:\n1. Simple idea that works very well\n2. Paper is easy to understand\n3. Formulation of the problem is well grounded theoretically\n4. Shows advantages over several different areas of deep learning\n\nAreas of improvements:\n1. While results are good, there is a lack of explanation on why the results are good. My sense is that the combinations of pairs of data span a much larger input space than to feed data one by one into the network. In a sense the amount of ‘independent’ data grows exponentially. This is a huge regulariser\n2. The data is not strictly independent since the authors feed different combinations of pairs. In any case, iid assumptions is not strictly enforce in many real world problems\n3. Icing on the cake would be having some theoretical proofs of properties of this network. Especially some regularising properties.\n\nQueries:\n1. How is the images being fed in? Side-by-side or concat channel wise? Fig1 seems to suggest side-by-side.",
            "summary_of_the_review": "Good piece of work, can be enhance some more with more extensive analysis of the results. E.g. more explanations of the observations.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper introduces a pairwise loss function for supervised learning, named self-joint learning. Instead of predicting the conditional distribution of the label given one data sample as in conventional supervised learning, the proposed self-joint learning framework predicts the conditional joint label distribution of a pair of data samples. The paper proposes the corresponding constructions of the conditional joint label distribution and several strategies for making predictions in the inference phase. The paper shows empirical studies on dealing with overfitting, adversarial robustness, and detecting OOD data.",
            "main_review": "Strength:\n\n1. The main idea of modelling the conditional joint label distribution of a pair of data samples of the paper is straightforward. The proposal of the parameterisation of the conditional joint label distribution and strategies for making predictions in the inference phase is intuitive.\n\n2. The experiments cover a wide range of applications.\n\n3. The paper is well written and easy to follow.\n\nWeakness:\n\n1. I think it is analytically unjustified how or why explicitly modelling conditional independence can help deal with overfitting, adversarial examples, detecting OOD data, although the paper shows a considerable amount of experiments in various applications. For example, it is not clear where the robustness against adversarial attacks of the proposed method comes from. I would expect some theoretical or analytical study on this point. Also on this point, in Eq. (9), $Y_iY^T_j$ indicates the true labels of two samples, which is a one-hot matrix, without any label correlation. It is hard for me to understand why MLE in this case helps.\n\n2. Although the paper studies several applications, it is a bit hard to position the paper in the literature. Specifically, for each specific applicaltion, the paper mainly compares with the fundamental methods. But there is a rich literature in each application. For example, in adversarial machine leanring, there are many advanced methods that improve adversarial robustness without adversarial training, e.g. in [1] where an ensemble model is used. I'm wondering how the proposed method performs in the comparion with the state-of-the-art methods in their specific application.\n\n[1] Pang, Tianyu, et al. \"Improving adversarial robustness via promoting ensemble diversity.\" International Conference on Machine Learning. PMLR, 2019.\n\n3. It would be interesting to study the complexity (running speed) of the proposed method in both training and inference phases. As the proposed might need to sample multiple pairs of data to make prediction of one data sample, it is important to show how much addtional complexity would be added in the inference phase.\n ",
            "summary_of_the_review": "The paper studies an interesting idea and shows that the proposed idea improves some baselines in several applications. But I feel that it needs additional study on why the idea works and more comparison with more advanced methods.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The Authors propose a new way of training classifier models. Instead of classifying each i.i.d. example independently during training and inference, they jointly classify a pair (X1, X2) of them, returning a joint distribution of labels (Y1, Y2). The loss function is modified so that the model learns that labels Y1 and Y2 are conditionally independent given X1 and X2.\n\nThis approach is shown experimentally to lead to superior model performance for the same model capacity (measured as relative layer width). The Authors attribute it to the regularising effect of new training method. Additionally, joint training on pairs of examples allows the use of auxiliary unlabelled examples.\n",
            "main_review": "The empirical results look convincing, and the Authors' interpretation of the method as regularising training sounds convincing. The term Y_i Y_j^T in Eq (9) means that maximising the MLE (8) fully would lead to a classifier which evaluates each sample independently. Hence, classifying two samples at a time matters only if MLE is not fully optimised. Hence, it's a regularisation scheme, as the Authors say. If the model architecture does not ensure the factorisation of the h() function by design, the model has to competing objectives to fulfill: get better at classifying samples correctly and get better at classifying them independently. Also, a training dataset containing N examples produces N^2 pairs of examples, which makes the training loop harder for the model. This means it's harder to overfit it.\n\nHowever, if the model architecture was big enough, it would probably overfit nevertheless, because it would be able to produce 100% conditionally independent label predictions and overfit each of them independently. This raises a question: could a similar effect could be achieved simply by training a smaller model, or stopping training earlier?\n\nSome parts of the paper are not clear:\n- In the Introduction, the Authors say \"By training with a pair instead of one sample, the model explicitly learns sample-to-sample relationships\". But if the labels are deterministic (as is most often the case), there are no such relationships (deterministic random variables are always independent). What \"relationships\" are learnt then?\n- When computing the gradient for training, are the pseudo-labels \\hat{Y} differentiated too?\n- How were the network architectures modified to support increased input and output dimensionality. Are the images concatenated together? The output layer must be larger (c^2 instead c logits), but what about the hidden layers?\n- How is the model capacity compared between standard and joint models in Figure 2? What does x = 10 mean on the X axis of the plot? Do both models have the same number of weights? Or the joint one has 2x as many?\n\nFinally, there are many small problems with writing and style, which should be corrected (e.g. \" using loss penalties and to use stochas-\nticity\", \" Even a well-trained model could benefit from having additional data available at the time of inference much, as a human expert can still benefit from having\", \"Although, these approaches learn...\", etc.)",
            "summary_of_the_review": "The presented method is interesting and should be accepted for publication. However, the paper could be improved and there are some open questions which should be investigated.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}