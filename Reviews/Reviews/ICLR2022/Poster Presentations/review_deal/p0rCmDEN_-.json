{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper explores the idea that fixational drift of a sensor over an image (something that primate eyes do) could be used to achieve visual hyperacuity, i.e. image recognition with low resolution images equivalent to what would be achieved with high resolution images. The authors construct networks where the bottom of a deep convnet is replaced by recurrent networks and the network is then trained on low-resolution versions of high-resolution images that are sampled with fixational drift across the image. The authors show that this approach allows their system (dynamical recurrent classifier, or DRC) to get much better classification performance on CIFAR images than can be achieved without the early recurrence and drift. The authors also show that the most robust classification mandates drift trajectories with higher curvature, and they show that this matches some of the properties of visual drift trajectories in humans. \n\nThe reviews on this paper were highly divergent (ranging from 3 to 10). Three of the reviewers felt this paper should be rejected, but one felt very strongly it should be accepted. The primary concerns from the negative reviewers were lack of appropriate controls, lack of insight into why the system works, lack of appropriate references to past work, and lack of connection to biology. The authors made a very concerted effort to attend to all of the reviewers' comments. They ran all of the requested control experiments, updated the text to better reflect past literature, and included some comparison to psychophysics data. In the end, only one reviewer increased their score, though, leading to final scores of 3, 10, 5, and 3. Discussion did not lead to any more consensus. \n\nThus, this paper was still very much in the borderline zone, and required AC consideration. After reading through the paper, reviews, and rebuttals, the AC felt that the authors really had addressed the primary concerns as best as could be hoped for in the time-frame for ICLR, and that the paper was sufficiently interesting and informative for ML and neuroscience to be worthy of publication. Some of the negative review points stand, e.g. there are still some mysteries as to why this works and there is certainly a lot more that could be done to make this paper informative for neuroscience. Nonetheless, in total, the AC felt that this paper deserved to be accepted, given that the authors did most of what the reviewers requested of them."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors train a neural network to do object recognition on downsampled, moving images of objects.  They show that by using a recurrent neural network in the early layers, it can learn to produce representations that result in recognition performance nearly as good as with static, full resolution images.\n\n",
            "main_review": "The overall thesis here is very interesting, however there are numerous concerns:\n\nOne of the main claims of this paper is that dynamic retinal input combined with recurrent processing is key to how the brain manages to do hyperacuity.  The results of Table 1 seem at first glance to support this.  However none of the baseline models considered the most important control:  what if you just feed in a series of static images without motion to the DRC-FE?  As is, all that we can conclude from this paper is that a recurrent network in the early stages somehow helps, but it is not clear that the motion in the input image has anything to do with this.  In fact, if the motion did help, it would beg even more questions.  The 8x8 images were created by downsampling from the 32x32 images with bicubic interpolation - essentially smoothing or lowpass filtering.  If you simply move and resample a lowpass filtered image, there is no new information that can be exploited by later information processing, assuming that it was lowpass filtered below the nyquist rate for an 8x8 image (which presumably it was, an important detail that is missing) - this is given by basic signal processing.   It seems plausible that recurrent computation in the early layers helps - it is essentially like making a deeper network - but it would appear the effect has nothing to do with the motion in the input.\n\nThe paper seems motivated by neuroscience and psychophysics, but there is very little attempt to tie anything about the neural architecture of the model to substrates in the brain.  For example it is mentioned that neurons exhibit temporal dynamics with phasic responses, but none of this is incorporated in the model.  This seems like run of the mill deep convnet engineering as opposed to neuroscience.  I'm not sure what we learn here from a neuroscience point of view.\n\nThere is no overall theory presented as to how the brain could benefit from motion of the sensor in building a higher acuity representation enabling tasks such as hyperacuity.  There is much verbal reasoning in the introduction, however there is now much engineering and mathematical know-how about how such problems can be solved - e.g., super-resolution.  These works are mentioned at the end in the discussion, but then almost immediately dismissed because they reconstruct the image rather than doing recognition.  This is a shame because the theory behind these models is exactly what the authors need to implement their idea.  Instead, all of the requisite established theory is tossed aside and the authors resort to training a neural network to solve the problem, yielding a non-transparent solution providing little insight into how the brain might actually solve this problem.\n\nThe introduction does not properly attribute prior work.  First, Rucci et al have been writing and talking about the benefits of image motion for more than a decade now, but you wouldn't know this by reading the intro.  Although Rucci is cited, it is about drift motion in general and not with regard to his theory of *why* image motion is helpful, which is well known in the vision science community.  Burak's (2010) important earlier work is cited but misattributed as providing an account for how how drift motion could improve acuity, which is wrong.  Burak's model shows how the cortex could disentangle shape from motion from retinal spike trains so as to recover shape information on the retina, but does not address the question of why the motion may be beneficial to begin with.  Also missing in the intro is any mention of Ratnam et al. (2017) and Anderson et al. (2020).  Those works are brought up in discussion at the end, but given the high degree of relevance of these prior works to the authors' thesis it is baffling why they are not brought up earlier, especially with regard to what the authors hope to do here that goes beyond or improves upon this prior work.\n",
            "summary_of_the_review": "An interesting idea but implementation is problematic.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "none\n",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper takes inspiration from the biological phenomenon of fixation drift, slow, low-amplitude movements during fixation that are believed to result in hyper-resolution in human vision. They hypothesize that this phenomenon can be explained by a model that has a recurrent convolutional front end that integrates over fixation drift, feeding into a well-trained back-end from a conventional model (ResNet 50). They demonstrate that this \"Dynamical Recurrent Classifier\" (DRC) is capable of restoring performance on 8X8 images to nearly the performance on \"high\" resolution 32X32 CIFAR images (actually, no one would call 32X32 high resolution!). They analyze the representations learned by the model and show they have strong spatio-temporal features, with some learned features emphasizing spatial features, some emphasizing temporal features, but most combine the two. Finally, they show that using curved trajectories improves performance over more random walks, which can potentially explain recent results in humans. They suggest this model can be useful in AI applications involving limited resolution but with multiple samples over time. ",
            "main_review": "This paper is well-written, proposes a highly innovative model that is consistent with behavioral and neural data, and obtains excellent results. The paper addresses a long-neglected aspect of human vision (fixation drift) in neurocomputational models of human vision, and shows that it has efficacy in challenging conditions. They don't stop at demonstrating that the model improves accuracy over a static model that uses multiple images. They develop a method for assessing the dynamical features of their model, because the usual activation maximization technique doesn't work in this setting. Finally, they demonstrate that a recently discovered phenomenon, curved paths in the fixational drift, promotes higher classification accuracy.\n\nThe front end of the model is a two-layer, recurrent convolutional network. It is trained by feature distillation from a layer of ResNet 50. The ResNet 50 is pre-trained on imagenet and then fine-tuned on either CIFAR-10 or CIFAR-100. The recurrent net is provided with 8X8 images, and trained to match the features activated by the 32X32 versions in the ResNet. The inputs are shifted slightly based on dynamical difference equations with random perturbations that determined the x,y coordinates of the next input, simulating fixation drift. The network was trained to reproduce the activations of the teacher network after 5 or 10 inputs. Then, the output of this front end was input to the remaining layers of the ResNet50 network, which was then fine-tuned to improve performance. The baselines are quite reasonable: a network trained directly on the 8X8 images, the same network, but using the average prediction over the 5 or 10 images, a ResNet + RNN network trained on a sequence of 5 8X8 images, with or without positional information. They show that with increasing number of inputs (5 images or 10 images), performance of the DRC improves, while the static network flatlines at 5 images. Positional information also improves performance. In the end, a 10-step DRC network with positional information achieves performance nearly as good as the original 32X32 ResNet50 in both CIFAR 10 and CIFAR 100. \n\nThey then go on to analyze the features. They perform the usual gradient ascent procedure to obtain maximally-activating 32X32 inputs for the features of the ResNet50 network used to train the features of the DRC network. They find that this same procedure doesn't converge for the DRC network, so they have to invent a novel technique for finding the optimal features. They use the idea of the generative network (Nguyen, et al., 2016), modified for their setting. The generative network has to learn to generate a *sequence* of 8X8 images that maximally activate the DRC features. These resemble the corresponding ResNet features they were trained on, but obviously have dynamics. To evaluate the spatial and temporal aspects of these units, they also apply the same procedure, but only allow the generative network to generate one image that is repeated, giving the best spatial activation of the feature, or, they only allow the generative network to vary the images, but all the images have to have the same pixel everywhere, giving the best temporal activation, but without form. These activations generally aren't as high as the unconstrained optimization. It took me a while to parse Figure 2B, but once I figured it out, it was reasonably clear. \n\nFinally, they set up the fixation location dynamics in such a way that they can control the curvature of the drift. They find that more curvature in the drift dynamics, the better the accuracy. In fact, an enforced \"spiral\" dynamics gives the best results. It turns out the performance data is based on this model, which is about 4% better than the less constrained model. This is interesting because it accords with recent human data from Michele Rucci's lab that finds curved drifts are used by subjects when the recognition problem is challenging. (I haven't read that paper, so I don't know how faithful they are to Rucci's data, or that this correctly describes his results).\n\nWeaknesses, with concrete, actionable feedback\n\nThe weaknesses are mainly in the exposition: I had several clarification questions:\n\nIt is unclear what the representation of the positional information is.\n\nIs the RNN an LSTM network?  \n\nIn general, I'm confused about the role of \"Small-net\" in this paper. Please clarify.\n\nThe procedure by which the generative network determines the optimal features is not clear - this could be described more clearly. The supplementary material is insufficient in this regard. You have an unused half-page in the main text, so that should be enough room to elucidate how this is done. \n\nMinor comments, wording, etc.\n\nPage 1, 3rd line from the bottom: dominate -> have dominated\n\nFirst sentence in section 2.1.1: -> We applied a feature distillation learning paradigm...\n\nNext paragraph: therefor -> therefore. Spell check!\n\nAlso in this paragraph, I initially thought you were saying you applied feature distillation to an 8X8 layer using 56X56 features, which is not what you did. This is one of those places where the role of Small-net is unclear. \n\nstackup -> processing stack\n\ncosyne -> cosine\n\nOur model was mostly implemented in *the* Keras package ... with *the* convolutional GRU...\n\nIn the sentence beginning \"The accuracy of the reference (teacher)...\", it isn't clear which entry in the table you are referring to here. I believe it is \"Naive training\", so call it that in this sentence.\n\nMiddle of page 6: \nsaptio-temporal -> spatio-temporal. Spell check!\n\nproperty -> properties\n\nVarious places: use two left apostrophes (below the tilde on the standard keyboard instead of \" in LaTeX on the left side of a word. (e.g., \"spirals\" near the bottom of page 6).\n\nIn Figure 2B, you say you are showing predominantly temporal, predominantly spatial, and mixed examples here. If that's the case, I would expect one call-out to be from the far left point in the upper-left hand corner (predominantly spatial - your choice is reasonable here, but the point to the left of it would be even better), a point in the lower right-hand corner (predominantly temporal), and then the third one you show. The point you use from the lower left hand corner corresponds to 0 temporal and low spatial. So, there isn't a \"predominantly temporal\" example here. Can you pick one from the lower-right hand corner instead?\n\nThird line of Figure 2 caption: students -> student's\n\nthird line from the bottom of page 7: reported at Tables... -> reported in Tables...\n\nlast sentence in Figure 3 caption: wors -> worse. Spell check! Don't annoy your reviewers!\n\nWording suggestion for Discussion: \n\nThis setting is novel and has been hardly addressed in the...->\nThis setting is novel and has been mostly neglected in the...\n\nmiddle of page 8: stack-up -> architecture\n\nlast word in third paragraph from the bottom of page 8 is not the one you want!\n\nprepossessing. -> preprocessing step.\n\nFurthermore -> Furthermore,\n\nto idealistic -> to the idealistic \n\nFirst line, last paragraph: it sets -> our work sets\n\nlast sentence, last paragraph: This is not really a sentence in english. \nRewrite as: This is enabled by a solution...\n\ncaption of supplementary Figure S4: The second sentence is garbled. It needs a \"right\" somewhere, or \"compared to\"\n\nI don't know what \"same\" means in the padding column in your supplementary tables. Same as what?\n",
            "summary_of_the_review": "This paper is well-written, proposes a highly innovative model that is consistent with behavioral and neural data, and obtains excellent results. The paper addresses a long-neglected aspect of human vision (fixation drift) in neurocomputational models of human vision, and shows that it has efficacy in challenging conditions. This result suggests that it can be used in engineering applications where the stimuli are low-resolution. It has some confusing parts, but these can be fixed by the authors. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper's main claim is that recurrence aids to enhance visual acuity in settings with limited resolution, such as the one imposed by limited photoreceptors in the retina. The authors therefore build a convolutional network with recurrent connectivity in its early layers (termed DRC) that receives a time-series of low resolution frames and learns representations -- for classification in CIFAR -- from a teacher network receiving full resolution inputs. DRC outperforms a low-resolution baseline and approaches standard resolution performance. Additionally, the paper visualizes the DRC's learned features.",
            "main_review": "pros:\n* as far as I can tell, this is a novel setting and I have not seen much work investigating the impact of low retinal resolution on object recognition models\n* the results on CIFAR-10 and CIFAR-100 are clearly described and show that the recurrent DRC model aided by a full-resolution teacher can regain most of the performance of a standard resolution model\n* the paper provides good background on the biological motivation for modeling low-resolution photoreceptors\n\ncons:\n* lack of connection to biology: the proposed model is motivated from biological observations, but model predictions are never tested against any experimental results. Are the model's resulting features any more brain-like? Does it exhibit the same hyperacuity as observed in biology?\n* requirement of a teacher: the DRC is only tested when learning representations from a teacher which both has a non-obvious connection to biology and is an unfair comparison to the non-recurrent baselines which do not use a teacher. Would any of the baselines perform better when trained with a full-resolution teacher in the same way as the DRC?\n* unclear benefits for computer vision: it is not obvious to me if/where processing sensory data with low resolution but many temporal samples will be helpful to the machine learning community. Some connection is made in the very last paragraph to always-on cameras such as body worn cameras but it is not made clear if those are really in the regime of low-resolution and high temporal sampling.\n\nminor: \n* some more discussion of related recurrent models, e.g. https://papers.nips.cc/paper/2019/hash/7813d1590d28a7dd372ad54b5d29d033-Abstract.html and https://www.pnas.org/content/116/43/21854.short, would be helpful to contextualize the work\n* the second-to-last paragraph on page 4 states that the ResNet+RNN \"achieved accuracy lower by 3.5% and 10%\" respectively for CIFAR-10/100, but Table 2 has its accuracy as 83.94/59.61 compared to the standard resolution 96.83/82.94 -- this seems to be inconsistent\n* it is not clear to me what to take from the visualization of features (figs. 2 and 3). In general, I found the paper a bit hard to follow at times, the consistent story is not clear to me. E.g. how do the feature visualizations support the main claim?\n* figure 3 caption has a typo: \"The wors case\"",
            "summary_of_the_review": "The paper lacks a clear demonstration of usefulness: either an improved fit to biological data (since the motivation starts from limited sampling in the retina), or a clear use case in computer vision. Since neither is demonstrated, I find it really hard to contextualize the work and cannot tell if the proposed model makes any improvements over previous models (see the main review for detailed suggestions). The use of a full-resolution teacher network is also not well motivated especially in connection to biology, and the second half of the paper is a bit hard to follow (i.e. what to take from the feature visualizations).\n\nREBUTTAL UPDATE: I have increased my score following the authors' attempts at connecting to biology more directly, but I still believe key comparisons are missing: either a stronger link to biology and concretely relating model predictions to experimental results, and/or explicit comparisons to alternative models in ML tasks.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Here, the authors attempt to leverage spatio-temporal computations for object recognition on the standard CIFAR-10 and CIFAR-100 datasets. In short, they use a network with a front-end of recurrent units (ConvGRU) to recognize objects given spatially jittered downsampled images – effectively approximating an active sensor. The network is trained in a student-teacher configuration, where weights in a temporal pooling layer after the recurrent layers are trained to match the weights of a feature layer inResNet50. Next, the network is fine-tuned to increase classification accuracy.\n\nAltogether, the authors are asking if spatio-temporal computations are enough to produce a feature layer similar to a larger network train on full-res images and, in turn, if this feature layer supports object recognition on par with full-res performance of ResNet50. The authors demonstrate that their network is almost as performant as ResNet50 with 4x downsampled images, especially when the down-sampled images are jittered in a spiral formation. They also present analysis demonstrating that the network is in fact performing spatio-temporal calculations.",
            "main_review": "Strengths:\n\n1. The use of an \"active sensor\" (i.e. jittering the input image) is an interesting idea that capitalizes on recent developments in neuroscience and psychology.\n\n2. On first blush the results are relatively strong (however with a caveat that more controls are needed to interpret them)\n\nMajor issues:\n\n1. A central claim made by the authors is that spatio-temporal computations *in the front-end of the network* are important. The main evidence here is that the ResNet+RNN network, i.e. putting the recurrent computations on the *back-end*, does not work nearly as well. In fact, ResNet+RNN appears to do no better than simply averaging the prediction of ResNet over 5 frames, which is surprising. This needs to be evaluated much more systematically, since it is not an apples-to-apples comparison. Why is the RNN only used *after* the global average pooling layer? Here, the DRC is using convGRU while the comparison is made with vanilla GRU units in the ResNet+RNN network, so they do not have access to spatial information. So really, the comparison is spatiotemporal computation for the DRC network and temporal only computation for ResNet+RNN.  It is also unclear how the ResNet+RNN network incorporates spatial information since I could not find the parameters related to \"Input Trajectory\" in the appendix. It is also unclear how the ResNet+RNN network was trained.\n\n2. Figure 2 demonstrates that the DRC network uses a mixture of spatial and temporal computation, but the results are under-analyzed.  Does the network produce a similar distribution across different random initializations? Does the performance covary with these distributions? What happens if units with specific criteria are ablated? Much more analysis is needed to be able to interpret the importance of what's shown in Figure 2. \n\n3. Another central claim is that the trajectory of images over time is important (Fig. 3). In general this result is under-analyzed and difficult to interpret as is. As k becomes more negative and the trajectories more curved, trajectories are likelier to remain closer to the center and have more overlap, yet I could not find any analysis of this. If indeed curvature matters, then the authors must show that curved trajectories are better-performing than other trajectories with less curvature but similar aggregate statistics (e.g. the trajectories are a similar distance from the center-point and have similar degrees of overlap). A very simple control here is to shuffle the trajectories over time, in this case the statistics should be the same, but the degree of curvature from point to point will be destroyed. If the DRC network performs just as well with the shuffle, then the overall statistics matter more than curvature.  This is essential to understanding what allows the DRC network to perform well.\n\nMinor issues:\n\n1. Please check for typos\n2. Figure 3 please provide a legend\n3. Figure 3 I do not understand why the authors are plotting an average of *2* datapoints.  Many more points should be computed to estimate the distribution properly, so we can visualize a reasonable confidence interval for each parameter setting.",
            "summary_of_the_review": "In summary, I found the core idea of author's network interesting; however, the author's claims are currently not justified by the results. Many more controls are needed to ensure that the DRC network performs better than alternatives. If the DRC network does perform better than all other control networks, then additional analysis is required to understand how the network is able to improve its performance.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}