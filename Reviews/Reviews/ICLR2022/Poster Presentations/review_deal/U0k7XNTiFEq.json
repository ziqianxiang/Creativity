{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper seeks to find an answer to some quite interesting research question: can deep vanilla networks without skip connections or normalization layers be trained as fast and accurately as ResNets? In this regard, the authors extend Deep Kernel Shaping and show that a vanilla network with leaky RELU-family activations can match the performance of a deep residual network.\n\nFour reviewers unanimously suggested acceptance of the paper. There were concerns about the clarity or marginal performance improvement. However, they all including myself agree: achieving the competitive performance with the vanilla deep model itself can be seen as a big contribution and the clarity has been improved to some extent through revision."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studied the problem of DNN training and generalization in vanilla architecture (without BN and Skip Connections in ResNets). It follows NTK theory and the approach of applying certain transformations to the activation functions. This work improves an existing work DKS, and solves its incompatibility to ReLU activations (\"Leaky\" ReLUs). This work introduces the necessary modifications to the Q/C map conditions for using Leaky ReLUs and shows empirical improvement over DKS or an easier method EOC on ImageNet.",
            "main_review": "Strong points:\n\n1. A study of an interesting problem with rigorous theoretical analysis.\n\n2. Both theoretical extension and empirical improvement.\n\n3. Solves the ReLU incompatibility issue of existing DKS work with new Q/C map conditions and LReLu, which also looks nicer than the the DKS's model class-preserving transformation.\n\nWeak points:\n\nI only have one major concern. The claims that DHS is not compatible are a bit too strong, at least the experiments (LReLu+DKS) did not show that way. At the end of Section 3, it says \"However, DKS is not fully compatible with ReLUs,\", which needs an exact explanation with the DKS condition.  DHS is compatible with LReLUs, and works pretty well, as in Table 7. So overall,  I feel some of the claims frequently mentioned throughout the paper are a little bit exaggerating. DHK, in their paper, claims quite differently, \"small decrease in generalization performance\" for removing BN, and \"similar results\" for removing skip connections. \n \nThe other claim is ** Using TAT, we demonstrate for the first time that a 50-layer vanilla deep network can nearly match the validation\naccuracy of its ResNet counterpart when trained on Imagenet.**. What \"for the first time, ..., nearly...\" means? I can tell, from Table 7, that  DKS is also very close, even with SoftPlus if you insist LReLU+DKS is your contribution. \n\n",
            "summary_of_the_review": "I would suggest the authors be careful about their claims. It appears to me that some extension from DKS motivated by the similarity between the model class-preserving transformation and Leaky ReLU is nice, but is not world-changing. \n\nI appreciate the detailed derivations. \n\nMy overall rating is 6.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper extends the Deep Kernel Shaping (DKS) method to Tailored Activation Transformation (TAT). As a result, deep vanilla neural networks with ReLU-family activations match the performance of deep residual networks.",
            "main_review": "Strength: (1) The paper is well-organized --- although the technical backgrounds are complicated, the authors try to introduce them in reasonably short sections. (2) The paper succeeds in increasing the performance of deep vanilla networks with ReLU-family activation, matching the performance of deep residual networks. \n\nWeakness: (1) The current draft is unfriendly to practitioners. Maybe the authors would like to include an algorithm box at the beginning to explain the proposed method (without understanding how to derive it). (2) Although the proposed method supports ReLU-family activation (TReLU), the performance gain is very marginal compared to smooth activations with DKS (at most 0.3%, Table 7). Thus, it raises the concern whether ReLU-family activation is needed. Is there any tradeoff using ReLU-family activation verse using smooth activation? ",
            "summary_of_the_review": "Although the authors have well-organized the paper, the current draft is hard to follow due to its technical complicity. Moreover, given the marginal improvement compared to DKS, a more extensive discussion on smooth and ReLU activations is desirable.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Based on the analysis in the concurrent work (DKS), this work develops a new set of conditions for the Q/C maps to set the non-trainable parameters for the activations (Leaky ReLU and transformed smooth activations), which enables training deep feedforward networks at comparable speed as ResNets on ImageNet, and achieves significantly better test accuracy than those obtained with EOC. ",
            "main_review": "This paper makes solid theoretical contributions on analyzing the initial conditioning of feedforward networks without shortcuts. Most of the analysis are derived directly from the concurrent work (DKS). It provides insights for proper initialization and configuring the activations for feedforward networks to achieve rapid training speed, improves our understanding of neural networks and has the potential to impact future design choices of them. Although the main point of the paper is to remove shortcuts, the analysis can also be applied to rescale the shortcuts of ResNets and achieve even better results than standard ResNets (with BN) when BN is not used (Table 4). \n\nI feel the only weaknesses of the paper are the clarity and the significance of the results. Limited by the pages, many important details are either abbreviated or require reading the 172-page DKS paper. The results of shortcut-free networks are still slightly worse than ResNets, and the merits of removing shortcuts do not seem to offset the drawbacks. A minor concern is the current analysis cannot be applied to Batch Normalizations and attention mechanisms, but I believe the theoretical contribution is enough.\n\nIn addition, I have following questions or suggestions:\n\n1. For each experimental result, how many different random seeds are used, and what are the standard errors of these results?\n\n2. Is training using K-FAC really faster than SGD? Most tables show better results achieved with K-FAC than SGD under the same number of epochs, and it has been shown in Figure 5 that K-FAC can be made 2x as fast without losing accuracy, but the wall-clock time comparison with SGD is not shown. How is the wall-clock time like for the 90-epoch K-FAC and 180-epoch SGD? \n\n3. The paper did not clearly specify how the weights are initialized. It is mentioned in Section D.6 in the appendix, but it was not clearly specified in Section 2.1. It would be better to include some form of pseudo code to specify the weight initialization and setting the parameters of activations. It is also not clearly described in Section 4.2 that the smooth activations are transformed.\n\n4. The method itself introduces extra hyperparameters that require additional hyperparameter search, such as $\\eta$. This increases the cost of hyperparamter search. Could there be some ablation studies on the impact of $\\eta$?\n\n5. How is the result like for even deeper networks, e.g., ResNet-152?\n",
            "summary_of_the_review": "This paper makes solid theoretical contributions on analyzing the initial conditioning of feedforward networks without shortcuts. The analysis can even be used to rescale the shortcuts of BN-free ResNets and achieve even better results than standard ResNets (with BN). However, the method also introduces an extra hyperparameter $\\eta$, and results of shortcut-free networks are still slightly worse than ResNets. The clarity of the paper also needs improvements.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper mainly discusses the training of neural networks without residual connections. To close the gap between residual-free and regular models, an activation transformation technique named \"Tailored Activation Transformation (TAT)\" is introduced.  Compared to the state-of-art method DKS,  the proposed TAT can yield better results for models using ReLU-family activation. Overall the motivation is well-discussed and sufficient ablation studies are provided. ",
            "main_review": "## Pros\n\n1. This paper is well-organized and easy to follow.  The main contribution of this work is to adapt DKS to models with ReLU-family activations. The authors have explored the reason why DKS is not fully compatible with ReLU activations, then providing reasonable solutions by changing some Q/C Maps conditions. The proposed methods can be easily applied to activation layers and only involve a few extra computations. \n\n2. Effectiveness of  the proposed TReLU has been demonstrated by a series of ablation studies. With K-FAC optimizer, the TReLU ResNet can maintain performance without any residual connections and normalization in some cases. \n\n## Cons\n\n1. The problem of removing residual connections on NN is interesting, and this paper has provided a good solution. However, existing methods seem not to be so bad as claimed in the paper. As discussed in the paper, the main drawback of DKS is incompatibility with ReLU-family activations. But as shown in Table.7, DKS+LReLU+K-FAC+ResNet50 seems to slightly outperform TAT counterparts, which means the mentioned incompatibility of DKS can be alleviated by using other methods like K-FAC optimizer and thus making TAT not so significant. Anyway, I thought DKS is a very recent work compared to this paper, and the proposed TAT can also yield comparable results.\nTherefore,  I still lean to accept this work.",
            "summary_of_the_review": "The paper provides sufficient discussion and experiments for improving DKS. The proposed approaches yield good results on models without residuals and normalizations.  However, when compared to existing DKS methods, the improvements seem to be somewhat marginal, thus restricting the significance of this paper.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}