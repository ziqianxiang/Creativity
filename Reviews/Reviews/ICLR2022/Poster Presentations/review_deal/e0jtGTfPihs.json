{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper builds on previous work on supermasks. It  proposes to replace binary masks by a signed supermask, i.e. a trainable, trashold-based mask that can take values from {-1,0,1}. This change (in combination with the use of ELUS activation functions and an ELUS specific initialization strategy) leads to a significantly higher pruning rate while keeping competitive performance  in comparison to baseline models. \n\nMost reviewers agreed that the paper is well written and that the proposed approach and the experimental findings are interesting. The motivation to improve interpretability was commonly perceived as misleading. Another  downside that was mentioned is the training time/efficiency. This however, should not be taken too much into account since the work focusses on finding the smallest possible subnetwork that still performs well (without changing the weight values) and- in line with work on the lottery hypothesis-  aims at understanding more about the structure of the „winning tickets“ which is interesting for itself. The paper therefore should be accepted."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This work extends to (Zhou et al., 2019) and (Ramanujan et al., 2019) to allow sign flipping in the supermasks, without updating the magnitudes of weights. The main technical contributions are the new thresholding-based training method and the new weight initialization scheme that considers the supermasks. Empirical results show better performance on small conv nets and also residual networks on CIFAR-10/100 datasets.",
            "main_review": "Firstly, this paper is written clearly and easy to follow. The main technical contributions are clear. Performance improvements over (Zhou et al., 2019) and (Ramanujan et al., 2019) are clear.\n\nHowever,  there are several concerns I have for this work.\n\n1. I think more detailed ablation study is needed for the experiments. For example, it is imaginable that the threshold tau will have an influence on the number of remaining parameters but the corresponding experiments are lacking. Having some will be nice. Also, the new training technique is always coupled with the new initialization method. Ablation study to decouple the effect of these two contributions will be good for the readers to understand the which one is more important. This is crucial because in (Ramanujan et al., 2019) the authors also investigated a certain form of scaling of the parameters which is found to improve the performance significantly.\n\n2. In the abstract, the authors posed this work as tackling the issue of difficult interpretation and training/usage in real-world applications of highly overparameterized networks due to their size. However, I don't think the proposed method save anything during training, e.g., memory, computations (in FLOPs) or time because it uses latent weights.\n\nMinor point:\n - In table 2, should the baseline accuracy for conv 8 be 82+% instead of 72+%?",
            "summary_of_the_review": "Overall I think this work proposes an interesting extension to previous supermask works but lack necessary experiments, making the results less convincing.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces \"signed supermasks\", which builds on top of the supermasks line of work of finding binary masks on untrained networks that result in good performance. This work extends the original supermask by adding the ability to flip the sign of the weights. The proposed method learns parameters of a mask, and converts the mask parameters into a tertiary mask through the use of two thresholds. The paper further proposes to use ELU activation function and a ELUS initialization scheme that is more tailored to supermask training. Performance is compared against fully-trained models and prior work on supermasks on MNIST, CIFAR10, and CIFAR100, and the proposed method shows competitive results.",
            "main_review": "**Strength:**\n\n- This paper is well written and the proposed method is clearly described. \n- The proposed method is fairly novel, and the resulting high performance and sparsity level of the Conv 2,4,6,8 models are inspiring. \n\n**Weakness:**\n\n- The paper motivates the signed supermask as a way to improve the interpretability of the trained model. While this would be very interesting, I find that the supporting interpretability analyses to be lacking in substance. The mask visualizations are limited to the first layer of a FC network on MNIST, and cannot be directly extended to more layers or other architectures. The observations of layer-wise pruning ratios are interesting, but are not new and have been well studied in the pruning literature. \n\n- Another motivation is in terms of efficiency due to sparsity and compression of the final trained model. To this point, I think it is more appropriate to compare performance to other tertiary network methods, as the supermask baselines have much less capacity due to maintaining the sign at initialization. Is signed supermask a competitive method for training tertiary networks? How does it compare to other tertiary methods in terms of the performance and efficiency tradeoff? Testing the limits of how barebones can a network be is an interesting question, and it may very well be the case that the signed supermask pushes this limit, but it needs to be made clear through a quantitative comparison to similar methods rather than just a discussion.\n\n- Comparisons to the other supermask baselines are missing in the ResNet models for some reason.\n",
            "summary_of_the_review": "This paper presents an interesting new approach of training tertiary neural networks with high sparsity levels. The paper is well written and several of the results look promising and would be of interest to the community. However, I take some issue with the motivations of this paper. As a method that is proposed to produce insights on neural network training and enable interpretablility of networks, I find a lack of new insights provided in the paper. As a method that improves efficiency and compressibility, I find it to be very intriguing and promising, but the paper currently lacks the appropriate performance comparisons with other tertiary neural networks to fully convince me of its advantages.\n\n### Update after rebuttal\n\nThank you to the author for the update. I agree that there is value in training the most bare-boned network that we can. Even though not all the value has been demonstrated in the present work, I think it will be useful in many, perhaps unexpected ways in the future. Although this is not really discussed, I'm particularly interested in how this form of training can act as a particular form of regularization that is orthogonal to most other forms. I'll raise my score from 5 to 8 and encourage the authors to continue in this direction.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper discusses the signed supermask that can improve the model accuracy of untrained neural networks significantly while enhancing sparsity compared to the original supermask idea. The authors introduce \"-1\" as an additional mask value to enable flipping a sign of an initialized weight and suggest related activation functions and fixed threshold hyperparameters to achieve sparse networks. Analysis on sparsity and corresponding accuracy is given for various CNN models including ResNet models. The role of batch normalization for large models is also studied.",
            "main_review": "Previous studies on supermask are interesting because they provide insights that even untrained models have chances to achieve reasonable model accuracy only if we can find such supermask in advance. This reviewer is not sure whether minor improvements over the original supermask idea are critical if a large amount of training is still required.\n\nIf the authors can reveal that the model accuracy can be significantly improved even for untrained models compared to previous supermask, it would be helpful to understand the inherent characteristics of the neural networks initialization. But as shown in Table 1, compared to the work by Zhou et al, even test accuracy is slightly degraded even with additional \"-1\" mask while 'normal training' is still required. The original idea of Supermask is meaningful because it shows a new insight that untrained model has already a sub-network of reasonable accuracy. If the authors want to claim that we can generate a new Supermask generation method, it would not produce additionally new insight. Such concern is obvious for residual networks. Without training batch norm parameters, it is unavoidable to see noticeable accuracy degradation. Then, batch-norm hyperparameters are known to be affine hyperparameters that are superior to normal weights in terms of expressive power. Since the existence of Supermask is already known, the focus of new research in Supermask would need to be finding Supermask efficiently with minimal efforts. Otherwise, Supermask would not be practical in the field. Unfortunately, as indicated in Table 1, training time to find Supermask is quite slow. Then, why we don't just perform usual pruning method to achieve better model accuracy?\n\nComparison on the previous works is missing. At least, this paper needs to include \"What's Hidden in a Randomly Weighted Neural Network?\" in CVPR 2020.",
            "summary_of_the_review": "Since this paper introduces only marginal improvements over original Supermask work, this reviewer cannot find significantly new insights from this paper. Supermask needs to involve much higher model accuracy (than previous works) or minimal efforts to be computed. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes Signed Supermask,  an extension of the original Supermask work (Zhou2019) for finding more efficient untrained subnetworks. Instead of learning a binary mask, Signed Supermask claims and shows that adding another dimension -1 to the masks leads to higher sparsity with higher accuracy. The main contribution of this paper is the introduction of weight flipping, as an improvement over the existing approaches. The method is simple and effective. The empirical experiments validate the effectiveness of the proposal. ",
            "main_review": "##########################################################################\n\nPros: \n\n \n\n1. The paper proposes a method to find untrained subnetworks that can approach the performance achieved by trained networks. The performance improvement achieved by allowing weights to flip is interesting.\n\n \n\n2. Overall, the paper is well written. Readers can directly grasp the main idea of the paper.\n\n\n\n##########################################################################\n\nCons: \n\n1. My main concern is the motivation and the usage of the proposed method.  As the authors said in the introduction \"However, due to their sheer size, the networks not only became difficult to interpret but also problematic to train and use in real-world applications...\", I expect a method that can either reduce the number of trained parameters or require a fewer number of FLOPs to be proposed. While the authors claim that the subnetworks discovered are untrained, I find that it requires training mask matrices with the same size as the model weights. Considering that straight-through estimator is used to estimate gradients, I suppose the backward pass is also dense and thus leads to no acceleration for training. Compared with directly training a dense network, I suppose the overall training FLOPs required by Signed Supermask is similar. The larger \"TT / Epoch\" over the baseline in Table 1 confirms my concern. I believe it is necessary to explain the benefits of Signed Supermask compared with training a dense network and prune or directly training a sparse network. With the sparsity learned by Signed Supermask, I believe even directly training a sparse neural network with static or dynamic sparsity (e.g., Mocanu et al [1], Evci et al [2], Liu et al [3]) can have similar performance, while with much fewer training FLOPs. I encourage the authors to clarify this.\n\n2. There are some related works that are missing in the paper, e.g., Diffenderfer & Kailkhura [4] (accepted by ICLR 2021) and Chijiwa [5] (accepted by NeurIPS 2021). Since the number of works on this topic is quite small, I expect a good submission should at least do a good related work job by introducing them. What's more, as they are aiming to search for untrained subnetworks without adding another dimension, comparisons with these works are encouraged. The current experiments only include comparisons with Zhou2019 and Ramanujan2019 with small convolutional networks, which is too small to draw a solid conclusion. For instance,  Ramanujan2019, [4] and [5] all provide results on large scale dataset ImageNet.\n\n3. As I understand, the performance improvement of Signed Supermask comes from (1) allowing weight flip, (2) ELUS initialization. I expect ELUS  is a universal method that can improve all the related works. However, I didn't see any ablation study of these two components. It is not clear to me if the improvement is caused by one of them or both of them. \n\n4. As mentioned by the authors, the two threshold hyperparameters in equation (1) control the sparsity level. I expect to see more experiments with how these thresholds influence the sparsity level and the corresponding performance of Signed Supermask.\n\nMinor typos: \n\nInstead of starting with a new paragraph, there is an inexplicable blank after some lines. E.g., 2rd paragraph on page 1; 1st paragraph on page 7.\n\nA space is missing before this sentence \"The gradient is estimated\" on page 3.\n\nReference:\n\n[1] Mocanu, Decebal Constantin, et al. \"Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science.\" Nature communications 9.1 (2018): 1-12.\n\n[2] Evci, Utku, et al. \"Rigging the lottery: Making all tickets winners.\" International Conference on Machine Learning. PMLR, 2020.\n\n[3] Shiwei  Liu, et al. ''Sparse training via boost-ing pruning plasticity with neuroregeneration.'' NeurIPS, 2021.\n\n[4] Diffenderfer, J., & Kailkhura, B. (2021). Multi-prize lottery ticket hypothesis: Finding accurate binary neural networks by pruning a randomly weighted network. ICLR, 2021.\n\n[5] Chijiwa, Daiki, et al. \"Pruning Randomly Initialized Neural Networks with Iterative Randomization.\" NeurIPS, 2021.\n\n",
            "summary_of_the_review": "While the performance achieved by untrained NN is interesting, the motivation and experiments need more work. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}