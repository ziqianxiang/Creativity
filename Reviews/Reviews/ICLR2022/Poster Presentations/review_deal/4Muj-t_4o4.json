{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper tackles the problem of generalizing to a new environment by learning a small set up anchor policies (even just 2 for the final approach) which span a sub-space that can be searched efficiently in a new environment.\nThe discussion and additional experiments managed to convince most reviewers that the method indeed works as the authors had hypothesized (especially regarding functional diversity). At the moment the analysis is mainly based on empirical observations, it would be good to also have a thorough theoretical analysis of the method."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The work proposed a method for training robust policies in RL by optimizing convex combinations of different policies, and showed that it achieves good performance against parameter variations in various Mujoco environments. ",
            "main_review": "While I do find the observations interesting, I am not convinced that the method is working for the reasons that the authors have formulated. The training of uniform samples of convex combinations of anchor policies, while regularizing the anchors to be as different as possible, is effectively searching for regions in the policy space that are flat -- the policies parameterized in this way are trained to perform well in expectation. The robust performance should come from the fact that with this region of parameters, the behavior of the system can tolerate small variations better in the first place, regardless of whether it comes from the environment dynamics or the policy parameter changes. One limitation is of course this search is unlikely going to be successful for hard control problems, where not arbitrary perturbations on the policy parameters would work, and in those cases the training itself may in fact perform badly. These problems can be hard to see in the Mujoco environments. \n\nThere are a lot of issues of using wrong mathematical terms in the technical part. First of all, the approach is not learning a \"subspace\" of the parameter space -- in the current formulation it is clearly not closed under linear operations. Also, \"simplex\" is reserved for the convex hull of n+1 points in an n-dimensional space. So taking a few samples in the policy parameter space, which by itself has a large number of dimensions, the subset described in the paper can not be called \"simplex\". In section 3.3, it should not be called a \"line\" which needs to be unbounded in either directions, since it's just a segment. \n\nOverall I think the authors made interesting observations, but the mathematical analysis is weak and does not fully explain the observations. ",
            "summary_of_the_review": "Overall I think the authors made interesting observations, but the mathematical analysis is weak and does not fully explain the observations. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper considers the problem of learning policies in a training environment that adapts fast to a different test environment. The proposed method learn a subspace of policy parameters that are optimal for the training environment, with the hope that one of these policy parameters will generalize better to the test environment than a single point estimate.",
            "main_review": "\nThe problem is interesting, and the proposed method is simple and apparently easy to tune. However, I am concerned about whether or not this method will scale to more challenging environments than the Gym benchmark tasks because of various qualities of the method I describe below. While this is a promising direction of research, I think that this paper requires some iteration before acceptance.\n\nStrengths:\n\n- The proposed method is very simple, and seems to be relatively simple to optimize hyperparameters\n- The empirical results seem to convincingly indicate that this approach outperforms prior work\n- I appreciated Figure 1a: it neatly conveys the main approach of the paper\n\nWeaknesses:\n\n- While the authors seem to try to argue otherwise, it appears that enabling generalization to new test environments requires diversity in behavior / state distribution ($\\pi(a|s)$ or $d^\\pi(s)$) and not diversity in parameter space (since if parameters differ, but result in the same function, then there is no benefit conferred). However, unlike prior work (e.g. Kumar et al 2020b), this method does not optimize for functional diversity, and only parameter diversity, and in fact there is nothing in the loss that prevents the objective learning the same policy (e.g. in a \"flat\" part of the parameter space). No analysis is done as to how functionally diverse the learned policies actually are in the test environments as well (it would be interesting to see that)\n- It seems concerning that the optimal instantiation of this method is with $n=2$ anchor points, effectively only leading to a \"line\" of policies, and not the more general subspace of policies alluded to in the introduction / title. While there is a little discussion with n=3 in the experiments section, I think the paper would be stronger if it analyzed more deeply why this approach does not do well with more anchors. As tasks get more complicated, it seems that the \"line\" of policies with only one factor of variation will be unlikely to capture the variation needed to perform well in a shifted test environment.\n- An important missing comparison is just learning an ensemble of policies (either w/ or w/out standard diversity based metrics), and using the best ensemble member in the test environment. While this (hopefully) should not outperform the proposed method, this comparison would be useful to understand how much the continuous space of policies helps the algorithm, over simply just having a diverse set of policies.\n- In the experiments section, it says that the hyperparameters for the comparison methods are tuned based on performance in the training environment. It was unclear to me if training environment performance was used for tuning $\\beta$ for the DIAYN comparisons, since later it also says that the results provided are for the best value of $\\beta$ (which seems to imply test env performance was used). I would appreciate this if this were clarified.\n- Is there a way to tune the method depending on how different we expect the test-time environment to be? Abstractly speaking, if we know that $D(\\mathcal{M}, \\hat{\\mathcal{M}}) < \\epsilon$, how should we choose the hyperparameters for this algorithm? It seems reasonable to expect our method to be more conservative when the test environment is expected to be far from the current one, and less conservative when not. My original reaction was that $\\beta$ in the equation after Eq5 may do this, but it seems in your experiments that the constraint $C(\\cdot, \\dots, \\cdot)$ goes to zero regardless, so the behaviour would not change for other values of $\\beta$? Please let me know if I am misunderstanding. \n- The particular structure of the expected reward objective in Equation 4 looks exactly like running policy gradients on a particular stochastic latent-variable policy (where $\\pi(a|s; z, [\\bar{\\theta_k}]\\_{k=1}^n)$ = $f_{\\sum_{k}z_k\\bar{\\theta}_k}(a|s))$). With that perspective in mind, I'd be curious to hear the authors intepretation of how the regularization in Eq 5 compares to other works that consider regularized latent variable policies, e.g. (Kumar et al 2020b)?",
            "summary_of_the_review": "(Copied from above) The problem is interesting, and the proposed method is simple and apparently easy to tune; however, I am concerned about whether or not this method will scale to more challenging environments than the Gym benchmark tasks because of various qualities of the method I describe below. While this is a promising direction of research, I think that this paper requires some iteration before acceptance.\n\n-- Post Rebuttal -- \n\nGiven the new experiments and analyses, I am increasing my score to a 6. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a method for rapid adaptation from a single training task to an unseen test-task in reinforcement learning. The method optimizes to find a convex subspace, specifically a line, of parameters that minimize the objective in expectation over a uniform distribution. For adaptation, the authors propose to find a convex combination that performs well on the test task.",
            "main_review": "The paper is generally well-written and easy to follow. \n\nThe proposed method is motivated by a robustness argument which is intuitive but could benefit from further exploration. While it is easy to believe that parameters that are robust to perturbations will perform well when the model is perturbed, this is not obviously true (in fact, one can easily construct examples where this does not hold). It is also unclear why this property would be preserved if the robustness is limited to a 1-dimensional line in parameter space. Additional explanations would be especially welcome as the baselines used in this paper are based on diversity and thus follow a different intuition. Furthermore, the authors should situate this work within the broader literature of robust reinforcement learning which has existed for decades. \n\nThe evaluation on multiple variations of control domains is promising and shows that the method can lead to improved adaptation with a relatively simple training procedure. The analysis on hyper-parameter sensitivity is welcome and it is good to see that the method does not require a lot of fine-tuning. Overall, the analysis focuses on relatively easy problems for adaptation and it would be interesting to see how this scales to harder tasks where a non-adapted policy fails completely and to analyze at which point the method stops being able to adapt. It would also be interesting to see a characterization of the learned sub-space, i.e. what kind of policies are being learned by the agent and how do they qualitatively compare to the kind of policies learned by diversity objectives. Finally, while increasing K leads to apparent improvements, it would also be interesting to see how the method compares to regular reinforcement learning when K=0, i.e. if we see a benefit from finding robust parameters, even if we do not optimize within the learned subspace. \n\nIn the paragraph on K-shot adaptation (Section 3.1), the authors claim that only one value of z needs to be executed in a deterministic environment. This is only true if the learned policy is also deterministic which does not seem to be the case in the proposed set-up. \n\nIn Section 3.2, the authors claim that the proposed method does not lead to learning sub-optimal policies at train-time. This statement is too strong as there is still a trade-off. In particular, the objective encourages the agent to find wide local optima and may forgo a better, but more narrow local optimum that it would otherwise choose.\n\nIn Section 2 on notation, r(s,a) should map to R, not R+.\n",
            "summary_of_the_review": "The paper is generally well-written and easy to follow. The problem formulation proposed by the authors is both relevant and important and the proposed method is simple to implement while showing promising results; however, the paper could benefit from additional analysis and more comprehensive literature review covering robust reinforcement learning.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper studies how to discover a subset of policies that will become useful for fast adaptation of future tasks. The subspace is defined as the convex hull of some anchor policies which are discovered, and are encouraged to perform well with respect to the extrinsic reward while being as different as possible from each other in the parameter space. The quality of the set is then evaluated by its ability to quickly adapt to new tasks. ",
            "main_review": "I enjoyed reading this paper, it studies an important problem and proposes new methods to tackle it. Thats said, I have some questions about the proposed new ideas in this paper and the connection to related work. I also have a few questions about the experiments. \n\nRegarding the line of policies. Let's assume for a moment that the anchor policies are deterministic, and consider the state occupancies of these policies. Using standard arguments (see [1] for example), we will get that any policy along the line (or the convex Hull) is a stochastic (or mixed) policy, and in addition, that for any reward signal, one of the deterministic policies will always be at least as good than any of the stochastic policies. That is, the anchor policies will be at least as good as any policy along the line. Now, there are a few differences between this construction and the approach taken in the paper, in particular, since the line is in the parameter space and not in the policy/state occupancy space. Nevertheless, I would like to see some evidence that the non-anchor policies are actually used over the anchor policies and to understand why that is. I am also very interested to see if the line in the parameter space extrapolates the convex Hull in the state occupancy space. This should be correlated with having policies along the line that outperform the anchor policies for some reward signals.  \n\nRegarding the experiments, the results are very impressive. One thing that I am missing is the data about performance during training time, and in particular, what is the average reward and diversity score (e.g. the cosine similarity or mutual information) achieved. This is important, since a potential scenario in Cheetah might be that a set that is more diverse but less good in terms of reward is leading to better fast adaptation. If this is the case (if one has to sacrifice reward to quickly adapt better) then I would like to learn that. \n\nOne baseline that I am missing, and follows the same line of thinking is a constrained MDP approach as was taken in [2,3]. The idea here is to set a constraint on how much a method will satisfy the extrinsic reward and then to maximize diversity under this constraint. Also note that there is a strong connection between the cosine similarity and the average reward in [3], but in [3] it is an inner product (not dividing by the norm) and being used in the successor features space instead of the parameter space. \n\n[1] Zahavy, T., Barreto, A., Mankowitz, D.J., Hou, S., O'Donoghue, B., Kemaev, I. and Singh, S., 2020, September. Discovering a set of policies for the worst case reward. In International Conference on Learning Representations. \n\n[2] Kumar, S., Kumar, A., Levine, S. and Finn, C., 2020. One Solution is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL. Advances in Neural Information Processing Systems, 33.\n\n[3] Zahavy, T., O'Donoghue, B., Barreto, A., Flennerhag, S., Mnih, V. and Singh, S., 2021, June. Discovering diverse nearly optimal policies with successor features. In ICML 2021 Workshop on Unsupervised Reinforcement Learning. ",
            "summary_of_the_review": "Nice paper, some questions about the proposed idea needs answering and some of the experimental results need further clarification. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}