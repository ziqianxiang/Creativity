{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This work proposes a new strategy for prioritized experience replay. It is based on the argument that the TD error itself may not be a good indicator for priority, so we should rely on other factors that are easier and more reliable to learn. The new method is based on two modifications: (1) modifying the critic's objective so that it learns a good model of the environment (reward and transition dynamics) as well, and (2) use the combination of the TD loss and the model loss in order to define the priorities in the ER queue.\n\nThe majority of reviewers are positive about this work. They believe the method is novel and the experiments are extensive. The authors improved the paper during the discussion phase, so many of the questions have already been answered.\n\nThere are some concerns though, some of them shared by reviewers and some after my own reading of the paper:\n\nOne concern is about the justifications for the method, which are based on the heuristic and intuitive arguments, rather than principled approach. Currently it is not clear, at least to me, why adding a model error to the objective is a good idea.\n\nAnother concern, which is not shared by the reviewers, is that there is much overlap in the confidence band of figures and confidence intervals of tables. For example, in many of the subfigures of Figure 2 or 4, there is a significant overlap in the shaded areas. Or many of the numbers in Table 1 (with and without MaPER) are within each others' confidence intervals. Are the results statistically significant?\n\nAnother comment, again not shared by reviewers, is regarding how the loss functions are defined. Consider the loss in Eq. (1):\n- Is there a squared missing? I assume that it is missing. Although it does not matter at this stage, when you add other terms to the loss, it would, i.e, the minimizer of $f(x) + g(x)$ is not necessarily the same as the minimizer of $f^2(x) + g^2(x)$.\n\n- Is the target value based on a fixed parameter (not optimized), or do you actually consider the expected of the TD error, which would be equal to the empirical Bellman error. If the latter, it would be a biased estimate of the Bellman error. And it is not what DQN or the TD method optimizes (that's why Sutton and Barto's textbook calls them pseudo-gradient).\n\nThese requires some clarifications.\n\nAnother question related to the model: Is it assumed that the model $T_\\theta$ is deterministic and predicts a next-state, as opposed to predicting a distribution over them? (cf. equations after (5) )?\n\nAll strengths and concerns considered together, I believe this is a good paper overall, and can be accepted at ICLR. Hopefully we get a better understanding of what this method is actually doing in the future research.\n\nI have the following suggestions to the authors:\n- Perform statistical significant tests on your results. In some cases, it might be helpful to increase the number of runs from five to a larger number. It may also be more visually clear to provide standard error instead of standard deviation.\n- Clarify issues about the definition of the loss function.\n- Please consider improving and clarifying your argument of why you method works.\n- Please consider the remaining comments by reviewers in order to improve your paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies the problem of experience replay. It proposes Model-augmented Prioritized Experience Replay (MaPER), a novel experience replay method, based on the intuition that the model is easier to estimate than Q-value. It also proposes a modification to critic network, Model-augmented Critic Network (MaCN), by predicting the reward and dynamics model additionally. Experiments on MaPER show that MaPER can be applied to both discrete action space (DQN) and continuous action space (SAC), both model-free RL algorithms (SAC) and model-based RL algorithms (MBPO), as well as sparse reward tasks, and improve the baseline algorithms a lot. ",
            "main_review": "Writing: In general, the writing is very clear. With that being said, I do have a few questions. \n1. In Algorithm 1, The index set $\\mathcal{I}$ is never used after initialization.\n2. When are the priorities in $\\mathcal{P_B}$​ updated? \n3. In Eq (10), why are all losses weighted equally? \n4. How is MaPER combined with MBPO? \n\nExperiments: The experiments are quite extensive. I do appreciate the experiments of applying MaPER on different algorithms. \n\nI would like to have another ablation to check the effectiveness of MaPER: Can we use MaPER without MaCN? For example, we have two separated networks, one of which predicts Q and another predicts the next state and reward. This method has a larger computational overhead than vanilla PER, but can better decompose MaCN and MaPER. \n\nDoes PER in the experiments (Fig 2, Fig 4) use (4) to stimulate exploration? If not, the comparison is not very fair. \n\nNovelty: The method sounds novel. The idea of MaCN is pretty natural so it might exist in some prior work but I'm not sure. Using model/reward prediction error for prioritized experience replay is indeed novel. ",
            "summary_of_the_review": "Given the novelty of the proposed method and extensive experiments, I like this paper and would recommend acceptance. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a new method for prioritizing experiences used in the prioritized experience replay (PER) method. The proposed approach is simple: the critic network also learns the reward function and transition function. The two errors (absolute value) are added to the absolute TD error to calculate the priorities in the PER method. The authors provide some intuitions to their method. Experiments on both Mujoco and Atari environments are presented to show the method’s effectiveness. ",
            "main_review": "Strength: \n\n1. The paper focuses on an important topic: what should be the sampling distribution for ER? And the proposed method seems new. \n\n2. The presentation is reasonably clear. \n\n3. The empirical results show that the proposed method is promising. \n\nWeaknesses:\n\n1. Although the proposed method is supported by some intuitions, it does not have a solid motivation. \n\nIn page 1, it is said “learning to predict … is difficult, so sampling based on TD-error is far from optimal.” What do you mean by “short term”? And why sampling from TD error is far from optimal? Any reference? \n\nThe effectiveness of PER is questionable …. The authors never introduce what these limitations of PER are. I think the limitations of PER should be reviewed in more detail in this work and should be explained how the proposed method can address these limitations.\n\nAn additional note, reducing TD errors faster does not tell us anything about the sample efficiency of learning performance. I personally have some experience with it. Reducing TD errors fast can still have a bad policy. For example, when your buffer only covers a small subset of the state space.  Hence, I do not understand the effect of showing figure 1(b). \n\nAccording to your intuition, during early learning, samples with large model errors would get sampled more frequently. The authors should provide some insight (either empirically or theoretically) to explain why such sampling distribution is preferred. I am doubting it has some exploration effect. There is also work saying that TD error magnitude can be used for encouraging exploration (Smart Exploration in Reinforcement Learning using Absolute Temporal Difference Errors by Clement et al), so it seems it is not a problem to use TD errors during the early learning stage. You might compare the sample space coverages during the early learning stage between your algorithm and regular PER. \n\nThe intuition is not persuasive. During early learning, how do you know the model error plays a dominant role? It seems domain-dependent. And, it is also doubtful whether a representation can describe the dynamics imply it can predict Q values. They can be quite different. In some environments, learning a model can be much more challenging (e.g., high dimensional), it is possible that the model error term always dominates the priority. \n\n2.  The contribution is incremental. Basically, it adds a model error term to the TD error. \n\n3. What if the transition probability p(s’|s,a) is not Gaussian, how do you measure the error? \n\nThe three losses can have very different numerical scales, how to handle this? \n\n4. The experiments are not sufficient. \n\na. What is the parameter sweep for each algorithm? PER is sensitive to the alpha, beta parameter in (8), (9)\n\nb. Do you ever try to use a separate NN for model learning (but still use the same priority as you defined)? This can help identify if the auxiliary tasks are useful or not. \n\nc. It would be interesting to see how the method works when increasing uncertainty in 1) reward function 2) transition function. This can help identify if the benefit comes from uncertainty measurement when calculating the model errors.  \n\nMissing citations: \n1. The credit of ER should go to Self-Improving Reactive Agents Based On\nReinforcement Learning, Planning and Teaching by Lin, 1992. \n\n2. Page 2, “we remark … ” There are indeed works improving RL’s experience replay by employing model learning. Please see the SGLD-based sampling in MBRL (e.g., Frequency-based search control by Pan et al.) I believe there is a category of work along this direction.",
            "summary_of_the_review": "Please see main review. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Authors introduced Model-augmented Prioritized Experience Replay (MaPER) as a variation to the canonical prioritized sweeping. The main difference is that they extended the critic network on its last layer to also predict the reward and transition model. Then they changed the prioritization of sample for replay to also include the error in the reward and transition model, in addition to the TD-error. Experimental results in various MuJoC domains showed the performance boost introduced by MaPER on top of existing state-of-the-art techniques. Furthermore Authors carried an ablation study, investigation the impact of 1) incorporating the MaPER while all methods used the same network 2) incorporating various error values for prioritization, and 3) increasing the network size.",
            "main_review": "Strengths\n=========\n+ The paper is well-written overall and is easy to digest.\n+ Through and strong experimental results\n+ Simple technique that is generalizable\n\nRoom for Improvement\n=========\n- Dive deep into the Private Eye domain. Why didn't the proposed approach work? I love to see that inside the paper.\n- While the additional complexity of the algorithm was discussed (N(1+|S)), it would be really great if they have compared the performance of their algorithm against the others, where the x-axis is wall close-time.\n- Provide a more holistic number across all domains for better sending the message out. For example, normalizing the return between 0-1 and then provide the average across all domains.\n\nDetails\n=========\n- Perhaps remove the acronyms in the abstract.\n- Add line number to your Algorithm.\n- I did not follow the priority set on line 5 of Algorithm. Can you elaborate ?\n- Figure 2, Table 2, Figure 4 captions: \"across five runs with randoms\" => \"across five runs with random seeds\".\n- Please avoid reintroducing the acronyms. Example MaPER in sec 3.1\n- \"Since LfIW was only validated it on\" => remove it\n- Page 8: \"Besides, the performance between\" => \"Besides, the performance difference between\"\n- What was unique about private eye? Any insights there?\n",
            "summary_of_the_review": "I liked this paper. The proposed approach is simple and generic (more sparse reward domains should be investigated for their final claim to be substantiated) yet practical. The experimental results are comprehensive and I think it would be a great value to RL practitioners.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "\n*Summary Of The Paper\nModel-augmented Prioritized Experience Replay\nIn this paper, the authors consider using the dynamics prediction error in the priority calculation.\nThe priority is then used to decide the probability for each sample during training.\nThe method is applicable to both model-based and model-free setting\n",
            "main_review": "*Main Review\n\nPros:\n1. The experiment section is very complete.\nThere’s a model-free part that considers some of representative algorithms such as SAC, TD3,\nand there's a model-based part that compares against MBPO.\nAnd experiments are implemented on both the continuous robotics tasks and atari games.\n\n2. The details of the paper are adequate.\nThe main paper and the appendix section provides a lot of the details of the algorithm,\nmaking it quite reproducible.\n\n3. Sufficient related work.\n\n4. Compared to the baselines, the authors show that the proposed algorithm has better performance.\n\n5. The code is released.\n\nCons:\n1. The algorithm is relatively simple.\nAnd there’s no theoretical evidence that supports the proposed algorithms,\nand the inspiration from this paper is not clear.\nAdding the dynamics terms in the priority seems pretty random.\n\n2. The sensitivity. \nIt is not clear how sensitive the parameters in the algorithm are.\nFor example $\\xi_1$, $\\xi_2$, $\\xi_2$ in equation (5) are three variables i think that could be quite unstable and sensitive. It would be interesting to see if the algorithm only works on 1 set of hyper-parameters, or it is in general a very robust algorithm\n\n",
            "summary_of_the_review": "\n*Summary Of The Review\n\nI am quite worried about the fact that the algorithm is looking quite simple.\nBut the results are looking good.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}