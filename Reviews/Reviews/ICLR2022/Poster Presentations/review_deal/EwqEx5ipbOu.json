{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper performs a comprehensive investigation on self-supervised pre-training with streaming data. Reviewers agreed that the task studied in this paper is highly practical and important, and the analysis is insightful. Meanwhile, reviewers raised some concerns such as empirical setups and insights. In the revised paper, the authors provided more justifications and added more analysis such as few-shot evaluation and uniformity analysis. After the discussion period, most reviewers are positive about this paper.\n\nOverall, I recommend to accept this paper. I encourage the authors to take the review feedback into account in the final version."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies how models perform on downstream image classification tasks when they are trained via self-supervision using streaming data. Streaming data refers to a sequential training setting  (ST) where the entire training data is not available for training at the same time but can be only used in disjoint chunks. When training on one chunk of data, the previous chunks are unavailable for training during this sequential training. When the entire training data is available for training at the same time, it is called joint training (JT). The key insight of the work is that in the presence of little to moderate drift in the data distribution across chunks, the downstream task performance of self-supervised training in sequential setting (SSL-ST) is comparable to that of self-supervised training in joint setting (SSL-JT). While in the presence of severe drift in data distribution across chunks, SSL-ST performs worse than SSL-JT but through the application of continual learning approaches like data replay and MAS (Aljundi et al, 2018), this performance gap can be reduced to a large extent. Moreover, when training is done in a supervised manner (SL), sequential setting (SL-ST) performs much worse than joint setting (SL-JT)\n\nThe work experiments on four kinds of sequential settings with varying degrees of data distribution drift across consecutive chunks - (1) instance incremental sequence, where each chunk of unlabeled data is derived from the same set of classes having i.i.d. samples in each chunk, (2) random class incremental sequence, where data in each chunk belongs to a different set of randomly chosen classes, (3) distant class incremental sequence, where each chunk belongs to a different set of classes such that there is a larger semantic gap between the set of classes and (4) domain incremental sequence, where the chunks belong to different domains thereby exhibiting severe distribution shift. For the first three settings, Imagenet-1k is used which is split into 4 chunks as per the setting. For the fourth setting, DomainNet is used. MoCo-v2 is primarily used in the empirical analysis along with some experiments on BYOL are also conducted to show that the trends are observed in other SSL approaches as well.\n\nThe work also conducts an analysis on the amount of forgetting using the backward and forward transfer analysis which shows that compared to supervised setting where there is significant amount of catastrophic forgetting, self-supervised sequential setting has much lower forgetting. Further analysis via Centered Kernel Alignment (CKA) suggests that compared to supervised setting, SSL-ST has a higher feature similarity between two sequential models. Even the feature similarity of SSL-ST with SSL-JT is higher than the corresponding similarity of SL-ST with SL-JT. ",
            "main_review": "Strengths\n\n1. The paper provides an extensive framework for assessing the downstream task performance of sequential self-supervised learning through the four different settings of increasing severity of data distribution drift. I believe this framework unifies the different settings considered by previous works in this domain and provides a comprehensive benchmark for the community to explore the setting of sequential self-supervised learning in further detail.\n\n2. The paper considers downstream performance on several standard benchmark datasets that contributes to the generalization of the insights and analysis to other vision datasets. There is also analysis on both many-shot and few-shot classification tasks which is essential to cover the diverse range of downstream tasks found in practice.\n\n3. Understanding the performance gap between SSL-ST and SSL-JT at different degree of distribution drift allows to identify what level of distribution drift hurts the performance. I find this analysis particularly insightful. Moreover, the paper also experiments with some continual learning approaches to try mitigating this performance reduction. This information can be useful to understand in what type of sequential self-supervised training scenario, there may be a need to experiment with continual learning approaches.\n\n4. The analysis of feature similarity helps to understand that in self-supervised learning, sequential setting behaves quite similar to joint setting in terms of learned feature representations.\n\n5. The additional analysis on space-time efficiency and backward/forward transfer analysis of performing sequential self-supervised learning is also quite extensive and useful to understand the benefits of the approach.\n\nWeakness\n1. It seems that the paper overall has little to no insight of its own to offer on why the sequential self-supervised learning is much closer in performance to joint self-supervised learning. In some places (Sec 4.2 and Sec 6), the paper does provide some explanation to this behavior but that is (1) conjectured in that there are no experiments conducted or theoretically justified if that is indeed the rationale and (2) cited from other papers. The paper does provide empirical evidence on how well SSL-ST is doing compared to SSL-JT but that seems more of an outcome of how the downstream tasks are performing in the two settings and not exactly shedding light on what is making SSL-ST do so well compared to SSL-JT while SL-ST is much worse than SL-JT. I would suggest the authors to add more empirical evidence about why SSL-ST is not so much worse than SSL-JT across the different sequential settings considered. Further, there can be more empirical evidence added to the claims that there is ‘negative transfer’ happening (Sec 4.2) or the features learned by uniformly distributed over feature space (Sec 7) and this is indeed a potential reason for the trends observed.\n\n2. Continual learning settings are very sensitive to the empirical setup. Changes such as the number of classes per chunk and the size of the replay buffer can make a drastic change in the performance. I am not very clear why the sequential setting has just 4 chunks. I would suggest the authors to experiment on much longer sequences such as 10 or 20 chunks. Understanding the impact of having longer sequences is crucial to assess the full extent of forgetting and negative impact of distribution drift. Moreover, tasks like self-supervised learning are only practical when leveraging extremely large amounts of data as shown with Instagram 1B (Mahajan et al, 2018). Such works are already instance incremental and at large scales, having more than 4 chunks will be highly likely.\n\n3. The experiment details lack some important details to validate the soundness of the setup in which experiments were performed. For eg, when using MoCo-v2, what is the size of the queue used for training? Is the queue refreshed before training on a new chunk? Or is it simply preserved and enqueued/dequeued as usual? In the case of the latter, my concern will be that the setup is not really working on disjoint chunks and the observations may not be valid. Further, if possible, I would request the authors if they could provide the training loss curves for the different settings across the chunks. I would expect the training loss to spike whenever the training proceeds to a new disjoint chunk and the spikes should be higher for higher severity of data distribution drift. If this is not the case, there is a likelihood of a leakage between chunks which affects the sanity of the sequential setup.\n\n4. I am curious why there is a drop in SSL-ST wrt SSL-JT in figure 11 for instance incremental sequence setting. In Appendix B (Pretraining), it says that ‘we consider one random sequence as the data are randomly divided’. Does this mean that the way SSL-ST is implemented in an instance incremental setting, it is identical to SSL-JT? Please correct me if I am wrong. But if this is correct, shouldn’t the two curves have no performance difference?",
            "summary_of_the_review": "I would like to recommend the paper for ‘marginally above the acceptance threshold’ rating. The authors have addressed some of my concerns. Some of my concerns still exist such as how concretely the new analysis using sharpness of loss minima justifies SSL-ST to be compared to SSL-JT. I also have some concerns on the soundness of the loss curve and whether the rise in the loss is due to distribution drift or just recalibration of the SSL queue. Although there is certainly areas to improve, the paper does provide an interesting perspective to compare SSL and SL and also support it with different kinds of analysis that led me to revise the rating.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors of this submission worked on an interesting and quite practical task, i.e., to study the self-supervised behavior under the continual learning setup. To this end, the authors studied four types of incremental-learning settings, i.e., Instance incremental sequence, Random class incremental sequence, Distant class incremental sequence, Domain incremental sequence. The four types are designed to reflect different incremental-learning scenarios with different semantics.The authors have also conducted extensive empirical validations, including pre-training >500 models on four categories of pre-training streaming data from ImageNet and DomainNet, and evaluating them on three types of downstream tasks and 12 different downstream datasets. ",
            "main_review": "Pros:\n    1. The submission is well grounded. The task studied in the manuscript is highly practical. the motivation of conducting such an analysis is strong, since the incremental learning setup is indeed practical in many real-world scenarios.\n    2. The amount of experiments is enormous and sufficient to draw the conclusion. \n    3. The experimental designs, especially the four categories of incremental learning setups, are interesting and to the point.\n    4. The manuscript is well organized and well written, especially the abstract and introduction that gives a good tutorial.\n\nI have some concerns and comments as follows:\n    1. Please elaborate on the rationale behind the design of the four scenarios, as early as in the introduction. In other words, why these four categories but nothing in-between or beyond?\n    2. It would be good if the authors can provide more insights and potential applications apart from revealing the experimental observations. In other words, what message of the experimental results convey? \n    3. Intuitively, why do self-supervised models forget less than supervised models?\n    4. Given the performances of the downstream tasks, any insights one can have? Like the affinities across tasks?\n",
            "summary_of_the_review": "Th authors focus on an interesting and practical task. Prior works have largely overlooked this task and the proposed work is the first to explore along this line. The motivation of conducting such an analysis is strong, since the incremental learning setup is indeed practical in many real-world scenarios. Extensive experiments are also conducted to support the proposed method, which are very solid.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper explores a more realistic situation for self-supervised representation learning of training with streaming data. In this paper, four different situations for training with streaming data are investigated. The experimental results show that the self-supervised pretraining less suffers from catastrophic forgetting compared to the supervised pretraining, when the model is trained sequentially using streaming data. The experimental results also show that existing continual learning techniques are also effective for self-supervised learning with streaming data.",
            "main_review": "Strength\n1.\tThis paper firstly investigates the sequential training situation for self-supervised learning, which is an important problem for the widespread use of self-supervised learning in real world application. They also propose some scenarios for training model with streaming data.\n2.\tThe experimental results show that the problem of catastrophic forgetting due to training with streaming data is less problematic for self-supervised pretraining compared to the supervised pretraining, for all scenarios. \n3.\tThey show that some existing continual learning techniques are effective for self-supervised sequential pretraining. \n\nWeakness\n1.\tThe novelty and technical contribution are limited. Although this paper proposes some scenarios for sequential training with streaming data and shows that the self-supervised learning methods work better than supervised learning baselines under these scenarios, it is hard to find further contribution except this empirical discovery\n2.\tIt seems that the analysis results in Section 5 are not very helpful for understanding. For example, in subsection 5.1, there is no explanation what BWT and FWT mean and how this is related to the catastrophic forgetting issue. Also, it seems that the result of BWT, FWT, CKA and feature reconstructions are the natural result of less catastrophic forgetting, and provide meaningful insights to make a further improvement on sequential self-supervised pretraining. \n",
            "summary_of_the_review": "This paper empirically studied on sequential self-supervised learning problem, which is never done before, and show that self-supervised learning is more robust to catastrophic forgetting problem compared to supervised learning. Although exploring the problem of sequential self-supervised learning is meaningful, it is hard to find insights beyond empirical results to make improvement on the top of the discovery of this work in this paper. As a result, I prone to reject this paper. However, I am open to any objection and further discussion, if the authors make their point and insight of this paper clearer.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper considers the setting where data for self supervised pre-training comes in a streaming fashion, where models are incrementally trained on new data. They test 4 different types of streaming data which have different distribution shift properties, on 12 classification datasets and an object detection dataset, using MoCov2 as the self-supervised algorithm. The paper finds that for certain types of streaming data, the downstream task performance of streaming SSL versus \"joint\" SSL (which is the standard pre-training scheme where all data is available at once) are somewhat similar, while there is a large gap in performance for streaming data types with a large amount of distribution shift. They find that this behavior is unlike the gap between streaming and non-streaming supervised learning, where the gaps are larger in all types of streaming data. Finally, they reduce some of the gaps with data replay and regularization.",
            "main_review": "Strengths\n- The paper does a thorough empirical study on different types of streaming data and across a large number of datasets.\n- The streaming data setting is important to consider as we train larger pre-trained models and need to update them over time. The new data may or may not have distribution shifts, which this paper shows has an effect on downstream performance vs. re-training the model.\n- I am unaware of previous thorough empirical studies on the effectiveness of self-supervised pre-training on streaming data. \n- The paper gives interesting observations comparing self-supervised continual learning to supervised continual learning, showing differences under mild distribution shift.\n- The results on each individual dataset are plotted clearly. However, the plots do not have error bars.\n- The paper resolves some of the issues they see with continual learning using data replay. However, the replay method used seems to make the data size keep increasing - this makes the comparison a bit unfair since the data replay method will get to see more and more data (and the benefit of continual learning diminishes over time). There should be a tradeoff with seeing new and old data.\n- The paper also tests another self-supervised method, BYOL.\n- The post-hoc analysis of the sequentially trained models shows quite clearly that self-supervised learning suffers less from forgetting for certain types of streaming data (Fig 7).\n\nWeaknesses / Neutral\n- It's unclear what changes from a practical point of view here, since the suggestion is to use simple continual learning methods which are known. The main contributions are the observations across different types of streaming data (which we may not have control over?), but these are somewhat alleviated by employing the continual learning methods regardless of the type of streaming data.\n- There are other papers out there that seem to consider the streaming / continuing pre-training setting for self-supervised learning (https://openaccess.thecvf.com/content_CVPR_2019/papers/Aljundi_Task-Free_Continual_Learning_CVPR_2019_paper.pdf, https://aclanthology.org/2020.acl-main.740.pdf, https://arxiv.org/abs/2103.12718). Often the story here is that doing more pre-training on a more specialized domain can improve the accuracy on the specialized domain. This is a different point to this paper, which is not looking at the accuracy on specific domains over time (which could also be interesting to keep track of).\n- I'm left unsure why there is such a difference between supervised continual learning and self-supervised continual learning. They are definitely measuring different things - with SSL, there is an additional transfer step. Perhaps the finetuning/transfer step takes care of some of the small distribution shift? Some evidence for this: there is a larger gap between streaming and non-streaming when the downstream evaluation is few-shot learning (less finetuning) in Figure 3. Another source of difference could be the loss function - in SSL you need to model all the features whereas in supervised learning, you can focus on features correlated with the label.\n- On the setting, there is possible distribution shift across chunks in the streaming data, but it seems that if taken as a whole, the overall distribution is assumed to be stationary in some sense. Perhaps for future work, it may make sense to consider gradual distribution shift over time (of the overall distribution, which changes the relationship between pretraining and downstream). \n- In Fig 1, the class semantic trees are not really fully explained. For example, what are the parent nodes? In general the Fig 1 caption could be expanded to more fully specify what the reader should get from the figure. The different types of streaming data are also not explained before they are mentioned in the intro.\n- The \"instance incremental\" setting with IID datapoints, when considered in a huge data setting, is identical to the regular pre-training setup  in some cases - for example, in GPT-3, they didn't even finish one epoch of the data. Thus not having a huge gap isn't too surprising particularly in the large data setting where we make very few passes over the dataset. This paper studies this kind of setting and says that in supervised learning, the gap isn't big (https://arxiv.org/abs/2010.08127). It's interesting that in Figure 11 of your paper, there is a gap between supervised pre-training in a streaming or non-streaming fashion for downstream task performance. This could be an interesting distinction / comparison to make.\n- Is there any way to test the claim of \"negative transfer\" in 4.2? Often, when streaming suffers this \"negative transfer\", JT does not seem to, even with the same chunks.\n- At the bottom of page 3, re-training a model from scratch at every time point is referred to as joint training, but perhaps a more common terminology from online learning is \"follow the leader\".\n- In Figure 7 we only have to show the lower triangular part of the matrix",
            "summary_of_the_review": "The paper provides some interesting observations through pretty thorough experiments on an interesting/important setting for large pre-trained models. They show that continual learning methods can improve the issues they found. There isn't much in the way of explanations of the observations, and the methods aren't novel, but the observations are valuable.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}