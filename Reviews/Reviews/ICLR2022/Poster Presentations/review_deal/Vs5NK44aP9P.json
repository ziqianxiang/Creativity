{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "### Summary\n\nThe key idea behind this approach is a new technique to map irregular sparsity to a regular, compressed pattern. The results can, in principle, therefore overcome several standard limitations with irregular data storage formats.  The results improve over existing (though related) techniques.\n\n### Discussion\n\n#### Strenghts\n\n- An interesting and timely topic to study\n\n- Results show non-compute improvements\n\n#### Weakness\n\nThe primary weakness noted among the reviewers was the lack of study on actual decoding performance. As I note below, this is a serious oversight that given the already existing theoretical work in the area warrants study as the community should begin to turn towards mapping that theory to practice.\n\n### Recommendation\n\nI recommend Accept (poster). This is a strong piece of theoretical work. However,  I would like to note that while I believe this work meets the current evaluation standards set in the area, it is time for follow on work to take the additional step to validate the practicality of the approach through a performance evaluation (either in simulation or FPGA/ASIC work)."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper tackles an interesting and important question which is how can we make the pruned network regular such that it can run quickly on GPU / common hardwares. The problem essentially originates from the fact that typical computational hardware can be slow when accessing in-contiguous memory address. This paper proposes a sequential encoding-decoding shemes and can compress irregular pruned weights to a regular (block-wise) structure stored in memory and shows experimental results in large-scale models / datasets.",
            "main_review": "I like the motivation of this paper which is a very important problem in practice. To the best of my knowledge, the solution is novel and interesting, and I believe it is an orthogonal contribution to many other literatures in the compression community (e.g., magnitude-based compression, variational dropout etc). The experimental results look promising and the theoretical analysis is sound to me. I have a couple of comments regarding how the paper can be improved in the revised version. First, the paper starts with an motivation of improving the parallelism running the pruned network. However, it's a pity that the paper doesn't show any practical acceleration on ImageNet or WMT. Only compression ratio and memory reduction have been showed for various of methods. (Although Appendix A kind of showing sparse matrix multiplication is slow, I think it's still better to directly show the runtime reduction using the pruned model on real data). Second, the figures are somewhat cluttered and a bit hard to understand. I would recommend spend some time and revise them. Third, literatures and references spread across sections. An individual section (Related Work) for discussing compression literatures would be preferred. ",
            "summary_of_the_review": "The motivation is persuading and the method is novel. The author also demonstrate the effectiveness of their method on large scale datasets with a number of compression techniques. Although the writing can be improved, I think this is a good paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose a sparse weight encoding scheme based on XOR-gate networks and a method for finding the requisite sequence which minimizes encoding error. They show that their method can achieve near-zero overhead at low encoding-error rates.",
            "main_review": "At a high level, the paper has two sides: a method and the context in which it is intended to be used. The authors do a good job at presenting the first. The technique is described in (sometimes intense) detail, and the evaluation gives straight answers to clearly-defined questions. The latter (context) is a bit weaker. The authors choose to treat their problem in isolation, which means that the paper sometimes loses sight of why the method was proposed to begin with. Computational aspects are glossed over completely, despite being the nominal motivator for the paper, and outside of a couple of statistical assumptions, there's not much connection to the ML models they test. The optimistic view is that their approach is successful regardless of input; the pessimistic view is that the impacts of various model characteristics which this community cares about are largely unknown. Appendix E does add a bit of insight here.\n\nStrengths:\n\n- The authors are fairly comprehensive in explaining the factors involved in choosing the method's tuning parameters. They clearly explain trade-offs and provide instructions on how to arrive at ideal values.\n\n- I appreciated the minor analyses the authors ran. The bit inversion technique, for instance, concisely raised a problem and summarily offered a solution. Walking the reader through the pitfalls of a new technique makes the paper more enjoyable to read.\n\n- The prose is generally clean and easy to read.\n\nWeaknesses:\n\n- No mention of the actual computational performance of the method. This seems important to the paper, considering the whole motivation for fixed encoding is memory access regularity. I don't necessarily expect a highly-tuned implementation that beats vendor kernels, but if the nominal goal of pursuing fixed-sized encoding is to improve speed via improved memory behavior, I would expect some concrete evidence that the proposed method achieves that purpose and some notion of the computational costs it pays for that benefit. From the method described, it looks efficient (especially if offloaded to specialized hardware), but it hurts the paper not to see quantitative support.\n\n- I was expecting to see a mention of end-to-end model accuracy. With best the $E$ values listed, I anticipate the end-to-end effect is (hopefully) marginal, but it would be good to confirm that this is the case.\n- In some cases, the paper's math could be clearer. Terms occasionally appeared in figures before their introduction in text, and I would've appreciated a brief mention of word length somewhere in section 2 or 3. It didn't help that many important terms were $N_{foo}$ or $n_{bar}$. This is minor and admittedly subjective.\n\nQ: This work seems to share many parts with the references it seems to build on: Kwon20 and to a lesser extent Ahn19. While I have a general sense of where this paper diverges, can the authors briefly summarize what they feel the novel contributions to the community over those works are?\n\nAnd as feedback: given the overlap, it might strengthen the paper to be more explicit about the differentiating factors. I would probably have appreciated a direct comparison (perhaps in a related works section, seeing as one is not supplied).\n\nQ: Many other aggressive encoding schemes leverage the pruning mechanism to improve encoding behavior (e.g., weight sharing to reduce indexing cost for CSR schemes). These reduce encoding costs to lower than would otherwise be statistically expected. What's the analog here? I.e., with additional manipulation of non-zero weights (lossy or not), does this method offer avenues for improvement?",
            "summary_of_the_review": "The paper does a reasonable job of presenting its sparse encoding scheme, and the analyses and evaluation seem reasonable. The paper feels a bit isolated and theoretical, which is odd given its stated goals. The method doesn't seem bad, but the reader is left with a lot of open questions about putting it into practice.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors proposed an encoding scheme and decoding mechanism for efficient communication of irregular sparse DNN parameters, and demonstrated with sparsified transformer and resnet50.  \n",
            "main_review": "The method is novel.  I have the following: \n\nConcerns:\n\n- The compression scheme with associated encoder/decoder purely solves the communication problem where HW performance is bottlenecked by limited memory bandwidth under irregular access patterns.  How does compute factor into the picture?  How does compute overhead of decoding scale?  \n- Experiments are performed on GPUs.  It would be desirable to understand applicability to other accelerator architectures, specifically spatial architectures.  \n- On page 2: ``higher sparsity leads to higher variance on pruned weights in a block\".  Is this true?  Variance of the number of zero/nonzero should be nonmonotonic w.r.t. sparsity; extremely sparse or dense cases have zero variance.  Am I missing something?  ",
            "summary_of_the_review": "Novel idea of significance and potential practical value.  Presentation could use further clarity (e.g. elaboration of basic concepts such as fixed-to-fixed) since the readership of this conference is probably not experts in communications.  \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None. ",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work proposes a sequential fixed-to-fixed encoding scheme for sparse neural network weight encoding/decoding. The core component of the algorithm is shifted registers that expand the decoding window and extra code for recording unmatched bits. ",
            "main_review": "This work proposes a sequential fixed-to-fixed encoding scheme for sparse neural network weight encoding/decoding. The core component of the algorithm is shifted registers that expand the decoding window and extra code for recording unmatched bits. \n\nPros: The proposed shift register structure offers a much better improvement on encoding efficiency than increasing the coding block size. To my knowledge this algorithm is novel.\n\nCons:\nIt is unclear how much actual benefit this algorithm can offer as to my understanding this algorithm requires very high decoding computing and space complexity. With such kind of complexity CSR probably can also overcome its shortcoming. And such kinds of decoders would probably not fit in the very limited space of computing cores such as a GPU. \n\nThe paper is difficult for a reader that is not familiar with prior works such as Kwon et al., 2020 and Ahn et al., 2019. I only managed to comprehend this work after going through those two prior works. I suggest references to those works should be introduced in Sections 1 and 2 along with some related background info to help readers.\n\nI suggest renaming the XOR-gate network to something like XOR-gate decoder, given this community generally associates “network” with neural networks.\n\nP2: Please explain how you arrived at Figure 1A.\n\nP2: Please explain “x with Mask” in Algorithm 2.\n\nP2: Please precisely define memory bandwidth. What you described here seems vague. \n\nP2: “Since higher sparsity leads to higher variance on pruned weights in a block”. Please clarify this sentence. Are you sure this is correct?\n\nP3, last line: “(masked) weights”. Do you mean weights not subjected to pruning? Shouldn’t such weights be called not masked weights?  \n\nFigure 4: “S is entire pruning rate”, I believe “entire” here is not necessary and is causing confusion.\n\nP4: A reference to GF(2) is probably necessary.\n\nP4; the sentence just above Section 3.1. : I believe the logic of this sentence is not valid. Even though the conclusion is correct.\n\nP7: “(Nin × Ns) is empirically limited to be less than 26” What you mean by this? If you chose to have $N_in=8$, does that leave you with $N_s<3$? If you need 32GB memory for decoding, is this algorithm applicable in real-world applications?\n\nTable2 and 3 probably contain too much redundant information. I suggest removing some results so there will be enough space for improved clarity.",
            "summary_of_the_review": "Given the issues I raised in the main review I will not recommend this paper for publication for now. But I would be happy to change my evaluation if the authors can improve the clarity of the paper and address my concerns.\n\nAfter rebuttal:\nI raised my evaluation from 5 to 6, given the improvement included in the new revision. However, there are still two main issues I want to point out. First, the authors built their work upon Kwon20 and Ahn19, but they only cite those two papers in the latter part of their manuscript.  And related works section is only included as an appendix section. This layout is unusual, and it made their manuscript hard to comprehend for an audience that is not familiar with the specific topic. Second, despite my repeated request for clarification on the computing complexity, the authors still only gave a vague description of the issue. I hope the authors can fix those two issues in their camera-ready if the paper is accepted.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}