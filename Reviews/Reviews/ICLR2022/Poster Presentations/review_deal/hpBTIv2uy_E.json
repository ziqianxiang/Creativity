{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes a hypergraph representation learning based on multiset encoding, which  covers most existing propagation methods for hypergraph neural networks. The authors provide theoretical proofs that both CE-based and tensor-based propagation rules can be represented as a composition of two multiset functions, and propose two different multiset encoding functions, based on DeepSets and SetTransformer. The authors validate their method for its semi-supervised node classification performance on multiple benchmark datasets, showing that it is superior or comparable to a large number of existing works on hypergraph representation learning. \n\nThe following is the summary of the pros and cons of the paper mentioned by the reviewers:\n\nPros\n- The proposed framework generalizes existing message passing methods for hypergraphs, and authors provide theoretical proofs on how it can generalize to two different types of propagation rules for hypergraph representation learning. \n- The paper is well-organized and is clearly written, and the code is provided for reproduction. \n- The experiments consider a wide range of hypergraph datasets and baselines, and the proposed method either outperforms them or at least achieves comparable performance.\n\nCons\n- It is still unclear where the benefits come from, due to lack of ablation studies and deeper analysis. \n- Experiments are only restricted to the semi-supervised node classification task. \n\nWhile the initial reviews were split due to these negative points, all reviewers unanimously recommended for acceptance after the discussion period, as they found the responses from the authors satisfactory.  \n\nIn summary, this is a well-written paper that proposes a neat framework for hypergraph representation learning that generalizes to most existing methods, backed up by compelling performance on benchmark datasets, which will make it a strong addition to ICLR. However, as mentioned by the reviewers there should be more ablation studies and in–depth analysis of what makes the proposed multiset function more effective, as this is lacking in the current version of the paper. It would be worthwhile to also validate the proposed framework on other tasks (e.g. graph classification tasks). \n \nOne minor thing that I want to point out is regarding the claim that this is the first attempt to connect the problem of learning multiset functions with hypergraph neural networks.  [Jo et al. 21], which is a hypergraph-based framework for edge representation learning, utilized GMT [Baek et al. 21], which performs multiset encoding using SetTransformer for hypergraph representation learning, and thus I suggest the authors to tone down on the claim that this is the first work that connects multiset encoding with hypergraph neural networks, and properly acknowledge this. \n\n[Jo et al. 21] Edge Representation Learning with Hypergraphs, NeurIPS 2021"
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a generalization of the hypergraph neural networks using multiset functions.",
            "main_review": "This paper proposes a generalization of the hypergraph neural networks using multiset functions. Instead of doing clique expansion as in many existing methods, the proposed method learns explicit hypergraph representations, and uses two multiset functions to perform message propagations, one from nodes to hyperedge and another from hyperedge to nodes. The method is shown to be a general framework that encompasses existing GNN methods. Experimental results show that the AllSetTransformer achieves competitive results or outperforms existing methods on a large set of benchmark datasets. \n\nThe proposed method unifies various methods for hypergraph neural networks, and gives a competitive formulation across a wide spectrum of problems. This is a nice contribution to the community.\n",
            "summary_of_the_review": "The paper makes a nice contribution to the community.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a framework for hyper graph neural networks.  It argues that existing work propagates a hyper graph by first transforming it into a regular graph trough clique expansion, which may lose information.  Alternatively, the propagation is based on tensors.  The proposed framework seeks to combine these different ways of propagations into one unified framework.  The proposed framework is called AllSet, with two instantiations AllDeepSets and AllSetTransformer.  Experiments are conducted on several graph datasets.",
            "main_review": "Strengths of the paper:\n\nS1. The problem is well-motivated, with significant coverage of existing work based on clique expansion and tensor-based propagation.\n\nS2. Experiments include a significant number of baselines (ten models), as well as on ten different datasets.\n\nS3. There are theoretical analyses showing how the proposed framework is a generalisation of some existing works.\n\n\nWeaknesses of the paper:\n\nW1. As the proposed framework is in some sense a generalisation or a unification of existing propagation mechanisms, such as clique expansion and tensor-based propagation, there should be an ablation analysis to understand the contribution of each component.\n\nW2. The experimental discussion are focused on just showing numerical comparisons.  A more discursive analysis that explains why the method works better than the existing methods would help to reveal where the sources of improvements are.\n\nW3. Since the core is to derive hyper graph representations, experimenting with a single task is rather limited.  It would be good to have additional evaluation tasks beyond semi-supervised classification.",
            "summary_of_the_review": "The paper proposes a framework for hyper graph neural networks, which is an interesting problem.  Though rigorous in its treatment and experiments, further analysis on what makes the work better then the baselines would help in better appreciating the significance of the contributions.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Transferring standard graph operators to the hypergraph setting is non-trivial and several message passing operators have been considered in the setting of hypergraphs, including clique-expansion and tensor based. In this paper the authors propose a general framework where learnable multiset functions are used in order to learn the hypergraph propagation map from the data. In particular, this framework contains  many previously considered propagation maps as particular cases",
            "main_review": "Strengths:\n\n- Overall, I think this is a well-organized paper and I enjoyed reading it. The motivations for the model design are well discussed as is the related work. \n- The proposed idea is simple and neat. Many hypergraph Laplacian-like mappings have been considered and this gives a possible data-centric answer to the question \"which one should I use?\"\n- The experiments consider a wide range of hypergraph datasets, which is somewhat rare in hypergraph classification literature. Moreover, the authors have collected several new hypergraph datasets, which I believe will be very useful for the community\n\nWeaknesses:\n- The experiments are not completely clear to me. In particular: (1) I think you should specify more clearly what is the depth of MLP and Set Transformer layers in AllDeepSets and AllSetTransformer and (2) you should more clearly specify what is the overall amount of known labels you used in the experiments and what is the amount used for training the AllSets architectures\n- The notation in equation (8) should be better specified (e.g. what is LN or || )\n- Other propagations which are \"in-between\" CE and tensors have been considered recently, e.g. [1-3]. Would these be particular cases of your proposed framework too?    \n[1] Tudisco et al - Node and edge nonlinear eigenvector centrality for hypergraphs  \n[2] Arya et al - Adaptive Neural Message Passing for Inductive Learning on Hypergraphs  \n[3] Prokopchik - A nonlinear diffusion method for semi-supervised learning on hypergraphs  \n",
            "summary_of_the_review": "Overall, this is a solid paper which proposes one simple, yet novel and efficient idea to generalize propagation maps on hypergraphs. Solid experimental results are provided in the paper.\n\nI like this paper and I would like to see it accepted. I will raise my score if the weaknesses are properly addressed\n\n-----\n\nI am happy with the feedback provided to my concerns and those of other reviewers so I will raise my score",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Hypergraphs can capture group/set relationships in real-world data. \nSeveral hypergraph neural networks have been proposed in the literature to exploit both group relationships among nodes and node features for learning with hypergraph data. \nThe contributions of the paper are \n1) generalisation of most existing methods into a single framework (named AllSet), \n2) exploration of AllSet based on Deep Sets [NeurIPS'17] and Set Transformer [ICML'19], and \n3) empirical evaluation on existing benchmarks and three curated hypergraph datasets ",
            "main_review": "### **Strengths**\n\n1. **Quality of Paper**\n\nThe paper is technically sound. \nThe methods used (AllSetTransformer, AllDeepSets) are appropriate for the downstream task (node classification in hypergraphs). \nIt is interesting to see that AllSet generalises several neural networks on hypergraphs and message passing neural networks on graphs. \n\n2. **Clarity of Presentation**\n\nThe paper is clearly written and well organised. \nThe appendix is comprehensive with both theoretical proofs and empirical details (e.g., dataset construction, hyperparameters, etc.). \nThe authors have also released the source code as part of the supplementary material. \n\n\n\n### **Weaknesses**\n\n1. **Originality of Contributions**\n\nThe key idea of AllSet is to use rules of the form $f_{\\mathcal{V}\\rightarrow\\mathcal{E}}$ and $f_{\\mathcal{E}\\rightarrow\\mathcal{V}}$ as given in equation 5. \nThese update rules are well-known ideas in both (i) hypergraph neural network literature [e.g., Be More with Less: Hypergraph Attention Networks for Inductive Text Classification, EMNLP'20] and (ii) bipartite graph neural network literature [e.g., Hierarchical Representation Learning for Bipartite Graphs, IJCAI'19].\nMoreover, AllSetTransformer and AllDeepSets are straightforward applications of SetTransformer and DeepSets in the AllSet framework.\n\n\n2. **Significance of Empirical Evaluation**\n\nFrom Table 2 it is clear that the proposed methods show marginal or no improvements on all but three datasets viz., zoo, a tiny dataset, and Yelp, Walmart both curated by authors.  The Yelp dataset is about review rating prediction for restaurants where all methods achieve less than 37% accuracy: it is unclear how significant hypergraph modelling and the curated node features (e.g., latitude, longitude, city, state, etc.) are for rating prediction. The Walmart (and House) datasets do not come with node features, so it would be more compelling if hypergraph-only methods are used as baselines [e.g., Re-revisiting Learning on Hypergraphs: Confidence Interval and Subgradient Method, ICML'17].\n\n\n3. **Positioning with Prior Work**\n\nExperiments are restricted to transductive node classification in hypergraphs. The idea of $f_{\\mathcal{V}\\rightarrow\\mathcal{E}}$ and $f_{\\mathcal{E}\\rightarrow\\mathcal{V}}$ update rules have been successfully used in other tasks, e.g., (i) inductive text classification  [e.g., Be More with Less: Hypergraph Attention Networks for Inductive Text Classification, EMNLP'20], (ii) hyperlink prediction [e.g., Principled Hyperedge Prediction with Structural Spectral Features and Neural Networks], and (iii) recommendation [e.g., Session-based Recommendation with Hypergraph Attention Networks].\nIt would be interesting to see how AllSetTransformer and AllDeepSets would perform in these tasks.\n\n\n### Update\nAfter reading the responses to all the reviews, I have updated my recommendation from 5 -> 6",
            "summary_of_the_review": "Overall the paper is clear and of good quality in which claims are well supported by theoretical analysis. However, the novelty is incremental and experimental results are marginally significant. Positioning with missing prior work and evaluation on other hypergraph tasks would improve the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}