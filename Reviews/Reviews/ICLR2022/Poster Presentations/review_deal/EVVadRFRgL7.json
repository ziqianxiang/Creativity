{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "While the reviewers place this manuscript right at the threshold of acceptance, I find the revisions that they have made to address the majority of the reviewers concerns. That, combined with some of the reviewers' scores being slightly miscalibrated with their (largely positive) reaction to the author feedback, I am advocating for this paper to be accepted."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a Bayesian Learning to Optimize technique called Uncertainty-Aware Learning to Optimize (UA-L2O). The main contribution of this paper is its introduction of the concept of the space of optimization algorithms and treating an optimizer as a random sample from it, and deriving a proper Bayesian inference algorithm based on that concept. The space of optimization algorithms, to simplify, is expressed as a space of parameters of learned optimizers $\\phi$, and a Gaussian prior is placed on those parameters. The posterior inference procedure is approximated with variational inference with reparameterization tricks, and the actual deployment of the learned optimizer is done with Monte-Carlo approximation with learned variational distribution for the optimizer parameters $\\phi$. The proposed algorithm is applied to various synthetic and real-world optimization problems and showed some useful calibration behaviors.",
            "main_review": "The paper is generally well-written and easy to follow. The idea of quantifying the uncertainty of the optimization algorithm is novel and interesting. However, there are some unclear points that make me not sure of the acceptance of the paper.\n\n1) Motivation is not clear - why do we need optimizer uncertainties?\n\nThe \"contributions\" section in the introduction states that the paper discusses the fundamental question of 1) why modeling optimizer uncertainty, and in my opinion, this would be the most important question to be discussed. However, it is not clear from the paper why the optimizer uncertainty really matters.  At least, judging from the experiments, uncertainty-aware optimizers do not always give superior optimization results in terms of error, as can be seen from the results for the synthetic functions. UA-L2O seems to improve calibration, so the optimizer uncertainty somehow increases for the difficult optimization problems, and the authors stated in section 4.4 that \"such calibration capability is critical to detect the failure cases in real-world application\".  However, I don't think better calibration can actually lead to better detection of failure cases. For instance, in the synthetic experiments, the uncertainty calibration metrics displayed in the second and third columns do not show significant improvement over baselines without uncertainty; I mean UA-L2O is indeed better for some cases, but the margin is not that significant, so it is not clear we can actually make use of such uncertainty to actually detect the failure cases. In order to argue this, 1) one should first clearly define what a \"failure\" case is, and 2) do some experiments to run multiple optimization runs, and 3) do the binary classification based on the estimated uncertainty and compute AUROC to quantitatively show that we can actually benefit from the estimated uncertainty in terms of detecting failure cases. For the purpose of detecting failure cases, I can also think of very naive baselines, such as just monitoring the validation loss or training loss values and treating the ones with loss values larger than a certain threshold as a failure.  \n\n2) Important details are missing\n\nAs written in the review of the paper, L2O algorithms usually require a meta-training stage where the learned optimizer is trained with multiple optimization runs for the tasks sampled from some specific task distribution. Such meta-training is indeed time-consuming, and requires care in design; for instance, we need a diverse task distribution to guarantee the generalizability of the learned optimizers. However, I fail to see the detailed description of the meta-training protocols, other than the description of the ELBO being used for the inner optimization objective.  Also, it is not clear how the Monte-Carlo estimation is done for the actual deployment of the learned optimizer; how many samples are required? Are the samples drawn at the initial stage of the optimization or drawn at each iteration of the optimization? How sensitive is the algorithm to the number of samples? How does the algorithm scale with the number of samples being used? I particularly think the last question is important because it is directly related to the practicability of the algorithm; at least, I'd like to see the wall-clock time of the UA-L2O compared to non-Bayesian baselines.\n\n3)  The \"space of algorithms\" is a slight over-statement\n\nAlthough I find the notion of the \"space of the optimization algorithms\" quite appealing, the actual implementation given in the paper seems rather disappointing; as the authors already stated in the paper, there are many elements to be considered for a single optimization run. Needless to say the parameters $\\phi$ for the learned optimizers, we also have the things like initialization scheme (or initial distribution for the parameters and their hyperparameters),  learning rate schedule, batch size, number of epochs to run, and so on. The uncertainty modeled for optimizer in this paper is restricted to the meta-parameter $\\phi$, so I think to say that the proposed algorithm considers a \"random sample of an optimizer $g$ from the space of optimizers $G$\" is a slight over-statement. \n\n4) Some important baselines are missing\n\nThe easiest baseline I can think of is ensembles; each run of an ensemble (with different random seeds) can be considered as a sample from an implicitly defined space of optimizers. Ensembles are known to excel in many applications both in terms of predictive accuracy and uncertainty calibration (although such uncertainty is for prediction, not like the optimizer uncertainty discussed in this paper). The downside of ensembles is that they require a longer training time than the usual optimization algorithms, but L2O also requires a considerably long meta-training time. So I think it is definitely worth comparing the proposed approach to (deep) ensembles with non-Bayesian optimizers. Another baseline I think worth comparing is stochastic gradient MCMC algorithms such as stochastic gradient Langevin dynamics or stochastic gradient Hamiltonian Monte-Carlo; such algorithms can be understood as noise-injected versions of gradient-based optimization algorithms, and samples collected from such samples can also be used to construct optimizer uncertainty (at least in terms of proximity to the true optima). \n\nConsidering the above-mentioned concerns, I recommend rejection, but I'm happy to discuss with the authors. Please let me know if I'm misunderstanding something.\n",
            "summary_of_the_review": "The idea is interesting, but there are a few points to be addressed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper considers the problem of quantifying the uncertainty of an optimizer used to do inference on a given model. The authors take a Bayesian approach and treat the optimizer as a random variable in the space of algorithms. They derive an approximate posterior via variational inference.\n",
            "main_review": "I found the main idea of the paper novel and interesting. The experimental section is extensive and shows the benefit of using the method across different applications. However I believe Section 3 of the paper is hard to follow and understand. Particularly, I think the authors should clarify the following points:\n\n1) Provide an intuition for what the likelihood and the prior represent. What is $p(g|\\mathbf{z}_t)$ that is  $p(g|\\mathcal{D})$ representing? In standard Bayesian inference $p(\\theta|\\mathcal{D})$ gives how probable is $\\mathcal{D}$ to be generated from $\\theta$. Is it correct to say that in your setting $\\mathcal{D}$ are the data points $(x_0….x_t)$ evaluated via optimization (and potentially the gradients) and $p(g|\\mathcal{D})$ represent the probability of $g$ being the optimizer used to generate the data, ie used to optimize? \n2) It is not clear to me why you are starting from an optimizer $g$ and the you speak about $\\phi$. It seems to me that $g$ is a deterministic function parametrized by $\\phi$ so I don't see the point of having both eq 2 and eq 3.\n3) Definition 1. In point 2 there is no $\\mathbf{x}$ on the left but the distribution on the right factorizes across $\\mathbf{x}_i$. Are you making the assumption that $\\mathbf{x}_i = \\mathbf{z}_i$ or is $\\mathbf{z}_t$ a set of historical information (iterates + gradients) as specified above?\n4) How exactly are you sampling from (5)? \n5) Could you specify the form of the assumed variational distribution $q(\\cdot)$ in Section 3.2?\n6) It seems like the paper is highly based on L2O, it would be useful to be more explicit about what you are improving with respect to that paper. Even thought that method has no uncertainty estimation it would be nice to have it in the experimental session? also, most baselines under comparison are non-Bayesian methods and the primary goal of the paper is to quantify uncertainty. Why are you not comparing to BO? \n7) In the experimental session, in the paragraph \"Evaluation metrics\", $\\mathbf{x}^*$ is sampled from $p(\\mathbf{x}^*)$. Is this supposed to be $p(\\mathbf{x}^* |\\mathbf{z}_T)$ as given in Eq 5?\n8) It is not clear to me why a lower value of $r_{\\sigma}$ should be preferred? My understanding is that the bounds in Eq 6 are computed by sampling from Eq 5 and then getting the lowest value for $\\hat{\\mathbf{x}}$. The true value $\\mathbf{x}_{true}$ is not involved thus we could have a narrower posterior but be off the true optimum. \n\n\n**Minor comments**\n1) What do you mean by “**Optimizing** the posterior via Markov chain Monte Carlo (MCMC)”? Do you mean sample from the posterior distribution? Or computing the MAP? I don't see how that would lead to a degenerate posterior distribution.\n2) Figure 1 could be used to explain the methodology better. It is not mentioned in the text and the caption is to short to be informative.\n3) It would be nice to include a discussion of why using a NN to model the optimizer is a good choice\n",
            "summary_of_the_review": "I really like the idea of considering the additional level of uncertainty and I find it novel. However, I found the methodological session difficult to follow and I think that could be improved by providing an intuition on the prior on $g$ and the connected posterior. At the practical level it is not clear to me how the proposed approach could improve the performance of existing algorithms. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper introduces uncertainty-aware learning-2-optimize methodology. In particular, they parametrize a optimizer space as a neural network g and use variational inference to compute approximate posterior of g and the attained best solution x*.",
            "main_review": "In the very beginning, you say that no optimization algorithm can guarantee global optimum in general. It almost sounds like you say that your solution can. Do the authors agree with this?\nAlso, Bayesian optimization *in theory* can guarantee global optimization, but in practice probably not.\n\nI would like the authors to give concrete ideas of how the space G looks. Does it contain usual optimizers such as Adam? And different levels of momentum etc. \nIs it always a good idea to just make this space so high-dimensional?\n\nOn page 4 you say you extend the \"Bayesian treatment\". Can the authors expand on what they mean? Do you use more then Bayes theorem?\n\nWhy does MCMC lead to degenerate solutions? My belief is that MCMC in general is more accurate than variational inference, so is there something to worry about here?\n\nThe first equality in (4) is not the ELBO. The KL distance here is the difference between the ELBO and the marginal log likelihood.\nCan the authors do the computations in the second equality for me?\n\nCan I use the variation in g to conclude when convergence has happened? \n\nYou mention, but only very briefly, warm-starting theta. How is this done in practice? And how good is the solution immediately after warm-starting? In other words, how much improvement is done after this warm-start?\n\nI found the experiment section very difficult to read, and therefore I am not convinced of the method's superiority over the compared methods. \nAlso, in future versions, I think a comparison with some hyper-parameter optimization is necessary. \n\n",
            "summary_of_the_review": "There are some clear errors in the paper, and I am not convinced that the empirical performance justifies this methodology to be accepted at ICLR. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors looked at uncertainty quantification (UQ) of optimization algorithms. Working under the L2O framework and adopting the Bayesian approach, they used a neural network to model the updating rule, and endowed the network weights with a prior. By treating the solution iterates and their gradients as data, the authors defined a conditional likelihood on them. The resulting posterior on the network weights is then approximated using a variational algorithm. To conduct inference, the authors computed the predictive density of the solution through Monte Carlo sampling. This end-to-end pipeline is called uncertainty aware L2O (UA-L2O) and it is applied to quantify uncertainty in various learning tasks.",
            "main_review": "Treating optimizers as random elements in an algorithmic space is a novel concept to the best of my knowledge. Endowing priors on updating rules and defining likelihoods on the iterates also seem to be new in the Bayesian nonparametrics literature. In addition to data and model uncertainties, this paper brought attention to the role and importance of modeling optimizer uncertainty.\n\nHere are some comments / questions:\n1. It is known that the variational distribution often underestimates uncertainty, e.g., see ``Covariances, Robustness, and Variational Bayes\" by Giordano et al. (2018). However from Figure 3, it seems that UA-L2O is still able to capture the estimated confidence quite well for low dimensions. So how did UA-L2O solve this underestimation problem?\n\n2. Is the neural network architecture of $g$ fixed? In the experiments, it seems to me that the authors modeled $g$ using LSTM. However this contradicts with Definition 1 because $g$ is allowed to change according to the prior?\n\n3. Why $\\hat{\\mathbf{x}}$ is not computed using mean of the posterior distribution of the solutions $\\mathbf{x}^*$? Is there a reason to use the minimum of the generated solutions and not the posterior mean?\n\n4. UA-L2O seems to have trouble optimizing Rastrigin, as evident by the plot of $E||x^*-x_{\\text{true}}||_2$ against dimension $D$ in Figure 3, which looks like it is increasing to infinity. However the structure of Rastrigin seems to be the simplest among all three test functions. Can the authors give some intuition behind this?\n\n5. For Ackley and Griewank in Figure 3, is there a reason why there is a dip in dimension $27$ in the precision of the estimated confidence (2nd column)?\n\n6. In (3), what is $\\mathbf{x}_t^{\\phi}$?\n\n7. There are quite a number of grammar mistakes and some parts are quite puzzling. For example, the line after (2), Prior arts on hyper-parameter?\n\nTYPOS:\n1. The first line in Section 3.1, am -> an\n2. In (3), $\\exp(-\\lambda\\|\\boldsymbol{\\phi}\\|_2^2)$ -> $\\exp(-\\frac{1}{2\\lambda}\\|\\boldsymbol{\\phi}\\|_2^2)$",
            "summary_of_the_review": "I think UA-L2O is a novel and promising method that might lead to further research in quantifying optimizer's uncertainty.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}