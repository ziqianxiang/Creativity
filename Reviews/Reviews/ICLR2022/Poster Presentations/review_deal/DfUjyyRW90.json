{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The reviewers agree that the proposed method to create a more robust representation of a task for model-based-RL is interesting and has significant merits. After some revision, more critical reviewers improved their ratings of the paper, such that there is unanimous agreement that the paper can be accepted to ICLR."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper introduces a method to learn representations for model-based\nRL. The key idea is to find a representation that maximises\ninformation between the action at the previous step and the current\nlatent representation, given the representation at the previous step,\nthereby maximising the information that the representation encodes\nabout effects of an action. The work contributes a method and\nevaluation on tasks where high-dimensional visual inputs are used to\ncontrol an agent in an environment with complex backgrounds as\ndistractors.\n\n",
            "main_review": "Strengths:\n\nI like the idea of the paper in that it appears to be logical and\nalmost obvious, after reading about it here. The solution elegantly\naddresses the problems of representation, and policy learning, using\nthe same formalism. I also like the information-theoretic view as a general\nformulation independent of a specific model.\n\nResults appear to be very good on the chosen benchmarks. Results of\nthe ablation (appendix) and comparison without distractors is useful\nto see, and the description of the distractors at least give a good\nidea (but see below).\n\n\nWeaknesses:\n\nFrom the results table (in the main paper) it appears InfoPower comes\nwith much higher variance than other approaches. For results without\ndistractors (in the appendix), this is almost reversed. The variance\n(and the differences) is not much discussed in the paper, but it may\nbe worthwhile to.\n\nI cannot see the results being easily reproduced for a lack of detail\n/ availability on the set of distractors, and was wondering if future\nwork and comparisons could be helped by data and/or code allowing\nsimilar distractors as those used in the paper.\n\n\nOther feedback:\n\nWhile the approach and solution to the problem is new as far as I\nknow, relevant ideas have been discussed much earlier in a paper by S\nStill, Information theoretic approach to interactive learning, EPL 85,\n2009 https://arxiv.org/abs/0709.1948 that would be worth referencing.\nI'd like to suggest including a brief discussion of the differences\nand objectives of both ideas.\n\nIt might be useful to mention the relative difference in computation\nof the approaches (on similar hardware). Compute times are given in\nthe paper and useful.\n\nTypos etc\n- p2: problem statement section: \"captial\" -> capital\n- p4 bottom: \"summmarise\"\n- In table 1, should the C-Dreamer L2 Rew@1M be bolded (because of the overlap / variances) (as in the level 1\\\n case); the semantics of \"bold\" isn't explained so it's difficult to say.",
            "summary_of_the_review": "The paper is well written and motivated. I find the results compelling\nand useful. There is not much to complain, except it would be good to\nexplore differences in variance of the different methods. Minor issues\nare related to potential reproducibility / experimental setup, and a\nreference to prior work.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper tackles the problem of prioritizing functionally relevant information from complex observations for model-based RL. To this end, previous work has proposed to replace the reconstruction loss with contrastive loss. Building on that, this paper introduces an additional empowerment objective that is used for both representation learning and policy learning. Experiments show that the proposed model outperforms baselines on a set of DeepMind Control tasks with custom background distractions, which include other visually similar but uncontrollable agents. It is also shown that the similarity between learned states can match well to the similarity between groundtruth simulator states, according to the proposed metric.",
            "main_review": "I appreciate the idea of using the empowerment objective for both representation learning and policy learning. The agent-behind-agent distractors are also novel and more challenging than previous work. However, I have some concerns regarding the experiments and the clarity of the paper.\n- While most of the baselines were tested in previous work using the same set of videos from the Kinetics dataset as background distractions, this paper uses a different set of videos without providing enough details. For example, how many videos are there? Are the videos obtained from some specific dataset? Do training and evaluation use the same set of videos? This makes it hard to compare with published results. In particular, some baselines require quite extensive hyperparameter tuning depending on the dataset, and TIA even requires separate hyperparameters for each domain in DMC. The paper did not include any detail about how the baselines were tuned. From the experiments, I do not see the need to change the background videos from previous work, so I think it would be better to compare with the baselines on the same dataset used in DBC and TIA. As for the agent-behind-agent distractors, I think it would be better to have a separate dataset focusing exclusively on this challenging setting, rather than mixing the challenging cases with standard ones.\n- The comparison to baselines seems unfair, as the baselines do not use empowerment for policy learning. In fact, the ablation study in Figure 7 shows that removing the empowerment for policy learning significantly decreases performance of the proposed model, even on dense reward tasks like Walker Walk, where the ablated version underperforms C-Dreamer and TIA (compare to Figure 5). The paper can be strengthened by adding empowerment to the baselines.\n- The paper claims to \"outperform state-of-the-art model-based RL approaches by an average of 20% in terms of episodic returns at 1M environment interactions with 30% higher sample efficiency at 100k interactions\". I do not see how these numbers are obtained.\n- In Appendix A.10 Table 2, the proposed model achieves $1026 \\pm 21$ on Walker Walk without distractions. This seems impossible, and undermines the credibility of the reported results.\n- I find the paper a bit hard to follow. At the beginning of Section 3.1, I did not understand the relation between controllable representations and the conditional independence $I(A_{t-1}; S_t \\mid S_t^+)=0$. Is this a necessary and/or sufficient condition and why? Also the PGM in Figure 2 may need more justification/explanation. Given that both $S_t^+$ and $S_t^-$ affect the reward and transition, is it reasonable to only learn $S_t^+$ and discard $S_t^-$?\n\n**MINOR COMMENTS**\n- The paper can be strengthened by comparing to TPC [1], a contrastive approach that outperforms DBC and CVRL.\n- The total number of steps are different in Figure 5 and 7. It would be better to use the same number of steps for all tasks.\n- What distractor levels are used in Figure 5 and 7?\n\n[1] Temporal Predictive Coding For Model-Based Planning In Latent Space. Nguyen et al., ICML 2021.",
            "summary_of_the_review": "I am leaning toward reject, due to potentially unfair comparisons and incorrect results.\n\n---\n\nMy concerns have been adequately addressed during rebuttal, so I now recommend accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper introduces a variation on the DREAMER model-based RL architecture that aims to remove elements from the latent state representation that cannot be affected by the actor. To realize this, there is an inverse model objective included in the value function.",
            "main_review": "Strengths:\n\n- I like the overall idea. It's not too complicated and the results seem very promising.\n- With one exception, the illustrations and figures are helpful.\n\nWeaknesses:\n\n- W1. The paper is structured badly. It feels like the whole paper was written, it came out as 11 pages and hours before the deadline, hastily important sections were moved to the appendix. For example there are 2 theorems in the main paper but no proof. Why format them as \"theorems\" at all in the main body of the paper if there's no proof right there? I'd just trim this down, only include the description of these in prose and move the \"theorem\" and proof both into the appendix. I also felt like the theory part in the main body of the paper was a bit shallow because it's long and verbose but doesn't feel complete and as easy to follow as I would have expected based on how much space it takes up. Another example of this is section 4.2 where you \"introduce\" a new similarity metric but never explain it in the main body of the paper. Figure 4 doesn't make any sense without consulting the appendix. This needs to go back into the main body of the paper!\n- W2. The writing could have definitely used a proofreading. There are tons of errors in the main doc and a lot more in the appendix. Also some expressions are a bit handwavy and could have used a bit more rigorous writing. E.g. in the intro, you wrote \"... modeling the observation space so as to most quickly capture the functionality relevant factors... \" - (a) You usually don't model the observation space but \"states\" or a \"latent representation\" that's inferred from the observations; (b) \"quickly\"? Why quickly? That's not really explained; (c) \"functionality relevant factors\" - you probably mean \"functionally-relevant factors\", right? (d), functionally relevant to what? To a task, to control, to an understanding of the robot's odometry? You have a good example of this later in Figure 2 but here, without any context, this is too imprecise language.\n- W3. Fundamentally, I think to show your assumption properly, you should've included a task where the observations consists quantifiably of those 3 different factors: S+: images of the robot itself that can be affected by actions, S-: elements that can't be controlled by the agent but influences the reward, DS-: distractors that don't affect the reward. In your current setup, there's only S+ and DS- because there is a random video playing in the background. How about this: Include obstacles on the ground (e.g. in front of halfcheetah and walker) that obviously cannot be affected by actions but would make the agent stumble if ignored. Or alternatively or additionally, instead of a random video, include a video of the agent itself but shifted up or down. These would be a harder but more informative test if your method can actually learn to separate out informative state elements from uninformative ones.\n- W4. Why did you include a model-based version of DBC and not DBC itself? If you compare the results from Fig.3 in the DBC paper with your results in Fig. 5, I can see that DBC is supposed to be significantly better than this. The whole purpose of DBC is to remove distractions from the observations when learning a policy and it is already very sample-efficient (it can reach the performance of your model in 1e5 steps instead of 1e6 or 2e6). And speaking of which, in the appendix you mention training for 10e6 steps. Why didn't you include these results? What made you pick 1e6/2e6 for comparison and not 1e5 or 1e7? Feels a bit cherry-picky.\n\nNitpicks & Questions:\n\n- Q1. Section 4.4 \"the window size for distractors\" is not explained.\n- Q2. Spacing of Fig.3's caption needs to be fixed.\n- Q3. The algorithm 1 block needs to be improved a lot. E.g. you initialize dataset D but not the policy and not lambda? Also what is lambda, why is it important there? Why is the environments not stepped until T but only until T-1? What is the line below \"Compute latents\" refer to? Update your model parameters? If that's the case then you need to specify that these are your model parameters. Where is the loss calculated? Computing the latents is not inherently also calculating the loss.",
            "summary_of_the_review": "UPDATE (2021-11-21): The authors have incorporated all of the suggested changes and addressed my concerns reasonably, which is why I'm now recommending acceptance.\n\nOLD: The paper is overall okay. I don't think it's remotely in its best possible shape (in terms of structure, writing, Figure 4, etc.) but this is nothing that can't be solved with a couple hours editing and rearranging things. The main idea is good, nothing in the paper appears to me a major flaw and so I'm mildly recommending acceptance and strongly recommend polishing!\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a new non-reconstruction method for model-based RL from high-dimensional observations. The main idea is to make use of an information empowerment objective that prioritizes encoding parts of the environment that are influenceable by the actions. This allows the model to focus on functionally relevant information and filter out distractors. The same empowerment term can also be used to promote faster exploration when the reward signal is sparse. The proposed method outperformed the existing baselines in difficult Deepmind control tasks with natural video backgrounds.",
            "main_review": "#### **Strengths**\n- The paper tackles a very important problem in the topic of learning to control from visual observations. In real-life scenarios, the observations are likely to contain distractors or task-irrelevant information, and being able to focus on relevant aspects of the environment in those cases is crucial.\n- The proposed use of information empowerment for learning representations and exploration is novel to the best of my knowledge.\n- The experimental results look promising.\n\n#### **Weaknesses**\n- The first major issue of the paper is the lack of details in the presentation of the practical algorithm. Specifically, it is not clear from the text what components/networks constitute the InfoPower model. In Section 3.4, the authors derived lower bounds of MI terms that will be used to optimize the model. However, it is very confusing to read because the authors have not specified what $p$, $q_{\\mathcal{X}}$, $q_{\\eta}$, and $q_{\\psi}$ are and how they are parameterized. Moreover, these lower bounds also need to be derived in more detail as they are not straightforward from the text. In Appendix A.8, the authors presented the training and network details but again it does not provide enough information to fully understand the components of InfoPower.\n- The second major issue lies in the overall objective function, as there are multiple terms being optimized together. First of all, why do we need to maximize $I(O_t; Z_t)$ if we are already minimizing $I(i_t; Z_t \\mid Z_{t-1}, A_{t-1})$, which encourages encoding the predictable parts of the observations? In previous works [1,2,3,4] people have found that maximizing $I(Z_t; Z_{t-1}, A_{t-1})$, which is similar to minimizing $I(i_t; Z_t \\mid Z_{t-1}, A_{t-1})$, allows to learn a much better representation compared to maximizing $I(O_t; Z_t)$. While being a contrastive loss, $I(O_t; Z_t)$ does not behave differently from a reconstructions loss, as it requires the encoder to retain as much information in the observation as possible, which is contradictory to the motivation of the paper.\n- The use of both $I(i_t; Z_t \\mid Z_{t-1}, A_{t-1})$ and $I(A_{t-1}; Z_{t} \\mid Z_{t-1})$ also seems redundant. In fact, $I(A_{t-1}; Z_{t} \\mid Z_{t-1}) = I(A_{t-1}, Z_{t-1}; Z_t) - I(Z_t; Z_{t-1})$ and therefore maximizing $I(A_{t-1}; Z_{t} \\mid Z_{t-1})$ will also maximize $I(A_{t-1}, Z_{t-1}; Z_t)$, which has similar effects to minimizing $I(i_t; Z_t \\mid Z_{t-1}, A_{t-1})$.\n\n#### **Other comments and suggestions**\n- While the assumption that the features of the observations influenceable by the agent through its actions are important for control is true in the experiments that the paper considers, it is not true in general. Consider a robot trying to navigate from A to B by using visual information, its actions such as turning left or right can actually influence the distractors (trees, irrelevant subjects, etc.) in the scene that it observes. However, this can be left for future work.\n- The paper is missing some important references. PC3 [2] is one of the first methods that proposed to use mutual information/contrastive learning to learn representations for control. TPC [1] is a very relevant paper that was also proposed to address the problem of learning representations for visual model-based RL with distractors in the observations. TPC outperforms Dreamer, CDreamer and DBC and should serve as a stronger baseline. PI-SAC [3] is also a method related to the line of work that uses mutual information to learn representations for RL. In [4] the authors theoretically investigated different mutual information objectives used to learn representations for RL.\n- I want to see the performance of InfoPower vs the baselines in the standard setting (no natural backgrounds), as a good method should work well in both standard and natural background settings.\n\n[1] Nguyen, T., Shu, R., Pham, T., Bui, H. and Ermon, S., 2021. Temporal Predictive Coding For Model-Based Planning In Latent Space. arXiv preprint arXiv:2106.07156.\n\n[2] Shu, R., Nguyen, T., Chow, Y., Pham, T., Than, K., Ghavamzadeh, M., Ermon, S. and Bui, H., 2020, November. Predictive coding for locally-linear control. In International Conference on Machine Learning (pp. 8862-8871). PMLR.\n\n[3] Lee, K. H., Fischer, I., Liu, A., Guo, Y., Lee, H., Canny, J., & Guadarrama, S. (2020). Predictive information accelerates learning in rl. arXiv preprint arXiv:2007.12401.\n\n[4] Rakelly, K., Gupta, A., Florensa, C. and Levine, S., 2021. Which Mutual-Information Representation Learning Objectives are Sufficient for Control?. arXiv preprint arXiv:2106.07278.\n",
            "summary_of_the_review": "In general, the proposed method is promising, but the current presentation of the paper has major issues that need to be addressed and clarified.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}