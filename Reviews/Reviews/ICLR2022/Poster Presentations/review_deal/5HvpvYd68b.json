{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes several innovations for machine translation. The reviewers had several questions about the claims that were made and the authors addressed these and also acknowledged that some of their formulations (e.g. 'better') would need to be qualified. Overall, there are several interesting ideas that have been put together in a sensible way, but the story is not super consistent.\n\nThe detailed exchanges between the reviewers are authors are commendable!"
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper describes experiments in training a non-autoregressive Transformer (glancing Transformer, or GLAT) in a multilingual MT setting. The use of a non-autoregressive model, plus token-level language tags, enables a code-switching strategy where a source sentence in a given language can generate a target sentence containing words from multiple specified languages. This is used to augment the multilingual training corpus with back-translated data consisting of code-switched synthetic source sentences paired with original monolingual target sentences. A multilingual GLAT model trained with this strategy is shown to outperform many baselines including monolingual GLAT and several autoregressive multilingual models. Ablation studies demonstrate that code-switched back-translation is essential for achieving good performance.\n",
            "main_review": "Strengths:\n- Novel application of NAT to a multilingual setting, with an interesting new strategy for code-switched back-translation.\n- Impressive gains over a fairly broad selection of baselines, including strong auto-regressive models.\n- Potential to be a significant breakthrough in increasing both speed and accuracy of multilingual models.\n\nWeaknesses:\n- The capacity used is quite small for multilingual models. It certainly seems sub-optimal for the WMT-many setting, where both bilingual baselines are starting to pull away from the multilingual models. Since capacity is such a crucial parameter for multilingual models, experiments to show how this new technique compares as models grow would have made the paper more convincing.\n- There are two new things in this paper: the application of NAT to a multilingual setting, and the use of code-switched back-translation. It would have been interesting to tease them apart using NAT architectures other than GLAT: how well do other recent NAT models work on multilingual data; how much do they benefit from code-switched back-translation (since that idea should apply to any NAT model)? The NAT baseline used in the paper is not competitive.\n- There are many hyper-parameters involved in the setup. No information is supplied about how carefully they were tuned, or how they would generalize to new settings.\n- Some problems with clarity, see details below.\n\nDetails:\n- This is hard to understand: “\tSuch efficiency problem is more serious in multilingual setting because serving many translation directions in one model always leads to a higher throughput practically.”\n- This statement should be qualified: “NAT generates translation outputs in parallel (Gu et al., 2018), which leads to significantly faster translation speed.” See: Kasai, Jungo, et al. \"Deep encoder, shallow decoder: Reevaluating non-autoregressive machine translation.\" arXiv preprint arXiv:2006.10369 (2020).\n- s3.2 isn’t very clear. The code-switched generation (middle panel in figure 2) seems to have abandoned the GLAT training idea, and just be the standard CMLM algorithm with language tags added to the masks to get code-switched output?\n- s4.2 How exactly does switch-GLAT-w/o-glancing work?\n- 4.4.2 \"switch-GLAT is superior to all multilingual baselines on the average BLEU score.” - not really, it’s basically tied with Adapter and M-Transformer.\n",
            "summary_of_the_review": "The first application of non-autoregressive transformers to the important problem of multilingual MT, with a novel idea (code-switched back-translation) specific to that application. Results are generally impressive, but the experiments are missing crucial model-scaling data points, and they miss the opportunity to evaluate code-switched back-translation independent from the GLAT architecture the authors favour.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper extends non-autoregressive NMT models, particularly the GLAT model, from bilingual translation to multilingual translation. To enable such extension, the authors proposed token-level language tags for the decoder, code-switch decoder paired with back-translation, and also scheduled training between the traditional MLE loss and the BT loss. Experiments on several multilingual tasks based on WMT corpus show that the proposed model obtains better supervised translation quality than its auto-regressive counterparts and delivers better cross-lingual representations.",
            "main_review": "Strengths:\n\nThe authors examined the effectiveness of non-autoregressive NMT on multilingual translation, and obtained encouraging supervised translation performance on several data conditions.\n\n\nWeaknesses:\n\n* Some claims and proposals are not well supported by the current experiments;\n* Experimental details are not always clear to me;\n* Model comparison might be unfair and misleading, and more comparisons are required.\n\n\nComments:\n\n1)\tFirstly, at the core of the adaptation of the GLAT model to multilingual translation is the newly proposed code-switch decoder. This decoder uses token-level target language tag to produce code-switch translations. But the necessity of such design is not tested. Why use code-switch decoder not the vanilla GLAT decoder? A comparison to a vanilla multilingual GLAT should be given.\n2)\tAlso, the authors propose code-switch back-translation to improve model’s performance, but don’t prove why such specific BT is favorable or necessary. What if using the vanilla BT to the NMT model? For example, this can be achieved via random online back-translation proposed in [1].\n3)\tThe authors show that the proposed model improves cross-lingual representation compared to auto-regressive NMT models, but results in Table 3 show that BT delivers a large impact on such improvement without which switch-GLAT performs much worse. Then a question would be: what if applying BT to auto-regressive baselines as well (such as the above mentioned random online BT)?\n4)\t Some experimental details are very unclear! How did you train your baseline models, CLSR and Adapter? What’s the hyperparameters? For CLSR, what’s your budget constraint? How did you set it? In addition, how did you train the MNAT baseline? Did you apply the recently introduced optimization techniques as in [2]?\n5)\tYou mentioned in page 5 that knowledge distillation is used. What’s your teacher model, bilingual Transformer base or multilingual Transformer base? Did you also apply the KD to all baselines (autoregressive and non-autoregressive)? I would assume that the authors used bilingual models for KD and only applied it to the NAT models. In such a case, claiming multilingual NAT “outperforms” multilingual AT is non-convincing.\n6)\tAs explained in the introduction, multilingual NMT benefits low-resource translation, which also enables zero-shot translation. Neither are explored in the current experiments. WMT also offers low-resource languages. Would switch-GLAT benefit these languages in a multilingual setup? What about switch-GLAT’s zero-shot ability?\n\n[1] Zhang et al., 2020 Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation\n[2] Gu et al., 2020 Fully Non-autoregressive Neural Machine Translation: Tricks of the Trade\n",
            "summary_of_the_review": "In short, this paper explores an interesting direction: applying non-autoregressive models to multilingual translation. But the experiments suffer from unclarity, unfair comparison and over claims. The paper could be largely improved with more analysis and comparisons.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposed a novel method for the multilingual non-autoregressive machine translation (NAT) model. The key idea is to modify the decoder module of glancing Transformer (GLAT), making it generate the contextual code-switched translations, then use them to perform code-switch back-translation. To incorporate with the generation of contextual code-switch translations, the authors proposed to add token-level language tags to the first layer and the final output layer at each position. The empirical results show that: 1) the inference speed is faster than the autoregressive baseline model (M-Transformer). 2) the translation performance outperforms the NAT baselines in terms of WMT-EDF, WMT-EFZ benchmarks. Although the performance is slightly worse than the GLAT model on the WMT-Many benchmark, the author claim that the reason is due to the limitation of model capacity. 3) Two cross-lingual experiments and the visualization of representation prove that the proposed method has better cross-lingual capability compared to other multilingual models. ",
            "main_review": "**Strengths**: \n1. The paper presents a multilingual solution of NAT. The method works with GLAT and provides good insights on how to learn a good cross-lingual representation for NAT.\n\n\n2. The proposed method speeds up the decoding process of the multilingual Transformer (M-Transformer). The generation quality of the proposed method is close to the bilingual autoregressive models (Transformer).\n\n\n3. The author conducted comprehensive analytical studies on the improvement of the proposed method. It is clear to see how well the learned representations are aligned from the cross-lingual perspective. Besides, the word induction and parallel sentence retrieval task support the alignment ability in word-level and sentence-level, respectively.\n\n**Weaknesses**: \n1. The proposed method builds on the top of the GLAT method, which limits its generality. According to the ablation study, the performance dropped a lot without using the glancing mechanism (w/o glancing in Table 1 and 2).\n\n\n2. Although I can understand that how the proposed method works, the description text of Figure 2 seems to be problematic. In Section 3.1, 3.2, the authors said that the code-switch decoder and code-switch back-translation process are illustrated in the left and middle modules, respectively. It is quite confusing that why they do not match the upper annotations shown in Figure 2.\n\n\n3. There may be an error of mathematical symbol in Section 3.3. What does $L_{CS}$ refer to? Should it be $L_{bt}$?\n\n\n4.    Missed one result of representation visualization. I understand that the adapter cannot learn cross-lingual alignment well because it models language-specific capability. Since CLSR can learn the shared pathway and language-specific capability together, why the authors did not choose to compare it in Section 5.1?\n\n\n5.    The explanation that why switch-GLAT performs slightly worse than GLAT does not support by the experiments. The authors claim that it “may be owing to the limitation of model capacity”, will a bigger model or a balanced training set be helpful to alleviate this problem? It should be verified by additional experiments.\n\n\n6. In Section 3.2, the authors said that “In the ﬁrst iteration of $P_M$, the number of mixed languages is set to 1. Afterward, it will be increased to one-third of the total.” There are four languages in WMT-EDF and WMT-EFZ training sets, does it mean that the number of mixed languages is always set to 1 (4*1/3~=1) during the training process? In this case, how the cross-lingual alignment is learned?\n",
            "summary_of_the_review": "The paper is well-written and contributes a novel approach for multilingual translation in NAT. The design of the code-switch back-translation is reasonable for boosting the performance of multilingual translation and the empirical analysis is convincing. Experiments show that switch-GLAT is better than other NAT/multilingual baselines in limited benchmarks and has close performance compared to bilingual AT baseline.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}