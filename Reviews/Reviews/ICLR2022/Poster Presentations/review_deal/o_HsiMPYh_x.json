{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The authors propose a simple method to estimate the accuracy of a classifier on an unlabeled dataset given an in-distribution validation set. In extensive experiments the authors show that the proposed method is significantly more accurate than previous methods and other baselines. \n\nThe reviewers are quite consistent in their judgement, just the weighting of the different aspects is different.\nAfter the rebuttal four out of five reviewers recommend acceptance.\n\nStrong points:\n- simplicity of the method\n- strong experimental results for various tasks and domain shift problems\n\nWeak points:\n- there is no clear theoretical statement when the method is supposed to work\n- the discussion in Section 3.1 is pretty obvious and seems a bit like a waste of space whereas the motivation for the actual method is very short\n\nWhile I agree with the reviewers that there is little theoretical justification for the method, the strong experimental results on various datasets, tasks and different domain shifts make this paper interesting for a large audience. Thus this paper is a nice contribution to ICLR and I recommend acceptance.\nHowever, I strongly recommend to the authors to add more motivation in Section 4 and add a limitation section where the cases are discussed where the method is definitely not working. Section 3 is pretty obvious and could be significantly shortened or integrated into the limitations section. \n\nOne case which is highly relevant for this limitations section is the provable asymptotic overconfidence of neural networks which is discussed in\n\nHein et al, Why ReLU networks yield high-confidence predictions far away from the training data and how to mitigate the problem, CVPR 2019\n\nThis would definitely lead to a failure of the presented method as all predictions would get a score above the threshold. I would also assume that the method would predict high accuracy values for out-of-distribution tasks which are semantically similar e.g. training on CIFAR10 and then using CIFAR100 as unlabeled dataset. In that context it would be interesting to evaluate OOD-aware classifiers using ATC such as discussed in\n\nHendrycks et al, Deep Anomaly Detection with Outlier Exposure, ICLR 2019\n\nAlso it would be helpful to understand better the influence of the classifier performance on the original task on the performance of ATC on unlabeled data."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a simple yet effective technique for estimating a classifier's accuracy on test data which exhibits distribution shift from the training data. Given a score function for the model and a data point, the technique finds a threshold such that such that the relative proportions of validation data above and below the threshold are the same as correctly vs incorrectly classified points, respectively. This threshold is then used to predict the model's accuracy on test data, and the paper demonstrates that this predictor is significantly more accurate than prior methods on a range of distribution shift benchmarks. There are also some theoretical and empirical results on a toy problem that seem to indicate that the proposed technique is well-suited for at least this problem.",
            "main_review": "Strengths\n---\nI am not an expert in this particular area, but to the best of my knowledge, the proposed technique is novel. Moreover, it is demonstrably more effective than prior methods and particularly simple to implement and use.\n\nThe paper is of relatively high quality. An extensive number of experiments are carried out on different distribution shift benchmarks, and it seems like the proper prior methods are compared to, though again, I am not an expert.\n\nThe paper is well-written and well-structured. This is aided in particular by the fact that the proposed technique is so simple to begin with.\n\nFinally, the paper is likely to be of practical interest for researchers and practitioners studying and dealing with distribution shift.\n\n\nWeaknesses\n---\nThere are still a few substantive ways to improve the quality of the paper. First, it seems that neither of the theoretical sections (3 and 6) tie in well with the rest of the paper. In my view, the paper is primarily an empirical contribution. Section 3 ostensibly says nothing specific about the proposed technique, it just presents some general results that seem relatively obvious (though I am unsure whether they have been explicitly stated in prior work). But does the theory here motivate the proposed technique in any way? This is unclear to me. Section 6 is more closely related to the empirics, but this connection could still be improved. What are the key insights we can take away from the toy problem? How does it allow us to better understand for which real world problems the technique may succeed or fail? In my view, the paper could do a better job at addressing these questions.\n\nRegarding empirics, the paper is significantly stronger here, but there is still some room for improvement. Most concretely, evaluations on the other WILDS datasets could be quite interesting. This is not (just) a generic \"add more experiments\" comment: all of the current experiments focus on natural (with the exception of some rendition and sketch shifts) object-centric images, and WILDS contains other modalities such as medical and satellite images, which makes for a rather interesting additional testbed. WILDS also contains language tasks, which would be separately interesting to evaluate on, though I acknowledge this may take more time to set up.\n\nRegarding clarity, there are some minor ways to improve the paper. Section 4 could be expanded to provide more intuition and formal arguments for why the proposed technique makes sense and is justified, ideally from first principles. In fact, I may go as far as to say that this type of exposition would be more useful than Section 3 entirely, to the concerns I have noted above. Figures 2 and 3 were confusing to parse at first -- I think the lines are regression fits, but this should be made explicit if it currently isn't. Table 1 is a bit confusing in the first two columns because the datasets are listed but not necessarily the test sets. Finally, some of the comparisons such as IM should be explained in more detail.",
            "summary_of_the_review": "Primarily due to my concerns above, I am initially recommending a weak accept of this paper. I am happy to engage in discussion with the authors and other reviewers in order to reach a more confident final recommendation.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work proposes a simple method, Average Thresholded Confidence (ATC), to predict the OOD accuracy of a classifier based on a labeled source distribution and an unlabeled target distribution. First, the authors show a theoretical lower bound that such task is impossible without assumption on the nature of the shift. Then, the authors proposes ATC. Experiments show that ATC empirically performs better than baselines on different datasets and models. Theoretically, they show that ATC is a consistent estimator for a toy setting with spurious features.",
            "main_review": "Strengths:\n1. The experiments are thorough, conducted on a variety of types of shifts and models.\n\nWeaknesses:\n1. The proposed method lacks theoretical rigor. The threshold for source and target would differ based on different types of shifts, and there's no reason why the source threshold should be used to predict target accuracy. The fact that it works empirically is not a good justification.\n2. No theoretical analysis are are done on NE vs. MC. When should we use one over the other, and why?\n3. Section 3 doesn't fit with the rest of the paper. Since there's no general-purpose estimator, why does this paper still propose such an estimator? The impossible results are not surprising. It is perhaps more meaningful to study principled estimators under specific settings, just like the covariate shift or label shift settings.\n4. I wish to see the theoretical analysis on the toy setting generalized. What are the necessary and sufficient conditions for ATC to work?\n5. I wish the authors compare with other calibration methods beyond temperature scaling.",
            "summary_of_the_review": "The paper is strong empirically but lacks theoretical rigor.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a new scoring mechanism, Average Thresholded Confidence (ATC), to estimate out-of-distribution performance (i.e. accuracy) of a trained classifier using only labelled data from the source distribution (train distribution) and unlabelled data from the target distribution (test distribution). \n\nThe threshold is based on the logit output of a classifier and can thus be used on unlabelled data.\n",
            "main_review": "Overall, I find the paper quite interesting and intuitive. The mechanism is simple and seems to be effective compared to prior work. Given that this is an important real-world problem, I think the paper deserves to be accepted at ICLR. \n\nHowever, I am not a 100% familiar with the related work in order to judge the novelty of the paper. \n\n## Strengths\n\n1. an appropriate amount of experiments\n2. interesting discussion of related work\n3. relevant theoretical results to motivate the problem\n\n## Weaknesses and other Remarks\n\n1. **Derivation and/or Interpretation of ATC**:\nThe authors did a decent job of providing some related theoretical results in the beginning. However, when the authors continue to introduce ATC this seems fairly ad-hoc without any kind of deeper explanation. I think this would merit a longer discussion.\n\n2. **Proper contextualization of theoretical results**:\nAs I said before, I appreciate the theoretical results as motivating examples for considering empirical scores. However, I don't think they can be considered actual strong contributions of the paper. The main contributions are a heuristic-based score and accompanying experimental results showing the effectiveness. So please make sure that this is reflected in the writing. \n\n3. **Clarification around some experimental settings**: \n   a. Could you provide some confidence intervals around the scatter plot in Figure 2 and related figures, e.g., using bootstrapping. Why are there so many more sampled points around higher accuracies? \n   b. Could you add more discussion around temperature scaling (cf. _For all methods, we implement post-hoc calibration on validation source data with Temperature Scaling_?) Why is this relevant and what's the intuition? If such an approach improves your performance, could also try improving your performance using more sophisticated calibration methods? That would be interesting to see. \n\n\n## Ways to Improve My Score\n\nPlease address the weaknesses and I have listed above. I would be happy to engage in discussions during the rebuttal period.\n\n--- \n\n\n## Update after rebuttal\n\nThe authors addressed my concerns in their responses. Consequently, I am raising my score from 6 to 8. My confidence score will remain at 3 given my missing knowledge about some of the prior work. ",
            "summary_of_the_review": "The paper is well-written and I enjoyed reading it. I cannot 100% verify the novelty of the paper since I am not so familiar with the related work but given what the authors wrote in their related work section and their results it seems novel. \n\nOverall, I didn't have any major complaints about the paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper introduces Average Thresholded Confidence (ATC), a method that aims to predict a model's accuracy on an unlabeled test distribution that varies from the original training distribution by some form of distribution shift. \nThe paper first gives a theoretical analysis that shows that, in general, it is impossible to estimate test accuracy without assumptions on the particular distribution shift and that each method that tries to solve this problem has some underlying assumption on the distribution shift.\n\nTheir method ATC selects a threshold t on a scoring function (max confidence, or negative entropy) based on the error in the original source domain. The estimated error is then equal to the number of points in the test set that have a score lower than t. Importantly, this is done without using any label information from the test set. \n\nIn their experiments, they demonstrate improvements over various baseline methods on a large selection of datasets and distribution shifts. \nFinally, they have a toy example with very strong assumptions on which ATC-MC is a consistent accuracy estimator. ",
            "main_review": "-The proposed method is simple and easy to understand. I also think that their setting where no labeled data from the test distribution is available is highly relevant.\n\n-Their evaluation contains a large number of datasets and distribution shifts and clearly demonstrates that ATC is superior to baseline methods. The only thing that is not quite clear is which scoring function should be used in practice, as there exist certain configurations where ATC-MC and ATC-NE perform the best. \nThey also compare to a regression model that is trained with labeled data and are able to achieve similar performance without this extra information. \n\n-Is there an intuitive understanding why temperature rescaling helps? While temperature rescaling preserves the classification of the model, it can vary the ordering of the scores on different samples, however I would not have expected such large changes, especially in the order of ATC-NE pre and post T. Are there specific logit configurations that clearly show how temperature rescaling helps? \n\n-I liked the initial theoretical discussion of the problem. The authors state that \"every method of estimating accuracy on target data is tied up with some assumption on the nature of the shift\", is it possible to elaborate on the assumptions of ATC in this context? \n\n",
            "summary_of_the_review": "The paper offers some good theoretical insight into the general problem and introduces a simple method that clearly outperforms the baselines. The problem statement is interesting and very relevant to the community. The evaluation is extensive and clearly demonstrates the strengths of this paper. While the theory does not cover all aspects (eg temperature rescaling), I liked the paper overall. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies how to predict the target domain accuracy using only labeled source data and unlabeled target data. They propose Average Thresholded Confidence (ATC) to learn a threshold on the model's confidence. In particular, ATC predicts accuracy as the fraction of unlabeled examples for which model confidence exceeds the threshold. Extensive experiments have been conducted to show that ATC outperforms prior work across multiple model architectures, types of distribution shifts, and datasets. Theoretically, they prove that identifying the accuracy is as hard as identifying the optimal predictor, and the efficacy of any method depends on the assumptions on the nature of the shift.",
            "main_review": "The problem of predicting accuracy of the models is interesting and novel to me. How to identify the accuracy of a model under distribution shift is a practical challenge. The proposed method of ATC is extremely simple and makes intuitive sense. It is also reasonable to claim that identifying the accuracy is as hard as identifying the optimal predictor, and the efficacy of any method depends on the assumptions on the nature of the shift. \n\nQuality: The submission is technically sound. The claims in the contribution are well-supported by theoretical analyses and empirically results. It is a complete piece of work that outperforms prior work empirically.\n\nClarity: This paper is well-written and easy to follow. The problem is also well-motivated. The experimental details are also very specific, such that reproducing the results should be possible. ",
            "summary_of_the_review": "This paper proposes a novel method, Average Thresholded Confidence (ATC) to learn a threshold on the model's confidence. It is well-written and well-motivated. The proposed idea is simple and technically sound. The claims are well-supported by theoretical analyses and extensive experimental results.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}