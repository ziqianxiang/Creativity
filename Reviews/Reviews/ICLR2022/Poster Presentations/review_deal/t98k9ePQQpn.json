{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "All reviewers agreed that the idea proposed by the paper is interesting and is well-motivated for handling long-tailed recognition problems. \nAs suggested by the reviewers, it seems important that the limitations the paper be addressed in the final version of the paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a new method for post-hoc correction in long-tailed recognition. Specifically, they leverage the idea of optimal transport (OT) and propose a linear mapping to replace the original exact cost matrix in OT problem. From the experiments, the proposed method can be combined with existing methods and boost their performance further. ",
            "main_review": "Strengths\n1. The idea to perform post-hoc correction from the perspective of optimal transport seems novel. Even though the detailed operation in practice is not complicated. Such an idea is well supported by the existing theory guarantees. \n\n2. The results in the experiments also seem good. The proposed method can be applied to different baselines to boost their performance. \n\nWeakness\n1. Since this paper has many mathematical expressions, I suggest the authors give more detailed descriptions of the notations used. For example, in Section 2, what is r, c in U(r, c)? Is the c in equation (5) the same as c in U(r, c)?\n\n2. For definition 1 and 2, the author may remark more for better understanding, especially when the notions used are not directly related to the proposed method. The introduction of some notations also seems missing or hard to get. \n\n3. In equation (11), are the parameters W and Y optimized iteratively? The introduction of W also seems abruptly. A more natural and smooth transition between 3.1 and 3.2 may be needed. \n\n4. To be honest, according to the description in the introduction, I did not get well about the difference between the proposed method and the previous Logit Adjustment. Maybe more details are needed. \n",
            "summary_of_the_review": "Overall, the idea from OT to deal with long-tailed problems is novel. However, the writing of this paper makes it not very easy to understand or follow. Especially for people who don't have much background on OT. I thus give a score of 5.\n\n================================update===================================\n\nThe author addresses most of my concerns. Also, the latest version has admitted its limitations and put a more clear clarification on its advantages. I thus decide to raise the score to boardline accept.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper contributes an extension of post hoc correction of long-tailed recognition with Optimal Transport (OT). Unlike the previous work (e.g. logit adjustment) which focuses on sample-wise correction, this work, on the other hand, considers the marginal distribution of the overall data for correction. The method is further extended by learning a cost matrix. Their experiments show the effectiveness of optimal transport in the long-tail recognition problem via comprehensive comparison with previous works in terms of performance and efficiency.",
            "main_review": "General comments:\nThe overall idea proposed by the paper is interesting and seems well-motivated. Though applying optimal transport to deal with imbalanced data is not particularly new[1], the proposed method in a post hoc manner is generally more flexible. With extensive experiments, the method is demonstrated to be effective. \n\nStrengths:\n1. The paper is self-contained and the main idea can be easily grasped. The introduction clearly states the research question and the motivation of the work.\n2. Formalizing the post hoc correction from an OT perspective seems interesting and well-justified. How the authors draw the connection between OT and logit adjustment is particularly appealing to me. \n\nHowever, some substantial concerns should be addressed before the paper can be accepted.\n\nWeaknesses or Concerns:\n1. Does the paper inherently assume that the batch size during evaluation is rather large? I am asking this because the constraint in equation (5) may fail if there are only a few images (e.g. 2 images) in a batch. In practice, it is unlikely that we can always have a batch with the same distribution as the desired one. Sample-wise correction may have its downsides, but it fits in most setups.\n2. The training of OTLM seems to depend on the convergence of Sinkhorn’s algorithm. I am wondering how long would it take to train the OTML? For a post hoc approach, we generally expect it to be flexible. If the post hoc method involves learning from the data, I am not sure whether OTLM can generalize from one dataset to another. \n3. It seems to me that the variant, OTLM, is not well-justified. From Table 2 and Table 3, I notice that the performance of OTLM is not much different from OT(sometimes even worse) but with a significantly slower inference speed. I would be thankful if the authors could elaborate more on why do we need linear mapping and what has it learned.\n\nMinor Issues:\n1. Why the evaluation of OTLM is much slower than OT even on GPU? Isn’t there only one layer of feed-forward network?\n2. ‘our method outperforms by RIDE by 1.0% and 3.0%’ -> sounds a bit over-claimed to me since the results obtained by coupling RIDE.\n3. For iNaturalist, the training and evaluation of OTLM are both done on the validation set. It seems like an unfair comparison to me.\n\n[1] Yan et al., Oversampling for Imbalanced Data via Optimal Transport, AAAI 2019",
            "summary_of_the_review": "Despite some drawbacks on the assumption made and experiment evaluations, the paper would still benefit the community. I therefore lean towards a positive direction at this moment.\n\nUpdate:  As mentioned by reviewer uP2E, the strong assumption of knowing the marginal distribution of labels is not desirable in real-world scenarios. The proposed approach can be only applied to offline applications. I would recommend the authors include the following items in the future version:\n\n1. The performance of the proposed approach with an estimated marginal distribution;\n2. How can one apply the proposed method to online scenarios? (e.g. store evaluated samples to form a large batch)\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes to solve the long-tailed recognition by aligning the predicted label distribution with the test label distribution via Optimal Transport (OT).",
            "main_review": "Strength:\n\n1. Post-hoc adjusting the prediction via OT is novel.\n2. The proposed method is technically sound. By accurately aligning the predicted label distribution with the balanced label distribution (Table 4), the paper successfully outperforms the state-of-the-art post-hoc adjustment methods in terms of recognition accuracy (Table 1, 2).\n3. A convergence analysis for the proposed OTLM is provided (Sect 4.5), making the paper easier to be reimplemented and reproduced.\n\nWeakness:\n\n1. The described post-hoc correction may not be feasible in real-world applications. \n\n   In Sect 3.1, the authors assume that the \"expected distribution on the test set\" is known, followed by an explicit constraint $Y^T1_N=c$ for the optimization. In another word, the paper assumes the test set to have a balanced label distribution and optimize the predictions towards the balanced label distribution.\n\n   However, the assumption is not always true. The test set in many long-tailed datasets is not balanced, e.g., the LVIS dataset. Those datasets use balanced metrics, e.g., mAP, to measure the model performance instead. In this case, assuming that the test set's label distribution is known might be improper. \n\n   A follow-up question would be how to employ the method in real-world applications, where the test set could be small? If we try to make a single-image prediction, it may not seem possible to conduct the proposed OT optimization on only one data point. How should the method adapt to these scenarios?\n\n   Note also that the proposed method takes significantly longer inference time than compared methods (Table 3).\n\n2. Some empirical analysis is not sufficient. 1) In both Table 1 and Table 2, OTLM+RIDE seem to be missing. Can OTLM combine with ensemble-based methods? 2) What is the effect of the entropy regularization coefficient $\\lambda$ in Equation 9? There lacks of an ablation analysis.\n\n3. Some minor errors: 1) \"...how it can be achieved simply one-layer neural network.\" -> \"...how it can be achieved simply by a one-layer neural network.\" 2) \"As a matter of fact, as long as we make sure the entire optimization process is differentiable.\" The sentence seems to be unfinished. 3) \"DAPR\" in both Table 1 and Table 2 should be \"DARP\". 4) Table 1 and Table 2 substantially overlap with each other. They have inconsistent precisions as well. Merging them into one table could make it clearer. 5) Figure 1 is a little misleading. Should the feedforward network have the same amount of input and output (four in the case)?\n\n\n\n",
            "summary_of_the_review": "I recommend a borderline reject. Although using OT is a novel idea, my major concern is that it could be infeasible to employ the OT-based method in practice. Thus, I am unsure if the new direction is worth further exploration. \n\n=========== after reading author's latest revisions======\n\nI find the limitations of the methods adequately addressed in the latest revision. I agree with the authors that this paper's setting is consistent with some existing literature and the proposed method can be useful in some offline applications after reading the author's feedback. I would like to raise my recommendation from borderline reject to borderline accept.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}