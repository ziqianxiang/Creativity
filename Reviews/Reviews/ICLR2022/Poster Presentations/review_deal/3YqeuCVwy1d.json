{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes to use Anderson Acceleration on min-max problems, provides some theoretical convergence rates and presents numerical results on toy bilinear problems and GANs.\n\nAfter the discussion, the reviewers agreed that this paper makes a nice contribution to ICLR. Some concerns were originally expressed in terms of incrementality of the theoretical results with respect to previous work (KCYs, gBHU), but the authors have well clarified their contributions in the discussion, and have updated their manuscript accordingly. There were also initial concerns about the related work coverage, but this was also properly addressed in the rebuttal, with additional experimental comparisons as well as extended related work section, as well as an additional convergence result for convex-nonconcave problems."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "Minimax optimization is a central problem in machine learning. The gradient descent ascent method is the most commonly used algorithm to solve this problem. This paper views the gradient descent ascent method as a fixed-point iteration and solves it using Anderson Mixing to converge to the local minimax. They show theoretically that the algorithm can achieve global convergence for bilinear problems under mild\nconditions. Some numerical experiments have been conducted.\n",
            "main_review": "Strengths:\nS1 The authors propose using Anderson mixing strategy to solve the minimax optimization problem.\nS2. They theoretically show that the proposed method can achieve global convergence guarantees under mild conditions.\nS3. They conduct numerical experiments on some minimax problems for the CIFAR10 and Celeb A datasets to show that the proposed method converges faster for some convex-concave and non-convex-concave functions.\n\n\nWeaknesses:\nW1. The authors only discuss the convergence results for the simple bilinear games problems. They do not discuss the general nonconvex-nonconcave problem or the convex-nonconcave problem.\n\nW2. The Anderson acceleration technique is a standard method for accelerating the fix-point procedures. The algorithms developed in this paper seem incremental.\n \nW3. Many important references in this field are missing (See below). The authors should make numerical comparisons or discuss the differences between these methods.\n\nReferences:\n[1] A Single-Loop Smoothed Gradient Descent-Ascent Algorithm for Nonconvex-Concave Min-Max Problems. NeurIPS 2020.\n[2] An accelerated inexact proximal point method for solving nonconvex-concave minmax problems. SIOPT 2021. \n[3] On gradient descent ascent for nonconvex-concave minimax problems. ICML 2020.\n[4] Solving a Class of Non-Convex Min-Max Games Using Iterative First Order Methods. NeurIPS 2019.\n[5] Efficient Search of First-Order Nash Equilibria in Nonconvex-Concave Smooth Min-Max Problems. SIOPT 2021. \n",
            "summary_of_the_review": "See above.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper focuses on solving minimax optimization using Anderson mixing. Anderson mixing is a framework to accelerate fixpoint iteration. The authors tried to use  Anderson mixing for the minimax optimization problem. In their framework, they use a weighted average of the solution of last p iterates and optimized over the weights to come up with a new point. \nThey show that for the bilinear case, this algorithm converges to the optimal stationary point. Finally, the authors simulated their method on synthetic examples as well as image generation on CIFAR10. ",
            "main_review": "1) From the theoretical point of view, multiple analyses are missing. The only results are for GDA and bilinear cases. The authors didn't provide any analysis of their framework for the convex-concave case.\n\n2) Authors didn't provide any result for the Nonconvex-Nonconcave case( convergence to $\\epsilon$ stationary point). I think the result in [1] can be used to provide guarantees in this case. This is necessary to show the effectiveness of the method for GAN-related applications(e.g., the one in section 6.2).\n\n3)Author motivates the subject from a theoretical and practical point of view. \n\n4)In the introduction, the author reviewed the previous works carefully.\n\n5)The authors successfully provide simulation results for their algorithm.\n\n6)The paper is well-organized and well-written.\n\n7)the code is included for the bilinear case, but I couldn't find the code for the image generation experiment.\n\n\n[1]Wei, Fuchao, Chenglong Bao, and Yang Liu. \"Stochastic Anderson Mixing for Nonconvex Stochastic Optimization.\" arXiv preprint arXiv:2110.01543 (2021).\n-------------------------------------------------------------------\nPost rebuttal :\nI have read the authors response, and I increase\nmy score, but they should add the result on \ngeneral convex-concave problem and non-concave\nsetting and the discussion about it in the final version.\nAlso, I think there is still room to get faster convergence\n to saddle point that would be interesting if they can add \nto the final version.\n------------------------------------------------------------------",
            "summary_of_the_review": "1) From the theoretical point of view, multiple analyses are missing. The only results are for GDA and bilinear cases. The authors didn't provide any analysis of their framework for the convex-concave case.\n\n2) Authors didn't provide any result for the Nonconvex-Nonconcave case( convergence to $\\epsilon$ stationary point). I think the result in [1] can be used to provide guarantees in this case. This is necessary to show the effectiveness of the method for GAN-related applications(e.g., the one in section 6.2).\n\n3)Author motivates the subject from a theoretical and practical point of view. \n\n4)In the introduction, the author reviewed the previous works carefully.\n\n5)The authors successfully provide simulation results for their algorithm.\n\n6)The paper is well-organized and well-written.\n\n7)the code is included for the bilinear case but I couldn't find the code for the image generation experiment.\n\n\n[1]Wei, Fuchao, Chenglong Bao, and Yang Liu. \"Stochastic Anderson Mixing for Nonconvex Stochastic Optimization.\" arXiv preprint arXiv:2110.01543 (2021).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a new optimization method for min-max optimization (That could actually be generalized to any variational inequality problem) based on Anderson Mixing. They prove the convergence of their algorithm in the case of bilinear min-max games for both alternated and simultaneous updates operator. They eventually try their algorithm on toy bilnear problems and GANs.",
            "main_review": "## Strengths: \n- The method is well motivated and the paper is well written. \n- The theoretical results are sound \n- The method seems to scale to GANs (when combined with Adam)\n\n## Weaknesses: \n- The comparison with the related work could be improved (see my detailed comment)\n\n\n\nI think the authors could discuss more the rates obtained in Thm 4.2 and Thm 4.1 and the standard rates obtained in the literature (e.g. the ones presented in Remark 4.2). For instance, a first step would be to stress that the value of $T_p(1 + x)$ has a closed-form solution (below Equation 21). Also, one could compute the Taylor expansion of $T_p(1 + x)$ when $x\\to 0$ to have an idea of the convergence speed. Looking at the formula below (21) It seems that you get an accelerated convergence rate (i.e. $\\rho(A)^{1/p} = 1 - O(1/\\sqrt{\\kappa})$. However, I am a bit surprised that you get this rate for $p=1$ because in that case, GDA-AM corresponds to Gradient Descent with line-search. Can you comment on the formula for the expression of the Chebyshev polynomial of degree p at the point $1+ \\frac{2}{\\kappa-1}$?\n \nFor the experiments (Fig 4): Azizian et al. 2020 use *positive* momentum + EG. I would be interested to see if SIM-AM still outperforms EG + *positive* momentum. Also, how did you tune the step size and the other hyper-parameters of the methods for the results obtained in figure 5?\nMoreover it seems counter intuitive since EG + positive momentum has a convergence rate of $\\rho(A)^{1/p} = 1 - O(1/\\sqrt{\\kappa})$\n\n## Summary of the Questions asked (by decreasing order of importance):\n1. Can you comment on the formula for the expression of the Chebyshev polynomial of degree p at the point $1+ \\frac{2}{\\kappa-1}$? It seems that your formula gives $\\rho(A)^{1/p} = 1 - O(1/\\sqrt{\\kappa})$ while for $p=1$ it should be $\\rho(A)^{1/p} = 1 - O(1/\\kappa)$ \n2. Can you compare your method with EG + positive momentum? (Empirically)\n3. Theorem 4.2 seems to give a convergence rate of $1 - O(1\\kappa)$  Do you agree with this statement? \n\n## Minor Question\nFrom my understanding, in order to get the theoretical guarantees you do not even need to use Anderson Mixing at each time step but only every p timesteps (and you only need to ensure that linear combination of $x_{t-i}$ can generate any polynomial of degree $p$ with coefficients that sums to 1).\nIs it why you analyze the algorithm every p step?\nWhat is the computational overhead (solving each timestep vs solving each p steps?) \nDo you consider this algorithm because it is more likely that it will extend beyond the least-square case?\n",
            "summary_of_the_review": "As a summary, I think this submission is well written. I think some more work could be done to compare the obtained rates with the ones already present in the literature but it should be relatively easy to implement.  Also, I have some concerns regarding the theory and the comparison with respect to the baselines.\n\nI will increase or decrease my score depending on how the authors address my comments. (for instance, if everything is clarified I will increase the technical and empirical novelty score as well as the final score) \n\nFor now, I recommend accepting this paper.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose to use the Anderson Acceleration on min max problem. They show two theoretical results: convergence rate on bilinear problems when using simultaneous gradient decent-ascent, and convergence rate on bilinear problems when using alternating gradient decent-ascent. Finally, they present numerical results in favor of their approach.",
            "main_review": "*** Post rebuttal ***\n\nAfter a discussion with the authors, I updated my score to accept.\n\n******\n\n\nThe biggest concern of the paper is the novelty. The authors present the Anderson mixing technique and its link with GMRES, up to page 4. The claimed novelty begins at section 3: GDA-AM: GDA WITH ANDERSON MIXING. The author claim that  their work \"is the first work to improve the GDA dynamics by tapping into advanced fixed-point algorithms,\" although this idea was explicitly mentioned in a paper they cited: \n\n> ---\n> Recent work in optimization analysed adaptive algorithms, such as Anderson Acceleration (Walker and Ni, 2011), that are adaptive to the problem constants. They can be seen as an automatic way to find the optimal combination of the previous iterates.\n>\n> ---\n> Source: Azizian et al. (2020) , section 6.1.\n> \n\nIn the same paper, the authors refer to Scieur et al. (2016) and Bollapragada (2018) for the theoretical analysis of such a method. In particular, the result presented in Bollapragada et al. (2018) proposes an analysis of Anderson Acceleration for *any* nonsymmetric operator $\\textbf{G}$. They combine Theorem 3.1 (Crouzeix's bound) and Theorem 3.3. (optimal polynomial on ellipses from Fischer and Freund (1991), Th. 2), to obtain a general convergence bound. This result covers all the theoretical work presented in this paper.\n\nMoreover, it is surprising that the author did not use the Crouzeix's bound for their Theorem 4.2. In short, the Crouseix's bound says that when dealing with a nonsymmetric matrix $\\textbf{G}$ and polynomials $P_2$, we have \n\n$$\n|| P(G) ||_2 \\leq c \\max\\_{\\lambda\\in W(\\textbf{G})}  ||P(\\lambda) ||_2,\n$$\nwhere $W(\\textbf{G})$ is the *numerical range* of the matrix $\\textbf{G}$, and $c$ is a constant, bounded by $2$ (Crouzeix's conjecture) and proven to be upper bounded by $1+\\sqrt{2}$. In particular, it is unclear how the authors manage to prove the inequality (last display equation in the proof of Theorem B.2) (I removed the other details to focus on the important part)\n$$\n\\min \\_{f_p\\in\\mathcal{P}_p} ||f_p(\\textbf{G}) || \\leq \\left(\\frac{r}{p}\\right)^p\n$$\nwithout using the Crouseix's bound, as $\\textbf{G}$ is not symmetric.\n\n\nThat said, the numerical experiments are persuasive, but the paper should be presented in a whole other way to include the missing related work. I strongly recommend the author rework the paper and submit it to another venue, as Anderson mixing could positively impact the optimization of min-max problems.",
            "summary_of_the_review": "Because the original idea of the paper was briefly introduced in Azizian et al. (2020), but more importantly, because the theoretical results presented in this paper are already covered in Bollapragada (2018), I am not convinced the paper is novel enough to be published. Moreover, I have a concern about a detail in the proof of Theorem 2.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}