{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The authors present an improved method to convert ANNs to spiking neural networks (SNNs). First, a network with quantized activations is constructed, then it is converted. They analyze the conversion errors theoretically. In addition to previously considered errors [Li et al. 2021] they also consider an error they call \"unevenness error\" and propose a way to compensate for that.\nThey test the method on data sets such as CIFAR-100 and show good improvements over previous methods with respect to classification accuracy and inference time.\nThe reviewers agree that the manuscript presents interesting and valuable work with a significant novel contribution.The manuscript is well written.\n\nWeak points according to the first reviews were:\n- Lack of ImageNet conversion experiments.\n- Analysis of energy consumption was missing.\n- More related work needs to be compared.\nThe revision addressed all these points, This was acknowledged by the reviewers with increased ratings. All reviewers propose acceptance."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a Quantization neural network to spiking neural network (QNN2SNN) conversion method. The authors first analyze the conversion error between ANN and SNN. Then they construct the ann with quantized activation so that the error can be eliminated. Both theoretical and empirical results are presented in this paper. ",
            "main_review": "**Pros**:\n\n+ This paper gives a theoretical condition of zero conversion error. Under a uniform prior assumption, the authors propose to shift the activation and get zero expected error.\n\n+ The empirical results show good improvements. \n\n**Cons**:\n\n- This is a little suggestion: this paper has too many notations, it would be better to have a section or table to sort them out. \n\n- The analysis of conversion error has been proposed in Li et al. 2021. The only difference is the unevenness error where Li et al. 2021 assume it is 0. This paper is over-claiming and should point this out. \n\n- What is the difference between shift term $\\varphi$ in this paper and the optimal shift in Deng & Gu., 2020? Afaic, the uniform condition and the function of the shift are the same. \n\n- Lack of ImageNet conversion experiments. The authors are strongly encouraged to convert more challenging models on the ImageNet dataset. Therefore, I would reject this paper for now.\n\n",
            "summary_of_the_review": "My weakness is listed above. Besides, I also have some questions:\n\n1. If Threorm 1 & 2 are correct, why your model does not reach exactly the same accuracy as ANN when $L=T$?\n2. What is energy consumption? It is higher or lower?",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper first analyzed the various sources of ANN-SNN conversion errors namely clipping error, quantization error and unevenness error and then proposed quantization clip-floor-shift activation function to replace the ReLU activation function in source ANNs to train them. This resulted in ANN-SNN conversion at ultra low latency (for example t step of 4).",
            "main_review": "Following are my detailed reviews.\n\nStrenghts\n========\n1. The paper is well written and the general motivation of work on efficient ANN-SNN conversion is important as it can reduce the SNN training epoch over head. In particular, the general trend is that only ANN-SNN conversion asking for higher times steps (latency). So, achieving reduced latency for only conversion based approach is a promising direction.\n\n2. The organization of the paper and results are good.\n\nWeaknesses:\n===========\n1. The variable $\\textbf{v}^l(t)$ is defined twice and are not having an exactly similar definition. Please clarify whether its membrane potential at time $t$ or membrane potential at $t$ after spike output happens. The definition of $\\textbf{m}^l(t)$ is not clear either, what is meant by membrane potential after neuronal dynamics? I think, the latter is before applying the threshold ${\\theta}^l$ and the earlier is after applying that as per Eq. 5. So, please clarify this part. Also, if you are not using the return to zero mechanisms, why is Eq. 5 valid?  Shouldn't it have ${\\theta}^l_{reset}$? Or else clearly mention that $\\theta^l$ is $\\theta^l_{th}$ - ${\\theta}^l_{reset}$.\n\n2. An important way to reduce spiking activity is through weight sparsity. It would be interesting to see whether such \"only conversion\" based strategy go hand in hand with state-of-the-art model compression of SNNs [1, 2]. As, compression is a necessary part for model size reduction and is equally important as considering reduction of latency for edge deployment for real time applications.\n\n3. I am not 100% sure about the way unevenness error has been portrayed here. I think its largely a function of what the max. weight value and the threshold is. In general we set the max. $W \\times x$ value as the threshold. Where as in the example the authors has taken 1 as threshold and 1.5 as weight. Please also provide analysis with threshold as max ( $W \\times x$). Also, I think the goal of trainable thresholds [2] should be to mitigate such error by default when SNN training happens. The authors should provide at least some discussion on that to clearly justify its inception and other ways of possible mitigation. Also, when the pre-synaptic spike is 1 at t=1, then the potential starts accumulating, then why it should immediately reflect at output at t = 1? If the inputs spikes at t = 1 then shouldn't the output postsynaptic spike happen at t = 2? I am not sure whether the Fig, 1(b) and (c) is correct. I think presynaptic red pulses should be at 0 and 2. If that's not the case, please justify why.\n\n4. As the authors claimed the activation difference error is zero if the ANN is trained with the proposed activation non-linearity, then is it possible to convert with  T = 1? Please show results for T= 1 as that would make the limitations and possibilities much clearer. \n\n5. Gu et al. [3] has also suggested ReLU shifting function in their optimal conversion work, how does the Therorem 2's shifting proposal differ from theirs, please highlight that.\n\n6. One part may be confusing to the reader is that we generally evaluate threshold after ANN training, i that case how can we perform the proposed non-linear activation efficiently during ANN training. Please clearly state tis in the paper.\n\n7. Please mention clearly how many training or test images you used for conversion. \n\n[1]  Spike-thrift: Towards energy-efficient deep spiking neural networks by limiting spiking activity via attention-guided compression, WACV 2021.\n[2] DIET-SNN: A LOW-LATENCY SPIKING NEURAL NETWORK WITH DIRECT INPUT ENCODING & LEAKAGE AND THRESHOLD OPTIMIZATION, IEEE TNNLS 2021.\n[3] Optimal conversion of conventional artificial neural networks to spiking neural networks, ICLR 2021.\n",
            "summary_of_the_review": "The paper's effort to provide more similar ANN activation as SNN to reduce conversion error of ANN-SNN at low time step is interesting and promising. However, there are some concerns as I have detailed in my review that can further clarify the paper for good.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This research proposes an ANN-SNN conversion method based on quantization clip-floor-shift activation.",
            "main_review": "1.\tThere are other related works that need to be compared with this method. For example:\n\na.\tR. Massa, A. Marchisio, M. Martina and M. Shafique, \"An Efficient Spiking Neural Network for Recognizing Gestures with a DVS Camera on the Loihi Neuromorphic Processor,\" 2020 International Joint Conference on Neural Networks (IJCNN), 2020, pp. 1-9, doi: 10.1109/IJCNN48605.2020.9207109.\n\nb.\tS. Singh, A. Sarma, S. Lu, A. Sengupta, V. Narayanan and C. R. Das, \"Gesture-SNN: Co-optimizing accuracy, latency and energy of SNNs for neuromorphic vision sensors,\" 2021 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED), 2021, pp. 1-6, doi: 10.1109/ISLPED52811.2021.9502506.\n\n2.\tFigure 3 is not clear. It seems that the legend is missing to explain what is represented by the lines.\n3.\tSince they represent an important contribution, the Imagenet results should be reported and discussed in main manuscript, instead of placing them in the supplementary material.\n4.\tIn Section 6, it can be observed that the ResNet-20 is the only considered CNN model in which the accuracy with T=4 is significantly lower than the accuracy with larger T. The reason should be discussed more comprehensively in the supporting text.\n",
            "summary_of_the_review": "The achieved experimental results are notable. However, some key points need to be addressed (see the main review section).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a quantization clip-bottom-shift activation function to replace the ReLU activation function in ANNs, so as to better approximate the activation function of SNNs. The authors also prove that the expected error of ANN-SNN conversion can be reduced to 0 by using this method. The reported results on CIFAR-10/100 and ImageNet dataset show that this work archives state-of-the-art accuracy with fewer time-steps.",
            "main_review": "Strengths:\n(1) Excellently presented paper that provides a full explanation of the development in SNNs.\n(2) The idea of obtaining a high-performance SNN from a quantization ANN is intuitive. However, the converted SNN will suffer a performance degradation problem if the time-steps of SNNs dose not match up with the quantization step of ANNs. The authors propose to add a shift term to overcome this problem, which is very interesting and impressive.\n(3) The proposed framework is rigorous, and is supported by theoretical analysis.\n(4) The proposed approach achieves SOTA accuracy with fewer time-steps.\n \nWeaknesses & suggestions for improvement:\n(1) My main concern is about unevenness error. Is there such a case, the input is negative during a period as the weight is negative? Thus, the spiking neurons fire fewer spikes than we expected.\n(2) The accuracy of source ANNs on ImageNet is lower than some compared methods, which decreases the performance of converted SNNs.  \n(3) The accuracy of converted SNNs from ANNs with quantization clip-floor-shift activation (blue curve in Fig. 4c) is slightly higher than the source quantization ANN when the time-steps is large. I would like to know whether the performance is the same as or better than source ANN without quantization when the time-steps is large.\n(4) I would like to see the discussion of the limitation of ANN-SNN conversion.\n",
            "summary_of_the_review": "This paper proposes a novel ANN-SNN conversion framework and achieves SOTA accuracy with fewer time-steps. The suggestion for improvement is listed above.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}