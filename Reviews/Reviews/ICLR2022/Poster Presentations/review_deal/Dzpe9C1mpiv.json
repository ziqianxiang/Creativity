{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper extends the previously established connection between adversarial training (AT) and Wasserstein distributional robustness (WDR) to other adversarial defense methods such as PGD-AT, TRADES and MART, and connects them to WDR. While this connection itself is not surprising given earlier works connecting AT and WDR, the paper makes contributions in establishing it formally and proposing algorithmic variations (eg, softball projection) that show clear empirical gains on standard benchmarks of MNIST/CIFAR10/CIFAR100 over point-wise adversarial defense methods."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper applies the idea of distributional robustness for crafting adversarial examples. The methodology in the paper extends that of (Sinha et. al. 2017) by introducing learnable parameters. Moreover, a soft-ball based projected gradient descent is used instead of a hard one.\n",
            "main_review": "This is an excellently well-written paper that applies the idea of distributional robustness for crafting adversarial examples. The methodology in the paper extends that of (Sinha et. al. 2017) by introducing learnable parameters. Moreover, a soft-ball based projected gradient descent is used instead of a hard one. Extensive experiments are conducted to demonstrate that the proposed method UDR when applied in conjunction with existing perturbation methods, such as PGD and TRADES, produces better results on standard datasets.\n\nOne weakness is that the paper develops a white-box attack methodology, where the adversary (Algorithm 1) is assumed to have a complete knowledge of the model parameters (model parameters being simultaneously updated with the attack generation parameters). This makes the attack algorithm not that realistic.\n\nSome specific comments:\n\n1. It's a bit strange to see the symbol \\infty in an algebraic equation (Equation 10). The usual way of writing this would be with a symbol M, and stating that M is larger than M_0.\n2. The authors should explain precisely what is meant by \"lower semi-continuous\", and argue why that is the case with the defined cost function.\n3. CE (the cross-entropy loss) is currently undefined. It should be defined in a similar manner as the definition of BCE.\n4. stay closely --- stay close\n5. For the blackbox attacks (reported in Page 8), do you also learn the model parameters by Algorithm 1?\n \n\n\n",
            "summary_of_the_review": "I'm leaning towards an accept due to the use of the novel idea of soft-ball projection and the demonstration that the proposed method works on a number of different standard datasets.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper aims to establish a formal relation between Wasserstein distributionally robust optimization (W-DRO) and a number of Adversarial Training (AT)-like defenses against adversarial attacks. In particular, three well-known defenses, namely PGD-AT, TRADES and MART have been the center of attention.\n\nPaper derives a theory that shows each of the above methods are special cases of a W-DRO, settled in a properly-defined augmented space. ",
            "main_review": "Strength:\nAuthor(s) seem to be aware of the related literature based on their references to prior works, and also the exact description of the said works. Mathematical statements are rigor and correct as far as I have checked. Moreover, paper is very well-written.\n\nWeaknesses:\nEven though paper explicitly insists that its proposed theoretical results are $\\underline{\\mathrm{not}}$ trivial, at least I, as a reader, have failed to understand their significance. The relation between point-wise adversaries which are commonly used in AT-like methods and W-DRO is already known to the community. This fact has been mentioned even by the author(s) themselves; For example, see their reference to Staib and Jegelka (2017), or the seminal work of Sinha et al. (2017). I rely on author(s)' claim that results are new, since no prior work has tried to derive explicit relations similar to Theorems 1 and 2 of the current paper. Still, the current results do not seem surprising and worthy of publication at ICLR.\n\nAdditional note: paper claims that results are not trivial since the dual form that is usually utilized in W-DRO assumes a bounded loss function, while this condition has been relaxed here. TBH, I haven't checked the proofs, completely. However, no sophisticated mathematical tools have been used throughout the paper and proofs are rather simple and short. It would be more informative if author(s) can give more insight about the technical difficulties that they have faced while deriving the results and thus shed light on their possible significance.\n\nPaper also claims some experimental contributions. I am not completely familiar with the experimental side of this line of research, so I wait to see other reviewers' comments on that matter.\n\n-------------------------------\n\nMinor comments and suggestions:\n\n\"The see why\" -> \"To see why\"",
            "summary_of_the_review": "Paper seems to lack enough theoretical novelty at this stage. My current vote is Weak Reject, but I also like to see other comments and feedbacks.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a unified framework for adversarial training from the perspective of Wasserstein distributional robustness. It is proved that some standard previous adversarial training methods are special cases of the proposed framework. The paper also proposes an adversarial training method given the framework. Empirical results demonstrate that the proposed method is more effective than previous methods.",
            "main_review": "The paper is overall well-written and interesting to read. The proposed method, while unifying previous methods, performs better than previous methods (PGD, TRADES and MART). The proofs are not carefully checked. \n\nMy major comments follow.\n1.\tIt is proved that (14) is equivalent to the standard adversarial training objective, but is (14) the dual program of the Wasserstein distributional robust optimization? It seems that the equivalence of the dual program requires certain constraints on the function $g_\\theta$. \n2.\tI understand that why you need to use sign of the gradient of $\\hat{c_{\\mathcal{X}}}$ in practice. But I feel the need of more explanations to show where the difficulty comes from. For example, if the objective function (17) is fundamentally good for adversarial training, then it may not need such tricks. More concisely, I wonder if it is just an empirical trick or there are more underlying reasons.\n3.\tI’d like to understand more about the reasons of the empirical success of the proposed method. I see that the authors contribute some of the success to “soft-ball projection” and “utilizing the global information”, but I’m curious to understand more beyond the general discussion, e.g., theoretical analysis about the empirical success. I understand that in-depth theoretical analysis may worth another paper, but it would be great if the authors can provide more insights. \n",
            "summary_of_the_review": "The proposed method is novel and interesting to me. The empirical results seem promising. Overall, I think the paper has a decent empirical contribution, and it also has the potential to offer a unified theoretical explanation for adversarial training. Unless I misunderstand some key parts of the paper, I recommend accepting. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a new approach for improving adversarial training (AT) in deep neural networks (DNN). While previous approaches generated point-wise adversary examples to enrich the training data set, this approach extends previous work on Wasserstein distributional robustness (WDR) and suggests a unified WDR framework to encompass previous approaches and to improve AT. The theoretical results show that previous approaches can indeed be formulated in the WDR framework. The empirical results show that the newly derived methods for AT improve the robustness to adversarial attacks on several classic DNN example problems. ",
            "main_review": "### Theoretical contribution\nThe paper extends the previously established connection between AT, distributional robustness and optimal transport (Blanchet & Murthy [2016 on arxiv, 2019 published]; Sinha et al. 2017). However, my impression is that the main theoretical contribution for employing the work by Blanchet et al. for connecting distributional robustness with AT was made by Sinha et al. 2017: They proved a relaxed version of the dual form of the risk function using a Lagrangian penalty formulation and show the way to place PGD-AT in this WDR framework.\nThe paper proposed here extends this result to incorporate other AT approaches in the relaxed dual form established by Sinha et al. 2017. To achieve this, the authors reformulate the cost function proposed by Sinha et al. 2017 such that it incorporates TRABES and MART as well. As a consequence their cost function becomes unbounded (eq 13), which requires an additional proof for the dual form. The authors provide this proof, which defines the core of their theoretical contribution.\n\n### Strengths\n- The paper is well structured and clearly highlights contributions of the work.\n- The results are convincing: the paper gives both theoretical insights and the corresponding practical implications with competitive empirical results on standard deep learning classification problems (MNIST, CIFAR10, CIFAR100; Table 1), and improved robustness scores for increasing adversarial attack strengths (Table 2). \n\n### Weaknesses\n- In contrast to what the authors write at the end of section 3 in \"Theoretical contribution and comparison to previous work\", my impression is that Sinha et al's approach is actually based on the theoretical foundation of Blanchet & Murthy 2019 (Sinja et al. cite the version that was put on arxiv in 2016). \n- The adapted cost function for incorporating all three AT approaches is unbounded and is used as a smoothed version in practice to be differentiable (eq 16). This change makes sense, however, the authors do not elaborate on the effects this might have on the theoretical insights.\n- A clear comparison to the WRM approach by Sinha et al. (2017) is missing. Given that this is a related approach that already connects WDR with AT (limited to PGD-AT), it is important to highlight their differences. The authors do describe the approach by Sinha et al. (2017) but do not highlight the exact theoretical difference to their approach. Additionally, they do not show empirical results for WRM on the example problems. \n- Some of the empirical results appear a bit oversold, e.g. the authors state that in Table 1 their approach “boosts the model robustness” compared to the other AT methods. However, in many cases the difference seems relatively small, on the order of 0.1, or 1.0 percent. It is clear that the performance of their approach is competitive, but to make strong statements about them the results would need error bars, e.g., standard errors of the mean over 5 or 10 repetitions of the benchmark. \n- A detailed discussion of related work happens only in the appendix which is not part of the core submission and cannot be taken into account for evaluating the submission. It would be good to move part of that discussion to the main paper.\n- The paper has quite some grammatical errors like misplaced commas, missing words misspellings, which impedes clarity and readability.\n\n### Suggestions for improvements\n- If possible, find an example used in Sinha et al. (2017) to compare to, e.g., their MNIST example. If their code is not available or difficult to tune, it should be possible to conduct the experiment with your methods and compare with the numbers in their paper. \n- I do not think that SOTA is necessary for your paper to get accepted. However, I think that this is an important topic for the DNN community and that it should therefore be clearly communicated which approaches work well in which scenarios, e.g., does WRM work better for PGD-AT attacks? \n- weakening of the statements about the results in Table 1 and 2 and to conduct additional repetitions of the benchmarks to have error bars on the numbers in the table. \n- Add an actual figure caption for figure 1. \n- Clarify the theoretical contributions of your approach vs. Sinha et al. (2017), e.g., that their work is based on Blancet & Murthy (2019) as well. \n- Some references are not correct, e.g., Sinha et al. 2017 is not a preprint, it was actually accepted as an oral contribution to ICML 2018. Please check all your references and make sure you credit previous work correctly. \n\n### Questions:\n- Why is WRM fundamentally the same as PGD with lambda fixed? In WRM lambda is fixed as a penalty parameter before optimization, but this does not mean that it is the same as PGD. Could you clarify this statement? \n\n### Typos/minor points\n- There are several occasions in the paper where the definitive article to a noun, or the plural to make it indefinite, is missing e.g., already in the second sentence of the abstract. \n- paragraph between equations 2 and 3: iif <- iff\n- paragraph between equations 3 and 4: relaxation <- relaxed\n- section 2.2 first paragraph: was AML defined before? \n- sentence between equations 8 and 9: “one can derive to Eq. (9)” maybe you mean “one can arrive at” or “derive Eq (9)”? \n- paragraph after equation 10: “The see why the later is the case” <- “To see why the latter is the case”\n- paragraph after equation 12: “upon on” <- “on” (?)\n- paragraph after equation 15: “theory developed in (B & M., 2019)” <- “the theory developed in B & M (2019). \n- There are other occasions where the “in text” vs “in parentheses” citations are mixed up, please double check. \n- section 4 second sentence: “we first discuss about” <- “we first discuss”\n\nand it is likely that I missed some, so please make sure to check for typos carefully. \n",
            "summary_of_the_review": "The paper makes a theoretical contribution by extending the work on WDR for AT to other AT approaches. It shows promising results of their unifying framework on common example problems. Therefore, taking into account the weaknesses listed above I vote for a score marginally above the acceptance threshold. However, I am willing to increase my score if the weaknesses listed above are addressed, especially a thorough comparison to the WRM approach by Sinha et al (2017) (~500 citations, oral paper at ICML 2018). ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work aims to improve WRM (Sinha et al., 2017) with respect to Wasserstein distributional robustness to cover more state-of-the-art adversarial training methods in addition to the most commonly used PGD-AT method. To this end, this paper presents a unified framework to connect Wasserstein distributional robustness with PGD-AT, TRADES, and MART methods. The introduced Wasserstein cost function can cover those of commonly used AT methods as special cases. A family of algorithms are developed to generalize AT methods with better generalization. Extensive experiments show the improvement of the proposed robustness AT algorithms over those commonly used ones.",
            "main_review": "Strengths:\n\nThe unified distributional robustness framework presents a different perspective to adversarial robustness of deep learning through new Wasserstein cost functions. This work improves WRM (Sinha et al., 2017) to cover more adversarial training methods, e.g., TRADES and MART. This paper also develops a new family of algorithms that generalize the AT methods in the standard robustness setting, and experimental results show the proposed algorithms achieve better performance than standard AT methods.\n\nWeakness:\n1.\tThe theoretical contribution (Theorem 1) needs to be better justified. It is not very clear how this work differs from WRM (Sinha et al., 2017) with respect to the main conclusions of equivalence and proof techniques. From Theorem 1, it does not seem obvious that “the standard PGD-AT, TRADES, and MART are special cases of their UDR counterparts.” It is also unclear how significant the boundness of cost function is, although the extension of results from bounded to unbounded cost function may be not trivial.\n2.\tThe literature review does not seem comprehensive – some recent distributional robustness methods are not mentioned, e.g., feature scattering (Zhang & Wang, 2019), Adversarial Distributional Training (Dong et al., 2020)\n3.\tWhen using different SOTA attacks to evaluate the defense methods, why the C&W attack (Carlini & Wagner, 2017) is excluded from the comparison?\n4.\tSome typos here and there. For example, “The see why the later is the case, …”; “… and a new serious of risk functions in WDR, …”; “While when τ is set closer to 0, …”; “The results of this experiment is shown …”; “… are different from those the model is trained with”; “…, and the later use the hard-ball one, …”; “… therefore, benefits the adversarial training.”; “doing PGD adversarial training with larger cannot achieve …”\n",
            "summary_of_the_review": "In general, this paper presents some solid contributions to distributional robustness including both theoretical and experimental results. This paper is acceptable to the conference, but some weaknesses should be fixed, including the theoretical justifications and more comprehensive comparisons.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}