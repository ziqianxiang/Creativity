{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes an importance-sampling estimator for probabilities of observations of SDEs. The proposed approach has several advantages over conventional methods: it does not require an SDE solver, it has lower gradient variance, and shows nice results with a Gaussian process representation of the function. Reviewers were somewhat split on this paper, with some concerns that experiments were limited. On balance, however, the paper makes several nice contributions, the experiments are in line with related works, and the authors did a good job of clarifying Theorem 1 in the rebuttal. We note that Reviewer K19Y changed their opinion to accept (although they forgot to update the score). Please carefully account for all reviewer comments in the final version."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes an importance sampling method for the estimation of probabilities in SDE models, applicable to SDEs with state-independent (or transformable to state-independent) diffusion. By taking the advantage of Girsanov's theorem, the probability density of the observations is rewritten as a weighted probability with respect to a base Brownian motion process. Sampling results are presented qualitatively and compared to integration based sampling. Results also suggest that the proposed method is faster than integration based methods and the gradients have lower variances. In addition, a variational GP examples is demonstrated showing that the method can provide reasonable uncertainty estimation.",
            "main_review": "While the proposed method improves the speed and efficiency of SDE learning, the presentation is not strong enough. Below are my specific comments.\n\n* The scope of the work is limited to state-independent or transformable to state-independent diffusion. This is not properly reflected in the title and abstract of the paper and should be inferred by the reader throughout the paper.\n* Methodological novelty is limited. The paper is heavily based on a simple application of theorem 1.\n* Comparisons are done only on the toy examples, if the authors believe that the proposed method suffices for most application then results on higher dimensions, various simulations, and real world data should be presented. Provided that the technical novelty is limited, empirical validation is necessary for the publication.\n* Comparisons against the adjoint method is not completely fair, since it solves a way more general problem (state-dependent diffusion). Are there methods tailored to state-independent diffusion to be used for comparison?\n* Why gradient variance results is shown on a separate dataset than learning curves? Please include more than one example for the learning curves experiment and include standard deviation across multiple experiments. Also, can you show this as a function of dimensions, and when adding or removing observation noise?\n\n\n\n\nHere are some minor comments:\n\n* Introduction is very short and many applications of SDE are not mentioned.\n* Line 25: \"In many physical applications ...\" please provide examples and include citations.\n* Line 27: \"In more complex processes ...\"  please provide examples and include citations.\n* Line 46: \"Models that implicitly describe f as a Gaussian process\"  please provide examples and include citations.\n* Line 56: \"Can marginalize out the observation noise ...\" please provide examples of this, how do you marginalize out the observation noise?\n* Theorem 1: please spell out the conditions of Girsanov's theorem explicitly and what exactly should hold for function f and sigma.\n* Instances of informal language can be found throughout the text such as \"wandering around?\" or \"if need be\".\n* Line 184: \"It may be argued ...\" please provide evidence for this. Can you support the argument \"almost all real world problems ...\"  by providing examples from the literature?\n* Some of the text in the supplementary must be brought to the main. In the current form, it's not clear how exactly the experiments are done. I had a grasp on the experiments section after reading the supplementary.\n* Variational GP experiment is very unclear, what is the generative model? What is data (include equations)? What is the inference procedure?",
            "summary_of_the_review": "In summary, the methods presented can be of potential interest to the community, however the presentation is weak, scope and novelty is limited, and experiments are rather minimal.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper presents a new approach to compute the density of a given observation sequence under arbitrary drift and diffusion functions. The method relies on Girsanov's theorem, which requires evaluating the density of the observations under Wiener process. The authors further propose an inversion trick to handle state dependent diffusion functions. The experiments show that the method indeed learns accurate drift and diffusion functions much faster than standard integration-based approaches and has much smaller gradient variance",
            "main_review": "+ The paper indeed addresses a timely topic. Given that continuous-time dynamical system literature is currently dominated by neural ordinary differentia equations, contributions like this would pave the way for more stochastic differential equation publications.\n+ The results seem pretty impressive. However, the experiments are done only on very few dimensional systems. More experiments on bigger systems would help us identify trade-offs and potential issues with the methodology.\n+ The paper is written clearly and the very obvious motivation is communicated very well. A few notes to further improve the readability:\n    - What does the sentence in line 48 mean? More specifically, what prior/posterior distributions do you refer to?\n    - More details on the GP-SDE model in Sec 3.3 would be nice. Also, the experiment seems a bit repetitive.\n    - Alg 1 should be written more verbosely, e.g., the set of $t$'s (in line 3) and $\\Delta t$ can be explicitly given.\n\nThese being said, I'm not able to verify Th 1. Despite checking the references and more resources, I could not immediately see (9-10). If the authors can provide a proof (even if it is straightforward) or other reviewers can comment on it, I would be very happy (including the sum that appears in the last line of Alg 1). Similarly, (13) and the paragraph above is not clear. As I understand, an invertible mapping $T$ is utilized to switch between processes with constant and state dependent diffusions. Yet, I'm not able to immediately see (13). It would be much better to clearly define all the terms/processes and show where the (state) independence comes from.",
            "summary_of_the_review": "The proposed approach concerns an interesting topic and provides excellent improvements over existing approaches. However, I'm not able to mathematically verify Theorem 1 and the derivation in Section 2.2. This is why I preliminarily vote for a reject but would be very happy to re-consider my score if additional details are provided by the authors and discussions with other reviewers.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes an importance-sampling method for estimating the likelihood of observations in stochastic differential equation (SDE) models. Unlike classical integrator-based methods, the proposed approach imposes a structure that forces sampled trajectories to pass through observations thereby improving accuracy, especially in high dimensions. Furthermore, it removes sequential dependencies between sampled points thus allowing for speed-ups through parallelism.",
            "main_review": "- **Clarity.** The paper is mostly written clearly and succinctly. However, it could be more self-contained, possibly with additional background provided in the supplementary materials.\n- **Reproducibility.** Low-level details necessary to reproduce the results from this paper are not all included. However, source code is provided with the supplementary material, which appears to be well-structured and clean.\n- **Novelty.** The main technical novelty comes from the application of Theorem 1, as summarized in Algorithm 1. In particular, the authors propose to estimate the original SDE of interest with an linear SDE, which is an attractive alternative since it is easier to estimate their probabilities and expectations with importance sampling, and has the added benefit that the sampled paths pass through the observations. This appears to provide significant improvement, as evidenced by the empirical results.\n\n## Concerns\n\nOverall, I find the empirical evaluations feel a bit rushed.\n\n- Experiment described in Section 3.1: are you making use of parallelism in this experiment? If so, the details of this need to be described. If not, can you pinpoint and provide some explanation for the factor of 52 speed-up? From what I can tell, even without any parallelism, the method is able to provide speed-ups of this magnitude? Furthermore, the plots might be clearer visualized in log-y scale. Also, instead of truncating the training time along the horizontal axis, you might consider instead recording the amount of time required to attain a particular MSE value. As it is currently, the differing lengths of the curves make it slightly messy. Finally, the only baseline that is compared against is the adjoint method of Li et al. 2020. I would be interested in comparing the proposed method against some of the related methods highlighted in the Related Works section as baselines.\n- Experiment described in Section 3.3: I find the normality test carried out starting on line 271 to be lacking in precision. Firstly it is entirely qualitative and therefore somewhat subjective. There are simple statistical tools available to carry out normality tests. Line 280 concerning training speeds: the speed-up is mentioned in passing. Though not surprising, it would be good to see this quantified more rigorously with a table or figure, rather than as an offhand remark.\n\n## Miscellaneous Issues\n\n- The statement of contribution at the end of page 2 contains four bullet points. Only the first bullet point is the actual contribution. The remaining three are merely descriptions of the first and only contribution.\n- Line 14: \"which\" → \"that\"\n- Line 46: \"Gaussian process, [...]\" - extraneous comma\n- Line 146: \"which\" → \"that\"\n- Line 183: \"which\" → \"that\"\n- Line 222: \"The second metric we record the [...]\" - missing \"is\"\n- Figure 2: If I understand correctly, the two sub-figures shown here are of the same vector field and observations. Is that right? This would be made much more clear by having the subplots share the same vertical and horizontal axes.",
            "summary_of_the_review": "Given the numerous advantages of the proposed method over existing works, I am inclined to recommend acceptance of this paper. However, given the questions I raised concerning the empirical evaluations, I am reluctant to recommend a clear acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}