{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper presents a study of on-policy data in the context of model-based reinforcement learning and proposes a way to ameliorate the resulting model errors.\n\nThis is a timely and interesting contribution, and all reviewers agree on the quality of the manuscript.\nPlease incorporate all the remaining feedback from the reviewers.\n\nMinor comment: There might be interesting points of contact between this work and the concept of objective mismatch (https://arxiv.org/abs/2002.04523)"
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper addresses on-policy errors in model-based Reinforcement Learning. The authors decompose the policy improvement bound in terms that depend on the off-policy and on-policy model errors. They first present two major techniques, one purely based on the replay buffer, which asymptotically obtains zero on-policy error, and one based on a learned model, which leverages off-policy data. Then they propose On-Policy Corrections (OPC), a combination of the two techniques which can use a model and still maintain low on-policy error using the replay buffer. This method has asymptotically (with infinite data) zero on-policy error when using deterministic policies. If the policy is stochastic, the error grows exponentially with the trajectory length T. The authors implement their algorithm on top of MBPO and show experimentally that OPC-MBPO has superior performance in some environments.",
            "main_review": "I found this paper well structured and easy to read. I liked in particular the deep analysis of the illustrative example in section 4.1 and appendix B. The authors provide also an extensive ablation on what happens when varying the rollout length. \n\nThe intuition I got is that this method is able to retain very low error on the last term of equation (5), while at the same time it should also have a low off-policy model error since a model is being used. However, it is not clear to me how OPC behaves compared for instance to MBPO, when it is evaluated on all three terms on the right side of equation (5). Could it happen that while the on-policy error of OPC is low, the off-policy error becomes much bigger? Could it happen that both errors are low, but because of the model choice in OPC the model policy improvement is extremely lower? Perhaps this is something that can be analytically derived for a very simple MDP like the one in section 4.1",
            "summary_of_the_review": "This paper combines the use of a replay buffer and a model to lower the on-policy model error. The experiments seem convincing and the method is very well analyzed.\n\nWhile I am not familiar with the related literature and I might have missed something important (hence I lowered my confidence accordingly), I have the impression that this work is solid and would bring a nice contribution to the conference.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors introduce a model-agnostic technique for improving estimates of on-policy data and show improvements on MuJoCo and PyBullet domains. \n",
            "main_review": "Overall, I'd say this is a relatively thorough examination of a simple but seemingly effective technique. As far as I'm aware, the approach is novel in the context of MBRL, and I can see it being widely used by the community, so I recommend acceptance. \n\n**Issue 1:** Issues with Section 3.1/the bound in Eqn (5). \n\nClarity: The off-policy model error and on-policy error are just measuring the accuracy of $\\hat \\eta_{n+1}$ and $\\hat \\eta_n$. If we assume the data is generated by some $\\pi_0$ then both $\\pi_{n+1}$ and $\\pi_n$ will be \"off-policy\", so really this relies on $\\pi_n$ being the data-generating policy, which isn't stated until a few paragraphs later. \n\nMisleading: The bound also implies that our ability to estimate $\\eta_n$ is an important part of policy improvement, but in reality... it isn't. Ultimately the only term we care about is the final performance $\\eta_{n+1}$ (i.e. eqn (1)), and we cannot, generally, say that the ability to estimate the performance of the data-generating policy will impact the final performance. \n\nThe problem is that the bound is presented as a decomposition of policy improvement, but it's actually a bound on the model's estimate of how much better the current policy is better than the data-generating policy. So for example, if we instead bound $\\eta_{n+1}$ (as done in Janner et al.), by adding $\\eta_n$ to both sides of the equation, we arrive at $\\eta_{n+1} \\geq \\hat \\eta_{n+1} - \\text{off-policy model error}$, which does not include any on-policy term. \n\nOf course, this doesn't invalidate the technique, as I would still expect that improving an estimate of the on-policy policy would impact the model's ability to estimate an off-policy policy but I'm not sure this is as fundamental as presented by the authors.\n\n**Issue 2:** Theoretical results are nice but don't say anything comparatively. In other words, it's not clear if the presented bound is improved by including OPC. \n\n**Issue 3:** Issues when using OPC off-policy / suggested analysis. \n\nIn any MBRL policy improvement context there will be some amount of off-policy evaluation. If we are estimating the on-policy then this cancellation is convenient, however, when estimating a trajectory which is quite different than the on-policy trajectory, i.e. where $s_t$ and $\\hat s^b_t$ are not similar, then $\\hat s^b_{t+1} - \\hat f(\\hat s^b_t, \\hat a^b_t)$ seems like just noise added to the estimate $\\hat f(s_t,a_t)$, and it seems like this technique would be more harmful than beneficial. This situation also arises naturally when rolling out the off-policy trajectory for a few time steps, as we would expect that it will be quite different from the on-policy trajectory. \n\nAn interesting experiment would be to test the model's ability for off-policy evaluation (OPE) over varying levels of \"off-policy-ness\" (i.e. adding noise to the on-policy/behavior policy). This is a common experiment in OPE literature. I would expect that when near on-policy OPC would be helpful, but it's not clear OPC will be helpful when acting more off-policy. \n\n**Strengths**\n* Empirical results are strong/convincing. 10 seeds are used.\n* Reproducibility is high. The method is simple/easy to implement.\n* The appendix is thorough and includes a lot of additional experiments and details. \n* I can see the method being widely adopted by the MBRL community. \n\n**Minor Comments**\n- I think Figure 1.b would be clearer if the on-policy and off-policy estimates were separated into two separate graphs (in other words, cutting the figure in half). \n- Figure 5 is seemingly used to suggest that higher performance = higher data diversity, but I'm not sure how the authors made that jump. I can see that longer rollouts provides a higher performance, but there's no guarantee that rolling out is necessarily more diverse than the data gathered from the initial policy. \n- I find Figure 2 unclear. The meaning of dotted lines is not defined and it's not clear how the figure should be interpreted.  \n- A very similar technique has been proposed in value-based RL [1] (ironically named off-policy corrections instead) and should be cited.\n- I think an interesting experiment would be applying OPC to different models (rather than just the models used by MBPO).\n- Do the authors have any insight as to why this technique matters much more on the PyBullet domains? \n\nReferences:\n- [1] Harutyunyan, Anna, et al. \"Q ($\\lambda $) with Off-Policy Corrections.\" (2016).\n",
            "summary_of_the_review": "Overall, I'd say this is a relatively thorough examination of a simple but seemingly effective technique. As far as I'm aware, the approach is novel in the context of MBRL, and I can see it being widely used by the community, so I recommend acceptance. I have concerns regarding Section 3.1 which is the main motivation of the method, which limits my score. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a way to combine observed data with a learned model to mitigate the cost of each choice separately - the observed data is over-fitted to a specific policy while the learned model has approximation errors.\nThe authors mix the two options using probabilistic corrections based on the learned model applied to observed transitions. \nThe authors also provide convergence results for their algorithm,",
            "main_review": "Strengths:\n\n1. The idea of the paper is clear.\n\n2. The presence of theoretical results and a fitting empirical side.\n\nWeaknesses:\n\n3. The idea of correcting samples with the model sounds immediate, which means more is required of the analysis: \n\n(a) Since there is a trade-off between model based and sample based, how to implement\\calculate this trade-off in the proposed correction? \n\n(b) How do the theoretical results compare to model based and replay based results?\n\n4. I don't understand why include the MujoCo experiments if you can't see any improvement there. The improvements in PyBullet are mostly due to the initial state randomization, how does that fit the general theme of the paper?\n\n5. The comparison is made against SAC and MBPO, but why does the first method proposed (replay buffer based simulation) not compared? \n\n6. Consider adding the reference to the highly related (though not very new) work:\n\nBatch Mode Reinforcement Learning based on the Synthesis of Artificial Trajectories\n\nRaphael Fonteneau, Susan A. Murphy, Louis Wehenkel, and Damien Ernst\n\n\n",
            "summary_of_the_review": "The paper considers a straightforward combination of two methods which does not seem highly significant. \n\nThe paper is also somewhat lacking in its analysis and empirical claims.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper considers model-based reinforcement learning (MBRL) of the MBPO flavour, where model-free RL methods are accelerated using rollouts of a learned model. \nTHis paper proposes to alleviate (off-policy) model bias per rollout using the (on-policy) data and calls this approach on-policy correction (OPC). \nWith this perspective, a comparison is made between MBRL and iterative learning control (ILC).\nThis method is compared to MBPO on standard mujoco tasks and pybullet versions.  ",
            "main_review": "I thought this was a nice paper. The writing was generally clear and the theoretical justifications were interesting. The connection to ILC was also an interesting addition.\nWhile the idea in itself feels rather incremental, it is saved by the thorough analysis, empirical results and extensive ablations. \n\nI enjoyed Sections 1 and 2 and throught they were well written. Although the jump to Section 3 feels rather abstract. I think Section 2 discusses MBRL too broadly when the paper should really focus on the MBPO-style of MBRL since OPC is limited to this section.\n\nSection 3 proposes an improvement bound that can be decomposed onto on-policy and off-policy model-based improvement. These quanitities feel quite abstract to reason about, since they ultimately rest on the environment, choice of approximator and training protocol. However the paper focuses on compounding model biases which is reasonable. \nOne slighly strange aspect of the OPC model is its deterministic treatment, such that the aleatoric uncertainty is accessed by sampling different buffer indices, which to me indicates a rollout is deterministic after the choice of index b?\n\nThe paper assumes that the model error is constant and time-dependent, but there is no empirical study of how well this assumption holds. Given that the true ground-truth simulator is available, the model bias could be evaluated empirically to see how well this assumption holds in practice. \n\nWhile OPC appears superior to MBPO* in Figure 3, my understanding this that this MBPO sees no real data from the replay buffer. While Figure 11 shows the standard MPBO where it is comparable if not superior. There appears to be many discrepencies between Fig. 3 and FIg. 11, for example\nAre these figures meant to report the same results? (i.e. median vs mean?).\nHalf Cheetah: OPC mean is noisy and below 10 in Fig. 11, but smooth and above 10 in Fig. 3 \nAnt: MBPO* mean clearly goes up to 6 in Fig. 11, but in Fig. 3 it is at more like 4.\n\nMoreover ,OPC demonstrates signifiant improvement over MBPO on the pybullet environments. However, I am not sure how to feel about these experments, since the dramatic performance shift speaks to the evaluation assumptions in general rather than the performance of OPC. If the algorithm performance does not translate between simulations, it begs the question how reflective these simulated settings are of these algorithms performance. \n\nThe feature representation experiment in Figure 4 is also disconcerting, since the sine cosine features contain an inductive bias that should aid generalization.\nIs the idea that the model predicts theta, but uses the features as an intermediate transform for the model, or is the model also predicting the featurespace? In which case, predicting sine features over-complicates prediction since you have the manifold constraint.\nIs there not a bug in theta being whitened before the feature transform for MBPO?\nI dont see how the OPC addition relates to this issue of feature space, so I don't see how this experiment demonstrates the benefits of OPC. Also, as the code was not submitted I cannot check for bugs.\n\nThe paper is missing a reference to \nModel Predictive Actor-Critic: Accelerating Robot Skill Acquisition with Deep Reinforcement Learning, Morgan et al. ICRA 2021\nwhich does MBPO with path integral MPC rollouts. \n",
            "summary_of_the_review": "While this looks like it is a solid paper on what is arguably a somewhat incremental idea, I have some issues with the experimental results that need ironing out before I can accept. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}