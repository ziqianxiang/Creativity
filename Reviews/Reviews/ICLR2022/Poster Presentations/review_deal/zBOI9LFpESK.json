{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper proposes to learn a state-representation using bi-simulation in an RL setting. The approach is thoroughly evaluated on several benchmarks. In its current form the paper is mainly an empirical contribution, with now some theoretical contributions tucked away in the appendices. Nevertheless, an interesting approach with promising results.\n\nThe reviewers appreciated the revised paper and the discussion. The replies and discussions successfully addressed all serious concerns of the reviewers. Please also clarify the discussed points in the next iteration of the paper, and run the experiments with more seeds, as promised."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This work considers the problem of learning representations of high-dimensional pixel observations for RL. High-dimensional pixel observations often include many task-irrelevant details and ideally, an effective representation of such observations should only encode the task-relevant details. To implement this intuition, this work proposes to learn a two-part representation based off of a bisimulation metric. Two states that have similar first parts of the representation should exhibit similar reward structure, and two states that have similar second parts of the representation should exhibit similar dynamics structure. This work evaluates this learned representation on several continuous control tasks with distracting backgrounds and on CARLA, and shows that the proposed learned representation improves over baselines.",
            "main_review": "## Strengths\n- Overall, the approach proposed in this work seems technically sound and well-motivated. Learning a state representation based on a bisimulation metric seems quite reasonable, and doing so appears to yield strong empirical results.\n- The experimental evaluation is quite thorough and illustrates several interesting phenomena. I appreciate that this work both considers environments where most of the state observation includes only distracting information in the DM Control tasks, as well as environments where a large part of the state observation is potentially useful, as in the CARLA tasks. The ablation studies and results on transferring to different reward functions are also interesting, and present fairly promising results.\n- The paper is generally fairly clear.\n\n## Weaknesses\n- There are no critical or significant weaknesses that I am aware of, though potentially slightly incremental on top of DBC.\n- There are numerous minor typos and small errors throughout the paper. For example, the domain and range of the function $F^\\pi$ in Equation (3) appear to be some undefined \"met\" space. At the end of the preliminaries, \"partial observations\" is used, where I believe \"partial observability\" is meant. The equations (4) and (5) are defined as \"least squares error,\" where I believe \"mean squared error\" is meant, or this could refer to a form of least squares regression. I believe that these are generally easy to address, but it would be useful to clean this up.\n\n## Additional Questions / Comments\n- In Equations (6) and (7), shouldn't this be restricted to samples where $a_i = a_j$? If the two actions don't match, I don't think you can necessarily expect the rewards and dynamics to be similar, even in similar states, e.g., even in exactly the same state. This seems potentially problematic to me.\n- Why does the actor condition only on the CNN layers of $\\phi$ in Equation (11) and not on the full learned state representation like the critic?\n- Do the representations $\\phi_r$ and $\\phi_s$ also receive gradients through updating the critic in $J_Q$? Or are they learned exclusively via the $\\ell(\\Theta)$ loss?\n\n-------\n\n## Post-rebuttal comments\n\nI appreciate the rebuttal, which has alleviated the concerns I've raised. I still find the proposed approach technically sound and well-motivated, and find that this work provides a reasonable empirical contribution. Several of the other reviewers also raised concerns about whether the approach is empirically justified. I find these concerns to be reasonable, although they are partially alleviated via the rebuttal and I believe they are outweighed by the empirical contribution of the work. Therefore, I continue to recommend acceptance, though I do not feel strongly enough to raise my score to the next possible option, which is an 8.",
            "summary_of_the_review": "Overall, this work offers a principled and empirically strong approach. There are some weaknesses, but they seem relatively easy to address, and the contributions of the work outweigh the cons. There were also some points of confusion for me in the paper that seem potentially serious, which I included above. I initially recommend a score of 6, but am willing to raise my score if the points of confusion are clarified.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper leverages some recent work on bisimulation metrics to develop a pair of meta-learners to capture the two parts of a bisimulation metric: reward similarities and dynamics similarities. The authors evaluate their method on the environments used by the recent DBC (Zhang et al., 2021) paper, and compare against this and other related methods.",
            "main_review": "While an interesting extension of DBC from Zhang et al. (2021), I have 3 main concerns with this paper:\n1. **Theoretical issues:** My main concern with this paper is in terms of the theoretical justification for learning $c$.\n    * The $c$ term in equation (3) (which originally came from Ferns et al., [2004]) is _directly_ tied to $\\gamma$. Indeed, this is how the authors of that paper were able to prove the value-function bounds. Castro [2020] only uses the second $c$ term (and sets it equal to $\\gamma$), which still yields the value-function bounds. By making the $c$ a learnable parameter, it seems like the connections to the theoretical properties of bisimulation are completely lost.\n    * Furthermore, by having a varying $c$ term, it is not at all clear that the underlying metrics even converge to a fixed point!\n    * **Post-rebuttal note:** The authors have mostly addressed this in their rebuttal. There are some minor issues with their proofs and I've provided some suggestions.\n1.  **Implementation design choices:**\n    * Why is $f_r$ even necessary? Rewards are typically fully observed so it's not clear why this needs to be learned at all.\n    * The use of $V_*^{(1)}$ in the last term of equation (1) is a strange design decision, and I'm not sure I follow the justification. What is meant by \"make a consensus prediction\"?\n    * **Post-rebuttal note:** The authors have addressed these concerns in their rebuttal.\n1. **Statistical significance of results:*** 3 runs is on the lower-side of what should be used for these types of experiments. Further, the authors do not specify what the shaded areas represent in their figures.\n    * **Post-rebuttal note:** The authors have promised to run more seeds. I have provided some suggestions in my high-level comment.\n\n\n---\n\nSome questions/comments for the authors:\n1. In the third paragraph of page 2, the authors say \"their behavioral distance to the other state representations\". What other state representations are being referred to? It is not clear.\n1. In the third paragraph of page 2 the authors say \"more side information can be preserved\". What does \"side information\" mean, and how is it being preserved?\n1. In the third paragraph of page 2 it says \"observe that a smaller loss can be obtained\". What loss? A smaller loss with respect to what?\n1. In the third paragraph of page 2 it says \"the approximation precision issue\", which issue are you referring to, specifically?\n1. Can you clarify what you mean by \"least fixed point\" below equation 3?\n1. In figure 1, are all the $c$s in the figure (there are three of them) all the same?\n1. Below Figure 1 the authors say \"where $\\phi$ is the state encoder\", but no state encoder has been introduced.\n1. In Section 4.1, do $\\phi_r$ and $\\phi_d$ share parameters? Figure 1 suggests they do.\n1. In page 5 it says \"lead to large regression losses\". What regression loss is this, specifically?\n1. In page 5 it says \"which destabilze the representation learning\". In what way is it destabilized?\n1. In page 5 it says \"which however may loss[sic] part of useful information\". What useful information is being referred to, specifically?\n1. In the second paragraph of page 5 it says \"able to preserve more task-relevant information\". What do the authors mean by this, specifically? What task-relevant information?\n1. The sentence immediately above equation (11) ends with \"comprises of encoder is\". What is meant by this?\n1. In the line below equation (11) what is meant by \"a convention form of Q-function\"?\n1. Below equation (11), if Q depends on $c$, shouldn't $c$ also be a parameter of the Q function?\n1. In the learning curves, what do the shaded areas represent?\n1. In section 5.2 it says \"**AMBS + Drq** is DBC with data augmentation\", do the authors mean \"AMBS with data augmentation\"?\n1. In section 5.2 the authors tried fixing $c$ to $0.5$, but a more natural choice would have been $\\gamma$, given the main point made above. Further, neither DBC nor the original $\\pi$-bisimulation from Castro (2020) use the $(1-c)$ term in front of the reward differences. Fixing $c=\\gamma$ and removing the $(1-c)$ term would have yielded a more direct comparison to DBC.\n\n---\n\nSome minor comments:\n1.  In the first paragraph of the introduction, consider replacing \"based on some RL algorithms\" with \"based on various RL algorithms\"\n1.  In the first line of page 2, should be \"of **a** Markov Decision Process\"\n1.  A paper from early in the year is quite relevant and should be included in the related work section for representation learning:\n    * [Agarwal et al., Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning; ICLR 2021](https://arxiv.org/abs/2101.05265).\n1.  In the second paragraph of page 2, consider including two recent papers that have been accepted to NeurIPS:\n     *  [Castro et al., MICo: Improved representations via sampling-based state similarity for Markov decision processes](https://arxiv.org/abs/2106.08229)\n     *  [Kemertas and Aumentado-Armstrong, Towards Robust Bisimulation Metric Learning](https://arxiv.org/abs/2110.14096)\n1.  In the second paragraph in page 2, should be \"and potentially ***lose*** parts of ***the*** state features\"\n1.  In the third paragraph in page 2 they say \"pair of meta-learners that learn similarities\". Please specify which similarities are being referred to.\n1.  In the third paragraph in page 2 it should say \"hand-craft***ed*** form\"\n1.  Right above section 3, use `\\citet` for \"Pitis et al\".\n1. In section 3, specify the range of $\\gamma$. e.g. $\\gamma\\in [0, 1)$\n1. Right below equation 2, should say \"where $d$ quantif***ies*** the behavioral...\"\n1.  In the same paragraph, better to say \"defines a metric ***with respect*** to a policy $\\pi$.\n1. In the caption of Figure 1, should say \"which ***is*** jointly learned\"\n1. Below Figure 1 caption, $\\gamma$ is referred to \"the hyper-parameter\", but I think the authors mean discount factor.\n1. Below Figure 1 caption, should say \"combined with ***the*** reinforcement learning...\"\n1. In the first paragraph of section 4, remove the \"the\" before Figure 1 (i.e. \"is demonstrated in Figure 1\")\n1. In the first paragraph of section 4, should say \"two learned similarit***ies*** in a specific...\"\n1. In section 4.1 the network architecture is mentioned but has not been introduced. I'd suggest referencing the appendix where it is introduced.\n1. In the second paragraph of page 5 it should say \"and therefore ***the*** meta-learner...\"\n1. In the second paragraph of page 5 it should say \"the process of udpating ***the*** state encoder\"\n1. In the second paragraph of page 5 it should say \"Besides, $f_*$ is ***a*** non-linear transformation\"\n1. In equation (5) specify that you are using the closed form of the $W_2$ metric (as in DBC). This is mentioned in the appendix, but should be clarified in the main paper as well.\n1. Above equation (6) you mention the \"learned parameteric dynamics model\". Please add a reference to the appendix where it is defined.\n1. Above equation (6) should say \"two transitions sampled from ***the*** replay buffer\"\n1. In the first paragraph of 4.2 you should specify that $c\\in (0, 1)$ is enforced by a softmax. This is specified in the appendix but should be clarified in the main paper as well.\n1. In the paragraph above equation (11) should say \"we aim to make $f_r$ and $f_d$ ***symmetric***\"\n1. In the line above 5.1, should say \"which is a common***ly*** use***d*** off-policy...\"\n1. In the first sentence in 5.1, should say \"is a***n*** environment ***that*** provides high dimensional pixel observation***s*** for RL tasks\"\n1. In the first sentence of 5.3 it should say \"but have differen***t*** reward functions\"\n1. In section 5.5 should say \"learning of RL agent***s*** may suffer from...\"\n1. In section 5.5 should say \"DBC to create a***n*** autonomous driving...\"",
            "summary_of_the_review": "See main concerns above.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper studies the problem of learning invariant visual representations for reinforcement learning. Specifically, it builds on prior work Deep Bisimulation for Control (DBC) (Zhang et al), which learns an image representation predictive of rewards and dynamics and uses it with SAC. This paper makes a few modifications on top of DBC, specifically (1) instead of a single state embedding, it learns two separate ones (one for rewards and one for dynamics), (2) instead of using L1 distance in the embedding space to measure distance it instead uses a learned MLP which predicts distance, (3) using gradients from SAC it learns to dynamically adjust the balance between the dynamics weight and reward weight, and (4) adds image augmentation to the training. Experiments indicate on Distracting Control that on 2/4 tasks it matches DrQ and DBC and on the other 2 tasks outperforms them. Ablations suggest that all components are important, particularly (1) and (2).",
            "main_review": "*Strengths* \n\n- The paper studies an important problem in learning visual representations for RL robust to distractions. \n- The experimental results are pretty strong, outperforming the relevant baselines (DBC, DrQ) and including ablations which show all components of the proposed approach are useful.\n- Some components of the method, like combining bisimulation with data augmentation are well motivated.\n\n*Weaknesses*\n\nDespite the ablation which shows that each component of the method is important to performance, I still have some concerns/questions on the correctness of the proposed method.\n\nFirst, the idea of learning 2 separate state embeddings, one for reward and one for dynamics. The ablation suggests this is very important for good performance, however it seems to produce a metric that is no longer reflective of bisimulation. The bisim metric should capture states which are functionally similar (hence similar in both immediate reward and future state distribution). But by splitting the two up into separate embeddings, and training one only with the reward and one only with dynamics, it seems this functional similarity is no longer captured? The reward representation will only capture immediate reward, while its not clear what the dynamics representation will capture since its only objective is future state similarity in the same dynamics representation space. Is it not possible that the dynamics model would collapse to simple predicting 0 everywhere to minimize its loss?\n\nSecond, the intuition given for using a learned distance instead of L1 or L2 is unclear. While it may be easier to optimize, the point of using L1 or L2 is to explicitly structure the embedding space to make RL easier. When instead using an MLP its not clear how the embedding space itself will be shaped, or why it would be shaped in a way that is good for RL. Also, this is an important part of the method and should be clearly described in the main text, instead of in the appendix. Also, the framing of this learned distance as a \"meta-learner\" seems incorrect. From what I can understand it is simply using an MLP taking in the two embeddings and predicting distance, but not doing any meta learning. The confusing description + the fact that all implementation details about this are only in the appendix makes this component of the method difficult to understand.\n\nI hope the authors can clarify the above. Besides these aspects of the method which seem incorrect, the results appear strong, and if the authors can address the above concerns I'd be open to raising my score.\n",
            "summary_of_the_review": "The paper studies an important problem and has some solid results. However in their current form the major components of the method are not clearly motivated and there are some questions I have about the correctness/justification of the approach. \n\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "\nThis paper proposes a representation learning method for reinforcement learning via the bisimulation metric. It’s largely an extension of the work in Deep Bisimulation for Control (DBC) (Zhang et al., 2020) with some additional improvements:\n- Learned distance metrics (“meta-learners”) instead of L1 distance on the learned representations\n- Separate feature encodings for state and dynamics\n- Data augmentation from DrQ (Yarats et al., 2021) \n- Learned c tradeoff between reward and dynamics\n\n",
            "main_review": "\nStrengths: \n- The paper’s improvements over prior work are generally pretty reasonable and backed by convincing experimental results\n- The experimental results are generally pretty thorough with relevant comparisons to prior works and ablations\n- The writing is clear and exposition of the literature is thorough \n\nWeaknesses:\n- This paper generally feels like a “DBC with more bells and whistles” that enable it to work better; I think the general novelty is on the weaker side. The authors can consider a more thorough investigation into where existing representation learning methods fall short and address them more fundamentally\n- The experiments with natural video backgrounds generally feel pretty contrived (granted this is what’s in the literature). The only realistic setting is CARLA and I would have liked to see more environments of that nature or more detailed experimentation there.\n\nDetailed questions:\n- If reward and dynamics have different encoders, to what extent is this still learning a bisimulation, or mapping to an equivalent state space?\n- What do the experimental evaluations look like with more environment steps? Are the asymptotic performances of all methods similar or is AMBS just more sample efficient in the beginning? e.g. DBC uses 1e6 steps on CARLA\n- nit: it’s not really clear to me the naming of “meta-learner”, what is the meta train/test tasks here? I think something like learned distance metric would be clearer\n\n",
            "summary_of_the_review": "Overall this paper’s contributions are generally reasonable and empirically backed, however its novelty and experimental evaluation leave much to be desired. To this end, I am leaning towards acceptance but would not be upset if this paper is rejected.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}