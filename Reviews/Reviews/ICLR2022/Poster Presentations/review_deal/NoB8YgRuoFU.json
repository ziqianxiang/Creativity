{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper proposes a novel method, PI3NN, for estimating prediction intervals (PIs) for quantifying the uncertainty of neural network predictions. The method is based on independently training three neural networks with different loss functions which are then combined via a linear combination where the coefficients for a given confidence level can be found by the root-finding algorithm. A specific initialization scheme allows to employ the method to OOD detection. \n\nReviewers agreed on the importance of the problem of producing reliable confidence estimates.  The proposed method addressed some of the limitations of the existing approaches, and reviewers valued that a theoretical as well as an empirical analysis is provided. \n\nOn of the main criticisms was that the theoretical derivation of the method is based on the assumption of the noise being homoscedastic. This however is a common issue with other methods in this area, which are nevertheless all applied (and seem to work) on heteroscedastic data as well and are outperformed by the proposed method. Another main point that was criticized was that the empirical analysis was limited. In turn the authors added another experiments on another dataset and with another network architecture (a LSTM) to their analysis. Moreover, the authors adequately addressed a lot of the concerns and questions of the reviewers in their answers and the revised manuscript.  The final mean scores are exactly borderline (5.5) but with a higher confidence of reviewers voting for acceptance.  Based on the listed points, the paper should be accepted. \n\nI would encourage to  improve the discussion around the dependence on x in Section 3.2, which could still be made clearer, in the final version of the manuscript, and to add the discussion about the limitations of the theoretical analyses (i.e. the applicability  only to the homoscedastic settings) to the conclusion."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a new prediction interval method which is called prediction interval based on three neural networks (PI3NN). The motivation is to have a method that neither requires retraining neural networks for different confidence levels nor involves highly customized loss functions. In PI3NN three neural networks are trained once and a linear combination of their outputs is used for predicting intervals. Also, a variation of the method is proposed for OOD detection.",
            "main_review": "+ves:\n\n- The method seems to achieve its motivating goals mentioned above.\n\n- The paper has considered OOD detection and proposes an idea to help with that along some experiments.\n\n- The experiments show that the performance of the method is comparable or slightly better than competitive baselines and less sensitive to some hyperparameters.\n\nConcerns:\n- One of my main concerns is that there seems to be a disconnect between the theoretical justification and the algorithm. For example, based on the assumptions in Section 3.1, the expectation in (5) is independent of x. Going to step 3 in 3.2, the term is considered to be dependent on x. (9) does not seem to agree with (5).\n\n- Several setups of different methods have similar PICP values. It is not clear why PI width cannot be compared among them.\n\n- The experiments are mostly on toy or simple data and architectures. There is no experiment on other types of data like images.\n\n- The idea to help OOD detection is limited to single target ReLU networks. \n\n- While one motivation is to train the neural networks once, there is no experiment to show results for different confidence intervals for fixed trained networks.\n\n- The notation is unfortunately confusing at times. For example, given or at a specific x is inconsistently dropped or included.\n\n- One disadvantage is that the method is restricted to one dimensional output while some of the other baselines are more versatile.\n\n- There is no discussion about the scalability and run time of the method. \n\n\nMinor:\n\n- There seems to be a typo in Theorem 1 (> and < flipped)\n",
            "summary_of_the_review": "The paper is well motivated, but the theoretical analysis does not appear to be solid enough for the proposed method and the experiments are limited. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "**Summary**\n\nThe paper proposes a new method, PI3NN, for predicting confidence intervals with neural networks. The method trains three neural networks with different loss functions, which can then be combined without retraining to give intervals for with different confidence levels. With an additional adjustment to initialization and training, the authors propose a simple way to perform OOD detection with the predicted intervals.",
            "main_review": "**Strong Points**\n\n- Producing confidence estimates along with predictions is clearly an important problem that deep learning methods struggle with.\n- The method is more efficient than existing approaches that require retraining for different confidence levels.\n- The mathematical details are easy to follow and complement the paper.\n- PI3NN outperforms the baselines considered in their experiments.\n\n\n**Weak Points**\n\n- The method makes the assumption that noise is homoscedastic, which is often not the case in deep learning applications—although competing methods make this assumption also.\n- The baselines compared against could be more extensive. Also see the question about the use of Equation (12) below.\n\n\n**Questions**\n\n- Did you try training with L1 loss to predict the median rather than the method outlined in Step 2 from Section 3.2?\n- u_\\theta and l_\\xi are written as functions of x in Section 3.2. However, aren’t you assuming the targets are independent of x? This is even mentioned in the proof of Lemma 1.\n- For the other methods compared against in Table 3, are they intended for OOD detection? It seems like you are comparing their performance based on the metric you define in Equation (12), but I’m unclear on whether this is reasonable. I’m curious if there are other methods for OOD detection that do not use PI that could be compared against.\n\n\n**Additional Feedback**\n\n- The assumption that the noise is homoscedastic should be discussed in the paper’s introduction.\n",
            "summary_of_the_review": "**Recommendation**\n\nIf my questions and concerns below are addressed, I think the paper is interesting enough for acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose neural networks-based method which calculate prediction intervals for uncertainty quantification. The authors validate their method with its baselines including QD, PIVEN, SQR, and DER on nine UCI datasets and  flight delay dataset. The authors suggest that their method can calculate prediction intervals without retraining neural networks and the estimated prediction intervals can avoid the crossing issue. \n",
            "main_review": "The targeted goal is interesting and meaningful and the authors achieved this by calculating prediction intervals for confidence levels.  The proposed methodology is well-described and introduced experimental settings are properly designed with multiple public datasets. The experimental results seem to be reasonable.\n\nThe paper addresses the import issue of uncertainty quantification, which can hugely affect real-world applications. The introduced experimental settings seem to be reasonable to show the effectiveness of its method in calculating prediction intervals and the OOD identification capability.  Although interesting, It’s a bit hard to say the introduced technique is quite new.\n\nI believe it will be better if the authors perform the ablation studies with different parameters and more datasets with different data structures (such as images or time-series data) to generalize their findings.",
            "summary_of_the_review": "Although the targeted problem is interesting and the proposed method is mathematically correct with reasonable experimental results, I think more analysis is needed to make their method more interesting. It's a reasonable draft of a promising line of work.\n\n*Response\n\nAlthough this paper has a good motivation and the authors improve the quality of experiments, it's still a bit hard to say the proposed method is novel enough as it is a combination of pre-existing methods.\n\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Quantifying the uncertainty of neural networks is a challenge. In this paper, the author(s) proposed a method for calculating prediction intervals (PIs) based on three neural networks (PI3NN).  And the author(s) claimed that this paper addressed the limitations of existing PI methods including crossing issues, the requirement of extra hyperparameters, and lack of out-of-distribution (OOD) identification capability.  In addition, as demonstrated by the author(s) that empirical experiments showed superior performance of the proposed method over state-of-the-art (SOTA) approaches. ",
            "main_review": "# Strengths\n\nBy empirical experiments, PI3NN showed the advantages over existing SOTA approaches. And the theoretical analysis and proven properties of PI3NN are provided.\n\n# Weaknesses\n\nHere are some of my comments/questions. I would appreciate it if the author(s) could give a response. If I am wrong, please  correct me.\n\n- Although the design of Eq. (2), as stated by the author(s), can guarantee uniqueness, does such a design have practical validity, or is it just a theoretical trick?\n\n- How to determine $c$ in Section 3.3 for empirical experiments? Is there any basis for selection? How do different $c$'s affect the results?\n\n\nIn addition, this paper is generally well written, but some places (some issues) in this paper should be further clarified/fixed.\n\n\n- The format of many references is inconsistent, for example, Refs.[3] and [4]\n\n- Some references lack journal information, for instance, Ref. [20], please provide the journal information such as\n\n   - H. Wang and D.-Y. Yeung, “A survey on Bayesian deep learning,”  ACM Computing Surveys (CSUR), 53(5), pp.1-37, 2021\n\n- In this paper, especially in Paragraph 2, if the abbreviations are not widely used and standard, please use the full name and abbreviations when first referring to these methods, just as Paragraph 1 of this paper adds the abbreviation \"NNs\" to the name of neural networks.\n\n- In Paragraph 2, the introduction of related work is grammatically confused and inconsistent.\n",
            "summary_of_the_review": "As claimed by the author(s), this paper addressed some of the limitations of the existing approaches. Empirical experiments showed the superiority of PI3NN, and theoretical proven properties of PI3NN are provided. So I think this is an interesting paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}