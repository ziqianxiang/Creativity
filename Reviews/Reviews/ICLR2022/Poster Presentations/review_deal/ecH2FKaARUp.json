{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "To tackle the problem of classification under input-dependent noise, the authors proposed the posterior transition matrix (PTM) to achieve statistically consistent classification. Specifically the information fusion approach was developed to fine-tune the noise transition matrix. Experiments demonstrated the effectiveness of the proposed approach.\n\nI would like to thank the authors for the detailed feedback to the initial reviews and also further feedback to the reviewers' additional questions. Many concerns were clarified by the feedback, and the additional experiments still demonstrate the effectiveness of the proposed method.\n\nThe issue of data augmentation still remains, which should be at least experimentally investigated,\nbut the contribution of the current manuscript is still valuable to be presented as ICLR2022."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes to estimate an input-dependent noise transition matrix (PTM). The authors make a strong assumption that the neural network output approximates the underlying clean probability and estimate the PTM by extracting the output of each training sample using the network. To enforce the network to have better probabilities, PTM estimation is applied iteratively and warm-up is used during the training. The experiments are conducted in CIFAR-10 with different noise ratio, as well as a real world dataset, i.e. Clothing-1M.\n",
            "main_review": "The paper proposes an interesting direction of estimating \"instance-based\" noise transition matrix, rather than estimating a class-based transition matrix. The authors first provide a brief introduction of noise transition matrices in the existing work, and motivate the need for estimating an instance-based transition matrix.\n\nThe presentation and the writing of the actual method can be improved to make it easier for the reader to understand different steps of the pipeline. For example, the interaction of different components (the network, PTM estimation IF correction etc. ) is not very clear, and an overview figure can be used to visualize their interaction. Figure 1 is not clear and does not help reader understand the interaction of different components.\n\nMy biggest concern is the assumption of \"good\" network outputs to estimate the PTM. The network is known to overfit the noisy labels, and this assumption may not always work well, especially if the noise ratio is high. For example, what happens when the noise ratio is above 50%. Existing work usually report noise ratio up to 80%-90% on CIFAR, and it would be interesting to see how PTM behaves under such setting.\n\nThe experiments would also benefit from additional baselines. For example, Label smoothing (Szegedy et al., 2016) adds a uniform distribution to the assigned label. Mixup (Zhang et al., 2018) creates a combination of labels. These methods do not correct the label noise through an instance-based transition matrix like this paper, however the way the \"corrupt\" the assigned label adds a regularization effect and help denoise the noisy labels.\n\nThe Clothing1M results in Table 2 are missing better-performing comparisons, such as Divide-Mix (Li et al., 2019), ELR+ (Liu et al., 2020) etc.\n\n\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In CVPR, pp. 2818–2826, 2016.\n\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. ICLR, 2018.\n\nJunnan Li, Richard Socher, and Steven CH Hoi. Dividemix: Learning with noisy labels as semisupervised learning. In ICLR, 2019.\n\nSheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-learning regularization prevents memorization of noisy labels. NeurIPS, 2020.",
            "summary_of_the_review": "The paper proposes an interesting and original idea. However, I think a few important experiments missing (higher noise ratio, additional baselines such as label smoothing and mixup), and the Table 2 needs to be updated with more recent work. I would be willing to upgrade my evaluation if the authors address my concerns during the rebuttal.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper provides new insight on estimating noise-transition matrix in the setting of instance-dependent label noise. Specifically, the authors observe that the traditional noise-transition matrix cannot well bridge the gap between underlying distribution and empirical distribution. Inspired by this observation, the authors propose to estimate the posterior noise-transition matrix. Further, authors use information fusion (IF) to linearly combine noise-transition matrix and posterior noise-transition matrix to obtain more accurate transition matrix estimation with theoretical guarantees. Experiments are conducted on CIFAR-10, SVHN, F-MNIST, and real-world dataset Clothing-1M. ",
            "main_review": "Strengths：\n1, I think the posterior noise-transition matrix and information fusion (IF) approach are new and interesting. Theoretical analyses are also provided to prove the robustness of such a transition matrix. \n\n2, The paper is well written and organized. The authors did a good literature review on the previous methods.\n\nWeaknesses:\n\n1: In Theorem 3.5, authors assume that $e_T(x)$ and $e_W(x)$ are independent. I think it is impractical. By the definition of $e_T(x)$ and $e_W(x)$, these two terms may be strongly dependent. Thus Theorem 3.5 can not well explain the experiments. \n\n2: From experiments in Table 1, PTM does not show much performance gain compared to PTD. Thus the applicability of the posterior transition matrix may be limited.\nBesides, it is not clear how PTM and IF behave on the dataset with more categories such as CIFAR100.\n\n3:  Many methods have been proposed in the literature to deal with IDN (A1, A2, A3, A4, A5). However, the authors only compare PTD in the experiments. I think more comparisons are needed to further validate the effectiveness of IF. \n\n\nA1: Learning with bounded instance- and label-dependent label noise. ICML 2020\n\nA2: Part-dependent label noise: Towards instance-dependent label noise. NeurlPS 2020\n\nA3: Confidence scores make instance-dependent label-noise learning possible. ICML2021\n\nA4: Learning with feature-dependent label noise: A progressive approach. ICLR2021\n\nA5: Learning with instance-dependent label noise: A sample sieve approach. ICLR2021",
            "summary_of_the_review": "Overall, the quality of this paper is good. The proposed approach is supported by the theorem. The experiment shows that information fusion (IF) outperforms PTD (Part-dependent transition matrix) by a large margin when the noise rate is high.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes to do loss correction by a posterior transition matrix (PTM).\nThe authors first define the loss function using PTM which achieves the statistically consistent classifier, then propose an information fusion approach to reduce the variance in estimating the transition matrix.",
            "main_review": "### Strengths:\n\n1. The paper defines a new loss-correction approach, which uses the posterior transition matrix, to achieve the statistically consistent classifier.\n\n2. The authors also propose the corresponding approach to estimate the posterior transition matrix based on model predictions.\n\n3. The high-level intuition of using the Kalman filter to reduce the estimation error is insightful. \n\n### Weakness:\n\n1. It is not clear why the proposed information fusion approach works. Section 3.3 presents an information fusion method based on Kalman filtering, where the Kalman transition matrix is given by a convex combination of NTM and PTM. Although it can be proved that carefully assigning the combination weights makes the reconstruction error lower than both $\\sigma_T(x)$ and $\\sigma_W(x)$, the errors indicate the variance of the estimation. Only reducing the variance may not be helpful due to the estimation bias. In others words, a convex combination of NTM and PTM would introduce estimation bias, which makes $E[W_{km}(x)] \\ne E[W^*(x)]$. With the existence of bias, it is hard to conclude the proposed information fusion approach is helpful.\n\n2. It would make the claim stronger if the authors provide more intuitions, examples, or theoretical analyses to show why PTM could be easier to be estimated. According to the Bayes rule, we have\n\\\\[\nP(\\tilde Y=i|Y=j,X=x) = P(Y=i|\\tilde Y=j,X=x) \\cdot \\frac{P(\\tilde Y=j|X=x)}{P(Y=i|X=x)},\n\\\\]\nwhere $P(\\tilde Y=j|X=x)$ could be estimated from the noisy dataset and $P(Y=i|X=x)$ could be estimated by marginalizing $P(Y=i,\\tilde Y=j|X=x)$ w.r.t. $\\tilde Y$. From this aspect, estimating both matrices should have the same degree of difficulty.\n\n3. It is not clear how estimation error is defined in Figure 2. Recent methods to estimate NTM (e.g., end-to-end approach [R1], using clusterability of representations (Zhu et al., 2021b cited in the paper) can also achieve relatively low estimation errors according to their papers. Figure 2 would be more trustworthy if recent works are compared.\n\n\n4. In Page 6, $e_T(x)=W^*(x)-\\hat T(x)$ looks confusing. Should it be $e_T(x)=T^*(x)-\\hat T(x)$? Besides, typo \"nose\" in the caption of Figure 3 and Figure 4. Typo in Table 1, CIFAR10 PTM-F-V IDN-20\\%.\n\n[R1] Li, Xuefeng, et al. \"Provably end-to-end label-noise learning without anchor points.\" ICML 2021.\n",
            "summary_of_the_review": "The idea is interesting but the paper is not well-organized or presented. The information fusion approach, which should be the key novelty of this paper, is not demonstrated sufficiently. The main concern is that the proposed information fusion approach reduces the variance at the cost of introducing bias in estimation.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new information fusion approach to deal with the instance-dependent label noise (IDN). Specifically, the authors claim that the essential problem caused by IDN is empirical, instead of underlying, data distribution mismatch during training, which I think is new and interesting. Therefore, the authors fuse posterior transition matrix (PTM) and noise transition matrix (NTM) and design an unbiased risk estimator. ",
            "main_review": "Pros:\n1. The argument in this paper, namely IDN is indeed caused by empirical distribution rather than underlying real distribution, is interesting.\n2. The proposed estimation method for PTM and the fusion of PTM and NTM via Kalman filtering are new.\n3. Theoretical analyses in this paper are sufficient.\n\nCons:\n1. The title of this paper can be more concise.\n2. More related baseline methods are suggested to be added.\n\n",
            "summary_of_the_review": "Generally, I feel that the claim in this paper is reasonable, the model proposed is also new. The paper is written in a clear way. I only have some minor concerns:\n1. In the title, the authors use \"information fusion\", which I think is not concise. Note that ``information'' is a rather wide topic, so after reading the title, I cannot see what are fused in this paper. \n2. Since this paper studies instance-dependent label noise, I hope to see more comparisons with existing methods for handling IDN, such as \"A Second-Order Approach to Learning with Instance-Dependent Label Noise\" (CVPR 21); Tackling Instance-Dependent Label Noise via a Universal Probabilistic Model (AAAI 21);  Learning with Bounded Instance- and Label-dependent Label Noise (ICML 20). In the current experiment, only one method, namely  PDT (NeurIPS 20) is designed for IDN. Other methods such as Forward, T-revision are not specifically designed for IDN. Therefore, I feel that the comparison is not that fair.\n3. Is the solution for Posterior transition matrix in Thm 3.4 guaranteed to satisfy the constraint that the sum of every column is 1?\n4. From Thm 3.5 and Eq. 13, I feel that the solution is also related to statistical efficiency? See Section 2.4 and Section 3 of \"Centroid Estimation with Guaranteed Efficiency: A General Framework for Weakly Supervised Learning\" (TPAMI 20). If yes, the authors may analyze the connections or relationship between them.\n5.  In Fig. 1, I suggest the authors clearly annotating PTM in the figure. Note that the authors use \"noise transition matrix (PTM)\" in the caption, but write \"posterior TM\" in the figure. Such inconsistency in the figures should be avoided.\n6. Some language issues, such as \"a easy-to compute\" should be \"an easy-to-compute\"; \"is the instance vector of n-th sample\"->\"is the instance vector of the n-th sample\".",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I do not have ethics concerns on this paper.\n",
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}