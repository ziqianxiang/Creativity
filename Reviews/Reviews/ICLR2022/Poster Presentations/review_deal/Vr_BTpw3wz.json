{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The authors study the problem of open-ended knowledge-grounded natural language generation, in the context of free-form QA or knowledge-grounded dialogue, focusing on improving the retrieval component of the retrieval-augmented system. By retrieving more relevant passages, the generations are more grounded in retrieved passages. \n\nPros:\n+ The paper is clearly written and motivated.\n+ Presents a straightforward approach that shows improvement over a  strong baseline.  \n+ A strong paper focuses on a rather under-explored problem of knowledge-grounded open-ended generation, proposing novel objective, significant empirical improvements on two datasets in multiple metrics.\n+ The authors included human evolution results to support their findings. \n+ The authors did a good job addressing several questions raised during review period and added several new experiment results and discussions to strengthen their findings. The reviewer team was generally satisfied.\n\nCons:\n\n+ Several related work on knowledge guided dialog response generation is missing in the paper. Although the paper's focus is on retrieval based QA systems, the main focus is on open domain generation, which has overlaps with dialog response generation research. So the authors should cite the following papers in their paper: [1] Dinan, Emily, et al. \"Wizard of wikipedia: Knowledge-powered conversational agents.\" arXiv preprint arXiv:1811.01241 (2018). [2] Zhou, Kangyan, Shrimai Prabhumoye, and Alan W. Black. \"A dataset for document grounded conversations.\" arXiv preprint arXiv:1809.07358 (2018). [3]Zhan, Haolan, et al. \"CoLV: A Collaborative Latent Variable Model for Knowledge-Grounded Dialogue Generation.\" Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021. [4]Zhao, Xueliang, et al. \"Knowledge-grounded dialogue generation with pre-trained language models.\" arXiv preprint arXiv:2010.08824 (2020). \n+ There are several related work concerning with generation of a response given a relatively small set of evidence text such as the following ones:  [5]Lian, Rongzhong, et al. \"Learning to select knowledge for response generation in dialog systems.\" arXiv preprint arXiv:1902.04911 (2019). [6]Kim, Byeongchang, Jaewoo Ahn, and Gunhee Kim. \"Sequential latent knowledge selection for knowledge-grounded dialogue.\" arXiv preprint arXiv:2002.07510 (2020). Although these work do not include a retrieval part, the authors should cite and discuss similarities and differences to [5] & [6] in their paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper tackles a problem of knowledge-grounded open-ended generation and proposes a new model that is an extension of an end-to-end training of the retrieval and the generator. In particular, there is another model called “posterior-guide” that is jointly trained with the other two models using ELBo. Intuitively, this posterior-guide is similar to the retriever that scores the evidence, but conditioned on not only the question but also the response. It is used as a weight of each evidence in the generation in order to encourage the generator to ground more to the evidence that is more relevant to the response. The retriever is also trained to be close to the posterior-guide (minimizing KL-divergence). To my understanding, this is a very clever way to give supervision to the model when it is not easy to obtain distant supervision data (unlike in short answer generation where the gold evidence is easily obtained by whether the short answer is included in the evidence or not).\n\nExperiments are done on Wizard of Wikipedia and MS Marco NLGen. The model achieves significant improvements over the baseline retriever, based on three evaluation metrics: relevance, groundedness and generation quality.\n",
            "main_review": "Strengths:\n* The problem in this paper is important and underexplored --- despite recent advances in neural retrieval, knowledge-grounded open-ended generation is less studied compared to other problems like short answer generation. As the paper motivates, there are many reasons that prior work focusing on short answer generation are not suitable for open-ended generation.\n* The proposed method is novel, and shows very strong empirical results.\n\nWeaknesses:\n* One major concern is that the pipeline training baseline ---  the approach that trains the retriever and the generator separately --- is missing, although it is (arguably) the most widely-used approach and is a potentially stronger baseline. Pipeline training (Karpukhin et al, Khattab et al and more) has been much more widely used in recent retrieval work compared to joint training due to its simplicity, stability, and better empirical results (e.g., as shown in [1] which compares joint training and pipeline training under the same condition). Although I agree with the motivation for joint training in the paper, there is no justification for not adding a pipeline training baseline, especially given that the datasets come with annotated gold passage so it is pretty straightforward to train two models separately.\n* Three research questions for evaluation in Section 4 all look reasonable to me, but how the specific metric used in the paper measures those is not entirely clear to me. For example, is F1 / Nov-F1 between retrieved evidence and generated response a standard metric to measure the groundedness? If not, there should be some evaluation that shows that F1 / Nov-F1 correlates with human evaluation of the groundedness of the generated response.\n\nMinor comments / questions\n* Is “Success @ k” an existing term taken from prior work? This metric is identical to “Recall @ k” which is a standard, long-standing term. Unless “Success” is another widely-used term as an alias of “Recall” that I am unaware of, it should be replaced to “Recall”.\n* Is there a reason that ColBERT is pretrained on MS-MARCO or NQ? Has there been any ablation on the impact of this pretraining? Does any of findings in the paper change if the model is not pretrained on another dataset?\n\n[1] https://arxiv.org/pdf/2101.00408.pdf",
            "summary_of_the_review": "Overall a strong paper, tackling important and underexplored problem of knowledge-grounded open-ended generation, proposing novel objective, significant empirical improvements on two datasets in multiple metrics. There are a few concerns like absence of a potentially stronger baseline, justification of evaluation metrics, choice of terms, choice of implementation details.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper authors describe an approach to use the responses/ answers to guide retrieval during training of document grounded response generator. By having the retriever being trained using the posterior (p(document|response,context)), the model could learn a better (supervised) retrieval network (based on ColBERT) which could be used to guide the training of the prior ( p(document|context)).  Documents are retrieved using the dialog context and the top-k documents are used for training the posterior as well as the prior. Since the expectation for ELBOLoss cannot be computed exactly (due to the large document set),  it is computed by sampling documents from the top-r retrieved documents -- from either the posterior or the prior, guided by a parameter called alpha which governs the sampling proportion. The idea of doing variational training using a posterior network isn't particularly novel but the authors have made it work for open-ended response generation using this approximation for computing the ELBOLoss. The networks use BART for language generation and CoLBERT for modeling the retrievers. Experiments have been presented using the Wizard of Wikipedia dataset and the MSMARCO NLGEN. Experiments show an improvement over the baseline model (RAG - referred to as MarginalizedLoss) in both retrieval (success@k, MRR) as well as response generation (text-F1 overlap between response and grounded document and textF1 overlap between response and ground-truth output response). Overall this is a well written easy to read paper\n\n",
            "main_review": "--------------\nStrengths\n--------------\n1. Improvement in performance over a baseline RAG model by use of the posterior for training the retriever   \n2. Experiments on aspects related to retrieval as well as response generation\n--------------\nWeakness\n---------------\n1. Limited evaluation - no experiments to study hallucination/memorization. This is a big weakness because BART is a powerful generation model. \n\n----------------\nQuestions:\n----------------\n1. The paper discusses using an alpha-mixture but doesn't clearly explain the need for the same. I'd imagine there's a risk of not seeing the ground-truth document when sampling from P_eta (because of a poorly trained prior network) and that is probably why alpha is set low initially, but what happens if you only sample from Q? Perhaps experiments that demonstrate the benefit of using an alpha-mixture over just using the posterior Q could be useful.\n2. What happens if the ground-truth document is never sampled; the KL divergence term will not add anything meaningful in that case? Is there a risk of posterior collapse? Do you do anything in your training to address this problem? This is also tied to my comment about hallucination; in the event that the model does not see the \"correct\" document while training the retrievers, it will force the generator to learn without grounding. \n3. There is contemporaneous unpublished work submitted with perhaps the exact same approach (with minor differences in how the approximation is done):  https://openreview.net/forum?id=vdTCHL1nyEq Were the authors aware of this work? \n4. Why was pre-training of the retrievers done differently? ",
            "summary_of_the_review": "This is a simple and interesting idea which shows promise in performance. However, the experimental results are a little in weak -- in particular there is no study about hallucination/memorization in these models and some of the modeling choices aren't well studied.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper tackles the problem of open-ended knowledge-grounded natural language generation, in the context of free-form QA or knowledge-grounded dialogue, where models must ground their generations on passages relevant to the input context. Specifically, the authors explore improving the retrieval component of retrieval-augmented systems by utilizing posterior signal from the label. A “guide retriever” learns the relevant passage to retrieve by including the target output in its input context, and then, using an ELBo loss, provides this signal to the normal, in-system retriever. The authors find that their model improves three-fold over baselines: 1) it retrieves relevant passages more frequently; 2) its generations are more grounded in retrieved passages; and 3) its generations are closer to human generations.\n",
            "main_review": "# Main Review\n\n### Strengths\n\n- The authors indeed find improvement over a strong baseline (using a marginalized loss instead) in all three points across two unrelated datasets.\n- The proposed models are straightforward and not overly complex, with a proper diagram easily describing the training procedure, thus making the methods reproducible.\n- The findings are relevant and important particularly for those interested in what the authors describe as “one-to-many” generation tasks, where several plausible outputs can stem from a single input; the proposed model helps to narrow the “many” possible generations to the one most similar to the gold human response.\n- The submission is quite clear; methods (and the model architecture) are proposed clearly and concisely, providing the reader with appropriate background and technical knowledge to understand the model. The results are plainly displayed in tables in figures and are clearly described in the text.\n\n### Weaknesses\n- I think the authors failed to consider the model on an important subset of tasks that, in my opinion, would benefit most from this procedure: tasks without **gold retrieved passages available**. Though the authors compare to a “MarginalizedLoss” baseline, as it is used in some strong models in the literature, the real ablation to test is retrievers trained **_directly with the gold passage as target_**. WoW and MS-MARCO both provide such passages, and yet in the paper they are only used for evaluation of trained models. A more interesting task would be to apply these models to datasets without such provided targets.\n- Utilizing the target output to improve knowledge-grounded dialogue is not particularly novel in the literature:\n    - [1] proposes a system (TPPA) that learns to retrieve knowledge both from the input context and the response\n    - [2] utilizes the responses to inform the model where to pay attention to in the given knowledge, and thus can obtain models that ground more appropriately in the knowledge\n- The authors claim that a main contribution of the method is a model that produces generations more grounded in the retrieved passages than other methods. However, one could argue this naturally follows from the simply improving retrieval in the first place, in that the model will ground more often if it receives less loss from doing so, and it will receive less loss if it uses more relevant passages. So the overarching contribution here is improving retrieval through posterior-guided supervision, the effectiveness of which could be better supported if other situations were considered, see above..\n\n# Questions + Feedback\n### Questions\n1. Did you experiment with not updating the index, and instead just updating query encoders as in [3]? \n2. Why call your metric “success@10”, rather than “recall@10”? Are these fundamentally different metrics?\n3. Why were different pre-trained ColBERT models used for each task?\n4. In section 5, paragraph “Comparison to Fusion-in-Decoder”, you mention that “Fusion-in-Decoder is uniquely useful for QA style tasks”, however it is in fact used directly on the Wizard of Wikipedia task in [4] and achieves the best results reported there (better, e.g., than marginalized-loss methods such as RAG).\n\n### Minor Feedback\n1. Typo end of section 2: “we describe our method we separates those two concerns”\n2. Section 3, “Posterior as Retriever Paragraph”: “...and estimate the ELBoLoss ‘more’ accurately…”\n3. End of section 4.2: “... only use them for evaluation and validation and not for training nor for validation”, this sentence is unclear\n4. It would be nice to introduce “r.i.” as an abbreviation of “relative improvement” before it is introduced in section 4.4\n\n\n# References\n[1] Zheng et. al. 2020, Approximation of Response Knowledge Retrieval in Knowledge-grounded Dialogue Generation\n\n[2] Zheng et. al. 2021, Knowledge-Grounded Dialogue Generation with Term-level De-noising\n\n[3] Lewis et. al. 2021, Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n\n[4] Shuster et. al. 2021, Retrieval Augmentation Reduces Hallucination in Conversation\n",
            "summary_of_the_review": "The authors show improvements in two downstream tasks when improving retrieval via posterior-guided supervision of a retriever in a retrieval-augmented generation setup. However, the datasets considered are not explored to their maximum potential, with appropriate baselines missing from the paper (training the retrievers directly on the target passages); and, datasets where this solution appears to be most applicable are not considered at all (those without target passages). The paper indeed supports its claims, as mentioned in the summary, though some work is missing that could bolster the proposed efficacy of the method. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work focuses on the knowledge grounded text generation tasks. They argue that multiple passages can be valid and relevant to the context, but not all of them are observed/used in the target response. Therefore they propose that, during training, the target response should be utilized to train a \"guide-retriever\",  which predicts P(passage|context, response) and provides passage weight for the generator. A retriever is jointly trained with the generator and the guide-retriever, and the KL divergence between the retriever and the guide-retriever is included in the loss function.\n\nThe contribution of this paper is that they proposed a potential solution to the challenge that the model may be not effectively trained in case that multiple passages are valid but only one of few are used.",
            "main_review": "The strength of this paper is:\nThey novelly proposed to train a dedicated module, \"guide-retriever\", to predict the knowledge passage probability with extra information (target) and use that to guide (via KL divergence) the retriever. It seems to be a reasonable approach. \nThe evaluation and experiments are also relatively comprehensive.\n\nThe weakness is that this paper only compares their method with one baseline (MARGINALIZEDLOSS), but there're more baselines are designed for the similar tasks and should be analyzed: e.g., \"REALM: Retrieval-Augmented Language Model Pre-Training\" by Guu et al., and \"Retrieval-augmented generation for knowledge-intensive nlp tasks\" by Lewis et al.",
            "summary_of_the_review": "This work proposes a novel approach to the knowledge grounded text generation tasks via the design of a new \"guide-retriever\".\nThe methodology is reasonable and the experiments show significant improvement over one baseline. However, this work can be improved if comparison with more baselines is provided.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}