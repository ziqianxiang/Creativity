{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "#### Summary\n\nThe goal of this work is to reduce the costs of inference in ensembled models by ensembling sparse models. The paper also aims to reduce the costs of training these ensembles as well. The proposed techniques (DST and EDST) each these goals, respectively. \n\n#### Discussion\n\nAs noted by the reviewers, the paper is interesting and timely. The authors provided significant clarifications in the response that satisfied the reviewers' concerns. There is still significant room to revise the remaining points and polish the text of the paper for the camera-ready (I highly recommend proofreading from an individual who is not an author on the paper; there are still typos in the revised edits)\n\n#### Recommendation.\n \nI recommend Accept, due to the strengths above and the reasonably scoped remaining work to do going into the camera-ready."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors proposed a method to train an ensemble of sparse DNNs by means of DST such as RigL which, within the same training flop budget, is reported to generalize better than the enclosing dense net or other efficient ensembles; it is also reported to have higher inference-time efficiency.  Further analysis was performed to understand the effectiveness of the sparse ensemble.  \n",
            "main_review": "This paper pointed out an interesting and potentially useful way of training sparse DNNs bypassing the difficulty of optimizing sparse DNNs, i.e. by choosing to ensemble weakly optimized multiple sparse subnets, instead of post-training compression or lengthy DST to reach a single subnet.  \n\nMajor concerns:\n\n- Inference flops.  From the presentation of the results, I do not understand how emsembling $M$ sparse subnets would be efficient at inference time.  Of course at a certain high level of sparsity, the flop count is going to be lower than the dense baseline, but how can it beat a single subnet at the same sparsity, such as one trained with RigL, in flop count?  Even though the training flop count for a single subnet by DST is much higher, the $M\\times$ lower inference flop count would still be desirable in deployment.  If I understand correctly, a fair comparison (in the sense of equal inference flop count) should be between an ensemble of $M$ sparse subnets at sparsity $s$, and a single sparse subnet trained by DST at sparsity $1-M(1-s)$. However, the baselines for comparison are other ensembles instead of single nets.  Given the under-trained individual subnets (e.g. in Fig. 5b), single subnet even at the same sparsity might outperform a sparse ensemble.  \n- Dense ensemble baselines.  How does the sparse ensemble compare to an ensemble of thin, dense individual subnets at the same flop count?  \n- The paper presented hypotheses on the generalization performance of the sparse ensemble trained as such, which are interesting scientific questions, but did not provide convincing results or discussions to validate or disprove the hypotheses.  For example, it is not clear if *diversity*, i.e. disagreement between the subnets, is responsible for the generalization.  Counter-evidence exist, e.g. it has been shown that ensembling nets that are intentionally trained to agree with each other through knowledge distillation improves instead of hurts generalization (arXiv:1805.04770).  From a practical perspective, DST or EDST are by nature handicapped in producing diverse subnets, because the enclosing dense net architecture is still the same--in order to obtain diversity, wouldn't one ensemble individual nets of completely different architecture rather than sparse subnets of the same enclosing dense net?  Further, the ablation study on parameter exploration only reported diversity metrics, but how do they differ in generalization performance?  In Fig. 3, does ensembling a red, a green and a blue checkpoint give higher accuracy than ensembling say three red checkpoints?  \n\nMinor concerns:\n\n- Writing.  Language needs substantial polishing for publication standard.  Too colloquial at places, e.g. \"a tough nut to crack\", and redundant and ungrammatical at others, e.g. \"Currently, the current only way ...\", \"the training costs are far fewer ...\", \"exploring heavily a large fraction ...\", etc.  \n- Some figure/table fonts are too small to be legible.  \n- Sparsely gated mixture of experts.  How does the sparse subnet ensemble compare with sparsely gated MoEs at the same training and inference flop counts?  \n",
            "summary_of_the_review": "Though the paper presented interesting ideas, it unfortunately falls short of a systematic study to make a practically useful methodology, or of a compelling hypothesis test that shed light on underlying scientific questions.  \n",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work focuses on generating ensembles of sparse networks, called FreeTickets, that requires significantly lesser training FLOPs and parameters compared to single or ensemble dense networks while maintaining high prediction accuracy.  FreeTickets are expected to be diverse, highly expressive and easy to obtain. Apart from the methodology to generate these FreeTickets and ensure they match the desired properties, a key claim in this work is that FreeTickets demonstrate improved prediction accuracy, robustness and uncertainty estimation while being much more efficient, in terms of the computational resources required for training. ",
            "main_review": "Strengths\n- The outline of the training setups across different versions of DST and the proposed method was extremely clear and direct. It helps the reader understand the distinction between them easily.\n- Conceptually and practically, the improvements offered by training an ensemble of sparse networks to augment the failures of a single dense/sparse model is extremely useful. A stronger emphasis on analyzing the structural and representational diversity among the FreeTickets and drawing comparisons to prior literature on cascaded weak classifiers would help deepen the impact of the work.\n\nWeaknesses\n- The term \"cheap\" pertaining to one of the key properties required of FreeTickets is left undefined until Pg. 4, Section 3.1. While the comparison to dense networks and their ensembles is fair from the perspective of performance, could the authors clarify if methods like static LTR ensemble and others are similarly \"cheap\" to  DST and EDST? Ultimately, tying a quantifiable comparison of total FLOPs to calculate ensembles to emphasize the notion cheap would be preferable. \n- Could the authors clarify what $n^{(L)}$ refers to in Pg. 4, L. 2?\n- Could the authors discuss the different decision factors that influence the choice of $t_{ex}$? While M controls the amount of time each sub-network is fine-tuned, to what degree does $t_{ex}$ need to be explored so that a sufficient embedding space is learned?\n- Pg. 6, Paragraph on Baselines, the last sentence mentions that the LTR ensemble has low diversity and prohibitive costs. A quantifiable measure or citation would help qualify this statement.\n- Could the authors clarify if the training FLOPs include the total number of forward passes and the FLOPs for each pass or they quantify the FLOPs required for a single pass only? \n- The difference in ECE values between the best and remaining methods across different models and datasets is unstable. Could the authors comment on the significance of these values and their changes? These become critical as we observe a large variation in the best value across multiple methods and metrics, considering Tables 1, 2 and 3.\n- A breakdown of the various labels/samples that cause the disagreement between various FreeTickets would be instructional and add a visual cue to the diversity metric. Additionally, applying KL divergence across intermediate feature embeddings could trace the source of variations and how they have an effect on various metrics.\n- Fig. 3 and Section 5.2 do not comprehensively cover the tSNE projections of other models tested in Tables 1,2 and 3. Without a clear frame of reference and common scale factors it is difficult to compare across the various projections presented. I encourage the authors to try providing a projections for multiple baseline and remove any possible stochasticity in projections to ensure the comparisons are suitable.\n- Section 5.4: Based on Fig. 4 there is a claim that the pattern of ensemble accuracy is highly correlated with the accuracy of the individual sub-network. I encourage the authors to support this with a quantifiable measure. Additionally, visually they do  not seem to have the same pattern of evolution, wrt. sub-network accuracy or deviation in performance of sub-networks. Could the authors clarify the statement in the manuscript?\n- Section 5.5: The manuscript uses the term \"Individual Tickets\" which does not match the nomenclature used in Fig. 5.\n- Section 5.6: The adversarial robustness results for CIFAR-10 show that the dense model is more robust than the sparse models. Could the authors clarify their statements in Section 5.6?\n\nAfter Rebuttal\n\nI thank the authors for their detailed response to each of the comments posted and a well done revision of their manuscript.\nBased on their responses, I have updated my recommendation to an acceptance.\n\nFor the comment on t-SNE plots, on of the key issues when comparing across vastly differing feature spaces is that the 2D projection scales unequally. In providing such projections for alternative ensembling approaches, we can visually discuss the diversity of solutions (mirroring the discussion based on quantifiable metrics).\n",
            "summary_of_the_review": "The proposed methodology is supported strongly by the main set of results. However, there are some missing explanations and justifications required for statements in the analysis performed that need to be clarified. ",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose to improve the generalization of dynamic sparse neural network training methods, such as RiGL, by learning an ensemble of these models. The authors propose two methods of doing so, the Dynamic Sparse Training (DST) Ensemble, and the Efficient Dynamic Sparse Training (EDST) Ensemble. A DST ensemble is a straightforward ensemble of M DST trained models, which like all ensembles incurs high overhead in training and inference cost (training/using M models). An EDST ensemble on the other hand uses approximately the same training compute as a single model. It does so by splitting the training into two phases (of the total amount of steps as a single standard DST training run). The first phase is standard DST training (called the Exploration Phase), where training uses a large learning rate. The second phase (called the Refinement Phase) trains M models initialized from the weights of the model in the first phase, but with much smaller learning rates decayed over training. Hyperparameters determine the amount of exploration v.s. refinement training steps.\n\nThe authors evaluate the trained ensembles for Image Classification with Wide ResNet28-10 on CIFAR-10/100 and ResNet-50 on ImageNet,  and compare these to baselines including: an ensemble of sparse trained models (with a static mask), an ensemble of Lottery Ticket Rewound models, an alternative method with similar motivation (MIMO), a single dense model, BatchEnsemble and TreeNet. The DST and EDST Ensembles show similar or marginally higher generalization than baselines, but specifically in the EDST case, with much fewer (theoretical) training FLOPS. The authors also compare the diversity of the ensemble members in a few of these baselines, showing that DST has much higher diversity than EDST (as might be expected), but also interestingly that DST appears to have higher diversity than a dense ensemble. The authors perform an ablation study on their results, attempting to discern the affects of parameter exploration (i.e. dynamic mask updates in DST methods), affect of sparsity and affect of ensemble size on the results. The authors also claim to show better out-of-distribution performance and robustness, although these results are in the appendix and only referred to in the main text.",
            "main_review": "Strengths:\n- Well written paper, with good background, clear motivation.\n- This paper's contributions, while completely empirical, present a very good and I'd say relatively complete experimental analysis that will no doubt be useful for researchers and engineers in understanding how DST can benefit sparse neural network training in generalization and/or efficiency.\n- Results for EDST are compelling when generalization/theoretical training compute is considered (as it should be), results that I believe are novel and important - showing that dynamic sparse training can match dense generalization with less (theoretical compute).\n\n\nWeaknesses:\n- DST ensemble is not novel, as it is just an ensemble of DST-trained models which has been analyzed (although with a different focus) in other work.\n- EDST is marginally more novel, but clearly related to other existing methods of training deep neural networks.\n- Some of the analysis of the diversity of sparse solutions is not novel, e.g. Lottery Ticket rewinding not resulting in diverse solutions/producing poor ensembles relative to static sparse ensembles is one of the main contributions of the paper \"Gradient flow in sparse neural networks\", Evci et al., 2020. This paper is cited by the authors elsewhere but not in reference to these results. The authors expand on these results however, and I believe it would strengthen the author's results on the diversity of solutions if they cited this work in the appropriate places and compared their findings (which I believe corroborate that paper's).\n- Some parts of the method are not clearly described (i) through (iv) below:\n(i) 3.1, end of page 3: \"A free ticket is one of the sparse neural networks (subnetworks) generated *during* the dynamic sparse training procedure\". These free tickets \\theta_s^1, \\theta_s^2, \\ldots, \\theta_s^M)...\" this is *very* misleading as \"during\" the DST training there are multiple subnetworks generated over time with each mask update, and this text makes it sounds like the ensemble is of those - but I realized by the next page this is not the case. This needs rewording to make it clear subnetworks are the result of multiple DST training runs.\n(ii) example Algorithm 1 does not describe how the refinement model weights (\\theta_s^j) are initialized explicitly, and this isn't clearly defined in the text. I can only reasonably assume they are initialized from \\theta_s in the exploration phase.\n(iii) Another aspect of this method that's not clear: \"...at the end of each refinement phase\" in section 3.3, does this mean the refinement phase is run multiple times? Or is this referring to each of the subnetwork's refinements? Figure 1 also confuses me on this point as it seems to show multiple refine/exploration steps, and Algorithm 1 only seems to show one Refinement phase (of t_{ex} < ?? steps). Given the lack of clarity, I've assumed there is only one exploration & M refinement phases (consisting of M subnetwork training runs) but I would appreciate this being made clear in the text/Algo 1/Fig 1.\n(iv) several of the variables used in Algo 1 are undefined or misused, including s^i \\in (s^1, \\ldots, s^L) which seems to be the sparse masks, and t_{ex}, and t_{re} which are used as time counters/indices in the algorithm, but are the total amount of time in the text.\n- The paper's usage of \"forward passes\" is problematic - it's defined as a \"complete training phase\" in 3.3, i.e.  a whole training run, which goes against the common usage of the term and is very misleading for the careless reader in the results - change this term to \"training runs\"\n- Although the motivation is efficiency, there are no real timings *or* explanation on how these theoretical FLOPS gains might be realized. There are compelling papers that the author could refer to showing how to accelerate unstructure sparsity however on CPU and GPU (see \"Fast sparse convnets\", Gale et al. 2020), and I would encourage the authors to refer to these to allay such concerns.\n- Figure 1 is much too small, and yet there is a lot of wasted space in how it's presented on the page. I'm also pretty sure that ICLR doesn't allow text to wrap around figures like this - but I could be wrong. Regardless, it would be very easy to fix this figure so that it was more compact, a proper floating figure, and more legible at the same time.",
            "summary_of_the_review": "I believe this paper, albeit short on technical novelty, does provide a sufficiently novel and important empirical evaluation and details analysis of Dynamic Sparse Training (DST) ensembling methods, and could be accepted - assuming the authors can address the issues I have detailed in my review. In particular this paper has demonstrated, I believe for the first time, that DST methods such as RiGL may not match dense training when standalone, but can surpass the generalization of a dense solution as an ensemble - while still being (theoretically) more efficient to train than the dense model. This result will drive further research into DST methods, as well as encourage researchers to explore the practicality of DST training for making DNN research more computationally accessible.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "To my knowledge this paper provides the first extensive study on the ensemble of sparsely connected neural networks. Ensembles often improve performance, however they are costly. Sparse neural networks, on the other hand, can reduce the cost of each weak learner significantly. In addition to reducing cost at inference, authors also focus on reducing the cost of training, therefore they use end-to-end sparse training methods (dynamic sparse training). Results show that sparse neural networks can provide comparable (or better) ensemble performance while reducing the training and inference cost significantly. This work also provides some interesting insights into the ensemble of sparse neural networks.",
            "main_review": "Overall, I find the paper quite timely and exciting. It studies a new dimension where sparse neural networks can be used and has many strengths:\n- First large study on the ensemble of sparse neural networks. \n- Strong experimental evaluation and results which include Imagenet-r50 ensembles. Especially experiments in ResNet-50 shows significant gains over dense ensembles.\n- Useful for the community and has descent potential to inspire further research. \n\nHowever, the writing and the overall structure of the paper feels a bit rushed: there are many typos and some important baselines/papers are missing. Given the experimental nature of this work, I think it is important to further polish the paper both in terms of writing and experiments. I share my thoughts below:\n\n**1.Major Concerns/Limitations**\n\na) I think one important ensemble method is missing from the comparison. FGE [2] is possibly the most similar ensemble method to the proposed method. FGE uses cyclic learning rates to find different basins, similar to high q's used in EDST. Comparing these approaches is important I think for completeness. Specifically authors can compare (1) learning rate restart (2) DST training (3) Both LR restart and DST training for EDST.\n\nb) I think this paper has great potential to inspire future research on the topic, therefore I recommend adding pruning baseline to the results. Pruning costs more during training, but might provide better results. It would be nice to see whether that is the case.\n\nc) [3] studies ensembling of sparse NNs and the diversity of various sparse solutions (LT, static, DST). I think a discussion in Section 5.1 comparing results of both papers would be appropriate. Authors can also motivate why LTs are not good for ensembling using their results. \n\nd) Authors say that EDST requires one forward pass, but this is not clear to me; since each model would have a different connectivity and weights and thus would require different (but more efficient than normal) forward pass. The only difference between EDST and DST seems like the training. Do they have different inference procedure? If so, this needs to be clarified; otherwise I think EDST belongs to the same group as DST.\n\ne) Robustness metrics needs to be discussed in the text. I recommend authors to highlight the superior robustness of their approach. It looks like sparse neural networks provide better robustness. \n\nf) An analysis for the p/q used in sparse training would be nice. How does the diversity of the solutions are effected by q? \n\ng) An ablation on importance of grow criteria would be nice (random vs gradients; how much it matter?) An extension of this is to look at the parallel ensemble (i.e. growing different set of connections after removing q=80% of the existing weights); doing this with random would make sense and would tell whether it is better to follow a line, or branch from the same point. \n\nh) There are some things in Figure 4 and 5 that are not clear. In Figure 4, how many networks are ensembled? Given that smaller networks cost less training and inference FLOPs, it feels natural to use more of them for ensembling. Similarly these curves can be all presented on a single plot where x-axis is the inference/training flops and y axis is the ensemble accuracy. \n\n**Minor**\n- I am not sure it makes sense to introduce FreeTickets name as the individual methods are introduced later under different synonyms (DST and EDST). \n- I found the motivation in the intro a bit off. This paper proposes an efficient ensemble method, however the intro talks about different sparse training methods and their tradeoffs. I think authors can introduce these methods later in the text, while focus on the main problem this paper addresses (making ensemble more efficient?).\n- Line 6 and Line 19/20 does the same thing in Algorithm:1, right? It would be nice to use same notation. L20 is currently confusing.\n- Escaping current local minima -> escaping current `basin`.\n- Equation (2): I think a `union` operator would be more appropriate, \"+\" is confusing when you combine indices.\n- Baselines for the ensemble methods are mentioned in related work briefly. A longer description is needed. It would be nice to explain generic ensemble framework while doing this using the notation introduced (in Preliminaries). \n- In different instances authors mention \"with can produce in one shot many diverse yet accurate subnetworks\", however this is only valid for one of the methods proposed (EDST). I recommend rephrasing them accordingly.\n- \" Currently, the current only way for DST to match the performance of its dense counterpart on the popular benchmark, e.g., ResNet-50 on ImageNet, is to perform In-Time Over-Parameterization (Evci et al., 2020a; Liu et al., 2021b), \" -> do you mean rigl? [1] also achieves this I believe.\n- Another related work: Packnet [4]. \n\n**Writing**\nI think the writing of the paper can be improved significantly. Some of them (stopped at page3):\n- \"FreeTickets is defined as the ensemble of these sparse subnetworks\" -> \"Our method, FreeTickets\", uses the ensemble of these sparse networks. Overall, I think abstract can be improved. How about starting with the explanation of your method and then having something like: \"Since these diverse subnetworks are found through relatively cheap ways we call our method FreeTickets.\"\n- Recent works -> Recent work\n- over-extended training time -> longer training time\n- ensemble of models, - >ensemble method\n- FreeTickets is observed to demonstrate a -> FreeTickets improves over dense baseline in following criteria:\n- FreeTickets easily outperforms -> FreeTickets outperforms\n- \"researchers have investigated efforts to train sparse\" ->  researchers studied the training of sparse...\n- \"promising on training efficiency\" -> promising in terms of training.\n- \"g in general is a tough nut to crack\" -> remove i.e \"promising in terms of training efficiency, training a sparse network from scr\"\n- \"Currently, the current only way for\"\n- \"We introduce and conduct the concept\" -> not sure conducting the concept refers to\n- \"ds with can p\" -> \"which can\"\n- \"Our lightest method\" -> our light-weight method\n- \"PRELIMINARY AND SETUPS\" -> Preliminaries\n\n[1] https://arxiv.org/abs/2102.07655\n[2] https://arxiv.org/pdf/1802.10026.pdf\n[3] https://arxiv.org/abs/2010.03533\n[4] https://arxiv.org/abs/1711.05769\n\n## After Rebuttal\nI read your response and went over the changes made in the main text. I appreciate extra results added during the rebuttal:\n\nIt's nice to see sparse connectivity updates complements the learning rate restarts.\nDST ensemble matches pruning ensemble and reduces training costs, which is important to highlight.\nI believe this paper includes some important and through experimental work on ensemble of sparse networks. I increased my score to 6 (not 8 because it still needs some through polishing; which should be done before the camera-ready if accepted). Below I share some minor comments/suggestions.\n\n- Numbers when comparing random growth with gradient based growth looks suspiciously close. If random works as well as the gradient, it worth demonstrating this clearly, running ResNet-50 experiments might give better signals.\n- Algorithm in appendix use p (not q) for exploration.\n",
            "summary_of_the_review": "The work address an important problem \"Making Ensemble training and inference More Efficient\" using dynamic sparse training algorithms. Results demonstrate that sparse ensembles (1) reduce training and inference cost and (2) increase the robustness (and sometimes accuracy) over the dense models. Given that sparse models provide same accuracy using less parameters, (1) seems not surprising, however (2) is quite interesting and possibly novel. Authors introduce two algorithms for finding sparse networks however both algorithms seem to do the same thing at inference (contrary to how they are explained); this part and some other points need further clarification. Overall, I believe this work has great potential, however current version lacks few important baselines and written poorly. Therefore my initial rating is weak accept. ",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}