{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper presents a new computational framework, grounded on Forward-Backward SDEs theory, for the log-likelihood training of Schrödinger Bridge and provides theoretical connections to score-based generative models. The presentation of the results is not satisfactory (the algorithm should be clarified in several places and the notation is not accurate which raises doubts about the soundness of the method). The paper is thus very hard to read for the non-experts on the subject. Furthermore, some reviewers raise concerns about the similarity of this method to other algorithms that were never cited in the paper. Finally, the empirical analysis, as of now, is limited.\n\nIn the rebuttal the authors carefully addressed lots of the comments. However paper's presentation still needs to be substantially improved (de-densification of the paper would be extremely important since now the main narrative is very convoluted). The authors made several changes in the manuscript, but detailed discussion regarding training time complexity still seems to be missing (main body and the Appendix) in the new version of the manuscript, even though this was one of the main raised concerns. Overall, the manuscript requires major rewriting. Since the comments regarding the content were successfully addressed (the reviewers are satisfied with detailed answers given by the authors), the paper satisfies the conference bar and can be accepted."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "Inspired by recent work on score-based generative modeling, this paper proposes to solve Schrodinger bridges for generative modeling. Different from other Schrodinger bridge works, this paper connects the training to maximum likelihood, and provides a way to compute the log-likelihood of the model. The resulting method can shrink the gap between $p_T$ and the prior distribution, and produce high quality image samples and likelihood comparable to score-based generative models.",
            "main_review": "## Strengths\n1. The proposed method is motivated from optimal stochastic control in a principled way. It bridges the gap between $p_T$ and the prior distribution in existing score-based techniques.\n2. Compared to other works based on the Schrodinger bridge, the approach described in this paper seems to be more scalable, without the need to hand design part of the transformation as in Wang et al., 2021, or iterative proportional filtering as in De Bortoli et al. 2021.\n\n## Weaknesses\n1. The efficiency of the proposed method is unclear in current writing. In Theorem 4, the loss function is described as integrals of expectations. However, it is unclear with respect to which random variable is the expectation taken. How do you estimate the expectation in the loss function? If the expectation is over $x_t$, don't you need to simulate the forward SDE in equation (12a) for each datapoint? Since equation (12a) depends on $z_t$, which is parameterized by a deep neural network, there is no easy way to sample $x_t$ without solving the SDE numerically. By contrast, in score-based generative modeling, $x_t$ is sampled as a noise-perturbed Gaussian with a closed form.\n\n2. There are many errors in writing that affect reading and understanding. For example, in page 3, $p_t^{(2)}$ is never defined. In Lemma 2, I am not sure how to understand the expression $v(t, x) = y(t, x)$. As the solution to an SDE, shouldn't $y$ be a stochastic process? How can a stochastic process equal to $v(t, x)$, which is a deterministic function? What's the definition of $y(t, x)$? Similarly, in Theorem 3, how is $y_t \\equiv y(t, x_t)$, what's the definition of $y(t, x_t)$, and how is $y_t$ a function of $x_t$? In Corollary 5, why does an ODE contain the stochastic term $g d w_t$? In Algorithm 1, why are there references to (23a) and (24)? Do you mean other equations?\n\n3. Authors reported better performance compared to prior optimal transport methods like Wang et al., 2021. However, is this because the network architecture used in this paper (NCSN++) is better than those used in previous methods? Will it be more fair to compare with the same network architecture?\n\n4. Isn't the framework proposed in this work exactly the same as score-based generative modeling? If you let the forward SDE be $d x_t = (f + gz_t )dt + gdw_t$, and parameterize the score network as $z_t + \\hat{z}_t$, then the reverse SDE is the same as (7b). In this case, you can derive Theorem 4 and Corollary 5 automatically from the theory of score-based generative models.",
            "summary_of_the_review": "The paper proposes a useful Schroding bridge framework for generative modeling that is simpler than previous counterparts. However, there are concerns on efficiency issues, writing errors, and fairness in experimental comparison. It is also unclear how much difference is this method compared to the original formulation of score SDEs.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Inspired by stochastic control and forward-backward SDE theory, this paper proposed an iterative algorithm to approach the Schrodinger bridge (SB) problem, generalizing variational likelihood-based training of score-based generative models (SGM). The proposed method is then used to train both the generative process and the inference process of SGMs, recasting the generative modeling problem as the problem of SB. The author then performed experiments on standard image datasets. \n",
            "main_review": "This paper introduced a new way to look at the SB problem via a non-linear Feynman-Kac representation of a PDE that defines the SB optimal path measure. This overall framework seems to provide significant new insight on how SB can be solved practically, which is a key merit of the work, and I applaud the authors for the finding. \nThe strengths of the paper are:\n* Proposing an interesting framework to approach the SB problem, via solving the SB optimization problem using a PDE to define a pair of equivalent forward and backward SDEs whose path measures correspond to the SB solution, and then solving this SDE using a Feynman-Kac type of representation. \n* Making a connection with prior work, i.e. generalizing the likelihood-based training of score-based generative models. \n\nNovelty aside, I do find the paper requires some major improvement in terms of clarity and a clearer discussion and analysis of limitations and the similarity/difference with prior work. Below are the weaknesses of the paper. \n* Clarity needs work: the narrative at times is not very clear, which makes it harder to grasp what is being solved. (See more detailed questions below)\n* Differences with some prior work (such as iterative proportional fitting) are not not well explained, even though the proposed likelihood training of the forward SDE and the backward SDE seems very similar. \n* Limitations are not properly discussed: training the inference process is done at the cost of an increased cost of computation, which is arguably one of the most important features of SGM. Convergence property of the proposed method not discussed, i.e. whether or not the proposed likelihood training stage will lead to convergence towards the SB solution. A thorough discussion of these limitations and perhaps an experiment on compute cost analysis would be needed. \n\nSome more detailed comments and questions:\n* Throughout the introduction, it is not very clear what “computing the log-likelihood objective of Schrodinger Bridge” means. The narrative needs more work. More precisely, it wasn’t very clear to me what the “model” is until I finished reading section 3, and it took me multiple passes. For example theorem 4 refers to the log-likelihood of SB, it’s the log-likelihood of which model exactly? Fixing the inference SDE (i.e. $z_t$) and looking at the marginal likelihood induced by $\\hat{z}_t$? Is it a likelihood or a lower bound?  The (bold) $z$ used in the previous theorem seems to not depend on any parametric form (as it solves the SB problem), but here it suddenly becomes parametric. \n* End of 2.1, it is not clear why having a more flexible framework will help mitigate the instability problem just mentioned.\n* Is the order of the arguments of $h$ in (11) incorrect?\n* The presentation of Lemma 2 is a bit confusing, is it correct to say that despite the randomness induced by the Brownian motion used in the ito integral, the solution of y (an SDE) will still be a deterministic and smooth function, as it is after all a solution of the PDE? What’s confusing is that v is a solution of a PDE and y is a solution of an SDE, which is random by nature. \n* Presentation of theorem 3 also needs work: is the bold $z$ related to the regular $z$ used in Lemma 2? The notation hasn’t been introduced.\n* Last paragraph of page 5: what does the new interpretation of optimal control bring to us? Is there any practical benefit of it? \n* Last paragraph of 4.1, I am not sure how meaningful the comparison is with some other OT methods, especially since the SB problem is connected to “entropy-regularized” OT, and is not exactly OT.  \n\nMinor points / typos:\n* End of page 6, is it a maximization (of likelihood) instead of min?\n* The last sentence of page 7: can be founded -> found\n",
            "summary_of_the_review": "The paper introduces a new framework for likelihood-based training of the forward and backward SDEs that are inspired by the Schrodinger bridge problem, which is quite novel. But the paper is not very well written and the practical limitations of the proposed method are not sufficiently addressed. Therefore I do not vote for acceptance given its current form. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a framework for likelihood-based training of Schrödinger bridge-based generative models using the theory of Forward-Backward SDEs. In doing so, the paper draws relations to score-based generative models (SBGMs) and shows that the proposed framework is a generalization of the SBGM framework. The proposed framework also provides additional control to the forward SDE to reach the prior distribution unlike the case of SBGM. The authors propose a practical training algorithm which alternates between likelihood training of the forward and backward controls. Experiments on multiple image generation benchmarks demonstrate that the proposed SB-FBSDE performs well as a generative model of the data.",
            "main_review": "[Novelty and Significance]\n\nThe paper presents an elegant framework for likelihood training of Schrödinger bridge (SB)-based generative models. SB generative modeling is a developing area in the field of diffusion generative models and this paper makes a significant contribution to the area. It draws interesting connections to (and generalizes) the framework of previous diffusion-based generative models (e.g., SBGM). \n\n[Writing and Clarity]\n\nThe paper is written fairly well, albeit for the expert reader. There exist some clarity issues:\n\n- For Theorem 1, please provide a proof-sketch and/or cite the exact theorem number. In particular, it is unclear to me how to arrive at 7(a) and 7(b) from the Kolmogorov equations in (6). Is this after time-reversal?\n- For the proofs, please include a brief description of each step. In the current state, some steps of the proofs are unclear.\n    - In the proof of Theorem 3, why do you begin with (1) as the reference measure $\\mathbb{P}$ and not 7(a)?\n    - In several steps, $dt^2$ and $dtd\\mathbf{w}_t$ terms have been dropped. Please mention this in the proof text for clarity.\n    - In Eq. (20), I do not understand how you go from $\\nabla \\cdot(g \\hat{z}_t-f)$ to $-\\hat{z}_t^{\\top}(g \\nabla \\log p_t^{\\mathrm{SB}}) - \\nabla \\cdot f$.\n\nCorrections:\n\n- Eq. (15) is not an ODE and looks like a typographical mistake — the $g\\mathrm{d}\\mathbf{w}_t$ term should not be there.\n- \"... which can be _probability_ expensive on high-dimensional datasets\" — *prohibitively*.\n- Section A.1: ito —> Ito.\n- Section A.1 before Eq. (17). What is $b$? I believe it should be $f$.\n\n[Empirical Evaluation]\n\nThe empirical evaluation, although not extensive, is reasonable.\n\nOne thing that is unclear from Table 2 are the primary baselines; in my opinion, they should be Multi-stage SB and SGMs. It's good that the authors have reported results from several previous works; however, most of the results are not directly comparable, so it is important to specify what the main baselines are. A particular example is DOT which is not a generative model in itself but a sample improvement technique that operates on a pretrained GAN. \n\nThe authors are missing the following work in Table 2 in the optimal transport model class:\n\nAnsari, Abdul Fatir, Ming Liang Ang, and Harold Soh. \"Refining deep generative models via discriminator gradient flow.\" *arXiv preprint arXiv:2012.00780* (ICLR 2021).\n\nOn reproducibility: From the paper, it appears that the practical implementation has several moving parts and is not straightforward. The paper does not contain a reproducibility statement and also does not provide enough details for an expert reader to reproduce the results. I encourage the authors to release their code as supplementary material and include further details of the practical implementation in the Appendix for the benefit of the research community.\n\n[Questions]\n\n- Can you elaborate what exactly is meant by \"... SGM by enlarging the class of diffusion processes to accept *nonlinear* drifts ...\"?\n- How is $\\nabla\\log p^\\mathrm{SB}_t$ computed?\n- What is the form of the function $f$ in your practical framework/implementation?\n\n[Suggestions]\n\n- It may be better to move the paragraph on \"Connection to flow-based models\" before \"In practice, we parameterize the forward...\". The current structure breaks the flow of the reader.\n- To make the paper more accessible to readers unfamiliar with Schrödinger bridges, it would be helpful if you provide a brief review in the Appendix.",
            "summary_of_the_review": "The paper presents a novel framework for likelihood training of Schrödinger bridge (SB)-based generative models. The technical contribution of this paper is significant: it generalizes the SBGM framework and provides a practical algorithm for likelihood training of SB models. Although the paper has some clarity issues, I believe these can be fixed in the revision. Overall, I think this is a good paper and I recommend acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper the authors introduce a score-based generative model which relies\non a Schrodinger bridge formulation. More precisely the authors derive a loss\nfunction for the control of a forward/backward SDE. By alternating the\nminimization of the loss function between the forward control and the backward\ncontrol they obtain an approximation of the Schrodinger bridge. This loss\nfunction is obtained using the forward backward SDE (FBSDE) framework to obtain\na non-linear Feynman-Kac representation of the evolution of the\nlog-potentials. This theoretical/methodological contribution is accompanied by a\nexperiments in generative modeling on the MNIST/CelebA/CIFAR-10 datasets.",
            "main_review": "STRENGTHS:\n\n-Overall the presentation of the paper is good with a clear presentation of the\nscore-based generative models and the Schrodinger bridge problem. \n\n-The idea of using the FBSDE framework is new and original. I think that the\nidea of using Feynman-Kac based representations to solve (non)linear PDEs is\ndefinitely a good idea. \n\n-The toy experiments and the generative modeling experiments are satisfactory.\n\nWEAKNESSES:\n\n-My first concern is with the formulation of the loss function that I find\nopaque to say the least. Looking at (14) it turns out that the loss function is\nin fact given by\n$\\int_0^T \\mathbb{E}[\\| \\hat{z}_t - g \\nabla \\log p_t^{SB} + z_t \\|^2]$ where\nthe expectation is taken over the path measure induced by the forward SDE. It is\nnot clear how $\\nabla \\log p_t^{SB}$ is computed. The classical ideas of [1,2]\ncannot be used since the $\\nabla \\log p_t^{SB}$ is not given by a\nOrnstein-Ulhenbeck process. Therefore the authors might rely on the idea used in\nthat approximate $\\nabla \\log p_t$ for general processes using their Langevin\ndiscretization. The authors should clearly state what loss function they use in\npractice (discretization of the time + approximation of $\\nabla \\log p_t$). Maybe they use the first part of (14) to compute the loss function but this is not clear either. The\npseudocode provided by the authors is not helpful and there is no anonymous\nrepository to check out the code.\n\n-My second concern is a consequence of the first one. The authors claim that\ntheir work is different from [3,4] but in practice Algorithm 1 is the same\ntraining algorithm as the one derived in [3]. Indeed because the authors propose\nan alternate minimization of the loss function they derive the loss function\nthey minimize at each step corresponds exactly to compute the IPF step\n(Algorithm 1 in the current paper is almost the same as [3, Algorithm 1], see also [4, Algorithm 1], with the only difference being in how the loss is computed which is not clear from the current paper, see previous comment) . In this sense\nthe algorithm provided by the authors corresponds to a rewriting of [3,4]. Hence I\nthink the claims of the authors concerning the originality of the method is overstated and misleading. Similarly it is misleading to state that no\nconnection with Langevin based sampling as been established for Schrodinger\nbridge models, see [3,4]. To summarize there is a clear overlap between the\nmethod proposed by the authors and the work of [3,4] which is not acknowledged in\nthe paper. \n\n-Another concern I have is related to the derivation of the algorithm. Given how\nthe authors derive Algorithm 1 there is no guarantee that 1) the procedure is\ngoing to converge and 2) if it converges that the limiting bridge is indeed the\nSchrodinger bridge. Both these facts can be obtained using the IPF approach, see\n[3, Proposition 6]. As of now Theorem 4 is only valid at equilibrium, i.e. for\nthe optimal set of controls $(z_t, \\hat{z}_t)$. For arbitrary controls \nTheorem 4 might still be valid (or at least in a lower-bound form) but this is not\nwhat is currently stated in the paper. The way the paper is written it looks like Algorithm 1 is motivated and justified by Theorem 4 which is not the case. \n\n\n-I have a minor concern with the contribution of the authors to the FBSDE\ntheory. It seems that Lemma 2 and the current FBSDE is not enough to provide a\nnon-linear Feynman-Kac representation. I find that this contribution is not\nclearly presented by the authors. Also the proof seems to be very heuristic, the\nauthors do not check the necessary regularity conditions to apply Ito\nformulas. They refer to the ``same regularity conditions in Lemma 2'' but Lemma\n2 is not informative because it is only stated that the functions ``G, f, h and\n$\\varphi$ satisfy mild regularity conditions''. The concept of viscosity\nsolution is not reintroduced in the paper (even in the supplementary material).\n\n-Finally I have also concerns with the experiments. In particular there is no\ncomparison with [3,4] which are currently the concurrent for score-based\ngenerative modeling using Schrodinger bridge. In particular, since Algorithm 1\nand [3, Algorithm 1] appear almost identical it is unfortunate that the authors do not\nprovide any comparison with this approach in similar setting (same number of\nsteps, stepsize and without correcting step). \n\nCOMMENTS:\n\n-It would be interesting to observe what is the generative model obtained after\none iteration of the algorithm, i.e. when the backward network is updated for\nthe first time. This should recover the generative model obtained using SGM and\nfurther steps are a refinement of the method. \n\n-It would be interesting to precisely quantify the influence of the corrector\n(which seems to constitute the only difference between Algorithm 2 and the sampling\nprocedure proposed in [3]).\n\n-``Poof'' and ``Remakrs'' in the title of the sections in the appendix.\n\n-In Algorithm 1 do the authors really use a gradient descent algorithm? Or is a\nmore efficient like ADAM is used to train the neural network?\n",
            "summary_of_the_review": "To conclude I think that the idea of using FBSDE to derive the Schrodinger bridge\nis original. However the method has a lot of overlap with the\nworks of [3,4] which is not acknowledged by the authors. The authors do not\nprecise which loss function they use in practice and because they do not give\naccess to the code (or at least to a detailed pseudo code) it is hard to\ndetermine what is the original methodological contribution of the paper.\nI think that the paper is not mature enough and that a true justification of Algorithm 2 with FBSDE is still missing. Experimentally speaking the authors did not compare their work with existing SB methods.\nBased on these comments I recommend the rejection of the paper.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}