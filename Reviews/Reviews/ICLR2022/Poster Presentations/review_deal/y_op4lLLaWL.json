{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper analyzes the behavior of VAEs in modeling data lying on a low dimensional manifold. It formally proves some of the conjectures/informal-statements in an earlier work by Dai and Wipf (2019) in the case of linear VAE and linear manifold, and disproves the same for the nonlinear case. In particular, it proves, by analyzing the objective and its gradient-flow dynamics, that VAE captures the intrinsic dimension of data distribution correctly. For nonlinear cases, the paper shows a counterexample to the conjecture in (Dai & Wipf; 2019) where the support of VAE generators is a superset of that of data distribution. \n\nTwo of the reviewers had raised following specific concerns -- (i) Theory only considers linear encoders and linear/1-hidden layer nonlinear decoders, (ii) The convergence behavior during training is only provided for linear VAEs, (iii) Some statements in the introduction/abstract misrepresent the results in (Dai & Wipf; 2019). However the authors have adequately addressed (i) and (ii) in their response -- the paper shows that the correct manifold will only be recovered in the linear case; in the nonlinear case, even for simple manifolds (1-hidden layer) the correct manifold is not recovered as shown by the counterexamples. Authors have also promised to modify the statements in the abstract and introduction to address the concern in (iii). Other two reviewers are largely positive about the paper. The paper makes an important contribution to the VAE literature in further clarifying VAEs' behavior while modeling low dimensional manifolds, and will be a good addition to the conference program."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper builds on a recent theoretical work by Dai & Wipf [1]. Dai & Wipf analyze the *optimal* behavior of VAEs, when applied to manifold-valued data. This work analyzes the *training* behavior of VAEs, when applied to manifold-valued data. Both consider the non-trivial case where the manifold’s intrinsic dimension is lower than the ambient dimension. Dai & Wipf considers VAEs with arbitrarily complex encoders and decoders. The present work considers VAEs with linear encoders and linear/single-hidden layer nonlinear decoders.\n\n[1] Bin Dai and David Wipf. Diagnosing and enhancing vae models. arXiv preprint arXiv:1903.05789,\n2019.",
            "main_review": "I believe that a deeper insight into the training behavior of VAEs is a very important research area. The paper provides further insight into possible degenerate optima that can be obtained by VAEs, both linear and nonlinear, during training.\n\nHowever, I have several concerns regarding the accuracy of the paper. This is my current understanding, and I am fully willing to change my score if I have misunderstood anything.\n\n1. Though the authors heavily cite and build on Dai & Wipf's paper, they do not explore whether the two-stage VAE proposed by Dai & Wipf remedies any of the problems posed in this paper. Without a central theorem or a clear theme that ties together the provided theorems, the current work feels somewhat disjointed.\n\n2. I take issue with the way this paper positions itself with respect to Dai & Wipf. Namely, it misinterprets the theoretical results provided in the cited paper. \nIn the abstract, it is implied that Dai and Wipf’s result is on the convergence properties of the VAE during training, and that the present work expands on these convergence properties. See: \n“Recent work by Dai and Wipf (2019) suggests that on low-dimensional data, the generator will converge to a solution with 0 variance which is correctly supported on the ground truth manifold. In this paper, via a combination of theoretical and empirical results, we show that the story is more subtle. Precisely, we show that for linear encoders/decoders, the story is mostly true and VAE training does recover a generator with support equal to the ground truth manifold, but this is due to the implicit bias of gradient descent rather than merely the VAE loss itself.”\nHowever, it is my understanding that all of Dai & Wipf’s results do not consider any convergence behavior in training. Instead, they only analyze *optimal* behavior of the VAE. They do construct sequences of encoder/decoder functions that converge to these optima. But these sequences are unrelated to the training behavior of the VAE. Moreover, Dai & Wipf’s results only imply the generator only converges to a solution with 0 variance if the variance of the decoder goes to zero.\n\n3. The first bullet point in Lemma 1, a restatement of Theorems 4 and 5 in Dai & Wipf, does not seem correct. Translating the notation in Dai and Wipf to that of this paper, Theorem 4 in Dai & Wipf implies the existence of a sequence of $f_t, g_t, D_t, \\gamma_t$ such that $\\gamma_t \\rightarrow 0$ and $L(f_t, g_t, D_t, \\gamma_t) \\rightarrow -\\infty$. Lemma 1 appears to state the converse, i.e. all sequences $f_t, g_t, D_t, \\gamma_t$ s.t. $L(f_t, g_t, D_t, \\gamma_t) \\rightarrow -\\infty$ have that $\\gamma \\rightarrow 0$.\nMoreover, in both parts of Lemma 1, $f_t, g_t, D_t$ are allowed to be any sequence such that $\\mathcal{L} \\rightarrow -\\infty$, whereas in Dai & Wipf they are fixed w.r.t. $t$ and optimal w.r.t. the loss $\\mathcal{L}$.\n\n\nStyle:\n\nPage 6. \"There exists $\\tilde{A}_1, \\tilde{A}_2, \\tilde{B}$ s.t. for $\\tilde{\\epsilon}_t \\rightarrow 0$ there exists $\\tilde{D}_t$ s.t.\" This setup for Theorem 4 was difficult to parse\n\nPage 8. $(\\sigma(\\langle a^∗, \\tilde{x}_{:r} \\rangle) − \\tilde{x}_{r+1})^2$ ...\n\n\nshould $\\tilde{x}_{r+1}$ be instead $\\tilde{x}_{r+1:}$?\n",
            "summary_of_the_review": "While the paper provides insight in a very exciting area of theoretical research in machine learning, I currently do not recommend acceptance due to inaccuracies and the lack of a cohesive thesis.\n\nPros:\n+ Novel theoretical analysis of VAEs during training and at global optima\n+ Theoretical observations are corroborated by empirical results\n\nCons:\n- Theory only considers linear encoders and linear/1-hidden layer nonlinear decoders\n- The convergence behavior during training is only provided for linear VAEs\n- Work feels disjointed, writing could be clearer (e.g. Theorem 4 has very little discussion)\n\n**AFTER REBUTTAL**\n\nMy understanding of the work has been improved by the rebuttal. However, I do not feel that the stance taken in the manuscript adequately represents the stance taken in the rebuttal. I would be more willing to recommend acceptance if the misunderstandings were properly addressed in the manuscript---however, the manuscript, as it stands, has not been edited in such a way. (See my reply to the rebuttal.) I still stand by most of my points. Here is the updated summary:\n\nPros:\n+ Novel theoretical analysis of VAEs during training and at global optima\n+ Theoretical observations are corroborated by empirical results\n\nCons:\n- Theory only considers linear encoders and linear/1-hidden layer nonlinear decoders\n- Work feels disjointed, writing could be clearer (e.g. Theorem 4 has very little discussion)",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors study further on the conjectures of Dai & Wipf (2019). For the linear case, they provide the proof that the conjecture is true. For the non-linear case, the paper disagrees with the conjecture, and they argue that the VAE training frequently learns a higher-dimensional manifold which is a superset of the ground truth manifold.  \n",
            "main_review": "Strength\n- The work tackles previous work (Dai & Wipf, 2019), which is very interesting.\n- Theorems and proofs are formally provided.\n\nWeakness\n- The problem statement or the motivation is not properly provided.\n- The paper seems to be revised, and written in formal way.\n- Linear VAE is barely used. Instead of linear VAE discussion, it would be better to discuss more on the non-linear VAE case, which is more widely used.\n- No analysis in benchmark or real-world dataset. The authors only deal with toy datasets. ",
            "summary_of_the_review": "I'm negative on this paper. The paper needs to be revised in formal way. The motivation should be clearly introduced (rather than citing the paper), and the conclusion should be also provided. The paper seems not ready to be published, however, the work is interesting and I'm looking forward to have the paper (with additional experiment on benchmark or real-world) in the revised version.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper examines the conjecture given in (Dai & Wipf; 2019) on the support of distribution that VAE learns. \nThe contributions are listed as follows: \n- For the linear case where the data is Gaussian with rank-degenerate covariance, and encoder/decoder are both linear, this paper proves that VAE captures the intrinsic dimension of data distribution correctly by analyzing the objective and its gradient-flow dynamics. \n- For nonlinear cases, the paper shows a counterexample to the conjecture in (Dai & Wipf; 2019) where the support of VAE generators is a superset of that of data distribution. \n- Numerical experiments are presented to support the theory. ",
            "main_review": "The strengths of this paper are: \n- Combination of loss landscape analysis with gradient flow analysis sounds sensible and validates the claim by (Dai & Wipf; 2019) for the linear case. \n- The sigmoid dataset example is simple enough to see VAE may overestimate the intrinsic dimension. \n\nThe weaknesses are: \n1) the organization of the paper could be improved, and \n2) the paper lacks any tips for practitioners. \n\n1: Section 3 explains the general ideas while Sections 4 and 5 give rigorous discussions about the loss and optimization dynamics, respectively.  \nI noticed this structure only after reading through the three sections, which was confusing to me. \nExplaining the structure of the discussion at an appropriate place (at the end of Introduction or at the beginning of Section 3) may improve the clarity. \n\n2: Most practitioners use nonlinear VAEs, but the paper provides a negative result in that the VAE objective can induce a larger intrinsic dimension and larger support of data distribution. \nIs there any wisdom or tip from the theory discussed in the paper for practitioners? \nSuch information will strengthen the importance of the paper. \nFor example, the impact of the choice of latent dimensionality $r$ or a way to choose $r$ can help VAE users. \n\nQuestion about gradient flow analysis: can we extend to stochastic gradient setup? \nIn the linear case, the expectation over latent variable $z$ in the VAE objective is analytic. \nOn the other hand, we need a Monte-Carlo approximation for typical nonlinear cases using the reparameterization trick, etc. ",
            "summary_of_the_review": "The paper gives a sensible discussion, but a lack of practical aspect limits the broader importance. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work revisits the population loss analysis for VAE ELBO (Dai and Wipf, 2019) and note that undesirable \"asymptotic global optimas\" exist, where the support of the model distribution $p_{model}(dx)$ has higher dimensions than the true data manifold.  Additionally, it shows that, for linear VAE, such optimas are excluded due to the implicit bias of gradient descent, but empirically nonlinear VAEs often stuck in such optimas.",
            "main_review": "### Original Review\n\nThe results and informal discussions in Dai and Wipf (2019) can be misinterpreted in a few ways, and clarifications are valuable to the general ML audience.\n\nThat being said, most results here are arguably unsurprising, and largely stem from an unfortunate definition of \"asymptotic optimality\": it includes any (trajectory of) parameters which have loss tending to infinity, without accounting for the rate of divergence.  It is clear from e.g. Dai and Wipf (2019, p. 7) that any (suitable) VAE models parameterizing a $\\hat{r}<d$ manifold will have its loss tend to infinity, as its leading term is $-\\frac{d-\\hat{r}}{2}\\log \\gamma$, where the decoder variance $\\gamma$ is assumed to converges to zero.  It is also clear from the same source that if we lower bound $\\gamma$ by a *small positive constant* $\\gamma_0$, then (among the parameter sets considered in Dai and Wipf (2019) and this work) the global optima should have as small a dimension as possible, because the leading term of the loss will be $-\\frac{d-\\hat{r}}{2}\\log \\gamma_0$.  Dai and Wipf (2019) stated that their practical recommendations start from this form of the leading term of loss (last sentence of Section 3).  The result is relevant because in practice the optimal decoder variance will be bounded by the reconstruction error of the AE (Dai et al., 2020), although the theoretical picture is far from clear.\n\nThere are nuances around the informal discussions in Dai and Wipf (2019): one of the most important limitations is their restriction to a particular set of parameters (decoder variance is either 1 or near zero along each axis).  They also ignore errors from estimation, approximation and optimization.  However, the fact that there exists different parameter (trajectories) leading to model distributions with \nsupport having different dimensions does not appear to be one of them.\n\nThe interesting part is the analysis of the implicit regularization effect from optimization, and the nonlinear simulation here reveals one surprising limitation of previous work that may have real consequences.  It would greatly enhance the paper if the authors experiment with simple fixes for this issue: e.g. clipping the decoder variance from below.  Regardless of whether the fixes work, the results will provide more insight as well as possible guidance for practitioners.\n\nMinor comments:\n\n1. In Theorem 4, $\\tilde z$ is not defined; immediately below Theorem 5, it will be helpful to clarify the rotation is applied to both the model parameters and ground truth.\n\n2. You may want to cite the following two papers, which improve the understanding of ELBO landscape over Dai and Wipf (2019):\n    1. Kumar and Poole (2020), On Implicit Regularization in β-VAEs, ICML 2020.\n    2. Tang and Yang (2021), On Empirical Bayes Variational Autoencoder: An Excess Risk Bound, COLT 2021.\n\n### Post-rebuttal Update\n\nMy original concern is that the main results here appear irrelevant in light of the informal discussions in Dai and Wipf.  While I'm not sure if these discussions are not obvious, they do rely on strong assumptions and do not account for the optimization process.  More importantly, the findings in the new experiments provide contrary evidence to the discussions in Dai-Wipf.  This is interesting and demonstrates the relevance of the results in this work.  For this reason, I change my score to acceptance.",
            "summary_of_the_review": "(edited)\n\nStrengths\n\n+ The cautions against misinterpretation will be useful to the broad ML community.\n+ Findings about the implicit bias are interesting.\n\nLimitations\n\n+ Further efforts are needed to understand the discrepancy between the results here, which do not provide rate of divergence, and the informal calculations in Dai and Wipf, which implicitly places assumptions that do not appear consistently supported by experiments.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}