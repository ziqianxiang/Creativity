{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This is a borderline paper which elicited much discussion.\nThe paper proposes to extract features from pre-trained networks through Kernel functions. It develops the idea of Fisher kernels for Neural networks calling it NFK. The methodology applies to both supervised and un-supervised setting. \nThe paper shows that proposed kernel has low rank structure and serves as the basis for developing an algorithm for computing the kernel on large datasets. The idea of extending Fisher kernels, their efficient computation, and investigating their usage in both Supervised and Unsupervised are some of the key strengths of the paper.\nThe reviewers though appreciative suggested (1) several new experiments, (2) inclusion of more background work related to Power method \nand (3)  have more technical discussion clarifying the contributions related to background. \nThe author(s) during rebuttal tried to incorporate most of the suggestions in the revised draft. \n\nSince there was consensus on the novelty, the detailed discussions, and the results of the additional experiments, one could potentially accept this paper if there is space. The results will be interesting will be those who investigate the interplay of kernel methods and Deep Networks."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper investigates the representation of modern neural networks from the perspective of kernels by extracting features from pre-trained network models. The authors show the effectiveness of the proposed neural fisher kernel (NFK) for both unsupervised and supervised learning tasks. A low-rank approximation strategy of NFK is adopted to reduce computational burdens.",
            "main_review": "This paper is interesting and technically sound, its novelty comes from two perspectives. First, it considers to represent/modify neural networks from the view of kernels. The proposed NFK serves as a unified kernel for both supervised and unsupervised learning models. Besides, the authors take advantage of the low-rank structure of NFK to avoid directly handling unmanageably high dimensional vectors.\n\n1. The effectiveness of the proposed NFK is mainly evaluated based on CIFAR-10 dataset, how is the performance of the proposed method on other datasets?\n\n2. The authors mention some representative unsupervised learning tasks such as denoising. Have the authors tried to combine the proposed NFK with the simple CNN for image denoising/super-resolution/enhancement?\n\n3. Some figures could be used to help to better explain the framework of the proposed method and also its effectiveness.\n\n4. Some terms such as VAEs and GANs should be defined,  for readers without too much background knowledge may be confused.",
            "summary_of_the_review": "The paper is technically interesting, but the authors should carefully revise their experimental section to address the concerns.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No Ethics Concerns.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a new strategy for extracting compact and intuitive representations of the data from a neural network. The approach exploits the feature map associated with the kernel representation of a neural network. In particular, the authors consider the Neural Fisher\nKernel and propose a series of linear approximations to make their approach scalable.\n",
            "main_review": "strengths:\nThe general idea of using the linear space associated with the kernel feature map for extracting data representation is interesting. Simplifying the representation through low-rank approximation of the kernel matrix is an interesting technique and may have a good impact on representation learning. Using the power method for computing the low-rank approximation through automatic differentiation could have applications that go beyond the proposed approach.\n\nweaknesses:\nThe authors should specify better what is new and what has been already done. For example, it looks like using Fisher vectors in representation learning and approximating kernels through truncated SVD are not new ideas. Also, the authors could have spent some more words to outline the main differences and advantages of using the Fisher kernel instead of the standard Tangent kernel. It is not clear if the main contribution of the paper is the extension to unsupervised learning or the proposed low-rank approximation.   \n\nquestions:\n- Is the approximation of the fisher tangent kernel the key novelty of the paper? If so, what is the technical challenge compared with approximating other kernels?\n- I like the idea of using automatically computed Jacobians in the power method. Is this a new idea? Can this be applied to the standard tangent kernel?\n- The experiments do not compare the proposed approach with non-fisher similar methods. Would it be possible to run a quantitative comparison with the tangent kernel (at least for the supervised case)? \n- Are there computational gains associated with using the Fisher kernel for its low-rank approximations to train a model or is the proposed method only suitable for obtaining more intuitive representations?\n- Is Montecarlo Sampling needed in the proposed method? And what are the computational cost and feasibility limits of that power iteration approach?\n- In the third line of the last paragraph of p2, Ux I Ux should be Ux I Uz, right?\n",
            "summary_of_the_review": "An interesting idea but the authors should specify better what is new and what comes from existing work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper introduces a new kernel, the neural Fisher kernel (NFK), to learn data representations for generative models in both unsupervised and supervised settings. NFK extends the Fisher kernel to neural networks, and a low-rank approximation of NFK is used to scale to large datasets. ",
            "main_review": "The paper proposes a formulation for the NFK for unsupervised (GANs and VAEs) and supervised neural networks. The kernels are defined as inner products of the Fisher vectors (derived from the Fisher score) computed from pre-trained neural networks. \n\nStrength:\n\n•\tNice idea to see extensions of kernels, here the Fisher kernel, to neural networks.\n\n•\tSimilar formulations for supervised and unsupervised settings.\n\nWeaknesses:\n\n•\tMore motivation and derivations of the different Fisher scores $U_x$ (for GANs, VAEs, supervised) would be beneficial for understanding better the paper. \n\n•\tMore discussions on the non-diagonal version of the Fisher information matrix models (see comments below) would be beneficial.\n\n•\tDiscussion on the dependence of the quality of the NFK embedding on the quality of the pre-trained neural network.\n\n•\tDiscussion on the choice of the low-dimensional embedding dimensionality $k$\n\nGeneral comments:\n\n•\tUsing the diagonal of the Fisher information matrix (FIM) seems desirable from a computational reason, however a natural question is what happens if one tries to use the full matrix. Given the size of the parameters $\\theta$ in a neural network, estimating the whole matrix would indeed be extremely computationally expensive, but by discarding them, one loses significant information. Could the authors comment on that? Is the diag of FIM related to other known concepts in statistics? Does using only the diagonal imply that the off-diagonal elements are zero meaning the parameters are orthogonal? How does this affect the results and interpretation?\n\n•\tCould the authors give the derivation of $U_x$ in eq (1) for GANs (I see part of the derivation is in the paper by Zhai et al, 2019)? For VAEs and supervised case, FIM $\\mathcal{I}$ is the inner product using $U_x$, but for GANs it acts in the output space of the generator. Why is this the case (some explanations are given in the original paper but would be helpful to discuss this a bit more here)? The derivation of eq (4) would also be useful.\n\n•\tLow-rank structure of NFK and Alg 1: How does one choose the feature dimensionality $k$? Many methods that rely on kernels and manifold learning make the assumption of low-dimensionality/low-rankness and show that a small number of eigenfunctions is sufficient to reconstruct the input data. How is this different for NFK? The way I understand “low rank” is that the data has its own rank, which is low, and could potentially be learned. However here the authors input the dimensionality/rank $k$ which might be or not close to the true rank in real applications.\n\n•\tHow does this work relate to the work of Belkin et al (2018) – “To understand deep learning we need to understand kernel learning”, where the authors look at other kernels (Laplacian and Gaussian)?\n\n•\tCould the approach be used for neural networks that are not pre-trained, as the neural tangent kernel NTK?\n\n•\tThe experimental results are nice, however the focus on computation is not so relevant given that it only uses the diagonal of the Fisher information matrix. Comparisons using the whole matrix would also be needed. What error is used in Table 3 (MSE, MAE, RMSE)? The goal of the paper is to present a method for supervised and unsupervised settings, however in the results an example on semi-supervised is also presented. I wonder if the examples on semi-supervised and knowledge distillation could leave room to improve the supervised and unsupervised settings discussions, and potentially be moved to the Appendix?\n\nOther comments:\n\n•\tPlease update reference (Jaakkola et al): year, conference, also there should be no “et al” there are only two authors\n\n•\tDoesn’t, don’t, won’t, it’s , etc -> does not, do not, would not, it is\n\n•\tBoth the concepts of “data representation” and “feature representation” are used. Do they always refer to the same thing? If yes, would be good to specify that.\n\n•\tExpression of $K_{fisher}$ => second $U$ should be subscript $z$ not $x$? \n\n•\t“FIM defined as the variance of the score …” -> the FIM matrix is defined between all pairs of parameters $\\theta_i$ and $\\theta_j$, so it should be a covariance?\n\n•\tAppendix Fig 3: Not sure I fully understand this example. Could one try the reconstruction of the digits using a simple method, such as PCA using the first 100 principal components as a baseline?\n\n•\tNot familiar with the “Fisher vector” terminology, except in image classification and the “Adversarial Fisher vector” from Zhai et al, 2019. Are there other references?\n",
            "summary_of_the_review": "The paper presents interesting ideas of extending kernels, here the Fisher kernel, to neural networks. I find that the paper would need to make a stronger connection with existing literature (eg, Belkin et al, 2018) and give more motivation/discussion/intuition on the derivation of the different $U_x$s. My grading suggestion is a 6, as I believe the paper presents nice ideas but would require more work.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}