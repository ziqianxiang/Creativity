{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper proposes a plug-and-play method for solving imaging problems. Plug-and-play methods use a denoiser to solve linear inverse problems. The paper proposes a plug-and-play method and uses convex optimization tools from analyzing proximal gradient methods to provide convergence guarantees. The algorithm is applied to a variety of inverse problems showing that the method works well. \n\nAfter the discussion period, all four reviewers recommend acceptance. \n- Reviewer QQES provided a detailed review and raised a few concerns including a clear motivation for and description of the denoiser, and unsupported claims, in particular related to a proof in the paper. The authors revised the paper and responded in length to the claims, in particular they detailed steps and assumptions related to the theorem in their response. As a response, the reviewer changed their score to accept.\n- Reviewer xYLt strongly supports acceptance based on the strong theoretical results and a very good exposition.\n- Reviewer GZzY likes the overall idea of the paper and raised a few minor concerns and questions, which were addressed by the authors. \n- Finally, reviewer E8QG also appreciates the method, convergence analysis, and extensive validation. The reviewer also raised a few minor concerns and asked for clarification, and the response of the authors resolved those concerns. \n\nBased on my own reading and based on the reviews, I recommend acceptance. The paper provides a variant of a plug-and-play method, proves interesting convergence results for the method, and has a strong experimental evaluation of the method. I encourage the authors to take the feedback of the reviewers into account, which they have done for the most part already, and it would also be interesting to see the performance of the method for compressive sensing problems."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposed a new denoiser, based on the gradient of a proposed trained regularizer, to be employed within a plug-and-play (PnP) framework, which is claimed to have a convergence proof compared to other existing methods (which claimed to use \"unrealistic\" assumptions). Experiments are provided to support the efficacy of the proposed method.",
            "main_review": "\nThis work is well-written and easy to follow for the most parts. \nIt discusses a new denoiser to be employed within a plug-and-play framework, which indeed is a hot topic and an interest of the community. However, the current version of the work is not ready for publication due to following reasons.\n\n\n\n\n**Primary issues:\n\n1 – the proposed regularizer in (5) is obscure and no interpretation around itself is provided. What does it impose on the recovery?  Besides, $N_{\\sigma}$ which seems to play the most important role in (5) is introduced only as “parametrized by a neural network”, and later it is restricted to be a differentiable neural net or “net with differentiable activation functions”; it is unclear what $N_{\\sigma}$ does, and how it is related to (8) and Remark 1. \n\n\n\n2 – The GS-PnP (9) is deviated from being a PnP-HQS while using (4) as the employed denoiser. The author(s) claimed it (“Departing from a slight modification of the PnP-HQS framework….”); however, the right-hand side of (9) seems like a heuristic approach which may lead to a better numerical result by adjusting parameters like $\\lambda$ and $\\tau$ which are not introduced in the denoiser (4).\n\nThe HQS for (1) is either $Prox_{\\tau f} \\circ Prox_{\\lambda \\tau g}$  or  $ Prox_{\\lambda \\tau g} \\circ Prox_{\\tau f} $, where PnP-HQS replaces  \\Prox_{\\lambda \\tau g} with a denoiser $D$. The strength of the filter $D$ is then either determined by $\\lambda$ and $\\tau$ (and the sparsifying transform used within $D$) consistent with the analysis of proximal operator, or one can use a heuristic approach to set the filtering strength (or $\\sigma$) through cross-validation. The results by the right-hand side of (9) might seem appealing (marginally) due to tweaking of the parameters $\\lambda$, $\\tau$ and $\\sigma$. \n\n\n3 – it is not clear whether the proposed GS denoiser (9) is limited to be employed within the HQS (half-quadratic splitting) method or can be used within other recovery frameworks such as ADMM and FBS. For instance, RED is independent of the recovery framework; is the GS (9) the same?\n\n\n\n\n\n4 – the Proof of Theorem 1 (i): First of all it is not clear how (16) is derived, by which the rest are built upon. The (16) can be valid only when “$f(x)$” satisfies restricted strong convexity. While the authors claimed “this new PnP algorithm allows for non strongly convex data terms”.\n“bounded and/or nonexpansive  denoiser”+ “decreasing step-size”+ “Lipschitz constraint”, “These settings are unrealistic as deep denoisers do not generally satisfy such properties.”\n“This comes at the cost of imposing strong convexity on the data term f, which excludes many IR tasks like deblurring, super-resolution and inpainting”\n“One strength of this approach is to simultaneously allow for a non strongly convex (and not smooth) data term …. .”\nNothing has been proved for the cases where “$f(x)$” is not strongly convex; however, the proof of theorem 1 is based on the strong convexity of “$f(x)$” and .\n\n\n\n5– “we plug a denoiser that inherently satisfies equation 3 without sacrificing the denoising performance” + “But imposing hard Lipschitz constraints on the network alters its denoising performance.”\nThe main objection on these claims is that how the performance of the denoising is inspected?\nIndeed, an stand alone denoising task is different from a general image restoration problem (e.g., deblurring, super-resolution, compressive sensing and inpainting) as the underlying degradation within each problem is different. Therefore, using the same denoiser for different image restoration problem is problematic, and expecting that a denoiser tuned for stand-alone AWGN denoising task works well for any IR task is unrealistic. \n\n\n6 – it is not clear how the tuning is performed for the other methods used for the comparison. For instance in Table 2, how are the results obtained by other comparative methods (e.g., IRCNN and DPIR)? Did you use the same set of parameters tuned by their authors for CBSD64 over CBSD10?\n“Due to large computational time of some compared methods ...” please ignore those slow methods (which I think is EPLL providing the least of results – or even RED in deblurring) and obtain the results for the whole CBSD64 for the sake of fair comparison and avoiding possible overfitting. This applies to other experiments too.\n\n**Secondary issues:\n1 – (2) is incorrect! It should be $Prox_{f}(z) = argmin_{x} \\{ \\frac{1}{2} ||x-z||_2^2 + f(x) \\}$ … \n\n\n2 – “Besides, this new PnP algorithm allows for non strongly convex data term, and thus can address ill-posed IR tasks ….” is indistinct and vague!! By “data term” do you mean “data-fidelity term”? If so, how and where non-convexity (or non-strongly convex!) data-fidelity is investigated? \n“Besides, this new PnP allows….” Does it mean other PnPs do not allow such capability but the proposed one does?\n\n“This comes at the cost of imposing strong convexity on the data term f, which excludes many IR tasks like ….” the same issue stated above!\n\n3 – “Though providing excellent restorations, such schemes are not guaranteed to converge for all kinds of denoiser or IR tasks.” is this claim correct? How is it related to your work? Does it mean the proposed work is able to work with any denoiser and for any IR tasks (e.g., compressive sensing)?\n\n\n\n4 – “contrary to Romano et al (2017), our denoiser exactly represents a conservative vector field.” any proof? Does it have a symmetric Jacobian?\n\n5 – “inpainting noise-free input images lead to a non differentiable data term $f$”! why?\n\n6 – what are the number of parameters in Table 1?\n\n\n**Minor issues/modifications/suggestions:\n\n1 – perhaps the regularization parameter in (1) should be positive, i.e. $\\lambda>0$\n\n2–  equation 4, equation (22), (6) … please use a consistent way of citing equations, e.g., use within parentheses.\n\n3– The forward model in page can be formulated in a separate equation due to its frequent use throughout the paper.\n\n4– HQS (half-quadratic splitting) is used but is not cited, please see https://doi.org/10.1109/83.392335\n\n5–  “external denoiser” is vague! Perhaps “generic denoiser” is a better alternative.\n\n6– “exact” proximal mapping is vague! Perhaps “explicit” proximal mapping.\n\n7– “subsequent integration in PnP schemes” + “subsequent PnP image restoration” are vague/incomprehensible!\n\n8– the first paragraph of page 3 is vague! It should be rewritten.\n\n “One can relate the deep denoiser” → “a deep”\n\n “The latter shows that the Minimum ….” what is the former?\n\n What are “$p$” and “$*$”? they are not introduced. Do they correspond respectively to density and convolution?\n\n “intractable” is vague!\n\n9– “equation 4 comes down to including”\n\n10– “Algorithm (9)” typo. Either (9) or Algorithm 1. \n\n11– “is non-increasing and converging” → converging toward?\n\n12– It would be better if only the average results are represented in Table 2, instead of so many numbers.\n\n13– “Both IRCNN and DPIR realize PnP-HQS”\n\n14– “i-e” typo. → “i.e.”",
            "summary_of_the_review": "Post-rebuttal:\n\nI thank the authors for their careful responses to the reviewers’ concerns, comments and questions. Their response addresses most of my concerns. I highly recommend to include the responses to the major issues in the paper to rectify the ambiguities, and re-write Section 3 accordingly. I would like to change my evaluation based on the authors' response, to accept.\n\n\n-------------------------------------------------------------\n\nIn a nutshell, the submitted manuscript suffers from the following issues:\n1) lacking a clear motivation/description on the proposed denoiser (4) and (5),\n2) ambiguity of the proposed PnP method (9),\n3) unsupported claims and lacking a clear proof for Theorem 1 by which the contribution of paper is built upon, \n4) insufficient experiments,\nwhich overall could not convinced me to accept the paper, but to evaluate it as “reject”.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper presents a new form of the plug-and-play (PnP) half-quadratic splitting algorithm with provable convergence. \nIn contrast to directly formulating the denoiser as previous works, they propose to parameterize the denoiser with a learnable score-based function. This builds a bridge between the recently emerged score-based generative model and plug-and-play methods. And even more surprisingly, they show such a new formulation within the PnP framework leads to a very strong theoretical convergence guarantee under mild realistic assumptions. They also demonstrate good empirical results on three image restoration tasks i.e., deblurring, super-resolution and inpainting, verifying the empirical convergence rate is usually faster than $\\mathcal{O}(\\frac{1}{k})$ -- the worst-case rate established theoretically. ",
            "main_review": "## Pros.\nI enjoy reading this paper -- the writing is so great, the theory proved is so strong and general, the empirical simulation is also extensive and convincing, making this paper one of the best papers I have ever reviewed. \n\nThe proposed gradient step denoiser formulation in the PnP framework is particularly interesting. By such a modification, they can reach a very general theoretical convergence result without relying on assumptions of strongly convex data terms $f$ and non-expansiveness on $g$ --- the best result obtained from prior research. Moreover, they claim the established theorem can even be extended to non-convex $f$ --- this is also super exciting. \n\n## Cons.\nI have some questions that need clarification: \n- The main results are obtained based upon HQS algorithm. I'm wondering whether the theoretical results are still validated on other proximal algorithms, e.g., proximal gradient descent, ADMM, etc. \n- I think in Remark 1, the gradient of $g_\\sigma^*$ should be equivalent to the negative log gradient of the smoothed image prior $p_\\sigma$, according to Eq.(3) in [R1]. \n- The gradient step formulation of denoiser requires the calculation of Jacobian (Eq. (6)), which makes the training (in Eq. (8)) a bit cumbersome. Could the authors provide more details on how they compute the Jacobian? How to do that with the PyTorch automatic\ndifferentiation tools? \n- Recent advances on PnP methods, e.g., [R2] should be mentioned in this paper.\n\n[R1] Solving Linear Inverse Problems Using the Prior Implicit in a Denoiser, arxiv 2021  \n[R2] Tuning-free Plug-and-Play Proximal Algorithm for Inverse Imaging Problems, ICML 2020\n",
            "summary_of_the_review": "From my point of view, this is a truly innovative work with significance on both theoretical and practical sides. Since PnP is a very important tool in image reconstruction domains, this paper is thus worth being highlighted and would inspire further works in future. \n\nDisclaimer: I don't check the full details of mathematical derivations. The justification here is therefore contingent upon the prior assumption: all the Theorem/Remarks in this paper are rigorously established. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper considers a novel and interesting idea: designing a deep neural network denoiser that makes Plug-and-play priors (PnP) convergence analysis clean and simple, motivated by PnP-HQS [Zhang et al., 2017b] and regularization by denoising (RED) [Romano et al., 2017]. Existing works have proved the convergence of PnP and RED with contractive and nonexpansiven denoisers. It is an activate area for designing deep neural net denoisers that ensure PnP/RED convergence. Specifically, this paper focuses on designing denoisers that correspond to the gradient descent step in terms of the $L$-smooth regularizer function $g_\\sigma (x)$. By using such denoising step within PnP followed by the proximal of data fidelity $f$, one can guarantee convergence via traditional non-convex optimization given the objective function $F(x)$.  Since the handy $F(x)$ is available, a standard backtracking scheme is used in this work to ensure convergence without needs for tracking the exact Lipschitz constant of $\\nabla g_\\sigma$. Finally, the performance and stability of the proposed method is evaluated over three image inverse problems such as deblurring, super-resolution and inpainting, with satisfactory results compared to existing methods based on PnP and RED. ",
            "main_review": "I think the overall idea of this paper is great: designing a differentiable neural network, by construction, that equals the gradient descent step of an explicit regularizer function, the fact which provides new insights for solving an important open problem about the convergence of PnP and RED.  The idea sounds simple but promising and makes it possible to be directly applied for other variant PnP/RED algorithms. My main concerns / remark with this paper are as follows:\n\n1). It is unclear for me why RED led to poor results for both deblurring and super-resolution. This may be due to the improper parameter tunning such as regularization trade-off, step-size and denoising intensity of the denoisers. Since the fixed points of PnP/RED algorithms are directly affected by the step size used in those algorithms [Ahmad et al., 2020, p.108], to show the true performance of DPIR and RED, the step size must be fine-tuned against PSNR. This can be done directly on the testing images. Additionally, the neural network architecture of denoisers used within RED need to be clarified.\n\n2). To no loss of completeness, an additional interesting path to follow for this proposed method is to have a connection between convex regularization and CNNs, which would help to facilitate the optimization process and allows to derive recovery methods with global convergence. Hence, a brief discussion for providing insights of building convex $g_\\sigma(x)$ through deep neural networks $N_\\sigma$ is highly recommended.\n\n3). Since the setups for $g_\\sigma$ is not necessarily convex, resulting a non-convex problem. An initial point of the update sometimes is important. It would be interesting to see how different initialization influence the final reconstruction results.\n\n4). In order to have a better understanding about how the regularization parameter $\\lambda$ explicitly adjust the balance between the data-fit and the unconstrained deep neural network regularizer, it is useful to include a figure showing the evolution of images reconstructed by PnP-GS for different $\\lambda$.\n\n5). It is helpful to include the proposed GS-DRUNet and DRUNet light denoiser architecture in the supplementary materials to make this work easy to follow.\n\n6). Having a convergent plot against PSNR (dB) would help to see the speed of improvement in imaging quality. \n\nMinor comments:\n\n1). Table (5) in the supplement, “31.70” is displayed in bold while “31.93”is not. \n\n2). Some missing citations related to PnP:\n\n[1] Buzzard et al., 2018, Plug-and-Play Unplugged: Optimization Free Reconstruction using Consensus Equilibrium.\n\n[2] Ahmad et al., 2020, Plug-and-play methods for magnetic resonance imaging: Using denoisers for image recovery.\n",
            "summary_of_the_review": "This paper is proposing an interesting alternative to training contractive/nonexpansiven denoiers for PnP and RED with convergence guarantee. Although I would expect a little bit more technical depth at least via numerical simulations, I believe this is a well-written paper with some new results that could spark further research in the important topic of designing explicit regularizers for PnP/RED. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper makes an extension of the plug-and-play framework by formulating an explicit regularizer $g(x)=||x - N_\\sigma(x)||_2^2$ whose gradient $\\nabla g$ corresponds to the noise residual $x-D_\\sigma(x)$. By replacing the proximal of regularizer with this gradient step denoiser, the authors proposed the GS-PnP algorithm based on the half quadratic splitting algorithm. Since the explicit regularizer is known, a convergence analysis is hence established by assuming the Lipschitz continuity of $\\nabla g$.\n\nThis paper is closely related to the recent trend of learning a regularizer functional by using deep neural networks. The difference between the existing literature and this paper is that the former trains a deep neural network to directly output a scalar value, while the latter trains a neural network to output an image-size vector and then envelopes it with a $\\ell_2$-norm. In fact, the proposed regularizer shares the same formulation as the one stated in Sec 5.2 *\"An Alternative Prior\"* in Romano's RED paper. However, the difference between the two is not clearly stated in the paper.\n\n**Strength**\n1. An extension of the PnP with explicit regularizer formulated.\n2. (Non-convex) convergence analysis under the common assumptions of the Lipschitz continuity of $\\nabla g$ \n3. Extensive validation on image denoising, deblurring, and super-resolution.\n\n",
            "main_review": "**Weakness**\n\n1. As stated, the proposed regularizer is similar to the alternate prior proposed in RED. What is the difference? Why the re-invention is new in the context of the paper.\n\n2. The convergence analysis seems to follow the standard proof in the non-convex optimization literature when the explicit regularizer is given. To me, the analysis is a straightforward extension of the classic results. Could the authors elaborate more on the difficulties in the proof?\n\n3. Could the authors explain more about how the network is trained? Since the output of the neural network is an image-size vector and the regularizer is $||x-N_\\sigma(x)||_2^2$, it is a little weird to me if one forces $\\nabla g$ to be the noise residual. Why not make $N_\\sigma$ a denoiser and explain $\\ell_2$-norm as accommodation of the Gaussian noise.\n\n4. Various paper has proposed backtracking steps to determine the step-size ($\\tau$ in GS-PnP). Perhaps a comparison and discussion are required.\n\n5. The final performance is about the same as the state-of-the-art plug-and-play algorithm (DPIR). If the authors can explain the novelty of the proposed regularizer properly, this will no longer be a downside. However, if the work is not novel in terms of theory, a marginal/zero improvement in the empirical performance will be a weakness.\n\n6. Given the linkage to RED, the current paper should provide a proper review of the progress of RED as well.",
            "summary_of_the_review": "In general, I vote for *weak rejection* based on the current evaluation of the paper.\n\n—— After rebuttal ——\nI now vote for acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}