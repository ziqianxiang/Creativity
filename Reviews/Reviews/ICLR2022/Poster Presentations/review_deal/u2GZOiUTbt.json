{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes a few-shot learning method that uses Fisher information matrix-based task affinity. The experimental results show that the proposed method achieved better performance than existing methods. This paper is well-written. The newly proposed task affinity score is interesting. The experimental results and theoretical analysis support the effectiveness of the proposed method. The authors are encouraged to address the reviewers' concerns in the paper.  Although the distance between task representations is symmetric in neural processes, they do not use the symmetric distance for meta-learning. They input the task representations into the neural network, so the output can be asymmetric."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents a new affinity score based on the Fisher Information matrix from a source to a target task. The authors also develop a few-shot learning procedure based on a pre-trained Whole-Classification network approach. In this procedure, training labels are matched via a maximum matching algorithm, and the target task for training is selected using the proposed affinity score. The effectiveness of the proposed score is demonstrated using benchmark datasets in the problem of few-shot learning.",
            "main_review": "Strong points.\n- The authors introduce a new score based on the Fisher Information matrix; its mathematical analysis is also presented.\n- The performance is compared with many meta-learning methods in the empirical study.\n- The paper is well-organized.\n\nWeak points.\n- The proposed few-shot learning scheme seems incremental.\n- The advantage of the proposed score (TAS) is not clear.\n- A more thorough evaluation is needed to show the effectiveness of the proposed method.\n\nComments.\n1. In my understanding, the main contribution of this study is to incorporate a process of constructing a related-training set via Task Affinity Score (TAS) into a few-shot learning procedure. Another approach to consider the task similarity is to embed tasks into hidden space, as in conditional neural processes (NPs) [R1]. The authors should clarify the advantage of the proposed approach compared with such prior works. Also, it would be helpful to add the empirical evaluation with conditional NPs, etc. \n\n2. In Section 3, the authors present TAS as a general one; however, the few-shot learning scheme is constructed using a specific approach, i.e., Whole-Classification. Is it possible to use TAS for few-shot learning based on another approach?\n\n3. In Tables 1 and 2, the authors demonstrate that the proposed method outperforms many previous meta-learning methods in terms of prediction accuracy. However, this is not enough to convince the effectiveness of the proposed method. I would like to see more evidence of why the proposed method works well. For example, it would be helpful to analyze the related source tasks selected by TAS.\n\n4. As another aspect, the authors would add a discussion on the computation complexity of the proposed method.\n\n[R1] M. Garnelo, D. Rosenbaum, C. Maddison, T. Ramalho, D. Saxton, M. Shana- han, Y. W. Teh, D. Rezende, and S. A. Eslami. Conditional neural processes. In International Conference on Machine Learning, pages 1690–1699, 2018. ",
            "summary_of_the_review": "This paper proposes a new few-shot learning procedure and shows its effectiveness in terms of prediction accuracy. However, the advantage of the proposal should be discussed compared with previous works; the authors should conduct a more thorough evaluation. Accordingly, I vote for rejecting this paper.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a few shot learning method, which tries to measure the affinity degree between different tasks. Based on the affinity score, the relevant tasks are exploited for training to boost the performance of target tasks. The Task Affinity Score (TSA) is proposed, which is novel  to measure the dependency between different tasks. The reasonability of TSA is also validated with mathematical proof.",
            "main_review": "Strength:\n1. The idea to exploit Fisher Information Matrix for relevance measurement between different tasks is novel.\n2. The experimental results are promising.\n\nWeakness:\n1. The paper exploits the empirical Fisher Information matrix. The reason for this approximation should be discussed in detail.\n2. The authors did not conduct some necessary experiments to validate the effectiveness of TSA, e.g., the authors should prove that models trained with source tasks selected via TSA are consistently better than those trained with randomly sampled source tasks.\n3. The proof of TSA holds when the loss function is strictly convex, which is a strong assumption.",
            "summary_of_the_review": "This paper exploits the fisher matrix to measure the relevance between different tasks which is an interesting attempt. In addition, the experimental validate the effectiveness of the proposed method.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose a task affinity score based on maximum bipartite matching algorithm and Fisher information matrix. And then utilize this score to find the closest training data labels to the test data and leverage the discovered relevant data for episodically fine-tuning the few-shot model. Experimental results on few-shot learning setting achieve the state-of-the-art performance on four widely-used benchmarks.",
            "main_review": "pros:\nThe paper is well-written and neatly presented. \nThe idea that leveraging task affinity score to find most relevant tasks for better few-shot learning is impressive, and seems to be technically sound.\n\ncons:\n\tThe contributions are not well highlighted.\n\tThere is no ablation study in this paper, the experimental results are insufficient to validate the idea and many detailed experiments are not provided.\n\tThe main idea and the motivation are easy to follow, however some details of the proposed model are still not well specified.\n\nDetailed comments and questions:\n\t1. In Definition 1, what is the meaning of epsilon-approximate network? How do you determine the value of epsilon, does it have a significant impact on the performance of the model?\n\t2. In Definition 3, why use task Ta's query data to compute the Fisher information matrix instead of using support data?\n\t3. It is mentioned in this paper that the classifier model is fine-tuned only with the labels of the related-training set, the complexity of training of the few-shot model is reduced. However, from Algorithm 1, the relevant task selection seems quite time consuming. Has the author considered the computational complexity of the model?\n\t4. In 1-shot setting, the class centroids of the target task are the sample itself. Will the closest source task calculated in this case be biased due to the randomness of the sample selection?\n\t5. In Section 4.2.2, the author says that top-R scores and their corresponding classes are finally selected. Then Algorithm 1 selects the task with the minimum TAS. Which method does the author use?\n\t6. How to use the source task fine-tuning model, which the authors did not explain? For example, in the miniImageNet, 2000 source tasks are randomly selected, then the model is fine-tuned on each source task to calculate TAS to the target task. Is this process time-consuming? \n\t7. In the tieredImageNet, why only 120 labels are selected for each source task when the test set of this dataset has 160 labels?\n\t8. In Table 1 and Table 2, why use different backbone to compare IE-Distill methods respectively? miniImageNet uses standard resnet-12 while tieredImageNet uses wide-layer resnet-12.\n\t9. What is the baseline of the paper? Is the performance gain from TAS-Simple to TAS-Distill derived from knowledge distillation or the method proposed by the authors?\n",
            "summary_of_the_review": "The overall idea of using task affinity score for few-shot learning is impressive. However, I think this submission is incomplete and lacks a lot of technical details, which makes the paper not easy to be understood. Meanwhile, the paper also has not conducted any ablation study to show the real empirical improvement of the proposed strategies and modules. Moreover, although the method proposed by the author does not introduce additional model parameters, I am quite concerned that the computation cost of the fine-tuning source tasks during the task affinity score calculation.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}