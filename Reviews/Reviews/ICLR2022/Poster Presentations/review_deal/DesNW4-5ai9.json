{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes integrating three existing approaches to give a simple algorithm called TAIG for generating transferable adversarial examples under blackbox attacks.\n\nIn the original reviews, some strengths and weaknesses of the papers were highlighted although some of them have not reached general agreement after the discussion period.\n\nRegarding the merits, it is generally felt that the experimental results are good and the idea of updating along the integrated gradients is new (despite a simple idea) and has some theoretical justification.\n\nNevertheless, even after the discussion period, some concerns still remain, including the technical novelty of the proposed method and the high computational requirements of the proposed method, among others.\n\nWe appreciate the authors for responding to the reviews by clarifying some points and providing further experimental results. The paper would be more ready for publication if all the comments and suggestions are taken into consideration to improve the paper more thoroughly."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies adversarial attacks to deep neural network. To achieve good attack performance, this paper proposes the attack to the integrated gradients and the method (TAIG) is shown to be highly transferable to other black-box models.",
            "main_review": "The proposed Transferable Attack based on Integrated Gradients (TAIG) generated the adversarial examples on the integrated gradients, which has shown good attack performance against some deep neural networks. The proposed attack method can be analyzed from three aspects: optimization based, attention based, and smoothing decision surface based. However, I have some concerns about this paper:\n1.\tIt seems the paper only combines the traditional attack (FGSM) on the integrated gradients. Will the other attacks achieve better performance on the integrated gradients? More novel contributions should be stated;\n2.\tWhy the paper selects a black image as the reference?\n3.\tThere are still some extant works that generated the adversarial examples based on the sensitive feature in the image. It is better to describe the difference and compare the performance;\n4.\tI am not convinced by the explanation that sign(IG(f,x)) approximates sign(f) such that 68% elements are the same according to Fig. 2. It is better to propose any other theoretical explanation;\n5.\tIn the experiment, the authors select thirty sampling points to estimate TAIG-S and TAIG-R. Please explain why it works?\n6.\tThere are some typos and grammar errors in the paper, it is better to revise them.\n",
            "summary_of_the_review": "The paper proposes the integrated gradients and the method (TAIG)  to attack black-box models, which has shown good performance. But there are still some place to improve.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper applies the Integrated Gradients technique to the problem of creating adversarial examples in the black box setting. This produces results that are better than the state of the art methods for most of the networks tested, and it can be used in combination with those existing methods to produce even better results.",
            "main_review": "Strengths:\n- The application of the Integrated Gradients technique to produce adversarial examples is novel\n- The evaluation of the method is comprehensive, and demonstrates state of the art performance in almost all cases\n\nWeaknesses:\n- The technical novelty is not enormous, in the sense that it consists purely of a straightforward application of an existing technique, and the method section mainly consists of a justification of why the method makes sense in this domain",
            "summary_of_the_review": "The paper is overall quite good, with thorough experiments demonstrating good performance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In practice, adversarial examples are generated in three ways, i) solving a standard optimisation problem, ii) leveraging the salient regions of an image, or iii) smoothing the decision surfaces. The authors propose a simple technique named Transferable Attack based on Integrated Gradients (TAIG) that combines all these three approaches. Unlike the existing systems, which leverages other methods by adding additional terms to the objective function, TAIG integrates them into a single objective.",
            "main_review": "The authors propose a simple but effective technique for generating adversarial perturbations in a black-box setting. In addition to this, the core component of the proposed technique (the Integrated Gradients and the Random Path Integrated Gradients) can be used to improve the existing techniques. I found the paper to be clear. I would encourage the authors to additional experiments to justify the claims made in the paper. \n\nBelow are a few questions for the authors:\nHow does your method stack up against other classes of black-box adversarial attacks? For instance, I would encourage the authors to compare against gradient-free attacks.\nHow imperceptible are the examples generated by the proposed technique? For instance, even though most black-box adversarial have higher success rates, they generally forgo imperceptibility.\nHow good is the proposed technique in evading the recent class of adversarial example detection methods?\n\nI would also encourage the authors to include a detailed set of ablation studies, such as how many samples are needed to ensure that the surrogate models are sufficient. Does TAIG require less in comparison to others? Also, over how many samples are the Integrated Gradients computed, does that impact your results? In addition to the above, it will also be helpful to report run-times for each of the techniques.",
            "summary_of_the_review": "In general, the paper explores an exciting direction, but I found the experiments a bit lacking. I would encourage the authors to address the questions listed above",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Two methods are proposed in this paper. They are Transferable Attack using Integrated Gradients on Straight-line Path (TAIG-S) and Transferable Attack using Integrated Gradients on Random Piecewise Linear Path (TAIG-R). Compared with typical gradient-based attack methods, the TAIG-S uses integrated gradients to update adversarial examples, and TAIG-R uses random path integrated gradients to update adversarial examples. Experiments on ImageNet can prove the effectiveness of these methods.",
            "main_review": "Strengths:\n1. The experimental results are good.\n2. The idea of updating along the integrated gradients is new and has theoretical support.\n\nWeaknesses:\n1. The most important weakness is the confusing experimental settings. Firstly, this paper lacks the details of calculating integrated gradients, i.e., how to calculate Equation (1)? According to the experiments section, I guess the integrated gradients are computed by averaging the gradients of 30 sampling points. Thus, the TAIG-S is 30X slower than most methods with the same iterations, such as LinBP and AOA. Although SI also requires calculating multiple gradients, TAIG-S is still 6X slower than SI (with the number of scale copies of 5). Furthermore, TAIG-R requires more gradient calculations because it should calculate function IG(f,x) E (the number of turning points) times. However, the authors did not report the value of E in the experiment section. Thus, I think it is better to compare the performance with other methods under the same gradient calculations instead of iterations. \n2. In section 3.3, the authors claim that sign(IG(f,x)) approximates sign(\\nabla f). I think it is a very weak approximation since there are only 68% elements of sign(IG(f,x)) and sign(\\nabla f) are the same, while random guessing can achieve about 50% of the same elements as sign(\\nabla f). The approximation of RIG is much worse (only 58%). Is this because the limited number of sampling points? Will the similarity between sign(IG(f,x)) and sign(\\nabla f) be higher if more sampling points are used to calculate IG? Besides, TAIG-S approximates sign(\\nabla f) better than TAIG-R while TAIG-R is better than TAIG-S in all experimental results. How do you explain this phenomenon?\n3. Ablation studies are missing. It’s necessary to see the sensitivity of the number of sampling points (30 in this paper) and the number of random paths (E, which is not be reported in this paper).\n4. From Table 1, LinBP seems to be the strongest counterpart. Why not compare with LinBP when VGG19 (or the other surrogate model listed in Table 4) is taken as the surrogate model? Finally, as I said before, LinBP only calculates gradients once in each iteration, it is unfair to compare it with TAIG-R which calculates gradients 30E times in each iteration. I recommend the authors to compare with some advanced methods which require more gradient calculations [1,2, 3].\n5. Maybe a typo: What’s the name of your methods? In the abstract, the method is called Transferable Attack “based on” Integrated Gradients. In later, the name is changed to Transferable Attack “using” Integrated Gradients.\n\n[1] Wu W, Su Y, Lyu M R, et al. Improving the Transferability of Adversarial Samples With Adversarial Transformations[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 9024-9033.\n[2] Wang X, He K. Enhancing the Transferability of Adversarial Attacks through Variance Tuning[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 1924-1933.\n[3] Wang X, He X, Wang J, et al. Admix: Enhancing the transferability of adversarial attacks[C]// Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). 2021: 16158-16167.\n",
            "summary_of_the_review": "This paper proposes a novel algorithm to generate adversarial examples, and the results are good. However, some analysis and experimental results are confusing. The most important weakness is the high computational cost. Please address issues listed in the detailed comments during the rebuttal and make the paper clearer.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}