{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper opens the area of adversarial-attack research on streaming data (e.g., real-world settings such as self-driving cars and robotic visual tasks for a robot). For instance, online adversaries can focus their attack on a small subset of the streamed/online data, but still cause much damage to downstream models. This work highlights the need for stateful defense strategies. Connections to online algorithms and the k-secretary problem are made, along with improvements to some online-algorithms work of Albers and Ladewig.  \n\nOverall, the attack model introduced is important, and the bridge to online algorithms would be useful for the ICLR community. I also believe this topic lends diversity to the typical set of ICLR papers."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper focuses on the online attack problem, where two elements are emphasized that attackers must operate under partial knowledge\nof the target model, and the decisions made by the attacker are irrevocable. To solve this problem, they propose VIRTUAL+ and give theoretical guarantee on competitive ratio. The findings in the related experiments also validate the necessity and efficacy of proposed method in online setting.\n",
            "main_review": "Strengths:\n1. This paper is technically solid and detailed. It is in well structure and easy to follow. The whole story is complete with the theory supported.\n2. The experiments are also good. Some findings in the paper do motivate us further reflecting on the difference of online attack and offline attack. This may provide a new perspective why deep learning model is so fragile.\n\nHowever, i still have some concerns w.r.t. experiments. \n1. As mentioned in the introduction, several works also study the online attack problem, more detailed description about the difference between their cases and this paper should be discussed, maybe in the related works? To be honest, i did not get the difference well.\n\n2. Even previous works study a slightly different setting. Could their methods be adapted as more strong baselines for comparison?\n\n3. How are the FGSM and PGD implemented in this paper？do we perform FGSM or PGD directly on the selected samples?\n\n4. As discussed in Section 4, a surrogate function is used when the true model is unaccessible. How the surrogate function is selected in the experiment? have you tried different surrogate functions?\n\n5. In the experiments,  counter-intuitive results are obtained. Have you tried different defence strategies for comparison? \n\n6. I notice that the experiments are conducted on MNIST and CIFAR only, is the proposed method can be efficiently implemented on large-scaled dataset?",
            "summary_of_the_review": "This paper is solid and well written, however, there are some points i am confused. I thus give the score of 6.\n\n====================================update========================================\n\nFirst of all, I appreciate the authors' hard work in the rebuttal. Most of my concerns have been addressed in the discussion. I would like to raise the score to accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed a new threat model for adversarial attacks called online adversarial attacks. Unlike traditional threat models, the online attack model assumes the data feed as stream and the decision made by the attacker are irrevocable. Towards this, the author made a connection between the online adversarial attack and the k-secretary problem, and proposed a new algorithm Virtual+. The experiments show that Virtual+ achieves better online fool rate than other baseline methods.",
            "main_review": "Strengths:\n- This paper proposed a new threat model called online adversarial attacks, which could open the door for a new research direction.\n- Based on existing algorithms, the authors proposed Virtual+. This algorithm could also be used for tackling other online problems.\n- The experiments show the proposed algorithm achieves better results than other baseline method.\n- The theoretical analysis is thorough and technically sound. \n\nWeaknesses:\n- I feel hard to understand the motivation of studying online adversarial attack. I agree it is a new problem, but I am not sure about what kind of attacker need to launch online attack on what type of dataset? Please give a concrete example.\n- The proposed method seems disconnected with the adversarial attack scenario. Which part of the Virtual+ algorithm is tailored for adversarial attacks? If the Virtual+ is a generalized algorithm which can be deployed on any k-secretary problem, why not position this paper as an algorithm paper instead of concentrating on adversarial attacks?\n- The experiment only evaluated on simple image datasets like MNIST and CIFAR. This does not support the author's motivation that \"Countless real-world applications involve streaming data that arrive in an online fashion (e.g., financial markets or real-time sensor networks).\" Even for the image domain, MNIST and CIFAR-10 are considered to be insufficient.",
            "summary_of_the_review": "I am on the fence about this paper. On the one hand, the authors proposed a new threat model and provided with theoretically-grounded algorithm for tackling that problem. On the other hand, the motivation of the problem is not clear to me and the evaluation is also insufficient. I'd like to see other reviewer's opinion and the author's comment before making my final recommendation.\n\nPost rebuttal:\nI appreciate the author's effort on addressing my concerns and answering my questions. Most of my questions has been addressed. Thus, I'd like to raise my score to 6. However, I am still not fully convinced about the significance and motivation of studying online adversarial attack. My final recommendation is perhaps we could accept this paper and see how it can inspire future research on this direction.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": " This paper studies the online adversarial attacks on deep learning (DL) models. Unlike other works that focused on adversarial attacks on  DL models, this work studies, on half of the attacker, how to choose a subset of data points to attack in an online fashion. Since the data arrives as a stream, the attacker has to make an irrevocable decision on whether to attack when each data point comes. This setting aligns with the situation given in the well-studied k-secretary problem. Built on top of a famous algorithm in the k secretary problem called VIRTUAL, an algorithm called VIRTUAL+ is proposed by the authors, which provably generates the best competitive ratio among all the single threshold-based algorithms (when the number of data points being attacked is less than 5). The attacker may not have access to the target deep learning model but can estimate the query value. Hence, the authors obtain theoretical bounds on the performance of the proposed VIRTUAL+ algorithm for the stochastic setting. This paper conducts experiments on both MNIST and CIFAR-10 with both vanilla and robust classifiers.\n",
            "main_review": "Instead of studying how to craft an adversarial example x', this work supposes that the attacking strategy of finding x' is given and focuses on finding the right subset of data points to attack in an online fashion. \n\nOne question the audience may have is under what real-world circumstances, the attacks can be carried out in such an online fashion. \n\n* The authors mentioned networked control systems. I agree that control systems operate in real-time. But it does not mean that the attack model here can be applied to attacks on control systems, because in control systems current state depends on previous states (so do the data points, which means data points are not independently ordered). Also, in control systems, usually, the goal is not to optimize certain loss functions but to ensure stability. \n\n* Can the authors provide more real-world examples/situations where the attack model studied here can be applied? If possible, please consider conducting experiments on these examples.\n\n* For MNIST and CIFAR-10, data points usually arrive in batches instead of single data points. Can the authors also justify that in the manuscript? How would the authors deal with data batches? Can the current model extend seamlessly to the \"batched data\" situation?\n\nAnother major concern is the performance of the VIRTUAL+ algorithm. The theoretical results show that the algorithm produces the best competitive ratio only when k<5. In real applications, the number of data points involved can reach millions if not billions. The number of data points being attacked can easily go beyond 5. Does it mean that the VIRTUAL+ algorithm can only perform relatively better under some extreme cases and the application of the algorithm is limited?\n\nOther minor concerns include:\n* In C.1, it is not clear how table 4 is produced. In particular, it is not clear how the surrogate model is generated.\n* please mention several ways of counter-attack in the conclusion section, e.g., arranging data points in a way such that the effect of  VIRTUAL+ attacks is mitigated.\n* Figure 4 appears before figure 3",
            "summary_of_the_review": "In the review, I proposed two major concerns toward the paper along with some minor ones. I did not check the proof carefully but they look correct.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper formulates an online threat model and use the VIRTUAL+ algorithm their proposed to solve this k-secretary problem. The proposed method is supported by theoretical justifications, and the numerical experiment shows a better performance of VITRUAL+ over other existing algorithms.",
            "main_review": "The new algorithm VIRTUAL+ is interesting, but I vote for weak reject given its motivation and novelty towards adversarial learning. The explanations towards some observations in the experiments do not convince me either.\n\n(1) It is not clear why this paper considers the online adversarial attack. The main contributions are built upon the proposed new algorithm towards k-secretary problem, and the online adversarial attack is only one of the possible applications of this k-secretary problem. Different from most literature in adversarial training, it does not deepen the understanding of attack method or robust features. The current title and abstract are somewhat not accurate to describe the main interest of this paper. Please consider polish the writing.\n\n(2) Given the problem setup of \"selecting k samples to attack in a stream of data to mislead a fixed neural network to make wrong predictions\", the proposed problem forumlation in this paper is a natural solution. However, in reality, e.g. filtering system to detect spam email, the classifier is updated along the time. It is more interesting to consider a changing f_t rather than a fixed target f_t. Similarly, we can also train f_s through receiving the streaming data.\n\n(3) In the experiment, the authors observe a much-better-than-PGD performance for FSGM. In theory a PGD attack is always not weaker than FSGM. I checked the code and found that the PGD attack setup is (nb_iter=40,eps_iter=0.01) for eps=0.3 in MNIST. This setup is used in other literature, but could you please try to increase nb_iter to see its performance? It is possible that the attack is hard to train for some data, so a larger nb_iter may help diagnose this. The current explanation for this observation does not convince me. \n\n(4) In the experiment, the authors observe that in CIFAR-10, the robust model is more vulnerable to online attack compared to the non-robust model. To explain this, the authors mention that in Fig. 5c the non-robust models have a heavier tail making it harder for A to pick successful attacks. I do not understand this explanation. If the non-robust models have a heavier tail, does this mean that more data are easy to attack in the non-robust model? Or do you mean that the loss does not reflect the exact decision? Since this observation is counter-intuitive, please have some detailed and convincing explanation for it.\n\n\nMinor things:\n\n(5) The paper provides motivation and intuition of the proposed VIRTUAL+ algorithm. It would be great if discussions can be provided for the technical difficulties of Theorem 1.\n \n(6) The title \"Non-Robust Model\" in Figure 5(c) is misleading. Please remove it. Also the caption of Figure 5 says \"MNIST (or CIFAR)\", please make it clear which dataset the subfigures are using.\n",
            "summary_of_the_review": "The new algorithm VIRTUAL+ is interesting, but please make it clear about the motivation and novelty of this work towards adversarial learning. Please also provide convincing explanations towards some observations in the experiments.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}