{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes Hindsight Foresight Relabeling (HFR), an approach for reward relabeling for meta RL. The main contribution is a measure of how useful a given trajectory is for the purpose of meta-task identification as well as the derivation of a task relabeling distribution based on this measure.\n\nReviewers agreed that the paper tackles an interesting problem and found the main insight to be simple and intuitive. While the initial reviews raised some concerns regarding novelty, the performance gap, and using the learned Q-function to estimate post-adaptation returns the rebuttal did a good job of addressing these concerns. Overall, the paper proposes a non-trivial extension of hindsight relabeling to meta RL and while the results could be stronger I think the paper provides useful ideas and insights so I recommend acceptance as a poster."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes an approach for using data relabelling for meta-RL for better sample efficiency and to enable training on sparse reward environments. Specifically the proposed method combines Pearl[1] with a modified version of HIPI[2], where the trajectories chosen for relabelling are effective for adaptation, and not necessarily high in reward themselves.\n\n[1] : Efficient Off-Policy Meta-RL vis Probabilistic Context Variables (Rakelly et al.)\n[2] : Rewriting History with Inverse RL (Esyenbach et al.) \n\n\n\n\n",
            "main_review": "Strengths \n\n1. Usefulness of relabelling\n\nThe problem considered is an important one, since even though hindsight relabelling is standard in multi-task RL, and has been shown to enable learning on sparse reward environments (which are otherwise very difficult to solve), this approach hasn’t been applied to the meta-RL setting yet. This is despite the fact that meta-RL also considers a multi-task distribution, and can benefit from explicitly using data for a different task and relabelling it under the corresponding reward function. The mathematical formulation of the approach closely follows HIPI[2], with the difference that post-adaptation trajectory return is considered instead of current trajectory return, to be aligned with the meta-learning objective. The authors show experimentally that current meta-RL approaches do not begin to make progress on sparse-reward tasks, showing the importance and effectiveness of relabelling.\n\n2. Extent of Evaluation and Analysis\n\nThe paper includes evaluation on 5 different sparse reward environments, 3 dense reward environments which show that the relabelling scheme offers benefits over current meta-RL approaches mainly in the sparse reward setting. The authors also include ablations/analysis of specific components, such as using a learned reward instead of the true reward, using hardmax instead of softmax for sampling the relabelling task etc. The paper is well written, the presentation is clear and well-motivated. \n\nWeaknesses \n\n1. Small performance gap with HIPI, Simplistic Environments \n\nOut of the 8 experimental domains chosen, the performance of the proposed approach (HFR) is significantly better than HIPI on only two domains (ant-goal and sawyer-push). This indicates that most of the benefit is coming from the relabelling scheme for most environments, either because the adaptation procedure doesn't actually lead to better performance, or because the environments are too simple to require a lot of adaptation to held-out tasks. Given that performance is much better than HIPI on the hardest environments (ant-goal and sawyer-push), I am inclined to think the issue is the latter instead of the former, which can be addressed by evaluating on harder environments. \n\nThese could include other single-family robotic tasks from meta-world (eg: sawyer-door-open, sawyer-box-close etc). Even better would be meta-training across task families, using MetaWorld ML-10 or ML-45. This would test adaptation to tasks that are semantically different, and would make the paper a lot more compelling. \n",
            "summary_of_the_review": "The approach introduces relabelling (which has already shown to be important in multi-task RL) in meta-RL, and shows superior performance on sparse reward environments. The paper would be more compelling if it included evaluations on more challenging environments, to establish the importance of the adaptation component. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a way to share data across different tasks in meta-reinforcement learning (meta-RL), where the data from one task is reused in another task by relabeling the rewards. Based on the HIPI method ([1]), the authors construct a relabeling distribution to relabel the pre-adaptation trajectories from one task to be used for another task. The relabeling probability of a trajectory is chosen to be proportional to the exponentiated utility function, which is defined as the expected return after the agent uses that trajectory to adapt. In practice, the post-adaptation return is approximated using the learned Q function. The authors apply this relabeling distribution to PEARL, an existing off-policy actor-critic style meta-learning algorithm.\n\nThe authors conduct experiments on simulated robotics experiments. The results suggest that the proposed method outperforms prior methods on sparse reward tasks, while performing roughly the same on dense reward tasks. \n\n\nReferences\n\n[1] Eysenbach, Benjamin, et al. \"Rewriting history with inverse rl: Hindsight inference for policy improvement.\" arXiv preprint arXiv:2002.11089 (2020).\n",
            "main_review": "Overall I think this paper presents an interesting idea for sharing data between tasks of a meta-RL problem. The paper is well written and the ideas are presented clearly.\n\nPros:\n\n1\\. I find the main insight of the paper simple and intuitive. The idea that we need to relabel not according to how much return we achieve but according to how much information we can gather for task identification gives us a clear distinction between multi-task RL and meta-RL. The derivation of relabeling according to the exponentiated post adaptation return follows naturally.\n\n2\\. The ablation study in the paper is very informative. The ablation study gives us clear comparisons, showing us which component is more important. From the ablation study, it seems that using the partition function and softmax relabeling distribution are the most important components.\n\n3\\. The paper is well written. The insights, ideas, algorithms and experiments are easy to follow. \n\n\nCons:\n\n1\\. I am somewhat skeptical about the approach of using the learned Q function to estimate return after adaptation. In the base meta-RL algorithm PEARL, the context encoder is trained to identify the task from a distribution of tasks, producing a posterior distribution of context z corresponding to that task. This means that given the relabeled trajectory, even if the context encoder predicts the context corresponding to a wrong task, as long as the produced context is within the distribution of tasks, the expected return will still be high because the policy is trained to do well also on that wrong task. Therefore it is not clear to me why using the learned Q function is a good way to estimate return on that specific task. \n\nIn order to verify this, I’d like to ask the authors to include the following experiments. First train the proposed algorithm to convergence and freeze the weights of the context encoder. Then train a N-way classifier on top of the context encoder to classify the context into one of N training tasks without using any relabeled trajectories, and then report the accuracy of the classifier. Finally relabel the trajectories according to the proposed method and report the classifier’s accuracy on top of the relabeled trajectories. If the relabeling mechanism using the learned Q function is correctly capturing the task information, we would see that the classifier’s accuracy on the relabeled trajectories is comparable to that on the true trajectories. In fact, this experiment could also lead to an even simpler relabeling strategy: directly use the true task’s probability under the classifier’s prediction as the source of relabeling signal. \n\n\n2\\. The empirical performance of the proposed method does not seem very strong. Only in 3 of 5 sparse reward tasks the proposed method significantly outperforms the baselines, and the proposed method does not show much improvement on dense reward tasks.\n\n\nGiven these limitations, I’m leaning slightly towards not accepting the paper. I’d highly encourage the authors to conduct the experiment I suggested in order to verify that the proposed method is indeed capturing the task information correctly.\n\n\n## Update After Author Response\nThe authors conducted additional classifier experiments I requested and the results suggest that using learned Q function to estimate returns is highly informative about the task. Therefore my main concern about the proposed method has been addressed, and I'm now leaning towards accepting the paper.\n\n",
            "summary_of_the_review": "The paper presents an interesting idea about reusing data across tasks in meta-RL. The idea is very intuitive and the paper is well written. However, I’m not sure about whether the approach used to implement the idea of the paper really does what the authors claim it does. Therefore I’d like to see more evidence before I can recommend accepting the paper.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper studies task relabelling in hindsight to increase the efficiency of meta-reinforcement-learning.\nThe authors propose a strategy for calculating a distribution of tasks for which a particular batch of data would be useful for adaptation, and sample from this distribution to construct a relabelled batch which augments the training data.\nThe authors show empirically that this improves sample efficiency over more naive relabelling schemes, particularly for sparse reward tasks.\nA series of ablations further justifies several design decisions or investigates robustness to hyperparameters.",
            "main_review": "I like this paper overall. The motivation is sound: meta-RL is almost by definition slow, by using a slower timescale for meta-learning than the fast learning or adaptation, and so data-efficient methods are key. Task relabelling, like in multi-task or goal-conditioned RL, makes a lot of sense in this context.\n\nThe particular proposed method seems reasonable, although I have some concerns about the detail of the exposition – I found section 4.1 fairly difficult to follow.\nFirst, it’s not 100% clear to me how to map eq(1) to the objective written in terms of utilities because eq(1) does not define where tau_pre come from. Presumably this is just following pi_theta, and the conditioning of q(tau | psi) on psi only results in differing rewards in tau, not differing state-action sequences?\nThen, most importantly for understanding this section, I don’t follow why the objective for (theta, phi) should be maximised by adjusting this q for fixed (theta, phi). The paper says this “facilitates alignment with the goals of the meta-learner” but I’m not sure what this means.\nThe derivation then continues in a very brusque manner. I’m not a fan of “it is easy to show”: in general if it is easy rather write it yourself (in an appendix if need be for space) or cite appropriately. Eventually we arrive at an ‘optimal’ relabeling distribution but I don’t understand in what sense it is optimal due the previous confusion.\nIt could be I’m missing something simple or these things are all straightforward and clear to a reader with the right context.\nHowever, I encourage the authors to substantially clarify and elaborate this section to engage with a broad audience.\n\nThe issue that I have more intuitively with the method is that the optimal task inference should depend on the true distribution of tasks. By altering this distribution through relabeling, it seems it would change the optimal (theta, phi). Can the authors elaborate on whether or not this should be a consideration, perhaps by clarifying the exposition given in S4.1?\n\nThe implementation of the approach is quite neat. I like the use of PEARL’s particular type of value function to efficiently estimate the value of the post-adaptation policy without sampling any fresh transitions.\n\nI also like the empirical study. The performance gains seem substantial in several tasks, and I appreciate the credible baselines which are more naive but not just vanilla PEARL without any task relabeling.\nI appreciated the informative ablations.\n\nMinor comments or questions:\n - Paragraph 2 of the intro says meta-RL is \"inherently on-policy\": this is incorrect.\n - Why relabel with just one task sampled from q(psi|tau)? Why not several samples, or weighted samples?\n - In Algo 1, maybe use a different letter to distinguish N in GetLogPartition and in N in ComputeUtility\n - The paper would benefit from more details on the setup with a learned reward function.\n\n----------------------\n\nThe authors were able to clarify the points that had confused me in my initial reading. I am persuaded that the optimal meta-learned solution will not be biased by the proposed relabelling; and that the derivation is sound. Optimising the relabelling distribution for the immediate post-adaptation returns makes sense as a somewhat myopic heuristic to accelerate meta-learning.\nI also appreciate the additional experiment carried out for HrwG. Further, while the connection to prior work is close in many ways, I believe the adaptation of the method for this context is sufficiently novel and effective to warrant acceptance.",
            "summary_of_the_review": "The work is well-motivated intuitively, but the mathematical justification for the specific method is difficult to follow (so I cannot quickly verify its soundness).\nThe empirical study is well done overall, so I lean to accept the paper but would likely increase my score and confidence if the authors can clarify the theoretical motivation for their relabeling strategy.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a trajectory relabeling method for meta Reinforcement Learning (meta-RL), aiming to share some of the collected trajectories to improve sample efficiency during meta-training. The relabeling method is built on HIPI (Eysenbach et al., 2020). Instead of relabeling the trajectory based on the total reward as in HIPI, the paper argues that in meta-RL, the metric of interest for trajectories from different tasks is their usefulness for task-identification rather than returns. The paper further proposes a meta-RL algorithm based on PEARL (Rakelly et al. 2019). The experimental results on several sparse-reward tasks show that the method outperforms other relabeling methods as well as PEARL.",
            "main_review": "(a). I am a little concerned with the novelty of the paper.  The authors made an interesting point that compared to multi-task RL, the objective in meta-RL is to learn to learn a new task, so the metric of interest for relabeling trajectories in meta-RL is their usefulness for task-identification, rather than the returns like in multi-task RL (Section 4, page 4). The paper’s main contribution is a trajectory-relabeling algorithm in meta-RL setting based on this intuition. However, the proposed algorithm still seems to try to relabel trajectories based on their returns on other tasks. Specifically, the resulting learning objective Equation (8) is quite similar to that of HIPI (Eysenbach et al., 2020), as the utility function similarly aims to maximize the expected return of the trajectory on the new task, which I think would be exactly the total reward of trajectory on the new task in the multi-task setting. This seems to be contradictory to what the papers says about the difference of relabeling trajectories of previous work in multi-task rl and relabeling trajectories in meta-RL proposed in this work. Plus, the actual implementation looks like a straightforward combination of HIPI and PEARL (Rakelly et al., 2019) to me.\n\n\n(b). The proposed algorithm makes an important assumption that the actual reward function is known for each task in this meta-RL setting. This makes the meta-RL problem setting confusing as the paper also says the tasks share the same dynamics and only differ in the reward function. But like the paper mentions, there may exist some scenarios where this assumption is reasonable. The experimental results show that the proposed algorithm improves performance compared with other relabeling methods (HIPI and random) in such settings. First, I think there might exist methods that, under the same assumptions, are able to do better than meta-RL algorithms. For instance, one can relabel all the collected data with the new task reward and run some kind of offline RL algorithms on it (without meta-learning). In my opinion, that would be another good baseline to strengthen the author’s claim under the same assumptions. Secondly, in the experiment section the paper mentions a variant of the proposed method that also considers the scenario where the true reward function cannot be queried for individual transitions. This is a more interesting setting and I think the authors should elaborate on this part (e.g. how do you learn the reward functions? Is there anything specifically designed for meta-RL settings?) And it would be better to show more experimental results under these settings. For instance, the author can compare to the state-of-the-art meta-RL algorithms under such more common settings, especially in sparse-reward environments where the proposed algorithm is more competent. Potential baseline: 1. MetaCURE: Meta Reinforcement Learning with Empowerment-Driven Exploration, Zhang et al., ICML 2021;  3.  Towards Effective Context for Meta-Reinforcement Learning: an Approach based on Contrastive Learning, Fu et al., AAAI 2021. \n\n\n(c). I have some minor comments listed below:\n\n1. Equation (4), On the left hand side, $j$ should be superscript instead of subscript.\n\n2. Figure 4, the actor $\\pi$ and critic $Q$ should be parameterized using different denotations, instead of jointly using $\\theta$.\n\n3. Figure 6, could the authors explain why randomly relabeling the trajectories can achieve competitive performance or even better performance than HIPI and PEARL?\n\n4. The adaptation procedures listed on page 6 before equation (9) confuse me. Could the authors provide an algorithm bar for the meta-test phase (maybe in appendix)?\n",
            "summary_of_the_review": "The idea in the paper is well presented and carefully investigated. The proposed method is simple and effective. However, I am not quite convinced about the novelty of the proposed idea and I think the experimental settings can be improved to strengthen the paper’s claim.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}