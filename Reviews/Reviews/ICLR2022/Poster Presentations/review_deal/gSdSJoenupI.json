{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "Realizing the fact that cross-entropy loss and focal loss are widely used for training deep learning models but mathematical understanding and exploration for such losses are lacking, the authors propose a simple framework named PolyLoss to express the loss function as a linear combination of polynomial functions. \n\nIn this framework, the aforementioned cross-entropy loss and focal loss are the special cases of PolyLoss by easily adjusting the importance of different polynomial bases depending on the targeting tasks and datasets. The final version of PolyLoss, Poly-1 formulation, is simple with one line of code and an extra hyperparameter but outperforms the cross-entropy loss and focal loss on 2D image classification, instance segmentation, object detection, and 3D object detection tasks, sometimes by a large margin.\n\nThis paper is well-motivated by a novel perspective of polynomial expansion. The proposed method is novel, simple to implement, and effective in practice. The authors have a deep and thorough discussion with reviewers, and most concerns were well addressed. After rebuttal and discussion, reviewers increased their scores, and all agreed with acceptance. AC checked the paper and all relevant information, and found sufficient ground for acceptance."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper analyzes the cross-entropy loss and focal loss through the polynomial expansion perspective. Further, this paper proposes a family of loss functions called PloyLoss. Three instances of PloyLoss are analyzed and obtain the final version as ploy-1. Detailed and extensive experiments are provided.",
            "main_review": "## Strengths\n(1) The polynomial expansion perspective is interesting and the PloyLoss makes sense.\n\n(2) Extensive experiments are provided and further produce some findings. \n\n(3) The proposed loss function can improve various tasks: image classification, instance segmentation, and 3D object detection.\n\n## Weaknesses\n(1) surprisingly, the final version of PloyLoss is Ploy-1. I think this final version is trivial. Does this mean the high-order items are useless? I understand the results of $L_{Drop}$ in Figure 2(a), but I still think the poor results of the low-order loss function are because of the bad coefficient tuning. So can you provide the following versions for comparison?\n- L1 loss with hyper-parameter tuning. (Drop the CE part in the Ploy-1.)\n- L1 and L2 loss with the hyper-parameter tuning.\n\n(2) Because this paper proposes a general loss family, the exploration is still insufficient. There are many variables of the PloyLoss family, such as the coefficient (particularly analysis in this paper), the start order ($\\gamma$ in Focal loss), and the coefficient decay strategy.\n- CE and focal loss only provides one kind of coefficient organization strategy as the logarithmic function. What about others? For example, the exponential decay for the coefficients.\n\n(3) The name is misleading for $L_{ploy-1}$ in image classification and $L_{ploy-1}$ in object detection since they use different base loss functions (CE and focal respectively). \n\n(4) It seems like some errors in the label of the y-axis in Figure 2(b).",
            "summary_of_the_review": "This paper starts from an interesting loss family as PloyLoss, but the final version is quite simple and trivial. Considering both the experiments, motivation, and the final method, I would like to rank this paper as 5. I am positive about this paper, but more experiments and clarification are expected.\n\nAfter rebuttal, most of my concerns are addressed. Thus, I raise the rank to 6.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "- **Motivation**.\nThe paper argues that the original cross-entropy loss and focal loss are primarily used in the classification task. \nA general loss function should get rid of constraints of learning tasks and datasets. \n\n\n- **Method**. Motivated by this insight, the paper expresses the loss function as a linear combination of polynomial functions and shows that cross-entropy loss and focal loss are special cases. \nThen inspired by Taylor expansion, the paper proposes a polynomial loss function, called PolyLoss.\nThe final version $\\text{Poly}-1$ is with first-order. \n\n\n- **Experiments**. The paper verifies the proposed loss on ImageNet for classification, MS-COCO for detection and instance segmentation, and Waymo Open dataset for 3D detection.",
            "main_review": "- (a) Paper is well-written, especially the main part of PolyLoss, which has a clear logic. \n- (b) The paper has an interesting idea that shows cross-entropy and focal loss can be expressed as a linear combination of polynomial functions. Then it unifies these two loss functions as a part of the general polynomial form. The final PolyLoss is simple and effective.\n- (c) The experiments are solid enough to support the main idea.",
            "summary_of_the_review": "I like this paper. The idea is new, and the discussion is deep. I have no comments on how to improve the paper. Although partial experimental improvements are marginal, I think the novelty is promised. It deserves to expose to the community to inspire future works.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes a framework for creating loss functions based on the polynomial expansion of known loss functions. The authors show that, by fine-tuning the polynomial coefficients of those expansions can bring improvements in multiple computer vision tasks. The authors experiment with image classification (ImageNet-21k), instance segmentation and object detection (COCO), 3D object detection (Waymo), showing small improvements over the chosen baselines. \n\n",
            "main_review": "# Strengths\n\nThe paper reads well and has almost no typos. It provides an interesting observation showing how different losses could all be expressed in a common polynomial form where only coefficients differ.\n\n# Weaknesses\n\nThe motivation is not entirely clear. While loss functions could be expressed in the form of Taylor expansions, the fact that the polynomial coefficients would have to be fine-tuned in order to obtain further performance improvements introduces even more hyper-parameters for in model giving even more room for possible overfitting. Moreover, the claimed improvements are only marginally higher than the chosen baselines. It is not entirely clear if the added complexity (in both having to fine-tune the coefficients as well as determining where the expansions should be cut) would be justifiable.\n\n## Minor\n\n - \"polynmoial\" (p.9, conclusion)",
            "summary_of_the_review": "The paper is interesting, but it is unclear to me whether replacing losses with their polynomial expansions while bringing only small performance improvements to existing methods would suffice for a publication in ICLR. \n\nAfter rebuttal, most of my concerns have been addressed. I am therefore increasing my rating to 6.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "\nIn this paper, the authors introduce a new loss function for classification problems. To be specific, the authors introduce Taylor expansion of cross-entropy loss + focal loss and show that various subsets of this expansion can improve the models on image classification, 2D object detection and 3D object detection problems.\n\nI've reviewed a previous version of this paper at NeurIPS2021. Compared to the previous version, I see that the authors improved the paper significantly, mainly in terms of structure and presentation.",
            "main_review": "Strengths:\n- Clear and easy to follow text.\n- An application of the Taylor expansion of cross entropy loss and focal loss on different classification problems.\n- Experiments on different classification problems.\n\nWeaknesses:\n- The major problem that I had with the first version, which persists with the current version, is that some results are not significant. The authors obtain very minor improvements on several models and tasks. The large values for AR are actually not impressive since many simple tricks can provide similar (even better) improvements for AR. And the reported improvements for all other measurements and tasks are rather minimal.\n\nMinor comment:\n- \"can improves\" => \"can improve\".",
            "summary_of_the_review": "In general, I am positive about the paper, though I need to be convinced about the significance of the results and the importance of the work.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}