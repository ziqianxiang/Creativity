{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "In this submission, the authors presented a framework (GIANT) for self-supervised learning to improve LM by leveraging graph information. Reviewers agree that the method is somewhat novel, the (partial) theoretical analysis is interesting, and the evaluations are strong. We thank the authors for doing an excellent job in rebuttal which cleared essentially all the questions reviewers initially raised."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents a new self-supervised learning framework to enhance language model based on graph information. ",
            "main_review": "Strengths: The proposed method is simple and reasonable. The experimental studies are extensive.\n\nWeaknesses:\n\n(1) The novelty is limited. In my view, masked language modeling aims to predict the masked tokens given the context, and in this paper, neighborhood prediction aims to predict the relation given the context. Relation-prediction-based objectives have been widely applied in knowledge/entity oriented pre-training, such as K-ADAPTER (Wang et al.),  ERICA (Qin et al.). In addition, the impact of the proposed method could be rather minor given their experimental results. For example, in Table 1, on the ogbn-arxiv dataset, based on GIANT-XRT, GraphSAGE, RevGAT, and RevGAT+SelfKD gain accuracies of 74.59%, 75.96%, and 76.12%, respectively, and based on TFIDF+NO PIFA, gain accuracies of 74.09%, 75.56%, and 75.85%. With such small gaps, it would be important to know whether the difference is actually statistically significant.\n\n(2) Some details in Figure 1 are not clear, e.g., denotations of A and Y, full terms of XMC. To make Figure 1 self-contained, the caption of the figure should provide more necessary information.\n\n(3) The idea are verified on three node classification datasets, while some details are missing. “Split ratio”in Table 1 is confusing, I cannot figure out what the ratios for train/test/development are. How many classes for each dataset? It would be better to show some real instances.\n\n(4) What is the dataset used for pre-training GIANT?\n",
            "summary_of_the_review": "The novelty is limited, and the impact of the proposed method could be rather minor. Some necessary details should be provided.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposed a self-supervised learning framework for learning node feature by exploring the correlation between the node feature and the graph structure, which leverages the graph information based on neighborhood prediction. To be specific, the proposed GIANT approach is combined with the pre-trained language model BERT, and incorporated the XMC formalism based on XR-Transformer. Partial theoretical analysis is also presented. Experiments conducted on three large benchmark datasets show promissing improvements.",
            "main_review": "\nStrengths:\n+ Introducing the idea of neighborhood prediction to guide self-supervised node feature learning is interesting and somewhat novel.\n+ Connecting neighborhood prediction with the XMC problem is novel.\n+ Extensive experiments are conducted on OGB and show new state-of-the-art results.\n\n\nWeaknesses: \n- The reviewer has some concerns on the provided theoretical analysis based on cSBM (Deshpande et al., 2018). It seems misleading and incomplete. \n\nThe theoretical analysis could be deduced from the analysis in Baranwal et al. (2021) with a few changes. In Baranwal et al. (2021), the cSBM is used to analysis the effect of graph convolution operation on the linear separability. The established theoretical results show that: if the means of the two mixture of Gaussians is not large than a threshold, the results after graph convolution are not guaranteed with high probability to improve the linear separability. \n\nHowever, the statements in Theorem 4.4 is relatively vague. Note that PIFA is just one step of a graph convolution with the node features, plus a normalization step. What can we say about the performance of using the PIFA embedding?  Without the characteristic of the node features and the affinity of the graph convolution, it is hardly to have a convinsing conclusion. \n\nFurthermore, the requirement on $p > q$, i.e., the probability $p$ of having a link between two nodes having the same label $y_i=y_j$ should be larger than the probability of having a wrong link between two nodes having different labels $y_i \\neq y_j$. Is it necessary or not? Why? \n\n",
            "summary_of_the_review": "The idea is clear and the empirical evaluation is strong. Since that the reviewer has some concerns on the provided theoretical analysis, it would be safe to decide after reading the feedback from the authors. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper develops a self-supervised learning framework to extract node features with the aid of graph. Connections between neighborhood prediction and the XMC problem are also established. Experiments on large-scale data show the superiority of the proposed method.",
            "main_review": "Strengths:\n1. The problem is well motivated.\n2. The proposed framework could be useful in general.\n3. Both theoretical analysis and experiments are convincing.\n\nWeaknesses:\n1. the efficiency is not supported with experiments.\n2. There are some typos.\n",
            "summary_of_the_review": "This paper develops a self-supervised learning framework to extract node features supervised graph. It is an interesting problem. Theoretical analysis is also provided. Extensive experiments on large-scale data validated the effectiveness of the proposed method.\nThere are some typos, e.g., \"These have to be predicted using the textual information in order to best match the a priori given graph topology. This is achieved this by using the state-of-the-art XR-Transformer (Zhang et al., 2021a) method for\nsolving the XMC problem.\"",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}