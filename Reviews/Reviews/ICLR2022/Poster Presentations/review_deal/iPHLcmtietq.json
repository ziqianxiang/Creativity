{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes that the superior performance of modern convolutional networks is partly due to a phase collapse mechanism that eliminates spatial variability while ensuring linear class separation. To support their hypothesis, authors introduce a complex-valued convolutional network (called  Learned Scattering network) which includes a phase collapse on the output of its wavelet filters and show that such network has comparable performance to ResNets but its performance degrades if the phase collapse is replaced by a threshold operator.\n\nReviewers are all in agreement about the novelty and significance of the work. They also find the empirical results compelling. The main weakness of this work which was highlighted by all reviewers is clarity. The paper can be significantly improved in terms of the writing. While I am recommending acceptance, I strongly recommend authors to take reviewers' feedback into account and improve the writing significantly for the final version so that more people would benefit from this paper and build on it in the future."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies within-class variability which reduces along the layers of deep neural networks. They mainly question the effect of sparsity and soft-thresholding introduced by ReLU. They show that these classification improvements by eliminating spatial within-class variabilities rather come from a phase collapse, which eliminates the phase of network coefficients. Eliminating the phase of zero-mean filters improves the separation of class means, hence increase in classification accuracy. They introduced a complex-valued neural network in which spatial filters are defined as complex multiscale wavelets and learning is reduced to $1 \\times 1$ complex filters across channels. Their results show that such a network is able to reach ResNet-18 performance on CIFAR10 and ImageNet.\n\n\n\n\n",
            "main_review": "### Strength:\nThis paper is a nice extension to the related works [Zarka et al (2020, 2021), Ulicny et al.(2019)]. The main contribution (and difference to the literature) is that they hypothesize classification performance mostly results from iterated phase collapses. They supported their hypothesis  (necessary and sufficient) by first explaining the performance of iterated phase collapses by showing that it progressively improves linear discriminability. Secondly, they replaced the phase collapse with a soft-thresholding and showed that this considerably decreases the classification accuracy.\n\n\n### Correctness: \nThe core assumptions seem correct to me, e.g. the images classes are often locally invariant to translation. Small translations are used to be approximated by a phase shift. The empirical results are correct, the theoretical results seem reasonable, but I did not fully examine the proofs in the appendix or reproducibility of the code in the supplementary materials.\n\n### Weakness: \nGenerally, the writing quality is not good and hard to read, e.g. long sentences. Page 3, before the (Krizhevsky et al., 2012) reference, I am not sure if filters are usually localized oscillatory patterns or filters usually localize oscillatory patterns? Sometimes the text is not scientifically addressed, e.g. page 4: $\\mathbf{E}[X_y * \\psi] = 0$ then $\\mathbf{E}[\\rho_b(X_y * \\psi)] $ is \"usually\" close to zero. What does usually mean here? Formula4: \"One can verify...\"? It is now clear to me how. And \"This integral is already well approximated by a sum over 4 phases\"?  MNIST probably needs to be cited for \"The MNIST database of handwritten digits (1998)\". The operator \"$*$\" is not defined. It is also not clear to me why the authors used skip-connection-based neural networks? What about CNNs without skip connections? \n\n\n\n>update after rebuttal: I would like to thank the reviewers for addressing my concerns. However, I found it difficult to find the changes in the rebuttal version. I now see the Results with/without skip connections in Table1 and Table2. The text is still hard to read and follow (this is also addressed by other reviewers). As promised by the authors the text would be improved (e.g. shorter sentences). In conclusion, I would increase my score.",
            "summary_of_the_review": "The paper mainly questions the effect of sparsity and soft-thresholding introduced by ReLU. The authors show that the classification improvements by eliminating spatial within-class variabilities come from a phase collapse. This counts as a contribution compared to related works. The hypothesis is clearly defined and supported using theory (and assumptions) and experiments. However, there are pitfalls that need to be addressed. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This is a very interesting paper that is based on scattering networks, for which it shows that the so called ‘phase collapse’ leads to state-of-the-art results on par with modern architectures, like ResNet. To derive this, the paper shows that having neural networks on complex numbers is similar to a structure deep network (like with wavelet filters) on the reals. Phase collapse is then when there is an operation, like the modulus (absolute value function), which eliminates the phase from the complex number and maintains only the amplitude. All in all, the paper shows that maintaining the amplitude while eliminating the phase, is what brings the high accuracies, while the reverse (keeping the phase, eliminating the amplitude) yields terrible accuracies. As a corollary, I would say that the paper explains why non-linearities, like ReLUs, are so successful.",
            "main_review": "The strengths of the paper are:\n- I find the novelty and the results, both theoretical and empirical, fascinating and thought provoking. Making a link between operators like ReLUs to complex numbers, their phases, and collapse is a great idea. In particular, I find the results in Table 2 very convincing.\n\n- The theoretical analysis is nice, and not to hard to follow, once one commits. It would be interesting to make more explicit the analysis for different types of nonlinearities. For instance, if sigmoid are soft-thresholding functions, I suppose the same is also for tanh. However, tanh tends to work better. Why is that the case, under this framework? What about other non-linear activations, like the swish function?\n\n- The results are generally strong and quite convincing, at least in ablation comparisons.\n\nThe weaknesses of the paper:\n- I found the writing involved, if not subpar for the quality of the paper. It is clear that the authors have a very good understanding of the area, however, they do not make it very easy for the average reader to understand their thought process, The sentences are not really immediately connected, that is there exist jumps, which sometimes can be inferred based on follow-up text, while other times they are simply hard to understand. It could be that for readers who are better versed in the literature, the text is easier. However, any paper should do a minimum effort to communicate the message clearly. Certainly, the introduction contains several forward references and that at least can be improved.\n\n- I found section 2 particularly hard to follow, as various concepts are being introduced at random points, without any clear structure. Almost ironically, I couldn’t find any direct definition of what comprises phase collapse, which made matters all that much harder. While it is sort of expected/intuitive what phase collapse could mean, given that the paper is on the theoretical/conceptual side of things, it makes it hard to follow the thought process. \n\nThe only ‘definition’ I found was that phase collapse eliminates the phase of a complex number with a modules just below eq (3). It is not clear if this a general definition, just a logical deduction based on fundamental concepts, or something that the paper introduces. In any case, it is also unclear why this holds. I assume it is because the modulus of the exponential is 1, so the phase that is introduced by the exponential is eliminated?\n\n- It is also unclear how equation (5) is derived. Again, is it a logical deduction based on the nature of the ReLU? Or is it derived based on the aforementioned definitions of filtering with wavelets and complex numbers? It should be immediately clear from the text what is expected to be background knowledge, what is derived knowledge, what is a simple deduction.\n\n- From a technical point of view, what I found unclear was what is the point of stacking ‘phase collapse’ operators, since one of them is supposed to remove all the phases from the feature, that is, it learns translation invariant features. From deep networks, I assume that deeper layers combined the wavelet activations from the previous layers. However, I find hard to reconcile this with ‘translation invariance’. For instance, a full connected layer is translation invariant, while a convolutional layer is translation equivariant. Does phase collapse then correspond to fully connected layers, and if yes, what is the point of stacking them? After all, with fully connected layers it is also of little added value to stack them, usually.\n\n- Complementing on the previous point, can it be that adding the skip connections is beneficial such that to make the operations also translation equivariant, thus not completely eliminate all phases and translation information? Something like that is analyzed in the bottom of page 5, but maybe this point can be furthered.\n\n- I find the motivation of the projection operators P a logical one in the context of deep networks, however, not really convincing in the context of phase collapse scattering networks. Do they contribute anything to the amplitude or the phase? What would be the benefit, considering that all phases are anyways immediately eliminated afterward? Do they help with having class means that are well separated perhaps? Maybe there can be a motivation that relates more to the main story of the paper.\n\n- Last, I would like to clarify that in my attempt to find out more about ‘phase collapse’ as a concept, I found out the arXiv version of the paper accidentally. I got a bit annoyed by that, as I believe it biases my opinion, and I do not find it fair against other papers.",
            "summary_of_the_review": "Based on the novelty and the strong results, I vote for acceptance. I have one remark that I couldn’t completely understand and it would be nice if the authors could help me there. All in all, I find that I learned something from this paper, which I think is a great result for any publication.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors present some theoretical results, followed by some experiments, in support of their argument that the linear separability for image classification in deep convolutional neural networks mostly relies on a phase collapse phenomenon. By eliminating the phase of zero mean filters they improve the separation of class means.They present a Phase Collapse Scattering network and demonstrate Resnet-like accuracy. The authors argue that phase collapse are both necessary and sufficient to discriminate class means on complex datasets.",
            "main_review": "Strengths\n-The paper is interesting in that it presents theoretical results followed by solid experimental results related to the theoretical results demonstrating that the phase of the signal can be removed while improving the classification performance of DNNs\n-The paper is for the most part well written\n-The authors present good results on CIFAR10 and Imagenet. While the results do not seem state of the art I think they are sufficiently good in support of the authors' arguments\n\nWeaknesses: \n-Figure 1:The authors assume color printers are used (orange/blue). I suggest they use bold/solid, dashed lines to distinguish between the different steps\n-Page 2: clarify why both directions (necessary & sufficient) hold. I see how the sufficient condition holds but it is not clear how the necessary direction emerges. Does it make sense to phrase this as a conjecture?\n-Eq 4: It may help many readers to derive this in the appendix, it should be a quick derivation\n-The ResNet results while good are not state of the art. If SOA feasible using the PCS framework? If yes why wasn't a SOA presented in the paper. If not, what do the authors believe hinders achieving SOA?\n-Page 3: Clarify better when the random vector X_y represents. Does it represent the features used by the classifier? In that case I do not see how the classes are linearly separable if E[X_y] are all different. Do you mean this is a sufficient or necessary condition? For example if X_1, X_2 are 2D vectors, each representing two concentric circles. The two sets of features represent different classes but the means are the same (the circle center). Obviously we can have E[X_y] with the same mean but represent different classes. Limiting ourselves to linear separability makes the problem trivial. I suggest the author rewrite this section to clarify what they mean\nPage 3: The authors make a lot of assumptions that X_y are stationary. How important is this assumption in their results? Do they think perhaps that a significant set of problems, such as those often dealt with by RNNs, are not well modelled as stationary?",
            "summary_of_the_review": "Overall I found this an interesting paper. It is well written, and has a fairly solid experimental section in support of some not so well known theoretical ideas as to why deep neural nets achieve good classification\nOverall here is my summarized assessment:\n\noriginality: The authors present some theoretical results to justify their experimental framework. The paper is not just blindly attempting different DNN architectures. From this point of view I consider this a fairly original piece of work\n\nquality: I believe the exposition and experimental results are of sufficient quality for ICLR. The paper is in general well written, though as I indicate elsewhere certain parts need to be improved. \n\nclarity: I mentioned above in the \"Weaknesses\" section some clarity issues that I noticed. The authors should address these issues. I was ambivalent as to whether the paper should be rejected because of these clarity issues, but since they can all be addressed I think I decided to be a bit lenient. But for the final ICLR submission they should be addressed.\n\nsignificance: Unless I missed it, are the authors providing source code? The paper's significance and clarity will be improved if source code is provided, to make the results reproducible",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes that using phase modulus operators instead of thresholding non-linearites is beneficial to classification performance in the case of scattering like networks. The authors show that such a network with some learnable operators can come close to performance of small ResNets when they use the modulo operators and performance degrades when using other non-linearieis. This is demonstrated on ImageNet and CIFAR-10 and is accompanied with relevant analysis and theory. ",
            "main_review": "I think the paper addresses some interesting questions and it is somewhat interesting that a network with a fixed wavelet basis can achieve reasonable performance on ImageNet classification. I also think the paper provides quite a bit of theory to support some of the claims.\n\nHowever I feel the paper suffers from several weaknesses:\n\n* It is not clear to what is the purpose of the proposed model? if it is to show that such a model can indeed be comparable to current architectures then OK, but it does not go beyond that. I don't feel there is a lot \"actionable\" conclusions here nor that we learned anything fundamental about how these models work.\n\n* There is almost no discussion as to what the learnable operators P do - what happens if they are not used? how do they relate to other parts of the model and, more importantly, to what ResNets learn?\n\n* Why are the resulting networks so parameter heavy? they have many more parameters when compared to the corresponding ResNets and perform worse. What can be done?\n\n* How does the proposed model scale? with the number of layers? number of activations at each layer? etc.",
            "summary_of_the_review": "I think this is a potentially interesting paper but I feel it leaves quite a bit to be desired. \n\npost author response update:\n\nI have read the other reviewers comments as well as the author responses - it would seem that I have been harsh in my scoring and this has been revised. I think this is a valuable contribution to our understanding of the field and a nice demonstration that fixed wavelet filters can, under the right circumstances, perform as well as learned networks. My confidence in the score still remains low as I feel was not the right person to review this.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}