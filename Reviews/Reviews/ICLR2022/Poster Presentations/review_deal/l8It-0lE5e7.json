{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The paper is a nice addition to the developing theory of implicit bias in neural training. While the results are somewhat expected, the technical aspects are fairly involved due to the adversarial component."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "*Implicit Bias of Adversarial Training for Deep Neural Networks*\nexplores how minimizing the exponential loss (i.e., $l(x) = e^{-x}$)\nof a homogeneous neural network (i.e., a neural network such that $$\nf = a_L W_L \\circ \\sigma_L \\circ \\cdots \\circ \\sigma_2 \\circ a_1 W_1 =\n\\prod^L_{k=1} a_k^c (W_L \\circ \\sigma_L \\circ \\cdots \\circ \\sigma_2 \\circ\nW_1 $$ for activation functions $\\sigma_L, \\ldots, \\sigma_2$, weights\n$W_L, \\ldots, W_1$, $a_L, \\ldots, a_1 > 0$, and $c \\geq 1$) on samples\nwith perturbations that maximizing the loss influences the optimized\nneural network's weights. Specifically, this paper proves that, for an\nexponential loss and a multi-c-homogeneous neural network, the limit point for $\\frac{W}{\\lVert W \\rVert}$\nwith respect to the gradient flow \n$$\n\\frac{dW}{dt} = - \\left( \\frac{d\\tilde{\\mathcal{L}}}{\\partial W} \\right)^T\n$$\nof the adversarial training objective\n$$\n\\tilde{\\mathcal{L}} = \\frac{1}{n}\\sum^n_{i=1}\\ell(x_i + \\delta_i(W), y_i)\n$$\nunder $\\ell_2$-FGM, FGSM, $\\ell_2$-PGD, and $\\ell_\\infty$-PGD is along the Karush-Kuhn-Tucker (KKT) point \nof the constrained minimization problem\n$$\n\\min_{W_1, \\ldots, W_L} \\frac{1}{2} \\lVert W \\rVert^2_{\\ell_2} \\text{ s.t. } \\tilde{\\gamma_i} \\geq 1\n$$\nwhere $\\tilde{\\gamma_i} = y_i f(x_i + \\delta_i(W))$ and $W = (W_1, \\ldots, W_L)$.\n\nThis theorem demonstrates that---for a class of neural networks and adversarial perturbations---adversarial training has\nan implicit bias that can be expressed in closed form. This result provides an important contribution to understanding \nhow adversarial training improves adversarial robustness.\n",
            "main_review": "*Implicit Bias of Adversarial Training for Deep Neural Networks*\nanswers a prevalent question in the theory of adversarial robustness:\nhow exactly (in closed mathematical form) does adversarial training\nimprove adversarial robustness? The paper proves a theorem stating\nadversarial training produces an implicit bias on the normalized\nweights $\\frac{W}{\\lVert W \\rVert}$ (with a more precise statement in\nSummary Of The Paper). The majority of the paper is clearly written\nand correct. The paper's results place a milestone in the theory of\nadversarial robustness. However, the proof of Theorem 2 may have\npotential errors (which may be from my confusion on some statements\nand notation). Adversarial training in linear neural networks is a corollary to Theorem 5,\nso any potential errors in Theorem 2 do not invalidate Theorem 5's results.\n In addition, there are several statements and notation\nthat are vague and ill-defined in the theorems. This makes the exact\nstatement of the theorems difficult to ascertain.\n\n## Potential errors in the proof of Theorem 2\n\nLemma 5 requires a constant step size $1/\\beta(r, \\bar{r}, \\epsilon)$\nfor\n$$ \\beta(r, \\bar{r}, \\epsilon) = r^{3L}L^2\\left[ \\alpha + \\beta +\n\\frac{\\epsilon}{\\bar{r}^L} \\left( 2\\alpha + \\beta + frac{\\alpha\n\\beta}{\\bar{r}^L} \\right) \\right].$$\nThe constant step size implies in Lemma 5 that\n$$\\max_k \\lVert W_k(t) \\rVert > r(t)$$\nfor some $t$. However, in Lemma 2, the step size $\\eta(t)$ is not constant where\n$$\\eta(t) = \\min \\{ 1, \\beta(r(t), \\bar{r}(t), \\epsilon) \\}$$\nand\n$$r(t+1) = r(t) + \\mu(t)$$\nfor $W(t+1) \\not\\in \\mathcal{S}(r(t) - \\mu(t))$. As the $\\eta(t)$ decreases whenever \n$W(t+1) \\not\\in \\mathcal{S}(r(t) - \\mu(t))$, the step size $\\eta(t)$ \ndecreases at $\\mathcal{S}(r(t) - \\mu(t))$ and not at $\\mathcal{S}(r(t))$.\nIt is not obvious that Lemma 5 holds as the step size is not constant in $\\mathcal{S}(r(t))$.\nThis is particularly important as a Lemma 6 and Theorem 2 assume that\n $\\max_k \\lVert W_k \\rVert_F \\to \\infty$. Could you please provide a short proof\nthat $\\max_k \\lVert W_k \\rVert_F \\to \\infty$ under the assumptions of Theorem 2?\n\nIt is also unclear how statements under\nEquation 39 hold as a result of $\\max_k \\lVert W_k \\rVert_F \\to \\infty$. Why does \n$$U_k \\Sigma_k \\Sigma_k^T U_k^T \\to V_{k+1}\\Sigma^T_{k+1}\\Sigma_{k+1}V^T_{k+1}?$$\nWhy does this imply $\\Sigma_k \\Sigma^T_k$ and $\\Sigma^T_{k+1}\\Sigma_{k+1}$ are\n\"approximately the same\"? How is \"approximately the same\" defined? Why do all layers \nhave rank 1?\n\n## Vague language, ill-defined statements, and undefined definitions\n\nThe paper suffers from several instances of vague language. For\ninstance, \"alignment phenomenon\" on pages 6, 15, and 17 are not\ndefined in the paper. Although I can figure out your intended\ndefinition, it makes Theorem 2's precise statement difficult to\nascertain.  Other instances include \"approximately the same\" on page\n15 and improper use of limits on Equation 52.  In addition, the\nassumptions in the theorems sometimes do not match the proofs.  The\nproof of Theorem 2 assumes only a logistic loss function while Theorem\n2's statement assumes a broader class of loss functions.\n\n## Limited experimental results\n\n**These comments did not impact my review recommendation.**\n\nIn many theoretical papers, experimental sections typically\nquantitatively measure the difference between general theoretical\nstatements and common real-world cases. For example, experiments show\nhow theorems in compressed sensing deviated from typical use cases. In\nthe paper, a particular area for improvement is the experimental\nsection. This section show the training accuracy and the normalized\nmargin. Both plots will clearly increase as a result of adversarial\ntraining and do not add any useful information to the paper. Do the\nweights' singular values diverge to infinity in practice? Does Theorem\n5's statement on the normalized weights still occur when you use other\nlosses and neural networks or remove Assumptions 2 and 4?\n",
            "summary_of_the_review": "*Implicit Bias of Adversarial Training for Deep Neural Networks*\ncontributes a milestone result to the theory of adversarial\nrobustness. The majority of the paper is clearly written and\ncorrect. However, the Adversarial Training for Linear Neural Networks\nportion of the paper has several flaws: potential errors, vague\nlanguage and ill-defined statements (such as \"approximately the same\"\nand improper use of limits in Equation 52), and use of undefined\nnotation and definitions (such as alignment phenomenon). Nevertheless,\nthe contributions are significant and novel, and the paper would\nreceive an accept if Adversarial Training for Linear Neural Networks\nis amended or removed. \n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper characterizes the bias of adversarial training toward specific minimum-norm solutions or KKT points of a particular optimization problem. Their results generalizes the work of Li et al 2020 by proving the directional alignment with the adversarial max-margin solution for deep linear models for L2 perturbations (Theorem 2) as well as convergence in direction for homogenous networks for L2 FGM, FGSM, and L2, Linf PGD perturbations (Theorem 5).",
            "main_review": "Strengths:\n- The results are novel and extend prior theoretical results.\n- To the extent I have verified, the proofs are correct.\n\n\nMinor comments:\n- Theorem 5: one limitation of this result is that it depends on the adversarial perturbation as part of the constraints in Eq 19. That is in comparison with related results of Li et al 2020 and Faghri et al 2021 that their characterizations make the difference between the solution for various Lp-norm perturbations clear. Understandably, Theorem 5 is a more general result for homogenous models but it would still be useful to derive prior linear results as corollaries of Theorem 5.\n- Assumption 4 can easily be false for large perturbation sizes. The footnote says similar assumptions have been made in prior work but those were not about separability of adversarial examples. Can you provide more justification for this assumption?\n- Figure 1: This is an interesting plot confirming the increase in adversarial margin. Can you plot FGSM and PGD on the same plot? I understand that the adversarial margin for the two is different because the corresponding optimization problem is different. However, a natural question is, is there a relation between the two problems?\n- Page 9, Trade-off between standard and adversarial accuracy: I’m not sure I understand the theoretical argument of this part. Is there a concrete result based on Theorem 5?\n- Section 4: Have you verified these results for varying epsilon size (other than 16/255)? How about other network architectures? Is there a challenge in doing so?\n",
            "summary_of_the_review": "This paper makes a solid theoretical contribution. It could be improved with more empirical verification of the results.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper studies the adversarial training problem under deep linear network classifiers and standard L_2 and L_\\infty perturbations. The paper's main result suggests that in the linearly separable case the adversarially trained model via gradient descent will asymptotically converge to the max-margin solution. Some extensions of this result to homogenous neural networks with exponential loss function have been provided. The paper also performs some preliminary numerical experiments to support the theoretical results.",
            "main_review": "This paper focuses on the convergence behavior of adversarial training methods. The paper tries to show the implicit bias of adversarial training in a simplified setting with a deep linear neural network and linearly separable data for a binary classification problem. Under this setting, the paper proves the gradient descent algorithm will converge asymptotically to the max-margin solution. Later, the paper extends this result to homogeneous neural network functions with the exponential loss function and a separation assumption in Assumption 4. Overall, the paper targets an interesting question and proves some useful results on the behavior of adversarial training problems. However, I have some comments on the assumptions made to simplify the analysis, some of which seem to be quite restrictive and limit the results' application to real adversarial training problems.\n\nRegarding the paper's theoretical setting, I think some of the assumptions are quite strong. In section 3.1, the analysis is limited to deep linear networks and linearly separable data in binary classification. Also, the convergence result is an asymptotic guarantee which does not bound the iteration complexity of finding the max-margin solution. The convergence guarantee to the max-margin solution also holds for the standard training algorithm, which questions whether the result can distinguish adversarial training methods from standard training algorithms.\n\nWhile the results in section 3.1 are written clearly, I think section 3.2 lacks a clear presentation and puts several limiting assumptions. First, the gradient steps are replaced with gradient flow which is inconsistent with real adversarial training experiments. Also, the exponential loss function used for theoretical analysis is not used in practical adversarial training experiments. It is not clear whether the results can be extended to standard cross-entropy and squared-error loss functions in deep learning classification problems. Also, Assumption 4 on the separability of adversarial examples is pretty strong and essentially assumes the adversarial training method finds a perfect solution at some iteration t_0, which is too strong given that the paper wants to study the convergence behavior of adversarial training. Therefore, I think the assumptions are too restrictive for a real adversarial learning setting and significantly limit the application of the results to practical deep learning experiments. I will look forward to the authors' responses regarding the reasoning behind these assumptions to give my final score.",
            "summary_of_the_review": "While the paper shows some insightful results on the convergence of adversarial training for deep linear networks, the assumptions for the analysis of deep nonlinear networks seem too restrictive to me. Also, replacing the gradient steps with the gradient flow seems incompatible with standard adversarial training experiments. The paper would become much stronger after relaxing some of the assumptions and performing the analysis for the actual gradient descent algorithm rather than considering the gradient flow.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper aims to understand the training results of adversarial training, and proves that under certain conditions, adversarial training results maximize the margin for the adversarial training samples. Similar results have been observed in the cleaning training of DNN, this paper's contribution is to extend them to the adversarial training settings. The paper seems technically sound (although I don't have the time to go over all the appendix). The results, to be honest, are not surprising, given previous works on standard training. But I believe the rigorous justification presented in the paper is of importance.",
            "main_review": "Some minor concerns:\n1. the notation of loss function is abused. The loss function in (6) takes both x and y as the input, while in Assumption 1, the loss function only takes one argument. I suppose the loss function in (6) means l(x,y;W) = l' (f(x;W)*y) where l' is the loss function in Assumption 1.\n2. I don't quite understand the remark at the top of page 9. LHS of (20) is about the prediction of the clean test sample, the RHS of (20) is about the fitting of the adversarial training sample. How does this inequality relate to the trade-off between robustness and accuracy? Please add more discussion.\n3. Page 21, after eq (81), \"and Lemma 4 can be applied to \". It should be \"and Lemma 9 can be applied to\".",
            "summary_of_the_review": "The theoretical contribution of this paper is good to me (unless other reviewers find technical errors in the proof). I think it is a good supplement to the current theory of adversarial DNN training. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}