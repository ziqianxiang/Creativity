{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper present a way to fully binarize a BERT model. The authors convincingly demonstrate that a naive binarization results in large quality losses and then propose amendments. It is pretty impressive that it is possible to get a fully binarized model to work at all.\nAt the same time, the quality losses are still significant and in practice one might prefer to use distillation (as long as the hardware doesn't require binarization). One could also envision combinations of the proposed technique (perhaps in the 1-1-4 setting) with distillation."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a full binarization of BERT (including 1-bit activation), called BiBERT. The key observation of the paper is that binary activation becomes a big challenge for the attention weights in self-attention. To solve this problem, the paper proposes a new attention binarization approach that maximizes the information entropy. The paper further proposes a Direction-Matching Distillation (DMD) scheme which encourages the similarity pattern matrices of keys, values, and hidden states in the original BERT and BiBERT to be minimized. Experimental results on GLUE datasets show that BiBERT significantly outperforms other models under the full binarization scheme.",
            "main_review": "The paper proposes a full binarization of BERT (including 1-bit activation), which is a very challenging task. The proposed bi-attention for maximum information entropy is a very interesting design for fully binary attention models. My question here is in section 2.1, when analyzing the effect of each distillation term, whether the authors used a fully binarized attention map which only contains 1, just like the Figure 4(b). In that case, I don't understand the meaning of attention distillation in Figure 3 since the binary attention map is fixed.\n\nIt is also interesting to see that the activation distillation can cause the optimization direction when the quantization becomes more aggressive, which well motivates the proposed direction-matching distillation term.\n\nThe experimental results are solid and impressive. In the 1-1-1 setting in which the paper mainly focused, the proposed BiBERT model achieves significant improvement over the 1-1-1 baseline model.",
            "summary_of_the_review": "The paper is well-motivated, novel, and well supported by the experiments.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper focuses on the full binarization of BERT. Since current pre-trained models achieve promising results on NLP tasks, it is a trend to apply these models to real-world applications with constraint computation resources.  Quantization is one of the prime choices to reduce computation requirements. In recent years, BERT quantization has made significant progress. However, training binary BERT is still an important but challenging question. This paper finds that the worse performance of binary BERT can be mainly attributed to information degradation and optimization direction mismatch respectively in the forward and backward propagation. To address these problems, the authors propose two solutions, Bi-Attention and Direction-Matching Distillation. Experimental results on GLUE benchmark demonstrate that the proposed work outperforms several popular baselines. Also, ablation studies verify the effectiveness of the proposed solutions.",
            "main_review": "Strength:\n\nThe idea is well-motivated and easy to follow. The paper starts from information theory and focuses on the mutual information between binarized and full-precision representations. Considering that the ideal binarized representation should preserve the given full-precision counterparts as much as possible, it is very natural to maximize the mutual information between two representations. \n\nThe authors also find that the direction mismatch hinders the accurate optimization of the fully binarized BERT. This finding is interesting and well-motivated. \n\nExperiment results show that the proposed approaches achieve much better results than previous baselines. \n\nWeakness:\n\n I have noticed that [1] reported higher results in their paper.  It would be better to explain why the higher results are not reported in this paper. \n\nThe work of BinaryBERT uses 1-bit weight quantization and 4-bit activation quantization. It not only achieves better results than the proposed approach,  but also almost reaches the results of the full-precision model.  Since BinaryBERT and BiBERT have similar parameter sizes, the empirical contribution of BiBERT is a little bit marginal to me. \n\n[1] BinaryBERT: Pushing the Limit of BERT Quantization\n",
            "summary_of_the_review": "It is a well-motivated paper presenting an alternative understanding of the challenges of training binarized BERT. The empirical contribution is a little bit marginal since there is a large gap between the proposed approach and the best result in [1].",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes to binarize both the weights and activations of the BERT model. The authors find that binarizing MHA causes the most significant accuracy drop and simple distillation on MHA by minimizing the MSE between the attention scores of the full-precision model and the quantized model can harm the performance sometimes. Thus the authors propose new binary representations that maximize entropy and distillation methods that maintain direction. Empirical results on the GLUE benchmark show that the method achieves good performance \nwhen the weights are binarized and the activation is binarized/ternarized.\n",
            "main_review": "The authors stated that the bi-linear quantization function in Equation (2) is motivated by Rastegati et al., 2016 and Qin et al., 2021.  However, (2) is quite different from the quantization methods in both papers. Specifically,\n- in Rastegati et al., 2016, both the weights and activations are not zero-meaned before quantization and have scaling factors. \n- in Qin et al., 2021, the weights are both zero-meaned and uni-normalized. \nCan the authors specify in more detail the derivation of (2), and the connection/difference with the two papers?\n\nThe theory part also requires some more clarifications. Specifically,\n- Theorem 2 is based on the assumption that B_Q and B_K are entropy maximized. However, it is not clear why this assumption holds. \n- Even though the softmax function is order-preserving, maximizing the entropy of sign(A - \\phi(\\tau, A)) is not equivalent to the same as the original problem in (9).\n- Moreover, given the implication from Theorem 2 that the optimal \\phi(\\tau) is zero,  the binarization of the attention scores  sign(A - \\phi(\\tau, A)) =  sign(A) does not equal bool(A) in equation (11).\n\nThe experiments also require some more clarification. According to the original papers of BinaryBERT and TernaryBERT, the activations are only quantized to as low as 4 bits. However, in Tables 2-3, activations of these two methods are quantized to 1 or 2 bits. How are the activations quantized for these two methods?\n\nMinors:\n- Figure 3: missing the full stop at the end of the caption.",
            "summary_of_the_review": "This paper is overall structured clearly and easy to follow. However, the motivation, theory part and experiments require some further clarifications.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces an accurate fully binarized (i.e., 1-bit weight, embedding, and activation)  BERT, BiBERT, as a robust model compression method. It saves 56.3× and 31.2× on FLOPs and model size for real-world devices. Moreover, it addresses the substantial performance issues in the straightforward full binarization method. To tackle the performance drop issues, it proposes Bi-Attention and Direction Matching Distillation (DMD) mechanism.  The paper contains an evaluation of the proposed model on GLUE benchmark and a comparison with SOTA quantized BERT models.\n",
            "main_review": "Strengths:\n1. The paper is well constructed and easy to go through the idea; encompasses rigorous theoretical justification of the approaches.\n2. This paper addresses the crucial problems in fully binarized BERT and introduces BiBERT which is the very first attempt to implement an accurate fully binarized BERT.\n3. In this framework, they incorporate a Bi-Attention approach which maximizes the information entropy to mitigate the information degradation. And a Direction Matching Distillation (DMD) mechanism which resolves the optimization direction mismatch.\n4. BiBERT also impressively obtain savings on FLOPs and model size which is a great advantage for real-world edge devices.\n5. It also contains an ablation study on the GLUE benchmark which validates the contribution of the individual components.\n\nWeaknesses:\n1. The proposed method didn’t properly address the performance drop due to the binarization of activation. Results on the GLUE benchmark showed a drastic performance drop in comparison to the related approaches and full precision BERT.\n2. In sec 2.1, it is evident that a stochastic sign function is used to binarize the BERT, therefore, why the deterministic function is used in sec 3.2.1 is not clear.\n3. In table 1,2, 3, we see that data augmentation method has been utilized for experimental purposes. But it is not properly addressed/clarified in sec 4.1 and 4.2 why it is employed and what is the significance of this approach here.\n4. Also, these two closely related works should be critically analyzed in the experiment section:\n- Bai, H., Zhang, W., Hou, L., Shang, L., Jin, J., Jiang, X., ... & King, I. (2020). Binarybert: Pushing the limit of bert quantization. arXiv preprint arXiv:2012.15701.\n- Zhang, W., Hou, L., Yin, Y., Shang, L., Chen, X., Jiang, X., & Liu, Q. (2020). Ternarybert: Distillation-aware ultra-low bit bert. arXiv preprint arXiv:2009.12812.\n5. Figure 2, 3, 4 are not mentioned in appropriate sections to help understand the analysis.\n\nSome minor issues and typos:\n1. There is no full stop(.) at the end of each caption of the figures and tables.\n2. At the end of the 2nd paragraph of the Introduction: “So far, previous studies have pushed down the weight and embedding to be binarized, but none of them have ever achieved to binarize BERT with 1-bit activation accurately.” -> please refer to the specific previous studies.\n3. Section 2, first paragraph: BRET -> BERT\n4. Section 2.2: which can be unobstructed applied -> which can be unobstructedly applied\n5. Appendix B.2: There is no citation for DynaBERT.\n",
            "summary_of_the_review": "This paper portrays the related performance issues of straightforward fully binarized BERT and introduces the idea of BiBERT as a solution. The paper has proper experimental proofs and analysis to justify the framework. It might be a positive inclusion towards an accurate fully binarized BERT architecture. The proposed method obtained impressive savings on FLOPs and model size. However, it seems that the related SOTA approaches are missing in the comparative performance analysis section, and I would really be looking to see this in a conference paper that meets ICLR's high standards.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper addresses the problem of fully binarizing BERT model including the network weights, embeddings and activations. Through theoretical and empirical analysis, the authors find: 1) the direct binarization of softmax-ed attention matrix is problematic; 2) distillation of the attention scores from the full-precision teacher can be harmful to the binarized model. Therefore, they propose two different methods to alleviate the issues. One is the Bi-Attention which directly quantizes the attention matrix and excludes softmax. The other is Direction-Matching Distillation which chooses to distill the covariance matrices of query/key/value rather than the attention matrix. Experimental results on GLUE benchmark show that BiBERT model outperforms previous quantized BERT models on the full binarization setting.\nThis main contribution of the paper is that it is the first work on full binarization of BERT models. With detailed analysis and observations, it proposed two effective methods to improve the training of the binarized model, and achieve good performance on some of the NLU tasks.",
            "main_review": "Overall, the paper is well-motivated and easy to follow. To the best of my knowledge, this is the first work on full quantization (weights-embedding-activation) of the BERT model to 1-bit (denoted as 1-1-1), considering that previous work such as BinaryBERT only achieves 1-1-4 quantization. Binarizing all the activations without losing much performance is challenging. By making detailed analysis of the performance variances in binarizing intermediate outputs at each step and the different distillation losses, the authors find the major problems that hurt the model performance most. Two new methods are proposed to avoid or alleviate the problem in training the quantized model, which are theoretical sound and shows substantial improvements to the accuracies on downstream tasks. The proposed BiBERT model also surpasses the previous BERT quantization model on the setting of 1-1-1 quantization.\n\nThe weaknesses of the BiBERT model are as follows: \n1) The performance of BiBERT on GLUE is still far behind the full precision model (67.0 vs 82.3). This large performance gap may hinder application of the BiBERT model in many real scenarios (e.g., CoLA, STS-B tasks). While the BinaryBERT with 1-1-4 quantization, which has similar model size and a little bit higher computation as the BiBERT, can achieve GLUE score of 81.9 with almost neglectable performance loss. This raises the question why full binarization of the BERT model is necessary or preferred. One possible way to justify this is to enlarge the network size while keeping it binary and see if the performance can consistently improve.\n2) In the experiments, the authors only report BinaryBERT (1-1-2 and 1-1-1) and TernaryBERT (2-2-1 and 2-2-2) which are not present in their original papers. The author should clarify in the paper how the results are obtained. Are the results from reimplementation of their algorithms or based on some open-source code/models? Are the reimplementation correct? On CoLA task, BinaryBERT and TenaryBERT simply seem to diverge, and the baseline quantization method without Bi-Attention and DMD also outperform BinaryBERT and TenaryBERT.\n3) The author report Q2BERT and Q-BERT on the 2-2-8 setting, while the TernaryBERT have much higher performance (72.9) on it. Why not report these results?  \nOverall, I think the BiBERT is still not applicable to some NLU tasks given severe performance drop. Some of the experimental results are not convincing (or biased) regarding the baseline of BinaryBERT and TenaryBERT.\n",
            "summary_of_the_review": "The paper tries to tackle a challenging problem of binarizing BERT and find effective ways to improve the performance. However, the model performances on some tasks are still far from good and comparison in the experiments are not convincing. Therefore, my current recommendation is weak reject unless the author can clarify the concerns.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}