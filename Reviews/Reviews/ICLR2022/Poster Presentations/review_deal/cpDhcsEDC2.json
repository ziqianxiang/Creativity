{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "The focus on this paper's proposed FILIP method is to perform word-patch alignment by token-wise similarity matrix through cross-modal late interaction by modifying only contrastive loss, leading to training and inference efficiency. The authors also collected FILIP300M, a large-scale cleaned image captioning dataset for FILIP’s V-L pre-training. FILIP achieves strong performance on zero-shot image classification and image-text-matching tasks, and the paper also visualizes the ability of fine-grained (visual-textual token) classification and localization. Overall most of the reviewers appreciated the idea and the generalization results, but had some concerns about not enough technical novelty over the Khattab and Zaharia Colbert paper, which this paper adopts for multimodal tasks. Some reviewers also had concerns about the dataset release but the authors promise to address this. Some reviewers were also not fully convinced about the high storage requirements and scalability for some of the retrieval tasks that the authors tested.\n\nNOTE: The authors are also asked to describe any ethical considerations or issues that arise in their large scale dataset collection in the camera ready version of the paper, see https://arxiv.org/abs/2110.01963 for examples."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes to utilize the fine-grained alignment between visual tokens and text tokens in the contrastive loss for language-image pertaining. More specifically, the similarities between images and captions are calculated by averaging the token-wise maximum similarities. The experiments on both zero-shot image classification and image-text retrieval with different pre-trained datasets validate the effectiveness of the proposed model.    ",
            "main_review": "Strengths:\n1. Strong performance on zero-shot classification and image-text retrieval, compared with original CLIP. A fair comparison with CLIP pre-trained on the YFCC dataset.\n2. The investigation on token-level interaction to provide a more fine-grained contrastive signal in Language-Image Pre-training is interesting and meaningful for future research in this direction.\n\nWeakness:\n1. Novelty in total is limited. The proposed cross-modal late interaction is inspired by [1], but the three modifications mentioned in the paper seem to be a bit trivial. And image and text augmentation are widely used in previous works.\n2. The study on token-level interaction is not comprehensive in terms of methodology. The paper only uses max-mean late interaction without exploring different variants. For example, how will the mean-mean, max-max, or mean-max interaction work? One step further, how about we ensemble the token features inside each modality with a set of learnable weights and then compute the similarity between ensembled features?\n3. One type of experiment is missing - linear probing on the visual encoder. This is to prove the transferable representation learned by Language-Image Pre-training. \n4. The authors do not mention whether to release their collected dataset FILIP300M. If not, then it is hard to fairly compete for researchers interested in this direction. \n\n\nRef:\n[1] Omar Khattab and Matei Zaharia. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In International ACMSIGIR Conference on Research and Development in Information Retrieval, pp. 39–48, 2020",
            "summary_of_the_review": "The main drawback of this paper is that the proposed model is not that novel in terms of methodology. But this paper provides an angle to look at the token-wise interaction in contrastive Language-Image pre-training. Moreover, the performance is quite impressive and the experiments are conducted on different pre-training datasets and downstream tasks to prove the model's efficacy. Overall, I think it is marginally above the threshold. I may change my mind based on the authors' rebuttal and other reviewers' feedback.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes an approach for large-scale vision-language representation learning that focuses on achieving fine alignment between modalities through a late interaction strategy.  Experiments on zero-shot image classification and image-text retrieval tasks show that the proposed approach is able to achieve state-of-the-art performance. A large-scale image-text dataset is also constructed from the web for pre-training.",
            "main_review": "Strengths: \n\n1. The paper is quite well written and it is easy to follow the authors.\n2. The approach is simple yet efficient. It seems natural to extend the prior clip and align the approach with fine-grained interactions.\n3) New large-scale image-text pair dataset called FILIP300M would be valuable to the community. \n4) Experiments show strong performance on vision-language tasks.\n\nWeaknesses/Concerns:\n\n1) Model novelty is limited considering (I) similarity to  Khattab & Zaharia, and (ii) prior Vision-Language Pre-training models. \n\n2) On a similar note to the previous point, the paper states that they follow Khattab & Zaharia (2020). The changes to the proposed compared to Khattab & Zaharia seem trivial and nothing especially novel except applying to a new task. I would request the authors to discuss this in further detail to make the paper contribution clearer. The only thing the paper states 'we discard the padded tokens and use average instead summation of token-wise maximum similarities when computing the image-text alignment, which enhances the cross-modal representation learning and stabilizes training. Furthermore, we construct a large-scale pre-training dataset named FILIP300M from the Internet.'  -- This seems a minimal change. Also, where are the experimental results comparing the paper and supporting these arguments - enhanced learning and stabilized training?\n\n3) Ablation: The paper states that the use of object detector-based approaches leads to scalability issues in training vision-language models. I do not expect that to create a significant issue based on prior experience. Did other papers report also the same? Or, Did the authors' experiment and find the issue? How much impact it has?\nOn a related note, it is also important to see - How the performance changes when a faster rcnn based detector is used to understand the tradeoff?\n\n4) As the paper is interested in efficient inference, it is important that the paper include inference time for compared methods, especially in Table 2 and Table 3.\n\n5) An approach to get fine-grained interaction between visual and textual modalities avoiding the problem of pre-computing features offline), is not novel in general. As far as I am aware, such ideas are especially not uncommon in video-text retrieval. For example, prior work(Weakly Supervised Video Moment Retrieval From Text Queries, CVPR 2019) adopts a late visual-language interaction strategy with weighted pooling for video-text retrieval. However, the proposed fine-grained interaction approach seems to be new with recent transformer-based vision-language models.\n\n6) MInor: Did the authors consider - For a visual token, similarity with the full-text representation for fine-grained interaction (as a visual patch may be related to full text or multiple words, not only one word token)? Compared to For a visual token, similarity with all textual tokens (i.e., what used in the paper). Any comment on this?\n\n7) Will the dataset be publicly released?",
            "summary_of_the_review": "The paper is well-written with strong results. However, the proposed approach has limited novelty and some critical claims are made without strong support. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "(1) The core idea of the proposed FILIP method is to achieve word-patch alignment by token-wise similarity matrix through cross-modal late interaction by modifying only contrastive loss, which is both training and inference efficient. The authors collected FILIP300M, a large-scale cleaned image captioning dataset from the Internet for FILIP’s V-L pre-training. \n\n(2) FILIP achieves SoTA performance on ZS image classification and ITM tasks. Visualization results show its promising ability of fine-grained (visual-textual token) classification and localization. \n\n(3) To the best of our knowledge, FILIP is the first work with such high accuracy on F30k that not only does it reach double full mark of R@K (i.e. i2t[R@5 and R@10] are both 100.0), but also it achieves 580+ of rsum with only one single model (no ensemble). ",
            "main_review": "Strengths: \n(1) The proposed FILIP method achieve SoTA performance  on ITM task, which is significant for this area. \n(2) FILIP300M is a dataset of 300 million image-text paired samples collected and cleaned from the Internet, which is a huge amount of work, and will be beneficial to the whole V-L community. \n\n\nWeaknesses:\n(1) The core idea of FILIP method, namely to achieve word-patch alignment by token-wise similarity matrix through cross-modal late interaction, as far as we know, is first originated from the classical ITM method SCAN [1] (see below). So, FILIP’s cross-modal late interaction by Eqs. (3)(4)(5) is only equivalent to of SCAN’s Sum-Max (without attention) by Eqs. (9)(10), which is only a special case in SCAN’s ablation studies (see Table 3). Therefore, FILIP is lack of novelty, and we suggest the authors perform more comparative experiments about various pooling/attention/scoring methods different from SCAN [1]. \n\n(2) FILIP is not efficient in similarity matching stage, althougn it is efficient in feature extraction and embedding stage. That is because FILIP is a special case of SCAN [1], and SCAN is time consuming in cosine similarity matching, which is a commonly recognized in V-L communities. So, when the test data are very huge (for example, the real image-text search engine, usually millions of image-text pairs in total, rather than 1K/5K test data), the similarity matching time will beyond endurance. So, we suggest the authors perform some comparative experiments on the 1K/5K testing speed/time on f30k/coco with other methods like CLIP, SCAN [1] to prove their efficiency on inference phase. \n\n(3) The large-scale V-L pre-training dataset are not public/available. This might not be a very big problem since many other similar works also did this. But we cannot sure whether there are similar images/texts between the large-scale pre-training dataset (training + validation split) and the f30k/coco baseline dataset (val/test split). If the answer is true, then it is definitely unfair. \n\n(4) The SoTA performance are mainly come from 2 aspects: \n(i) The FILIP300M large-scale pre-training dataset and its cleaning/pre-processing steps. \n(ii) Grid/Patch feature (1 image, multiple features) methods like ViTs, which are the latest advances of backbone networks in CV. \nSo, it’s SoTA performance should not be largely due to the proposed cross-modal late interaction. \n\n(5) ViTs (1 image, multiple features) have the common problem of low efficiency in training/inference speed than traditional CNNs (1 image, 1 global feature). So, this shortcoming may be Inherited by FILIP. So, the training/testing speed may not exceed CLIP/ALIGN. Can the authors give more explanations? \n\n(6) The statements in this paper have a double standard on “offline/pre-computing”. \n(i) For other works: “This design complicates ... due to pre-computing and storing a large number of ROI features” (page 1). \n(ii) For their work：“FILIP successfully ... while simultaneously ... to pre-compute image and text representations offline” (page 1) \nTherefore, we don’t know weather offline/pre-computing is efficient or not? \n\n(7) There are some inappropriate expressions/mistakes in the introduction (page 1).\n(i) “One line of work” can be substitute by “Bottom-up attention [2] (see below) methods”. \n(ii) “Another line of work” can be replaced by “Cross-attention methods”. \n(iii) “self-attention (Kim et al., 2021)” is inappropriate, because ViLT is a typical cross-attention method (see Figure 3, the cross-modal Transformer Encoder), rather than (pure) self-attention. \n\nreference(s): \n[1] Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xiaodong He. Stacked cross attention for image-text matching. In ECCV, 2018.\n[2] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR, 2018.\n",
            "summary_of_the_review": "The performance is very good, which mainly comes from  “big data + big model + data cleaning / augmentation + supercomputers”. The whole method is lack of novelty. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper tackles the problem of large-scale vision-language pre-training which has been quite popular recently. The main issues identified by the paper are: 1) fine-grained information might be lost due to the use of global features, and 2) alignment between text and language is typically done using cross- or self-attention, which are inefficient. The authors proposed a dual-stream method that 1) uses visual \"patch\" tokens from ViT models and 2) perform late fusion of the two modalities using a variant of the method from Khattab & Zaharia (2020). ",
            "main_review": "Strengths:\n* The paper is well written and the assumptions made by the authors are well motivated and described with respect to the state of the art and literature.\n* The model is quite simple and easy to implement when compared to UNITER and Unicoder-VL.\n* The results of the experiments consistently show improvement wrt other state-of-the-art methods.\n\nOn the fence (to discuss):\n1. Novelty is quite limited given that it seems to be a combination of existing models. \n   1. The use of ViT in the vision-language pre-training setup is quite interesting, thought it is not the main novelty since it has been used in other vision-language pre-training models (e.g., CLIP and ALIGN).\n   2. The most interesting contribution is to do alignment via late fusion following the method in Khattab & Zaharia (2020). The authors explain the differences wrt that paper at the end of page 4 and beginning of page 5. However, they are quite limited: using average instead of sum, excluding padded tokens, and change to contrastive loss. \n   3. Although each single piece taken independently is not very novel, I think there is some value on the combination especially in such applications.\n\n2. I understand that most of these new methods are trained on a very large scale dataset scraped from the web and it is hard to share them. At the same time, it is impressive to see that the authors have created their own cleaner version of such pre-training datasets. However, I did not see a single signal in the paper, that the authors plan to release the data or links to rebuild the data or even code. This limits the reproducibility of the experiments and also it lowers the trust on the results. I'd suggest the authors to argument on this topic and possibly commit on sharing any of the resources to being able to reproduce experiments and compare with their method.\n\n3. It is unclear why the authors run the ablation studies using YFCC100M for pre-training and not FILIP100M. It is appreciated that there is a thorough ablation study on YFCC100M. But why wasn't it carry out on FILIP100M?\n\nWeaknesses:\n\n4. Computational complexity at inference time is not clear. It seems that one can lower the computation burden of training by just using heuristics, such as keeping 25% of tokens. It makes absolutely sense to be able to do this during training, because of different epochs, randomization and large dataset. However, it is not clear what happens at test time:\n   1. Does the method extract 25% of tokens of the query and the data in the search index? \n   2. If yes to 4.1, which tokens should be chosen for the query (can be done on the fly) and which one for the search index (it should be fixed for computational reasons)?\n   3. If no to 4.2, then the claim that the method is more efficient is not valid anymore, because the comparisons to made are still quadratic as in self-attention. Anything that I am missing that makes the method more efficient for a different reason?\n   4. If we need to store the full fine-grained tokes in a search index, this means that we will have 77 tokens (as stated in the experiment section) compared to the 2 for CLIP-based methods. This means that the index is way bigger than the original one, if we were to use global features. Is this thinking correct? If yes, any idea on how to make this better?\n\nOther improvements\n* Usage of the word \"Interactive\". I am not sure why the authors used this word in the title and in other places in the manuscript. I do not see how the method requires interaction from a person or user. To me it is more about correlation or alignment between image and text.\n* Eq 3, 4, 5 are a bit confusing. They can be simplified as did in the paper of Khattab & Zaharia (2020), where you use the max(\\cdot) in eq 3 directly inside eq 4 and 5. So you do not need eq 3. \n",
            "summary_of_the_review": "The authors do a good job on putting together ideas from different papers and create a novel pre-training dataset which leads to new state of the art results on vision-language pre-training tasks. However, the contributions seem to be marginally significant, and I'd like to hear more from the authors about the questions I raised above. Moreover, the computational complexity claim needs to be revised and discussed more in depth, since the high price for storing the index might not compensate the small improvement in accuracy performance. If any of those two points are covered in the discussion phase, I am willing to reevaluate my score recommendation. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}