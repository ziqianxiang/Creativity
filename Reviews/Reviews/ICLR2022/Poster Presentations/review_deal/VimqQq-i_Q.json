{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper presents some insightful suggestions for researchers studying generalization in federated learning by separating two types of performance gaps between training and test performance, the participation gap (due to partial client participation) and the performance gap (due to data heterogeneity). They suggest that federated learning researchers use a three-way split between participating clients' training data, participants clients' validation data, and non-participating clients' data to measure the generalization performance of an FL model. The paper presents thorough experiments to support their conclusions. A common concern about the paper is that the authors' suggestions, although relevant and reasonable, are somewhat unsurprising and have been noted in different forms in other works in federated learning. Another concern is that the conclusions are purely based on experiments and are not supported by theoretical justification. Despite these concerns, the reviewers commended the overall insights presented in the paper. \n\nThere was a healthy post-rebuttal discussion and some reviewers reevaluated the paper and raised their initial scores. Therefore, I recommend acceptance of the paper. I encourage the authors to take the reviewer's constructive suggestions into account when preparing the final version of the paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors propose a different view of performance in federated learning, by measuring separately the performance out of sample  and the performance on novel (but related distributions) from a “non-participating” client. They propose one approach of structuring experiments that allows these kins of measurements using a three way split. They then study how the dataset, client number and diversity affects the perforamnce.\n",
            "main_review": "\nStrengths\n\n-The paper analyzes performance measurements in the popular federated learning paradigm. The proposal is sensible and addresses an important problem. \n-The experiments are thorough and use best practice\n-The authors discuss releasing a code base for running their experiments\n\nWeakness/Comments\n\n-The experiments are done using FedAVGM. First of all this is not even mentionedi n the main paper. I think it would be interesting for the proposed study to revisits how different methods perform under the evaluation protocol and also use SOTA methods (e.g. FedYogi), perhaps conclusions might be different about which method is best. \n-In some federated settings the distribution shift is in the labels, how do the authors consider this case in ther analysis of non participating clients. For example a natural non-participating client might have a label unseen in other clients, yet the model is relevant,\n-(Minor) Figure 2 makes it hard to notice the per column similarity in the digits (and thus the distribution shift). I would suggest to instead do something more obvious (different color digit or using a different digit in each row altogether)\n",
            "summary_of_the_review": "\nOverall the paper addresses important topic and has conclusions that could benefit the community. The experiments seem too be well done and rigorous. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In many federated learning applications, not all clients participate in the averaging process, but are, e.g., randomly sampled from the set of available clients. This paper proposes to study not only the generalization gap in federated learning, i.e., the difference between risk and empirical risk, but also the difference between the risk of clients that participate and those that do not participate in the current averaging round, which is called participation gap in this paper. The paper formally equates the risk of unparticipating clients with the expected risk over all clients, where the expectation is over the sampling probability of each client. The paper then proposes a set of data splits to estimate those gaps with a focus on heterogeneous local data distributions. ",
            "main_review": "The paper points out an often overlooked issue in federated learning: when clients are sampled for averaging, some clients may hold deprecated models. Those clients will have a worse performance than their recently averaged fellows. The paper shows that for homogeneous local data distributions, this error vanishes in expectation, but remains an issue for heterogeneous distributions. These insights are valuable.\n\nThe main contributions of the paper, however, are fairly straight-forward and the theoretical result (Prop. 3.1) is trivial. The proposed three-way-split is a straight-forward estimator that gives little novel insights. The semantic partitioning of data into heterogeneous local datasets is interesting, but the advantage of the method over other strategies (simple distance based clustering, or using semantically similar datasets (e.g., MNIST, MNIST-M, SVHN, USPS, and SynthDigits)) remains unclear. The empirical evaluation is sound and confirms the effect of participating clients on the overall performance, but yield little novel insights.\n\nRegarding the community suggestions, I feel that (at least in my bubble) these suggestions are not novel. Using naturally partitioned datasets is fairly common in personalized FL or FL from heterogeneous data sources (e.g., in FedBN [1]). The semantic partitioning might be an interesting new technique, but from this paper its benefits remain unclear. Suggesting to report the distribution of metrics seems particularly odd, since this is common practice in most ML papers (at least, reporting standard deviation or variance). In FL, the variance of local client performances is also often reported, or visually illustrated. Only the first suggestion - to analyze the participation gap - seems appropriate, but is limited to approaches where clients are sampled and - as the experiments suggest - the sampling probability for individual clients is sufficiently low.\n\nGiven this lack of novelty, I do not think this paper is ready for publication.\n\nDetailed comments:\n- Why is it sound to equate the expected risk over all clients with the expected risk over unparticipating clients? Doesn't that convolute the issues? It seems to me that there are two issues: the difference in risk between participating and unparticipating clients and the difference in risk between participating clients and all clients.\n- Regarding generalization, there is work on quantifying the generalization gap in federated learning from heterogeneous datasets (cf. [2]) \n- Sec. 4.1.: How can you ensure that the pre-trained neural network captures semantically relevant features? How does the embedding vary with different networks? \n- Sec. 4.1./App. D: Why does the dimension have to be further reduced, why is PCA the right method, why should it be reduced to exactly 256 dimensions, and why is it not better to train some neural network with a 256-dimensional representation right away? \n- Sec. 4.1.: Why are mixtures created per label? How would the splits look if the mixtures where created independent of the label?\n\n[1] Li, Xiaoxiao, et al. \"FedBN: Federated Learning on Non-IID Features via Local Batch Normalization.\" International Conference on Learning Representations. 2021.\n[2] Huang, Baihe, et al. \"Fl-ntk: A neural tangent kernel-based framework for federated learning analysis.\" International Conference on Machine Learning. PMLR, 2021.\n\n************* after rebuttal ******************\n\nGiven that I misunderstood a key concept in my original review and that we had an interesting discussion in this rebuttal period, I feel compelled to write a second review. \n\nSummary Of The Paper:\nIn many federated learning applications, not all clients participate in the averaging process, but are, e.g., randomly sampled from the potentially infinite set of available clients, each endowed with its own local data distribution. This paper studies the decomposition of the generalization gap into the out-of-sample gap, i.e., the generalization ability of participating clients to unseen data drawn from their local distribution, and the participation gap, i.e., the difference between the generalization ability on unseen data drawn from the local distributions of participating clients to data drawn from all possible local distributions. The paper proposes a three-way-split of clients and local datasets to estimate these two quantities separately. \n\nMain Review:\nIn many federated learning applications, local data distributions are not homogeneous. This fact has been studied extensively and methods have been proposed to address the difficulties in training on such heterogeneous distributions. In case the set of clients is large (or potentially infinite) and each client has a (potentially) different local data distribution, then at any given time, only a subset of those clients will have participated in federated learning. Current estimations of a federated learning system's generalization performance only assess it on data similar to the data seen by those participating clients. This overlooks the error on data drawn from a distribution from a client that not yet participated. While this error in general cannot be measured, it can be approximated if clients are drawn iid by holding out a subset of clients as \"validation clients\" to assess this participation gap. Naturally, this allows to also assess the level of heterogeneity in the local data distributions.\n\nThe main contribution of this paper hinges on the assumption of a potentially infinite number of clients, each endowed with its own, distinct local data distribution. Given this assumption, the proposed three-way-split is a straight-forward and sound (and fairly practical) method to assess the participation gap. This can indeed provide valuable insights about the performance and nature of the overall federated learning system.\n\nSince standard benchmark datasets for non-iid distributions provide only a very limited number of different data distributions (e.g., the quintuple MNIST, MNIST-M, SVHN, USPS, and SynthDigits), the paper proposes a semantic partitioning of data. This allows one to simulate the setting of a very large set of clients each with a different local data distribution. \n\nMy main concern with the paper is that this contribution is not presented clearly enough and that its main assumption and limitations are not discussed in detail. Thanks to the great discussion with the authors, I am now convinced that the decomposition of the generalization gap into out-of-sample and participation gap has merits in the setting considered. It has also become clear that this is not so meaningful in case of a limited number of potential clients or a (small) finite number of potential distributions. Thus, in its current form the paper falls behind its potential. \n\nI suggest the following improvements:\n(i) Clearly point out the setting considered and its limitations. This includes separating it from cross-silo scenarios (where the number of potential clients is usually small) or many internet of things scenarios (where the number of potential types of devices is limited and devices of the same type share the same data distribution). I guess, the typical scenario is learning on mobile devices where the number of potential clients is indeed huge and local distributions are typically heterogeneous. Still, it would be great to discuss this assumption in a bit more detail: results from humanities (sociology and psychology) suggest that human behavior clusters around a fairly limited amount of archetypes, so it stands to reason that distributions obtained from human interaction with a device might also cluster around such archetypes. In the discussion with the authors, we established that in this case, the behavior or a federated learning system could quickly converge towards one with only a limited number of potential local data distributions. \nWhile these discussions seem to limit the applicability of the paper, I think they greatly improve its significance, since it gives rise to interesting questions for future studies. \n(ii) The empirical evaluation shows that the participation gap can be observed in practice. Here, the results on the naturally partitioned data are the most significant and should be pointed out more. \n(iii) Since the results are not applicable to all federated learning settings, I suggest incorporating the community suggestions into a discussion section, such that these suggestions are properly contextualized within the assumptions and limitations of the approach.\n\n\nThese issues require some extensive changes to the manuscript that usually go beyond what is considered minor revisions. However, given that the discussion contained most of these points already, I am willing to trust that the authors can make it. The theoretical contributions remain straight-forward and the three-way-split is fairly natural. Thus, I am on the fence about this paper. Since the paper addresses a novel issue, I slightly tend towards acceptance. I have changed my score accordingly.\n",
            "summary_of_the_review": "The issue of the different performance of participating and non-participating clients in federated learning - in particular for heterogeneous datasets - is interesting. The proposed contributions of this paper are a straight-forward data split strategy, a proposed strategy to split data into heterogeneous local datasets, and an empirical evaluation of the performance of participating and non-participating clients that confirms the intuitive expectations on the effect of participating clients. While the issue the paper addresses is interesting, the proposed contributions are fairly straight-forward and provide little novel insights. Thus, I vote for rejection. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "- The authors propose a study on generalization in Federated Learning (FL). They focus on the cross-device setting, where the clients are sampled from a large population and are characterized by unreliable availability, which implies that many clients may actually never take part in the training process. Driven by such realistic assumptions, they distinguish between participating and non-participating clients and identify two generalization gaps: \n    - the __out-of-sample__ __gap__, i.e. the difference between the empirical and expected risk for participating clients, or the gap in performance of the model when tested on previously unseen data; \n    - the __participation gap__, i.e. the difference in expected risk between the participating and non-participating clients, or the model drop in performance when faced with unseen distributions (i.e. new clients).  This is a direct consequence of the heterogeneity in FL: in heterogeneous settings, clients have access to different data distributions, leading the model to worse performance when new distributions are introduced. So the gap becomes a natural measurement for dataset heterogeneity, capability of the model to generalize and client diversity.\n- The authors propose a three-way split for federated datasets in order to  perform an estimation of the risks and gaps: by identifying participating and non-participating clients first, and then training and validate data within the participating clients’ local data distribution.\n- Finally, they also point out how federated datasets are usually split based on the labels distribution (each client is assigned a distribution on the labels), or on some natural information, e.g. the writer of a character, and show that this __label-based partitioning__ may not fully represent realistic client heterogeneity. \n    - Therefore, they propose a semantics-based framework for partitioning the dataset, which behaves similarly to the natural and more realistic method, without requiring naturally-partitioned data.\n- Their extensive analysis is tested on six different tasks and all information for experiments reproducibility are provided. Studies on the hyperparameters tuning are proposed as well.\n",
            "main_review": "- **Pros**\n    - The paper is well written and easy to understand for most parts.\n    - To the best of my knowledge, existing works do not take into account the participation gap in their analysis or in their datasets split, therefore this work can be an important contribution to the FL community.\n    - the authors provide all the necessary information about the results of the experiments, the hyperparameters tuning and the specifications to reproduce them, making this work a reference benchmark for future extensions.\n    - the proposed framework is scalable and easily adaptable to the existing state-of-the-art strategies and methods.\n    - assumptions and hypotheses are verified either theoretically or empirically.\n    - release of an open-source framework for semantic-based dataset partitioning and participation gaps measurement.\n- **Cons**\n    - In Sec. 3, the authors propose the initial analysis on EMNIST-62 and then continue on CIFAR datasets, without explaining what happens to the previous setting (label-based or natural partitioning) when their contribution is introduced (semantic-based partitioning).\n- **Questions**\n    1. On results consistency to model size and hyperparams:\n        - 1a) By looking at supp C1.1 and the consistency of results for different LR choices, I was wondering if the participation gaps are sensitive to the choice of the server optimizer.\n        - 1b) Actually, more in general I think that the analysis would be much more complete if you included experiments with recent methods that have been developed to reduce heterogeneity issue in federate learning. eg. MIME, FedDyn, Scaffold, FedProx, FedBN,  etc.. It would be great to see if the participation gap is sensitive to the algorithm or optimization method. \n        - 1c) Is the partecipation gap sensitive to the choice of the number of clients sampled at each communication round?\n        - \"1d) \"the participation gap is still reasonably large for ResNet-50.\" Results in C2.3 figure 16 for resnet50 are not at final convergence. Is this claim valid at the end of training? Also, I imagine that these gaps should be independent on the choice of the architecture e.g efficientNet or others. Have you verified that?\n        - 1e) very deep learner question: I'm curious to see how these gaps behave on large scale datasets as Inaturalist or Google Landmarks, but this is out of the scope of the paper.\n    2. it would be interesting to see how the experiments in Fig 4 change when EMNIST is created with the semantic partitioning\n    3. In the label-partitioned CIFAR, which is the chosen alpha for the Dirichlet’s distribution?\n    4. Table 1: looking at the plots in Appendix B.1, the part_train accuracy on Shakespeare looks close to 0.62, where does the 60.7 in the table come from?\n    5. How are the datasets balanced between clients (i.e. mean and stdev of data distributions among clients)?\n    6. In Appendix C, you provide detailed information on the experimental setup. The best learning rates chosen for the experiments in Table 1 are not clear.\n- **Minors**\n    - Section 5.1, third row: typo “participating” is misspelled\n",
            "summary_of_the_review": "A very nice paper. It proposes to measure both out-of-sample generalization and participation gaps and give several recommendations for training federated learning models following a new protocol. It also proposes a new way to create synthetic federated datasets that reflect heterogeneity in a more natural way than label-based split.  The experimental analysis is very concrete and the results look solid. I recommend acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a framework for disentangling the performance gap in federated learning into out-of-sample gap and participation gap, which should serve as a better tool for explaining the model generalization performance. They also give a semantic synthesis strategy that enables realistic simulation to create more natural federated data splitting. More analysis on the relationship between gaps and data heterogeneity is provided to support the validity of the proposed splitting framework.",
            "main_review": "Decomposing the performance gap into out-of-sample gap and participation gap is not new. The very simple way is to split the whole dataset into training and validation sets, and the training data is distributed across workers, then training data on each worker is further split into local training and local validation data. In this way, the local validation loss could be viewed as the out-of-sample gap and the global validation loss can be viewed as the participation gap. Evaluating the loss on local data can be found in [1,2].\n\nAnother question is if some clients never participate in training, how can we evaluate the participation gap since the loss should be calculated on these data?\n\nThe experimental results are trivial from the view of the commonly-used data splitting described in the first paragraph. Could the authors please elaborate on the novelty of this paper? How can we deal with the performance gap due to data heterogeneity?\n\n\n[1] Liang, Paul Pu, et al. \"Think locally, act globally: Federated learning with local and global representations.\" arXiv preprint arXiv:2001.01523 (2020).\n[2] Hao, Weituo, et al. \"WAFFLe: Weight Anonymized Factorization for Federated Learning.\" arXiv preprint arXiv:2008.05687 (2020).\n\n-------------------\nAfter reading all the reviews and the rebuttal, some of my concerns have been addressed, I am willing to increase the grade.",
            "summary_of_the_review": "I feel the idea and the experimental results are trivial if we view it from a commonly-used federated data splitting perspective. If the authors can propose some remedy to this participation gap, this will be an interesting piece of paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}