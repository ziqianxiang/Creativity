{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper follows the recent line of work of theoretically analyzing the Neural Collapse phenomenon, by making certain simplifying assumptions on the problem setup. In this case, the authors use cross-entropy loss on an unconstrained model where second-to-last representations become free variables. Their main results characterise the NC as the only global minimiser. \nReviewers were positive about this work, and concluded it presents a valuable addition to the growing analysis of NC. They also pointed out several clarity issues that should be addressed in the final revision, including a more objective comparison to prior work. Ultimately this work will be an interesting addition to the conference, and therefore the AC recommends acceptance."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper studies the recently discovered Neural Collapse (NC) phenomenon for deep neural network training using Cross Entropy (CE) loss. It theoretically analyzes the problem with a so-called unconstrained layer-peeled model (ULPM), and then it shows that the ULPM with CE loss has a benign landscape.",
            "main_review": "Strengths:\n- The paper writing is good, and the presentation is clear\n- The theoretical part is interesting, which builds connection between max-margin implicit bias and NC, and the theories are seriously developed\n\nWeaknesses:\n- I find the empirical result section, i.e. Section 4, is quite weak. It only contains a Figure 1, which itself is not well-presented since it is hard for me to gain much insights. This section really needs to be improved.\n- Some statements are debatable. In the first bullet point of the main contributions, it says \"feature regularization is unrealistic and never used in practice\", but the L-2 norm regularization, or weight decay, is widely used in practice. In fact, in the experiments of [Papyan et al. 2020], it uses such weight decay by setting the parameter to be 1e-4 or 5e-4. Therefore, in Table 1, I don't think the \"feature norm regularization\" spot for [Papyan et al. 2020] should have a cross mark there.\n- The \"Notations\" section in the bottom of page 4 can be moved earlier; otherwise many notations are used widely before without introduction, e.g. [K]\n\n\nTypos:\n- In the second line after Eq. (1.1), the \"y_i\" is unbolded while I think it should be bolded?\n- In the paragraph above Eq. (2.3), the notation \"N\" should be \"n\"? since we are using \"n\" in Section 2.1\n- In the first equation in Section 3.1, \"k \\in [1,K], i \\in [1,n]\" --> \"k \\in [K], i \\in [n]\"\n",
            "summary_of_the_review": "Overall, despite its weakness as I described above, this paper does provide some interesting approach in understanding the NC with CE loss. Considering its theoretical nature and the serious study, I would say it is marginally above the acceptance threshold.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper provides new analyses for understanding the neural collapse phenomenon observed in training deep learning classifiers. In particular, to understand the last-layer features and classifiers, the authors study the gradient flow of the unconstrained layer-peeled model (ULPM) without any regularizers and show that gradient flow converges to neural collapse classifiers and features. Experiments are provided to demonstrate the neural collapse phenomenon when no weight decay is used.",
            "main_review": "## Strengths\nThis paper studies neural collapse, a widely observed phenomenon in training deep neural networks. Existing work on the analysis of neural collapse is based on either constraints or regularizers on the features and classifiers. The authors analyze the gradient flow of the ULMP without any constraints or regularizers, and provide second-order analysis of the objective function, ensuring gradient descent converges to neural collapse solutions. \n\n## Weaknesses\n1. Weight decay is often employed in training neural networks. This paper studies the training problem without any regularizers, but it is unclear why this formulation is closer to the practice than the one with regularizers. \n\n2. Eq. (3.3) only shows that a non-optimal solution is not a local minimum. It may be a higher-order saddle. If the results show the (Riemannian) Hessian has a negative eigenvalue, then I think this is a much stronger result and should be described in Theorem 3.4.  \n\n2. As mentioned in the paper, the training dynamics will diverge to infinity where the cross-entropy loss archives its minimum. Thus, is possible that eq. (3.3) holds for any finite $(W,H)$ (if we allow any direction for $(\\Delta W,\\Delta H)$), i.e., any finite $(W,H)$ is not a local solution?\n\n3. Which manifold does the tangent space in Definition 3.1 refers to? In eq. (3.3), why highlight the direction on the tangent space? \nIn general, the tangent space is used in the manifold optimization where the variables are constrained on the manifolds, but here the training loss has no constraints. Can you clarify this? Also in Remark 3.4, it says Hessian matrix. Is it Riemannian Hessian? ",
            "summary_of_the_review": "Overall, this paper provides new analyses for understanding the neural collapse phenomenon. The analyzed formulation removes the constraints and regularizers on the features and constraints, which is different from the existing approach.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper analyzes the phenomenon of neural collapse from the simplified perspective of an unconstrained features models, whereby the only two optimization variables in the model are the last layer classifier and features, which are fit to some labels by minimizing the cross entropy loss. The authors leverage their observation that maximizing the margin results in a model achieving neural collapse, and then study the corresponding problem of minimum-norm classifier, and show that gradient flow converges directionally to the neural collapse solution. The authors then show that the optimization landscape is benign, in that all critical points that are not global minimum have a decreasing direction in the tangent space. ",
            "main_review": "This is an interesting contribution. The problem analyzed in this paper is important, and their technique based on the optimization landscape coupled with the connection between the min-norm classifier and neural collapse is insightful. In addition, the paper is well written and clearly organized. \n\nMy main concerns are the following:\n\n- The narrative of the paper seems to be directed towards conveying the message that their studied model (without norm constraints) is more realistic, and in several places they comment on how other models studied in related works are \"further from practical networks\". I find these comments at least exaggerated, but potentially also inaccurate: in the studied model, the norms of the classifier and features diverge (they only converge in direction), which does not happen in practical deep learning models displaying neural collapse. As a result, one could also argue that the presented model is further from practical settings.. To be clear: I'm not trying to argue this is the case. I'm simply suggesting that trying to make a point along these lines sounds counter-productive, and the contribution of this paper would be better served without these subjective distractions.\n\n- I am slightly confused about the claim of the authors that they present a global analysis of the optimization landscape: it seems like their result holds for a proper initialization that has a loss function less than log(2), which implies that the features and classifiers are already separable. I agree with the authors that this is still relevant and interesting, as it precisely captures the behavior observed in the final stages of training. However, if this is the case, then the term \"global analysis\" is inaccurate, and this holds only upon a correct initialization. Could the authors explain why they refer to this analysis as global?\n\n- The empirical results on Fig.1 are slightly confusing to me: there are two different colors dedicated to \"last layer features\", in different graphs. Why is this so? Also, as far as I can tell, the variance of the class-specific classifiers does not seem to converge in either case, which contradicts the presented results. It is true that \"they stay within small values\", but this is different from the convergence results that the authors present as their main result. Can the authors explain this?\n\nMinor comments:\n- Page 2: \"Finally, We verify.. \"\n- page 7: \"unoptimal points\" -> non-optimal\n- Theorem 3.4 \"is (or is not) neural collapse solution\" -> \"is (or is not) a neural collapse solution\".\n\n\n\n\n\n\n",
            "summary_of_the_review": "I believe this is a valuable contribution, but have some reservations about the presentation of the results. I would be happy to increase my rating after reading the responses by the authors.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper shows that for the cross-entropy loss, when updating both the weight vector and the features by gradient flow, the final solution exhibits the neural collapse behavior, where the neural collapse refers to an observation pointed out by (Papyan 2020), which is that (NC1) the within-class features collapse to their class mean, (NC2) the average of the features of each class converges to having the equal length and all pairs of the class-means have equal-size angles, (NC3) the weight and the class-mean are aligned, (NC4) the weight converges to the nearest class-center of the features.\nThe optimization problems (or the simplified model) in  the previous works that theoretically show the neural collapse require some constraints or regularization, which are seldom used in practice. The optimization model (ULPM) in this paper does not require these, and so is much closer to the realistic (of training deep networks).\n",
            "main_review": "Overall, this is a good paper and the result is interesting. The paper first shows a connection between the limiting point of gradient flow on the cross-entropy loss and the KKT solution of a minimum-norm separation problem. It also shows that every global optimal point of the minimum-norm separation problem satisfies the neural collapse condition. To deal with the concern that a KKT solution is not necessarily a global optimal point of the minimum-norm separation problem, a second-order landscape analysis of the minimum-norm problem is provided to show that at all the other KKT points of the minimum-norm separation problem, there is a direction that leads to a lower objective (of the Lagrangian of the minimum-norm problem).\n\nI think a clarification is needed on whether the paper shows that gradient flow on ULPM is guaranteed to return a solution that is a global optimum point of the minimum-norm separation problem, i.e. is guaranteed to converging to a point that exhibits the neural collapse. \n\nTypo: On the second line of the proof of Theorem 3.1, there is a typo regarding the definition of $\\tilde{w}_i$: $\\frac{1}{n} \\rightarrow \\frac{1}{ K}$.\n",
            "summary_of_the_review": "(see above)",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}