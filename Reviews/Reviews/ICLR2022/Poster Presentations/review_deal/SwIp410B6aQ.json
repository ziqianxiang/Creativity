{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "Based on the previously observed neural collapse phenomenon that the features learned by over-parameterized classification networks show an interesting clustering property, this paper provides an explanation for this behavior by studying the transfer learning capability of foundation models for few-shot downstream tasks. Both theoretical and empirical justifications are presented to elaborate that neural collapse generalizes to new samples from the training classes, and to new classes as well.\n\nThe problem that this paper delves into is important. The paper is well-motivated, and well structured with a good flow. Both theoretical and empirical analyses of the paper are solid. Preliminary ratings are mixed, but during rebuttal, multi-round responses and in-depth discussions were carried out between authors and reviewers, and the final scores are all positive with major concerns well addressed. AC considers the paper itself and all relevant threads, and recommends the paper for acceptance. Authors shall incorporate all response materials into the future version."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "In this work, the authors investigate the success of transfer learning with foundation models from the perspective of the neural collapse phenomenon. More specifically they study the transfer learning capability of foundation models for few-shot downstream tasks. The paper provides both theoretical and empirical justifications to elaborate on this point.\n\nNote: The paper is heavily inspired by the pre-existing literature. It takes the theoretical perspective from Papyan et al. and somewhat uses that to provide justifications (understanding) of empirical results in Tian et al. and Dhillon et al.",
            "main_review": "I don't have serious concerns but would appreciate it if the authors could expand on the following comments and questions.\n\n- Do you always pick the last layer for features in the foundation model? Did you try selecting second last or third last layers for example? I realize that those layers might not experience neural collapse, but it might be interesting to see how much performance drops. This can also be a baseline to examine whether the neural collapse (using the last layer) is really the reason behind transfer learning performance gains.\n- This phenomenon of decreasing variance of the features of a given class (or the concentration of features to mean feature value) seems equivalent to finding invariant features amongst classes. This phenomenon could also be the possible explanation behind the generalization and robustness of the models as explored in IRM [1] (here you can consider each class as a distribution i.e. $P_{c}$). What are your thoughts on this? do you think that there could be an alternate explanation?\n- Can the representations be used from a model trained with self-supervised objective instead of cross-entropy loss using labels? or the assumption of finding the foundation model $f$ based on $l$-class classification is very strict (relying on neural collapse phenomenon)?\n- I haven't thoroughly checked the proof of proposition 1, but you mention that you expect the generalization gap terms to typically scale as $O(1/\\sqrt(m_c))$. Since this is one of the main assumptions could you please point out the circumstances where this scaling might not happen?\n- In Section 5.1, while transferring to the few-shot classification task, in addition to training the new linear layer $g$ do you also finetune $f$?\n- In Fig.1 the legend (or at least the caption) should indicate that the plots correspond to the number of source classes.\n\n[1] https://arxiv.org/abs/1907.02893",
            "summary_of_the_review": "Even though the paper takes heavy inspiration from the previous works, I believe the results are important and would be beneficial for the community. It provides new insights for effective transfer learning in the few-shot setting.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper analyzes neural collapse in the transfer learning setting using foundation models. ",
            "main_review": "Strength:\n1. This paper provides some theories on the neural collapse in the transfer learning setting\n\nWeakness:\nThe theories are not sufficient, see details below:\n1. On page 4, before Section 4, there is a claim: \"Our version of neural collapse (at training) asserts that lim_{t\\to\\infty} Avg V_f(S_i, S_j) =0\". Why is this true? This paper doesn't give any justification from either the theoretical or numerical perspective.\n2.  Proposition 1 basically claims that the empirical and population version of V_f are close to each other, which is a variant of generalization bound. The establishment of this generalization bound is the key part in this kind of statement, however, this paper neglects this part by directly assuming there exists a generalization bound for the first and second moments of f, which makes the theoretical contributions not sufficient. \n3. Proposition 2 is more interesting, but the current results are established with respect to the expectation, and the RHS does not depend on the sample size at all. A more refined analysis that shows explicit dependence on the sample size (and the number of source tasks) would help.\n4. It would be better if we have a formalized theorem for the classification error in Section 4.3. \n\n",
            "summary_of_the_review": "This paper claims to provide theoretical insights on the neural collapse in the transfer learning setting, but the theoretical results are not sufficient. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Large pretrained models that can be effectively adapted to various tasks via transfer learning have been recently characterized as foundation models. (Papyan et al.) recently introduced the idea of neural collapse(NC) in deep networks. It identifies the training dynamics of deep neural networks for classification tasks, where the penultimate layer features associated with training samples from the same class concentrate around their class' feature means.      \n        The authors recognize the recent results that show the competitive performance of a single classifier trained over many classes on few-shot learning problems. They try to provide an explanation for such behavior based on NC. They theoretically show that if NC happens on a training set, then it generalizes to unseen samples from the same task under natural assumptions.   \n        Second, they show that NC is expected to happen in new classes when classes for pretraining(source tasks) and finetuning(target tasks) come from the same class distributions. This would imply that representations learned during training are adaptable to other tasks. To demonstrate the above results, authors work with class-distance normalized variance(CDNV) to identify NC in contrast to (Papyan et al.) that use Class-Covariance Normalized Variance (CCNV). The intuitive explanation for using CDNV has been described by the fact that when the variance of features of a certain class decreases in comparison to distance from another class, one expects a higher accuracy for classifying features belonging to different classes.     \n        Finally, the authors study the relation between CDNV and the classification performance and show that low CDNV corresponds to low error probability, thereby explaining the success of foundation models in few-shot settings when NC occurs. The authors also validate their theoretical results empirically, by demonstrating that NC is strongly correlated with accuracy in few-shot settings.   \n\n[1] Vardan Papyan, X. Y. Han, and David L. Donoho. Prevalence of neural collapse during the terminal phase of deep learning training. Proceedings of the National Academy of Sciences, 117(40): 24652–24663, 2020. ",
            "main_review": "#### Strengths\n - The paper provides a possible explanation as to why representations learned over many classes by a lone classifier can serve as a competitive solution to few-shot learning problems, by exploring the idea of neural collapse.    \n - Overall the explanation(hypothesis) provided for the success of foundation models in few shot settings due to NC, is well-motivated by the authors and the general flow of the paper makes sense.    \n - The theoretical justifications about NC occurring in unseen classes, and therefore translating to better accuracy in few-shot tasks seem like a novel contribution that gives an insight into the reasons a single classifier works well in a few shot problems when compared to algorithms devised specifically for such tasks.     \n- It is good to note that experiments have averaged reported performance over various random initialization and the last few epochs of training to ensure that results are more robust.    \n\n#### Weaknesses\n- The performance of the solution proposed is not the best and is especially far from it in the case of MiniImageNet. While I understand that the paper is not focused on beating the SOTA (Tian et al.) but rather develops on the insight that they gather from neural collapse, I don't agree with the reasoning provided for it. The authors attribute other methods' higher performance to sophisticated initialization, scheduling schemes, or distillation procedures, etc. I don't fully agree since the performance gap is not very small to be attributed to such reasons. For instance, the performance of (Tian et al.)'s simple Baseline model(not even the distilled model) performs much better on 5-shot 5-way MiniImagenet (79.64 ± 0.44).   \n- The experiment setting chosen can be thought to be similar to 5-way 5-shot problems. An important experiment to demonstrate their solution would be a 5-way(or n-way) 1-shot setting. Since the paper claims the competitiveness of foundation models on such tasks over some specialized algorithms, it is important that results on 1-shot tasks are also reported on as similar works have also been done. For example, (Tian et al.)'s simple baseline model (which works on the principle of pretraining like foundation models) has reported SOTA scores on 5-way 1-shot MiniImagenet (73.9 ± 0.8).    \n  \n[2] Tian, Yonglong, Yue Wang, Dilip Krishnan, Joshua B. Tenenbaum, and Phillip Isola. \"Rethinking few-shot image classification: a good embedding is all you need?.\" In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIV 16, pp. 266-282. Springer International Publishing, 2020.   ",
            "summary_of_the_review": " Even though there are some concerns about the experiments in the paper, I believe the paper has more merits than flaws, in proposing a possible explanation for the superior performance of foundation models in few-shot settings, and validating it theoretically and empirically.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies the ability of foundation models for classification to learn representations that are transferable to new, unseen classes in the perspective of neural collapse. Specifically, this paper demonstrates both theoretically and empirically that neural collapse generalizes to new samples from the training classes and new classes. As a result, representation learned by foundation models can be easily classified in the few-shot setting.",
            "main_review": "Strengths:\n+ This paper is well-written, and it is enjoyable to read.\n+ Neural collapse is only investigated existing samples and classes before. It is significant that this paper shows that neural collapse generalizes to new samples from the training classes and new classes both theoretically and empirically. \n+ Comparisons to prior work validate the views of this paper in some respects.\n\nSome concerns:\n+ It is too idealistic to only consider a two-class classification problem in section 4.3.  The real classification problems are often multiclass classification problems. Can the conclusion in section 4.3 be extended to multiclass classification problems?\n+ Neural collapse is an emergent phenomenon in deep learning. In the few-shot settiing, is it related to the architecture of the network? More architectures (e.g. VGG or Inception) will be appreciated.\n+ It is known that learning rate strategies in SGD need to be carefully tuned. If other optimizers (e.g. adam) are used, are the experimental conclusions consistent?\n+ It is shown that neural collapse occurs when training neural networks under MSE-loss minimization with different settings of regularization. Are the experimental conclusions consistent under MSE-loss minimization?  Also, it is very interesting to find the role of neural collapse for regression problems (MSE-loss minimization is commonly used in regression) in the few-shot setting.\n+ Are the conclusions of this paper consistent in other areas, for example, NLP?",
            "summary_of_the_review": "Neural collapse is only investigated existing samples and classes before. It is significant that this paper shows that neural collapse generalizes to new samples from the training classes and new classes both theoretically and empirically. However, there are some minor concerns and I think this paper is marginally above the acceptance threshold.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concerns",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Summary:\nThe paper study neural collapse for classification in transfer learning setting, where the large and overparameterized foundation models trained over many classes could provide feature maps that are transferable to new, unseen classes with few-shot learning . \n\nThe paper demonstrate both theoretically and empirically that neural collapse generalizes to new samples from the training classes, and to new classes. \n\nContributions:\nThey demonstrate that neural collapse generalizes to new data points and new classes (e.g., the target classes), when the model is trained on a large set of classes with many samples for each class. In addition, they show that in the presence of neural collapse, training a linear classifier on top of the learned penultimate layer can be done using only few samples.\n\nOn experimental side, the experiments on neural collapse in transfer learning is new to my knowledge. And the experiments are at large scale, with clear organizations.\n\nOn theory aspect, the paper's contribution is mostly to accompany and support the experiments.\n",
            "main_review": "Strength:\n1) The experiments are done neatly, I appreciate the amount of hard works of large scale computing to generate fig 1., and their experimental results supports their argument very well. \n2) Study Neural collapse on unseen classes for transfer learning is new\n3) The writing and flow of the paper is very clear, especially the statement of their transfer learning's math setting are pretty neat.\n4) Their own version of neural collapse definition via class-distance normalized variance (CDNV) is good, although it is kind of a natural rew derivative quantity of \n (a) the signal to noise ratio in the original NC papers (for eg. https://arxiv.org/pdf/2106.02073.pdf).\n (b) the notion of separation in NC by https://arxiv.org/pdf/2012.10424.pdf\n\nWeaknesses:\n1) The theory analysis section(section 4) tries to supports the experiments, as a faithful decoration to an experimental paper. It did a decent job, but the analysis is not too innovative or insightful. \n\n  Feedback to improve:\n  (a) It would be good if the paper can provide a more in-depth analysis of why NC works for some unseen classes in transfer learning in terms of the distance between unseen classes and the classes that are learned in original train dataset for foundation models in semantic sense.\n  (b) It would be good if the paper can provide a mathematical analysis that touch the inside of deep neural network rather than treat it as embedding and only study the last linear layer \n(c) study beyond the standard techniques to derive generalization bounds to new class-conditional distributions that tightly capture deep neural network's representation power.\n\n2) On experimental side, all the measurement are done with x axis being the course of training iterations. Another natural x-axis for measurement of interest include: \n(1) number of transfer dataset sample size per class \n(2) number of train dataset sample size per class \n\n(2) would probably require another works but I think (1) is related to the paper's claim on few-shot learning.\n\n\n",
            "summary_of_the_review": "The paper did solid experiments on Neural collapse on unseen classes for transfer learning. The angle is new and the experiments are done neatly with a good amount of hard works of large scale computing. And their experimental results supports their main claim very well. The theory part is more standard and I have not found surprising theoretical insights from it. Overall it is a good paper worth recommending to ICLR as a solid incremental work in the field of Neural collapse.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}