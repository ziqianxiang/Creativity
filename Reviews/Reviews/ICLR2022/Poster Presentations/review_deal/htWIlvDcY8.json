{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper presents a meta learning framework to learn novel visual concepts with few examples. The proposed FALCON model uses an embedding prediction module to infer novel concept embeddings. This is done via paired image and text data as well as supplementary sentences. The resulting systems shows improvements on a series of datasets with synthetic and real images. The reviewers were supportive of this submission and praised the novelty, central ideas and experimental setups.\n\nConcerns included:\\\n(a) [2P5Z] Justifying the formulations in this paper and situating it with past work -- \"Why is this an ecologically valid problem formulation?\", \"why a meta-learning approach is the best formulation to tackle the problem?\", \"Why the box embedding space?\"\\\n(b) [W3YC] More details required about the dataset and approach.\\\n(c) [98FU] Failure patterns\n\nThe authors provided detailed responses to these concerns. Concern (a), (b) and (c) were well addressed in the rebuttal and paper, and led to in increase in the reviewers rating.\n\nGiven the above, I recommend acceptance. But I do urge the authors to add the details provided in the rebuttal into the main paper. In particular, the concerns/suggestions by reviewer 2P5Z can hugely help in improving the paper and informing the reader."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents a unified meta-learning neuro-symbolic framework for fast visual concept learning from diverse data streams. It introduces a new embedding prediction module to integrate visual examples and relations to infer novel concept embeddings. It’s meta-learning continuous learning approach also uses supplementary sentences to relate concepts to one another. They show improvements for the end task of question answering with novel concepts by first meta-learning those concepts during a meta-testing phase.\n",
            "main_review": "Overall, I felt like this paper has potential but it just is not there yet. The writing and paper structure needs work. It took a really long to get to the true contribution of the paper, which if appears to be a new training and evaluation setup to evaluate how well a model can answer questions about novel concepts. The problem setup involves a meta-learning step where the model is presented with the novel concept in the form of a sentence and an image along with optional supplementary sentences. Aside from the problem formulation, the paper proposes a concept learning module on top of the Han et al. 2019’s Visual concept meta-concept learning model. The concept learning module is introduces as either a graph neural network or an RNN architecture that learns to relate the concepts extracted from the images, sentences and supplementary sentences.\n\nAll older concepts are learned during a pretraining stage (the one introduced in Han et al.). A new addition is the box embedding space that improves performance of the embedding space.\n\n\nThe problem I have with the paper is that it introduces (1) a new problem formulation, (2) a new meta-learning model, and (3) uses a geometric embedding space. But none of these are really justified. \n\nI would have expected to see a paper that answers: (1) Why is this an ecologically valid problem formulation? How does it relate to existing problem formulations? Why do existing baselines fail? (2) why a meta-learning approach is the best formulation to tackle the problem? And (3): Why the box embedding space? There should be more justification for by the box embedding space is the right one to use for the given data / task. The paper has the following sentence: “Here, we focus on the box embedding space (Vilnis et al., 2018) since it naturally models the entailment relationships between concepts.” but doesn’t dive into what these entailments are.\n\nAside from this, I didn’t find any ablation that studies how the model performs as the number of supplementary sentences changes. How does it scale as the number of base concepts changes or the number of object categories? Is there a cold start problem of requiring a certain number of base concepts during pre-training?\n\n\n\nMinor:\n- There are a few typos and incomplete sentences. Ex, “Our goal is to use meta-learning a system that can learn new concept quickly.” and “4) use the learned concept flexibly in different concepts”.\n\nThere are too many terms that all sound the same in the paper: “relational concepts”, “relational representations of objects”, “object-based concepts”, etc. It took me a while to wrap my head around which ones mean what. I suggest using consistent terminology throughout the paper when referring to a specific concept.\n",
            "summary_of_the_review": "Overall, I think the contribution would have been stronger with a proper grounding of why the problem is ecologically valid, why the solution proposed is an appropriate one, and why specific design choices (box embedding) were made. There are also missing ablations and rigorous evaluation of the model.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper tackles concept learning problem. This paper designed a neural symbolic machine based approach that is able to inference the concept embedding from a given image and sentence pair. It can also mediate the concept embedding using additional text explanation. To verify this approach, this paper follows a meta-learning setting. Specifically, the concepts are split into training, validation, and testing. The model first learn the base concepts from the training set and then quickly infer the concept embedding for the testing split. The paper achieved a superior performance over three datasets against baselines.",
            "main_review": "The paper's idea is very interesting and novel. I really enjoy reading this paper. I have some questions for the specific details and the datasets.\n\nFor the dataset (sect. 4.1), if I understand the approach correctly, the approach first needs a paired image and sentence to learn the base concepts. In this setting, the sentence should be a descriptive sentence that describes the object/concept in the image. If my understanding is correct, the CUB dataset setting is very natural. However, I wonder how would CLEVR and GQA dataset follow this setting? IIUC, the text in CLEVR and GQA are all questions, which are not descriptive sentences.\n\nFor the approach (sect. 3.3), IIUC, the proposed approach needs to extract concept relations from a supplemental sentence. First, the sentence is in the natural language form. How to do the semantic parsing of the sentence? The parsing might contain error. How to resolve that? As the sentence is in the natural language form, the name of the concept might in different form. How to resolve that? Second, it is pretty rare to see a paper that makes neural symbolic machine works for the real language. The neural symbolic machine works very well for the template language. Then I wonder how to design the operators in the neural symbolic machine? \n\nFor the setting, I wonder what is the setting for CUB? IIUC, CUB is a dataset for bird classification. From Fig. 4, it seems this paper treat the CUB as a retrieval task? The images of CUB dataset only contains one single bird. Therefore, I think the object centric approach might works pretty well. (Optional, this might be a stretch) I wonder would this approach works for some other datasets like Flickr30k or COCO? ",
            "summary_of_the_review": "I think this paper is a very interesting paper for concept learning from image and natural language. The motivation of this paper is very natural and the architecture is chosen appropriately. As long as the author could address my questions for the setting, datasets, and details of the approach, I would recommend accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper describes an approach to learn visual concepts with a few examples. The paper considers a problem consisting of paired images and sentences, optional concept descriptions, and the target question answering task for a new concept. The proposed approach utilizes the object detector (He 2017) to map visual objects or relationship into the box embedding space (Vilnis 2018), and also utilizes a neuro-symbolic program to identify the referring object as well as relationship among concepts. The new concept representation is inferred after the two graph neural networks, and the networks are trained to solve for the downstream task, which in this paper is question answering. The proposed approach is evaluated in three benchmarks (CUB, CLEVR, GQA) and shown to outperform the recent baselines (Hudson &Manning 2018, Mao 2019).",
            "main_review": "## Strength\n\nThe paper describes a novel technical approach to fast concept learning from a few examples. The proposed framework integrates the neuro-symbolic program executor, box embeddings, and graph neural networks to achieve the competitive performance in the considered benchmarks.\n\nThe main technical idea seems to lie in the use of the box embedding in the concept-centric (neuro-symbolic) approach. Section 2 concisely summarizes the context of this work.\n\nThe evaluation protocol looks comprehensive and convincing, including both detailed ablation and SoTA comparisons in multiple datasets.\n\n## Weakness\n\nI do not have a major concern on this paper. Below are a few minor questions.\n\nThe paper uses a pretrained Mask R-CNN for extracting objects in a given image. I wonder what is the effect of detection errors and the influence of on what dataset these detectors are trained. Presumably the neuro-symbolic reasoning module should be able to handle errors, but it seems possible to completely miss a correct object in the scene.\n\nAnother similar question is the error in language parsing. I assume the generator in this work does not incur an error in semantic parsing thanks to the use of templates, but was there any error originating from parsing / program execution?",
            "summary_of_the_review": "The paper proposes a novel framework for fast visual concept learning that builds on a neuro-symbolic program executor, box embeddings, and graph neural networks. Even if the individual components might be from the existing work, the overall approach presents a novel technical approach. The evaluation based on downstream question answering shows the competitive performance of the proposed model over baselines in multiple benchmarks. Given the novelty and the effectiveness, I believe the paper makes a solid contribution.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "The paper shares common concerns on machine learning systems (fairness, biases, etc), but there seems no extra specific concern in this work.",
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper provides a method for learning new concepts from a few examples with both images and natural languages. For the evaluation, the proposed model is able to answer questions about some images related to the visual concepts.\n",
            "main_review": "The paper is very clear to read. It is a relatively novel task to learn the new visual concepts from a few images and sentences. In the experiments, the authors validate their methods in both real-world and synthetic datasets.\n\nThe model that the author provides contains a few separate components, such as some semantic parsers, that cannot be jointly optimized. The proposed method cannot recover from the errors made by those components.\n\nIt would be great to explain in a bit more details about the failure patterns of the proposed method.\n",
            "summary_of_the_review": "I find the paper interesting to read, and believe there are enough novelties in the proposed paper. I recommend a rating of “weakly accept”.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}