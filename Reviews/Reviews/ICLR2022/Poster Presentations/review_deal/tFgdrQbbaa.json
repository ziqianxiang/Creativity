{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "After carefully reading the reviews and rebuttal, I believe this work is of sufficient quality for acceptance. Understanding continual learning from a theoretical stand point is a very important topic. I find that one of the main issue raised by reviewers was about the exact meaning of Continual learning, and whether what the authors studied was more akin to sequential learning. While I don't mind the term sequential learning, and is quite descriptive of the work as well, I disagree that the considered setup is not continual learning."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper provides a theoretical analysis for sequential training based on related NTK results.  It shows the similarity between the target functions is a key factor for forward and backward transfer, and when the target functions are the same the samples size of each task is a key factor to forward and backward transfer. It also shows even a slight difference between the target functions can cause catastrophic forgetting and the forgetting might still happen for the same target function when the sample size of a later task increases. ",
            "main_review": "The paper provides some theoretical insights on the forward and backward transfer in sequential training.  My main concern is that I have some doubts regarding its strong assumptions on the data distribution. As stated in the last paragraph on page 4, the input samples of task A and task B are i.i.d. and generated by the same distribution. This assumption cannot be satisfied in most cases of continual learning, especially for class incremental learning the data distribution of each task has significant differences.  In this sense, I think the claims of this work are more about sequential training on a large dataset rather than continual learning.  And the experiments are insufficient to verify the theoretical results in typical continual learning benchmark tasks. The experiments on CIFAR10 are basically sequential training on chunks of a dataset. I'm wondering how much of these theoretical results can be conveyed to typical continual learning tasks. As in most benchmarks, the sample size of each task is the same. Even when the target function is the same, such as in domain incremental learning, the forgetting would like to happen (e.g. permuted or rotated MNIST tasks). But in Sec.4.3, the authors claim no forgetting appears for equal sample sizes, which is not the case for most continual learning tasks in my experience. \n\nSome detailed issues:\n1. Fig.2 does not match the description as the blue line E_A is always larger in the figure. \n2. What's the error in Fig.3? The generalization error over the last task? \n3. Most experiments results are about forward transfer, I think it would be better to show backward transfer as well.\n",
            "summary_of_the_review": "The theoretical results provided in this paper are kind of interesting but I think they are more about sequential training rather than continual learning. The experiments part is a bit weak to support the claims. \nAs I'm not familiar with the NTK related work, I didn't check through the proofs of those theorems. ",
            "correctness": "2: Several of the paperâ€™s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies the generalization/transfer behavior of the continual learning/sequential learning under the neural tangent kernel (NTK) regime. It gives the formulation of the generalization error between two tasks in forwarding and backward ways and analyzes the influences of target similarity and sample sizes. Then the self-knowledge transfer case with the same target function is studied, which shows the universality of the catastrophic forgetting and the influence of the sample size. The results are then generalized to many tasks. ",
            "main_review": "Theoretical studies are crucial for understanding how deep neural networks work on the continual learning/sequential learning problem. NTK is one of the powerful theoretical tools for analyzing the behavior of neural networks. This paper extends the studies of NTK, specifically the Spectrum Dependent Learning Curves [1], on the continual learning setting via studying the generalization between the sequential tasks. The analyses reveal the relationship between the transfer/forgetting and the dataset for standard neural networks. However, there are still some drawbacks that may limit the significance of the paper:\n1. The paper focuses on studies of the behaviors of the standard networks (NTK) trained under standard strategies. Thus the results mainly reflect the influence of the data characteristics and show fewer insights related to the networks and the training strategies. Given the original results in [1], the results in the paper are obvious, further limiting the paper's significance. The results in the paper so far give limited guidance related to model designing, in my opinion. \n2. The studies are restricted to continual learning with explicit task boundaries. It is acceptable. But I suggested the authors claim the settings more explicitly at the beginning of the paper/method for clarity. \n\n[1] Bordelon, Blake, Abdulkadir Canatar, and Cengiz Pehlevan. \"Spectrum dependent learning curves in kernel regression and wide neural networks.\" In International Conference on Machine Learning, pp. 1024-1034. PMLR, 2020.",
            "summary_of_the_review": "Although the paper's significance is a little limited by the relationship with previous work such as [1] and the restricted scenario, the results (especially the self-knowledge transfer case) are interesting. It could be a starting point for more general and informative analyses of deep continual learning. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper is a theoretical paper about continual learning, that studies 2 tasks settings in the NTK regime. \nThe paper study in particular transfer from tasks with the same target functions and introduce the concept of \"self-knowledge transfer\", i.e. transfer from two tasks with the same target function.",
            "main_review": "\nThe paper is well written and well structured. Each section is clear in what the authors try to explain demonstrate.\nNevertheless, the vocabulary used is a bit different from usual continual learning papers, which might be a bit misleading.\nA head in CL literature refers often to a group of output neurons while it seems that in this paper it refers only to one output neuron.\n\"Target function\" is also unusual to refer to the function specific to a group of targets. This terminology does not hurt but it should be explained earlier in the paper to avoid confusion. \n\nI acknowledge the use of a theoretical approach to continual learning to better define and understand the problem we are solving.\nOn the other hand, it is a bit difficult to link the paper setting, with continual learning classical settings.\nContinual learning is usually about learning in a non-iid setting with several tasks, where each task might lead to forgetting of the previous one.\nIn this paper, two different tasks are not supposed to interfere with the other as long as they have different target functions \"in the NTK regime, the interaction between different heads do not cause knowledge transfer and forgetting\".\nThe concept of \"self-knowledge transfer\" is not very intuitive, if I understand correctly is about learning again what the model is supposed to already know. \nBut it does not learn it from himself (i.e. from its own knowledge) but still from the data... that why \"self-knowledge transfer\" terminology seems to not fit the phenomenon that the authors want to describe. \"self-knowledge transfer\" would better fit to a pseudo rehearsal method or to a generative replay method.\n\n\nQuestions:\n- Section 4.3 : \"the training on task B degrades generalization even though both tasks A and B learn the same target.\"\n How is it possible? is it overfitting of the target function or just that A and B tasks are about two different modes of the same distribution?\n is it a setting similar to domain incremental settings?\n\n- Section 4.2 : \"positive transfer is guaranteed under equal sample sizes\" I believe sample size is important but if N_A == N_B and all the samples are the same,\n there is no reason for enhancement, i.e. positive transfer. On the other hand, if you have the same number of samples but with a lower variability, you can have a negative backward transfer.\nWhat am I missing here?",
            "summary_of_the_review": "To conclude, I like the aim at theoretically approaching continual learning but it seems there is still a consequent gap to make the theoretical setting proposed fit an actual continual learning setting.\nI still will grade this paper above the acceptance threshold, because I think the paper can be useful for the community. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper presents a study of continual learning of neural networks in the neural tangent kernel (NTK) regime. The authors extend recent work which estimates the generalization error of such networks using a statistical mechanics technique known as the replica trick. The theory makes a number of predictions regarding backward and forward transfer learning which describe well the behavior of finite neural networks trained with gradient descent.",
            "main_review": "This is a very well written paper. I never learned the replica trick and I'm not an expert of NTK theory. But the authors provide just enough background detail on the NTK and the base Bordelon et al. (2020) and Canatar et al. (2021) papers to have a grasp of the main tools used to derive the new results. These new results are concisely presented and appropriately discussed. Overall, I found the paper a pleasure to read.\n\nThe theory is approximate: it is developed for infinitely-wide networks, and it is not exact even for such networks, as it rests upon some \"tricks\" that are part of the replica method. The authors therefore carefully validate their semi-analytical results with actual neural network simulations. The theory is in impressive agreement with the large-network simulations. The fit of Fig. 1b, which shows rich nonlinear behavior, is particularly remarkable.\n\nThe paper contains a number of interesting results. It justifies the adjective \"catastrophic\" in \"catastrophic forgetting\" and it discovers complex transfer behavior as the relative training set size and teacher noise levels are varied. I think that the simulations alone are worth publishing; the fact that they can be described with semi-analytical methods is a strong plus.\n\nA weakness of the paper, which limits the generality of its conclusions, is its focus on the restricted case where p(x) is assumed to be the same for every task.\n\nLike other studies in the NTK regime, another obvious weakness of the paper is the lack of experiments that try to validate to which extent the theory holds when the network width becomes smaller.\n\nI was not capable of reviewing the calculations presented in the appendix. Therefore I cannot stand for the correctness of the results presented in the paper. As I am not an expert in this field, it is also possible that I missed some important related work that should have been cited.\n\nSome other small remarks:\n- It would be useful to provide some more detail as to how the $\\eta_i$ are computed.\n- There are typos and small English mistakes everywhere that should be fixed.\n- The discussion section would benefit from some comments on which (if any) existing continual learning algorithms that are designed to reduce forgetting might be analyzed in the NTK regime, with similar tools as those used here.\n",
            "summary_of_the_review": "This is a well-presented theoretical study that applies previously developed techniques to analytically describe transfer/continual learning in neural networks. The results are to the best of my knowledge novel. The theory reveals interesting rich phenomena that occurs in actual neural network simulations.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}