{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Oral)",
        "comment": "This paper provides well-written and thorough analysis demonstrating that closed-set recognition performance correlates with open-set recognition performance, and that simply making the close-set model strong via augmentation, label smoothing, etc. along with small scoring changes (using logits rather than softmax probabilities) can get close to (or better than in some cases) performance than much more complicated methods. The authors also propose a large-scale benchmark that varies the semantic similarity across classes, allowing for a more fine-grained analysis of this problem. \n\nOverall, all of the reviewers thoughts that the paper provides very thorough validation of an insight that would be very interesting to the community. Reviewer HAFU had some concerns about novelty, since a number of papers have shown closed-set classifier improvements (and therefore better embeddings) benefit related problems such as few-shot learning and generalization to novel domains, as well as proposed large-scale experiments. The rebuttal convinced this reviewer, however, that some of the contributions and findings are unique and provide additional evidence to the community, and the new setting provides more fine-grained analysis. Reviewer dw7J had a number of suggestions in terms of additional evaluations, and the rebuttal either clarified why it is not possible or added them. As a result, after the discussion the reviewers all supported acceptance of this paper. \n\nGiven the above discussion, and rebuttal/changes to the paper, I recommend acceptance. It is a very well-done empirical paper, provides interesting findings, stronger baselines, and thorough experimentation. Further, some of the smaller findings (ViT correlation experiment) as well as larger relationship between open-set recognition and out-of-distribution detection are valuable contributions to the community. Finally, I would recommend this paper as oral, given that it may garner a good discussion of these contributions."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper makes an observation that good representations for open set detection would be correlated with high closed set accuracy.\nTo validate and illustrate this observation they improve closed set accuracy on existing open set benchmarks with different architectures and demonstrate  improvement in open set detection performance. \nThis paper also proposes new benchmarks for openest detection with fine grain details, which haven’t been studied in the setting of openest detection.",
            "main_review": "The core technical contribution or claim of this paper is that closed set accuracy is important for openest detection. This is known for researchers working on openest detection, though not highlighted enough. For any representation one key challenge of open set detection is in distinguishing confusing instances (incorrect predictions) within closed set vs novel category instances along the decision boundary. So as the quality of closed set classification improves this overlap could decrease and hence would enable easier detection of open-set instances.\n\n[1] makes similar claims that vision transformers result in better open-set detection without complex detection mechanisms. [3],[4] highlight the fact that good representation is all you need for meta learning or out of distribution generalization which though orthogonal to open-set detection, informs the research community that ‘quality of representation & performance on closed set’ is useful for auxiliary and relevant tasks of OOD, few-shot learning.  \n\nSimilar to the observation of this paper of closed set accuracy, many works in ‘openworld’ competition [5] at CVPR has taken various strategies to improve closed set accuracy in pre-training phase to improve closed set accuracy and leverage it for open-set performance boost.  \nAnother key contribution of this paper is proposing large scale ImageNet benchmark, [5] is a CVPR’21 competition and existing benchmark along the similar lines which unfortunately makes benchmarking contribution not significant enough. \n\nStrengths:\n\nThis paper focuses on an important problem and highlights an observation and it’s simplicity is especially relevant for practical settings and safety-critical applications.\nThis paper is well written and has great empirical evaluation methodology and demonstrate SOTA compared to other methods with additional components.\nThis paper proposes additional benchmarks for openset detection leveraging fine-grained classification datasets, and this is adds a valuable dimension to openset benchmarks.\n\nWeakness:\n\nThe core technical contribution(s) does not seem convincing to warranty a conference acceptance, especially given previous works making similar conclusions and contributions. \nThis lacks sufficient commentary on why a good closed set accuracy would result in better open-set detection.\n\n\nSuggestion to Authors:\n        Visualization of representations (albeit noisy) with and without augmentations/modified training could be insightful. May be including few examples of how hard novel instances/samples map onto different representations.\n\tFor completeness please consider including CIFAR100 (close set) vs CIFAR10(open set), as the number of closed set classes increase the task of openset detection becomes more challenging, this version of experiment is included in [2] which this paper includes as baseline and [1].\n\nReferences:\n\n[1]  Exploring the Limits of Out-of-Distribution Detection. http://www.gatsby.ucl.ac.uk/~balaji/udl2021/accepted-papers/UDL2021-paper-001.pdf\n \n[2] H. Zhang et al. Hybrid Models for Open Set Recognition https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123480103.pdf \n\n[3] Y. Tian et al. Rethinking Few-shot Image Classification: A Good Embedding is All You Need?  \nhttps://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590256.pdf\n\n[4] J. Miller et al. Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization. https://proceedings.mlr.press/v139/miller21b/miller21b.pdf\n\n[5] Open World Vision CVPR’21 Competition and workshop https://www.cs.cmu.edu/~shuk/open-world-vision.html, https://eval.ai/web/challenges/challenge-page/1041/overview \n",
            "summary_of_the_review": "This paper focuses on an important problem of open-set detection and demonstrate that a good closed-set representation is a very strong baseline and competitive with more complex methods for open-set detection, which is valuable for many practical settings.\nThough empirical evaluation is comprehensive and illustrate performance on large scale benchmarks, given existing prior works novelty of contributions seems rather limited",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper first analyzed the correlation between closed set classifier and open set classifier. Then based on this finding, cross entropy closed set classifier is enhanced with a few recent accuracy improvement methods, then it is transferred to a open set classifier and achieve good performance. And also experimentation is conducted to compare with other SOTA close set classifiers, and that the proposed enhanced OSR classifier based on cross entropy baseline can achieve similar accuracy. Finally a new benchmark dataset is generated. ",
            "main_review": "This paper is dealing with the open set recognition problem which is a challenging research problem. Based on some findings about the correlation of closed set recognizer and open set recognizer, this paper found a way to achieve a strong OSR through an enhanced close set recognizer. This is a valuable finding for future open set recognition research. \nThe weakness of this paper is that there are not more in depth analysis the behind reason for these findings, thus the value of this paper is not that significant. So this is possibly a direction to make this paper even stronger.",
            "summary_of_the_review": "This paper proposed interesting finding about the correlation between closed set classifier and open set classifier, and leveraging this finding, a new SOTA open set classifier is generated from a baseline classifier, which has similar performance as  other SOTA close set classifiers. Also a new benchmark dataset is generated.\nOverall it is a valuable paper, which is clearly written, and with good experimentation results to support. The weakness of this paper is lacking of more in depth analysis, and providing more insights to the behind rationale, which lower the value of this paper.  \nSo overall I will recommend marginal acceptance of this paper. \n ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper considers the problem of open-set recognition, in which a visual classifier should be able to distinguish images of the trained categories from images of other different categories.\nThe authors show that there is a strong correlation between results on open-set scenario and the close set-scenario (the classic problem in which the model is trained and tested on the same semantic categories). Then, they show that a baseline based on ranking the logits of a model trained on standard cross-entropy training can be very competitive with more complex methods when trained with strong data augmentation and other improvements. \nFinally, they propose new benchmarks for open-set recognition emphasising the fact that in contrast to other similar tasks, in open set scenario, the images that should be classified as other classes not seen during training should belong to a different semantic category, and not to other kind of distributional shifts.\nThe take home message of this paper is that at this stage of research for improving open-set recognition, the best that we can do (or almost) is to use stronger models for close-set recognition (as the title suggests).",
            "main_review": "Strengths:\n- The paper is well written, clear and straight to the point.\n- The appendix add more experiments and details to further understand the main paper statements.\n- Up to my knowledge the most important references in the field have been considered\n- The paper belongs to the category of very important papers that show that sometimes in research we consider very complex solutions, whereas a properly tuned standard approach can do the job as well.\n- The authors propose a clear distinction between open-set recognition (in which the none-of-the above class comes form well defined classes) and out-of-distribution (in which the none-of-the-above can be any kind of image). It is important to clarify the different tasks.\n- It is interesting to see that in new and not overfitted datasets, cross-entropy baseline seems to perform better than the best method for open-set scenario. \n\nWeaknesses:\n- Authors compare with only ARPL and OpenHybrid approaches because they say that the other approaches perform lower on open-set benchmarks. I think it is still important to consider also the other methods especially because the authors propose new benchmarks and on these benchmarks the ranking between the methods changes. So it could be for the other methods. I see that a more complete evaluation is performed in the appendix E (Table 6), however this is not done for the new datasets (Table 3).\n- In Fig.2 I would expect to see also results for OpenHybrid. Also, in the figure, for the simplest datasets (with performance close to 100) there is not much correlation between open-set can close-set performance as ARPL does not improve over cross-entropy on the close-set scenario. It is something that should be mentioned.\n- Results in Table 1 for the cross-entropy are presented with a ranking based on the classifier logits, which in my opinion makes more sense. However, as previous works use the softmax scores, it is important to compare the two ways. I see a comparison in Appendix B (Fig. 6c), but this is only for a model. I would like to see this comparison at least for the models in tab. 1.\n\nAdditional questions/comments:\n- Fig.1a presents ARPL in the legend, without really having yet introduced the method.\n- The authors propose a clear distinction between open-set recognition and out-of-distribution. However, is this distinction really necessary. Could not the two fields be unified? It would be more interesting to know if the same conclusions of this paper are valid also for out-of-distribution problems.\n- The \"more theoretical\" justification of the results at bottom of page 4 does not add much, but I do not have suggestions on how to improve it.\n- In Fig.3 it is interesting to see that ViT seems to have a better generalization to the open-set scenario on ImageNet. Is it due to the fact that it has been trained on more data or it is the reduced inductive bias (no convolutions)? It would be interesting to see if with different sizes of ViT, the correlation between open and close-set scenario still holds.",
            "summary_of_the_review": "The paper is well written and introduces interesting results that can change the understanding of the open-set recognition. Experimental results prove the main conclusion of the paper, but some additional experiments are needed (see above) to further understanding the problem. Also, in my understanding the distinction between open-set recognition and out-of-distribution is ficticial and the proposed cross-entropy baseline should evaluated also on out-of-distribution settings.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}