{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Oral)",
        "comment": "This paper takes inspiration from Global Workspace Theory to propose a modification for attention-based network architectures. This is exemplified both in transformer models and in recurrent models (RIMs). The key idea is to replace the quadratic, pairwise communication between \"specialist\" units (which in transformers corresponding to the positions) by a higher-order communication model which consists in a competitive, sparse writing step into a shared workspace, followed by a reading step where information is broadcasted from the global workspace to all specialists. The competitive writing step establishes a limited bandwidth channel for this communication which encourages specialization.\n\nThe reviewers agree that this is an interesting and very well-written paper which unifies several existing ideas. The main contribution of this paper is in establishing a connection to GWT which may inspire future research to keep developing these ideas. The experiments on relatively small tasks (but challenging ones) provide a good proof of concept. Some concerns pointed out by some of the reviewers include a certain overstatement of the capabilities of the proposed model, as well as lack of experiments that scale up the model to larger and unstructured datasets. The authors replied with additional experiments included in the appendix, which in my opinion address these concerns convincingly. \n\nOverall, this is a strong paper and I recommend acceptance. I encourage the authors to take into account the reviewer's suggestions in the final version. I also think that the connection to related work could be improved, as there is several related works [1, 2, 3] which asks/investigates similar questions to this paper and should probably be acknowledged:\n- The \"shared global workspace\" of this paper (Transformer + SW) is reminiscent of the Star-Transformer [1], as well as other more recent works which use special units (e.g. CLS tokens) to encode \"global\" representations. While that work does not include the competitive component (the \"bottleneck\"), I think it should be acknowledged.\n- Variants of transformers with competition among specialists via sparsity have also been proposed, e.g. adaptively sparse transformers [2]. That framework is an alternative to top-k softmax used in this paper.\n- Empirical studies which analyze the redundancy among specialists (in this case attention heads) and propose strategies to prune them have also been made by [3]. \n\n[1] https://arxiv.org/abs/1902.09113\n[2] https://arxiv.org/abs/1909.00015\n[3] https://arxiv.org/abs/1905.09418 \n\nMinor point: \"Hence unlike pairwise interaction, messages passed among neural modules in the shared workspace setting also include HO interaction terms\" -- I believe higher-order interaction happens too every two layers with pairwise interaction. Perhaps this should be clarified."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a communication framework to have multiple modules communicate and switch precedence efficiently, taking inspiration from Global Workspace Theory in cognitive science. The primary contribution is a scheme to replace complete pairwise communications in modularized architectures with a single, limited-capacity workspace that persists and changes over stages of computation. This workspace is implemented with a read/write scheme that iterates through stages of computation (layers in a Transformer or steps in a recurrent architecture): in the write phase, the shared memory is updated according to the current states of the modules that are most informative to the shared memory's current state as determined by a key-value attention scheme, with the modules competing via softmax. In the read phase, the internal states of all modules are updated via another key-value attention scheme. The advantages are claimed to be 1) higher order interaction among modules because every module learns from every other one (at least, more than pairwise), dynamic filtering because the memory persists and updates stage to stage; and linear computational complexity because the number of memory slots doesn't change much (and it's typically small, 1-10).  \n\nThe related work motivates this paper by the classical AI principle that intelligent systems should have multiple specialized modules rather than one general entity. It distinguishes from prior slot-based memory work in that memory writes here are sparse and competitive, and prior work on reducing computational complexity of Transformer dot product attention through its persistent memory. It aptly assesses itself as a unification of existing ideas. \n\nThe experimentation section tests different parts of the proposed scheme. The triangles experiment tests the comparative speed to convergence (and accuracy) of the HO communications here compared to pairwise in baseline Transformer. The MNIST generation experiment with TIMs shows that the shared workspace gives an advantage on domains where input dimensions are mostly independent. The CATER experiment shows a similar result to the triangles experiment (quickly picking out only relevant information) but in a time series, and the Sort-of-CLEVR experiment again reinforces the power of the shared workspace on sparse tasks. The physical reasoning shows general improvement, and the Atari performance shows considerable improvement due to modularization. \n",
            "main_review": "I enjoyed reading this paper! I think the strengths fall into three main categories: good problem setup that took the reader from concept to formulation well, an interesting idea with good scoping, and an impressive experimentation suite. More details:\n\n**Good problem setup:** I found the discussion of advantages of a shared workspace compelling. The proposed structure is intuitive and simple, but nonetheless convincing about its claimed advantages of higher-order communications, dynamic filtering, and complexity advantages. The qualitative parts of the writing are strong, effectively building a case for reading this paper. \n\n**Interesting idea with good scoping:** I appreciate that this is one architectural change that considers all inputs and outputs, and shows its advantages over the right baselines. This work avoids the trap of making minor changes and then being unable to really ablate them. The paper does seem to be a unification of existing ideas, but I distinguish that from (and prefer it over) merely concatenating existing ideas. \n\n**Impressive experimentation suite:** There are of course many experiments with thematic consistency, which is great - always better than papers with two testbeds and virtually the same experiment over and over. The first four testbeds make a nuanced point about sparse inputs, and the results on Atari are particularly exciting because they make a general claim and show significant improvement. \n\nMy critiques are as follows, in order of significance: \n\n**Content**\n1. Main critique: the goals of this paper seem to be to show that accuracy and performance improve **overall**, with a mechanism of higher order communications, dynamic filtering, and linear complexity that we will be convinced are attributes of the SW. This paper doesn't quite get there, though it gets part of the way there. \n\nFrom my understanding, Triangles, CATER, and Sort-of-CLEVR all tell us that the higher-order communication and single channel will help identify relevant information earlier in the pairwise communication, leading to faster convergence - *if* information is sparse in the input. The MNIST generation experiment tells us something similar about independent regions in inputs. Figs 4 and 5 are convincing to this end. However, the main claim - higher-order communication and a sparser connection of graphs - help for inputs with sparse information in various locations - is less surprising than a claim of general improvement. Even if that's still interesting (it is), the intro and conclusion seem to make a more general claim. Physical reasoning and Atari show more general improvement (and the results on Atari are impressive), but they're also the weakest, most qualitative parts of Sec 4. The ideal solution would be more robust testing of general improvement in these two testbeds, but if not then I would at least appreciate a claim that acknowledges the common and specific nature of the first four testbeds. \n\nThis is the crux of the motivation behind my score: I worry that this paper overstates its claim and the real claim is elegant but not that surprising. It's made even murkier by a couple promising results that receive little attention. \n\n2. Relatedly, the Atari section makes a claim about considerable improvement due specifically to more appropriate modularization. That would be cool and a real testament to this approach, but I would need more specific experimentation to demonstrate that the effect really was more appropriate modularization. \n\n**Presentation**\n1. The mathematical formulation is quite clear, but it's not presented clearly. I had to puzzle over the paragraph for a while, not because it was overly complicated but because it took a lot of treasure hunting to find all the parts and put them together. The issue is that the *Notation* and *Step 1/2/3* sections write the math into large blocks of text without pause. It would be more helpful to present some end-to-end equations, then break those down in text. It can be unclear without doing it myself whether something is being element-wise updated, transformed, dotted, etc. \n\n2. I really appreciate the slew of testbeds. However, that's how they come across: a laundry list of experiments, one after another, even though they support similar claims. That robustness can be shown by having a claims-driven structure for the results section rather than a testbed-driven structure, which to the reader is arbitrary and doesn't deliver the important information as well as it could. E.g. section titles like \"Performance advantage on sparse data\", then talk about the different results that show it. If one experiment shows multiple advantages, multiple sections can still point to the same figures. \n\n3. The figures are confusing - they are cramped, small, and have lots of acronyms I have to hunt down in various parts of the paper. They're all helpful content-wise, but this could be shown better with something as simple as better aesthetics and tagging.\n\n4. Nit: the text seems to switch between \"specialist\" and \"neural module\". Are these the same thing? Is \"specialist\" an abstraction/metaphor for \"neural module\"? Is \"neural module\" an example of \"specialist\"? All of those are great, just signpost and explain the switch. \n\n \n",
            "summary_of_the_review": "**Strengths:**\n1. Good problem setup\n2. Interesting idea with good scoping \n3. Impressive experimentation suite \n\n**Critiques:**\n*Content:*\n1. The experimentation section mainly convinces me that this architecture is good for accuracy and convergence speed on sparse inputs, not that it's generally advantageous - but the latter seems to be the claim of the paper, and the former is less surprising given the nature of the architecture (though still interesting - it's compelling to see that the architecture works). \n2. The experimentation on general improvement is a bit weak. \n\n*Presentation*\n1. Formulation is clear, but it's presented piecemeal in long paragraphs that make it hard to parse and put together. Since the formulation is simple (a good, elegant thing), it's frustrating to spend a lot of time hunting down all the pieces.\n2. Excellent thoroughness in testbeds, but the results don't have to be presented in such a process-driven way. They can and should be claims/driven. \n3. The figures need to be more readable \n4. \"Specialist\"/\"neural module\" distinction is unclear (see main review)",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a modification for attention-based network architectures drawing inspiration from the global workspace theory in cognitive science. Essentially connections are made sparser, with different ‘specialist’ units communicating with each other through a limited bandwidth channel.\n",
            "main_review": "Strengths\n\n1. Interestingness of the approach\n\nThe proposed method is appealing since it applies a theory from cognitive science (the global workspace theory) to information processing in networks. The implementation of the idea is quite simple - individual ‘specialist’ units interact with a shared memory layer instead of with each other directly, and the shared memory then broadcasts information back to each of the specialists. The exact means by which this interaction is carried out is by using key-value attention. This approach when applied to transformers leads to much higher computational efficiency (linear instead of quadratic in the sequence length).\nFurther research in probing similar ideas for much more efficient information processing, backed by ideas from cognitive science, would be very beneficial for the field in my view and this paper would help the community in that regard.  \n\n2. Experimental Evaluation \n\nThe paper includes exhaustive experimental evaluation over 5-6 environments (including object tracking, and relational reasoning) where the solution requires considering a small portion of the input data, the authors demonstrate that adding the shared workspace model to attention based architectures like Transformers and RIMs leads to superior asymptotic performance, and faster learning. These environments include object tracking (CATER), relational reasoning (sort of CLEVR) and Atari games. The authors also include details about the experiments and algorithm implementation in the appendix, which will aid in reproducibility. \n\nWeaknesses \n\n1. Unclear if the approach will scale to larger, more unstructured datasets (where current attention based architectures have already been shown to thrive)\n\nMost of the experiments considered involve problems that only require a small portion of the input to be solved (eg: just the patch with the points for the equilateral triangles, or just the target object for Cater object tracking). I am concerned whether the framework proposed here will also be effective in settings where this is not necessarily the case, such as general modeling of language and images. For example, adding shared workspace to transformers imposes a communication bottleneck between representations at different positions of the sequence. It is possible that for problems where the solution does not depend on only a small portion of the input, considering the pairwise relationships of representations at every point in the sequence is critical for good performance. \nTo study this, the shared workspace model would have to be evaluated on larger, more unstructured datasets, where transformer based architectures have already been demonstrated to do well (for example the data on which GPT-2 was trained). Adoption of this approach would be much more widespread if the authors can demonstrate that on these larger datasets, training transformers with the shared workspace doesn’t lead to worse performance than training the regular transformer based models that are currently used. ",
            "summary_of_the_review": "The paper proposes an implementation of an interesting theory from cognitive science for more efficient information processing in networks, by making connections between entities sparser. While there is extensive evaluation on environments, most of these involve processing a small portion of the input. The paper will be made a lot stronger if the approach can be shown to scale to larger more unstructured datasets where transformers are known to work well. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a method of using external memory called \"shared workspace\" for communication among different neural modules or \"specialists\". The key idea is that there are limits on the communication bandwidth and the specialist modules must compete for access. The communication limit encourages specialization and compositionally and facilitate better synchronization. Experiments over a variety of tasks indicate the proposed Shared Workspace model improves performance over baselines.",
            "main_review": "Strengths:\n\n- The paper is written very well. The introduction provides a good motivation and challenges and illustrates the use cases clearly using examples. The related work compares the proposed approach with other memory-based neural models. The description of methods is concise, but sufficiently detailed.\n\n- The proposed approach is original to the best of my knowledge. The key idea is very intuitive and motivated by insights from cognitive science literature.\n\n- The authors perform experiments on a wide variety of tasks including a toy task for detecting equilateral triangles, multi MNIST Generation, object tracking, relational reasoning, physical reasoning and Atari video games. I also like that the authors use different types of backbones (Transformers and RIMs) in different experiments which indicates the proposed Shared Workspace method is not specific to certain kind of backbones.\n\nWeaknesses:\n\n- Although the authors perform experiments with wide variety of tasks, multi-agent tasks are missing where I believe coordination is more important. In all the tasks used in the paper, different neural modules process different parts of the input to make a common prediction. Multi-agent tasks are more challenging as each agent would get a different input but also predict a different action.\n\n- It is unclear how certain hyper parameters are chosen. For example why is the patch size 4x4 in equilateral triangles, 6x6 in object tracking? The number of memory slots, size of memory slots, etc. different across different experiments. How are these chosen?\n\n- The authors propose two versions of Shared Workspaces, soft and hard (SSW and HSW). Some experiments one contain a single version. Why are both SSW and HSW not evaluated in all the experiments?",
            "summary_of_the_review": "The paper proposes a novel method for coordination between neural modules which is well motivated. The experiments are comprehensive although some details are missing. Addition of multi-agent tasks would make the paper stronger.\n\nEdit after author response:\nI read the author response and I would like to maintain my positive rating. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Paper proposes a novel mechanism for information exchange between different neural subnets. It replaces the pairwise interactions with a share memory space. The memory is updated by a competitive scheme which the top-k updates are selected via the key-query-value\nattention mechanism. The shared memory/workspace then broadcast the updated state to all other upstream specialist subnets. Each specialist subnet then update its representation/weights accordingly.\n\nPaper claims 3 advantages of this approach: \n1. Higher-order (HO) interaction among neural modules\n2. Dynamic filtering due to persistence of memory. \n3. Lower computational complexity of using shared workspace for synchronizing different specialists\n\nExperimental results are very promising across a wide variety of tasks, namely DETECTING EQUILATERAL TRIANGLES; SORT-OF-CLEVR; CATER: OBJECT TRACKING;  for transformer architectures. Also, experiments on RIM for the BOUNCING BALL task and TIMS for the MULTIMNIST GENERATION and Language Modeling Task were also done. Finally, the approach was also experimented on model  free RL on ATARI game task.",
            "main_review": "Strengths\n1. Novel and significant contribution by proposing a new mechanism for inter-subnets information exchange/learning.\n2. Of high relevance to the ICLR community. The generic approach cuts across different AI discipline as the experiments are performed on vision, language and reasoning tasks.\n3. Experiments are comprehensive and well-explained.\n\nWeaknesses\nMinor: Some technical details are not elaborated for readers who are less familiar with the respective topics. For example, the section on the key-query-value attention mechanism is quite brief. It's the key contribution on the success of the proposed approach.",
            "summary_of_the_review": "Paper proposes a novel approach which produces commendable experimental results across a wide range of tasks in vision, language and reasoning. The impact of this work is very significant and is of high relevance to the community. While I am not entirely familiar with the cited prior work on key-query-value attention mechanism which forms the backbone of the proposed approach, the entire paper is very well-written and comprehensible. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concerns.",
            "recommendation": "10: strong accept, should be highlighted at the conference",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}