{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Oral)",
        "comment": "This paper examines the extent to which a large language model (LM) can generalize to unseen tasks via \"instruction tuning\", a process that fine-tunes the LM on a large number of tasks with natural language instructions.  At test time, the model is evaluated zero-shot on held out tasks.  The empirical results are good, and the 137B FLAN model generally out performs the 175B untuned GPT-3 model.\n\nAll reviewers voted to accept with uniformly high scores, despite two commenting on the relative lack of novelty.  The discussion period focused on questions raised by two reviewers regarding the usefulness of fine-tuning with instructions vs. multi-task fine-tuning without instructions.  The authors responded with an ablation study demonstrating that providing instructions at during tuning led to large gains.\n\nOverall the paper's approach and detailed experiments will be useful for other researchers working in this fast moving area in NLP."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper describes an approach to fine-tuning large language models which can improve zero-shot accuracy on unseen tasks. ",
            "main_review": "Overall well-written with compelling results, this paper describes a new language model (FLAN) and shows how it improves upon the zero-shot task performance of previous language models such as GPT-3. While the paper is lacking some additional analysis, I am hesitant to recommend extremely compute-intensive ablations due the large size of the model (137B parameters).\n\nStrengths:\n - Considers a reasonably wide set of 62 datasets; although the inherent arbitrariness in dataset clustering was listed as a limitation, the clusters look quite reasonable to me, and the removal of overlapping datasets (e.g., \"Reading Comprehension w/ Commonsense\") seems appropriate.\n - Results are better than a strong Base LM baseline, as well as existing state-of-the-art models (GPT-3)\n - Overall the approach is intuitive and conceptually compelling\n - Highly relevant to ongoing work on language modeling, prompt tuning, and zero-shot learning\n\nWeaknesses:\n - From these experiments, it is unclear whether models are actually \"learning to follow instructions\" or just learning a very large space of tasks from the fine-tuning procedure. In other words, even though prompt variance is reported at inference time, the models could potentially perform just as well with nonsense or missing prompts during fine-tuning. As far as I can tell, no experiments that rule out this possibility exist.\n - Although qualitatively useful, the analysis in 4.1 does not conclusively show that the number of instruction tuning clusters aids performance, or that this trend is likely to continue with more clusters. Most of the gain could be acquired by tasks which are most difficult, or most similar to the heldout task, and this analysis cannot disprove such an interpretation. A proper analysis would consider more heldout tasks and permutations of training data, but presumably this is prohibitively expensive.\n - The paper is missing important details about hardware usage and training time\n - Some possible issues which might be resolved by the additional questions below\n\nAdditional Questions:\n - \"For each dataset, we manually compose ten unique templates that use natural language instructions to describe the task for that dataset.\" Do you have unique prompts for each dataset or only for each dataset cluster? Based on a cursory look at the supplementary material, I would assume the latter.\n - I didn't fully understand the justification for the OPTIONS token. Are the fine-tuned models successfully putting (almost) all of their probability mass on the corresponding options? How is the Base LM evaluated (if it's not fine-tuned, presumably it doesn't learn how to handle these options)? \n - Figure 6A: why does the untuned model see worse performance with more parameters?\n\nNits:\n - Figure 1 (Bottom) is possibly misleading, since AFAICT zero-shot FLAN underperforms few-shot GPT-3 on the majority of tasks\n - Not clear what \"turning the task around\" means for some tasks, or why this is a useful type of prompt diversity",
            "summary_of_the_review": "I give this paper a strong recommendation, in spite of some missing ablations. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper explores a simple and effective method to improve zero-shot performance of pretrained language models. Authors take a 137B parameter pretrained model and finetune it on multiple tasks verbalized via natural language instruction templates. As the result, the instruction-tuned model performs well on un-seen tasks with the zero-shot setting.",
            "main_review": "Pros:\n1. The problem addressed has high practical value: it tries to make large pre-trained language model more accessible to a range of NLP tasks. The \"instruction tuning\" idea will significantly reduce the cost for task-specific fine tuning, labeled data and prompt engineering compared to other approaches. \n2. The method is simple and easy to implement. Authors carefully design the experiment to minimize the leakage between the fine-tuning and inference data. Given that, it still shows superior performance on different types of NLP tasks. The result on specific task can be further improved when adapting with \"prompt tuning\" on labeled data, which shows that the instruction-tuning process does not drop much task-specific knowledge from the original pretrained model.\n3. The analysis presented in the main paper and the appendix is thorough enough. Authors also discussed about the limitation of model when downstream tasks are more similar to language modeling tasks.\n\nCons:\nThere are still a few questions that can be addressed to make the analysis comprehensive.\n1. Have authors try to use the FLAN prompts on GPT3 or BaseLM and how does the performance look like?\n2. Since instruction tuning will adjust all the parameters in the original pre-trained language model, there is a question what about what is the potential impact of this tuning process? Will it drops any knowledge of any tasks, which will be a disadvantage when the task's labeled data is available? In the Analysis C in the appendix, it will be good to have results for tasks other than classification such as summarization or question answering; and also to have a baseline where the BaseLM model is fine-tuned directly with the task labeled data (without prompt/soft-prompt).\n",
            "summary_of_the_review": "Overall, the paper proposed an interesting idea and showed strong empirical results, hence I vote for accepting.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper creates a dataset of over 60 NLP tasks described via instructions (using templates for each task) and finds this boosts zero-shot performance on unseen tasks.",
            "main_review": "----\nDetailed comments:\n----\n\n- \"For each dataset, we manually compose ten unique templates\":  Why not have templates per task cluster instead of per dataset?  it is likely a relatively minor effect given the results from Appendix B but it seems like it could slightly prevent overfitting\n\n- The ablation in 4.1 was great (number of clusters).  Nit: I would have tried to move the (datasets per cluster/templates per dataset) ablation to the main body as well and shortened Section 3\n\n- The 4.2 (scaling laws) ablation is perhaps the most interesting of all.\n\n- In figure 6A, why was performance not increasing for untuned models w.r.t model size?  This seems to contradict findings from Brown et al where larger models did better on essentially all tasks.  Were there perhaps some poor datasets that happened to be in the held-in split (since the held-out tasks don't seem to have the same trend)?\n\n----\nAppendix:\n----\n\nI liked the section B ablations (as implied above).\n\nThat more templates per dataset didn't help is particularly interesting and suggests some questions. You hypothesize that more templates doesn't help because \"models at such scale do not easily overfit to a finetuning single task\" - but my intuition is for an opposite explanation -- that the models at such scale easily memorize a small number of templates!  One may even wonder if the instruction nature of the templates is helping at all.  \n\nFrom what I can tell, Appendix C on prompt tuning (which is very interesting) is maybe the primary evidence the instructions are important.  I think more could be done here, some ideas, probably there are better ways to test:\n- Have templates that leave out \"instructions\":  I would guess it wouldn't affect held-in task performance much, but would affect held-out tasks.  \n- Consider HellaSwag/PiQA/etc, where FLAN underperformed few-shot and even zero-shot.  One might hypothesize that if using a (subotimal) template that is less natural for language modeling, that zero-shot performance would suffer, but that FLAN performance wouldn't\n- One might hypothesize that the \"turn the task around\" templates help more than the other more straightforward templates that don't swap information between the prompt and response.\n- Easy but probably not great thing to try:  held-out tasks with wrong/useless templates\n\nA final thought:  It's not obvious that using as many training examples per dataset as possible is optimal, given that the model could overfit to dataset-specific spurious correlations.  This could be another area to investigate\n\n----\nMisc: \n----\n\n- UnifiedQA seems potentially worth citing as prior work",
            "summary_of_the_review": "Overall, the paper's idea is powerful (but of somewhat limited novelty) and the results are good (but not great).  Its greatest strength IMO was the ablations.  My biggest complaint is that it's not completely clear the instructions themselves are important at all - I suggest a few more experiments, though they don't seem crucial.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a simple method, \"instruction-tuning\", to improve the zero-shot learning capability of large language model, which 1) annotates prompts for a wide range of tasks and then 2) fine-tunes the model to \"answer/respond to\" those prompt. The empirical results are impressive: after instruction-tuning, the 0-shot performance is better than GPT-3 (0-shot, sometimes few-shot) on a wide range of datasets; nevertheless, on datasets with formats already similar to language modeling, the performance gain is negligible or even negative.\n\nThe paper also made a few other observations 1) performance benefits from the number of task clusters 2) instruction-tuning is only beneficial when the model size is larger enough, and 3) few-shot learning still helps. ",
            "main_review": "While the method is a simple and straightforward scaling up of concepts and ideas from prior works (e.g. Zhong et al, Adapting ...; Mishra et al, cross-task generalization ...), the empirical results are thorough and impressive (outperforming GPT-3 with a slightly smaller model). The analyses also helps us understand when this method would work and inform us about future research directions.\n\nBelow are my concrete questions and comments: \n\n**Additional Tasks Results (3.4)**\n\nIn Appendix A.1, the paper mainly draws conclusions based on comparisons between GPT-3 and FLAN, which I do not think are fair: GPT-3 and FLAN differ in model size and pre-training data distribution. Instead, I think Base LM vs. FLAN might be a better comparison between “Off the shelf LM” and “instruction-tuned” model (though it won’t change the conclusion).\n\nIt is also worth pointing out in the main paper that for most of the additional tasks, even though it does not lead to higher accuracy, the performance of FLAN is still at least comparable (e.g. <1% worse and difference generally negligible) to Base-LM 0-shot. The only outlier seems to be ReCorD where the performance drops significantly after instruction-tuning, and this probably deserves some discussion. \n\nAlso I might have missed it - for the Base LM 137B zero-shot result, is it on the average template, or the best template? \n\n**Number of Task Clusters (section 4.1)**\n\nFor Figure 5 can you add the untuned model to the curve with the x-axis=0 (0-task cluster)? This can help us understand how much even 1 cluster (e.g. summarization) may help. \n\n**Explanation for scaling (section 4.2)** \n\nIt is an insightful empirical result that instruction tuning only works when model size reaches 68B. However, I am not entirely sure about the potential explanation of “model capacity”. There might be two potential explanations to this phenomena: 1) “model capacity”: as the paper has mentioned, smaller pre-trained models do not have enough model capacity and underfit the instruction tuning data, and 2) “better OOD generalization”: better quality pre-trained models have higher OOD generalization ability (and OOD accuracy) and they are less likely to “overfit” to in-distribution data. \n\nI personally find the second explanation more convincing. For example, Sahn et al. (https://arxiv.org/abs/2110.08207) finds that even models with only 11B parameters can generalize to unseen tasks, using T5 (MLM) and a larger set of prompts. The use of MLM objectives (might) improve the pre-training quality, while more prompts reduce the “overfitting to in-domain data” issue. \n\nI appreciate the fact that the author explicitly states the model capacity hypothesis more as a conjecture rather than a solid explanation. It’d be great if the authors can support the explanation further with more empirical evidence. On the other hand, however, since the results from Sanh et al came out only 2 weeks ago,  I would not change the score based on the response to this question. \n\n**In-context Few-shot vs. Fine-tuned Few-shot (Section 4.3)**\n\nCan the authors compare “fine-tuning/prefix-tuning an instruction tuned model with 16 examples” (appendix C but with only 16 examples) with “in-context prompting” (in 4.3 of the main paper), similar to Chen et al. (https://arxiv.org/abs/2110.07814 )? This would further inform us how we should use the few-shot learning examples for larger language models: put it in-context, or fine-tune? Again, since the comparison of Chen et al. came out only 2 weeks ago and the paper limit is 9 pages, I would not change the score based on the response to this question. \n\n**Others**\n\nResults of Appendix C are interesting and potentially impactful - this might imply that instruction-tuned models will become the new “base” model for the pretraining-finetuning paradigm. Is it possible to briefly mention it in the main paper as well (and redirect the readers to the appendix to see the full results)?\n\nIt might be too late to change the name, but “Finetuned LAnguage Net” (FLAN) is uninformative, since it does not capture any unique aspect of this method. What does “LAnguage” mean here, \"natural language instruction\" or \"language model\"? If it is the former, then directly including the word “instruction” might be better; and hopefully it’s not the latter, since even fine-tuned BERT on SST-2 counts as a fine-tuned language model ...\n\n**Typo**\nIntro: \nInstruction tuning is “a” simple method that, as ….\n\nConclusion:\nMoreover, our work supercedes recent work such “as” \n",
            "summary_of_the_review": "While the method is not new, the empirical results are strong and comprehensive. Though I disagree on the interpretation of some empirical results, overall the additional analyses bring us further insights on what method works for very large language models (i.e. > 100B dense model). I highly recommend the paper to be accepted to ICLR 2022. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}