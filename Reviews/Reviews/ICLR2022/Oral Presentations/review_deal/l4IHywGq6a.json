{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Oral)",
        "comment": "This paper presents an approach to learn graph grammars for molecule generation in a very data-efficient way.  The approach combines bottom-up grammar construction by contracting graph substructures and evaluation-driven learning of the parameters in grammar construction in order to optimize metrics of interest.  This paper is well written and the graph grammar learning approach is novel and can potentially have impact beyond just generating molecules.  All reviewers unanimously recommended acceptance of this paper.\n\nA few things emerged in the reviews and discussions with authors, regarding in particular the computational cost and scalability of the approach and the actual molecule sampling process after learning.  I hope the authors can clarify these in the paper, and as the authors said in the discussion that exploring a more expressive model that uses learned probabilities on the production rules can make the model more powerful, which is a promising direction for the future."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper introduces a new grammar-based generative model for the molecule generation task. The graph grammar is defined as a set of production rules that operate on module graphs (i.e. the molecule graphs are not linearized as done in some previous work). The graph grammar is learned by iteratively contracting hyperedges (i.e.  edges that can connect multiple nodes, defined by simple chemistry-inspired rules) into non-terminal nodes and the submission proposes to learn how to sample the hyperedges by a REINFORCE algorithm that optimizes for several molecule generation metrics. \n\nThe evaluation is done both on small and large molecule generation datasets. Notably, the proposed method achieves strong performance while being very data-efficient.",
            "main_review": "The proposed method is novel and opens potential for new line of research in graph grammar. I personally find the submission to be clear and well-written. Overall, I am positive and would like to give a \"accept\". \n\nHowever, I have few more comments that I hope to help improve the paper. Several of them are around the modeling choices and some of them are to clarify my understanding of the practical usefulness of the proposed method.\n\n1. The proposed method uses pretrained graph neural network to generate the sampling weights and claim that this can be replaced by any plug-and-play feature extractor. Can the authors provide data to support this claim? If this is actually true, I would suggest to use the simplest feature extractor to keep the proposed method clean.\n2. Is the REINFORCE algorithm stable across random seeds? My hunch is that they are not. Can the authors provide error bars in for their data? Can the authors provide convergence curves to demonstrate the learning? \n3. While the proposed method is efficient, I have the impression that it does not scale up well to more data (the authors have to subsample the training set for the experiments in Table 3). My first request is for the authors to report the actual computation time for the proposed method on the experiments they have done. Second, aside from the computational challenge, can the proposed algorithm benefit from more data? If so, can we estimate how much we can gain by including the whole training set of the large polymer dataset?",
            "summary_of_the_review": "I find the proposed method to be novel and effective. the paper is clear and well-written. I have a few comments around modeling designs but I think the comments are addressable. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a sample efficient hypergraph grammar learning algorithm for generating molecules. The bottom-up search algorithm conducts iterative contraction according to a learned policy, where in each iteration, several hyperedges are sampled, contracted into a single node, and written into a production rule as part of the final grammar. The search policy is being trained using RL where the rewards are given by evaluating a set of molecules generated by the policy. The proposed method is evaluated on a small and a large monomers dataset and outperforms existing molecule generation algorithms in terms of the synthesizability and the membership rate.",
            "main_review": "This paper proposes a generic solution for learning to generate graphs in an extreme few-shot manner where only dozens of examples are provided, which is pretty common in the polymer/monomer domain. The method is general and can have a large impact on the graph learning community. Although technically the proposed method is similar to some previous work like MHG[1], it cleverly preserves the class-specific properties by operating on subgraphs instead of individual atoms. Also, the idea of learning a grammar search algorithm using RL instead of directly learning the grammar is novel and interesting in the domain. The proposed method is well-supported by experiments on datasets of different scales.\n\nThe paper is well-structured overall but there is still room for improvement. The notation in section 4 is a bit unclear. In section 4.1 the subscripts are usually used for denoting the number of iteration, and the superscripts are used for different connected components. While in section 4.2 right before Equation (3) we see there is $e_t^{(j)}$, where the meaning of superscript $(j)$ is unclear. And $t$ here is used both for denoting the current iteration and the total number of iterations. Also if I understand correctly, $X$ is a binary matrix, and instead of $p(X)=\\prod_t\\prod_j \\phi(e_t^j;\\theta)$, it should be $p(X)=\\prod_t\\prod_j \\phi(e_t^j;\\theta)^{X_{tj}} (1-\\phi(e_t^j;\\theta))^{1-X_{tj}}$?\n\nFor the experiment section, both Appendix A and Table (3) contain critical information for understanding the experiment setting and results, and so they should be put earlier in the main text. \n\nQuestion:\n\n1. The paper proposes a clean algorithm for learning grammar. But in practice, there must be a lot of hyperparameters to tune, such as the number of production rules, the maximum/minimum length of a single production rule, etc.? How do you decide these parameters? Do you do any pruning for the production rules?\n\n2. The main text does not mention how to generate molecules using the learned grammar rules. Do you sample the production rules uniformly randomly during testing? If yes, do you plan to learn a policy for generating molecules? How to jointly learn the generation policy together with the rule search policy?\n\n3. The space of grammar rules is combinatorial as mentioned in section 4.2. How does RL circumvent that and learn a reasonable policy?\n\n4. Why do you only use diversity and RS as the optimization objectives, as mentioned in Appendix A?\n\n[1] Hiroshi Kajino. Molecular hypergraph grammar with its application to molecular optimization. In International Conference on Machine Learning, pp. 3183–3191. PMLR, 2019.",
            "summary_of_the_review": "Overall, this paper makes decent contributions in both technical and empirical aspects, and therefore I recommend accepting the paper provided that the authors correct the writing issues.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents a data-efficient graph grammar-based approach to molecular generation (with a focus on polymer generation). The model requires that the molecule be represented as a hyper-graph. The productions of the grammar are basically rules for contracting connected nodes into a single node (a non-terminal symbol), which are later expanded during generation. Each rule is essentially derived from a random sample of the hyper-graph edges: the grammar is produced by iteratively contracting the nodes incident to the edges into non-terminal nodes until the hyper-graph consists of a single non-terminal node. This procedure is applied simultaneously to all the graphs of the training set. Once a grammar is obtained, it is optimized by maximizing some desired metrics (such as diversity). The optimization is non-differentiable since it requires generated graphs to evaluate the objective function, and thus the authors resort to approximate the gradient's expectation using MC sampling and the REINFORCE score function estimator.\nThe experiments aim at showing that the proposed model is data-efficient (achieving strong performances using a small fraction of the molecules used by competitors), capable of extrapolation (since it can produce molecules outside of the training set), and explainable (since it is shown to identify functional groups which characterize the family of generated molecules).\n\n",
            "main_review": "This is a pretty solid contribution. Although the paper is written clearly, I admit I had trouble trying to understand the method initially, but once I got the big picture, everything lined up nicely. The methodology in itself (generative learning through graph grammars) is not new, and the proposed model presents some similarities to the HMG model of Kajino. However, it draws from previous methods in a clever way and addresses most of their limitations (for example, the production rules involve substructures instead of single atoms). This, in my opinion, is a plus. The main advantage of this model is its data efficiency, which the experiments corroborate nicely. Nonetheless, I have a few points I would like the authors to address before I give complete acceptance. \n\n1) It is unclear to me what is the computational cost of constructing the grammar and training the model since generating the production rules involves graph matching (and although you affirm this isn't an issue). How does your training/grammar inference cost compares to other approaches? \n\n2) \"we also show that with more training data (0.3% of the whole dataset), our method can achieve better performance\". I might have missed it, but where is this shown?\n\n3) Why are some results in the tables underlined? You should explain it in the captions.\n\n4) This is more of a request for clarification. How is the final set of grammar rules chosen? If I understood correctly, the bottom-up construction process generates a lot of near-duplicates, as well as potentially useless production rules, especially during the initial iterations. Is there any pruning process after the optimization has been performed, or just everything is kept?\n\n5) How many rules does the grammar construction process produce in the datasets you evaluated?",
            "summary_of_the_review": "I am decidedly leaning towards acceptance. In my opinion, this is a strong contribution that deserves to appear at ICLR. I will most likely raise my score to an 8 after the authors will answer my questions and modify the text accordingly where needed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposed an interesting grammar learning-based data-efficient molecule generation model. \nThe key idea of this method is to learn a set of production rules via a hyper-edge selection process that optimizes a set of evaluation metrics. \nThis method achieves impressive performance on extremely small datasets and achieves competitive performance compared with end-to-end models trained on a large dataset. \n",
            "main_review": "Strength:\nThis method is very data efficient given grammar-based learning. \nThe hypergraph contraction-based production rule learning process is very novel (at least from my perspective).\n\nWeakness:\nIt seems like the model can only work on a small set of molecules to generate the rules due to the computational complexity.\nThe way this model learns the production rule is actually very related to a domain called task-based learning, where people tend to optimize their model towards some non-differentiable evaluation metrics. \nThe authors should reference some of those works, e.g. \n\"Task-Based Learning via Task-Oriented Prediction Network with Applications in Finance\", \n\"Task-based End-to-end Model Learning in Stochastic Optimization\".",
            "summary_of_the_review": "Due to the novelty of this data-efficient grammar-based molecule generation method and its impressive performance, I recommend to accept this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}