{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Oral)",
        "comment": "The paper proposes a new technique to handle oversquashing in GNNs by introducing a novel rewiring technique. The reviewers are quite positive about the paper and the rebuttal phase greatly helped clarify the method and it's impact."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper analyzes oversquashing in GNN using geometric methods. The paper proposes a novel rewiring method based on negative curvature to construct graphs that are less susceptible to oversquashing. Though the theoretical contribution is strong the empirical evidence is not strong. Overall a good paper. ",
            "main_review": "Overall the paper is well written, however, the paper improve further to have better readability. It would be helpful to have more intuitive clarity on the use of hyperbolic spaces since many readers may not be familiar with the topic. Throughout the paper, homophilic and hetherophilic nature of graphs are mentioned, however, the exact definitions of them are not clearly defined. Further, in experiments “low-homophily” is mentioned, but there is not measure for low or high homophily. I suggest that the authors provide a proper definition for homophilic/heherophilic as [1,2] and provide numerical measures for datasets used in experiments as in [1]  \n\nThe main weak point of the paper is experiments. First of all, is it fully-supervised node classification of semi-supervised node classification? The experiments are not substantial in terms of dataset selections, comparisons with baseline methods and obtained accuracy. \n\na) The selected baseline methods are limited, can the authors use further baselines methods? Can the proposed graph rewiring method be compared with the +FA method in  [1]?\n\nb) To my knowledge Cora is not a homophilic dataset [1]. It would be helpful if more datasets from hetherophilic graphs (Citeseer, Pubmed) are also considered in experiments, which would allow us to  have a better understanding of the over-squashing phenomenon.\n\nc) Cornell, Texas and Wisconsin are small scale datasets. I suggest to experiments with Chameleon or/and Squirrel dataset, which have large number of nodes and dense graphs. \n\nd) Why are accuracies of Cornell, Texas, and Wisconsin low? There are methods that have shown greater accuracy for these datasets [1]. I wonder whether the low accuracy is due to the use of GCN and if a different GNN method is used in with the proposed rewiring method the accuracy would increase. Do authors have any experience with other models?  \n\n\n\nReferences\n[1] Eli Chien and Jianhao Peng and Pan Li and Olgica Milenkovic, Adaptive Universal Generalized PageRank Graph Neural Network,\nInternational Conference on Learning Representations (2021)\n\n[2] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric graph convolutional networks.  International Conference on Learning Representations (2019)",
            "summary_of_the_review": "A good paper with a novel ideas on oversquashing in GNN. The main limitation of the paper is the limited of empirical support.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a new graph rewiring approach that utilizes a discrete notion of Ricci curvature to mitigate over-squashing. This is motivated by a link between negatively curved edges and graph bottlenecks. The paper has a theoretical focus, but also provides a set of validation experiments to demonstrate the proposed approach.",
            "main_review": "Detailed comments:\n- A critical aspect of rewiring is its effect on the structure and topological properties of the underlying graph. This is not captured by just analyzing the number of added/ removed edges across the graph. I think that it would be more useful to analyze graph characteristics (such as the node degree distribution) or to measure the distance of the original and the rewired graph globally, e.g., with a transportation distance. \n- Can you comment on the cost of curvature-based rewiring vs. random walk-based rewiring? If curvature-based rewiring is less efficient than random walk-based rewiring, is this (in your experiments) mitigated by a reduced cost in the downstream task (due to the smaller number of edges).\n- In section 4 you briefly remark that curvature-based rewiring may reduce over-smoothing. I think it could be interesting to expand on that.\n",
            "summary_of_the_review": "I found the paper very interesting in that it analyzes the over-squashing problems through a new, geometric lens. The theoretical motivation for using curvature-based tools as opposed to random-walk-based rewiring is well-done. My main concern is that the authors’ notion of graph structure preservation is not convincing to me. The experiments could be more comprehensive. Please see detailed comments.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": " The paper provides an analytical explanation for the over-squashing and GNN bottleneck phenomena. While Alon & Yahav (2020) demonstrated the over-squashing phenomenon empirically and provided mainly intuitions as to why it happens, this paper performs a deeper analysis and connects over-squashing to combinatorial curvatures. The paper shows that negatively curved edges are responsible for over-squashing. This is important because it allows to measure over-squashing, and pinpoint specific edges that are responsible for it in a given graph. \nFurther, the paper proposes a method (called SDRF) to re-wire the graph based on these insights and shows empirically how this method alleviates over-squashing.\n",
            "main_review": "## Strengths\n* The paper provides good intuitions to over-squashing. This intuition is visualized in Figure 1 that gives a good intuition of over-squashing and of the proposed SDRF solution.\n* Further, the paper provides a good connection to graph curvatures that allows to measure and analyze over-squashing.\n* The paper provides a thorough analysis using the proposed notations and connects it to further geometric theoretical and classical ideas.\n* The paper provides a good conceptual comparison to random-walk-based rewiring and whether this can address the GNN bottleneck, and also an interesting discussion about the likelihood of GNN bottlenecks in homophilic and heterophilic datasets.\n* The evaluation shows that the proposed SDRF method outperforms the DIGL baseline without a significant increase in the amount of computation compared to the original graph structure, while the DIGL baseline adds an order of magnitude more edges, which increases the computation cost.\n\n## Weaknesses\n\n* Generality - I am not sure how do these results generalize beyond Graph Convolutional Networks (GCN). The use of GCN's augmented normalized adjacency matrix is assumed at the beginning of the paper, and this assumption is never questioned.\nAre Lemma 1 and Corollary 2 correct only for GCN, where it is common to use the augmented normalized adjacency matrix? Specifically, this matrix has $D^{-1}$ which decays messages over paths. What happens if nodes are summed (as in GIN)? How does the signal then decay with the number of layers? Intuitively, I can understand that summing an exponentially growing number of nodes in a single vector cannot compress all the information after some compression steps (that is, GNN layers). But how is this expressed in the curvature theory? If the analysis in the paper refers only to GCN and GNNs which perform averaging, please mention this explicitly.\n\n* Writing - as is, the paper can be perfectly understood only by audiences that are very familiar with all related work. The paper can be significantly improved by giving more background, explaining equations more intuitively, and avoiding unneeded citations which confuse the reader. For example, the paper uses the names such as \"Ricci\", \"Poincare\", \"Ollivier\", and \"Forman\" frequently. These references can confuse audiences who are not familiar with all terms and papers. Specifically, calling the main method \"Balanced Forman\", gives the reader the feeling that they would not really understand the proposed method without understanding the original Forman first. I believe that this is not the case, and that the paper *could* be standalone.\nWhile the authors show an impressive understanding and familiarity with the related and classical work, I advise the authors to avoid this \"name-dropping\", in favor of better readability and accessibility to a broader audience.\nI even think that some parts of the paper can be moved to the appendix, in favor of the readability of the remaining parts. For example, I am not sure that Corollary 2 is new (see below), and I'm not sure whether this is directly related to the point the paper tries to make, or whether is it a side-result.\nSee more specific details below.\n\n* The Evaluation could also be strengthened by addressing more datasets and more GNN types.\nSee more details below.\n\n* Reproducibility - please provide more training details, hyperparameters tuned and ranges, and the final selected hyperparameter values for the experiments in Section 5. A table of dataset statistics (in the appendix) can also help, before and after applying every type of preprocessing method (similarly to Table 3, possibly with absolute numbers as well).\n\n\n## Writing\n\n* Section 2.1: what is \"$(i,j) \\in E$ if $i \\sim j$\"? What does $i\\sim j$ mean? Does it simply mean that there is an edge between them? If it only means that there is an edge between them, why is this a different notation than $(i,j) \\in E$?\n\n* What is $\\hat{a}$ in Equation (1)? I cannot find its definition. Is it $\\hat{A}$? It is unclear from the text.\n\n* Section 2.1:\n>\"$d_{G}$ is the standard minimum-walk (geodesic) distance on the graph\"\n\nIs it simply the **shortest-path**? If so, I suggest using this term which is much more common to the broader audience. If not, the term \"minimum-walk (geodesic) distance\" should be better explained. \n\n* Section 2.1: \n> \"the node features and representations are assumed to be scalar from now on\"\n\nthat is, $p_0 = 1$ and all $p_l$ equal to 1?\n\n* Corollary 2 - I think that the \"Jumping Knowledge\" paper (Xu et al., 2018) had already recognized this role of self-loops. What is the difference between this corollary and Theorem 1 in Xu et al. (2018)? Also, as the role of self-loops is not directly connected to the main contributions of this paper, I suggest considering moving it to the appendix, or removing it if it was already recognized by Xu et al, in favor of more space for explanations of other parts.\n\n* Section 3, definition of $\\lambda_{max}$ - what does \"traversing the same node\" mean? the same $k$ node from the previous definition of 4-cycles? A formal definition of $\\lambda_{max}$ will be helpful.\n\n* Section 3, the example describing Figure 3: I cannot understand the examples because in (ii), $\\sharp_{\\square}^{i}$ is defined for a specific edge $\\sharp_{\\square}^{i}(i,j)$, and here it appears as $\\sharp_{\\square}^{0}$ and $\\sharp_{\\square}^{1}$\n\nAlso, why is $\\sharp_{\\square}^{1}={5}$, and not also the node 0 is included, as it also creates a 4-cycle (1-0-3-5)?\n\n* Corollary 4: what is \"the volume of the geodesic balls\", and what does growing \"polynomially\" exactly mean? How is the volume measured, and what does it grow over? (what is the \"x axis\"?)\n\n* Theorem 5 - it would help if the authors could clarify this theorem, explain its meaning in words and intuitively. I am not sure I understand why every detail there is necessary, and what are its implications. \n\n* Cheeger constant (Equations (5)+(6)) - I did not really understand the notations in Eq. (5), what is the meaning of Equation (6), what is its significance?\n\n##  Evaluation:\n1. It would really improve the evaluation if the authors could experiment with more GNN types and more benchmarks. Specifically, if the authors could take the datasets and exact settings used in Alon & Yahav (2020) and show how the proposed SDRF method improves the results there. The reason that this would be helpful is that Alon & Yahav showed that these datasets already suffer from over-squashing, by taking existing source code of other papers and improving their results. It would be interesting to compare SDRF and the (computationally expensive) solution of Alon & Yahav in terms of the tradeoff between accuracy and computation cost (the overall number of added edges).\n\n2. DIGL is compared to SDRF as the main baseline. However, I couldn't find what exactly does DIGL do. The paragraph about random-walk-based rewiring on page 7 does explain the general idea, but it is unclear whether this is explaining directly about DIGL, or in general about PageRank-style approaches?\n\n## Additional questions to authors\n* According to this analysis, in the authors' opinion, are over-squashing and over-smoothing the same thing or not? What is the difference between them? (The answer to this question will **not** affect the rating negatively, as it is obviously out of the scope of this paper)\nIf the authors have an insightful explanation, I recommend including it in the body of the paper.\n\n\n\n### Other minor comments:\n* The paragraph about Discrete curvatures on graphs on page 4 is completely unclear for the audience who are unfamiliar with the described curvatures. I suggest delaying this to later or even moving to an appendix in favor of more text that will help clarify the rest of the paper.\n\n* Theorem 5(i) writes $\\ell \\in [0, L-1]$. For clarity, I suggest being consistent which previous notations, for example as Lemma 1 which denotes $0 \\leq \\ell \\leq ...$.\n",
            "summary_of_the_review": "Despite my many comments, I think that this is an important paper, which deepens our understanding of the over-squashing phenomenon from a geometric perspective.\nWhile many recent GNN papers introduce new application domains, new features, and new GNN architectures, this paper improves our understanding of the foundations of existing GNNs and their limitations, which is even more important.\n\nI will increase my rating if the authors would answer all questions in the \"Writing\" section above, in the \"Additional Questions\" section above, and provide more evaluation datasets (as detailed in the \"Evaluation\" section above).\n\n===== Update =====\n\nI have increased my rating to 10, good work.\n\nNote that Table 2 is now too wide and goes beyond the page's borders.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors work on the over-squashing effect that has been recently observed in the GCN literature, namely the effect that, in a message-passing paradigm and for a learning problem that necessitates long-range interaction between distant nodes, the existence of bottlenecks in the graph (for instance an edge e with very large betweenness centrality) will intuitively distort the information that needs to travel from distant nodes and thus hinder the GCN's performance.\n\nOne natural direction of research is to add edges (and sometimes remove others in order to keep the complexity under control) to alleviate the bottlenecks. This rewiring process may be done in different ways and the authors suggest one particular algorithm that they compare with state-of-the-art.\n\nIn particular, the contributions are:\n- a precise (even if somewhat arbitrary) definition of an over-squashing measure between two nodes $i$ and $s$ that is simply the Jacobian of the node representation at $i$ with respect to the entry $x_s$. The smaller this measure, the less $i$ \"feels\" $s$, the larger the over-squashing effect. The goal of the paper is to find a way to understand how to alleviate the bottlenecks responsible for this effect\n\n- to this end, the authors suggest a new definition of Ricci-like curvature defined over the edges of the graphs. This new definition has one main property which is theorem 3: it lower bounds the Ollivier curvature. This then enables to obtain Cor. 4, and finally the main result Thm 5, that states, in a nutshell, that negatively curved edges are the ones causing bottlenecks.\n\n- a concrete rewiring algorithm is suggested, in which two steps are repeated until convergence or max iteration is reached: a first stochastic step is performed where an edge is added to alleviate the edge that has minimal curvature, before a deterministic step where the edge with maximal curvature (typically those in cliques) is removed if it is larger than a threshold\n\n- a tentative analysis, in the form of Thm 8, as well as experiments show the authors' method performs as well or outperforms other rewiring methods (they mainly compare with DIGL) while keeping the number of total edges after rewiring under control",
            "main_review": "The paper is well-written and well-organized. Formalizing the concept of over-squashing is interesting and timely, and is of interest to the field. This paper, as is often the case when one delves into novel grounds, makes somewhat arbitrary choices in order to move forward: the definition of over-squashing is somewhat arbitrary, the choice of working with that particular definition of Ricci curvature could be more motivated. Efforts are however made in an interesting direction, and the several discussions around the Cheeger constant gives depth to the paper, and its main messages.\n\n- minor: Appendices F and G are announced at the beginnning of the Appendix section but I did not have them in my pdf document\n",
            "summary_of_the_review": "This is novel work on a seemingly important effect hindering GCN performance in general. Giving a formal definition of over-squashing and providing an analysis based on curvature is novel work, and potentially very useful to the field. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}