{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Oral)",
        "comment": "A conceptually and technically highly innovative paper which reinforces an existing powerful connection between the critical set of two-layer ReLU networks and suitable convex programs with cone constraints. The reviewers are in strong consensus that the paper is sound and has merits for publication."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper explores the landscape of the objective function in training a single-hidden layer neural network with ReLU activation and L2 regularization. Impressively, the paper has the following contributions:\n\n1. It advances the results of Pilanci and Ergen from 2020 by showing that all *global* optimum points can be found via the convex program introduced by Pilanci and Ergen. \n2. It shows that for a large enough width of the hidden layer (at most 2*(n+1), where n is the number of training examples) there are no spurious valleys (i.e . all local minima are global), and GD won't get \"stuck\".\n3. It defines a subclass of single-hidden layer neural networks which it terms \"nearly minimal\". It characterizes  stationary points of the optimization problem by showing that every such point must be a nearly minimal neural network.\n4. It gives a polynomial time algorithm for checking whether a stationary point is a global optimum.\n",
            "main_review": "Strengths.\nI find the results to be novel extensions of of Pilanci and Ergen from 2020 and important theoretically: characterization of the global and local optimum points of the objective function.\n\nWeaknesses.\nThe paper only considers a single-hidden later. Can this analysis method be extended to multiple layers? \n\n\nSeveral typos and corrections I found:\n- The upper bound in the rightmost summation in equation (1) should be 'm' not 'd'.\n- Page 1, penultimate line,  no definition for $C_i$ for $i\\in[p+1, 2p]$ is given.\n- Page 3, Section 1.3, first paragraph, third line:\n  - Fix the mismatching brace. \n  - I believe the $\\sigma$ should be dropped.\n  - The $1(Xu\\geq0)=s$ seems to be a slightly different definition of the cone corresponding to $s$ than the original definition in page 1. Namely, the entries of $Xu$ corresponding to $0$'s in $s$ are required to be negative in this definition and nonpositive in the definition in page 1.\n- In equation (5), B(u_j, \\alpha_j) has vectors with $d+1$ entries (the last entry comes from $R_{>0}$ or $R_{<0}$ from the definition of  $B_i$) and $C_i$ has vectors with $d$ entries, so the containment under the summation doesn't make much sense here.\n\n\n\n",
            "summary_of_the_review": "See above.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper establishes some very interesting connections between the global optimizer of the two-layer ReLU neural networks with the solution of a convex optimization program with cone constraints. More concretely, the authors construct explicit mappings between a smaller class of neural networks called minimal neural networks (that contains all the global optima) and the optimal solutions of convex programming problem. They also show that first-order methods such as SGD can find\nnetworks that can be merged to a minimal representation. Lastly, they provide an explicit path of non-increasing\nloss between any point on the loss landscape to the global minimizer. With this, they prove that has no spurious local minima, provided that the number of neurons is sufficiently large.",
            "main_review": "First of all I have to say this is a very interesting theoretical paper and is the best  among all submissions I have reviewed in ICLR 2022. This paper reveals an interesting hidden convexity structure in  two-layer ReLU neural networks by mapping minimal neural networks to a convex optimization problem and vice versa. The construction of the mapping is clean and elegant. The idea of connecting non-convex loss landscape with convex optimization can potentially be further explored to explain the success of deep learning.  This paper is strong enough and I do not see much weaknesses. But I left to the authors few minor comments which  can be seen as future considerations.\n\n- The first comment that comes to my mind is how the results obtained in the paper can be generalized to neural networks with other activation functions. It seems that the positive homogeneity of ReLU is the key to derive the convex programming problem. Can the authors comment on whether (and how) it is possible to establish similar results for smoother activation functios, e.g. $\\text{ReLU}^{k} $ or Sigmoid?\n\n- Can the authors comment on whether and how the results can be extended to deep ReLU neural networks? \n\nSome relevant references are missing.\n\n- Huiyuan Wang, Wei Lin, Harmless Overparametrization in Two-layer Neural Networks, arXiv:2106.04795.\n\n- Quynh Nguyen, On the Proof of Global Convergence of Gradient Descent for Deep ReLU Networks with Linear Widths, arXiv:2101.09612. \n\n- Quynh Nguyen, Pierre Brechet, Marco Mondelli, When Are Solutions Connected in Deep Networks?, arXiv:2102.09671.\n \n",
            "summary_of_the_review": "This paper provides novel tools and insights in revealing the convexity structure of non-convex loss landscape of two-layer neural networks. The ideas developed here can be used to explain the global convergence of gradient descent algorithm in training neural networks. I recommend accepting the paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "none",
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Along the line of Pilanci & Ergen (2020), this submission deals learning two-layer ReLU neural networks through convex optimization. It introduces a number of new notions such as (nearly) minimal neural networks, developing a set of interesting tools, and draws connections between the minimal neural networks and the convex optimization landscape. This paper provides a rich framework along with new analyses and solutions for learning two-layer ReLU networks through convex cone optimization. ",
            "main_review": "Strengths:\n\nInteresting observations about and practical ways for constructing the optimal neural network set from the solution of the convex cone program.\n\nThe paper is mostly well-written and organized, and technically precise. \n\nThe analysis is mostly clean and easy to follow, which may be of interest for addressing related sets of nonconvex problems. \n\nWeaknesses:\n\nThe (or similar) regularized convex formulation has been studied in previous works, see e.g., Pilanci & Ergen (2020), Ergen & Pilanci (2021) among several recent others by Pilanci et al. Though a more-in-dept analysis as well as theoretical observations are provided, how technically novel and practical useful of these results shall be further compared and elaborated. \n\nErgen, Tolga, and Pilanci, Mert. \"Convex geometry of two-layer ReLU networks\", 2021.\n\nA number of related efforts dealing with learning two-layer ReLU networks by analyzing or modifying the landscape through introducing regularization terms are missing in the discussion; see e.g., Wang et al, 2019. Learning ReLU networks on linearly separable data: Algorithm, optimality, and generalization. \nMoreocver, the contributions can be considerably enhanced by providing experimental validations across different tasks such as regression, or classification tasks. \n\nOne of the key difficulties in deep learning theory is that learning of deep neural networks is nonconvex due to their compositional structure. How would the results extend to deep neural networks?\n\n\n",
            "summary_of_the_review": "The paper contains some typos and grammar errors. \n\nin (1), the second summand should be m terms instead of d terms?\n\nI do not understand why the positive homogeneity of the ReLU activation function directly leads to σ(Xui)αi = σ(Xwi), as \\alpha_i's is not necessarily positive in general?\n\nThe paragraphs regarding the partions of D_i's are not easy to understand, which shall be improved. \n\n\n----------------------------------------------------\nAfter rebuttal: I have read the other reviews and authors' replies. My minor issues were addressed. I will keep my score.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors study the training of two-layer ReLU networks with weight decay.  A previous paper (Pilanci and Ergen 2020) introduced a convex optimization problem that corresponds to this non-convex case.  In the present paper, the authors prove that all optimal solutions of the nonconvex formulation can be found via optimal solutions of this convex formulation.  (Whereas the Pilanci and Ergen paper only constructed a single solution).  The authors additionally show that a Clarke stationary point of the nonconvex objective corresponds to a global minimum of a subsampled version of the convex problem, and they provide a polynomial time algorithm to test if a neural network is globally optimal.  Finally, the authors prove that the nonconvex loss landscape has no spurious local minima provided the number of neurons is large enough.\n",
            "main_review": "Strengths of the paper:\n- The authors provide a complete characterization of the global minima to the nonconvex 2-layer ReLU network training problem, in terms of the solutions of a convex program\n- The approach does not rely on duality and/or lifting perspectives of previous convex perspectives on neural network training\n- The approach provides an algorithm for testing optimality of a neural network in the studied context\n- The work provides significant extensions of the work upon which it is based\n\nWeak points of the paper:\n- The method only applies to 2-layer neural networks with ReLU activation and weigh decay.    It is a great contribution that may inspire further developments to weaken these assumptions.\n\nQuestions for authors:\n\nOne of the technical themes of the paper is the idea of minimal / nearly minimal networks.  Several of the theorems are stated explicitly in terms of nearly minimal networks.  Is it necessary to have minimality in the theorem statements?  That is, could the primary claims be stated without reference to minimality (and where minimality would be a technical detail in the proofs)?\n\n\nAdditional feedback with the aim to improve the paper:\n\nIn the model there are no bias terms.  I presume this is because they can be tucked into X with a row of 1's.  Perhaps this is worth a comment in the paper.\n\nTypo: Sec 1.3 \"neruons\"\n",
            "summary_of_the_review": "Clear accept.  It significantly clarifies the relationship of the solutions of the nonconvex ReLU neural network training problem with a corresponding convex program.  That relationship allows them characterize all global minima and to test whether particular networks are indeed global minima.  While the work only applies in the 2-layer case, it is nonetheless a significant theoretical extension that can inspire extensions to multi-layer cases.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}