{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Oral)",
        "comment": "This paper extends the Neural Collapse (NC) phenomenon discovered by Papyan, Han and Donoho (2020) on deep learning image classifications with Cross Entropy (CE) loss, to the scenario with Mean Squared Error (MSE), that achieves similar performance to CE and favors deeper analysis. In particular, the paper shows that the least square loss can be decomposed orthogonally into a 'central' path as the optimal least square loss, and its perpendicular loss. Moreover, the paper shows by experiments that after the zero training error (Terminal Phase of Training, or TPT) the perpendicular loss is typically much smaller than the optimal least square loss, and the optimal least square loss is further decomposed into the NC1 loss which is the dominance and NC2/3 loss (even smaller than the perpendicular loss). Such a discovery with loss decomposition is very thought provoking to understand the training dynamics of deep neural networks.\n\nReviewers unanimously accept the paper, so is the final recommendation."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper extends the recent work on Neural Collapse, using Mean Squared Error (MSE) instead of CE, as MSE is easier for analysis. With this, the paper shows that the least square loss can be decomposed into one that corresponds to a so called 'central' path (namely a set of optimal tuples (W, b, H) given H for which there is an optimal loss), and a perpendicular loss. The paper shows that the perpendicular loss is much smaller than the optimal least square loss and thus Neural Collapse appears due to the fact that the optimizer focuses on the central path. Empirical studies confirm the findings.",
            "main_review": "The strengths of the paper:\n- The phenomenon of Neural Collapse is certainly an interesting and thought-provoking and analyzing it further to understand its premises is certainly worthwhile of publication. In that sense, the contributions of this work are very relevant and welcoming.\n- The experiments seem to corroborate that the intuitions/derivations of the paper are on the right track. I particularly like the idea of decomposing the loss in two terms, one that is the 'optimal loss' and a secondary loss that accumulates the remaining errors.\n- I find the connection with the dynamics of learning a fascinating direction, and I think this is where research should focus on. It would be very interesting if indeed the dynamics of the learning, perhaps by examining the eigenvalues of the signal-to-noise ratio matrix, lead to Neural Collapse. However, I am not sure about this, because in all honesty it was hard to parse the respective subsection.\n- I would spend more time on the central path, which I find an interesting concept. What does it really mean? Are the features H supposed to be 'fixed' or at least 'fixed within infinitesimal time steps'? I believe this is the largest and most interesting contribution, and a good proof why using the MSE is relevant in this context.\n\nThe weaknesses of the paper:\n- A major weakness of the work is writing and structure. While very interesting ideas are in it, and it is clear that there is a worthwhile message, it is very hard to understand in precise detail the claims, so that a 'third' reader can derive their own insights. As one example, the intro is very technical, with lots of forward references, and extends till p 4. It creates a feeling of repetition and unclarity at the same time, eg, proposition 1 is basically defined twice. Lots of different concepts, terminologies, and ideas are mixed and the text often jumps from one place to another, even referencing later parts of the text that have not been read, assuming someone does read a paper sequentially. Another example is that not notation is not always clearly explained. For instance, what is the time t in equation 5? I suppose time steps during training. For the lack of writing clarity alone, I am not sure if the paper should be accepted, it would be a pity for the work itself.\n- It is not clear (at all) what are the 'assumptions' that are made for the sake of the analysis.\n-- For instance, equation 5 assumes that the features H (and thus X) are converged, that is the manifold of the identity-covariance features is fixed? Or, is it assumed that in infinitesimal training steps the features H (and the manifold H) remain roughly equal? In practice, what does it mean to have the loss L_{LS} for given features H, since the features H change during the training? How is this computed in the plots of figure 2?\n-- In the sentence 'As LNC1 decreases, individual activations would need to tend to their corresponding class-mean', how is this conclusion derived? Do you assume W_{LS} to be fixed? Otherwise, W_{LS} can also reduce the loss, no (in fact, that is the point of learning)?\n-- Although that is a point for the original NC paper, I think NC4 is self-evident.\n- Focusing on the figure 2, it shows that NC2/3 is much much smaller. Does this mean that the model basically distributes features 'uniformly' early on (thus NC2/3 goes to zero fast) and from thereon, it tries to group/cluster each class features as much as possible? What about overfitting? How well does the models generalize if keeping the training till zero loss? Doesn't this contradict standard practices, like 'early stopping'? To put otherwise, what would happen if having small training sets, say 50 or 100 examples per class.\n- Isn't the zero-global mean after bias b_LS an obvious conclusion, in that after subtracting the bias, the average is zero-mean? That is the point of the bias, no?\n- How do you obtain the A^{-1} in equation 8? Is this part of some definition, or Linear Algebra? More generally, is equation (8) suppose to derive a result or to state it? How do you go from W(AH) to W(H) A^-1? I think you mean to say that the W operator is linear (matrix multiplication), so the multiplier A can go out, but how do you derive the A^-1?\n- What is the intution behind the signal-to-noise ratio matrix in the off-diagonal elements? Class confusion so to speak?\n- Section 3.3 is very involved and I am not sure I understand how the NC1-4 are derived. It is stated what we are to conclude from it, but no guidance is provided on why this is the case. I am not asking for the proof, but for the interpretation of the results. For instance, in equation (11) we have the \\omega_max in the denominator, while #1 in Corollary 1 tells that the eigevalues go to infinity. Does this imply that the SNR divided by inifinity goes to 0? Generally, I am not sure what am I to take from this section.\n\n",
            "summary_of_the_review": "All in all, the paper had very nice ideas, but the writing and presentation is suboptimal. This means that it is not ready yet for publication.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies the phenomenon of Neural Collapse (NC) and empirically shows that it occurs during the training of deep networks with the MSE loss. Then it theoretically analyzes NC with MSE loss by decomposing it and introducing the notion of central path. It shows a closed-form dynamics predicts NC in this setting.",
            "main_review": "Strengths:\n- The paper empirically shows that NC also occurs when training deep networks for classification using the MSE loss on five canonical datasets and three backbone networks.\n- The theoretical part of the paper to justify the NC phenomenon for the unconstrained features model seems rigorous. The decomposition of the loss function and how it is helpful in understanding NC is interesting.\n\nWeaknesses:\n- I think the paper contains too much information to be wised packed in a single 9-page conference paper. Specifically, for one of its main contribution, namely the empirical study of the NC with MSE loss, almost all the supportive experiments are deferred to the Appendix, while the main body of the paper only focuses on explaining the theoretical part. I have doubts on such practice (claim the contribution are two-fold while only mainly presenting one of them in the main paper). Note that the authors also admit in Section 1.3 that the empirical study is too long to be included in the paper. \n\n- Some statements need more clarification. In the legend of Figure 2, the term \"Lperp\" should be referred to as \"L^\\perp\" in the caption. Also, in the caption it says \"early in the training, L^\\perp becomes negligible compared to the dominant term LNC1\"; however, I don't see this from the figure: for many of them, the L^\\perp curve is on the above of LNC1 during the early phase of training. How to explain this?",
            "summary_of_the_review": "The strength and weakness of the paper are very clear, as described above. I would give an overall score of marginally above the acceptance threshold based on its theoretical nature and serious study of the phenomenon.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Recently, Papyan, Han and Donoho (2020) have observed that training neural networks beyond zero training error leads to simplex arrangements of the features. The submission studies this phenomenon, called neural collapse, from a theoretical perspective, when training is done via minimizing the square loss.\n\nIn particular, it is shown that the square loss can be split, such that one summand corresponds to the quality of the features (quantified by the loss of the MSE classifier on them) and that the other summand corresponds to the quality of the classification layer (quantified by the its deviationfrom the least squares classifier). Furthermore, the first summand quantifies the closeness of the features to neural collapse.\nIn a second step, the authors investigate the gradient flow induced from the first summand and derive closed form solutions.",
            "main_review": "**Pro**\n\n\\+ The submission is well motivated and coherently structured. (Empirical observation -> Splitting the loss function in summands -> Examining the gradient flow from the predominant summand)\n\n\\+ The theory is able to explain, some aspects, of the empirical observations by Papyan, Han and Donoho (2020) . It separates itself from the prior work, as it contains closed form solutions of the class means (of renormalized features).\n\n\\+ The submission is relevant, as understanding why and how optimization of current neural networks works is crucial for making informed improvements in future algorithms.\n\n**Contra**\n\n\\- The main result (Section 3.3) is formalized in terms of the singular values of the SNR matrix. However, the interpretation is not obvious. I would advice to expand on the footnote 9 and move it to the main text as a brief discussion of the findings.\n\n\\- There is an additional assumption which is required for Theorem 3, which is not mentioned in the statement and the subsequent discussion, but somewhere later: The within class covariance $\\Sigma_W$ is full rank. While one expects the assumption to be fulfilled, this assumption should still be stated more prominently.\n\n\\- There is a correctness issue, due to which I currently cannot give a higher score. The conclusions might not necessarily be wrong, but I am a bit wary. I hope, this can be resolved with the author response. \n\n1. It is claimed that Corollary 1 and Eq. 11 imply neural collapse. So in particular, they imply (NC1): $\\Sigma_W \\to 0$. However, these corollaries are concluded from the dynamics specified in Equation 5, where the representations are always such that $\\Sigma = I$ is constant.\n\n2. The first limit in Corollary 2, i.e. the statement that the singular vectors  of SNR stay constant with respect to $t$, is reasoned for by the fact that $L(w(t))$ only depends on the singular values (and not the singular vectors) and so its gradient does not depend on the singular vectors. However, the dynamics in Eq 5 do not only depend on the gradient, but also on an projection operator, which might change the singular vectors.  \nIntuitively, the dynamics in Eq 5 correspond to gradient updates followed by a renormalization step, i.e. matrix multiplication such that $X$ has identity covariance. This multiplication might possibly chance the singular vectors.\n\n\n*Questions and suggestions unrelated to score:*\n\nIt might be a good idea to reduce the notation in section 2, by considering only the case of zero bias and move the general case to the appendix. In Section 3, the setting is reduced to the case of no bias term anyway.\n\nIs symmetry really required for equation 8 to hold, or does invertibility suffice?\n\nCould you explain the terminology central path (Equation 4)? I guess, central refers to the least-squares optimality, but why is it a path? It does not appear to be one dimensional.\n\nIn appendix D, there is a rather informal discussion on the feature space as a fiber bundle. Can the renormalized gradient flow be formalized as a connection on this bundle?\n\nThere is a typo in Corollary 1. The second limit should depend on $c_3$ instead of $c_4$.",
            "summary_of_the_review": "I am quite positive on the submission, however, there are two issues related to the proofs of the theorems. These issues might only be due to a misunderstanding on my part, but I am not sure. I hope the authors can clarify on this. Until then, I rate the submission as \"5: marginally below the acceptance threshold\".\n\n**Post author response update**\n\nThe authors adressed the correctness issues.\n\nRegarding 1. The authors confirmed, that Corollary 1, does *not* imply that NC1 ($\\lim_{t\\to\\infty} \\Sigma_{W,t} = 0$) and presented the remedy of replacing NC1 by a scale sensitive version, corresponding to the ratio of inner class variations to the between-class variations.  While this is a weaker result, it is still a wortwhile contribution and strongly connected to the observations by Papyan, Han and Donoho (2020).  \nIn fact, the updated definition of NC1, is precisely the quantity measured in the experiments by Papyan, Han and Donoho.\n\nRegarding 2. The authors added the missing proof to the appendix, that the singular vectors of SNR indeed stay constant when subject to the dynamics from Eq. 5.\n\nWith these two major issues out of the way I will adapt my recommendation to **8, accept**.\n\nLast but not least, I want to thank the authors for their detailed and clear responses. I really appreciate, that you took the time and effort to already updating the manuscript with proofs for my initial and follow-up questions.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors of this paper proposed a theoretical explanation of Neural Collapse (NC) for the DNNs with MSE loss. In particular, the authors showed that MSE loss can be decomposed into a sum of terms that corresponds to NC conditions and proposed the theoretical model (central path) that explains why NC emerges in DNNs with MSE under some mild assumptions.",
            "main_review": "Pros:\n\n--The authors provided extensive experiments to show that NC occurs in DNNs with MSE loss.\n\n--The authors provided motivation and explained the required conditions of the theorems and the limitations of their results. Overall, the conditions of the theorems do not look too restrictive and unrealistic.\n\n--The appendix has an extensive overview of related work.\n\n--In the paper, a new theoretical construct, central path, is proposed and analysed.\n\nCons:\n\n--Overall, the paper is well-written, but there are some places that are not easy to follow, for example, Section 1.1. I would recommend the authors to state the problem and all the necessary notions first and only after this to introduce what NC is. The first sentence of the introduction also does not sound good.\n\n--The authors investigate the NC phenomenon from the point of view of the last classifier layer only and do not consider how DNNs generate the features that are passed as input to this classifier. It is still not clear how the NC phenomena are connected to generalization. While MSE delivers similar performance to cross-entropy loss (CE), it is less used in classification settings than CE.\n\n\nAdditional comments:\n\n--In Section 1.3 I would recommend adding references to datasets and architectures.\n\n--I would recommend adding an explanation into Section 1.2 about how classification performed when the model is trained with MSE loss.\n\n--Could you please clarify for me whether NC4 is an independent condition or it follows from (NC1-NC3)?\n\n--Could you please clarify how condition (A2) is related to empirical observations?",
            "summary_of_the_review": "I would recommend accepting the paper. The paper further extends our theoretical understanding of deep learning. The result is new and supported by experiments.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}