{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper studies the adversarial robustness of vision transformers. The authors conclude that vision transformers are generally more adversarially robust than the convolutional neural networks. Several interesting empirical conclusions are made for the robustness property of vision transformers. Sufficient empirical experiments are conducted. Overall, the paper is well-written, well-organized, and interesting. However, there are some concerns about the current version. (1) There are some concurrent works having similar empirical findings, which have been formally published and would weaken the interest of readers in the paper. (2) The reviews suggest that the authors use the insights from the paper to design more robust and effective vision transformers. The four reviewers have unanimous recommendations below the acceptance threshold. We therefore cannot recommend acceptance. However, we believe that by taking the comments, the next version would be a very strong paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper performs a study on the adversarial robustness of vision transformers. It provides some insight on vision transformer from the robustness perspective.",
            "main_review": "Strengths: \\\\\n(1) As claimed, this work provides the first and comprehensive study on the robustness of ViT. \\\\\n(2) This study provides some interesting (but not surprising) insight.\n(3) The paper has a clear structure and is easy to follow.\n\nWeaknesses: \\\\\n(1) The insights of this work cannot be exploited for understanding or improving the adversarial robustness. This is the main reason that I believe this work might be below the ICLR bar. \\\\\n(2) Why is ViT more robust than CNN? This work claims that ViTs have less low-level information and more generalizable. How do you define high-level and low-level? Why does high generalization contribute to superior robustness? Any proof or reference? It would be interesting if the authors can provide more concrete insight into the mechanism behind the reported phenomenon. \\\\\n\n\nSome works [1-10] might be worth a check: \\\\\nThey do not affect my rating of this submission.\n\n[1] Understanding and Improving Robustness of Vision Transformers through Patch-based Negative Augmentation\n[2] Certified Patch Robustness via Smoothed Vision Transformers\n[3] Adversarial Robustness Comparison of Vision Transformer and MLP-Mixer to CNNs\n[4] Towards Transferable Adversarial Attacks on Vision Transformers\n[5] On Improving Adversarial Transferability of Vision Transformers\n[6] Reveal of Vision Transformers Robustness against Adversarial Attacks\n[7] Intriguing Properties of Vision Transformers\n[8] Vision Transformers are Robust Learners\n[9] On the Robustness of Vision Transformers to Adversarial Examples\n[10] Towards Robust Vision Transformer",
            "summary_of_the_review": "Overall, this work has some merits but I expect more than that to get in for ICLR. Some investigation is rudimentary and is suggested to provide more deep insight. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N.A.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "\nThis paper studies the adversarial robustness of ViTs. There are several significant strengths and weaknesses in this paper. Especially, the novelty of this paper against one published paper is limited. It would be good to see my detailed comments below.\n\n",
            "main_review": "\n(Negative) Throughout the paper, the authors claimed that this is the first paper to introduce an adversarial attack on ViT to examine the robustness of ViTs. But actually, until today, there have been a number of papers working on this topic, e.g., [r1-r4]. Especially, [r4] has been published in ICCV 2021. I understand that the authors might argue that this paper was submitted to Arxiv on 29 Mar 2021, which is earlier than some of the other papers. But, the authors should understand the following facts. First, please note that, [r4] was submitted to Arxiv on 26 Mar 2021, which is even earlier than the authors' paper. Moreover, [r4] has been published by ICCV 2021, and the authors' paper is unpublished yet. Second, because this paper is still not yet published and currently there has been a number of paper workings the same ideas of this paper, it would not be appropriate for the authors to overstate this is the first work in ViT robustness, considering many of the concurrent works provide more deep insights in this topic. Third, [r5] was the earliest paper studying the robustness of non-local attentive models among all of these works. Fourth, the insight of this paper is relatively shallow. There are not many valuable discoveries in this article, so the author keeps discussing the same thing from different angles. Fifth, I understand that the author wrote this paper many months ago, and they don't want to improve their paper because they think many \"advance papers\" are later than theirs. But I encourage the authors to improve their paper with more advanced insights to catch up with the recent developments. For example, the author could propose some practical solutions to ViT robustness. I also have some experience that I was the first to propose some techniques, but my paper was rejected. Then, the later paper working on a similar idea was published. I was frustrated. But I continued to improve my paper, and finally, my paper was accepted by another venue as an oral presentation. \n\n\n[r1] Intriguing Properties of Vision Transformers\n\n[r2] Vision Transformers are Robust Learners\n\n[r3] RobustART: Benchmarking Robustness on Architecture Design and Training Techniques\n\n[r4] Understanding Robustness of Transformers for Image Classification (ICCV 2021) 26 Mar 2021\n\n[r5] Feature Denoising for Improving Adversarial Robustness.\n\n\n(Negative) It would be good not to say that this work provides the first study on the robustness of vision transformers (ViTs) against adversarial perturbations. There have been other works discussing the ViT adversarial robustness. It would be good for the author to recognize the contribution of the community. It would be good for the authors to be humble.\n\n(Negative) The following observations are natural and not new: \"we find that ViTs possess better adversarial robustness when compared with convolutional neural networks (CNNs).\"\n\n(Negative) The following observations are natural and not new: \"Features learned by ViTs contain less low-level information and are more generalizable, which contributes to superior robustness against adversarial perturbations.\" Existing papers found that ViTs do better than CNNs in adversarial robustness but do worse than CNNs in natural noise robustness.\n\n(Negative) The following observations are complementary to the above observations. They thus are less informative: \"Introducing convolutional or tokens-to-token blocks for learning low-level features in ViTs can improve classification accuracy but at the cost of adversarial robustness.\"\n\n(Negative) The following observations are complementary to the above observations. So these observations are less informative. Increasing the proportion of transformers in the model structure (when the model consists of both transformer and CNN blocks) leads to better robustness. This makes readers feel that there are not many valuable discoveries in this article, so the author keeps discussing things from different angles.\n\n(Negative) I'm afraid I have to disagree with the authors' following claim: \"Pre-training on larger datasets does not significantly improve adversarial robustness though it is critical for training ViTs.\" I think the authors performed the wrong pre-training. Pre-training on the larger dataset should be robust pre-training, i.e., it should be adversarial training, but NOT vanilla pre-training. In my experiments, doing robust pre-training on a larger dataset significantly improves adversarial robustness.\n\n\n(Negative) The following observations are too natural and contain little information: \"ViTs are less sensitive to high-frequency perturbations than CNNs and\nthere is a high correlation between how well the model learns low-level features and its robustness against different frequency-based perturbations.\"\n\n(Negative) The following descriptions need to be corrected: \"it remains unclear on the robustness of ViT against adversarial perturbations, which is critical for safe and reliable deployment of many real-world applications,\" given that the literature has fully studied the robustness of ViT.\n\n\n(Negative) The following descriptions need to be corrected: \"In this work, we conduct the first study on examining the adversarial robustness of ViTs on image classification tasks and make comparisons with CNN baselines.\"\n\n\n(Neutral) The following observation is natural and ok: \" Using denoised randomized smoothing (Salman et al., 2020), ViTs attain significantly better-certified robustness than CNNs.\"\n\n(Positive) The following description is correct and insightful: \"short-term memory (LSTM) or CNN, with a theoretical explanation provided in Hsieh et al. (2019). However, due to the discrete nature of NLP models, these studies are focusing on discrete perturbations (e.g., word or character substitutions) which are very different from small and continuous perturbations in computer vision tasks.\"\n\n(Negative) The following description needs to be modified: \" To the best of our knowledge, this work is the first study that investigates the adversarial robustness (against small perturbations in the input pixel space) of transformers on computer vision tasks.\"\n\n(Negative) The following description needs to be modified: \"In the context of computer vision, the most relevant work is Alamri et al. (2020), which applies transformer encoder in the object detection task and reports better adversarial robustness.\"\n\n(Negative) In the related work section, all the above-mentioned works should be discussed and compared to acknowledge the community's contribution.\n\n(Positive) I am happy to see that the authors also evaluate the certified robustness of the models using randomized smoothing, where the robustness is evaluated as the certified radius,\n\n(Negative) The following observations are too natural: \" Adversarial training can be applied to train robust ViTs.\"\n\n\n(Positive) The following finding is interesting and valuable: \"We conjecture that ViT may need larger training data or longer training epochs to improve further its robust training performance, inspired by the fact that on natural training ViT is not able to perform well either without large-scale pre-training.\"\n\n\n(Positive) The results in Table 4 are fascinating, valuable, and beneficial. I really like the results in this table.\n\n\n(Negative) Please use text instead of figures to give a sub-caption to each subfigure in Figure 4. The current version is lossy.\n",
            "summary_of_the_review": "\nBalancing the strengths and weaknesses of the proposed method, I would like to recommend a rating of weak rejection for this paper.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "\nNone.\n\n",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper provides a comprehensive study on the robustness of ViTs against adversarial perturbations. The authors found that 1) ViTs has better adversarial robustness than convolutional neural networks; 2) Introducing convolutional or tokens-to-token blocks can improve the classification accuracy but at the cost of the adversarial robustness; 3) More proportion of transformers has better robustness; 4) Pre-training on larger datasets does not improve adversarial robustness; 5) Adversarial training is applicable to ViTs. In addition, many experiments verify the findings on white-box, transfer attack settings and adversarial training.  ",
            "main_review": "1. Some findings lack explanations and reasons. For example, it is important to give a reason why ViTs have better adversarial robustness than convolutional neural networks. Does this advantage come from the splitted several tokens in ViTs? If yes, could you please conduct more experiments on different sizes of tokens? It would be better to provide more explanations for this.\n\n2. More general Transformer architectures should be explored in the paper. The authors mainly focus on ViTs, and the observations are mainly for ViTs. Recently, Swin Transformer gains more attention in computer vision tasks. The authors only discuss one type of Swin Transformer, i.e., Swin-S/4. It would be better to explore more types of Transformer and draw conclusions for the more general cases of Transformer.\n\n3. The experiments should be improved.  The authors only consider PGD attack and AutoAttack. These two types of attack methods are linear, i.e., add small perturbation on clean data. It would be better to conduct more experiments on other nonlinear attack methods, e.g., ADef [1], which applies small deformations to the clean data. \n\n    [1] ADef : an Iterative Algorithm to Construct Adversarial Deformations. ICLR 2019\n\n4. In Table 1, the attack success rate (ASR) of  Deit-S/16 is the best result, but the ASR of ViT-L/16 is 98.2. Is there a mistake in these two results?\n",
            "summary_of_the_review": "It would be better to provide more explanations for some findings, and further improve the experiments for more general Transformer cases and different types of attack methods.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work studies the adversarial robustness of vision transformers comprehensively. The author first presents their finding via empirical experiments. Concretely, they show the robustness of vision transformers from the perspective of adversarial attack, transferability, and certified robustness, adversarial training defense. Then, they analyze the reason behind the observation. ",
            "main_review": "Strengths:\nThe study on the adversarial robustness of ViT is comprehensive.\nThe analyst behind the observation is provided.\nThe paper is clearly written and easy to follow.\n\nWeakness:\n1. The authors state that ViT is more robust than CNN. It is only true when a weak attack is applied. When a standard attack (with perturbation range epsilon=0.03/0.01), both ViT and CNN can be fooled with 100% fooling rate. The observation indicates that both are equally vulnerable under standard attack. Hence, the statements or claims made in this paper should be carefully formulated.\n\n2. The observations on the difference in model robustness are presented in this work. However, the difference is always attributed to the model architecture difference in this work. Compared to ResNet, the ViT and DeiT are trained/finetuned in a different setting, e.g. different data augmentation, different training schedules. It is not clear how the different training/finetuning settings contribute to the different model robustness. It might be too early to attribute the observation to model architectures.\n\n3. Similarly, the author lists the following point as one of their findings:\nPre-training on larger datasets does not improve adversarial robustness though it is critical for training ViT.\nHowever, previous work shows pre-training can improve model robustness [1]. More discussion should be provided hier. The claims should be careful. \n4. In Figure 3, the author shows ViT-small shows higher certified robustness than ResNet18. In the work DeiT, they show that the CNN counterpart of DesT-small is ResNet50 instead of ResNet18. If it is too hard to compare them in a 100% fair fashion. Please show the curves of both ResNet50 and ResNet18.\n\n5. The adversarial training on ViT is also studied in this work. However, the experiments are conducted on CIFAR10 datasets. The ViT is expected to behave well only on a large dataset. What is the performance of ViT in a standard-setting on CIFAR10 dataset?\n\n[1] Hendrycks, Dan, Kimin Lee, and Mantas Mazeika. \"Using pre-training can improve model robustness and uncertainty.\" International Conference on Machine Learning. PMLR, 2019.",
            "summary_of_the_review": "ViT demonstrates the potential to work as an alternative to CNNs. The adversarial robustness of ViT is indeed an important topic. As claimed by the author, this work makes the first comprehensive study on this topic. However, some concerns listed in weaknesses above remain to be addressed. Therefore, I rate this paper blew the acceptance threshold. I happy to raise my rating if the concerns are well addressed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}