{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper has been reviewed with four expert reviewers. The reviewers have reached the consensus that the paper is not yet ready for publication. The main concerns are related to novelty. All reviewers gave substantial and constructive feedback. Following the recommendation of the reviewers, the meta reviewer recommends rejection."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes the problem of few-shot attribute learning (FSAL) that follows the standard few-shot/meta-learning paradigm but focuses on attributes instead of object classes. The authors argue in Section 2 that the multi-label nature of attributes (smiling & wearing eyeglasses can be present simultaneously) makes this problem different; the context of each positive example in each episode can drastically change what the model is asked to learn. \n\nThis paper also proposes three benchmark datasets to study this problem, based on Celeb-A, Zappos50K, and ImageNet-with-Attributes. The paper evaluates multiple existing approaches on these benchmarks as well as improves upon them further.\n",
            "main_review": "## Setting (FSAL)\nThe problem of rapid learning of attributes is well-motivated and practical. At a high level, Section 4 put the proposed setting in different contexts well.\n\n## Benchmark\nWhile the three proposed datasets are diverse in domains (faces, shoes, and general), I feel, however, that there is not enough concrete discussion about comparisons between proposed benchmarks and existing ones: FC100, tiered-ImageNet, Meta-Dataset, as well as Arnold and Sha 2021. My concern is that the proposed benchmarks seem to be very limited in the number of attributes (~20-80 in total each). Further, random splits are adopted without leveraging insights from previous work that split the semantic object classes. Would it be possible to put your task construction in the context of your goal of understanding generalization as well as in the context of related work that split the data in different ways?\n\n## Experiments\nExperiments are quite extensive with multiple baselines considered, with the proposed approaches improving upon existing ones further. However, it is unclear how the findings in Section 5.3 differ from previous findings when semantic object classes are considered. What would be our takeaway from the trend that SA performs poorly and U performs well?\n\n## More detailed comments\n- Setting: The claim that the setting with attributes is different from the standard few-shot setting on object classes is sound. I agree that the setting could be useful in understanding “the generalization performance semantic classes in standard few-shot learning” [Last sentence of page 1, last sentence of the intro]. However, I was wondering if the authors could say something more specific about what kind of insights we expect from exploring the setting? For example, how exactly would this offer “a more systematic framework to measure the relatedness and transferability between the traing and test set” [Last sentence of “Few-shot learning” in Section 4)? Do you expect this setting to be harder or easier than the one with object classes? Do you expect that the reuse of images for learning different attribute concepts in different stages might make the problem easier?\n- Related Work: Should Arnold and Sha 2021 fall into “Few-shot learning” instead of “Generalization to novel tasks”. More generally, perhaps more discussion on how you separate these two items.\n- Experiments: Have you tried “Baseline++” (Chen et al. 2019) for SA?\n\n",
            "summary_of_the_review": "Novel and potentially useful setting to study but some concerns on execution related to benchmarks and how we should interpret the results.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors consider the problem of few-shot attribute learning. Contrary to most past approaches on learning with attributes, including classical zero-shot learning, the authors deal with the case where the attributes seen at test-time are previously unseen. As a result, few samples must be used to quickly adapt the learning algorithm to new attributes.\nThe approach works in 3 stages. The first uses a classical representation learning algorithm to learn a visual representation, SimCLR. Following this, the network is given supervised training to fine-tune the representation. In the last step, the authors adapt a previously devised few-shot learning strategy that consists in training a linear classifier.\n",
            "main_review": "*** Positive aspects:\n- The paper is clear, and well written. The contributions are also clear.\n- The proposed approach delivers good performance against a set of reasonable few-shot learning baselines. More generally, all aspects of the experiments seem good. In particular, I appreciate the effort put into ensuring that no outside data is used in part 1 (which is a problem often acknowledged in zero-shot learning) [1]\n- The problem setting is clearly motivated.\n- Ablation studies are performed, giving more details into the effect of the different components.\n\n*** Concerns:\nThere are no major concerns with the approach: the proposed method seems to work well. \n\nAmong the less important concerns, the most important one is that overall the proposed method is an assembly of previously trialed approaches. Combining unsupervised learning techniques with classical approaches to few-shot adaptation does not constitute a novel method.  In this review, I am not arguing for novelty for novelty's sake. I am rather just mentioning that the paper consists in introducing a variation on a classical learning problem (few shot strategies with attributes have been extensively studied in the literature), which is then solved using techniques that were designed for related problems. \n\n*** Questions\n- What motivated the choice of few-shot algorithm in Part 3 of the learning algorithm? Was there a theoretical motivation, or rather a practical one? \n- How are hyper-parameters for each of the 3 parts optimized? All together or separately for each?\n- What would be the effect of injecting attribute information directly in Part 1 (by an auxiliary loss e.g., which would combine part 1 and 2)?\n- I wanted to confirm the following with the authors. When running experiments on Imagenet-1K, the SimCLR model they use has never seen any of the the test images, is that correct?\n- Have the authors tried other self-supervised learning algorithms for part 1? Did other methods lead to additional insights, apart from performing worse? (i.e. is there a link between different families of self-supervised learning algorithms, and performance?).\n\n\n*** References\n[1] Xian, Yongqin, et al. \"Zero-shot learning—a comprehensive evaluation of the good, the bad and the ugly.\" IEEE transactions on pattern analysis and machine intelligence 41.9 (2018): 2251-2265.",
            "summary_of_the_review": "The approach works, the components are well motivated. Despite this, in my opinion, the approach can be understood as a variation on more classical attribute-related few-shot learning tasks, solved by combining methods that work well on these. I would like to highlight that, despite the previous point, I feel the proposed idea has scientific merit, hence borderline score I have given currently.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper investigates a new and interesting topic, few-shot attribute learning (FSAL), where the model is trained with base attributes with abundant samples and tested with novel attributes with only a few samples. Experiments were performed on three benchmark datasets with attribute annotation. The paper found that self-supervised learning helps to train a backbone that generalizes well to novel attributes, compared to training the backbone with only base attributes. The author also compares several few-shot learning models, e.g., MatchingNEt, MAML, ProtoNet, in this FSAL setting and found that the logistic regression layer works the best.",
            "main_review": "Strengths:\n1. The problem setting, learning attributes in a few-shot setting, is novel. The problem may generalize to many realistic applications.\n2. The authors reveal an interesting observation that self-supervised training can help the backbone to focus on generalizable features, which might also benefit the general FSL tasks.\n3. The authors perform extensive experiments and ablation studies. The experiment results indicate that the proposed model leads to consistent improvement.\n\nWeaknesses:\n1. The three stages in the model are introduced in other papers before, which hinder the novelty of this paper. The self-supervised training is proposed in [1r], the fine-tuning stage adopts [2r], and the few-shot learning stage adopts logistic regression proposed in [3r].\n\n2. Some model configuration and experiment details are not clear. \n* If the LR outperforms NN and NC in the Few-shot learning stage, I'm curious about the performance of utilizing LR in the fine-tuning stage.\n\n* The episode construction: on page 6 paragraph 1, the author randomly selects one or two attributes and looks for positive examples belonging to these attributes simultaneously. From Figure 2 we can observe that mustache is learned alone while Brown Hair and High Cheekbone are learned together. When would the author select two attributes and why is this happening?\n\n*  In Table 3 the ablation study is performed under a 20-shot setting, however, nearly all methods achieve similar results using 20 samples. Why not use the results in the 5-shot setting? \n*  In the CAM visualization of different methods, the author claim that the proposed method produces accurate and localized heatmaps for each attribute, which is even better than the oracle setting SA*. Why is the proposed model producing localized attention? I do not see any training constraints for this. More explanation for this phenomenon is appreciated.\n* In related work the author claim that \"In this paper, we study learning novel contextual similarities in the form of attributes using only a few training examples\". What does context mean? And where is the evidence to support this claim?\n3. My other major concern lies in Section 5.4, the generalization analysis. The author defines a transferability score and reveals a positive correlation between transferability and generalization ability. However, is this real transferability? My point is that some attributes are heavily correlated with each other the model might take advantage of the correlation to predict novel attributes. For instance, earings and lipsticks co-occur in most of the images. If the network has seen lipsticks as the base attribute, then given a few images with those two attributes showing together, the model will naturally misunderstand our intuition and predict the lipsticks instead of earings. Some more qualitative results might help us to understand if the network successfully generalizes to novel attributes, or just take advantage of the attribute correlation and misunderstand the attributes.\n\n[1r] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. CoRR, abs/2002.05709, 2020.\n\n[2r] Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems 30, NIPS, 2017.\n\n[3r] Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look at few-shot classification. In Proceedings of the 7th International Conference on Learning Representations, ICLR, 2019.\n\n\n",
            "summary_of_the_review": "This paper introduces an interesting topic and performs extensive experiments to verify the conclusion. However, the technical novelty is hindered since most of the contributions are not new. Besides, there are some unclear points and claims that need to be clarified and discussed. I'm looking forward to the authors' responses.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The article proposes to address attribute detection with a small data constraint in the training phase (few-shot approach). The experimental protocol is divided in a series of episodes, each defined by a support set that specifies the attributes to be predicted and a query set used to evaluate the prediction. The generic algorithmic structure to solve this problem is divided in two steps: a common pre-training phase that combines self-supervised representation learning and supervised fine-tuning, followed by a supervised learning phase to solve each episode task. A training dataset containing image samples with attributes is used to learn the pre-trained representation space. The pre-training phase is compared to other approaches on three benchmarks implementing few-shot attribute prediction episodes. An indicator computed from a logistic regression between attribute values is proposed as a way to predict transferability between the learning dataset and the few-shot problem.\n",
            "main_review": "**Strengths\n\n- Introduces a formulation for the problem of few-shot attribute learning.\n\n- Proposes reasonable baselines and experimental results on three benchmarks.\n\n- Provides some insight about learning behavior (explainability heatmaps, transferability of attribute, influence of shot number).\n\n\n**Weaknesses\n\nBasically I found that the article addresses an interesting issue about attributes as an intermediate representation for transferability but that it fails to provide convincing answers to this question mostly due to lack of rigor and precision in the way it is presented and justified. Most of my remarks go in this direction.\n\n- The target scientific goal of the paper is unclear: Finding ways to better analyze transferability from attribute prediction? Justifying that supervised fine-tuning is needed for pre-training? Proposing a new algorithm to solve few-shot attribute learning?\n\n- The task definition is also unclear: I didn't understand whether the attribute prediction task is only a binary classification on a single attribute (or a conjunction) from the unseen set, or a multiple-attribute detection problem? Is each episode considered as a \"2-way/n shot\" problem, the two classes being the target attribute present or not? The notations used to define such a task are ambiguous or incomplete.\n\n- The real algorithmic novelty of the proposed approach is low: the main result seems to be that self-supervision yields better features for the problem of attribute detection (this is not a real surprise given the literature [1,2]) and that supervised fine-tuning also improves features. Indeed, the proposed approaches (UFTA & UFTE) are very elementary and should rather be considered as baselines.\n\n- The transferability analysis could be interesting but is not clearly conducted. For instance, what does \"subtracting U as a baseline\" mathematically mean, how is it done? Also, it is said that the transferability score is an average on attributes in each episode, but I understood an episode as testing a single attribute (see my question about task definition). The formula used to compute it should be made more explicit as it is a major contribution of the paper.\n\n- The evaluation protocol for FSAL only exploits one split on the attribute set: the study about transferability uses multiple random splits. Why not use such a protocol for the main task? It would give more consolidated results. \n\n- Is attribute prediction fundamentally different from multi-label classification, an already studied area [3,4]? The introduction also speaks of expanding the vocabulary but the paper does not seem to evaluate such incremental capacity.\n\n- Compared methods:  it is not clear how they are modified to take into account the attribute detection problem instead of a classification with exclusivity between classes. More details should be given, at least in the supplemental material. \n  \n- The idea of \"context\", presented as a key issue in the paper, is not clear: is it given by the training dataset or by the support set for each episode?\n\n- It is now well established that self-supervised learning (SSL) is able to provide more general and robust features than supervision: the impact of the SSL method and of the DNN backbones however are important for classification (see [1]), and the proposed study only studies one configuration for each dataset. How really general are the results provided? The difference between the U algorithm (i.e. that skips the supervised fine-tuning part) and the two proposed baselines is often small: the role of the supervised fine-tuning is not obvious. Also, one could use pre-trained features learned using a bigger external dataset. If the main objective of the paper is to justify that a supervised fine-tuning part is useful - a reasonable assumption - more competing unsupervised features need to be compared.\n\n- There are many other datasets with attributes or labels: COCO attribute, AWA, CUBS, Flower... which contain more attributes than the datasets used in the paper. Since the paper is to introduce a new problem, a justification of the benchmark is expected.\n\n- A missing reference [5] with a clearer presentation of a  few-shot attribute learning problem.\n\n- Writing style: Please avoid assertions such as \"our proposed method produces *surprisingly* accurate and localized heatmaps that pinpoint the location of the attributes (e.g. mustache or cheekbone); this is impressive since no labeled information concerning these attributes was available during representation learning\". It seems to imply that self-supervised representations magically locate attributes! The interesting scientific question would be to understand why such behavior happens, if it is a consequence of the self-supervision or supervised fine-tuning, for instance.\n\n- Formal description: please describe with better precision the mathematical spaces underlying symbols, and their meaning: it helps to fully understand the paper. \n\n\n[1] Sariyildiz, M. B., Kalantidis, Y., Larlus, D., & Alahari, K. (2021). Concept generalization in visual representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 9629-9639).\n\n[2] Van Horn, G., Cole, E., Beery, S., Wilber, K., Belongie, S., & Mac Aodha, O. (2021). Benchmarking Representation Learning for Natural World Image Collections. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 12884-12893).\n\n[3] Alfassy, A., Karlinsky, L., Aides, A., Shtok, J., Harary, S., Feris, R., ... & Bronstein, A. M. (2019). Laso: Label-set operations networks for multi-label few-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 6548-6557).\n\n[4] Li, Z., Mozer, M., & Whitehill, J. (2021). Compositional embeddings for multi-label one-shot learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (pp. 296-304).\n\n[5] Xiang, L., Jin, X., Ding, G., Han, J., & Li, L. (2019) Incremental Few-Shot Learning for Pedestrian Attribute Recognition. IJCAI.\n",
            "summary_of_the_review": "\nThe question of addressing transferability at attribute level is interesting. A \"few-shot\" formulation to do so, as proposed in the article, is a possible direction.\n\nHowever, the overall presentation of the problem is messy, superficial and lacks precision. The algorithm novelty is low and the experimental part relies on questionable protocols. \n\nThe article should be reworked with a clearer focus on the scientific hypothesis that is tested and better justified experimentations.\n\nFinal decision:\n\nMy conclusion about acceptance has not really evolved: the problem of attribute transferability addressed in the paper is interesting, but the way it is studied and presented must be improved and better justified (see detailed comment of rebuttal after author's response). ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}