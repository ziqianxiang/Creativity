{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper introduces a purely spike based method for training spiking neural networks with recurrence, by extending the recently published \"implicit differentiation on the equilibration state (IDE)\" technique. As a purely spike based method for both the forward pass and the gradient computation, the proposed technique potentially represents an important advance.\n\nBased on the original submission, the reviewers had difficulties understanding the paper's contributions and verify its claims. I commend the authors for engaging with the reviewers by answering their questions and updating the paper to better explain the contributions. However, even after considerable back and forth, the most positive reviewer still expressed major concerns [[1](https://openreview.net/forum?id=VQyHD2R3Aq&noteId=Ubrdawfds5)], and the other reviewers appeared unmoved, based on their scores.\n\nThe reviewers' principal concerns were a little hard to distill. It is possible that their initial difficulties with understanding and validating the paper's contributions made it hard to fully appreciate the paper. One reviewer is unable to verify that the algorithm is purely spike based, and that the energy costs are appropriately calculated. They are also unsure if the method will scale to more complex settings [[1](https://openreview.net/forum?id=VQyHD2R3Aq&noteId=Ubrdawfds5)]. A second reviewer was also initially unable to verify the same claims, was unsure if the theoretical improvements were sufficiently significant, and whether the method could apply to non-IF models of spiking neural networks [[2](https://openreview.net/forum?id=VQyHD2R3Aq&noteId=cc2tMpSst0t)]. The authors addressed this in their response by performing additional experiments with LIF neurons, but it wasn't clear if the main concerns were sufficiently addressed, since the reviewer did not change their score.\n\nBased on the largely negative appraisal by all reviewers, I recommend that be paper be rejected. However, I strongly encourage the authors to revise and resubmit their paper to a future conference, focusing on making sure that their central claims can be more easily understood and verified."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, the authors extend the work Xiao et. al., 2021 NeurIPS to elaborate on the ideas of training SNN based on the equilibrium and address some of the shortcomings. \n",
            "main_review": "The authors attempt to support their conclusion with mathematical evidence, which improves the reliability of the quantitative part of the results. Yet every coin has two sides. At the same time, the math details cannot tell the upper bound and the potential of the proposed method, which is my main concern for the series of approaches. \n\nIn this paper, the authors extend the work Xiao et. al., 2021 NeurIPS to elaborate on the ideas of training SNN based on the equilibrium and address some of the shortcomings. Overall, this paper is clearly written and easy to follow. I list my comments and concerns below and am more than happy to discuss with the authors about these issues. \n\n1. I think the idea of approaching SNN training via equilibrium is interesting, but it requires more reasoning about why it has the potential to go beyond existing methods and what is its unique methodological contribution given Xiao et. al., 2021 NeurIPS. \n\n2. If I understand it correctly, the equilibrium basically says that an integrally convergent input would result in an integrally convergent average firing rate. Since the assumption satisfies Lipschitz's condition with L < 1, I feel like the prove could be simplified with some known ODE theorems on perturbation. \n\n3. The derivative on the equilibrium gives the dependence between the convergent $a^*$ and $x^*$. I think it is more related to backpropagation rather than the  Hebbian learning rule. The analog here is not evident. \n\n4. Since the whole framework is built on the equilibrium, it implicitly requires the dynamics to be stable. Thus, it seems that the latency $T_F$ cannot be extremely shortened to an efficient case where the dynamics are not stable across time. As the authors also pointed out, this paradigm is not compatible with setups that may amplify the norm and maps, e.g. the BN-layers in training. This shortcoming from assumption may limit the practical utility of the proposed methods. In practice, the comparison in Table 3 is not up-to-date. For example, Ref [1] and [2-3] significantly shorten the conversion and training latency and work well for big datasets. The experiments in the current work should at least extend to CIFAR-100 and ImageNet. It would be also great if the authors can additionally add the results on the spiking dataset like CIFAR-gesture and CIFAR10-DVS as well. \n\n5. in Figure 3, it seems that the accuracy has a jump around epoch 50. Can the author explain the reason? \n------------------------------------------------------------------------------------------------------------------------------------------------------\n[1] Yuhang Li, Shikuang Deng, Xin Dong, Ruihao Gong, Shi Gu. A Free Lunch From ANN: Towards Efficient, Accurate Spiking Neural Networks Calibration. *Proceedings of the 38th International Conference on Machine Learning, PMLR 139:6316-6325, 2021.*\n\n[2] Fang, Wei, Zhaofei Yu, Yanqi Chen, Tiejun Huang, Timothée Masquelier, and Yonghong Tian. Deep residual learning in spiking neural networks. *Conference on Neural Information Processing Systems (2021). *\n\n[3] Wu, Jibin, Yansong Chua, Malu Zhang, Guoqi Li, Haizhou Li, and Kay Chen Tan. A tandem learning rule for effective training and rapid inference of deep spiking neural networks. *IEEE Transactions on Neural Networks and Learning Systems (2021).*\n\n------------------------------------------------------------------------------------------------------------------------------------------------------\nMy comments after the rebuttal: \n\nThe authors made a very attentive rebuttal and emphasized their contribution as a purely spike-based training approach. Yet my two major concerns still remain here. \n1. I cannot fully track every step in the updating formula to make sure that it is purely spiked-based. For example, does the calculation of (10) and (11) and the associated gradient calculation on page 7 still involve multiplication? If the general multiplication is somewhat supported by neuromorphic hardware, the other methods mentioned in the comparison would also work as well? \n2. If we accept the claim it is purely spike-based, we still have the question of whether the potential energy efficiency is worthy. For example, the performance on DVS-CIFAR-10 is ~15\\% less than the mentioned benchmarks on page 22. As the authors mentioned, the energy cost is related to both the latency and spiking rate. Thus it would be necessary to at least provide a comparison of the estimated cost with the relevant approaches and better to provide an implementation of at least the proposed method on proper hardware to support the purely-spike-based assertion. Also, it would be necessary to demonstrate that the proposed method can achieve at least comparable accuracy with Fang et. al., ICCV 2021 to support that the cost of changing to spike-based training is acceptable. \n\nBased on these two points, I tend to keep my rating at 5 but am open to marginally accepting this paper if the other reviewers strongly think it should be accepted. \n------------------------------------------------------------------------------------------------------------------------------------------------------\nLatest update: \n\nI encouragingly increased my score to 6 given the rebuttal effort but my judgment remains on the borderline as the major concerns are not fully addressed with sufficient numeric proof. \n\n",
            "summary_of_the_review": "Overall, this paper is clearly written and easy to follow, but the methods may not apply to the more complex scene with less latency. My concern is that this limitation is from its setup. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors proposed a method to train Spiking Neural Networks (SNN) with spike-based implicit differentiation on the equilibrium state.\nMain idea is to use a spike-triggered event instead of average firing rate to approximate implicit differentiation of Feedback Spiking Neural Networks (FSNN).\nTo enable such idea and to further reduce the approximation errors, the authors proposed several techniques such as adopting ternary spiking neuron couples and shifting resting potential.\nThe experimental results showed that the proposed method can achieve high accuracy in several tasks such as MNIST, CIFAR-10, and CIFAR-100 with fewer time steps for training compared to existing methods.",
            "main_review": "This paper proposed a meaningful implementation of the spike-based learning method for SNNs with higher plausibility.\nThere have been several works on training SNNs with spike-based learning rules based on the back-propagation or the equilibrium propagation, but most of them failed to provide successful experimental results.\nIt is impressive that the experimental results in this paper showed successful accuracy in several datasets including CIFAR-100.\n\nThe writing can be improved for easier understanding.\nIt is hard to understand basic concepts without reading the reference, so It would be better to explain more details about the background such as the IDE.\nEven though I have not fully understood the mathematical derivations in the paper, it was not easy to follow mathematical derivations in the paper without referring to other papers.\n\n========== Comments after rebuttal ==========\n\nI apologize for the late response and thank the authors for adding a detailed explanation of the proposed idea. It was very helpful to deepen my understanding.\nI agree that this paper has a clear contribution of proposing a fully spike-based derivation of IDE training methods for SNNs.\n\nHowever, there remains another concern.\nI missed in my initial review that this paper solely used the IF neuron model. Can the proposed method also be applied to other spiking neuron models?\n\nEven though the IF neuron model is 'common' in many SNN literature, I strongly believe that the study on SNN is meaningful only when temporal information carried by spikes is utilized in information processing. Averaging out spikes with rate-coding with IF neuron is functionally identical to quantized non-spiking neural networks, in which the activation function has quantized output. Multiplication with linearly-quantized values (e.g. low-bit fixed-point numbers) is already cost-effective and can also be implemented only by accumulation. I don't think there is a justifiable reason to use SNNs instead of this counterpart which can simply use conventional hardware. Major benefits of neuromorphic hardware such as event-drivenness can also be replaced by the zero-skipping feature that is common in NN hardware. Even though the proposed training method is fully spike-based, with the IF neuron model, it seems not to use any temporal information of spikes.\n\nI would like to hear the authors' opinions on this. Thank you.",
            "summary_of_the_review": "The proposed method effectively handles several obstacles while adopting the existing IDE method with spike-based implementation and provided convincing experimental results.\nHowever, I doubt that the theoretical improvements of this paper is significant enough especially with my poor understanding on the mathematical derivations in the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper aims at porting the IDE method  into a spike-fbased and more bio-plausible version. The previous IDE used firing rates rather than spikes for computation, although reference Xiao et al in NeurIPS 2021 had already addressed implementations in spiking neural networks. The authors analyze the approximation error that results from solving implicit differentiation by spikes and report a solution based on ternary spiking neurons, that can be implemented with pairs of standard spiking neurons. They achieve in this way quite good performance for MNIST and CIFAR10.",
            "main_review": "If one has an application of this method for neuromorphic hardware in mind, it becomes essential that one does not arrive in a rate-based coding regime. It has been demonstrated, e.g. for Intel's Loihi chip in (Davies et al., 2021) that one hardly gets and energy advantage in spike-based hardware if one works in a rate-based coding regime. Hence I am missing a discussion of this issue, and methods for arriving at sparsely firing neurons. If one aims instead at biologically plausible models, one also would have to look for solutions with biologically realistic firing rates of a few Hz.\n\nAltogether I find it difficult to identify the specific innovations of this paper because of its inadequate literature review. In particular, there are already quite a number of spiking neural network solutions that achieve comparable or higher accuracies with comparable latency and number of parameters, see e.g. \n\nZhou S, Li X, Chen Y, et al. Temporal-Coded Deep Spiking Neural Network with Easy Training and Robust Performance[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2021, 35(12): 11143-11151.\nFang W, Yu Z, Chen Y, et al. Incorporating learnable membrane time constant to enhance learning of spiking neural networks[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021: 2661-2671.\n\nThe abstract makes the really imprecise statement \"--most SNN training methods require complex computation or impractical neuron models\", which are hard to reconcile with publications such as\n\nBellec, G., Scherr, F., Subramoney, A., Hajek, E., Salaj, D., Legenstein, R., & Maass, W. (2020). A solution to the learning dilemma for recurrent networks of spiking neurons. Nature communications, 11(1), 1-15.\n\nThe authors should summarize and discuss more benefits of the proposed method. It is important for enhancing the impact of this work and drawing more audiences in the neuromorphic community.  In section 4.1, please explain why negative information is necessary for implicit differentiation calculation. \n\n\n",
            "summary_of_the_review": "The paper presents a nice step in an interesting direction. But it does not manage to clarify what exactly its innovations and possible applications are.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}