{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "After the author response multiple reviewers remained concerned over the degree to which the current manuscript makes the case for the proposed hyper-network approach to text-to-image generation. It was felt that this was mainly an empirical paper for which the reviewers remain unconvinced that the proposed hyper-network based modulation was better than simple channel-wise StyleGAN2 style modulation. While the authors have shown that their approach beats a StyleGAN2 baseline with sentence conditioning on CLIP-R the reviewers felt that the comparisons with StyleGAN2 baseline needed fairer word conditioning. Only one reviewer recommended accepting this paper.\n\nThe AC recommends rejection."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper adopts hyper-networks in text-to-image generation, which is used to control weights in both generator and discriminator, based on the provided text descriptions. Then, authors evaluate their method in traditional image generation and continuous image generation. ",
            "main_review": "Strengths:\n1. The implementation of the hyper-networks to control the weights in both generator and discriminator.\n2. A new dataset ArtEmis is adopt to evaluate the performance. \n3. The continuous image generation is explored.\n\nWeaknesses:\n1. The main technical contribution is the implementation of the hyper-networks in text-to-image generation tasks, I would expect to have more discussion about this implementation, like why text-conditioned hyper-network can improve the text-to-image generation task.\n\n2. In Table 2, the method based on StyleGAN2 with word conditioning and DAMSM loss is the selected best method proposed by authors, shown in Table 1. I am confused after the method also adopts the DAMSM loss, similar as other AttnGAN etc., the DAMSM-R is still not good, contracted to the claim that AttnGAN etc., achieve better DAMSM-R due to the adoption of DAMSM loss. Similar confusion about HyperCGAN INR with word conditioning and DAMSM loss.\n\n3. I am confused that the CLIP-R score for real images is much higher than the score for DAMSM-R. As I know, CLIP also adopts similarity calculation to train the model, why there is such significant difference between DAMSM-R and CLIP-R. Also, it would be better if authors can provide more details about how to calculate these two scores on real images.\n\n4. Although authors mentioned their method can be applied to continuous image generation, there are not many discussions and experiments about this shown in the paper.",
            "summary_of_the_review": "The paper evaluates the implementation of hyper-networks on text-to-image generation, for the technical novelty, it is not quite significant. Also, The discussion about the continuous image generation mentioned in the paper is not sufficient.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a novel approach for text-to-image synthesis. Instead of training the weights of a GAN directly the approach is to train a HyperNet which modulates the GAN's weights based on the text condition. The approach is evaluated for both StyleGAN2 and INR-GAN (a continuous GAN) and obtains good results compared to current baselines.",
            "main_review": "The paper proposes to train a Hypernet which directly modulates the weights of an underlying generator to generate images based on a condition. The training itself is done with the same losses as traditional GANs.\n\nOne of the novelties is how to structure the Hypernet to make it more efficient. Naively modulating the weights would imply to predict a modulation parameter for each parameter in the GAN which is inefficient and may not be computationally possible. Instead, the authors propose to use a low-rank decomposition to make the approach computationally feasible. They also show how to apply word-level modulation through their Hypernet.\n\nThe experiments evaluate their approach on both StyleGAN2 and INR-GAN on the COCO and ArtEmis datasets. The results are evaluated quantitatively based on IS/FID (for image quality) and R-precision/CLIP-R for text-image alignment. Additionally, user studies for text-image alignment were performed. The new approach outperforms current baselines (e.g. DM-GAN and DF-GAN) based on FID and CLIP-R which are arguably the two most important metrics.\n\nMy main question is about the exact motivation of this approach. You state that this is more powerful than the original approaches of directly conditioning the model on the text. It would be useful to have concrete examples of how/when your model is better or at least motivate in which settings your model would be (theoretically) better than current approaches. One of the motivations (from my point of view) may be that you can apply this approach directly to GANs that can produce high-res images like StyleGAN2. Have you tried using this to generate higher-res images (>256px)?\n\nThat said, how do you obtain the underlying GANs? Do you use pre-trained models (if yes, pre-trained on what)? Or do you use randomly initialized models?\nAlso, have you looked at other evaluation approaches for text-image alignment, e.g. SOA [1]?\nCould your approach be used for more fine-grained control over also object arrangement (can be evaluated by SOA) and relations between objects?\nAlso, XMCGAN seems to be missing from the baselines, both qualitative and quantitative.\n\n\n[1] Semantic Object Accuracy for Generative Text-to-Image Synthesis, TPAMI, 2020",
            "summary_of_the_review": "The paper proposes a new way of text-to-image generation by directly modulating GAN weights conditioned on the text input. The results seem promising and the evaluation is extensive. It would be interesting if this approach can scale to higher resolutions and can generate more complex images where we can also condition on invidiual object locations etc.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Authors explore the application of hypernetworks for text-to-image generation. Given the input text description, the hypernetworks learn the weights of convolution. Instead of directly leveraging hypernetworks, author propose to modulate the convolutional weights  by the generated weights. To validate the generalization of the proposed method, authors perform experiments on two generation architecture: StyleGAN and INR-GAN. Furthermore,  since hypernetworks lead to memory-intensive problem when using it to generate the weights  for each block, authors introduce new method to address this issue. The quantitative and qualitative results support the effectiveness of the proposed method.",
            "main_review": "Pros:\n1. It is first paper of using hypernetworks for text-to-image generation. It looks interesting.\n\n2. Utilizing hypernetworks  inevitably suffer from  memory-intensive problem, since it synthesizes the weights, especially for super network (e.g., StyleGAN), authors contribute some extend energy to fix it, which is convincing.  \n\n3. Different architectures and datasets are explored to validate the generalization. \n\nCons:\n1. I fail to get the intuition why we need hypernetworks for text-to-image. The paper could not show clearly the advantage when using hypernetworks. Compared to condition the text description on batchnorm layer, hypernetworks leads to more learned weights. \n\n2. Using hypernetworks has weak contribution in community, since we could insert hypernetworks into any current existed architecture. \n\n3. The reported results are not convincing, since both the baselines and the proposed method have different architecture. In fact, the proposed method benefits from the well-devised stylegan architecture. I am wondering what is the performance if we use stylegan-based architecture for text/audio-to-image translation.   Could authors show the number of parameters  (MB) and GFLOPs?\n\n4. It seems the proposed method has little advantage based on the reported results, which indicate that hypernetworks does not work well. In fact, what is the result if author remove the hypernetworks form the discriminator, and directly leverage the same one of styleganv2. I believe it improves the performance.\n\n5.  There are a few errors  in Figure 2, such as the overlap arrows and the repeated TGs in caption.  I do not appreciate the figure2, which is not beautiful and enjoyable. I would like authors to improve it. \n\n\n-------------After rebuttal------------------\n\nThanks for authors' response.  I still feel negative about this paper. One reason is that the proposed method is simple, which seems to use HyperNetworks for text-to-image generation. I fail to get the convincing intuition why Hypernetworks is perfect for this task. Second one is the advantage of the proposed method benefits from the stylegan-based architecture ($\\mathbf{Reviewer rpKE }$ also mentions it). To be summary, I am negative to this paper.",
            "summary_of_the_review": "Although this paper first explore text-to-image generation by using hypernetwork, I am wondering the intuition and the contribution. If only using hypernetworks, I think it is limited to be accepted.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a hyper-network based conditional approach for text-to-image synthesis. A hypernetwork takes as input the text representation and outputs a 4-D variable. This modulates the convolutions of both the generator and discriminator of the Generative Adversarial Network pointwise. Since predicting a 4-D Tensor is impractical, the hypernetwork predicts a low-rank decomposition, from which the 4-D variable can be constructed via tensor products.\n\nThe authors showcase this approach on a StyleGAN v2 backbone for discrete image synthesis and INR-GAN backbone for continuous image synthesis on COCO, CUB and ArtEmis Dataset.\n",
            "main_review": "**Strengths**\n\n* The authors achieve state-of-the art performance on FID, their newly proposed CLIP-R metric and human evaluations using the StyleGAN-V2 backbone.\n* *Auxiliary contribution*: The paper is the first application of continuous generative models for text-to-image synthesis allowing text-conditioned image extrapolation.\n* As far as I can see, the paper is also the first to augment StyleGAN for text-to-image synthesis. If this is true, the authors should mention this in the introduction.\n\n**Weaknesses**\n\nThe paper is unclear on some necessary details / simple baselines. I am open to adjusting my rating if the authors can answer them convincingly and make the corresponding changes to the manuscript.\n\n* Is the improvement from the baselines in Table 1 due to the StyleGAN v2 backbone or the proposed hyper-network based modulation? **Experiment Suggestion** : Run the channel-wise modulation (instead of the proposed modulation) as described in Eq 2). $s_i$ being a function of both the noise variable $z$ and the text embedding $c$. Report the results in Table 1.\n* The writing in Section 3.2 and Table 2 suggests that only word level or sentence level modulation is applied at a given instance. However in Figure 2, it looks as if both word and sentence level modulation are applied to the generator block. The authors should modify Figure 2, to show that only either of these is applied.\n* “To condition the final projection head in the discriminator on TDs(c), we use s = h >F(c), where h is the output of the last discriminator hyper-block, s is the output of the discriminator, and F(c), the tensor produced by our hyper-block.” Why is this done only for the sentence-level modulations?. Please also add dimensionalities to the corresponding variables. \n* In Section 3.2.1 “Extreme Modulating Tensor Factorization”, what are the outputs of the hypernetwork? Are they 4*R tensors ($t_1^1  \\dots t_n^1 \\dots t_1^R  \\dots t_n^R$ )? What is R set to be? How is it ensured that the outputs have their corresponding ranks during the course of training? Please carify these details.\n* Section 3.2.2 \"Word-Level Modulation\" can be motivated better. For, eg it seems like it is a cross-attention layer between the convolutional weights and the sequence. I suggest to run an ablation, removing this layer to showcase its importance.\n* INR GAN has both a generator network and a MLP. Is the proposed modulation applied only to the MLP? Why?\n* What architecture is the Text Encoder? How is the output pooled from shape (sequence length, hidden size) to (hidden size,) to form the sentence embedding? Is this trained end-to-end?\n* In the human evaluations, are the images from DF-GAN, DM-GAN and HyperCGAN shown at the same time or one after the other. In either case, what is the amount of time the human rater gets between two sets of images? Please add these details to make the study reproducible. Also consider adding them to Table 1.\n\n**Related Work**\n\nI recommend that the authors cite other (non-GAN based) related text-to-image synthesis works.\n\n[1] Reed et al Parallel Multiscale Autoregressive Density Estimation, ICML 2017\n\n[2] Reed et al Generating Images with Controllable Structure ICLR Workshops 2017\n\n[3] Mahajan et al Latent Normalizing Flows for Many-to-Many Cross-Domian Mappings ICLR 2020\n\n[4] Liang et al CPGAN: Full-Spectrum Content-Parsing Generative Adversarial Networks for Text-to-Image Synthesis\n\n[5] Mahajan et al Diverse Image Captioning with Context-Object Split Latent Spaces ICLR 2020\n\n**Other details**\n\n* It will be nice if the DAMSM loss is described in the paper to make it more self contained.\n* “When using sentence-level conditioning, we use an eightlayer MLP for the shared backbone that we call TGs body and a single-layer MLP as the output layer (TGs head), which is different for each generator block. For word-level conditioning, we use separate Conv 1x1 layers for each block in the generator (TGw block); see the Fig. 2 generator part”: A single-layer MLP and a Conv 1x1 for each block in the generator is equivalent?\n* Consider summarizing the “extrapolation meaningfulness results in a new table.\n\n**Typos**\n\n* The output of the backbone is consumed by the output layers, which then condition the generator via tensor modulation. What does “output layers” refer to?\n* For all human studies, 250 generations were selected at random. For each image we assigned 5 participants in Amazon Mechanical Turk and collected 1500 responses in total. I suppose this should be 1250?\n",
            "summary_of_the_review": "Please see the strengths and weaknesses.\n\nI have given this paper 5 (instead of 3) solely because of the empirical results.\n\nI have given this paper 5 (instead of 6 or 7) because the importance of the proposed pointwise modulation is unclear emprically.\nThe authors should report the improvements of their proposed hyper-network based condiioning over a simple StyleGAN-style channel-wise modulation. (ideally controlling for same number of parameters / FLOPS)\n\nAdditionally, some architectural and implementation details aren't clear enough, which I have highlighted in the weaknesses section.\n\n**Rebuttal Update**\n\nI read the authors response. Unfortunately, I have to stick to my lean reject decision.\n\nFor a purely empirical paper, the authors have not convinced me their hyper-network based modulation is better than a simple channel-wise StyleGAN2 style modulation.\n\nIn these experiments in the rebuttal, https://openreview.net/forum?id=z-5BjnU3-OQ&noteId=-oi3O8JQB-H, the authors have shown that thier model beats a StyleGAN2 baseline with sentence conditioning on CLIP-R. But they have not run their StyleGAN2 baseline with word conditioning / DAMSM loss, which makes me a bit sceptic about the gains with the hyper-network conditioning framework as opposed to just using the StyleGAN2 baseline.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}