{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper proposes a hardware-aware pruning method which structurally prunes the given deep neural networks to retain their accuracy while satisfying the latency constraints. Specifically, the authors formulate the latency-constrained pruning problem as a combinatorial optimization problem to find the optimal combination of neurons to maximize the sum of the importance scores, and propose an augmented knapsack solver to solve it, as well as a neuron grouping technique to speed up the training. The proposed method is validated for its classification tasks on two devices, namely Titan V and Jetson TX2, and for object detection performance on Titan V, and is shown to achieve superior accuracy/latency tradeoff compared to existing pruning methods, including latency-aware ones.\n\nThe paper received split reviews initially, and the following is the summary of the pros and cons mentioned by the reviewers.\n\nPros\n- The proposed formulation of the latency-constrained pruning problem as a constrained knapsack problem is novel. \n- The method achieves competitive performance against existing latency-constrained pruning methods. \n- The paper is written well, with clear motivation and descriptions of the proposed method.\n\nCons\n-  The idea is not very exciting since posing pruning as a combinatorial optimization problem, or a knapsack problem is not new, and the proposed method only adds in additional latency constraints.\n- The title “hardware-aware” is vague and misleading since what the authors do are latency-constrained pruning.\n- The experimental validation is only done on two devices, which makes the method less convincing as a “hardware-aware” method and how it generalizes to other devices (e.g. CPU, FPGA)\n- Use of lookup tables to obtain the latency constraints is not novel, has a limited scalability, and is inefficient.  \n- Missing discussion of design choices. \n\nDuring the discussion period, the authors cleared away some of the concerns, which resulted in two of the reviewers increasing their scores. However, one reviewer maintained the negative rating of 5, and the positive reviewers were still concerned with limited novelty.\n\nI believe that this is a good paper that proposes a neat solution for latency pruning, which may have some practical impact. However, the novelty of the idea is limited, as pointed out by the reviewers. The use of lookup tables also does not seem to be an efficient solution for adapting to edge devices for which the collection of latency measurements could be slow. The experimental validation on only two devices of the same type (GPU) also seems insufficient, as how the method generalizes to diverse devices is uncertain. It would be worthwhile to consider using a latency predictor (e.g. BRP-NAS [Dudziak et al. 20]), and perform experimental validation on diverse hardware platforms (e.g. CPU and FPGA). Comparing against recently proposed hardware-aware NAS methods could be also interesting, as there has been a rapid progress on the topic recently.\n\nThus, despite the overall practicality and the quality of the paper, the paper may benefit from another round of revision, since both the method and the experimental validation part could be improved. \n\n[Dudziak et al. 20] BRP-NAS: Prediction-based NAS using GCNs, NeurIPS 2020"
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This work tackles a latency-constrained structural pruning method by formulating the global resource allocation optimization problem and addressing them via an augmented knapsack solver. During the pruning process, to estimate latency on a target device and accuracy drop, this work uses look up table and global saliency score, respectively. The proposed approach is validated on classification and detection tasks with desktop GPU (Titan V) as the target device.",
            "main_review": "Strengths\n\n- This paper is well-written, well-motivated, and clear presentation.\n- The proposed method improves network throughput efficiency with competitive accuracy on classification and detection tasks, outperforming prior pruning approaches.\n- Formulating the hardware-aware structural pruning as a knapsack problem is new.\n\n\nWeaknesses\n\n- The reviewer's main concern is that this work is one of the lookup-table(LUT)-dependent methods that is not new and has critical limitations.\n    - After the emerging MNasNet[1], there are already numerous LUT-based methods improving the efficiency of neural networks under latency-constrained of each target hardwares.\n    - For the case that the correlation between the predicted latencies by LUT and the real latencies is low, the performance of LUT-dependent methods becomes poor. That means the performance of LUT is critical rather than the proposed method. The reviewer guesses that this method also would not guarantee high performance on the target tasks when the performance of LUT is poor.\n    - For considering many hardwares, building LUT process for all hardwares repetitively is not negligible and becomes a heavy burden. We should build LUT for each hardware and we should compile neural networks dependent on device platform types (sometimes it needs domain knowledge of experts).\n    - In addition, the information about LUT such as building process, building time, and correlation between the estimated and the real ones looks omitted in the paper.\n- Generality Issue on multiple hardwares. While this method is 'hardware-aware', the target devices used in the experiments are just two (GPU and Jetson). Recent hardware-aware methods for the efficiency of neural networks under latency constraints have been handled many hardware platforms and device entities. For example, OFA[2] considered 'GPU/CPU/Mobile Phone/Edge GPU/FPGA, BRP-NAS[3] considered 'GPU/CPU/Mobile Phone/Edge GPU/Edge TPU, and HELP[4] considered 'GPU/CPU/Mobile Phone/Edge GPU/FPGA/AISC/Raspberry Pi.\n\n[1] Mnasnet: Platform-aware neural architecture search for mobile, CVPR2019.\n\n[2] Once-for-all: Train one network and specialize it for efficient deployment, ICLR2020.\n\n[3] Brp-nas: Prediction-based nas using gcns, NeurIPS2020.\n\n[4] HELP: Hardware-Adaptive Efficient Latency Predictor for NAS via Meta-Learning, NeurIPS 2021.",
            "summary_of_the_review": "This paper has a clear motivation and introduces a knapsack algorithm for NAS, yet, validates the proposed method on limited devices and has weak novelty since the latency estimation is dependent on LUT.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper is an effort to perform structured pruning with regard to inference latency on hardware. The paper solves this by creating a look-up table of \"importance score\" and \"latency contribution\" and formulates this into a Knapsack problem, which can be solved efficiently. Evaluation shows that this method effectively reduces the latency with regard to hardware via pruning.",
            "main_review": "Importantly, the paper works on a very important topic. Model compression is very important when it comes to deployment to ensure QoS of the inference services.\n\nI believe the main benefits of the work are (1) formulating hardware-aware pruning while retaining good performance and (2) doing this efficiently. The formulation of the overall pruning procedure into a Knapsack problem seems to be where the main benefit comes from. Neuron grouping proposed in the paper also seems to be another source of speedup.\n\nQuestions I have are:\n\n* While the paper claims that this can be applied to other platforms too. However, this requires re-generating the look-up table for HALP. Could you provide how long this would take for a new platform? Can this be done efficiently?\n* How would this perform in relation to quantization? And, would similar mechanism for enforcing hardware-awareness in compression work for quantization?",
            "summary_of_the_review": "Hardware-awareness of pruning, while touched upon by some works, still opens up large potential for inference latency reduction. This paper proposes an interesting direction in optimization where each layer is assigned importance score and measured latency of each layer. Then, the overall pruning problem is formulated as a knapsack problem. Overall performance of the pruned network seems reasonable. I liked reading the paper, and I would like the paper to be in the program.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "==+== A. Paper summary\n\n\nThis paper proposes a method to do channel pruning. By building up latency and important score for each group (or channel), the paper use an ILP solver to select the channels that can achieve the best trade-off between latency v.s. Accuracy. The evaluation shows that the method achieves better speedup compared to traditional channel pruning with almost no accuracy loss.\n",
            "main_review": "\n==+== B. Strengths \n\nThe evaluation is strong and the analysis is thorough. The Pareto curves show the effectiveness of the algorithm.\nThe writing is clean. \n\n==+== C. Weaknesses  \n\nIn my opinion, the novelty is not very exciting.\nThere are a lot of design choices (e.g., computing important score) and the alternatives are not discussed.\n\nQuestions:\n\nCan you make a comparison between dynamic channel pruning (e.g., https://arxiv.org/pdf/1810.05331.pdf)? The term `dynamic grouping’ is confusing in your paper, you should clarify that it happens during training but not evaluation. Also, my intuition is that using dynamic pruning during evaluation can push the Pareto curves further (fig.3); so it should be an important baseline. \n\nThere is also a lot of design options in each part of your design. For example: 1) Computing the importance score you can use the method here (https://arxiv.org/abs/1810.02340) by computing the gradient of each neuron. 2) Regarding the latency prediction, you mentioned that latency is not linear to the number of FLOP; however, for a layer-wise model architecture (most of the models in your evaluation are layer-wise), this is not always true. I am wondering in which scenario latency and FLOPs are not linearly related.\n\nIn Table 3, are you sure the baselines (~8000h) are fair compared to yours (30m)? As far as I can tell, you did not include the extra GPU training time. The number reported is confusing, can you possibly show a time breakdown of your compilation process?\n\nMinor:\nYour title is so vague and it looks like a survey paper. You can consider changing the name of your paper.",
            "summary_of_the_review": "The quality of this paper is good. The experiments are thorough and solid. However, I think the novelty of this paper is limited.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The authors propose to formulate structural filter pruning as a global resource allocation optimization problem with latency constraints. To solve the resulting knapsack problem, the authors devise an augmented knapsack solver and further propose neuron grouping to reduce the pruning space and computational cost. Extensive experiments on image classification and object detection tasks show the promising performance of the proposed method. \n",
            "main_review": "**Contribution**: \n\n1. The authors formulate structural filter pruning as a global resource allocation optimization problem with latency constraint rather than theoretical computation cost (FLOPs).\n2. The authors devise an augmented knapsack to solve the resulting combinatorial optimization problem. \n3. Extensive experiments on image classification and object detection tasks show the promising performance of the proposed method.\n\n**Questions and points needed to be improved**:\n\n1. Algorithm 1 is confusing. Many notations are unclear and not rigorous. For example, what does dp_array denote? What does v_{keep} denote? Moreover, the authors do not provide any explanations regarding Algorithm 1, which makes it hard to follow. More explanations are required.\n\n2. In Section 3.3, the authors state that the proposed method groups the neurons sharing the same channel index from the connected layers in a network with skip connections, which have been proposed in [1-2]. It would be better for the authors to cite the related papers.\n\n3. In Section 3.4, the authors propose to perform pruning for k steps in total, which would improve the performance of the pruned models. It is unclear whether the good performance is resulting from multi-step pruning. It would be better for the authors to provide more ablation studies on the effect of different k.\n\n4. What will happen if the authors replace the latency constraints with FLOPs constraints? Does the performance improvement of the proposed method come from a better metric of computation cost or a better solver? More discussion and experiments are required. \n\n5. The authors conduct experiments on heavyweight networks (e.g., ResNet-50, ResNet-101, VGG-16) and a lightweight network (e.g., MobileNetV1). It would be more convincing for the authors to provide more results on lightweight networks with residual connections (e.g., MobileNetV2 [3]).\n\n6. To demonstrate the effectiveness of the proposed method, it would be better for the authors to provide the detailed configurations (the number of channels of each layer) of the pruned models, which will strengthen the paper.\n\nMinor issues:\n1. Figure 1 is too small to read.\n\n**References**:\n\n[1] Centripetal sgd for pruning very deep convolutional networks with complicated structure. CVPR 2019.\n\n[2] Neural Network Pruning with Residual-Connections and Limited-Data. CVPR 2020.\n\n[3] MobileNetV2: Inverted Residuals and Linear Bottlenecks. CVPR 2018.",
            "summary_of_the_review": "The performance is promising. However, some important details of the proposed method are missing, which makes it hard to follow. Moreover, experiments are not sufficient. More experiments are required to show the effectiveness of the proposed method.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}