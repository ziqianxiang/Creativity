{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "A heterogeneous federated learning framework is proposed which does\nnot require auiliary public data sets, and does not reveal the private\ndata to the server or answering parties if they operate as\nhonest-but-curious entities. It builds a new protocol for private\ninference, which can run on GPUs, and proposes a dataset expansion\nmethod to not need an auxiliary data set. The paper presents extensive\nempirical experiments on the method.\n\nThe paper was extensively discussed with the authors. The concerns\nincluded both technical issues and more general issues on missing DP\nguarantees and realisticness of the threat model. Many of the issues\nwere resolved by the clarifications provided by the authors, and as a\nresult two reviewers increased their scores. However, all reviewers still\nplace the paper to the borderline.\n\nWhile the paper contains solid work, and improves efficiency compared to\nprevious models, this is a borderline paper where the final judgement\nneeds to be based on importance of the presented new contributions in\nadvancing the field. The paper may not yet quite reach the bar, but\nI believe the reviewer comments have enabled the authors to improve the\npaper for further work."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper showed a new approach for heterogeneous federated learning, which uses augmented dataset instead of a public dataset for knowledge transfer between heterogeneous models. The authors also suggested a lightweight additive secret sharing technique to construct a series of tailored cryptographic protocols, which is friendly with GPU and the CUDA kernels.\nThe method was evaluated in a simulated scenario with three public imaging datasets. And shows superiority over the baseline methods, and also efficiency with regard to run time and communication cost.\n",
            "main_review": "The authors proposed to use synthetic dataset generated using the Mixup method. I may have the following questions regarding this approach:\n1. How can we guarantee that no privacy information will be breached using this synthetic method? \n2. Will there be possibility to reconstruct the original dataset from this simple mixture? For example, a mixup will be identical to a real sample when ùõå = 1. Will this still leak the privacy of local data?\n",
            "summary_of_the_review": "I feel not fully confident to assess the quality of the work as I am non-expert in federated learning.",
            "correctness": "2: Several of the paper‚Äôs claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper introduces a heterogeneous federated learning framework without requiring public datasets by designing a dataset expansion method and constructing cryptographic protocols for secure prediction. The main strength of this paper is regarding the experimental results. The authors have provided experiments on different datasets, heterogeneous local models, various degree of non-IID-ness across clients, and showed the runtime of three steps in their proposed secure querying protocol.   ",
            "main_review": "Query data generation: It is not clear how $\\lambda$ is selected?  Is there any theoretical justification for the claim that \"this methods provides a good coverage of the manifold of natural samples\"? Given two private samples, the authors generate multiple query samples. It is not clear how the private samples, $\\lambda$, and the number of synthetic samples for each given pair of private samples should be selected to achieve the optimal coverage of natural distributions. \n\nQuery-data sharing:  It is not clear how $Sk_{QA}$ can be constructed without direct communication channels between clients and considering that each client can play the role of both querying party and responding party at the same time (possibly responding to multiple queries)? \n\nResult aggregation: It is assumed that all clients participate. This is not the typical scenario in federated learning. In addition, a unique $r_j$ is generated for each client, which is not consistent with other steps of the algorithm. \n\nExperiments: How much will the accuracy degrade if you set secret-sharing protocols over other rings? \n\nAlgorithm 1: Why do you need line 5 while each client already trained local models on its private dataset? \n\nMinor comment: \n\nOn the RHS of Figure 4, $\\delta'_i$ should be $\\delta'$\n\n",
            "summary_of_the_review": "This paper is well-written and provides extensive experiments which show the efficiency of PrivHFL and accuracy gains compared to a baseline, which uses models trained on the local datasets. Some aspects of the dataset expansion method and  the secure querying protocol are unclear, which require further clarifications. ",
            "correctness": "3: Some of the paper‚Äôs claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "PrivHFL is a protocol for collaborative learning that does not reveal the private data to the server or answering parties $P_A$ if they operate as honest-but-curious entities. PrivHFL tackles the bottleneck of private inference by building a new protocol from scratch that can run on GPU. It is not assumed that parties have new data or that there is a public pool of data to run inference on but the additional query data for knowledge transfer is obtained by augmenting the private data with MixUp.",
            "main_review": "**Strong points:**\n\n1. The improvement in efficiency against the previous methods, e.g., CryptGPU or CaPC.\n\n2. Good design. PrivHFL builds on top of CrytpGPU to compute over integer values (64 bit ring elements). CryptGPU embeds the integer-valued cryptographic operations into floating-point arithmetic.\n\n**Weak points:**\n\n1. **Private data revealed in plain text if $P_Q$ colludes with the server**. The revealed data is an augmentation of the private dataset using MixUp but it still leaks information about the private data. In contrast, e.g., in CaPC, once the server (the 3-rd trusted party - Privacy Guardian) and answering parties collude, only the predicted label is revealed but not the private data.\n\n2. **The logits are aggregated** (as described in Figure 5) but these values are not normalized across models so each model can have a very different impact on the final predictions. This makes the method more vulnerable to attacks.\n\n3. **MixUp was already used for Federated Learning in [1]**.  Yoon et al. propose to apply MixUp to Federated Learning for addressing the challenge of non-iid data.\n\n4. MixUp is not a state-of-the-art (SOTA) data augmentation method. Could MixMatch [2] (it also shows improvements for MixUp) or FixMatch [3] or other SOTA be used? The other data augmentation methods from Figure 10 are rather rudimentary. \n\n5. **No DP provided**. The revealed logits leak information about private data from the answering parties ($P_A$).\n\n6. **Intermediate representations are released to $P_Q$**. From Section 3.3 \"After that, $P_A$ and the server disclose the secret-shared value [z] to $P_Q$, which then computes the sign of z, i.e., the sign of x. sign(z) is shared to the server and $P_A$ by using the protocol in Figure 2.\" Figure 4 shows that scaled intermediate results (inputs to the ReLU layers): $z = r([x]_1 + [x]_2)$ are revealed to the querying party ($P_Q$). How much can $P_Q$ infer about the model from these intermediate results? It seems that $P_Q$ can infer the model architecture that is used by $P_A$. Is z some scaling of the original intermediate representation, or every value from the intermediate representation can be scaled differently to obtain the representation sent to the querying party. The latter might make the method practically robust since the querying party is rather not able to infer the parameters of the model used by the answering party. However, this should be confirmed formally. In the former case (i.e., z is only a scaled intermediate representation - input to ReLU), the parameters of the model used by $P_A$ might be extracted from z. However, it is claimed at the end of Section 3.3 that: \"We give a formal security proof in Appendix E. Intuitively, PrivHFL reveals zero information to the answering parties $P_A$ and the server, and only reveals the final aggregated prediction to the querying party $P_Q$, since all intermediate values are secret-shared. Given the above, a corrupted $P_A$ cannot learn anything about the query data of querying parties, while the confidentiality of responding parties‚Äô model parameters against corrupted $P_Q$ is also protected.\"\n\n7. **No assumption on a public dataset**. For example, CaPC does not assume that there is a public dataset, thus the following is not correct: ‚ÄúBesides, to the best of our knowledge, **existing methods all require a public dataset** to implement heterogeneous federated learning.‚Äù Since the data is always encrypted in CaPC, the querying party can send the private data with a guarantee that it is not revealed to any other party. Additionally, CaPC can serve as a private consultation protocol. The natural new data to be labeled come from new patients or clients (in the case of hospitals or banks, respectively). \n\n8. **No direct communication between clients.** This is easy to fix. For instance, CaPC can be extended to a setting where there is no direct communication between clients. If we have parties P1, P2, P3. P1 and P2, as well as P1 and P3 can communicate directly. They just need to encrypt their messages and ask P1 to pass them along.\n\n9. CaPC is a modular protocol. CryptGPU, which has been published recently (after CaPC), could replace HE-transformer to accelerate the private inference while maintaining CaPC's strong privacy and confidentiality guarantees.\n\n10. Active learning was also used in CaPC.\n\n**Questions:**\n1. Why is scalability an issue (in terms of the number of clients and size of the models)? Could you elaborate on it, please? \nFigure 6: How many models have which architecture? \nTable 1: For ‚ÄúPrivate data‚Äù is the assumption for the baseline that there are not labels for the private data and they are obtained by querying the answering parties?\n2. How is the whole private inference implemented across all the layers? PrivHFL does not provide the source code. It would be informative to see how really the additive sharing is used for private inference and when the intermediate results are revealed to the querying party $P_Q$. \n3. Should not DReLU be defined as: $\\frac{1}{2}*(1 + sgn(x))$ ?\n\n**Comments:**\n1. Section D.1 is a repetition of the introduction.\n\n**Typos:**\n\ninferior than -> inferior to (Section 3.2)\n\nexperiment setup -> experimental setup (Section 4.1)\n\nthey \"are\" most uncertain (Section C.1.)\n\nSuggestion: \"number of private data\" -> \"number of private data samples\"\n\nthey using -> they use (Section F)\n\n\\citep instead of \\cite or \\citet should be used in most cases (e.g., the first paragraph in the Introduction).\n\n**References:**\n\n[1] FedMix: Approximation of MixUp under Mean Augmented Federated Learning. Tehrim Yoon, Sumin Shin, Sung Ju Hwang, Eunho Yang. (ICLR 2021).\n\n[2] MixMatch: A Holistic Approach to Semi-Supervised Learning. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, Colin Raffel. arxiv:1905.02249 (NeurIPS 2019)\n\n[3] FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence. Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Han Zhang, Colin Raffel.  (NeurIPS 2020)\n",
            "summary_of_the_review": "The paper shows much better performance (e.g., much faster execution of the private inference) but this is at the cost of lowering the security level. Additionally, the aggregation through logits makes the method more vulnerable to attacks than other proposed protocols, such as, CaPC. MixUp and Active Learning methods were already used in federated learning (FedMix, CaPC) so this is cannot be classified as a technical novelty.",
            "correctness": "2: Several of the paper‚Äôs claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}