{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The authors develop a technique for unsupervised learning of neurosymbolic encoders. Some of the difficulty with the paper came from the accessibility to a broader machine learning audience, though there is related work such as Shah 2020 in machine learning. The other difficulty came from the experiments: there was both a question about the metrics and the task. Quoting a reviewer\n\n\"Current evaluation seems not very convincing to me. The authors only show that with the help of symbolic program, the method could get representations with better cluster quality (program helps representation learning). But I think a more intersting perspective is to see whether the learned program itself is helpful. For example, whether it could be used to predict future trajectory (such as 3-body problem), or even help solving some high-level reasoning tasks.\"\n\nand another reviewer \n\n\"Maybe something comparing the programs of experts to what the latent representation learned?\"\n\nMaking the paper more accessible and improving the experiments will improve its quality."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes to learn a novel, interpretable neuro-symbolic encoder for sequences in an autoencoding framework. The key idea is that one learns both a symbolic as well as a neural encoder, and gradually makes the symbolic encoder more and more structured progressively increasing the complexity of it. Given such an encoder, the paper proceeds to show that the encoding from it is useful for clustering sequence data and demonstrates gains over other “unstructured” purely neural approaches.\n",
            "main_review": "Positives\n+ The general problem of learning programs to represent data in unsupervised learning is novel and potentially fundamental to machine learning and AI\n+ The proposed method is simple and interesting, and seems to achieve nice gains over purely neural approaches\n+ The paper is generally well written\n\nNegatives\n- My main issue is that the neuroscience data that the paper currently mainly evaluates on is not something a general ML audience might be very familiar with, and thus, it is hard to estimate what makes the task hard or easy. Could the authors give intuitions on why regular neural encodings fail to cluster in a better manner? Is it because of low SNR in the sequence data, or that RNN encoders pick up on spurious noise signals and thus do not yield a sufficiently discriminative embedding? If that is the case how would a convolutional encoder perform? Are the gains from the symbolic approach something that could just be achieved by a convolutional encoder? How much do we really need the symbolic approach and what are the inductive biases that we gain from them? I think these are important questions to answer in order to judge the efficacy of the proposed approach. (*)\n\n- In terms of the methodology, I do not understand why increasing the complexity of the programs gradually also leads to the purely neural part becoming less and less informative in the toy-dataset. Could it not have been the case that somehow the more complex program just learnt the features that the less complex program (with say length 2) learnt, and that in going from Fig. 3 d) to 3 e) we just learnt the same embedding space for the neural part? Is it encouraged that we learn more complex programs when we increase the length of the programs by tuning the corresponding gamma weights in Eqn. 5, or the corresponding channel capacities? (*)\n\n- I generally think such an encoding is potentially very useful for image data, and might be a big step towards better image representations and advances towards human-like intelligence. It would be great if the authors could comment on the potential for extending the approach to such applications and how far we might be from such use-cases. \n\nMinor Points:\n1) [A] is a closely related work that would be useful to cite in a neural-symbolic + amortized inference / VAE context.\n\n[A]: Vedantam, Ramakrishna, Karan Desai, Stefan Lee, Marcus Rohrbach, Dhruv Batra, and Devi Parikh. 2019. “Probabilistic Neural-Symbolic Models for Interpretable Visual Question Answering.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1902.07864.",
            "summary_of_the_review": "My main concerns with the paper are around whether the RNN encodings are overfitting and picking up on spurious correlations in sequence data that say a convolutional encoder might already fix, which will mean that one does not really need symbolic encoders for the current task. This is an even larger concern in light of the fact that a general machine learning researcher might not have a lot of great intuitions about the neuroscience sequence data and that makes it hard to assess the impact of the work. For the rebuttal I would encourage the authors to address concerns marked with (*) in the main review.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper is a clean extension of the supervised neurosymbolic approach\nof Shah et al. to the unsupervised setting where a VAE is used\nto encode input to the system as an interpretable program.\nThese latent representations are shown to be interpretable for\na synthetic experiment and useful for downstream tasks.\n\t",
            "main_review": "The paper is clearly written and the idea was easy to understand and\nfollow. The experiments are always going to be a challenge in a\nspace like this as how can we say if the symbolic latent\nrepresentation meaningfully captures something in the data.\nThe synthetic result while a little contrived is convincing that\nat least in a controlled setting a reasonable program is produced.\nI wish there was more exploration of how often are sensible\nprograms learned and how sensitive this is to the choice of DSLs.\nMaybe something comparing the programs of experts to what\nthe latent representation learned? Basically the program equivlaent\nof showing a grid of faces.\n\nThis would also benefit from comparisons that use purely the neural component\n\nI am also curious empirically how it's decided how many symbolic\nprograms should be in the latent representation? Would a similar\nprocess be used for real-world data?\n\nMy main concern is how heavily this paper relies on the NEAR\nwork particularly for learning the symbolic encoder and even\nthe experimental data used. As the VAE bits are fairly standard\nit makes the work feel fairly incremental.\n",
            "summary_of_the_review": "This is an interesting and novel way to learn programs as symbolic encoders of the input. I have some nagging concerns about how incremental the contribution given how much this work relies on Shah et al.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper tries to use VAE-based generative task to generate a latent representation that could be encoded as symbolic program. The authors show that in this way it could achieve better cluster results and also generate interpret-able and reasonable programs for each data.",
            "main_review": "I think the idea of utilizing self-supervised training task to generate symbolic programs is very interesting and worth exploring.\n\nMy major concerns lie in the approach and evaluation.\n\nFor approach:\n1) Though I read the paper carefully, I still didn't fully get how the authors represent the program via z(alpha, fi). It seems that the output of VAE should be a vector, and how did the authors decode z(alpha, fi) to alpha and fi, especially when alpha should be a \"discrete program architecture\". Is alpha a program or just a single operator? Also how did you encode q(alpha, fi) when alpha could be a complex tree-structured program?\n2) I'm also not fully understand equation 3. How to you search the child program of alpha in a differentiable manner? Why this is a supervised task (from my understanding it's more like finding a child program that outputs similar encoding vector as parent program).\n3) You mention you \"repeat steps 1&2 until q(apha, fi) is fully symbolic\". The meaning of fully symbolic is really confusing to me. What is the stopping criterion? Also, what's the intuition that you need to update the parameter agrain after fixing program alpha?\n4) The authors didn't mention how each function in the DSL is implemented in paper and appendix. I think it's very important to understand the whole architecture. Also, it's important to know how you handle the non-differentiable operators, such as if-then, and others.\n\n\nFor evaluation:\n1) Current evaluation seems not very convincing to me. The authors only show that with the help of symbolic program, the method could get representations with better cluster quality (program helps representation learning). But I think a more intersting perspective is to see whether the learned program itself is helpful. For example, whether it could be used to predict future trajectory (such as 3-body problem), or even help solving some high-level reasoning tasks.\n2) Lack of the ablation study of the proposed framework. For example, why it's important to update the parameter again. Is the design of DSL influences the final representation learning, etc.",
            "summary_of_the_review": "This paper proposes a very interesting research direction, but the writing and organization of the proposed method make it hard to understand it. In addition, the current evaluation is way too simple and not interesting enough. I highly recommend the authors to add some experiments to show that the learned programs can help some down-stream tasks.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose an unsupervised approach to train neurosymbolic encoders to obtain a programmatically interpretable representation using the dictionary of a domain-speciﬁc language (DSL). The experimental results show that the proposed method can outperform baseline neural encoders in extracting semantically meaningful representations of behavior in CalMS21 (mice) dataset. The results also show that the performance can be robust across different DSL designs by domain experts.\n\n\n\n\n",
            "main_review": "The strength of this paper is described above. Although the concept and method are simple and new, and the paper itself is well-written, the significance in the problem setting and experimental results were unclear to me at this stage. The specific comments are as follows. \n\nMajor concerns: \n1. Although the unsupervised neurosymbolic “encoder” is new (as described in Sec. 5, but the decoders existed in [Feinman & Lake, 2020] and also the following [1]?), but I did not understand why the authors considered that creating the encoder is important.\n2. Although the concept of the proposed method is general and interesting, the applied domains were limited to animal/human trajectories. That is, it is unclear to me why the authors selected these domains to verify the proposed method. Or verification using other domain data may be also required.\n3. The significance of the experimental results was unclear to me. The task to extract distinctive information between the data w/ and w/o interaction in mice may be considered less challenging for me. The basketball task including offensive/defensive players is more challenging, but the result of the proposed method was overall comparable with the baselines (whereas it improved slightly with respect to purity). In the basketball task, I am also concerned about the importance of offensive/defensive players detection (this is usually given). The final downstream task (not tasks) is a classification task, and the results were comparable with the previous work (Sun et al. 2021b). \n\nMinor comments:\n4. What is x in Sec. 2.2? If it is some inputs, what types of inputs?\n5. Which is correct, .428 in NMI of Figure 4 or .423 of Table 1? Or What DSL was used in Table 1 results? \n\n[1] Halley Young, Osbert Bastani, Mayur Naik, Learning Neurosymbolic Generative Models via Program Synthesis, ICML, 97:7144-7153, 2019.",
            "summary_of_the_review": "Although the concept and method are simple and new, and the paper itself is well-written, the significance in the problem setting and experimental results were unclear for me. Therefore, it is difficult for me to provide a higher rating at this stage. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}