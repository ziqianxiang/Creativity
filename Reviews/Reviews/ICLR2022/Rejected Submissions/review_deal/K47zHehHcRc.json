{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper introduces the notion of interventional consistency of a representation learned using autoencoders, which is claimed to be a desirable property for disentanglement. The reviewers agree that the contributions are novel and relevant, but they also found the paper hard to follow due to a lack of clarity and motivation. Further, they considered the underlying assumptions very strong and possibly hard to find practical instances where they may hold (e.g., the assumption that statistical dependencies in the prior are preserved by the response map). The reviewers also noted that some real-world examples showing the interventional consistency would be helpful. \n\nAfter all, the paper contains interesting ideas and we would like to encourage the authors to pursue this line of work. Still, the paper in its current form is not ready for publication. We encourage the authors to address the reviewers' comments explicitly in a future version of the manuscript."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a novel principle for representation learning using an autoencoder, named interventional consistency. The tractable objective functions, namely Invariance Score and Equivariance Score, are formulated, and Explicit Causal Latent Block is proposed to model the causal structure of the latent representation.",
            "main_review": "Strengths\n- The proposed concept, the interventional consistency, seems highly novel. It extends the notion of consistency in a meaningful way.\n\nWeaknesses\n- The largest weakness of the paper is the lack of clarity. The paper is very difficult to follow, and I believe the difficulty is attributed to the unpolished exposition. In general, Section 2 assumes the readers have a strong background in causal representation learning to an unnecessary extent. \n  - The ICM principle is introduced without stating its full name (independent causal mechanisms, I guess?). \n  - Since ICM works as a fundamental underlying principle for the work, it deserves a more detailed illustration and justification.\n  - It would be helpful for readers if it is stated that PA states \"parents\". \n  - What is the definition of the intervention distribution? What are the examples? What properties of them are assumed?\n  - Providing a concrete, real-world example of interventional consistency would be helpful.\n  - While the paper says $m$ and $k$ are assumed to be equivalent, Figure 1 depicts the scenario where $m$ and $k$ are different. Additionally, I couldn't come up with a meaningful real-world case where $m\\neq k$.\n- The other parts of the paper also have clarity issues.\n  - Texts in Figure 2, Figure 3, Figure 5, Figure 8 are too small and difficult to read.\n  - The figure numbering is missing for Figure 5.\n  - There is a broken reference link on page 6.",
            "summary_of_the_review": "I vote to reject the paper, due to the lack of clarity. To improve the clarity of the paper, significant rewriting seems to be needed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors argue that the interventional consistency is a desired property for the representation of auto-encoders and proposed a new metric on it. The authors further develop a new training procedure for auto-encoder that has extra regularization terms to enforce the interventional consistency.",
            "main_review": "Frankly, I am not quite convinced why the interventional consistency is desired even after reading the draft. Some assumptions seem too strong to me and I feel the main objective of the paper is not conveyed well.\n\nSome typos and issues listed here:\n\n1) Unexplained notations: \n\n      a) section 2, composition R = E\\circle D. here neither E nor D is defined. \n\n      b)  Figure 4 comes before the figure 3\n\n      c) the third paragraph of section 3, related work: there is a citation error with question mark\n\n\n2) Lack of clarity:\n\na) The major algorithms 1) and 2) are not described in the main paper.  We have to reach out to the appendix for the algorithm description. \n\nb) The optimization objective L_{INV} and L_{EQV} are not specified in the main paper either.\n\nc) The response map is mainly introduced by citing a related work. However, given its importance to this paper it is better explained and self-contained.\n\nd) There is not much explanation on the latent traversals.\n\n\n3) Assumptions:\n\nThere is an assumption that the statistical dependencies in the prior are preserved by the response map.  This assumption looks to me very strong. Not sure if such an assumption makes sense for real data.\n",
            "summary_of_the_review": "I feel the draft is not in a status for publication yet. The authors may need to polish it to have a clear story line. Also it needs to be self-contained.\n\n\n########Update after the authors' rebuttal###########\n\nI would like to thank the authors' explanations on the notations as well as the assumption concerns. Overall I feel this draft has potential to be a good one. As stated in the authors feedback, in its current status more work is needed to make it easy to read. (For example, the algorithm descriptions are still hidden in the appendix.) As such I am keeping my ratings unchanged.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes to use interventional consistency to regularise representation learning in VAEs.\nThe idea is well-motivated by the ICM principle and theoretically justified.\nThe paper suggests to use interventional consistency for both training and evaluation of the representation learnt by VAEs.\nResults show that the proposed idea can give more modular and interpretable representation.",
            "main_review": "Strengths\n- The idea is well-motivated and seems to be theoretically justified.\n- The empirical results are promising and support the main claim of the paper.\n- The limitation of the proposed method is also discussed (in the paragraph right before Section 2.1).\n\nWeaknesses\n- Presentation of the paper can be improved\n  - ICM is not defined when it's firstly used in page 2.\n  - There is a broken citation in the 3rd line of the 3rd paragraph of Section 3\n  - The first two figures in Section 5 is not numbered.\n    - The order of all methods in the two figures is weird/inconsistent. You should keep a consistent order for all baseline + variants.\n    - I also suggest better a better way to label them: You can colour each baseline model with a colour and fill the variants with (no filling, dots, dashed lines).\n\nQuestions\n- For Figure 2, how do I know the traversals are supposed to give low errors? i cannot tell if the traversal is along any disentangled dimension from the images.",
            "summary_of_the_review": "The paper proposes a well-motivated and (seemingly) theoretically justified way to regularise representation learning in VAEs.\nThe results also support the main claim of the paper.\nTherefore I believe this paper is beneficial to the community.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work proposes the notion of \"interventional consistency\" as a beneficial property learned representations should have and introduces a regularization term to enforce it in autoencoders. Moreover, the authors introduce an \"explicit latent causal block\" that allows learning a structural causal model (SCM) over the latent factors. The experimental section argues that both the regularization and the \"causal layer\" improve interventional consistency. ",
            "main_review": "- The paper is motivated by the goal of \"causal representation learning\" and explicitly mentions it does not address the important question of identifiability. The most important problem is that there is no justification as to why the authors' notion of interventional consistency has anything to do with what researchers in the past have meant by a \"causal representation\" (e.g. disentangled, modular, robust, ...etc.). \n- I found the paper hard to read even though the ideas proposed are rather simple. The paper is framed within the larger context of causality, but I am not sure this is the best way to present the contribution, which might explain my confusion. \n- The motivation for the \"explicit causal latent block\" is unclear. Why does it make sense to have the output of the encoder be the noise of the SCM over $Z$?\n- Some very important quantity like $\\mathcal{L}\\_\\text{INV}$ and $\\mathcal{L}\\_\\text{EQV}$ (which are the actual regularizer proposed) are defined only in the appendix. Important contributions should be presented in the main text. Also, the explanations for these central quantities lack clarity.\n- It is not clear why learning a SCM in the latent space of MNIST makes sense. For instance, I can't imagine the shape of a digit causing its thickness. \n- Figure 6 shows three latent traversals for a baseline and two variants of their approach and claim that their approach yield more modular traversal. However, it is not clear what is meant here by modularity and I do not see how the latent traversals of their approach are better.\n- The experiments show that indeed the regularizer improves \"interventional consistency\". But, as I said earlier, it is not clear how this relates to \"causal representation learning\" or why it is something beneficial regarding disentanglement, robustness, modularity etc. \n\nMinor:\n\n- The notation is sometimes imprecise: for example, in the equation between (2) and (3), it is not clear what is meant by $Z'\\_{j \\not= i}$. Is it all components of $Z'$ that are not $i$? If so the decomposition is wrong. Is it $Z'\\_{j<i}$ for some ordering of the nodes? If so what is the ordering? This point matters because (4) goes on specifying what is meant for the \"ICM principle to be preserved by the response map\" based on this above equation.\n- The notation is sometimes hard to parse: for instance, Section 2.1 has a few double \"hat\" like this: $\\bar{\\tilde{Z}}$. \n-  Table 1 and 2 have no error bars, making it hard to asses whether the gaps between different methods are significant or not. \n",
            "summary_of_the_review": "Given the points raised above, I cannot recommend acceptance. My main concerns are with (i) the lack of motivation for both the \"interventional consistency\" and the \"explicit causal latent block\", as well as (ii) the (sometimes significant) lack of clarity. The paper did not make a convincing case that \"interventional consistency\" is a valuable tool for representation learning.   ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}