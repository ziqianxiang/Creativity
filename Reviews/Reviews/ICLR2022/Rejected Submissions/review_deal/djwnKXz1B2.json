{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper presents a Bayesian GAN approach designed for a federated learning setting. In contrast to recent Bayesian GAN approaches that use Gaussian priors or iteratively-updated priors on GAN parameters, this paper proposes a more complex prior motivated by expectation propagation, dubbed as EP-GAN, and uses this formulation to construct a federated GAN. The paper claims that this prior better captures the multimodal distribution structure of the non-iid heterogeneous data across the different clients.\n\nThe paper looks at an interesting problem, i.e., federated training of GANs, which is indeed a problem that has received a lot of interest lately. The paper received mixed reviews. The reviewers raised several concerns, some of which included (1) weak baselines, (2) not considering what happens when we switch to more advanced GAN models, (3) performance of the approach when the number of clients is large, and (4) lack of clarity in the presentation. \n\nThe authors responded to some of these concerns and it is commendable that they reported some additional results during the discussion phase. However, after an extensive discussion among the reviewers and between reviewers and authors, and after my own reading of the manuscript, concerns still lingers over many of the above-mentioned points. Another concern is the overly complex nature of the approach as compared to other recent federated GAN approaches which raises the question as to whether the actual improvements warrant the complexity of the proposed approach. From the report experiments, the improvements appear to be rather slim.\n\nConsidering these aspects, unfortunately, the paper in its current shape does not seem ready for acceptance. The authors are advised to consider the feedback from the reviewers which will strengthen the submission for a future submission."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper aims to construct a GAN that can be applied to non-i.i.d. federated data. To achieve this aim, the authors propose an extension of Bayesian GAN called expectation propagation prior GAN (EP-GAN), which obtains a partition-invariant prior using expectation propagation. In particular, the authors introduce a closed-form solution for efficiency. The effectiveness of the proposed method is demonstrated using non-i.i.d data, including the toy data, image data, and speech data.",
            "main_review": "[Strengths]\n1. As far as I know, applying a GAN to non-i.i.d federated data is not actively studied in the previous studies and is an interesting research topic.\n\n2. The proposed method is solid and mathematically grounded. The detailed derivatives are also provided in the supplementary materials.\n\n3. The effectiveness of the proposed method is demonstrated using various data, including toy data, image data, and speech data.\n\n[Weaknesses]\n1. Improving the calculation speed of EP is included in the main contribution. However, its validity is not empirically demonstrated. At the bottom of Section 4.1, the difference in the calculation order is discussed; however, I cannot understand how significant the difference is in practice. In particular, I am curious whether the calculation in EP is dominant in the total framework, including the GAN training.\n\n2. It seems that in the baseline models, simple parameter averaging is applied to all the layers when FedAvg is used. However, it can be easily improved by introducing client-specific parameters using a conditional module (e.g., conditional batch normalization), which is used in a typical conditional GAN. The comparison with such a baseline is interesting.\n\n3. In practice, it is assumed that the number of clients is considerably large. However, in the experiments, the number of clients is relatively small (the order of 10). Therefore, in the current manuscript, I consider that the effectiveness in a practical setting is not sufficiently demonstrated, although some benchmark performance is provided.\n\n4. In Appendix F.3.4, a statistical significance test on the inception score is provided; however, a statistical significance test on the FID is not presented in the current manuscript. If the authors intend to emphasize the utility of Mixture-EP-ProbGAN, this test also should be conducted.",
            "summary_of_the_review": "This paper addresses an interesting problem and proposes a reliable method that is mathematically grounded. However, I still have some questions regarding the experimental evaluation. I expect that the authors clarify them in the rebuttal.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a method for learning Generated Adversarial Networks (GANs) for non-i.i.d data in a federated learning setting. This is accomplished with the use of a partition-aware prior via an Expectation Propagation (EP) algorithm embedded into a Bayesian GAN setting. Additionally, it proposes a closed-form solution for EP updates aiding efficient federated learning. The claims are substantiated with experiments on both synthetic and real data. \n\n\n\n \n\n\n\n",
            "main_review": "The paper is well written and the use of EP prior to this purpose is a fine idea. However, I would like the Authors to address the following questions. \n\n1. While Figure 1 is a good way to motivate the problem, it would be good to supplement all figures with some quantifiable metrics (at least for a few ones). It is difficult otherwise to ascertain the gains. \n\n2. In the Related work section, the two large paragraphs on Federated Learning and Bayesian GANs seem to be disconnected. It would be good to have a connecting paragraph would be good. \n\n3. The small paragraph on Federated Learning and deferring it to the appendix is not a good idea. Some details on Fed-Learning here will be apt. \n\n4. I am not an expert on EP but I am curious - why can't the approximated factor come from a non-exponential family?\n\n5. Related to the previous question, why are Gaussian distributions used for approximate factors? What bearing does this have on the entire method? \n\n6. I didn't quite follow the need for a sigmoid in Eq. 5.\n\n7. Is this method generic enough to be applied for other Bayesian GAN settings beyond the ones considered here?\n\n8. The other related question is, can this EP-prior help in the case of i.i.d data as well?\n\n9. The other important question I have is - It is not clear from the paper why should the proposed method aid in handing i.i.d data in the federated learning setting. There is empirical evidence but can there be a more principled way of describing the same?\n\n10. It would be good to include an i.i.d case too in Table 1, if possible. \n\n11. More baselines could be added for the speech experiment. In addition, the task is not well defined in section 5.3\n\n\n",
            "summary_of_the_review": "This paper addresses a very useful problem of federated GAN learning for non-i.i.d data. It is moderately novel to be considered for publication in ICLR. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The goal of this paper is to train a Bayesian GAN on non-i.i.d. federated data. Specifically, the authors propose to adopt the newly-introduced expectation propagation (EP) prior, being partition-invariant, to address the non-i.i.d. federated challenge. Experiments on synthetic and real datasets are conducted. \n\n\n\n",
            "main_review": "The writing should be improved significantly; the current manuscript is quite challenging to understand what's really going on.\n\nIn the Introduction, how and where do you ``identify the mode collapse problem of Bayesian GANs under non-i.i.d. cross-silo unsupervised\nlearning scenarios?''\n\nWhat's the novelty over existing methods for training GAN under federated learning settings? e.g., [1-2]\n\nIn the paragraph before Eq 1, $q(\\boldsymbol \\theta)$ is not a distribution, i.e., it's not normalized, right? Similar questions for the following  $q_{ep}^{t}(\\boldsymbol \\theta_G)$.\n\nIn Eq 5, how will you train the auxiliary neural network $f$ in federated learning settings? \n\nAlso In Eq 5, it seems one should specify an $f$-function for each $\\theta$ parameter of the GAN generator? How expensive is the proposed method, both in space and time?\n\nThe notations starting from Eq 5 are quite confusing. Eqs 6-7 are not easy to follow. \n\nIn the paragraph following Eq 14, I cannot see clearly why ``Theorem 4.1 shows that we are able to analytically approximate the prior of the global data distribution with the datasets stored on different clients while following the cross-silo federated learning settings?''\n\nThe notation $J_G$ is not defined in Eq 16.\n\n[1] FedGAN: Federated Generative Adversarial Networks for Distributed Data\n[2] Training Federated GANs with Theoretical Guarantees: A Universal Aggregation Approach",
            "summary_of_the_review": "The writing should be improved significantly; the current manuscript is quite challenging to understand.\nThe contents in Section 4.1, i.e., the main novelties, are challenging to follow.  \nImportant comparisons with existing federated-GAN training methods are believed missing.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors targeted federated generative modelling in an unsupervised setting. Specifically, the work is built on top of Bayesian GANs. In order to aggregate the information from different clients, the authors proposed to use expectation propagation (EP). It makes sense. Despite being a well established Bayesian inference algorithm, EP operating on the neural network parameters can suffer from intractability. The authors presented a low complexity solution. The experiment results showed improved FID and IS over multiple baseline methods. However, the overall performance is quite poor on the rather simple dataset CIFAR10, owing to the scalability issue of Bayesian models.",
            "main_review": "Federated learning with non-i.i.d. data partitions is particular challenging, as naive averaging does not work well. EP offers an information aggregation framework which can deal with different data partition styles in a unified way. In order to apply EP, it requires to have Bayesian GAN as the basis model at each client. While Bayesian inference is a powerful framework, the scalability issue of Bayesian GANs can potentially hinder the use of EP-GAN. For instance, the oracle baseline model still has FID on CIFAR10 above 25 while the best performing GAN on CIFAR10 is below 6. Moreover, the gap can potentially become even larger when the resolution is higher.\n\nOn the algorithm side, eq. (5) lacks a justification on why it suffices the quality of likelihood modelling besides being simple and thus permitting the closed-form EP update. For EP-ProbGAN, how does the newly introduced EP prior affect the guarantee claimed by ProbGAN.\n\nIn the experiment part, despite being introduced in the text, the performance of the baseline model BayesGAN (2) was actually not reported in Table 1. Furthermore, as reported by the authors of ProbGAN, the NS loss outperformed Wasserstein distance and LS for both ProbGAN and Bayesian GAN. However, the Table 1 did not consider the top performing case. Also, only having one natural image dataset is probably not enough, e.g., ProbGAN also considered STL-10 and ImageNet. Furthermore, EP-GAN variants on i.i.d. N=2 outperform oracle. What is the potential reason behind?",
            "summary_of_the_review": "Overall, the problem is trending, challenging and highly relevant. Within the Bayesian framework, the use of EP for federated learning is reasonable. Of course, complexity remains as a critical issue. The authors proposed some low-complexity solution and empirically showed the benefits of using EP over existing schemes, which however are not really developed for non-i.i.d. scenarios. Furthermore, the baseline models do not take the top performance configuration, which lead to my general concern on how strong the baselines are. On the algorithm side, Bayesian models need to deal with priors regardless of EP. Therefore, having or adding an EP prior in Bayesian models seems to be straightforward. The closed-form update is definitely interesting, but the authors shall (empirically) analyse its fidelity.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}