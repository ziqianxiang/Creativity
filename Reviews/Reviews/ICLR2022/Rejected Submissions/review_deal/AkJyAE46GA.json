{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper shows that active learning is an emergent property of pre-trained models. They show that simple uncertainty sampling improves sample efficiency by 6 times (up to 6x fewer samples for the same accuracy). This is an interesting and important observation that has practical implications. \n\nInitially, there were various concerns regarding the message of the paper, including the tile and use of uncertainty function in AL and lack of enough experiments that were addressed through rebuttal period. \n\nHowever, there are still remaining concerns that lead to the paper not being ready for publication. Namely, \n  (1) Clear discussion on how much of the gains are due to active learning vs pre-training with respect to different cases. it is also worth investigating additional causes for the failure cases.  \n  (2) there are many observations here without a clear narrative or theory.\nMoreover, making the story more cohesive will strengthen the paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper investigates if using large, pretrained models in an active learning setup helps achieve better performance with lesser data when compared to using randomly sampled data. In order to conduct this investigation the authors study the empirical performance of large pre-trained models on some image datasets and a text dataset. In both cases large pre-trained model is finetuned on a small amount of seed data and then an active learning procedure is used (in this paper the AL procedure is an uncertainty sampling procedure) to collect more data. The datasets are chosen to illustrate several conceptual issues (i) distinguishing causal from spurious correlations (ii) measuring robustness to distribution shifts (iii) role of data imbalance. \n\nExperiments are performed to show that using an active learning procedure indeed helps improve performance using only a small amount of actively labeled training dataset.  The paper is well written and the results are convincing and insightful. ",
            "main_review": "I wanted to see two additional directions of investigations.\n1. First, does the choice of active learning heuristic matter? My intuition is that it probably does not, but it would be nice to see a comparison between uncertainity sampling heuristic and another information seeking heuristic such as entropy sampling. The direction of investigating other active learning techniques (such as query-by-committe or expected model change) also remain open.\n\n2. All the pre-trained models considered are supervised pre-trained models. What happens if we use a large, unsupervised pre-trained model (for example a masked language language model)? Do the same results hold true?\n",
            "summary_of_the_review": "I like the key question and the experimental setup of this paper and would recommend an accept. It would have been better had the authors considered another AL method, along with uncertainty sampling to investigate the relative impact of pre-training and the active learning methodology in data savings. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors describe interesting empirical observations regarding using uncertainty sampling to select examples to fine-tune models that use pretrained embeddings and provide some hypotheses regarding the reasons for these performance improvements. Specifically, (1) from a methodological perspective, they propose using uncertainty sampling (i.e., least confident selection) to select examples for fine-tuning image/NLP pretrained models and (2) from an empirical perspective, they use Waterbirds/Treeperson/iWildCam2020-WILDS for image classification and Amazon-WILDS for review star prediction based on text and compare with random sampling — noting that these are settings where there is known covariate shift between train/test with semantic meaning to induce interpretable spurious associations (e.g., background in images). The proposed method works overall, especially on the image datasets, and they also dig into the types of examples selected — noting that they align with expected ‘difficult’ examples (depending on the setting).",
            "main_review": "To begin with, these are interesting observations and likely have practical implications. The strength of the paper are the empirical reductions in sample complexity and increase in performance. However, in my assessment, these results are fairly preliminary. While in two domains, the image classification results are clearly stronger than the text classification results (but no real explanation for the reason — I guess it is that the ‘task ambiguity’ is consistent from train to test (?)). If this is the hypothesis, it seems that one could perform a ‘dose-response’ like analysis of background shift prevalence between train/test and characterize the strength of this relationship, which seems important as this is the primary justification of why AL with pretrained models is more powerful. I have a potentially simpler hypothesis, that these improved representations remove a lot of the cold-start problems with AL that have previously been mitigated with pre-clustering, careful seed selection, etc. Basically, the AL selection can focus on fitting the conditional distribution associated with classification and not be concerned with the joint distribution of cold-start problems. The compressed representation has lower variance within coordinates and selecting ‘difficult’ examples is more likely to be fruitful (i.e., pathological cases are mitigated). Of course, I am just speculating and my hypothesis is neither orthogonal nor contradictory to the one proposed — but there is no careful empirical analysis or any analytical analysis to justify the proposed hypothesis either. \n\nOn a more practical level, I would also expect more AL querying functions as this is largely an empirical papers and I see no reason why uncertainty sampling is guaranteed to be superior (although it is intuitively appealing that pathological cases probably disappear). \n\nFrom a scholarly perspective, there has been recent related work at least in the NLP space (which I am more familiar with):\n[Yuan, Lin & Boyd-Graber, Cold-start Active Learning through Self-supervised Language Modeling, EMNLP20]\n[Margatina, Barrault & Aletras, Bayesian Active Learning with Pretrained Language Models, 2021]\n[Shelmanov, et al., Active Learning for Sequence Tagging with Deep Pre-trained Models and Bayesian Uncertainty Estimates, EACL21]\nThere are more, but this gives some good seeds to follow-up on.\n\nOverall, the writing is clear, but I would recommend a few things: (1) put an algorithm somehwere to verify if you are fine-tuning the embeddings or the overall model and bring a bit more of the model details into the paper from the appendices and (2) change the title as it is misleading; something like “Pretrained Models Make Good Active Learners”. \n\nIn summary, the preliminary observations are interesting and notable in some cases. There are enough ‘additional’ experiments to demonstrate that the authors have a promising path to a theory. However, to be a more impactful finding, I lean toward better contextualization, more experiments that vary the active learning querying function, and more empirical/analytical results to support a clear theory. Interesting, but I don’t believe ready for acceptance in ICLR.\n",
            "summary_of_the_review": "The authors make some interesting and potentially impressive observations regarding the performance of ‘simple’ active learning (i.e., uncertainty sampling) in the context of pre-trained models — showing positive results both on image and text classification problems. Additionally, they show that biasing toward difficult examples for selection may be correlated with being able to ignore spurious features (e.g., background in images). However, the experiments are limited to ‘least confidence’ uncertainty sampling, there is non-negligible missing contextualization wrt related work, and there isn’t a strong justification for the observed performance improvements. Thus, my assessment is that this submission should be rejected in its current form.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors set out to investigate if active learning is an emergent property of pre-training. That is if running active learning with pre-trained models gives better result than using the same models without pre-training. They run several experiments on different text and image datasets first showing that active learning performs better than random sampling on pre-trained models and secondly that pre-trained models perform better than un-pretrained ones for active learning.",
            "main_review": "I find the paper and the direction to be interesting and I think the paper is well written and well motivated. However, a major problem the paper faces is that there is not enough experiments in the paper to validate the claims made by the authors.\n\nIn the experiments, the authors show the results of pre-training on all the datasets they used and it is clear that active learning performs better than random sampling however on experiments comparing pre-trained models to un-pretrained ones, the authors only show experiments on two datasets. They report a failure case for one dataset in which the model performance is not distinguishable for neither pre-trained or un-pretrained models. \n\nFocusing on the two experiments, in Figure 8, active learning actually does perform better than random sampling even if the relative performance on the experiment is not as good as that of the pre-trained models. In addition, from the plots active learning performance is indistinguishable to random performance on Pretrained BiT-S-R50x1 and only gets marginally better on the other pre-trained models as more examples are added. \nOn both Figure 7 and 8, we observe that random sampling on pre-trained models outperforms active learning on the un-pretrained counterpart. A possible explanation is that the model has only seen few examples to learn good features that are representatives of the class. This explains why pre-training does better since it is trained on a large pool of unlabeled data and fine-tuned on the specific task hence it learns richer/better feature for model training.\n\nGiven the above limitations, it is hard to verify the main claims of the paper. Additional experiments on different datasets including text data will be needed. Also, it will be helpful to conduct experiments in settings where the un-pretrained models performs reasonably well on the task when trained with just the seed labels. Lastly, it is important to consider different architecture models for the pre-trained models and not just variants of the BiT model.\n\nMinor suggestion:\nIn low-shot settings, entropy sampling may be a better query strategy for the un-pretrained model.",
            "summary_of_the_review": "It's an interesting paper but the authors need more empirical validation to establish some of the claims. See detailed comments in main review.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper investigates the active learning performance of pre-trained models vs their non-pre-trained counterparts on both vision and NLP tasks. Specifically, the investigation focuses on datasets with spurious correlation, domain shift, and label imbalance. Empirical results generally show that the pre-trained models with the uncertainty acquisition function performs much better than the random baseline and their un-pre-trained counterparts. ",
            "main_review": "Strengths: \n\nThis paper is clearly written. It describes experiment setup and results very clearly. The advantage of pretraining is also clearly established. \n\nWeaknesses: \n\nFor a purely empirical paper (which is totally fine for me), that the depth of investigation is a bit lacking. I would like to see more comprehensive results to be convinced that the performance gap is indeed due to pre-training vs. no pre-training, instead of certain hyper-parameter choices happen to play well with the pre-trained models. Some specific items that I would encourage the authors to provde are: \n\n1. More acquisition functions should be studied. While uncertainty-based acquisition function is perhaps the most popular, many others have been recently proposed. For a recent list, see the related work section of [1]. \n\n2. It is not clear which part of the pre-training is most helpful for active learning. It is known that pre-trained models are very efficient at learning from few examples, but they seem to struggle here, especially in Fig. 2, where the random baseline makes very small progress even with hundreds of data points (which seems to be rare for pre-trained models). I suspect that this is at least partly due to the issue below, i.e. the mismatch between training and test distribution, so some clarification would be appreciated. \n\nIn addition, there seems to be a disconnect between the setup described in Fig. 1 and the actual experiments conducted. In the Fig. 1 setup, the existing (small) training set is not sufficient to disambiguate among a set of candidate functions, so the promise of active learning is to find the maximally confusing data point from the unlabeled set and thus achieve efficient disambiguation. Notably, the training and test distributions do not *need* to be different. However, in the experiments, in all three settings, the training and test distributions are intentionally different, so that the models that have undesirable working mechanisms, e.g. relying on spurious correlation, are \"defined\" to be bad. Is this a characterization of the datasets used? If so, it seems that here, active learning with pre-trained models are better not in the typical sense of improving the performance on the training task, but instead helping the model to gain some other behaviors. \n\nInterestingly, [1] shows that the optimal acquisition strategy (w.r.t. the training task) seems to sample quite homogeneously in the input and output space, and a uniformity regularization would actually improve existing acquisition functions. By comparison, Fig. 6 show strong mismatch between the acquired sample distribution and the data distribution. Given that the paper evaluates the \"other behavior\" performance, if my understanding in the paragraph above is correct, I do not find it surprising. Nevertheless, I would like to encourage the authors to experiment with the uniformity regularization (Sec. 6.5 of [1]) and discuss any findings and relationships. \n\n[1] Towards Understanding the Behaviors of Optimal Deep Active Learning Algorithms, AISTATS 2021\n\nMinor issues: \n\n1. Images should be produced as PDF files and embedded into the paper. This ensures that the image is saved in the vector format, rather than bitmap. For matplotlib, this is as easy as changing the suffix from png/jpg to pdf, i.e. `plt.savefig('figure.pdf')`. \n\n2. The texts on most figures are too small. In general, they should approximate match the text size, or at least the footnote size, but some of them are barely readable when printed out. \n\n3. A reference to [2] is recommended in footnote 1 when discussing calibration, since it is the original paper that investigates the confidence calibration problem of neural networks. \n\n[2] On Calibration of Modern Neural Networks, ICML 2017. ",
            "summary_of_the_review": "Overall, I think this paper is on track to make a good contribution, but would need a more rounded-out experimental execution. Therefore, I am giving a reject recommendation for now, but I'd be happy to increase my score if the authors provide the requested revision. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}