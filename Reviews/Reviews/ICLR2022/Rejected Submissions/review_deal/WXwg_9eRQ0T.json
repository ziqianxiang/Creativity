{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper is a fair effort, making some headway on a problem of practical importance. \nThere was some discussion of scoping and whether the contribution was Machine-Learning-y enough. \nI'm kind of ambivalent on that particular question: I think the general rule is that the further out-of-scope the paper seems, the better the results need to be for people to overlook it. \nI think in this case, unfortunately, even the two most positive reviewers did not evince enough excitement about this paper for it to get accepted in light of the scoping concerns. \nGiven the various constraints involved, I don't think I can recommend acceptance.\n\nIn order to get it accepted into a future conference I would recommend either:\na) Submit to a more Software-Engineering focused venue\nb) Really shore up the evaluation such that the reviewers sympathetic to this kind of paper will find it unimpeachable and score it more generously."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper addresses merge-conflict resolution in source code repositories. Whereas prior neural work (DeepMerge) can only do line-level resolutions (i.e., predict a resolution that chooses some sequence of entire lines from the original file version or the two divergent branches), this work refines the task further towards token level resolutions, thereby covering more interesting cases. Whereas DeepMerge cast the task as a pointer-sequence prediction (predicted a sequence of lines from the input), MergeBERT casts the task as a classification problem: for each conflict instance (some token-sequence conflict), one of 9 recipes are chosen to merge (e.g., just the original tokens, just one of the branches, some concatenation of the three, and some substitution of tokens in the original by concatenations). Similarly to DeepMerge, the input is encoded as a combination of aligned token sequences of the original snapshots and the divergent branches, as well as an encoding of the token-wise edit \"script\" for the pairwise diffs (same, inserted, deleted, replaced). But instead of encoding this edit sequence explicitly, MergeBERT uses it as a token type embedding in a Transformer encoder, along with a positional embedding and the token embedding itself.\n\nThe Transformer encoder is pre-trained with the masked language model objective, before fine-tuning on merge-conflict resolutions. The resulting tool is shown to resolve more merge conflicts than prior work, and also to generalize to a language unseen during pre-training or fine-tuning.",
            "main_review": "This is an important problem, and I'm excited to see more work in finding a neural solution to it.\n\nHowever, this submission seems more like a refinement of DeepMerge than a novel contribution on its own. The data representation for conflicts is a generalization from DeepMerge, and the model architecture is a straighforward form of a Transformer-based classifier. However, the limited novelty might be offset by the positive progress on this important problem, modulo a number of questions below.\n\nI wonder if there's room to incorporate the strategies of syntactic or lexical merging employed by FSTMerge and its variants and other non-neural baselines into the actions predicted by something like MergeBERT. It seems excessive to throw out the engineering that went into such non-neural tools, if they can be used for something more sophisticated than o, a, b, oa, ob, etc. Q1: I'd be curious if you think the strategies in those earlier tools are incompatible with an encoder like your Transformer-based encoder, too coarse grained, or perhaps just too complicated.\n\nThat said, I like the data-driven design of the task (e.g., the merge resolution \"recipes\" described in section 4). I would have liked to see where the 9 edit patterns came from though (presumably some analysis of real conflict resolutions?), since otherwise they appear arbitrary. Q2: are your classification labels arbitrary? Did you choose them by analyzing actual merge-conflict resolutions?\n\n## Non-conflicting token edits\n\nMy biggest concern with your work is that you seem to treat token conflicts as independent of each other, but of course they are most certainly not, in general. For example, on page 3, with respect to Figure 1, I'd be careful about calling a token with no direct conflict a \"non-conflicting edit\" (e.g., the statement on `var` versus `let`). Consider the following diff:\n```\n<<<<<<<< A\na = func2(y, 10)\n|||||||| O\na = func2(y, 9)\n========\na = func3(y, 10, 12)\n>>>>>>>> B\n```\nwhere `func2` is a function with arity 2, and `func3` is a function with arity 3. Here, although the `func2` to `func3` change on `B` is \"non-conflicting\", it is not independent of the change from `9)` to `10)` or `10, 12)`, since one keeps the arity at 2 and the other changes it to 3. Perhaps a model can learn these interdependencies but your model does not. I'd caution you against calling independent token changes \"non-conflicting\". Maybe there's a better motivating example than this one, or again, perhaps there's some analysis of real conflict resolutions showing that real conflicts are indeed token-independent.\n\n## Evaluation\n\nI don't quite understand your definition of precision and recall. Is precision perfect accuracy modulo whitespace per conflict region? What's recall? Why is it not 100%? Do you have a threshold on classification probability you're using that's causing you to lose some examples? It seems you have one interpretation of recall for diff3 (it cannot resolve the conflict) and possibly another for MergeBERT? Q3: Please explain your metrics with specificity.\n\nIt's not obvious how BLEU-4 is a relevant metric. In cases where token-level conflict resolution kicks in, A or B or even O probably share many n-grams with the resolution. What kind of solution does BLEU-4 allow you to score positively that would be genuinely acceptable for this task? An example would be helpful here. Q4: Please explain why BLEU-4 is a good metric for this task, perhaps with an example.\n\nThe zero-shot generalization to Scala is puzzling. Presumably the value proposition here is to argue that you don't have to pre-train for every language, and a single strong (potentially multilingual) model can do the job. But then I would want to see the comparison of pre-training/fine-tuning on Scala versus the zero-shot version with neither pre-training nor fine-tuning. In the absence of that, it's not clear what the headroom is for this language. Is the result good? Is it bad? Maybe pre-training and fine-tuning on Scala gives super high F1. Q5: How does MergeBERT do if you pre-train on Scala and fine-tune on Scala, and how does that compare to your zero-shot results?\n\nI don't understand the point of Section 8.2. You already do MLM pre-training on your unilingual and multilingual pre-training datasets. What does this section add to what you already do? Q6: Please explain what research question Section 8.2 is answering, and what the answer is.\n\n",
            "summary_of_the_review": "Important problem, but there are many unanswered questions making the submission feel incomplete.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents MergeBERT, a deep model for resolving program merge conflicts in software development. The authors introduce a new, hierarchical differencing and cast the problem as classifying over a fixed set of merge patterns, instead of generating . The model is pre-trained on a large corpus of GitHub code.",
            "main_review": "Pros:\n\n+ Hierarchical two-level differencing appears to be an intuitive and good idea.\n+ Casting the problem as classification over a set of merge patterns appears advantageous in both learning and computation, since most of the time the resolution comes from either of the two change versions.\n+ Pre-training can be applied directly.\n\nCons:\n\n- This idea is somewhat incremental, largely based on the work of Dinella et al., 2020.\n- MergeBERT is not as good as *diff3* in terms of precision, which is to me a more important measure. \n- Some parts of the techniques and experiments are not clear, for example the class label and data split. I have more detailed questions below.\n\n\nQuestions:\n\n1. It is somewhat unintuitive that `let` is not part of the token-level conflict. I understand that `var` is not changed in the version B, but it is important from both A and B perspectives that there is a change that needs an agreement in resolution. In that case, I don’t think merging `let` right away is an indisputable decision.\n\n2. For this change from `--num_cores=2` to `--max_length=256 --num_cores=2`, what is the result of the two-level diff?\n\n3. Is treating conflicting regions independently in Eq. (2) is oversimplified?\n\n4. How do you split the train and test dataset (Table 4)? Is there a validation dataset?\n\n5. How do you deal with conflicts from more than 2 versions?\n\n",
            "summary_of_the_review": "Overall, the paper presents several new technical contributions and insights, but some parts are not entirely convincing.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper is about using BERT for the automatic resolution of merge conflicts. The main idea is to cast the problem of automatic merge resolution as a classification problem with 9 classes (token-level merge patterns). The technique use token-level differencing to represent the input to the merge problem as four aligned token sequences, together with two edit-type embeddings. The input sequences are fed to BERT and the results are aggregated and used for classification. \nThe technique is compared to state-of-the-art automatic-merge tools and is shown to outperform them in all benchmarks. \n",
            "main_review": "Pros:\n+ This is a compelling task, merge conflicts are an important problem\n+ Reducing the problem to using a classifier is valuable practically \n+ The nine primitive merge resolution patterns covering 99% of merge cases is an interesting observation\n\nCons:\n+ The encoding is token-based and rather standard\n+ Missing ablation study\n+ Experimental evaluation partial with respect to most important baseline \n\n* Tackling the merge conflicts problem is interesting. Using a learned model to decide how to resolve it is interesting as well (but not novel), since, it’s not always clear what is the best resolution (which is consistent with the context).\n\n* It’s not clear how the model looks like, Do you train BERT only for word vectors? If so, what about exploiting the context? Is the encoder of the aligned sequences the pre-trained BERT? It looks like you are not finetuning it (based on the blue squares of Figure 2, showing that only the Edit Type Embeddings are finetuned), so how should it handle the new kind of inputs? (token embedding + type embedding + position embeddings)\n\n* How important is the edit type embedding used in finetuning? The paper refers to Figure 5 in the appendix that shows how these are used, but not how important are they for the final result. In general, the paper would benefit from a more extensive ablation study. \n\n* The comparison with DeepMerge includes cases where DeepMerge cannot produce a prediction, as these cases are not line-level merges. This is not an apples-to-apples comparison. What would happen if we compare DeepMerge/MergeBert considering only line-level merges?\n\n* Comparisons to JDime and jsFSTMerge are useful for showing the superiority of your neural approach, but I am more interested in understanding what makes your approach work well.\n\n* Table 3 is pretty much due to properties of BERT and not of MergeBERT, right? Similar experience reported in CodeBERT?\n\n* Primitive Merge Resolution Types: Appendix 10.1 seems to emphasize this, but I do have some comments:\n\"Our analysis shows that over 99% of all the merge conflicts can be represented using these labels\"\nBut FIgure 3 (left) says that you failed to label 21.64% of the TypeScript examples. Can you please elaborate on this point?\nIt’s not clear how the colors in Figure 3 are related to your 9 classes. Does “Remove base” correspond to classes 6-9?, is so, what does pink (“Other”) refer to?\nPlease align the legends on both sides of Figure 3.\nI would emphasize that there are almost 21.64% non-trivial examples (in TypeScript), which the token-level diff method can solve by using a classification approach.\n\n* It would be helpful to add an encoder-decoder baseline - given (A, B, O), generate the resolution.\n\n* Table 1- what is the meaning of the diff3 row? I guess it’s for the examples where the token granularity diff managed to solve conflicts the line-diff couldn’t. Please clarify that. \n\n* Table 1 - what about using BERTMerge with line diff and DeepMerge with token diff? It’s essential for a fair comparison.\n\n* Section 8.2: The impact of pretraining is not disputed, but again it is not a contribution of the technique presented here. \n\nMinor: \n\n* Figure 2: should all edit operations in the blue squares be <->? \n\n* Figure 7(b) needs to be corrected: `data +=` is also a conflict, and there is a piece of code after the last conflict - ``});`.\n",
            "summary_of_the_review": "The problem of automatic merge resolution is compelling. The authors present a solution that is based on token-level diff (no ML contribution), identify 9 merge patterns (no ML contribution), and finally use BERT with minor adaptations to classify which merge pattern should be used. For this model, some details of what exactly is being fine-tuned are not clear. \n\nThe suggested approach shows improvement compared to existing baselines. \n\nThe experimental evaluation is hard to follow because neither of the existing baselines addresses the exact same kind of merges. The closest baseline in terms of technique is DeepMerge, and the comparison to that baseline is quite partial. \n\nOverall, this may be a great software-engineering contribution, but I'm afraid that there's not a lot here for ICLR.\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}