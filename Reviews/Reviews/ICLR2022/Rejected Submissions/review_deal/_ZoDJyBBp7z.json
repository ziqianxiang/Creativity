{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The submission proposes \"feature flow regularization\" during training to enforce (approximately) sparse network weights which can then be post-hoc pruned.  The form of the regularizer is reasonably well motivated, and the method seems interesting.  The reviewers were split on this, with two recommendations for \"marginally above\" and one for \"marginally below\" the threshold of acceptance.  I therefore read the paper in detail, in addition to reading the reviews, rebuttal, and private reviewer comments.  The appendix on the sparsity-accuracy tradeoff and its relationship to the hyperparameters k1 and k2 is an interesting experiment, and overall the authors were very engaged in the reviewing process.\n\nIn an initial reading, the term trajectory is indeed vague, although it is presented as a definition.  This ambiguity is reflected in the reviewer discussion where in response to Reviewer wyu7, the authors indicate that there are two different meanings of the word in different papers that are being confused.  In a mature presentation, these definitions should probably be given mathematically early on in a formal definition box, but this would require significantly tightening up the mathematical notation early on.\n\nThe results table does not show that the proposed method Pareto dominates other methods (accuracy, sparsity, and latency), which themselves are necessarily limited due to the very high number of published papers on network pruning.  Furthermore, some of the selected comparisons appear to be optimizing for different metrics rather than network sparsity, e.g. DCP reports better accuracy for VGG-16 after pruning is applied.  This indicates that the proposed method is somehow in the crowd, but does not seem to show a clear consistent improvement over SOTA.\n\nAnalysis in Appendix A.3 does not really depend on which kind of norm is used - the same conclusion will be reached that ||X|| decreases, while structured sparsity, e.g. with expected sparsity rates, is dependent on the kind of norm.  As such, it's OK, but not a particularly specific result.\n\nOn the whole, this indicates that the paper is interesting, but borderline with room for concrete improvements that go beyond the scope of a simple refinement for a camera ready presentation."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a new method, called feature flow regularization, for structured sparsity and pruning in deep neural networks. In the proposed method, the length and curvature of trajectories connecting the features of adjacent hidden layers are penalized which is claimed to implicitly result in structured sparsity. Experiments compare the method with several state-of-the-art methods on two datasets and architectures.",
            "main_review": "The method seems to be original for structured sparsity and pruning and is rather simple. The experimental comparison is with SOTA methods, and the results show that its performance is reasonable.\n\nThe paper provides some analysis for the connection of the proposed regularization and sparsity but unfortunately the analysis is not really rigorous. Moreover, there is no discussion and analysis for the curvature term. Even in the experiments, there is no ablation study to dissect the effect of the two regularization terms. Although the presentation of the method and Eqs. 5 and 7 include separate hyperparameters for the two terms, there is no experiment in the paper to investigate unequal values for these hyperparameters.\n\nThe experiments are a bit limited, as they are only on two datasets and two architectures. While the results on CIFAR-10 are somewhat encouraging, overall, the results do not seem strongly in favor of the proposed method, especially on ImageNet.\n\n Also, one disadvantage of the method is that a user cannot select a desired pruning or sparsity ratio, and the sparsity is indirectly controlled by hyperparameters which are not adequately investigated in the paper. \n",
            "summary_of_the_review": "The proposed method seems original, but in my view the theoretical analysis and experimental support are not strong enough and need improvement. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper deals with network pruning from a fresh perspective. It essentially argues that networks that implement simple mappings are more amenable to pruning. The authors support this claim with experimental evidence. Personally I think this idea is intellectually appealing, and may have uses beyond the network pruning setting. Some concerns related to prior art linger.",
            "main_review": "I like the basic premise that encouraging simple mappings in networks 1) doesn't seem to hurt and 2) seems to lead to highly prunable models. I'm not super familiar with the latest developments in the crowded pruning literature and will have to rely on other reviewers on that. \n\nI want to underline a connection to some seemingly unrelated topics. The present idea seems to extend an earlier, frequently-reinvented idea that a network's output should change only a little when the input changes a little. This was probably first coined by Drucker and LeCun as \"double backpropagation\" [1], and has been heavily used in adversarial learning (input regularization [2]) and GAN training (R1 regularization [3]) in recent years. This was furthermore explicitly relied on in StyleGAN2 [4] for trying to guarantee a simple, well-behaved generator mapping between a latent space and output images (path length regularization). Now, I think your method goes further than any of these by regularizing entire paths through the network instead of just input --> output, thus potentially offering further upside in many applications. That seems like an exciting prospect.\n\nThat said, I think the authors need to contrast the method with \"trajectory regularization\" [5], which was put forward in yet another context, but would seem to do a closely related thing by encouraging simpler representations by regularizing transitions inside a network. It will be important to draw the connection to assess how far beyond pruning the novelty extends.\n\nIn my opinion Fig 1 should come sooner than page 4. It is useful mainly as a high-level intuition builder. Figures 3 & 4 are good. Typo: \"Strucuted sparsity\" (Fig 4).\n\nI did not check all the equations.\n\n\n[1] Improving generalization performance using double backpropagation, IEEE Trans. Neural Networks, 3(6):991–997, 1992.\n\n[2] Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients, https://arxiv.org/abs/1711.09404\n\n[3] Which training methods for GANs do actually converge?, https://arxiv.org/abs/1801.04406\n\n[4] Analyzing and Improving the Image Quality of StyleGAN, https://arxiv.org/abs/1912.04958\n\n[5] On the Expressive Power of Deep Neural Networks, https://arxiv.org/abs/1606.05336",
            "summary_of_the_review": "An intellectually stimulating take on network pruning, with possible applications elsewhere.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a regularization strategy for learning sparse deep CNN models. The Feature Flow Regularization (FFR), penalizes first and second order changes in the intermediate features between consecutive layers of the network. The intention of this regularization is to  smooth the evolution of features and make sparse weights. ",
            "main_review": "Strengths:  \n1. The feature flow regularization method is inspiring.  The feature flow regularization method (FFR) is simple and effective, and create smooth  consecutive features.   \n2. Authors provides detailed analysis in Sec.4 to show the method is valid.   \n  \nWeaknesses:   \n1. The curvature of two-order regularization needs more illustration. Why we need this item to create sparisty? Besides, ablation studies on the role of length and curvature are absent.    \n2. As we all know, Convolutional networks were inspired by biological processes. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. At the bottom level of CNN, the features are basically similar,  such as the edges. The higher, the more features of objects can be extracted (wheels, eyes, etc.), the different advanced features are finally combined into corresponding features, allowing CNNs to accurately distinguish between different objects.   \nHowever, with constraints on contingous features and two-order features, the representation power of intermediate features will be reduced significantly. The feature maps of CNNs are used to explore the high-level distinctiveness of inputs. So, more explantations about the FFR will make this paper clear.  \n3. The results of experiments in Table1,2 have no advantage in contrast to other methods. Expecially in Table.2, the baseline of resnet-50 on Imagenet is 71.56\\%, which is significantly lower than 76.1\\% officially.  \"SCOP: Scientific Control for Reliable Neural Network Pruning\" in Neurips 2020 had competitive results, and constastiation should be provided.\n4. Table.1 and Table.2 is not consistency. Table.1 reports the error rate, but Table.2 presents the accuracy.",
            "summary_of_the_review": "The paper proposes a regularization loss that penalizes L1 distances between consecutive feature maps that are projected to the same dimension. This loss leads to smooth features and this characteristic is unclear to the target of classification.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}