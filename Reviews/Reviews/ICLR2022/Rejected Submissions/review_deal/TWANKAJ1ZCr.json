{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "While the reviewers agree that the paper contains interesting ideas and the method is elegant, it unfortunately does not meet the bar for acceptance. I strongly encourage the authors to revise their paper, in particular using the numerous comments made throughout the discussion phase; for example:\n\n* It is important that the authors polish their work, in particular for the updates provided (e.g. Figure 3, see EFwa)\n\n* Reviewers pointed the lack of updates on important claims by the authors (in particular the claim regarding clustering vs decision trees, see EFwa, the comments on the lack of diverse datasets, see meXP, )\n\n* Some answers might have gained in clarity, such as the reply to EFwa on the application and conclusions following Wilcoxon sign test."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes to tune the number of models in a boosting ensemble in an instance-wise fashion. The idea is to first cluster the samples using a decision tree and then to tune the size of the ensemble independently for each cluster, instead of doing it globally for all instances. An efficient two-level cross-validation procedure is designed to tune both the number of terms in each cluster and the number of clusters. Experiments are conducted on 6 large-scale problems that show that local pruning brings some improvement with respect to the more standard global pruning technique.\n",
            "main_review": "Adapting the ensemble size in an instance-wise fashion is an interesting problem. It is not new however, unlike what is claimed in the paper. This is addressed at least in these papers:\n\n[1] Hernández-Lobato D, Martínez-Muñoz G, Suárez A. Statistical instance-based pruning in ensembles of independent classifiers. IEEE Trans Pattern Anal Mach Intell. 2009 Feb;31(2):364-9.\n[2]  Dayvid V.R. Oliveira, George D.C. Cavalcanti, Robert Sabourin, Online pruning of base classifiers for Dynamic Ensemble Selection, Pattern Recognition, Volume 72, 2017.\n[3] V. Soto, S. García-Moratilla, G. Martínez-Muñoz, D. Hernández-Lobato and A. Suárez, \"A Double Pruning Scheme for Boosting Ensembles,\" in IEEE Transactions on Cybernetics, vol. 44, no. 12, pp. 2682-2695, Dec. 2014, doi: 10.1109/TCYB.2014.2313638.\n[4] Rafael M.O. Cruz, Robert Sabourin, George D.C. Cavalcanti, Dynamic classifier selection: Recent advances and perspectives, Information Fusion, Volume 41, 2018, Pages 195-216.\n\nThe method proposed in the present paper, based on clustering and cross-validation, seems however novel and significantly different from these works, which perform pruning in an online way, i.e. when a prediction needs to be done. In addition, the motivation of most of these works is also to reduce memory and computing times more than improving accuracy. I think the related work discussion should nevertheless includes these approaches.\n\nThe proposed method is very straightforward. It makes sense overall but I don't totally buy some of the arguments that are given in Section 4 to motivate it:\n- First, the discussion focuses only on the impact of pruning on predictive performance. While pruning can be very useful to reduce storage requirement or computing times at inference (and this is angle adapted in, e.g., [3] above), I'm not sure that it is that useful for improving predictive performance. Boosting has been shown to be quite robust with respect to the ensemble size (if the learning rate is small enough) and actually, experiments in Figure 1 and 2 confirm this fact, since most error curves are monotonically decreasing (I see only one curve that really significantly increases at some point). So, I was not expecting a priori a huge improvement in terms of predictive performance. \n- Second, some statements to motivate the idea of clustering are not really supported either by a theoretical or an empirical analysis (although I agree that they make sense intuitively). If you want to show that \"It is essential to preserve the initial geometry\" and to exploit the labels at the clustering stage, then you should provide experiments to show that not doing so is indeed detrimental (by using e.g. unsupervised clustering). \n- Third, the fact that the clustering is carried out without consideration of the boosting model makes the approach also suboptimal by design. There is no guarantee that the clustering will be optimal when it comes to tune the ensemble size in each cluster.\n\nThe idea of the cross-validation approach based on a single model (per fold) is sound and very relevant to reduce the computational cost of the approach. Note however that the cost is still important with respect to no pruning at all since it requires to grow k+1 boosting ensembles instead of 1. But I agree that there is a negligible overhead with respect to tuning globally the ensemble size.\n\nThe empirical validation is carried out correctly from a statistical point of view. I find however that the authors overemphasize the significance of the improvement they obtain with their approach. Looking at Table 2, their local pruning technique brings an improvement of less than 1% in terms of 0-1 loss on four problems out of six and on the other two problems, the difference remains very small. I'm not sure that such level of improvement is actually worth the effort if one is only interested in predictive performance. These results are also obtained with a single dataset split (if I understand correctly). Given how small the difference is, I think it would have been important to repeat the experiment several times with different splits to get standard deviations and maybe also to carry out a statistical test to check whether the improvements are statistically significant.\n\nI don't think either that the experiments provide a satisfactory answer to the first two questions asked in Section 5. As already discussed above, to me, Figures 1 and 2 show that one can not expect strong benefit from the clustering since most error curves are monotonically decreasing. Part of the important diversity in the optimal size in the clusters seems to be due to the fact that long flat regions are observed which lead to an instable position of the optimum. I would have like also a more systematic and quantitative experiment on all datasets to answer the second question about the relevance of the validation protocol. Why not compare the optimum found by this protocol with the \"theoretical\" optimum found on the test set?\n\nFinally, I'm surprised that the authors don't talk at all about the benefit of pruning on computing times at inference. To me, this is one of the main motivation for using pruning in the context of ensembles. I would be interested to know how global and cluster-based pruning compare from this point of view.\n\nMinor comments:\n- In 3.1, the following sentence is unclear: \"This method has nothing to do with the double-descent problem\". What do you mean?\n- It's not clear how you fix the number of leaves in a decision tree? In the standard algorithm, the order in which the nodes are expanded is arbitrary. Do you apply some best-first strategy? Which one?\n- If I understand correctly the baseline in Table 2 is the standard globally pruned model. If so, I would like to see also the performance of a unpruned ensemble.\n- Only one setting of the boosting algorithm is explored (B=5000, a learning rate of 0.02 and default parameters for the CatBoost model). Yet, one expects that these parameters will have an impact on the performance (it's obvious for B at least). I think that more combinations should be explored to make the conclusions more general.\n- Looking at Figure 2, there are more error curves above the average curve than below, which suggest that clusters are potentially unbalanced. It would be interesting to report the size of these clusters.",
            "summary_of_the_review": "The paper is well written and the proposed method makes sense and is original. But it lacks a bit of theoretical motivation and I'm not convinced of its practical relevance, given the very marginal improvements observed in the experiments.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "An enhancement of the popular gradient boosting method is presented wherein the input space is subdivided into regions and the cross-validated optimal number of trees to include in the ensemble is chosen on a per-region basis rather than using a single global number of trees for the entire input space. Consistently superior performance relative to standard gradient boosting across 6 benchmark datasets is reported.",
            "main_review": "The idea of partitioning the input space and using a varying number of models in the ensemble makes a lot of sense to me and is novel, to the best of my knowledge. Gradient boosting and similar techniques do indeed remain widely used and very competitive for small-to-medium dimension tabular data, so the practical significance of the method is clear. Although some minor edits are needed, the presentation of the technique is clear for the most part.  \n\nMy main concern is that the number of benchmark datasets is smaller than I would have liked. Given the option nowadays to rent computing power temporarily via the cloud, it's not unreasonable to expect much more than 6 benchmarks to be tested for a method like this. For instance, I once refereed a paper for another conference which tested their method on 40 benchmarks- and (although I liked the paper), the paper got rejected ! I am not saying 40 is required, but maybe 12 or 15 benchmarks would make me more comfortable with the results than just 6.\n\nWith that said, the method is elegant and intuitive and widely applicable, so I don't mind at all if this paper is accepted. The small number of benchmarks is the reason I can only give it a 6.\n\nOne major suggested edit: the author use the term 'object' in a non-standard way, to refer to what are normally referred to as examples, ( or data points, instances, observations, etc), i.e. the labelled (x,y) pairs used to train a supervised learning model. I suggest choosing a term other than 'objects'.\n\nOne minor complaint: on page 6, it is claimed that cluster surfaces are hard to validate. In fact, clustering can be evaluated out of sample much in the same way as supervised learning, by looking e.g. at likelihood on a holdout sample, so as to choose an optimal number of mixtures in a mixture of gaussians. It is also possible to construct objective functions for something like k-means which trade off K with distance of each data point to its cluster mean. Nonetheless, it does make sense to me to partition with a decision tree, so I am not bothered by this misstatement much.\n\nA few other suggested edits:\n\nPage 3 The early stopping -> early stopping\n\nPage 8 Does the validation protocol proposed in Section 4.4 has good generalization ability? Has -> have",
            "summary_of_the_review": "A method for partitioning the input space for a gradient boosting algorithm and choosing a different number of trees to be included in the ensemble in a partition-dependent way is presented. Improvements on 6 benchmark datasets relative to standard gradient boosting are reported. The method is intuitive and elegant but the number of benchmark datasets is not as large as it could be, and for that reason, the paper gets a 6 (marginal accept).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a novel method to set the optimal ensemble size in gradient boosting. In particular, the authors propose an adaptive strategy that sets distinct ensemble sizes for different regions of the input space. For that, they propose dividing the input space in coherent regions (whose instances are similar both in terms of features and labels) and estimating the optimal ensemble size within those regions. For clustering data, they propose using a decision tree induced from the entire training set, and the leaves of the trees are the clusters that comprise the data partition.\nThey show the results of both a biased and a less-biased estimator for finding out the optimal ensemble size per region, and they compare their findings with the traditional strategy of simply pruning the ensembles based on a single optimal number of learners estimated from a cross-validation procedure. Results in 6 public datasets show that in at least 4 of those datasets the method seems to provide better results.",
            "main_review": "**Strong points:**\n- Very simple method, with easy implementation (making its reproduction straightforward);\n- Elegant idea for incorporating the approach within a cross-validation procedure.\n- Strong results (albeit in a short amount of datasets).\n\n**Weak points:**\n- Very limited experimental analysis. For addressing that I would recommend:\n    - You should try it on dozens of datasets, given that the method is allegedly very fast.\n    - You should compare your approach with several methods for hyperparameter optimization, not only with the naive CV procedure. I am curious to see whether a more sophisticated hyperparameter tuning approach can outperform you simple approach, and if so, with which efficiency it would do it.\n    - You should employ statistical tests for better assessing the statistical significance of results. In particular, there are several nonparametric tests that can help you pointing out significant differences, and also post-hoc methods for pointing out pairwise differences for the case of multiple methods being executed over multiple datasets.\n\n**Questions:**\n- You say you define the number of clusters (leaves in the decision tree) according to the procedure described in Section 4.4. It is not clear to me which procedure is that. Do you mean running Alg. (3) several times with distinct number of clusters, and using the one with the best results? How exactly did you do that? Since defining the number of leaves in decision-tree induction is not straightforward (you need to try and control that via tree height or minimum number of instances per leaf), I would guess that this step is a little bit more complicated than you make it appear (and also, the increased computational cost of running your entire procedure - which has a non-negligible cost itself - multiple times). I think that is a vital part of the method whose discussion is not done at all in the paper.\n- The \"shrink\" method in Alg. (3) is not detailed at all, so I am assuming it means getting the predictions of the full model executed over fold $S_q$ and \"cutting off\" the results that use more than $M_i^q$ models, would that be correct? I am sorry for the confusion, but that is not totally clear to me.\n- In inference (test) time, I would assume you need to see in which cluster the new instance falls on, and then using the number of models defined for that cluster, is that also correct? I ask because there is no mention on that at all in the paper.",
            "summary_of_the_review": "Paper with a nice, elegant, and very simple idea for adaptively generating different number of ensemble sizes throughout the input space, in an attempt to generating better results.\nThe weak point is that the experimental analysis is not up to the standards that one would expect here, as I would have expected the method to be tested over dozens of datasets, with a proper analysis against other hyperparameter optimization approaches, and statistical tests to validate the significance of the results.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}