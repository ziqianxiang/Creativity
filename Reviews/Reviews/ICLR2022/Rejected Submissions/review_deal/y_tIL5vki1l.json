{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes an unconditional GAN that learns a set of structured keypoints as the intermediate representation. It was shown that these learned keypoints may be used to control the image synthesis output. The paper received a mixed rating before the rebuttal, with one reviewer rating the paper marginally above the bar and three reviewers rating it marginally below the bar. While a couple of reviewers commented that the keypoint idea was interesting, several concerns were raised, including the seemingly challenging tuning requirement and the usability of the proposed method. Several missing related works were also pointed out. The rebuttal addressed some of the raised concerns but not fully. While Reviewer Jmzu raised the score from marginally below the bar to marginally above the bar, Jmzu still expressed concerns about the quality of the paper. Reviewer kpkc kept marginally above the bar rating but was not impressed with the contribution. Consolidating the reviews and rebuttals, the meta-reviewer found the raised concerns valid and would not recommend acceptance of the paper. The authors are encouraged to incorporate the feedback to strengthen the contribution."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "Authors proposed the LatentKeypointGAN that detects keypoints and provides the editable capability using the keypoints. The overall training is performed in the unsupervised way without the explicit keypoint supervision. In the aspect that GAN provides the capability of generating the images with the photo-realistic quality while the ability to control attributes are limited, the proposed methods contribute to the related society.",
            "main_review": "> Authors proposed an interesting idea that can detect keypoints of the object in the unsupervised way and offer the capability of manipulating images based on the detected keypoints.\n\n> Few related works are missing: Authors proposed the interesting idea and implemented it in the effective way. However, I could remember an approach that already provide the interactive capability for GAN by representing its intermediate representation as the segmentation map. I think authors need to add this reference and discuss their difference. Personally, in the sense that the segmentation map is similar; yet more advanced than the 2D heat maps, authors contribution could be significantly reduced by this paper.\n\n[1] Learning Hierarchical Semantic Image Manipulation through Structured Representations, NeurIPS'18.",
            "summary_of_the_review": "Overall, I am at the borderline for this paper; however towards more accept side. I could move towards the higher score, if authors could rebut my points effectively.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to use a set of structured keypoints as the intermediate representation for image generation, so that ideally each keypoint with control the location and semantics of a certain part. For each generated image, three noise vectors will be sampled from a gaussian prior distribution, which respectively estimates the locations of k keypoints, the global image style, and the semantics of background. Training is achieved by the standard GAN loss plus a background loss that disentangles background semantics from foreground semantics.",
            "main_review": "I think overall this idea is interesting and straighforward, where controlling the synthesis process via keypoints is pretty useful in practice.\nHowever, I think many related works that provide structural and local edits of GAN synthesis process are missed, making the experiments not very comprehensive.\n \nMajor concerns:\n1. Many related works are not included and discussed:\n\n[a] Kim, H., Choi, Y., Kim, J., Yoo, S., & Uh, Y. (2021). Exploiting Spatial Dimensions of Latent in GAN for Real-Time Image Editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 852-861).\n\n[b] Kwon, G., & Ye, J. C. (2021). Diagonal Attention and Style-based GAN for Content-Style Disentanglement in Image Generation and Translation. ICCV 2021\n\n[c] Edo Collins, Raja Bala, Bob Price, and Sabine Susstrunk. Editing in style: Uncovering the local semantics of gans. In CVPR, 2020\n\n[d]  Yazeed Alharbi and Peter Wonka. Disentangled image generation through structured noise injection. In CVPR, 2020.\n\n[e]  Peihao Zhu, Rameen Abdal, Yipeng Qin, and Peter Wonka. Sean: Image synthesis with semantic region-adaptive normalization. In CVPR, 2020.\n\n[f]  Fangneng Zhan, Hongyuan Zhu, and Shijian Lu. Spatial fusion gan for image synthesis. In CVPR, 2019.\n\n2. I consider latentkeypointGAN as a type of unconditional gan, thus it should not be compared to Pix2PixHD/SPADE/SEAN but recent works that also incorporate structural latent space for unconditional gans, e.g. the ones mentioned in point 1.\n\n3. Another question is that a background loss is adopted to disentangle background semantics from foreground semantics, will the semantics of these k keypoints be entangled with each other? Did you observe more than one keypoints control the same region (i.e. some kind of collapose) during training? If so, how should we resolve this? And if not, why?\n\n4. K is a pre-defined value? What do you mean by adding a keypoint in Figure 1? How should we choose the value of K? There is no ablation study about this.",
            "summary_of_the_review": "I think the idea is good. But related works are not discussed thouroughly. I also have concerns in terms of the baselines and ablation studies in experiments. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The work proposes a novel method for **locally controllable image synthesis via keypoints**. In contrast to existing auto-encoder frameworks, this work considers a GAN-based method to overcome the following limitations: \n- careful tuning of existing keypoint-based methods\n- better disentanglement of pose and appearance\n\nThe method operates in an **unsupervised** setting, i.e. training with rgb-images only.",
            "main_review": "### Strengths:\n+ The paper is well-written and easy to follow. The main claim of allowing to edit images via keypoints is well-supported in the experimental section and in the videos in the supplementary material. \n+ The paper includes many ablations to validate the architectural choices made. Overall, the proposed method appears technically sound.\n+ The paper shows impressive results on LSUN beds which is a difficult dataset that none of the existing unsupervised approaches has tackled so far. \nQuantitative evaluation on celebA-HQ suggests that the method produces similar image fidelity to methods that require more supervision.\n\n### Weaknesses:\n- There are not enough qualitative and quantitative comparisons to the baseline approaches. The method claims that existing works struggle with the disentanglement of pose and appearance, but the experimental section does not validate that their method outperforms existing approaches on that matter. From the results, it seems that the method also struggles with the entanglement of pose and appearance in challenging cases, e.g. the video on LSUN in the supplementary or background and hair on FFHQ. It is unclear if auto-encoder-based methods perform similarly well in simpler cases or if the proposed method really improves the disentanglement.\n- The method might require as much tuning as the baseline approaches. As one key limitation of TPS transformation-based approaches the authors list that they are “notoriously difficult to tune”. But training a GAN also requires a lot of tuning for each dataset and can be very unstable, e.g., the proposed method is highly sensitive to the choice of GAN-objective. The proposed method further introduces two hyperparameters (tau and K) which might also require tuning for each dataset. Hence, it is questionable if the proposed method is really easier to tune than existing approaches.\n- The results in the paper and supplementary seem curated and no uncurated samples are shown. Therefore, it is difficult to predict if the shown results are cherry-picked.\n\n### Ethics statement: \n- Second paragraph: One could still optimize for the latent codes of a real image to manipulate it.\n- Third paragraph: The claim that GANs have a low risk of memorizing training images is not supported well enough. If there is work that clearly indicates that training data cannot be recovered from GANs it needs to be cited here, otherwise the claim does not hold.\n\n### Additional questions / comments:\n- Did you try to sample keypoints from a pretrained keypoint extractor instead of learning them end-to-end? As this might simplify the task it would be interesting to see if it can further improve your results.\n- What exactly makes TPS-based methods so difficult to tune?\n- I suggest adding the FID results for all datasets to the main paper, as well as the FID results for the compared baselines on those datasets so that the reader does not need to rely on a few qualitative results only.\n- Lorenz et al. also evaluate on Human3.6M and PennAction which are very interesting for keypoint-based editing as they consider more complex scenes than e.g. celebA. Evaluating these datasets would substantially strengthen the claim that the proposed method “can be applied to many different objects and image domains”.\n- On the BBC pose videos it looks like the GAN overfits to the appearance codes, because e.g. the head pose changes but keypoints remain at the same position. This is an interesting and reasonable failure case that should be added to the limitations.\n- Appendix B: why do you not always use the tuned LatentKeypointGAN? What are the downsides of this architecture? Please add the FID evaluation similar to Table 1. Or is this the ‘keypoints’ row? Then the naming needs to be more clear.\n- Adding more and especially uncurated samples on all datasets to the appendix, e.g. latent interpolations of single keypoints would be really useful to get a better intuition on how well your approach works. \n- Related work: your approach would be even better motivated if you could state more clearly why it is important to tackle this problem in an *unsupervised* setting. Is it difficult to obtain reliable segmentation masks/landmarks in the settings you consider?\n- Intro: Both works (Li, Xu) that list limitations of auto-encoder-based approaches are only arxiv publications in your references which is not particularly strong as an argument. (Li is actually accepted at ICPR though). It would be more convincing to illustrate the limitations (e.g. entanglement) of the approaches here.\n- FFHQ also has landmarks available. For completeness, it would be good to report the keypoint detection results on FFHQ as well.\n- Table 1: Why is FID without keypoint conditioning better than with?\n- Table 2: Why is FID so much worse with additive global style while L1 error gets better? (I think you mention this in the appendix, but it might be good to add it here already)\n- How useful is the additional loss on the background? You could add a (qualitative) ablation to show that it is needed.\n\n### Misc:\n- Throughout the paper: if you refer to experiments in the appendix, please also put the link to the corresponding section.\n- First paragraph sec 3.1 “MLP to respectively generate the”\n- Last paragraph sec 3.1 “We show in the evaluation ... ” Please add a link to the respective section in experiments.\n- Sec 3.1: Add information that K is a fixed hyperparameter\n- Sec 3.3: Please explain spatial adaptive normalization so that the paper is self-contained\n- Sec 4.1 celebA: please explain MAFL to be self-contained\n- Sec 4.1 celebA: celebA-HQ version missing that is used for sec 4.4, also why do you use celebA-HQ there and not just celebA?\n- Sec 4.2 “appendix document” → “appendix”\n- Sec 4.4 celebA-HQ: what’s the resolution?\n- Table 2: contrastive learned keypoint embeddings are not explained in the method section, not sure what you mean by this\n- Appendix B:  “the convolutional generator is translation invariant” this is not trivially true, see “Alias-free Generative Adversarial Networks (StyleGAN3)” by Karras et al.\n- Figure 9: (top) (bottom) should be (left) (right)",
            "summary_of_the_review": "The paper shows **impressive results** for keypoint-based editing of synthesized images. However, there are **too few qualitative and quantitative comparisons to existing approaches** which makes it difficult to assess the quality of the proposed method in practice. In particular, it is not clear if the proposed method indeed outperforms existing approaches wrt. generalization, i.e. being easier to tune, and disentanglement of pose and appearance. Therefore, in its current state, I lean towards rejecting the paper.\n\n\n**Post-Rebuttal**:\nAfter the rebuttal, I lean towards accepting the paper as image fidelity improves over the baselines. However, my concerns regarding better performance wrt disentanglement and easier tuning than the baselines remain.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Allowing to edit images on keypoint level takes a step towards more realistic DeepFakes. The authors address this concern to some extent in their statement but this might not be sufficient (see \"Ethics statement\" in main review).",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed a new way to disentangle and control the GAN synthesis process via key points positions, key points appearance and background appearance. Since the key points are learnt in an unsupervised way, the proposed method avoids the labour-intensive labelling process. Extensive experiments verify the effectiveness of the proposed method.",
            "main_review": "Strength:\n1. The paper is well-written and easy to follow.\n2. The evaluation is thorough and convincing.\n\nWeakness:\n1. The proposed method is not quite novel. Although the paper emphasized key points, the incorporation of $w_{const}^j$ and the spatial embedding layer (section 3.2) that broadcasts $w^j$ according to a simple positional heatmap implicitly change the key points to a special kind of \"segmentation mask\". In other words, the core method (section 3.1 and section 3.2) is essentially a conversion from key points to a special kind of \"segmentation mask\" that is Gaussian-like and does not have sharp boundaries. This also explains why it can be easily incorporated into SPADE.\n2. As an unsupervised method, the performance of the proposed method depends heavily on two hyperparameters: the choice of the number of key points and their influence range $\\tau$. These are not highlighted in the main paper but are slightly discussed in F.2 of the appendix. I believe the choice of these two hyperparameters interferes with each other and can be difficult to tune, which can make the method less useful. I would suggest adding an in-depth analysis of how these two hyperparameters influence the performance.",
            "summary_of_the_review": "As abovementioned, I think the paper is well-written and polished to a high level. However, its contribution is a bit thin and it lacks an in-depth analysis of the two deterministic hyperparameters: the number of key points and their influence range $\\tau$ (the discussion in appendix F.2 is not sufficient). Thus, I feel slightly negative on the submission.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}