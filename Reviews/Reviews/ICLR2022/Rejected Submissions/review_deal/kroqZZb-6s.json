{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper has been independently assessed by four expert reviewers. Two of them recommend acceptance (one straight, one marginal), and two rejection (both marginal). Among the main limitations of the presented work, found by the reviewers, was the limited reproducibility of the results due to the use of private data. The authors attempted to defend their experimental design choice to use only one private dataset by stating difficulty in obtaining public data that would be suitable for their method. I am personally in a strong disagreement with that statement, and with the sub-statement of the authors that some publicly available ICU data may not be suitable. That either signals limited practical utility of the presented approach to non-ICU settings only, or is simply incorrect. The presented approach nonetheless has an intriguing potential and I would be inclined to recommends its acceptance. Alas, I find the lack of reproducibility to be a significant drawback of the way this work is currently presented and this limitation could not be easily resolved. Therefore I am leaning towards recommending a rejection."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents a way to predict outcomes of patients using EHR records. The authors propose a phenotyping model which clusters similar patients based on their outcome distribution. They also introduce feature-time relevance map which helps explain for a particular patient the feature-time combinations which are important for their prediction.",
            "main_review": "Strengths: \n1. The paper introduces time-feature relevance maps, which enable domain experts to look at features-time combinations which are in some important for their predictions.\n2. The paper is written in a manner which makes experiments fairly easy to reproduce. \n3. The proposed method is better than the National Early Warning Score used in the UK hospitals.\n\nWeaknesses/Scope for improvement: \n1. The TSKM baseline performs really well for the clustering problem. The authors claim that the superior results of TSKM are due to the metric bias towards convex clusters. However, given the capacity of deep neural networks and the ease of learning convex clusters, I am not convinced why the deep learning method would be at a disadvantage. The authors also claim that clustering on the input space and latent space cannot be compared. However, my understanding is that learning a smaller dimensional latent representation should instead help clustering, if the latent representations are meaningful. \n2. I believe that in addition to the distribution of likely outcomes, common characteristics of patients in a cluster would be clinically relevant. For example, the could the authors come up with a way to use their proposed feature-time relevance maps to characterize each phenotype? I believe that would be very helpful.\n3. There are no experiments which serve as evidence that the proposed loss functions are (1) needed, and (2) work. I can imagine that the authors can design simple experiments to illustrate the cluster collapse problem etc. which their loss functions are designed to handle. These experiments would both establish the need and serve as a \"proof\" that the loss functions are working as expected. \n4. I am not sure if the baselines to predict outcomes directly from EHR data are strong enough. I suggest using stronger baselines to directly predict outcomes, for example 1-D ResNets have been found to be really performant time series classification models.\n\nQuestions:\n1. Are the cluster representations $\\mathcal{C}$ interpretable? \n2. The authors are phenotyping patients based on unique distributions of outcomes. In this respect, how does one define \"clearer\" and \"more separable\" cluster phenotypes? More generally, how does one define a \"good\" phenotype? I feel the distributions in case of AC-TPC model are \"clear\" too.\n3. I am not sure how does the proposed model handle different features having different sampling frequencies. \n\nSuggestions: \n1. The figures do not seem to have high quality. I recommend saving them as .svg or .pdf to ensure the same.  \n2. Typographical/grammatical errors: \n     -- \"which can has a well-known solution\" => \"which has a well-known solution\"\n     -- \"with a smaller change of death\" => \"with a smaller chance of death\"\n3. Appendix A.10 has the true number of patients per outcome for each cluster. It would be interesting to see how close is the distribution of outcomes to the ground truth, based on a measure of distance between distributions. \n4. Ensure that the colour maps in Fig.6 have the same range i.e. the same mapping between the colour and the relevance value.",
            "summary_of_the_review": "I believe that the work is interesting, solves an important problem and does well. However, further experiments are needed, for instance, to check why the model is not performing as well in the clustering problem. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper describes a cluster-based learning method for EHR time series of in-hospital patients aimed to be interpretable and improve outcome prediction.\\\nIt claims to contribute with an interpretable framework and its training process, with a weighted loss to deal with the class imbalance and a custom loss to avoid cluster collapse.",
            "main_review": "Strengths: \\\nWriting is clear and well organized. \\\nNice figures and appendix content. \\\nGood performance at prediction combined with cluster interpretability. \n\n\nCons: \\\nOnly evaluated on private data. \\\nThe clusters initialization. Is there a specific reason to use K-Means? Was it evaluated a different initialization method? \\\nThe lack of ablation studies. The impact of removing or changing some of the framework's features, like the custom losses and Attention Block, should be evaluated to bring a stronger understanding of its impact on final performance. \\\nHow does your Attention Encoder differ from RETAIN[1]? It looks very similar. \n\n\nMinor comments:\\\nThere is a missing blue dot and Figure 5 \n\n\n\n\\\n[1] Edward Choi, Mohammad Taha Bahadori, Joshua A Kulas, Andy Schuetz, Walter F Stewart, and Jimeng Sun. Retain: An interpretable predictive model for healthcare using reverse time attention mechanism. arXiv preprint arXiv:1608.05745, 2016.\n",
            "summary_of_the_review": "It is a good paper with relevant contributions.\n\nI am not sure if the results are well evaluated so I expect to conclude at the discussion period.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors through this paper propose a deep learning framework to identify phenotypically separable clusters using EHR data and introduce a feature-time attention layer to better represent patient data including optimizing two loss functions to address class imbalance in the datasets. The study aims to make a more precision based clinical observation by leveraging feature dimensions and time-series nature of EHR data.",
            "main_review": "Strengths:\n1) The authors did a good job in explaining and navigating the time-series complexity and challenges posed by EHR data particularly the limitations of previous work and the results section.\n2) The paper provided comprehensive explanation of the proposed deep learning network especially the added model interpretability which plays a crucial role in integrating techniques/algorithms of artificial intelligence in healthcare.\n\nWeakness:\n1) My key concern about the paper is the lack of rigorous experimentation in designing the study cohort. Real-time diagnosis and prediction of clinical interventions is a major challenge especially using EHR data and for this study only the observations within 24 and 72 hours before the outcome were considered. The best practice while studying EHR-based predictions is to design an observation and prediction window, this practice helps understand how long the observations made by the model is valid and helps the clinicians to proactively assist their patients. The analysis presented in the manuscript lacks the experimentation with the sliding window and this further limits the scope for improvement and assessment of the models performance and determining the best possible subtype.\n\n2) Similarly as noted above, the missingness treatment in the study also lacks experimentation. The foundation of the proposed deep learning framework is based on the quality of data and missing data is more prevalent in EHR but the authors failed to elaborate more on the missing data imputation section as in why authors chose to impute missing data with previous time block and median when there are other state-of-the art techniques/algorithms available to impute missing values especially when the quality improvement of the underlying data could increase cluster separability and model predictions. This could possibly help future readers of this paper to approach/reproduce/handle missing data problems in their respective healthcare settings.\n\n3) The clustering separability of the proposed framework(CAMELOT) is significantly low than the benchmark TSKM and also the F-1 score of the outcome prediction scores is low, and AUROC is only marginally better than the other benchmark models. Unfortunately, the novelty in the method proposed by the authors does not reflect in the model outcomes.\n\nMinor comments:\n1) Authors should include what percentage of missingness was in the feature variables before imputation, perhaps in table A.2.\n2) In section 3.6, I do not see citations/references for the libraries/software's used by the authors for this study. Please cite them appropriately.\n3) I suggest authors choose a different color palette/schema to improve the interpretation of the feature-time cluster relevance map and make the heatmap(Figure 6) more visually appealing.\n4) In Discussion section, 'learnt cluster attention maps introduced in Figure 6 introduce yet another layer' -> 'learnt",
            "summary_of_the_review": "Although authors did a fairly good job in conceptualizing and developing the latter stages of their proposed deep learning framework, as I elaborated in my main review, the prior stages of the framework lack experimentation. Model-centric approach is important but data-centric approach is more crucial especially in healthcare where predictions play a vital role in assessing disease progression and therapeutic intervention.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This work proposes a supervised approach to phenotype patients given their EHR trajectory and predicted outcome. The authors argue that the lack of interpretability of current deep learning approaches does not allow clinically relevant phenotypes and propose a feature-time attention mechanism to tackle this issue.\n",
            "main_review": "This paper tackles the important problem of phenotyping with an attention-based recurrent model. The neural network predicts an assignment for population-level latent centroids.\n\nWhile the method is novel, it would be valuable to justify further the use of an LSTM, as attention-based mechanisms leverage long time dependencies while accelerating the training time of the model. The use of an LSTM might jeopardize this gain. A comparison with an MLP feature extractor would be valuable.\n\nFurther clarifications are necessary to describe the attention mechanisms (how is D_f chosen and what does it represent ?) and the model training (Why is the model trained iteratively ? It seems it could be done end to end).\nIn the experiment section, it is not clear why one should average performances over multiple seeds as it quantifies the modelling sensitivity to hyper parameter tuning and training, but not the model generalization to external datasets. Bootstrapped or cross-validation performances should be preferred. Additionally, the use of the 'Occam razor' selection of hyperparameters needs to be justified, as it is not clear to me how it is optimized in a large hyperparameter space.\n\nIn the results and discussion, metrics should be further described to highlight what they capture and what reflects a clinically relevant clustering. Figure 6 does not allow a better understanding of the clusters. It would be more interesting to show average attention for patients assigned to the same cluster.\n\nThe authors should also consider similar approaches proposed in the literature. \n- Mechanisms that leverage both temporal and EHR data have been recently presented [1]. \n- Models which both cluster and model an outcome are also used in the statistical literature as profile regression models [2]. \n- Balanced loss is also widely used in the machine learning literature, the authors should not claim novelty on this part but cite papers such as [3]\n\nFinally, a few minor revisions:\n- \"Traditional clustering models such as KMeans [...] have been shown to fail to capture the existing time dependent deature relationships\" would need citations\n- The authors describe a 4 hour moving window. Do they use separate window or overlapping ones, ie what is the frequency of the final data ?\n- Median imputation is using validation and test data, shouldn't be training data ?\n\n\n\n[1] Rocheteau, E., Liò, P. and Hyland, S., 2021, April. Temporal pointwise convolutional networks for length of stay prediction in the intensive care unit. In Proceedings of the Conference on Health, Inference, and Learning (pp. 58-68).\n\n[2] Molitor, J., Papathomas, M., Jerrett, M. and Richardson, S., 2010. Bayesian profile regression with an application to the National Survey of Children's Health. Biostatistics, 11(3), pp.484-498.\n\n[3] Cui, Y., Jia, M., Lin, T.Y., Song, Y. and Belongie, S., 2019. Class-balanced loss based on effective number of samples. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 9268-9277).",
            "summary_of_the_review": "This work addresses an interesting and important problem in medical machine learning. However, it would benefit from additional methodological descriptions and results' interpretations.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}