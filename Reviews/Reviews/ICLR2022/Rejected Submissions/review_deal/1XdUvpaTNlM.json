{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The reviewers consider the authors' approach to pruning of convolutional networks reasonable; but neither sufficiently novel nor sufficiently well explored for inclusion in the conference.  In particular, the reviewers would like to see a more explicit discussion of the effect on training time of the authors' method, and more discussion and comparison against previous probabilistic pruning methods."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents a probabilistic channel pruning method based on batch whitening to accelerate and compress CNNs.",
            "main_review": "The strength of this paper can be summarized into the following points.\n1. This method has some good properties for pruning, like structural pruning, simultaneously pruning and training, and so on.\n2. The analysis of whitening shows that the activation probability of important channels will become larger, which might be helpful for channel pruning.\n3. The method generally achieves better results compared to similar methods.\n\nThe weakness of this paper are:\n1. Batch-whitening is built on batch normalization, which may limit the application scope of this paper. In language modelings and vision transformers, layer normalization is widely used instead of batch normalization. Does whitening can also be applied to such settings? In addition, for downstream tasks, like object detection, the image resolution becomes larger, and the allowed mini-batch size will decrease dramatically. When the mini-batch size is limited, dose batch-whitening suffers from the poor estimation of $\\Sigma$? In another word, the proposed method may perform poorly when batch normalization is unreliable. \n2. Some arguments are not well supported by the experimental results. In the paper, the authors argue that 'bach-whitening increase the activation probability of useful channels while keeping the activation probability of unimportant channels unchanged'. This is demonstrated in Fig.3 (a.b). However, increasing the activation probability may not improve pruning. In Table. 3, Fig. the relative performance changes between case (3) and BL is similar to case (1) and BWCP. If increasing the activation probability is helpful, we should see BWCP achieves a more obvious performance gain compared to the case (3). The current ablation study does not support the argument that increasing the activation probability benefits pruning. \n3. It seems that the batch-whitening process incurs additional computational costs since you need to keep updating the covariance matrix. Although newton iteration improves the efficiency, the training time is also increased. If we measure the training budget by the training time, other methods actually use a smaller training budget.\n4. The experimental results are not very significant compared to previous methods. The improvement over previous baselines is around 0.3 for ResNet-50. ",
            "summary_of_the_review": "In summary, I think this is a borderline paper. It has some good properties, but it is limited by using batch normalization. It also introduces additional costs at the training time.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, authors propose to compress convolutional layers via batch whitening channel pruning (BWCP). By apply batch whitening on incoming feature maps, unimportant channels are automatically identified and stochastically discarded during the training process. A sigmoid-like transformation is used to push activation probabilities to either 0 or 1, so that additional fine-tuning is not needed, unlike many other channel pruning methods.",
            "main_review": "Strengths:\n1. Based on the proposed stochastic pruning scheme, the full channel space is preserved during the training process, which avoid stuck in some local minimum due to improper selection of channels to be pruned.\n2. The batch whitening module, together with the regularization term on per-channel scaling factors and biases, automatically increases the activation probability of informative channels, so that unimportant channels can be identified more easily.\n3. The effectiveness of the proposed method is validated by extensive experiments on various network architectures and ablation studies.\n\nWeaknesses:\n1. The training efficiency of the proposed method is not discussed. Will the stochastic pruning scheme require additional training epochs, compared with other channel pruning methods?\n2. Section 4.4, Equation (7). As shown in Proposition 1, the activation probability is determined by $\\beta_{c} / \\left| \\gamma_{c} \\right|$, so it seems more reasonable to define the second regularization term as $\\lambda_{2} \\beta_{c} / \\left| \\gamma_{c} \\right|$, rather than $\\lambda_{2} \\beta_{c}$ (or, is it due to the optimization difficulty of the former one?).\n3. Section 4.4, last paragraph. Authors state that a sigmoid-like transformation is used to make activation probabilities approach 0 or 1 during training. How is this carried out exactly? It would be helpful to provide more technical details to improve the reproducibility of the proposed method.",
            "summary_of_the_review": "The training efficiency remains unclear. Although the stochastic pruning scheme can explore the channel space more sufficiently, it is also possible that more training time are needed for such exploration. More details should be provided here.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a probabilistic channel pruning method (BWCP) for accelerating CNNs. The key newly proposed technique is “batch whitening”. They evaluate their method on CIFAR-10/100 and ImageNet compared to other recent filter pruning methods.",
            "main_review": "Strengths:\n\n1.\tThey propose a probabilistic pruning method, quite different from existing pruning methods which act in a deterministic manner. \n2.\tTheir method is shown effective on CIFAR-10/100 and ImageNet.\n\nWeaknesses:\n\n1.\tSevere idea overlap. \nThe main spirit of this paper, “unimportant channels are stochastically pruned with activation probability”, namely the so-called “probabilistic” pruning, has been explored in the literature. See [*1], they proposed to assign activation probabilities to different filters; more important filters have larger activation probability.  This is very similar to what is proposed in this paper: “we assign each channel with an activation probability (i.e. the probability of a channel being activated)”, “A larger activation probability indicates that the corresponding channel is more likely to be preserved”. Please clarify how your method is substantially different from [*1] (if any).\nBesides, there is no determined relationship between regularization strength and the final speedup in this method. Thus, “we should search for proper values for λ1 and λ2 to trade off between accuracy and FLOPs reduction, which is a drawback for our method”. However, in [*1], how many filters are imposed with probabilistic pruning is explicitly linked with the desired layer-wise sparsity, thus their method can achieve desired speedup for sure. In this sense, [*1] is also more convenient than the BWCP in practice.\n\n2.\tPerformance is not strong. Missing important comparison with more recent methods. \nIn Tab. 2, ResNet50, ImageNet. They do not have any 2021 papers compared. If compared to the more recent 2021 methods (e.g., [*3-*4]), BWCP is actually not as competitive. For example, [*3] achieves 75.36% top-1 accuracy under speedup 2.31x, while BWCP only achieves 75.18% under even less speedup (2.05x).\n\n3.\tIn terms of the proposed methodology itself, it is a BN-based pruning method. While not all deep CNNs have BN. For example, in image super-resolution (SR), BN is not used in SOTA SR network (such as [*2]). This limits the potential application of this method, while related works (such as [*1], HRank, DSA, [*3-*4], etc) do not have this limitation.\n\n[*1] Structured Probabilistic Pruning for Convolutional Neural Network Acceleration, BMVC, 2018\n\n[*2] Image super-resolution using very deep residual channel attention networks, ECCV, 2018\n\n[*3] Neural pruning via growing regularization, ICLR, 2021.\n\n[*4] Towards compact cnns via collaborative compression, CVPR, 2021\n",
            "summary_of_the_review": "There is a severe idea overlap between this work and previous work. Meanwhile, the empirical performance is not strong.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a way to train compact neural networks. The feature channels are whitened, then combined with a sampling method. At the testing time, this sampling is discarded and replaced with a deterministic selection. \n\nThe benefits of whitening in improving the convergence of NNs is widely explored. This paper explores another useful property of whitening,  the sparsity in the whitened results. Since the representation is sparser,  one might use fewer weights to reach similar performance. This paper presents such a solution.",
            "main_review": "Feature whitening reduces the correlation between features. It was shown that the whitened representation facilitates the training. Another property is that the whitening also leads to a sparse representation. This paper shows a nice application of this property in training compact neural networks. \n\nThis direction is very important, neural networks have been dense and large. Whitening has been found to be the more optimal normalization  in terms of training of neural networks. However, its benefit in compressing is barely known to the community. This paper is a good attempt in this direction.\n\nThe downsides: \n\n(1) Some reorganization could clarify the presentation. The reader may want to have a quick understanding of why whitening is chosen over standardization. Some plots similar to Fig. 10,11 in Ye et. al network deconvolution may be helpful. It will be beneficial to make the paper more self-contained, e.g. clearly formulate Gumble softmax  in eq 6.\n\n\n(2) It is not clear whether the improvement is due to whitening or pruning. Whitening based methods such as decorrelated Batch Norm and IterNorm by Huang et. al generally lead to better performance compared to standard normalization methods. The benefits of sparsity may have already been exploited by these methods. If this is the case, one might be able to directly train a narrower network without the Gumbel softmax trick. Can the authors provide more insights/experiments to this? How much better is this method compared to direct whitened training?\n\na typo in page 8: spare -->sparse",
            "summary_of_the_review": "The paper has room for improvements, but it is a nice new attempt to the pruning of networks.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}