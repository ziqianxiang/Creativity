{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper formally studies the problem of partial identifiability when inferring a reward function from a given data source (e.g., expert demonstrations or trajectory preferences). To formally characterize this ambiguity in a data source, the paper proposes considering the infinite-limit data regime, which bounds the reward information recoverable from a source. Furthermore, this ambiguity is then studied in the context of different downstream tasks, as recovering an exact reward function may not be necessary for a given task. The paper is primarily theoretical, and the results provide a unified view of the problem of partial identifiability in reward learning for different sources and downstream tasks.\n\nOverall, the reviewers acknowledged the importance of the problem setting and found the results promising. There is quite a bit of spread in the reviewers' final assessment of the paper with ratings 8, 8, 3, 3 (note: one of the reviewers with rating 3 has a low confidence). The authors' responses did help in discussions; however, a few of the concerns, as raised by reviewers, still remained. The key issues are related to the general accessibility of the paper and the lack of concrete examples to highlight the proposed theoretical framework. At the end of the discussions,  several reviewers (including those with an overall positive rating) shared concerns about the paper's accessibility.  With this, unfortunately, the paper stands as borderline. Nevertheless, this is exciting and potentially impactful work, and we encourage the authors to incorporate the reviewers' feedback when preparing a future revision of the paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper provides a theoretical framework to understand the relationship between data source/downstream tasks and reward functions. ",
            "main_review": "Some clarification questions:\n\n1. At the end of page 3, you mentioned the maximum entropy policy as the fixed point to $\\pi_\\beta = \\pi_{\\beta}^{\\pi_\\beta}$. Could you provide a definition of the maximum entropy policy and a proof sketch of why it is equivalent to the fixed point? In my understanding, a maximum entropy policy is the optimal policy result from solving policy optimization with an entropy regularization. It's not obvious to me why this solution will coincide with the fixed point defined above.\n\n2. X never appears in Definition 2.1. Do you mean to define $X = f(R) = f(t(R))$?\n\n3. I don't quite understand Definition 2.5, as the mathematical definition of $R'$ seems to be independent of $R$?\n\n4. 5 transformations are defined in section 2.1. Do they mean to form an exhaustive list? Or are there potentially other relevant transformations not discussed here? At a higher level, an object (e.g. expert demonstration) can induce a set of reward functions not described by any finite combination of the listed or potentially new transformations. So instead of characterizing the information flow through the reward function, why not directly analyze whether the information from a data source is sufficient for a downstream task? The answer to this question is completely independent of whether one chooses to learn a reward function or not. For example, the information captured in an expert demonstration is sufficient for the downstream task of optimal policy identification, and therefore it is well-known that despite the non-uniqueness of reward function in IRL, an optimal policy can be recovered regardless.\n\nMajor comments:\n\n**Discussion of related works can be significantly improved**: Most of the mentioned related works on reward learning are empirical in nature. It would be good to instead comprehensively survey the theoretical works that concern reward learning, since the paper itself is theoretical in nature.\n**Example application of the framework to improve upon/subsume prior results**: While the introduced reward-function centered information-theoretical framework provide a novel and unified view on the information-theoretical relationship between different objects, it alone is in my opinion not sufficient for publication. When you introduce a brand-new framework, it is also important that you make some convincing argument of why this framework is **useful**, in addition to being mathematically correct. A typical way of doing this is to show that using the new framework, one can easily recover/improve results from various prior works, and thus truly provide a unifying framework that subsumes prior literature.\n\nI would suggest the authors throw a majority of the theorems and transformations in section 2 and section 3 into the appendix, and use the space to set up one or two concrete examples on which the proposed framework can be useful in deriving new/matching upper and lower bounds.\n",
            "summary_of_the_review": "1. Discussion of related works can be significantly improved.\n2. Example application of the framework to improve upon/subsume prior results.\n\nIn summary, I think this paper provides some novel insight in the problem of reward learning, but there can be substantial improvements to be made to make the paper significantly stronger. I would highly suggest the authors make the additional effort, and I know it's gonna be a lot of work. But it will potentially make it a spotlight/oral paper instead of a borderline.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This is a theoretical work on understanding the intrinsic limits of various data sources that are used for reward learning in RL. In particular, by considering the infinite data limit of the data source, they study the level of reward ambiguity that can be obtained for a given downstream task. For example, for the expert behavior data source, they characterize the reward transformations that are determined by the optimal Q-function. Similar attempts have been made previously for specific data sources and specific planning algorithms (Ng & Russell, 2000). However, this work makes substantial contributions by conducting this study in a unified and rigorous way for variety data sources and downstream tasks. ",
            "main_review": "The paper is overall well written. The related work is clearly discussed, and this work is well-positioned in the literature. I enjoyed reading the paper. \n\nI haven’t checked the proofs of the theorems; however, the justifications provided before the theoretical claims are convincing. \n\nThe paper is a bit heavy in terminologies; however, it is inevitable due to the theoretical nature and rigor purpose of the paper. \n\nComments:\n1/ Re. the optimality-preserving transformations in Definition 2.5: is it possible to extend this definition to regularized MDP (e.g., entropy regularized MDP) objective as well? In that case, will the function \\Psi intuitively correspond to the regularized value function? \n\n2/ In Theorem 3.3, the S’-redistribution invariance and potential shaping invariances are not mentioned -- is this because they are less ambiguous than optimality-preserving transformations? \n\n3/ Re Theorem 3.10 for noiseless comparison based on the return: compared to other objects/theoretical statements, in this theorem, there isn't any “determines”-style claim. It is said that the precise monotonic invariances depend on the MDP. It would be good to have a clarification/discussion on this point. \n\n4/ All the results are derived for the finite MDP setting. A brief discussion on the applicability (or possible extension) of these claims to the continuous MDP setting would be helpful. \n",
            "summary_of_the_review": "This is a strong theoretical work; it makes a fundamental contribution to reinforcement learning literature. The reward functions are atomic to RL -- understanding the theoretical limits on how much information can be extracted from various data sources used for reward learning is important. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The submission considers a reward learning problem when the reward function is not uniquely recoverable from the data, even in the infinite-data regime. ",
            "main_review": "This paper is very hard to follow due to many non-standard notation and concepts. It is not even clear what the goal of the paper is. The notion of invariance is vague and confusing. I think the authors should have given more effort to setup the problem and goal more precisely and concisely, while using easier to understand notation and terminologies. ",
            "summary_of_the_review": "The paper is very hard to follow and I may not be able to assess the paper properly. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
        },
        {
            "summary_of_the_paper": "This paper characterizes the partial identifiability of data sources and the reward function. Then it analyzes the impact of this measure on the optimum and the algorithms. Some implications are given.",
            "main_review": "This is an unconventional paper. The problem it's attacking is very fundamental and interesting: given reward functions that are \"close\" to each other under some measure (doesn't have to be rigorous and literal \"measures\"), what will they affect (in both the optimum of the MDP and the behavior of the downstream algorithms). What the paper does is to provide a set of measures and a set of claims that partially answer the questions, for the cases that are more or less low-hanging fruits. These claims are not particularly strong I would say, but they do provide some reasoning and implications on this very important topic. I would personally be more interestied in a particular, presumably not that general, setting (say, just tabular and linear program), with a clear and complete answer to that question. To this end I would wonder if the manuscript better fits a journal publication or a book chapter etc. But I'm more or less open for discussion in case this manuscript could provide some support for future studies.\n\nSide question: Can authors just compile one single pdf of 21 pages for the main submission per the policy?",
            "summary_of_the_review": "Better fits journal publication or book chapter?",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}