{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This is a representation learning time series paper.\n\nThe reviewers appreciated aspects of the paper, but all agreed that primarily the experiments are lacking and to a lesser degree the presentation is unclear and needs further proofreading.\n\nSo definitely this work has merits. It is also much appreciated that the authors throughout the discussion have been engaged in adding results and further clarification. This can be used for an updated version for the next conference."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents a model for learning representations of time series. It addresses, in particular, the context of multivariate time series having exogenous and endogenous dimensions, and where the goal is to learn good representations independently from the context (exogenous dimensions). This work is an extension of existing work, the additions being the multivariate aspect and the context-invariant aspect. The context-invariant problem is treated thanks to an adversarial learning mechanism.  Experiments are conducted on synthetic and real datasets.",
            "main_review": "Pros:\n* The article is well written and easy to follow.\n* The model is elegant, well described.\n* The approach and the motivation are clearly stated.\n\nCons:\n* The experimental part is very light and does not convince me of the interest of the proposed approach. No other deep learning models are used as baseline (even if not adapted to the context-invariant setting) when there is a rich literature about anomaly detection in time series. I understand that the goal is not to evaluate the performances in classification/anomaly detection but the learned representation, but I can not judge the difficulty of the used datasets with the provided baselines.\n* The novelty of the paper is not great wrt the previous work, it consists mainly of the use of adversarial learning. ",
            "summary_of_the_review": "The addressed problem is interesting and the proposed model is elegant but I think that the experimental part is too weak to assess the quality of the proposed approach and that the novelty is not sufficient wrt previous works.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This article deals with representation of multivariate signals in the case where we can distinguish endogenous & exogenous channels. This article mainly relies on (Franceschi et al.,2019), whose proposal consists in representing signals using Dilated-TCNN and to apply triplet loss on the sequences as in word2vec. Then the authors introduce 3 cases: normal behavior, anomalies where both endogenous & exogenous are impacted and partial anomalies.\nThe encoded representation of a contextualized signal should be close to its temporal neigbors.\nThen, the authors tackle prediction & classification tasks on M5 and electricity datasets. The experimental part is a little bit confusing, as the datasets used in the different experiments are not the same.\nGenerally speaking, results are very difficult to interpret.",
            "main_review": "Pros\n\n+ interesting and relatively unexplored issue in the literature\n\nCons\n\n- article needs deep proofreading\n\n- experimental part not clear enough. Mix in the datasets between experiments is confusing.\n\n====\n\nThe article contains a number of English mistakes and deserves a thorough proofreading.\n\nFigure 2 is the weak point of the article: whereas it is suppose to give a general overview of the system, it not clear for me.\n\ndot should be removed from (2) and bolded font should be used for vectors to make the reading more convenient.\n\ntypo: in (4) there is a confusion between Lc and Lr\nmore generally, the formalization of section 2.2 could be improved.\nThe loss function R is not given in (5) whereas it is supposed to be an important contribution of the paper.\n\nThe classification tasks of section 4.1 are not clear for me: I don't understand the introduction of the synthetic dataset and I don't see the interest of classifying a global signal wrt the introduced architecture.\nSome additional content in appendix (& even in the body of the article) are required.\n\nRegarding the forcasting, we don't know the horizon of the prediction and we have no information about the number of scales taken into account. This information is critical both for understanding and reproduction.\n\nIn section 4.2, what is the discretization step? I don't know how to interpret results without this key information.\nThe proposed approaches should also be compared with a simple baseline to provide a better demonstration of the interest of the method.\n\nAt least, the authors should build a synthetic dataset that enables us to understand the strengths and weaknesses of the different formulations.\n",
            "summary_of_the_review": "This work seems very promising but the interest of the invariant representation is not clearly demonstrated according to me. Some experiments and critical information are missing so that I have to reject this paper. I strongly encourage the author to write & submit an improved version of this article in another conference.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper adapts self-supervised learning to the time series analysis domain for the anomaly detection problem. Authors propose a contrastive component to distinguish similar/dissimilar time sires and a domain adversarial component to ensure the embeddings are context invariant. ",
            "main_review": "This paper adapts self-supervised learning to the time series analysis domain for the anomaly detection problem. Authors propose a contrastive component to distinguish similar/dissimilar time sires and a domain adversarial component to ensure the embeddings are context invariant. \nGenerally, I think the studied problem is relatively new and the model design contains novel ideas. However, I also have the following concerns:\n1. Although the overall idea and the whole concepts of each component are very clear, some details of the proposed method are not easy to get. For example:\n* What is one training example feed into the network ($N \\times T$ matrix contains both internal and external series or just one series?)? \n* How to deal with the input with different resolutions? \n* What is the extract calculation of $L_r(W, U, a, b, x, y)$ in Eq. 4? What does it mean “The first loss term leads to good predictions of the exogenous variables shifted with respect to the embedded signals”?\n2. The justification for identifying representations of time series that are invariant to the exogenous variables is not enough. Especially, why this is important and necessary?\n3. While the experimental results seem good, they are not very strong and supportive to the motivation. First, the experimental results are not better than the baseline supervised methods. I know it’s reasonable since you do not use labels to train the embedding part, but the classifier is still trained with labels. For these small-scale datasets, I doubt the representative ability of embeddings since the classifier may influence the performance hugely. Second, the most attractive point of self-supervised learning is the transferability of learned features, which is not evaluated in this paper. Unsupervised learning features and then supervised learning the classifier can not support the method for anomaly detection applications. \n",
            "summary_of_the_review": "The paper studies a relatively new topic and the design contains new ideas in the specific area. However, some details of the model are not presented clearly and the experimental design could be improved. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}