{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper is concerned with fairness in the generative setting, specifically the setting in which various groups have very different sizes, and are therefore treated disproportionately by the model, with the group memberships further being unknown.\n\nThe reviewers generally agreed that the setting was interesting and important. However, they were critical of the writing quality, significance, and quality of the theoretical contribution.\n\nThe authors made significant improvements in the review period, and while these were not quite enough to satisfy enough reviewers, opinions clearly changed in a positive direction during the discussion period. Future changes motivated by the existing reviewer concerns should significantly improve the paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper focuses on the fair generative model problem. In particular, \"fairness\" lies in the balance of representation among groups in the data. The paper proposes to use TV as the distance metric in order to capture the difference between the generated distribution and the reference distribution. Empirical results are also provided.",
            "main_review": "Overall, the idea and approach are clearly presented. The experiment section also contains multiple different empirical studies. It would be very helpful if the authors can kindly clarify the following points:\n\n### q1: the utilization behind the distance metric w.r.t. the practical scenario described\n\nIn the abstract, if I understand it correctly, the practical problem that motivated the study is the unavailability of demographic group labels. It is not entirely clear to me how the technique (of fair generative model w/ the TV distance metric) can help solve this practical issue. Please kindly let me know if there is any misunderstanding.\n\n### q2: regarding Theorem 1\n\nWhile I understand the content of the theorem itself, I am wondering if the denotation of \"(Nowozin et al., 2016)\" is trying to indicate the fact that Theorem 1 is already proposed in the referred paper. Please kindly clarify this as this theoretical result seems to be an essential part of the contribution (apart from the empirical findings).\n\n### q3: regarding experiments\n\n- Table 2, Baseline II, 2.5% reference set size: I am wondering if the authors could kindly share some insights regarding why the fairness metric is so small (seems to be even better than the proposed approach in all listed cases).\n- I am wondering if the TV measure has anything to do with the sample size of the reference set. With a very big size difference (e.g., 1% reference set size), how can we justify that TV distances are well-estimated?\n- Figure 3 shows the generated images, both in terms of the representation balance and the quality. I am wondering if the authors can share some insights regarding why the proposed approach can (1) generate better-quality images (2) use weaker supervision (smaller reference sets) at the same time. Is it all about the TV metric, which does not seem to be the case based on the msg conveyed by Table 3, or something else?",
            "summary_of_the_review": "The paper proposes to utilize TV distance in the task of fair representation learning. The empirical experiments suggest a strong performance. It would be very helpful if the authors can kindly comment on the questions in the `Main Review`, so that the significance of the results can be better appreciated.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies fairness in generative models. When the training data is biased, it may result in imbalanced generated samples. They assume that they have access to scarce reference samples that are balanced.\nThey measure the unfairness of the model through total variation and propose a fairness-aware model that is regulated by total variation distance between the generated samples (biased) and reference samples (unbiased). The authors further assume that the sensitive attributes are not available. Finally, they provide numerical experiments to demonstrate the performance of their proposed framework. ",
            "main_review": "Even though the paper studies an interesting topic and tries to address a vital problem, in its current form the paper is not ready to go under revision. The paper is poorly written. The authors may find below my major and minor comments.\n\n## Major Comments:\n\n1) The optimization problem in~(6) is not a three-player game. Two players that are maximizing are collaborating. Therefore, this is still a 2-player game. It would have been a 3-player game if the optimization problem was in the form of min-max-min, which is not the case.  \n\n2) Why does the TVD regularization offer the best trade-off performances in fairness and sample quality compared to other divergence measures?\n\n3) Algorithm 1 proposes to use stochastic gradient descent with mini-batch. Are the gradients of the objective are unbiased?\n\n4) I am not sure how does reference dataset is collected in real-life? How one can ensure that the reference dataset is balanced in real-life?\n\n5) I do not fully understand why increasing the size of the reference set has a detrimental effect on the performance of the framework?  Can you provide more additional insights into the explanations in Appendix C.2?\n\n## Minor Comments:\n\n1) I am not sure what does the first sentence of the contributions section means. Which key benefit feature is mentioned in~Roh et al.?\n2) The paper is not self-contained. What is the attribute classifier introduced in Choi et al. (2020)?\n3) The first paragraph of Section 3.1 is very badly written. I would suggest the authors improve the English of this sentence. ",
            "summary_of_the_review": "This paper is not ready to go under review. I think this paper can be further improved in terms of writing. Once, it is written at an academic level it can go under review. I would suggest the authors improve the presentation of their paper. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "In their paper, the authors propose a novel generative adversarial network (GAN) which, more than realistic outputs, promotes fairness in the generative distribution. Specifically, when the training dataset is trademarked by some _unfair_ property (the authors' example being unevenly distributed gender or ethnicity), their proposed method-- a \"TVD based\" approach, which I will call TVD-GAN going forward --will rectify this in their generative distribution. The generative distribution is evaluated using an $L_2$ norm between the likelihoods of some samples given a reference distribution and the generative distribution ($\\mathbb{P}_{\\text{ref}}$ and $\\mathbb{P}_G$, respectively in eq.(1)).\n\nTVD-GAN minimises the \"total variational distance\" (TVD), through regularisation in eq.(5). As claimed on p.4, TVD \"offers the best trade-off performances in fairness and sample quality compared to other divergence measures\", as the results indeed seem to suggest. The main goal-- I believe --is to provide a more realistic sample using the larger training dataset, while still maintaining a fair distribution using the smaller reference dataset.",
            "main_review": "I have enjoyed reading this paper. It is clearly written (though some additional polishing would not hurt), the goal is clearly defined, and the method seems to solve it. However, there still remain some questions from my part which I list below.\n\n## Questions\n**[Q1]** Compared to related work, like Choi et al. (2020), the authors do no longer need a classification model (for training). This is a clear step forward from existing work. However, the authors _**do**_ need a \"reference distribution/dataset\". _Is this not a stricter assumption than needing a classification model?_ \n\nI can imagine having a dataset, unlabelled, which I would like to evaluate on its fairness. If I want to quantify exactly how unfair the dataset is, I would need some kind of label to provide a metric of unfairness (which is why the authors still need a classification model to evaluate their model). Using these labels, I can much more easily identify where my data is unfair. For example, if there is a gender discrepancy, a gender-label would indicate this. Using TVD-GAN, we have to assume that the reference dataset does indeed solve for this discrepancy. However, what if besides gender, there is also an ethnic discrepancy? Can I further assume the reference dataset to be fair? Minimising the distributional difference between $\\mathbb{P}\\_{\\text{ref}}$ and $\\mathbb{P}\\_G$ promotes fairness implicitly, which not only requires additional assumptions about $\\mathbb{P}_{\\text{ref}}$, but it also seems much less convenient.\n\n\n**[Q2]** The latter brings me to my second question. _How does this approach relate to domain adaptation and transfer learning?_ The setup detailed in this paper seems very much related. Mainly we wish to learn on one dataset ($\\mathcal{D}\\_{\\text{bias}}$), with a better performance on a second dataset ($\\mathcal{D}\\_{\\text{ref}}$). While there is no explicit prediction target, learning from one \"domain\" to increase performance in another, seems very much related. Would it be possible to use methods from these fields (e.g. see Table 1 in Wang and Deng (2018) for an overview)?\n\n\n**[Q3]** There is much work on fairness and generative models. While I don't want to be _that_ reviewer, I do believe some comparison is useful. In particular: FairGAN (Xu et al. (2018)), CFGAN (Xu et al. (2019)), DECAF (van Breugel et al. (2021)), FairGen (Tan et al. (2020)) (which is cited, but not compared against like with Choi et al. (2020)).  \n\n## Minor questions\n**[MQ1]** In Eq.(5), you introduce a hyperparameter, $\\lambda$. Essentially, $\\lambda$ controls how fair the eventual generative distribution should be. Higher $\\lambda$ indicates to regularise more towards $\\mathbb{P}\\_{\\text{ref}}$, whereas lower $\\lambda$ steers the optimisation more towards $\\mathbb{P}\\_{\\text{bias}}$. How does this affect performance (wrt fairness, and FID)? \n\n**[MQ2]** This remark is more a subjective remark, mainly because it concerns Fig.3, which in itself is an almost subjective result. The split between female/male in Fig.3 is quite close between Choi et al. and TVD-GAN. While this in itself is not an issue, it may become one if the generated samples are also quite close to the classification threshold between male and female. Numbering columns as c1, c2, ... and rows as r1, r2, ...; The bottom set includes samples like (r4, c1 -> male), or (r3, c15 -> female), (r4, c5 -> male), (r5, c15 -> male) that could equally well be classified with the opposite gender, making performance the same/worse as Choi et al. I wonder whether tuning $\\lambda$, more to $0$ would decrease ambiguity of samples (and how much fairness would suffer as a result)?\n\n**[MQ3]** Perhaps somewhat related to **[Q2]**, but would it not be easier to train only 1 discriminator? First on the biased data, and when converged on the reference data? This would not only make TVD-GAN composable with different notions of fairness (of which there are many), but it would also allow for any trained model to be adjusted when other unfairly distributed attributes need to be dealt with.\n\n\n_I wish to end my main review by stating that I look forward to discussing further with the authors and reviewers of this paper. My review, nor my score, is set in stone._\n\n### Additional references\n\nMei Wang, Weihong Deng, _Deep visual domain adaptation: A survey_, Neurocomputing, Volume 312, 2018, Pages 135-153, ISSN 0925-2312, https://doi.org/10.1016/j.neucom.2018.05.083. (https://www.sciencedirect.com/science/article/pii/S0925231218306684)\n\nDepeng Xu, Shuhan Yuan, Lu Zhang, and Xintao Wu. _FairGAN: Fairness-aware generative adversarial networks._ In 2018 IEEE International Conference on Big Data (Big Data), pages 570–575. IEEE, 2018.\n\nXu, Depeng, Wu, Yongkai, Yuan, Shuhan, Zhang, Lu, & Wu, Xintao. _Achieving Causal Fairness through Generative Adversarial Networks._ Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, (2019). \n\nBoris van Breugel, Trent Kyono, Jeroen Berrevoets, Mihaela van der Schaar. _DECAF: Generating Fair Synthetic Data Using Causally-Aware Generative Networks_ In Neural Information Processing Systems (2021)\n\n\n\n\n",
            "summary_of_the_review": "The topic handled in this paper is important, and the contribution is interesting. There are some remaining questions I have regarding the paper in general (listed in my main review above). Given that there are some points the authors have to clarify further, concerning their method as well as related work, I believe a score of 5 is correct at this stage. \n\nHowever, I do believe the points I raise can be handled during a rebuttal period. Hence, I look forward to discussing more with other reviewers and the authors of this work. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns"
            ],
            "details_of_ethics_concerns": "I am not an ethics reviewer. This paper provides a method that concerns with Fairness directly. While I do not believe there to be major issues, I believe it would be useful to also have a reviewer (who is more familiar with the ethical aspect of fairness) look at this work. ",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a new generative model with “fairness” constraints by adding a total-variation regularization. The authors conduct several experiments and compare them with benchmark methods. ",
            "main_review": "Overall, the paper is well written and easy to follow. The problem of studying fairness in generative models is significant. This paper also conducts an interesting approach by using total variation for regulating “fairness”. However, I have the following major comments.\n\n1. The definition of fairness adopted in this paper is unclear to me and needs further discussion. In particular, I have the following main concerns. \n\n(1) The definition relies on an attribute classifier. What if this classifier itself is biased? \n\n(2) In the same vein, it seems that generative models may produce unrealistic images which are wrongly labeled as female/male by the attribute classifier. How to deal with this situation?\n\n2. My second concern is the comparison with benchmarks in the experiments. In Table 1, the proposed method seems to have a higher discrimination score than Choi et al when the reference size = 25%. Hence, it is unclear if the performance improvement in this paper is significant. \n\n3. The authors observe that “the fairness performance exhibits slight degradation with an increase in the reference set size” in the experiments. However, the explanation given in Appendix C.2 is unclear to me. \n\n4. The authors compare the total-variation-based regularization with other regularization techniques through experiments and demonstrate the results in Table 3. Can the authors provide more implementation details? In particular, which ground metric (i.e., cost function) do they use for the Wasserstein distance? I guess a typical choice is the L2 loss but in the setting of measuring distance between facial images, L2 loss does not seem to be an optimal solution. There are a number of works on ground metric learning for the Wasserstein distance. I wonder what the author did.\n\n#####################\nAfter rebuttal\n#####################\nI would like to thank the authors for providing a detailed response to my previous comments. However, I still find the fairness definition adopted in this paper a bit strange, especially because it relies on a pre-trained attribute classifier. Hence, I will keep my score unchanged.",
            "summary_of_the_review": "The definition of fairness adopted in this paper needs further discussion.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}