{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper proposes to use evolutionary methods to learn auxiliary loss functions, demonstrating superior performance vs. typical auxiliary losses previously proposed in the RL literature.\n\nDemonstrating that it is possible to learn auxiliary losses by evolution, both for pixel and state representations, that help train significantly faster (even on new environments) is definitely a meaningful contribution, as acknowledge by the majority of reviewers.\n\nAlthough many of the original reviews' concerns were addressed by the authors during the discussion period, two major ones were only partially answered, both related to the limited empirical evaluation of the proposed approach (which is crucial for such a contribution that aims to demonstrate an improvement over existing related techniques):\n1. The limited set of environments used for evaluation (and in particular the lack of partially observable environments)\n2. The fact that the baseline being compared to was CURL, which the paper describes as \"the state-of-the-art pixel-based RL algorithm\", while reviewers mentioned DrQ and RAD as two more recent (and better) algorithms that were known well ahead of the ICLR submission deadline (note that the more recent DrQ-v2 is now even better). Since the data augmentation techniques used by these algorithms help shape the internal representation, like auxiliary losses do, it would have been important to validate that the proposed technique could be useful when plugged on top of such baselines.\n\nThe authors did try their best to address these major concerns during the rebuttal period, but the discussion between reviewers and myself came to the conclusion that this wasn't quite convincing enough yet. I encourage the authors to investigate these points in more depth in a future version of this work so as to make the empirical validation stronger (NB: the links provided in the last comment by authors on Nov. 30th didn't work, but this wasn't the main factor in the decision)."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, the authors intend to automatically search for the optimal auxiliary loss. To achieve this goal, the authors propose Automated Auxiliary loss for Reinforcement Learning (AARL), which uses an evolutionary search strategy to explore the very large loss space. The paper conducts extensive experiments on the DeepMind Control Suite and shows that the searched auxiliary losses have significantly improved RL performance in both pixel-based and state-based settings.\n",
            "main_review": "Advantages:\n1. The motivation of automatically searching optimal auxiliary loss is good. Handcraft loss function highly depends on researchers' domain knowledge, and its performance can not be secured. The automatic search strategy can significantly reduce locality and improve the RL agent's performance.\n2. The design of auxiliary loss function search space, as well as the evolutionary pipeline, is reasonable. The pruning of search space can greatly help to reduce search complexity.\n3. The experiments are extensive and supportive. The critical experimental parameters are provided, and thus there should be no issues with the repeatability of the experiments.  \n\nDisadvantages:\n1. My major concern is about the efficiency of the proposed method AARL. The overall loss function is a combination of reinforcement learning loss and evolutionary loss, both of which are difficult to optimize. The combination of RL and evolution would be even more difficult to converge. What's more, the efficiency could be very low, as the difficulty of convergence would not be a plus, but be a multiplication of RL and evolution.\n2. The combination of RL and evolution could be improved. Equation (1) optimizes the two algorithms by a simple weighted sum of the two loss functions. A more practical solution is to alternatively train the two losses. There are two reasons for doing so: 1). The scalabilities of the RL loss, as well as the auxiliary loss, are different, and it is difficult to confirm the weight \\lambda; 2). The required training steps for the two algorithms may differ.\n3. The contributions are not significant. The major technical contribution is to design a search space and an evolution strategy to derive an optimal auxiliary loss. And also, there is no theoretical justification for the proposed method.",
            "summary_of_the_review": "This paper has good motivation, a good solution for the problem, and extensive and supportive experiments. The major concerns are low efficiency, convergence difficulty, and non-significant contributions.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces a method for searching for the best auxiliary loss function from a huge search space automatically, while the best auxiliary loss function is defined as the one that encourages the agent to get a higher return. The searching space of auxiliary loss functions is finite and discrete. It is a combination of (1) manually designed similarity measures, which is used for encouraging the prediction and target to be similar in the auxiliary task, and (2) the binary mask, which is used for selecting auxiliary loss inputs. In this work, the size of the search space is around $4.6 \\times 10^{19}$. The space is pruned firstly by random sampling from the similarity measure space and choosing the one showing the highest averaged performance, then the evolutionary algorithm is applied to select top candidates for loss inputs, which is a method in the literature. The chosen auxiliary losses are empirically shown to be helpful with increasing the learning efficiency and have generalization ability to new tasks.\n",
            "main_review": "The paper shows the following advantages:\n\n(+) A method automatically selecting the auxiliary loss function is introduced in this paper. It avoids the hassle of manually designing and testing auxiliary loss functions, as well as provides a more efficient search method than grid-search when dealing with a huge search space of the auxiliary functions.\n\n(+) The experiments are done with either 5 or 15 seeds, providing reasonably reliable results, although more seeds could improve the accuracy of evaluation even more.\n\nThere also remain several concerns:\n\n(-) I am concerned about the accuracy of the first pruning step---randomly sample from the search space, then choose the similarity measure with the best performance which is averaged over 15 random seeds. I agree that this method is more computationally efficient than grid search. However, there exists a trade-off between computational efficiency and the accuracy of searching. There is no guarantee that a good candidate can be sampled in the random sampling step. The sampled mask for loss input may not be the best combination of the chosen operator (similarity measure). Thus, it introduces the risk that the best candidate is never selected and evaluated. It might be better to give an analysis on how large this risk is or the probability this situation happens.\n\n(-) In the experimental result section, I am not convinced by the result of Hopper-Hop and Quadruped-Run. With the chosen auxiliary loss function, there exists a high chance for the agent to perform worse than the SAC-DenseMLP baseline---the averaged value of the violin plot is slightly above the SAC-DenseMLP performance, leaving a lot of auxiliary loss function candidates performing worse than this baseline. This makes me worry that the risk of choosing auxiliary loss seems high. \n\n(-) The paper claims that the chosen auxiliary loss can generalize to unseen tasks. But it is not clear to me where this generalization ability comes from: the auxiliary loss selection method itself, which is a searching process, or the setting of training on multiple different and challenging tasks. Multi-source training itself can prevent overfitting. Therefore, if multi-source training is not part of the framework, it may be worth checking how the different number of training environments affects the generalization ability, especially in the case when there is one training task only.\n",
            "summary_of_the_review": "I consider the weakness to outweigh the strength. This work does provide an interesting idea, which is selecting auxiliary loss functions automatically from a huge search space. The empirical results suggest that the average performance of applying the chosen auxiliary task is higher than baselines. However, the risk of using this method seems high---at the first step of pruning the search space, there remains a high chance that good candidates are never tested, thus the pruning result can be based on the evaluation of bad combinations of operators and inputs. Moreover, the experimental result also suggests that this risk exists.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Reinforcement learning agents can benefit from auxiliary tasks. This paper introduces an evolutionary algorithm to automate the design of auxiliary loss functions, based on prior approaches like forward dynamics, inverse dynamics, contrastive state representation learning, etc. The evolution solves a bilevel optimization problem in which the inner loop is regular RL training while the outer loop evolves the loss function. Experiments are conducted on the Deepmind Control Suite for both pixel and state-based observations. ",
            "main_review": "**Strengths**\n* This paper is overall clearly written and easy to follow. The method is easy to understand and relatively straightforward. There are enough details to fully reproduce the training. \n* To my knowledge, evolving the optimal loss function for auxiliary RL tasks seems to be a novel approach. The central thesis of better representation learning in RL is a problem of good practical value.  \n\n**Weaknesses**\n\nMy major concern is the weak experimental results. To elaborate: \n\nFor pixel-based Deepmind Control Suite, the strongest baseline that the authors compare to is CURL (Laskin et al., 2020). However, this is quite an outdated baseline. The authors *ignore at least 2 recent strong baselines*:\n\n* Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels. Yarats et al. ICLR 2021 Spotlight. https://openreview.net/forum?id=GY6-6sTvGaf. The algorithm is known as \"DrQ\"\n* Reinforcement Learning with Augmented Data. Laskin et al. NeurIPS 2020. Also known as \"RAD\". \n\nBoth papers (neither cited in the paper) are published on top ML conferences before June 2021, so it is fair to request comparison with these prior SoTAs per the ICLR review guideline. Both papers above are about a very simple idea - vanilla reinforcement learning with simple data augmentation can be an exceptionally strong baseline. In fact, if we compare table 1 of the \"DrQ\" paper (Yarats et al.) with table 3 of this paper, we will see that DrQ *beats AARL-Pixel in 7 out of 12 tasks. For the other 5 tasks, none of the gains of AARL-Pixel is statistically significant.*\n\nThis indicates that even **simple RL with image augmentation can outperform the complicated bilevel optimization and auxiliary loss in AARL**, which greatly undermines this paper's contribution. \n\nFurthermore, in section 3.1, Fig. 4, the author does mention \"SAC with data augmentation\", which appears to use the same scheme as \"Reinforcement Learning with Augmented Data. Laskin et al.\" (RAD). However, SAC + augmentation (blue dashed line) consistently underperforms CURL (blue dotted line) in Fig. 4, which contradicts the results in RAD. In addition, the numerical results indicated by the lines also disagree with Table 1 in the RAD paper. I believe there are factual errors in these plots. \n\nFor state-based DMControl experiments, the paper claims in section 3.2 that \"there is no data augmentation in the state- based setting.\" This is not true. Section 5.4 in the RAD paper discusses simple ways to augment low-dimensional states, and shows that they are highly effective in boosting performance. \n\nMinor comment: table 1 has a misnomer. \"Inverse dynamics\" typically means action inference, instead of predicting the previous state. ",
            "summary_of_the_review": "The paper ignores at least 2 simple but important prior works, and do not outperform the results in those baselines. The contribution is greatly undermined by the fact that simple RL with image augmentation can outperform the complicated bilevel optimization and auxiliary loss in AARL. Some experiment plots may contain factual errors. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper introduces an approach to automatically discover optimal auxiliary loss for RL, the method is called (AARL). The authors claim that AARL outperform baselines on both pixel-based and state-based task on the DeepMind Control Suite. Also using their method the author analyse different auxiliary losses to identify common patterns.",
            "main_review": "Pro:\n- The paper is well written an easy to follow\n- Error measures are present in both the figures and the table report \n- The Appendix is well written and has many details\n\n\nCons:\n- In several parts of the paper, the authors claim that AARL discovers the “optimal auxiliary loss function for RL”. I think this statement is not supported by any mathematical derivation that proves the optimality. Even if we decide to forget about a proper mathematical derivation of optimality there is still the important issue that the authors only considered one domain: the DeepMind control suite. This is clearly not enough to support the broader claims made by the authors, in the abstract, introduction and conclusion.\n- Another issue of this approach is that the search space is done on the loss input (I) and the loss operator (f), but the encoder is kept constant. Changing the architecture of the encoder can have a massive impact on the auxiliary loss, especially the one that requires rollouts. For instance, would the conclusion hold if the CURL encoder is substituted with a Transformer? The authors have an ablation on Encoder Architectures in section 3.4, but it’s limited to the state-based case, which is less interesting than the pixel based one. I believe this is an important limitation as it could drastically change the conclusion of the paper.\n- Another significant limitation of using just the DeepMind control suite is that the environment is fully observable, at least in its normal implementation. I assume this is the case also for this paper as I don’t see any details in the main paper or in the appendix that go against this assumption. Claiming generality of a method based on conclusions drawn just on a fully observable environment is very dangerous. One known example of this is actually the action prediction auxiliary loss reported in the paper {s_t, s_t+1} -> {a_t}. Indeed, it is well know that this kind off loss works well in fully observable 2D environments, but it could have severe issues in partially observable 3D environments due to the increase state aliasing problem (e.g. Badia et al., 2020).\n",
            "summary_of_the_review": "The authors combine different AutoML techniques to automatically derive the best auxiliary loss function for RL. Although AARL is an interesting approach, the claim in the paper are not supported by empirical findings. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "-",
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}