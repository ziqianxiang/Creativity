{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper develops kernel functions in Banach spaces. However the results seem to be preliminary and further development is needed before\nthe manuscript can be published. Reviewers point out several errors and also author/authors have graciuously agree with the suggestion \nthat they will incorporate all the feedback in future submissions."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proves a Representer Theorem for the composition of functions from Reproducing Kernel Banach Spaces (RKBSs).",
            "main_review": "This submission looks more like a working draft rather than a conference paper. In particular,\n\n**There are many typos**, making the reading very difficult (I am highlighting a few of them, only on page 1):\n- which *have* their own limitation (abstract)\n- that uses *functions* from Reproducing Kernel Banach Spaces (abstract)\n- and *test* it on a range on ML tasks (abstract)\n- the acronyms MLMKL, DGP, DKL are never introduced (abstract and later)\n- *W*e are still (1st paragraph)\n- *kernel from Hilbert Space* makes little sense (1st paragraph)\n- There has *been* (1st paragraph)\n- *W*e then examine (2nd paragraph)\n- *two layer RKBS kernel* makes little sense (2nd paragraph)\n- the connect*ion* with multiple (2nd paragraph)\n- *W*e also talk about *the* library (2nd paragraph)\n- the bibliography entry [Scholkopf and Smola] seems wrong\n\n**The maths are not rigorous**, to the extent they are almost incorrect:\n- the introduction of RKBSs is very confusing, as the proposed definition looks more like the RKHS definition to me, see e.g., Definition 4.18 in \"Support Vector Machines\" by Steinwart and Christmann, 2008\n- Theorems 2.1 and 2.2, that are not part of the contribution, should be explicitly linked to the paper they come from\n- undefined notation in eq. (2.9)\n- I would like to point out the paper \"Autoencoding any Data through Kernel Autoencoders\" by Laforgue et al., 2019 which considers a similar framework as the one introduced at the top of page 3. A Representer Theorem is also proved therein, as in [Bohn et al. 2019], except that it applies to potentially infinite dimensional output Hilbert spaces\n- the introduction of the $\\Theta_l$ is not understandable\n- In section 3.2 the proof is said to be given in the Appendix, but the latter is empty\n\n**The contribution is very limited**:\n- the Representer Theorem (RT) being mainly due to orthogonality properties, it is not surprising to recover it for a class of functions that share this property with RKHSs\n- I cannot see any novel idea in the proofs, so proving another RT seems not a sufficient contribution for acceptance\n- I cannot find any motivation for the presented results\n- the authors themselves acknowledge that their experimental contribution is very limited",
            "summary_of_the_review": "This submission looks more like a working draft rather than a conference paper.",
            "correctness": "2: Several of the paperâ€™s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper presents a means of constructing deep (concatenated) reproducing kernel Banach space kernels.  The main contribution appears to be a representor theory for concatenated Banach space kernels.  Some experimental results are provided to accompany this result.",
            "main_review": "While I like the idea underlying this paper I found the paper somewhat unsatisfying.  Specifically:\n\n- unclear/lack of attribution: particularly for the background material, several theorems and definitions are taken almost word-for-word from Lin et al, 2019b.  For example the definition of RKBS corresponds to definition 1.1 in Lin et al, the feature-map construction is the boxed text on page 4 of Lin et al, and theorem 2.1 is the same in both this and Lin et al.  While I understand that re-wording is often tedious, at the very least a simple parenthetical citation (for example \"Theorem 2.1 (see theorem 2.1 in Lin et al): ...\") seems to be called for.\n\n- Related to the above, theorem 2.2 uses notation $A^\\vdash$ that isn't (as far as I can tell) introduced at all in the submission, which makes understanding somewhat difficult.\n\n- Continuing on the question of notation, what is $e_{k_2}$ in section 3.1?  I'm working on the assumption that this is some form of basis vector set (after all this is vector-valued regression) but this needs to be clarified.\n\n- Perhaps I am wrong, but (3.13) looks like it would, in practice, act like an RKBS with a N hyper-parameters and \\mathcal{O} (N^2) computation time.  I am curious how this affects the performance of an algorithm using such a kernel - in my experience kernels this complicated can have a severe effect on training and evaluation times, particularly for large datasets!\n\n- The experimental section is unsatisfactory.  A single experiment is described, but the size of the training set is not specified.  50 trials are completed, but no error bars are included.  A link to code is given but the algorithm is not described and training issues (training times etc) are not discussed.\n\nTo be clear I think this avenue of research is fascinating and potentially important, but I feel that the deficits of the paper need to be addressed first.",
            "summary_of_the_review": "- The premise is interesting.\n- The clarity of attribution in the background section needs to be worked on.\n- \"Niche\" notations used but not defined.\n- Experimental results on a single dataset without important details (training set size, training times, details of training algorithm etc0.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes to use kernels from a reproducing kernel Banach space, instead of a reproducing Hilbert space in tasks such as deep kernel learning.",
            "main_review": "There isn't a clear division between a background and contributions section, so it is difficult to see the distinction between the background and the contributions.  Nonetheless, it appears that the only contributions are to notice that deep kernel learning can be understood within their framework (Sec. 3.2) and to do a toy experiment on 2D data (Sec. 4).  This is simply not sufficient for a paper at a major machine learning conference, so I am forced to recommend rejection.",
            "summary_of_the_review": "Very limited theoretical and empirical results.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}