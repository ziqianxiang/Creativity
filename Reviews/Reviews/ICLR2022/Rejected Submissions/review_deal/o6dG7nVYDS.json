{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper provides a learning theoretic account of domain generalization in which domains themselves are treated as data, generated from some domain generating distribution. All of the reviewers were positive about this approach and found it interesting. There were, however, a couple of critiques raised by reviewers that lead me to recommend that it is rejected:\n\n- the theory provided in this paper does not remotely apply to the datasets that are used in the experiments. While, I agree with one of the author responses that DG benchmarks exist with many domains, DomainBed has very few domains, and it's not clear that their theory is a remotely satisfactory account of the experimental results presented in the paper.\n- Despite some back and forth on the wording and positioning of the paper, I think the writing still does not give enough credit to worst-case analyses of DG."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "In domain generalization, recent work has shown that ERM has comparable out-of-domain accuracy to state-of-the-art DG methods. The paper aims to explain this result through model complexity. The main argument is that ood generalization requires a smaller model complexity. The upshot is that DG methods can utilize cross-domain validation to obtain better generalizing models.",
            "main_review": "**Strengths**\n1. Generalization bound for DG setting\n2. Empirical experiments support that ood generalization requires smaller model complexity\n3. Propose cross-domain validation as a metric to optimize DG models \n\n**Weaknesses**\n1. No discussion on the size of n in theory section. Usually n is very small. \n2. Overclaiming model complexity as the main reason explaining the DG results.\n\nI feel that it is a good paper shedding light on ill-formed DG problem. I summarize my feedback for the authors below:\n\nThe generalization bound is a nice characterization to show that we may need additional regularization for OOD generalization. It may be good to discuss this relative to other papers that have made similar claims about regularization (e.g., causal regularization by Janzing https://arxiv.org/abs/1906.12179). The conclusion on regularization does not seem a novel idea to me, but I do like the theoretical rigor to support it.\n\nThe discussion after Theorem 2, however, misses the point of what empirical domain generalization is about. With enough diverse domains (e.g., 100-1000), of course any method (including ERM) would be good. But the tasks typically have 4-5 domains which makes generalization harder. Given extreme domain shift, DG methods can easily outperform ERM (see e.g., Arjovsky et al. IRM paper, or Mahajan et al. MatchDG paper). The only issue is that in the DG benchmark datasets (and many real-world settings with small domain shifts), ERM seems to perform better. So while Theorem 2 may be true, I do not think it has much of an insight for empirical DG. Perhaps the point that authors want to make is that ERM would work well for all settings, while a DG algorithm would work well in some datasets, but not all (based on its assumptions).\n\nI appreciate the empirical results. Nice formulation to use linear models over pre-trained representation, that helps to confirm the model complexity hypothesis. For Figure 2, however,  I would like to caution that the results only show that model complexity is _one_ of the factors that contribute to  ood accuracy. It is no surprise that ood accuracy increases up to a certain model complexity and then decreases. I would suggest the authors to stick to the main claim (ood gen requires smaller model complexity than iid gen). The claim that complexity determines the OOD generalization is not supported by the experiments. Are the authors saying that a more complex model can _never_ achieve higher OOD generalization? That would be a very strong statement.\n\nI would also tone down section 3.3: \"solving domainbed\". All the numbers reported are lower than state-of-the-art. The paper just compares a linear model with different validation schemes. \n\nIn theorem 1, did not see a derivation for the last term O(sqrt(ln(1/\\delta)/n) in the proof. How is that derived?\n",
            "summary_of_the_review": "Good paper explaining the recent empirical results against DG algorithms",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a theory of domain generalization based on statistical learning theory (Rademacher complexity) and demonstrates a trade-off between training loss and model complexity. They show that existing methods are actually controlling the model complexity. Based on the analysis, the authors argue that proper model selection is critical for complexity control. Instead of hyper-parameter search, they propose to use domain-wise cross-validation as the model selection strategy. Experiments on the DomainBed benchmark show the effectiveness of the proposed method.",
            "main_review": "# Strong point\n\n- Modeling domain generalization performance from a domain-complexity view is novel. The paper shows an interesting trade-off between model complexity and training loss in a derived upper bound.\n\n- Extensive experiments showcase the validity of the theory. A better model selection strategy is proposed.\n\n- It is nice to see that the optimal regularization of across-domain acc is consistently larger than the one of within-domain acc in Fig (1).\n\n# Weakness\n\n- The demonstration of the trade-off in Theorem 1 is nice. The results of the arguments rely on concentration bounds such as McDiarmid's inequality, which is typically used for large $n$. However, in reality, the number of training domains is small, e.g., 4~5 training domains. The bound seems vacuous when $n$ is small.\n\n- Another caveat is that theorem 1 bound the average risk w.r.t some environmental prior $\\mathcal{E}$. One would imagine that the average risk is dominated by the risk of environments in the high probability regions of the prior. The model can exploit the environmental-specific feature in those environments regardless of loss of the environments in the low probability region, which is the motivation of many existing DG works. \n\n- According to Eq (16), it seems that the Rademacher term $\\mathcal{R}_n(F)$ is the Rademacher complexity but not empirical Rademacher complexity. In addition, the notations are confusing. The underlying joint distributions for (x,y) are different for the two Rademacher complexity terms in Eq (7).\n\n- It seems that there is no clear trait of the trade-off in Fig (2). One interesting observation is that IRM has the smallest model complexity and held-out domain accuracy in the baselines, which contradicts existing works. [1] theoretically shows that it is easy to fit the IRMv1 objective while behaving like ERM on held-out domains. The empirical results in [2], sec 3.2 also suggests that the IRMv1-trained model behaves like the ERM-trained model. Is such discrepancy caused by disparate architectures (MLP versus ResNet)? Also, the held-out accuracy in VLCS datasets of ERM models seems to decrease with model complexity.\n\n\n# Minors \n\n- Across-domain accuracy and held-out domain accuracy are used interchangeably.\n- What is ``instance-wise\" model selection? The average loss of data instances in training domains?\n\n[1]: Elan Rosenfeld, P. Ravikumar, and Andrej Risteski.  The risks of invariant risk minimization. ArXiv, abs/2010.05761, 2020\n\n[2]:Yilun Xu and T. Jaakkola. Learning Representations that Support Robust Transfer of Predictors. ArXiv, abs/2110.09940, 2021",
            "summary_of_the_review": "\n\nTo summarize above, I find the theoretical results are somewhat unsatisfying due to limited domain numbers in practice. The experimental parts report results on small models, which contradicts some existing results on larger ones. All in all, I find the paper in its current form to be somewhat below the standard for ICLR acceptance, unless the authors address some of the points above.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper considers the problem of domain generalization (DG), wherein predictors are trained on a related set of training domains and evaluated on an unseen test domain.  The authors first present a learning-theoretic bound on the performance of an average-case formulation for DG, and then present a set of experiments that consider the trade-off between complexity and out-of-distribution (OOD) performance.  The experiments indicate that such trade-offs exist for linear models, and there is also evidence that the trade-off persists for shallow neural networks.",
            "main_review": "### Strengths\n\n**Problem setting.**  The setting is well-motivated, in that recent results have indicated that most DG algorithms cannot surpass the performance of ERM.  New theoretical insights are needed to understand why this happens, and indeed this work tries to fill this gap.\n\n**Insights from Thm. 1**  The idea of treating the environments themselves as data is interesting, as it allows the derivation of this kind of bound which is agnostic of the form of $\\mathcal{E}$.  I feel that the authors may consider taking this analysis further, as there may be new insights if e.g. $\\mathcal{E}$ has some particular structure, or when the hypothesis class is linear (or has some structure).  In any case, Thm. 1 seems to be a nice starting point for this work.\n\n**Interesting insights regarding accuracy vs. complexity.**  I quite liked the experiments in Section 3.1.  The takeaway seems to be that \"the optimal regularization for [DG] is stronger than for [supervised learning].\"  This indicates that the complexity of DG methods needs to be controlled more tightly than it does for regular supervised learning.  So at least in the linear setting, it seems to be the case that there is a clear trade-off between accuracy and complexity WRT test-time generalization.\n\n### Weaknesses\n\n**Confusing notation.**  There were several places in the paper where the notation/word-choice was confusing.  One point of confusion was that the risk $R_p$ and the Rademacher complexity $\\mathcal{R}_m$ look rather similar. Perhaps the risk could be changed to $L_p$, which is the notation used in [Shalev-Shwartz & Ben-David, 2014].  The word-choice used to differentiate supervised learning from domain generalization was confusing.  If my understanding is correct, the former is referred to as supervised learning, seen-domain, and within-domain whereas the latter is referred to as novel-domain, cross-domain, and domain generalization.  Perhaps choosing just one term to refer to each of these would make the paper easier to read.  A further point\n\n**Typos.**  There are a number of typos.  Both \"regularizers\" (page 1) and \"regularisation\" (page 2) are used; the paper should be consistent in choosing the American or British spelling.  The dataset is Terra-Incognito (page 5).  \n\n**The formulation of the DG problem.**  The authors assert that the goal of DG is to solve the following optimization problem:\n\n$$ \\min_{f\\in\\mathcal{F}} \\mathbb{E}_{p\\sim\\mathcal{E}} [R_p(f)] $$\n\nwhere $\\mathcal{E}$ is a distribution over distributions $p$.  This is not the only formulation for domain generalization.  Indeed, some would argue that this is not the most standard formulation for the problem.  Another formulation was considered in [Arjovsky et al., 2019], wherein the authors advocate for the following \"worst-case\" problem:\n\n$$ \\min_{f\\in\\mathcal{F}} \\max_{p\\in\\Delta\\} R_p(f)$$\n\nwhere $\\Delta$ is something like the support of $\\mathcal{E}$ in the notation of this paper.  The reason I feel that this is important is that the authors make assertions like this:\n\n> \"any future DG method that claims to be better than ERM should...\"\n\nThese assertions are only relevant to the first \"average-case\" definition of the DG problem, and do not necessarily apply to other common formulations, e.g. the \"worst-case\" formulation shown above.  One way to remedy this would be to move the proofs to the appendix and to add a more detailed discussion of related work and of past formulations and approaches for DG.\n\n**Lack of sufficient detail.**  There were numerous places where I thought the paper could have benefitted from providing more detail.  Here are some questions I had while reading, all of which could be addressed by adding details:\n* What do the authors mean by the \"domain-shift phenomena?\"  Of late there has been quite a bit of work on domain shift in the sense of Section 1.8 in [Quiñonero-Candela, et al., 2009], which defines domain shift to be a particular form of covariate shift.  However, it seems that the authors may be using domain-shift to informally refer to the problem of domain generalization, wherein there can be arbitrary shifts in the joint distribution over (X,Y) from domain to domain.  Resolving this confusion would make the problem setting clearer.\n* Define what is meant by \"data in the wild\" (page 1).  \n* A recurring question I had was the following: What do the authors mean when they say that X is a **causal factor** in the success or failure of DG methods.  Is \"causal\" meant informally?  Or does it mean something more technical vis-a-vis the field of causal inference.  This term is used repeatedly, and after reading the paper several times, I still am not sure what it means.\n* The authors talk quite a bit about the \"performance variability\" (page 2) or \"erratic performance\" (page 1) \"unreliable\" performance (page 6) of existing DG methods.  It's unclear whether we should read this as \"poor performance\" or \"non-stable performance.\"  I find this distinction important because the results of [Gulrajani & Lopez-Paz, 2020] indicate that on the contrary, the collective performance of DG baselines is actually quite stable in the sense that they almost all algorithms do more or less the same as ERM.  Now to be fair, this performance isn't all that great, given that there is a very large gap between in-distribution and out-of-distribution performance of each of these methods.  Thus, I think that there needs to be a bit more subtlety in how the authors specify the weakness of the DG algorithms that they want to improve.  Additionally, I think that this perspective that ERM is optimal for this \"average-case\" is not new; [Rosenfeld et al., 2021] also advocates for this view, so it may be good to cite their work here.\n* At the start of the experiments, the authors say they will work with linear SVM.  But then, the authors say that they train linear SVC (which I assume is the classification analog of SVM, although it's never stated).  This should be clarified.\n* What do the authors mean by a \"dynamic range of accuracy\" (page 7)?  What is \"dynamic\" about it?\n\n**Unclear explanation of experiments.** I didn't understand the result sin Section 3.3.  What is the difference between instance-wise and domain-wise validation?  This is never explained from what I can tell.  Does this simply mean in-domain validation vs. cross-domain validation?  It's also unclear to me what claim in [Gulrajani & Lopez-Paz, 2020] the authors of this paper are disputing at the bottom of page 7.  From my understanding, [Gulrajani & Lopez-Paz, 2020] argues that DG algorithms should be responsible for specifying a hyperparameter selection criteria.  How do these results refute this claim?\n\n**Claims in the abstract.**  Some of the claims in the abstract are somewhat dismissive of past work.  They say that \"most of the [past work in DG] is empirical, as the DG problem is hard to model formally.\"  They go on to say that they do something more principled with a learning-theoretic analysis of the problem.  This dismisses the large and growing body of work that has sought to understand the theoretical underpinnings of the DG problem.  Indeed, many of the proposed algorithms for DG are past on concrete theory; the fact that they don't in general significantly improve over ERM is a separate issue.  Furthermore, in the end, the authors actually do something quite empirical, in the sense that they extract features using a deep classifier and look at the performance of small neural networks on DomainBed.  This to me seems no less \"empirical\" than many of the other works that have sought to study this problem.  So from my reading of this paper, the authors also fall into the pitfall that they claim to identify early on in the paper.  More than anything else, I think this is an issue with how the idea proposed in this paper is discussed, rather than being a critique of the proposed method itself.  So I would recommend softening these claims and expanding the related works section, given that there is a great deal of related work in DG that has been left out.\n\n**Formal-ness of the proofs.**  In my opinion, the proofs here are presented too informally.  The authors use terms like \"ghost sample\" without defining or describing them for the reader.  I think that eq(7) should be proved formally, rather than hand-waving over the argument (whether or not the authors feel it is trivial).  The same can be said for Thm. 2 (which I would call Corollary 2 rather than Theorem 2), for which the proof is more of a sketch.  \n\n**Softening some of the claims.**  The authors have a section titled called \"Solving Domain-Bed with Linear Models.\"  I think that this is misleading, as the authors are not \"solving\" DomainBed.  \"Solving\" would indicate that they found a way to achieve perfect domain generalization by generalizing OOD. \n\n### References\n\n[Shalev-Shwartz & Ben-David, 2014] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.\n\n[Arjovsky et al., 2019] Arjovsky et al. \"Invariant risk minimization.\" arXiv preprint arXiv:1907.02893 (2019).\n\n[Quiñonero-Candela, et al., 2009] Joaquin Quiñonero-Candela, et al., eds. Dataset shift in machine learning. Mit Press, 2009.\n\n[Gulrajani & Lopez-Paz, 2020] Ishaan Gulrajani and David Lopez-Paz. \"In search of lost domain generalization.\" arXiv preprint arXiv:2007.01434 (2020).\n\n[Rosenfeld et al., 2021] Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. \"An online learning approach to interpolation and extrapolation in domain generalization.\" arXiv preprint arXiv:2102.13128 (2021).",
            "summary_of_the_review": "On the one hand, I think that this paper offers an interesting perspective for thinking about DG.  The theoretical work gives a new way of thinking about the accuracy-complexity trade-off in DG, and this leads to some interesting experiments.  However, I think that the paper tends to overclaim, in particular with regard to \"solving DomainBed\" and the implications of its theory for ERM.  Furthermore, I think that the writing is particularly unclear, and that the claims made in the introduction are not fully validated (e.g. the claims about the \"causal\" factor behind poor performance in DG).  Overall, despite the interesting insights, I think that this paper is not yet ready, and so I recommend that it not be accepted.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper has several findings taking a bias-variance trade-off of DG performance, that explains existing DG algorithm performance variability. (ii) The complexity control strategy used to determine bias-variance trade-off is crucial in practice, with peak DG performance achieved when optimizing model complexity based on domain-wise validation. (iii) Regularisation required for optimal DG is greater than for conventional optimization for within-domain performance.",
            "main_review": "\nStrenths:\n* The paper is one of the few early attempts to establish the theoretical framework of OOD/Domain Generalization.\n* The connections with Rademacher complexity is interesting.\n\nFlaws:\n* The title seems to be a response to [3] but the evidence is not solid enough in comparison. I strongly recommend toning down and rename it to emphasize the theoretical contribution of this work.\n* Some very relevant theoretical works are not cited and discussed like [1,2]. And indeed some findings are alreadly manifested in those works.\n* The theoretical significance is not enough for publication. Domain generalization is unachivable without the formalization of distribution shifts but here only considers the risk over the mixtures of source domain, which is insufficient to provide out-of-distribution generalization guarantees.\n\n\n[1] Towards a Theoretical Framework of Out-of-Distribution Generalization\n[2] Empirical or Invariant Risk Minimization? A Sample Complexity Perspective\n[3] In Search of Lost Domain Generalization\n",
            "summary_of_the_review": "The paper has promising motivations but lacks of theoretical significance for DG community.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}