{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This work introduces/applies the mirror descent optimization technique to adversarial inverse reinforcement learning (AIRL). As a result, the proposed algorithm (MD-AIRL) incrementally learns a parameterized reward function in an associated reward space. The two issues of standard adversarial imitation learning algorithms are 1) current \"divergence\"-based updates may not lead to updates that better match the expert (due to geometry) 2) \"divergence\"-based updates may suffer when only small number of demonstrations are provided. Thus the goal of this work is to (presumably)  to \"robustify\" the learning of reward function especially by addressing these issues. The proposed algorithm is evaluated on a bandits problem, a multi-goal toy example and standard mujoco benchmark.\n\n**Strengths**\nThis work attempts to address the important problem of understanding and improving the updates of IRL algorithms\nA theoretical analysis is provided\n\n**Weaknesses**\nThe major concern is clarity of the manuscript. Even after updating clarity remains a concern\nWhile a lot of experiments were performed, evaluation is not entirely convincing. One reason for this that it is hard to tie the results back to the original motivation/claims of this algorithm. As one reviewer notes \"it's unclear how the new algorithm affects reward functions\". Furthermore, reviewers find the experimental results not entirely convincing \n\n**Summary**\nAfter rebuttal and revision, the clarity and experimental analysis remain a concern. My recommendation is that the authors are encouraged to take the reviewers feedback and improve the manuscript. In its current form it's not quite ready yet for publication."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes an alternative optimization technique (Mirror Descent) for adversarial inverse reinforcement learning, claiming it will resolve some of the issues with previous methods. \n\n-The authors prove the convergence of the proposed optimization method and provide a regret bound. \n-They experimentally show the benefits of the proposed methods in discrete and continuous action spaces. \n\n",
            "main_review": "This paper seems to have an incremental contribution to AIRL and it claims AIRL benefits from Mirror Descent updates. It is incremental in a sense that  MD has been studied extensively in optimization research. However, providing some intuitions on the benefits of MD in IL might be interesting for some researchers.\n\nStrength: \n- Provided convergence and regret bound\n- Better performance on experiments (slightly) \n\nThe derivations seem correct, but I did not check them thoroughly.\n\nWeakness:\n- The presentation of the paper needs improvement, especially on the connection of the previous works and the proposed method. \n- It is unclear how MD affects the resulted reward function (or the corresponding policy) for examples as opposed to SGD? \n- Experiments are not sufficient as authors compared the proposed method only with one other imitation learning method (RAIRL). \n\nDetailed Review: \n\n\nFrom the presentation it it not clear for me the significance of the contribution. More precisely: \n- It seems the step size greatly affects the convergence rate of the proposed method. So does this method require properly tuning the step-size for each problem? Is there experiments on the effect of step-size on the produced policy/reward function? \nHow the choice of SGD vs MD affects the reward function?\n- \"A gradient may not be the direction of the steepest descent in this case due to geometric constraints\" How to verify this claim in IL setting, faster convergence or superior reward function?  At convergence and without any prior knowledge on the optimal step size, can you claim that it is expected that MD will result in a higher quality reward function as opposed to other AIRL methods?  \n-Can you discuss more on how and why the choice of MD will resolve the two mentioned issued in AIRL? Can it be shown in the experiments? \n-Can you explain in details the claim \"We verified that the proposed method has clear advantages over previous AIL methods in terms of robustness\"? Is it verified through multi-armed bandit experiment? \n\n\n",
            "summary_of_the_review": "Overall I think this is an interesting paper. However, my main concern is the accuracy of the claims and the presentation of the paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a novel mirror-descent adversarial inverse reinforcement learning (MD-AIRL) algorithm. MD-AIRL considers the reward function as an iterative sequence in a proximal method. MD-AIRL has been introduced with dense theoretical analysis and validated with diverse experiments covering both discrete and continuous action spaces. ",
            "main_review": "Pros:\n1. The proposed MD-AIRL are both theoretically and empirically justified. Theoretically, the reward derived by mirror decent algorithm ensures the minimization of Bregman divergence converges to local optima along with a rigorous regret bound. Empirically, the resulting MD-AIRL was evaluated with large discrete action spaces and continuous action spaces. \n2. It is good to see the dense theoretical analysis in the paper for introducing the MD-AIRL gradually. The MD can be implemented on top of existing adversarial imitation learning method. \n\nCons:\n1. This paper claims that the proposed Mirror Descent Inverse Reinforcement Learning is more robust than previous adversarial imitation learning method. However, in the experiments, there lacks evidence to support that claim. \n2. In Figure 8, the results show that the proposed MD-AIRL using Tsallis regularizer with p=2 is competitive or slightly better than RAIRL. How does the MD-AIRL perform with other choices of p and regularizer? \n3. RAIRL and MD-AIRL both use Tsallis regularizer, however, the relationship between the two methods has not been well discussed. \n\n",
            "summary_of_the_review": "Despite its novelty and dense theoretical analysis, the empirical evaluation can be further improved to better support the robustness of MD-AIRL. I also hope to see more discussion between MD-AIRL and the related RAIRL in the rebuttal. I read through both authors' responses and other reviews and decided to recommend a marginal reject. I have thought the submission is interesting and has its own contribution in some extent, but it seems like there haven't been signficant improvement during the rebuttal period. The performance improvement is not significant and the written clarity is not ready to be acceoted by ICLR. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes an algorithm MD-AIRL, where each iteration involves policy and reward updates, and an expert policy estimation. The algorithm is motivated by MD, especially the reward update is modeled as a projected update of an MD update. The sufficiency and necessities of the choices of stepsizes to guarantee convergence are theoretically justified. A practical algorithm with numerical experiments are also provided. ",
            "main_review": "Strengths:\n\n1. The idea of using MD for reward updates is novel. A detailed theoretical guarantee on sufficiency and necessities on the choices of stepsizes are provided. \n\n2. The paper tends to develop MD-AIRL step by step, and also provides illustrative figures to help to understand. \n\nWeaknesses:\n\nMy complaints most come from the writing and illustration part. Please see the detailed comments as follows. \n\n1. Figure 1: what does the solid curve in policy/reward spaces mean? What about the dotted angles? The authors are encouraged to elaborate more on the illustration.\n\n2. “For instance, $\\psi_{t+1}$ can represent a softmax policy for a discrete space, ...”: is $\\psi_{t+1}$ a regularized reward function or a policy? I am assuming that it should be a reward function. \n\n3. Equation 5: It is a maximization problem of $\\pi$ but I cannot find $\\pi$ in the objective. Also, I am not clear how the form of $J_\\Omega(\\pi_t, \\psi_{t+1})$ is derived in equation 5, and how it is related to equation 2. The authors are encouraged to elaborate more to make it clear. \n\n4. Below equation 8: Is $\\pi_*$ a function of $t$? What do the authors mean by “the expectation is taken over the entire steps”? How is the optimality condition $E[...] = 0$ is derived? I am confused about the whole paragraph. \n\n5. Theorem 1: When will the assumptions on $\\Omega$ in theorem 1 be satisfied?  Are there any examples? I am thinking about KL but it does not satisfy the conditions here. Please correct me if I am wrong. \n\n6. Theorem 2: The last inequalities involve $s$. What is this $s$ here?\n\n7. Proof of Theorem 2: what is $n$ below equation 33? I am not sure about the relation between $f_t$ and $D_\\Omega$ throughout the proof. \n\n8. Proof of Lemma 6: The authors state \"Assuming all states are reachable\". Is this an assumption of the results? What if such a statement does not hold?\n\n\nMinor issues: \n\n1. Section 8: functinos -> functions\n\n2. Above Lemma 1: greed manner -> greedy manner\n\n3. Below Figure 1: a update -> an update\n\n4. Equation 7: gradient of $D_\\Omega$ is a bit misleading; the authors should mention that the gradient is taken w.r.t. the first argument. \n\n5. Equation 8: $\\Omega(\\pi(\\cdot | s_i))$ -> $\\Omega(\\pi_t(\\cdot | s_i))$\n",
            "summary_of_the_review": "Overall I think the paper proposes a novel approach to IRL. But the most obvious drawback is that the paper is not clearly written, which has many significant issues leading readers to misunderstand/cannot understand the paper. I think the current revision is not sufficient to be published in ICLR, thus I recommend a marginal reject and tend to reject if there is no significant revision. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}