{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper changes the metric in self-paced reinforcement learning to be a Wasserstein distance and shows that this outperforms other metrics in simple toy-like experiments.\n\nEven after discussions with the authors, two major concerns were identified with this submission: First, the proposed modification of the metric appears to be rather incremental with regards to the original paper. Second, the proposed method is only evaluated on relatively simple environments. The approach should be evaluated on more difficult tasks.\n\nGiven that there was no strong champion for acceptance among reviewers of this paper and the above mentioned limitations, I recommend rejecting this paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors try to alleviate the need for parametric distributions (namely a Gaussian) from the self paced RL framework (SPRL). They seek to shift current distribution over tasks to the target distribution of tasks under 2 constraints: keeping the current distribution close to the target and also that the expected return of the current distribution remain above some threshold. They compare SPRL using a KL divergence metric between a discretized proposal distribution and the target as well as a Wasserstein distance metric between particle based approximations. They show that in a set of toy tasks with low dimensional goal spaces can outperform existing baselines, e.g. GoalGan, as well as the previous instantiation of SPRL with parametric distributions. ",
            "main_review": "Overall I found this paper to be well written and I understand the issues with prior work it aims to resolve. This being said, I will caveat my review by saying that I am not familiar enough with optimal transport math to be confident in my ability to review the equations and more technical details.\n\nI enjoyed Figure 6 as it illustrated why working with the wasserstein metric would give a smoother interpolation from unimodal to multi-modal distributions.\n\nThe goal spaces in this paper all seem rather low dimensional and simple. It would be more promising to me if there were experiments with e.g. harder mazes (more invalid regions). What happens when the starting distribution is \"deceiving\" — i.e. assume the particle must first move away from the goal down a corridor, will this notion of KL/wasserstein fail because the only reachable states will be in the opposite direction of the target distribution?\n\nFigure 3: Why do all methods underperform the default at the beginning of training? Is it due to the choice of starting distribution?\n\nIt looks to me like the margin between paragraphs has been editted. Please revert it to what is expected with the ICLR style.\n\nIn the maze task, why does NP-SPRL have non-zero (or closer to zero) mass attributed to the unreachable center part of the maze?\n\nPick and place is solvable without a recorded demonstration with e.g. hindsight experience replay and no curriculum. It's a bit disappointing that the experiments with this environment are with a recorded trajectory.",
            "summary_of_the_review": "I enjoyed the paper; however, because I'm not familiar with the math I will defer to other reviewers on their assessment on this front. I'm giving this paper a 6 because I felt that in the end it did not prove it's new formulation was better but rather showed it experimentally. For a primarily experimental paper, I felt the experimental domains were rather simplistic.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper investigates self-paced reinforcement learning (SPRL) which is a type of curriculum-based RL. The authors empirically demonstrate the (different) limitations of a few SPRL variants (one which uses a Gaussian approximation, G-SPRL, and a non-parametric one, NP-SPRL). They propose the use of a Wasserstein metric instead of the KL divergence (WB-SPRL) and show that this leads to higher performance, more desirable behaviors, and more meaningful interpolation between different MDPs / contexts, on three continuous control tasks. The authors also compare their approach with other automatic curriculum algorithms. ",
            "main_review": "### Strengths \n\n1) This paper tackles an important problem, namely that of developing principled automatic curricula for RL. \n\n2) The experiments are well designed and provide useful insights into the learning dynamics and properties of different methods. I particularly liked the analysis showing how the context distributions change throughout training, as well as the interpolation between different distributions, for G-SPRL and NP-SPRL. \n\n3) The paper is clear overall and I found the claims regarding the limitations of NP-SPRL and G-SPRL relative to SPRL to be well supported by the experiments. However, I believe a few of the empirical claims regarding WB-SPRL are not as well motivated (see more details below). \n\n4) Even if the proposed method is a natural extension of SPRL using the Wasserstein metric, this hasn't been done before (as far as I know) and it is well motivated, making it a reasonable contribution at ICLR (in terms of novelty). \n\n### Weaknesses\n\n1) While this paper makes a reasonably convincing case for using WB-SPRL rather than G-SPRL or NP-SPRL, especially when the distribution of contexts isn't Gaussian and the space is high-dimensional, it doesn't convincingly show or explain the benefits of using WB-SPRL over other curriculum-based methods. In particular, ALP-GMM is just as good on two out of the three tasks used for evaluation and almost as good on the remaining one. In addition, the paper doesn't contain an in depth discussion of how WB-SPRL compares with other curriculum-based methods and when one can expect it to work better or why one may prefer to use this method (rather than e.g. ALP-GMM) even if it performs just as well. \n\n2) Most of the evaluation domains used (except for PNP) seem somewhat contrived as if they were designed for NP-SPRL and G-SPRL to fail and WB-SPRL to perform well. Thus, it would be useful to see how these methods perform on more realistic / natural domains e.g. other robotics tasks similar to PNP. \n\n3) Evaluating the methods on other tasks (e.g. other configurations of Maze or even a setting where both the maze layout and the goal location are changing, different configurations of Point Mass as used in Florensa et al. 2019, or other more realistic robotic tasks), and comparing WB-SPRL with other effective curriculum-based methods (e.g. Asymmetric Self-Play by Sukhbaatar et al. 2017 which outperforms GoalGAN on certain tasks or an adaptation of AMIGo by Campero et al. 2020 to continuous control domains), would help better motivate the use of the proposed approach and shed light on when this method can help the most. \n\n4) The paper seems to be missing experiments with ACL-GMM on Maze and Point Mass. Given that ACL-GMM is the best method on PNP, it would be useful to see how it performs on the other domains as well to better understand whether WB-SPRL provides additional gains on those settings. \n\n5) This paper also misses mentions to a lot of the relevant literature on curriculum learning, intrinsic motivation, exploration, or other approaches for solving challenging / sparse reward RL tasks. I found the related work section to be too narrow, brief, and lacking the necessary context to understand the paper's contribution. A discussion of how SPRL and in particular WB-SPRL compares with other approaches is also needed. \n\n### Questions / Requests\n1) Could you provide experiments showing how the performance of NP-SPRL scales with the resolution of the discretization? Additional analysis showing the $p_{\\alpha, \\eta}(c)$ at different iterations would also be valuable. \n2) Do you have any intuition of why ALP-GMM performs just as well as WB-SPRL on Maze? And why does ALP-GMM perform worse on Point Mass?\n3) In Figures 4 and 5, it would be useful to add the insights drawn from these experiments in the caption (for completeness and ease of read).\n4) In Figure 8a, can you show p(t) for the other approaches (e.g. G-SPRL, NP-SPRL)? Can you explain what the colors mean and also add this information in the caption?\n5) Do you have any intuition of why GoalGAN doesn't work as well, especially on Maze and Point Mass, since these domains are very similar to the ones used in the original paper?\n6) When can we expect WB-SPRL to provide a benefit over other curriculum-based approaches such as ALP-GMM or GoalGAN? Or why should one choose to use this method over the alternatives?\n7) Can you include the goal density / context distributions of GoalGAN,ALP-GMM, and WB-SPRL for comparison i.e. include them Figure 2 in order to better understand how the learning dynamics / learned behaviors differ from other SPRL variants?\n",
            "summary_of_the_review": "Overall, I think this paper provides a good understanding of the different SPRL variants, their limitations, and proposes a new method which provides some empirical benefits (over other SPRL methods) and is theoretically motivated. \n\nMy main concerns are the incomplete related work section and the limited benefits demonstrated by WB-SPRL over other curriculum-based methods. Hence, I believe the paper needs some more work to warrant acceptance at ICLR.  \n\nI'm willing to increase the score if the authors include references to the relevant works from the literature, a discussion of how WB-SPRL compares with other curriculum RL methods (theoretically and empirically), and include more experiments that (more convincingly) demonstrate WB-SPRL's benefits over other approaches (with some intuition of why / in which settings we can expect WB-SPRL to be better). ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors extend the Self-Paced Reinforcement Learning, which samples environment instances from a distribution that shifts from an (easy) starting to a (hard) target distribution. The algorithm has previously been shown to enable agents to solve hard environments. Previously, the target distribution was limited to a Gaussian, which the authors now extend using Wasserstein barycenters. This extension allows more elaborate target distributions, e.g. expert demonstrations.",
            "main_review": "### Correctness\n\nThe paper clearly states the algorithm, intended changes and their purpose as well the experimental details. The change in metric itself makes sense, as demonstrated in the paper, to increase the range of problems to which self-paced reinforcement learning (SPRL) can be applied. The experiments not only show the increased performance using the curriculum update, but also explore how the curriculum itself develops.\n\n### Technical Novelty And Significance\n\nThe paper is incremental by extending SPRL by changing the metric. So, it’s a combination of two existing ideas (which totally makes sense), but the overall concept of the SPRL algorithm stays the same. Although the experiments show a clear performance improvement, the relevance of the results are not fully clear and therefore also not the significance of the proposed SPRL modification; see below for more details.\n\n### Empirical Novelty And Significance\n\nThe authors show extended use cases for the SPRL algorithm and use appropriate baselines, although the experimental scope is still quite limited compared to other curriculum learning papers in RL. Even though target distributions can take arbitrary shapes now, the authors stick to very narrow targets and do not discuss generalization beyond simple bimodal distributions. This limits comparability with other methods and ultimately the significance of the paper’s contributions. \n\n### General comments\n\nSections 5.1 and 5.3 in particular highlight the need for non-parametric target distributions. The PointMass experiment in 5.2, however, was not as convincing as it does not show how well the Wasserstein barycenters (WB-SPRL) approach scales in general. If the target distribution can now be bimodal instead of unimodal, it is still an improvement, but perhaps not as interesting as if 10 or more different gate configurations would be possible. I find this particularly relevant as this is the only experiment in which the naive discretization (NP-SPRL) looks to perform significantly worse than WB-SPRL. Would it be fair to say that for unimodal cases, there is no large performance gap between the two? \nI would rate the overall significance of these findings much higher if WB-SPRL can be expected to perform well beyond the bimodal case. Do you have any intuition on the limitations for context distributions for WB-SPRL?\n\n\nOn a similar line of thoughts, I wonder about the experimental setting. For example, in the experiments on Maze (Section 5.1), the context c was able to encode an infeasible environment. Thinking about real-world applications, I would expect that only feasible contexts and environments are available for training, especially given that we assume a fairly good understanding of the context space (we already know quite a bit about instance difficulties, for example). Furthermore, I believe that the target distribution \\mu should be fairly broad in many cases, since many possible environments have to be covered by a generalizing agent. The target distribution in the paper at hand are all fairly narrow, which might benefit SPRL’s performance compared to other baselines, but also limit the algorithm in practice. Could the authors please comment on the real-world applicability of their SPRL variant in relation to wider distributions of \\mu.\n\nI’d also be interested to know more about SPRL’s hyperparameters. While they are stated in the appendix, there is no clear indication to what degree they affect the performance and if this differs between the variations. How were they chosen in the first place? Was there any hyperparameter optimization involved?\n\nThe related work on curriculum learning in RL could be improved. There have been many publications with good results in the past years beyond SPRL, two additional papers on the topic from 2018 and 2019 do not reflect the state of the art. I recommend that this section should be extended, even though not all work on CL in RL may have exactly the same focus of solving specific hard instances or generating curricula through generating instances as SPRL. Examples include:\n* Automatic Curriculum Learning through Value Disagreement, Zhang et al. https://proceedings.neurips.cc/paper/2020/file/566f0ea4f6c2e947f36795c8f58ba901-Paper.pdf\n* Safe Reinforcement Learning via Curriculum Induction, Turchetta et al.\nhttps://proceedings.neurips.cc/paper/2020/file/8df6a65941e4c9da40a4fb899de65c55-Paper.pdf\n* Prioritized Level Replay, Jiang et al.\nhttp://proceedings.mlr.press/v139/jiang21b/jiang21b.pdf\n* Self-Paced Context Evaluations for Contextual Reinforcement Learning, Eimer et al.\nhttp://proceedings.mlr.press/v139/eimer21a/eimer21a.pdf \n* TeachMyAgent: a Benchmark for Automatic Curriculum Learning in Deep RL, Romac et al.\nhttp://proceedings.mlr.press/v139/romac21a.html \n\nCould the authors please also briefly explain in the rebuttal, why there is no empirical comparison against these baselines? Or why a comparison against other baselines made more sense? \n\n### Minor questions and comments:\n* Equation 4: Why can we get rid of \\delta from Equation 3?\n* End of Section 3: Why do you assume 2-Wasserstein distances under an euclidean metric?\n* There’s no reference for contextual RL in the related work section. The oldest reference I’m aware would be Hallak et al. 2015\n* Did you adapt the baseline hyperparameters at all from their original settings? I do not think the appendix states this explicitly. Did you re-implement them or could you use the original code for all of them?\n",
            "summary_of_the_review": "The paper proposes a well-motivated extension to the SPRL algorithm and demonstrates its usefulness in interesting experiments. The significance of these results is somewhat unclear, though, as the naive context discretization seems to perform just as well as the proposed metric in every but a very limited case. Additionally, the nature of the work is rather incremental with limited impact and novelty. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}