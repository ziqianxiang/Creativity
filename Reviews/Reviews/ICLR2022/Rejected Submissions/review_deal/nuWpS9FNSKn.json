{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper generated a large amount of discussion.  Three reviewers were marginally above and one marginally below.  The paper presents an intriguing relationship between self-supervised learning and topic model inference that extends earlier work of Tosh.  The result seems to be subtle because there was considerable discussion with the authors wrapping up with a reminder of what the main goal is:  SSL can achieve the state-of-the-art performance for topic inference problem, moreover (main goal) SSL can be oblivious to the specific topic model.  This is indeed intriguing.  But with all the discussion, and one persistent negative reviewer, I feel the paper needs to be polished.  Given the theorem gives a testable statement, I don't see why experimental results cannot be done for 4 different real data sets, to give us more confidence."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper introduces a new method for performing inference on topic models, based on self supervised learning. Their approach is generalizable to multiple types of topic models. Specifically, the authors show that the expectation of some polynomial function of the topic posterior will be a linear function of the output of function that optimizes the self-supervised learning objective. The authors explore 2 types of self-supervised learning objectives, one based on reconstruction and one based on contrastive learning that was explored in prior work. The authors provide proofs for their main result as well a result robust to approximate minimization of the self-supervised objective. In their experiments the authors show that their approach outperforms inference with a misspecified model. The authors also run semi-supervised learning experiments on their approach.\n",
            "main_review": "Strengths:\n\nThe main results here seem novel and potentially useful. The proposed method is a new way to perform efficient approximate inference for topic models with some guarantees. \n\nCompared to previous work the authors provide a new self-supervised learning technique for topic models with its own theory.\n\nThe synthetic experiments show that the approach is useful when a misspecified model is used.\n\nThe real data experiments show that this self supervised learning technique is useful for feature extraction.\n\nWeakness:\n\nI feel that some of the results of this work could use further explanation and that some of the experiments are confusing or underwhelming:\n\nI do not feel I was able to devote adequate time to understanding the details of the results shown in section 3 and I’d like to continue looking in to it. For the third paragraph of section 3, could the authors give additional intuition for how this approach gives the correct expectation of the posterior seemingly without knowledge of the prior p(w)?\n\nFigure 1 provides very little information, as it seems to only reflect a single synthetic document. I’m not sure what I’m supposed to take away from it.\n\nFigure 2 seems like a poor way to present this information. Even simple table would probably be more interpretable and precise.\n\nUnder: Robustness of self-supervised learning. What is the “traditional topic inference” used? Simply the sampling approach discussed above? It is also strange that the results are mostly shown comparing across misspecified models. I would expect to see comparisons between other types of inference (MAP, variational) using correctly specified models. Comparing explicitly misspecified models for synthetic data feels a bit contrived. \n\nWhy are the results on real data only about semi-supervised learning? This feels like a bit of a non-sequitur given the rest of the paper. I would expect to see at least qualitative results showing inference with this method on real data.\n\nThere has been quite a bit of prior work on supervised and semi-supervised topic models. I would expect at least some of these methods to be represented in section 6. Some examples being: https://proceedings.neurips.cc/paper/2007/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Paper.pdf, https://www.jmlr.org/papers/volume13/zhu12a/zhu12a.pdf, http://proceedings.mlr.press/v84/hughes18a.html\n\nGenerally the focus of this paper is on inference. Can this approach be applied within a learning algorithm for topic models?",
            "summary_of_the_review": "Overall the results in this work are interesting and possibly useful, but I feel that there are some lingering questions about applicability, particularly as the experimental results are underwhelming.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper considers the problem of performing posterior inference for probabilistic topic models. A \"general\" topic model consists of two key parts:\n\n- $A$ : the topic-word probability matrix\n- $\\Delta$ : the prior distribution over doc-topic probability vector w\n\nGiven these parts and a new document x, the goal is to infer the posterior over w: $p(w | x)$. \n\nIt is well-known that this posterior can be estimated in several ways using standard approximate Bayesian inference methods: MCMC, variational, etc. The detailed steps of these methods are usually customized to the choice of prior.\n\nThis paper suggests a new possibility: that a learned transformation -- denoted f(x) -- trained using a so-called *reconstruction* loss (see Eq 1), can be then translated into the desired posterior via a linear function. Surprisingly, the paper suggests that learning f(x) does not depend on the prior \\Delta, so the big idea is that this representation is robust to misspecification of the prior.\n\nConcretely, the contributions of the paper seem to be:\n\n1) Theorem 3, which claims that there exist linear weights \\theta that, when applied to the vector produced by the ideal learned transformation f(x) that minimizes the reconstruction objective (Eq 1), can exactly equal the expected value of any polynomial summary function of the doc-topic probability vector w under the posterior.\n\n2) Theorem 4, which claims that even if f is not an exact minimizer, if it is within an additive tolerance \\epsilon of the ideal loss, then there exist again weights \\theta such that the squared error between the true posterior summary and the linear function has bounded error for all documents.\n\n3) Theorem 5, which looks at the contrastive objective in Eq 2, and shows that there exists a linear function that can recover the expected polynomical summaries of random variable w under the posterior.\n\nExperiments in Sec. 5 look at toy data, with results in Fig 2 and Table 1. Essentially, these results show that the proposed SSL method produces lower error posterior estimates (in terms of  total-variation distance from ground truth) than using Bayesian inference with the wrong prior. \n\nExperiments in Sec. 6 look at real data (the AG news dataset), comparing the representations learned by SSL at a downstream classification task to baselines of bag-of-words, word2vec, and the contrastive method of Tosh et al. Fig. 3 suggests that the proposed method offers a few percentage points absolute gain in accuracy over word2vec, and is similar to the previous NCE method (Tosh et al) in accuracy.",
            "main_review": "\nStrengths\n---------\n+ Takes on an interesting and timely topic (self-supervised learning for probabilistic models)\n+ Broad scope of objectives, looking at both \"reconstruction\" and \"contrastive\" learning\n+ Broad scope of topic models, considering a \"general\" family that includes LDA, CTM, and Pachinko Allocation as special cases\n+ Seems to extend the \"nearest\" previous work (Tosh et al. 2020) meaningfully, focusing on reconstruction objectives not just contrastive learning\n\nWeaknesses\n----------\n\nI list 5 concerns here, with detailed discussion and questions for the authors below\n\n- W1: While theorems suggest \"existence\" of a linear transformation that will approximate the posterior, the actual construction procedure for the \"recovered topic posterior\" is unclear\n\n- W2: Many steps are difficult to understand / replicate from main paper\n\n- W3: Unclear what theorems can say about finite training sets\n\n- W4: Justification / intuition for Theorems is limited in the main paper\n\nResponses to W1-W3 are most important for the rebuttal.\n\n\n## W1: Actual procedure for constructing the \"recovered topic posterior\" is unclear\n\nIn both synthetic and real experiments, the proposed self-supervised learning (SSL) method is used to produce a \"recovered topic posterior\" p( w | x). However, the procedure used here is unclear... how do we estimate p( w | x) using the learning function f(x)?\n\nThe theorems imply that a linear function *exists* with limited (or zero) approximation error for any chosen scalar summary of the doc-topic weights w. However, how such a linear function is constructed is unclear. The bottom of page four suggests that when t=1 and A is full rank, that \"one can use the pseudoinverse of A to recover the posterior\", however it seems (1) unclear what the procedure is in general and what its assumptions are, and (2) odd that the prior may not needed at all.\n\n*Can the authors clarify how to estimate the recovered topic posterior using the proposed SSL method?*\n\n\n## W2: Many other steps are difficult to understand / replicate from main paper\n\nHere's a quick list of questions on experimental steps I am confused about / would have trouble reproducing\n\nFor the toy experiments in Sec. 5:\n\n- Do you estimate the topic-word parameter A? Or assume the true value is given?\n- What is the format for document x provided as input to the neural networks that define f(x)? The top paragraph of page 7 makes it seem like you provide an ordered list of words. Wouldn't a bag-of-words count vector be a more robust choice?\n- How do you set t=1 (predict one word given others) but somehow also use \"the last 6 words are chosen as the prediction target\"?\n- How do you estimate the \"recovered topic posterior\" for each individual model (LDA, CTM, etc)? Is this also using HMC (which is used to infer the ground-truth posterior)?\n- Why use 2000 documents for the \"pure\" topic model but 500 in test set for other models? Wouldn't more complex models benefit from a larger test set?\n\nFor the real experiments in Sec. 6:\n\n- How many topics were used?\n- How did you get topic-word parameters for this \"real\" dataset? \n- How big is the AG news dataset? Main paper should at least describe how many documents in train/test, and how many vocabulary words.\n\n\n## W3:  Unclear what theorems / methods can say about finite training sets\n\nAll the theorems seem to hold when considering terms that are expectations over a known distribution over observed-data x and missing-data y. However, in practical data analysis we do not know the true data generating distribution, we only have a finite training set.\n\nI am wondering about this method's potential in practice for modest-size datasets. For the synthetic dataset with V=5000 (a modest vocabulary size), the experiments considered 0.72 million to 6 million documents, which seems quite large. \n\n*What practically must be true of the observed dataset for the presented methods to work well?*\n\n\n## W4: Justification / intuition for Theorems is limited in the main paper\n\nAll 3 theorems in the main paper are presented without much intuition or justification about why they should be true, which I think limits their impact on the reader. (I'll try to wade thru the supplement, but did not have time before the review deadline). \n\nTheorem 3 tries to give intuition for the t=1 case, but I think could be stronger: why should f(x) have an optimal form $p( y=v_1 | x)$? Why should \"these probabilities\" have the form $A E[ w | x]$? I know space is limited, but helping your reader figure things out a bit more explicitly will increase the impact.\n\nFurthermore, the reader would benefit from understanding how tight the bounds in Theorem 4 are. Can we compute the bound quality for toy data and understand it more practically? \n\n\nDetailed Feedback on Presentation\n---------------------------------\n\nNo need to reply to these in rebuttal but please do address as you see fit in any revision\n\nPage 3:\n- \"many topic models can be viewed\"... should probably say \"the generative process of many topic models can be viewed...\"\n\n- the definition of A_ij is not quite right. I would not say \"word i \\in topic j\", I would say \"word i | topic j\". A word is not contained in a topic, Each word has a chance of being generated.\n\n- I'd really avoid writing $\\Delta(K)$ and would just use $\\Delta$ throughout .... unclear why this needs to be a function of $K$ but the topic-word parameters (whose size also depends on $K$) does not\n\n- Should we call the reconstruction objective a \"partial reconstruction\" or \"masked reconstruction\"? I'm used to reconstruction in an auto-encoder context, where the usual \"reconstruction\" objective is literally to recover all observed data, not a piece of observed data that we are pretending not to see\n\n- In Eq. 1, are you assuming an ordered or unordered representation of the words in x and y?\n\nPage 4:\n\n- I would not reuse the variable y in both reconstruction and contrastive contexts. Find another variable. Same with theta.\n\nPage 5:\n\n- I would use $f^*$ to denote the exact minimizer, not just $f$\n\n\nFigure 2 caption should clarify:\n\n- what is the takeaway for this figure? Does reader want to see low values? Does this figure suggest the approach is working as expected?\n- what procedure is used for the \"recovered\" posterior? Your proposed SSL method? \n- why does Pure have a non-monotonic trend as alpha gets larger?\n\n",
            "summary_of_the_review": "Overall I think the direction of the paper is interesting, but the main paper at present is missing some key pieces (esp. how the recovered topic posterior is produced once the SSL representation f is learned). In addition, I have concerns about the disconnect in how theorems might apply to finite training sets in practice, and how little intuition was provided in the paper for the theory. \n\nI could be persuaded to change my mind by a strong rebuttal.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper shows theoretically and empirically that self-supervised objectives can be used to extract useful information about the posterior of the topic proportion vector given a document, regardless of the underlying models. It extends the findings of Tosh et al.2020 about contrastive objectives to reconstruction-based objectives. It is interesting to see that one simple reconstruction objective recover the posteriors generated from the different probabilistic topic models on the synthetic dataset.",
            "main_review": "* Topic-word weight matrix $A$: It is known that one advantage of topic models is that each latent topic can be represented as a set of words. The topic-word weight matrix is learnt together with the posterior of the topic proportion vector $w$. All the theorems assume that $w$ is a condition, and it seems that $\\theta$ is the left pseudo-inverse of $A$. Looking at existing topic models, either probabilistic or neural, it is not clear to me how $A$ is learned by the network models used in the experiments.\n* Neural topic modelling: How is this work related to neural topic models based on autoencoding variational inference, for instance, by Srivastava and Sutton 2017? It might be good to see how a neural topic model can recover the topic posterior.\n* The synthetic experimental results seem to say that the reconstruction-based objective can recover the posterior of $w$ given a document. What is the implication of this from a topic modelling practitioner's point of view? Can one use the reconstruction-based objective with neural networks in place of topic models? However, it is known that one advantage of topic models, particularly the probabilistic ones, is that they can learn interpretable topics. Besides the approximation of the posterior of $w$, I cannot see how the proposed theorems could benefit the topic modelling community. \n* It is an interesting statement that \"attention-based architecture performs the best for recovering topic posterior distribution for CTM and PAM documents\". I wonder if the authors can discuss this more, like the possible reasons.\n* The experiments on the real dataset seems to show that the self-supervised objectives can learn useful representation for document classification. Again, I would like to see how neural topic models perform in this scenario. Furthermore, compared with the other embedding method, like the language models, what is the advantage of this work? ",
            "summary_of_the_review": "Overall, showing that both the reconstruction-based objective and the contrastive object can recover the posterior of the topic proportion vector is interesting. It would be good to demonstrate how this theoretical finds can benefit the wide topic modeling community with more experiments on real-world datasets. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The main strength of this paper lies in the thorough theoretical analysis and corresponding proofs. The authors not only prove that a new reconstruction-based objective can also extract posterior topic information for a general topic model, but also strengthen the guarantee for contrastive objective in Tosh et al. (2020) by removing some of their assumptions and the necessity of landmark documents. ",
            "main_review": "- The main strength of this paper lies in the thorough theoretical analysis and corresponding proofs. The authors not only prove that a new reconstruction-based objective can also extract posterior topic information for a general topic model, but also strengthen the guarantee for contrastive objective in Tosh et al. (2020) by removing some of their assumptions and the necessity of landmark documents. \n\n- While the theoretical analysis is interesting, objective and motivation of this paper is not entirely clear and convincing. It seems there is lack of outlook for future work where this analysis could be useful, and the empirical study on real data is also somewhat weak.  There is no comparison with more recent self-supervised and contextual document representation work, such as BERT mentioned in the beginning of the paper. Actually this is also a common issue in both Arora et al. (2019) and Tosh et al. (2000), which are two most closely related papers.\n    - The LDA “is actually doing very poor on several ‘objectively’ evaluable predictive tasks”, because “it is not designed, nor trained for such tasks, such as classification, there is not warrantee that the estimated topic vector is good at discriminating documents\" (http://www.cs.cmu.edu/~epxing/talks/ACL2.pdf).\n    - BOW, word2vec and NCE are weak base lines far behind more recent self-supervised and contextual representation models for document classification.\n    - The experiments are not conducted on a fair basis. For example, word2vec is a light-weighted neural network with only one layer, but this paper implements the proposed model using some deep neural networks including 8 heavy-weighted Transformer blocks, which still demonstrate no significant difference. \n\n- typos\n    - \"constrastive\" in Page 1\n",
            "summary_of_the_review": "The theoretical analysis is interesting, but the motivation of analysis and evaluations are not entirely convincing. I suggest to focus on theoretical analysis and clarify where this analysis could be useful.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}