{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper introduces some interesting ideas on how use causal random forests for conditional average treatment effects (CATE), with respect to some baseline treatment level (\"0\"), when the treatment variable is continuous. Figure 1 summarises the scope of the paper neatly. Scalability issues are also considered.\n\nI think this *is* a paper \"nearly there\" in terms of a impactful contribution. The main issues are some presentation kinks and extra steps in the theory. I think the very low scores from the reviewers are not quite representative of the overall quality (I would be more generous). However, I'm afraid I'm also inclined towards a reject. The paper neglects some other developments on ML for CATE with continuous treatment e.g. Bica et al.'s \"Estimating the Effects of Continuous-valued Interventions using Generative Adversarial Networks\" (NeurIPS 2020) and the references within. A focus on the theory would help to differentiate it, but I'm not that confident that the results are currently mature enough to claim them.\n\nAlthough I suggest a rejection, let me make clear I strongly encourage the authors to further pursue their ideas. You are doing good work, and the next iteration might nail it. As you found out in the discussion, emphasise the continuous aspect of it. I'd also emphasise the fact that you have a clear setup of the problem in terms of the contrast wrt to a baseline treatment effect instead of some generic contrast function. People in ML tend to be oblivious to such a setup, but I'm not convinced you are properly exploiting it."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a generalized version of the causal forest for heterogeneous treatment effect estimation for continuous treatment by non-parametric modeling of the dose-response function. To provide non-parametric modeling, the proposed method makes use of kernel-based double/debiased estimators. Experiments of the proposed methods are run on assorted synthetic datasets as well as real-world datasets to demonstrate the effectiveness of the proposed method compared to competing methods.",
            "main_review": "Strength:\n\n1. The paper is concerned with an interesting and important problem in causal inference, which is heterogeneous treatment effect estimation with continuous treatment. Modeling the nonlinearity of the dose-response function is also an important and challenging aspect of this problem.\n\n2. The paper presents solid empirical results on real-world data. The paper also presents a real-world a/b test to demonstrate the effectiveness of the proposed method in practice.\n\n3. The paper also presents a scalable implementation of the proposed method, which can be used to handle large-scale (production-scale) data.\n\nWeakness:\n1. Presentation is not clear. Important concepts such as score function, splitting criteria, etc are not specified. The presentation of the proposed method is also difficult to follow. For example, it is not clear to me what are the components of CF, and how GCF is derived and differs from CF in section 4.\n\n2. The key component of the proposed method is to model the DRF in a non-parametric, doubly robust fashion. It is not clear to me whether such a contribution is significant enough to motivate acceptance. This is also because the proposed method performs comparably with Kenndy on synthetic data.  So, it is not clear what might be the additional value brought compared to Kenndy. The author may consider providing further explanation and justification on the complexity and technical difficulty of replacing a linear DRF with the proposed one.\n\n3. There is a lack of theoretical justification of the proposed method. However, the empirical results on synthetic data and real-world experiments help to assuage this concern.",
            "summary_of_the_review": "Correctness: I did not identify major issues in terms of correctness.\nNovelty and significance: I have concerns about the significance of the proposed method in terms of modeling the RDF through non-parametric approaches. I appreciate the authors running real-world experiments with a/b test to demonstrate the performance of the proposed method.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors extend the generalized random forest by Athey et al. (2019) from partially linear models to nonparametric ones and combine it with a doubly robust estimation step. The paper contains a small scale simulation study and a small real data analysis.",
            "main_review": "Weaknesses:\n- The paper does not contain new methodology; it is purely a combination of existing techniques (generalized random forest and doubly robust estimator). \n- The paper does not provide a theoretical analysis of the proposed method.\n- The authors oversell their contribution: Generalizing the Generalized Random Forest (Athey et al. 2019) from a partially lienar model to a full nonparametric one is not very creative. \n- It is unclear to me how the estimator $\\hat{\\mu}(t, X_i)$ is constructed. It is the weighted average of something (?), where the weights w_i(x) are presumably the frequency with which the ith training example falls into the the same leaf as x (see Athey et al. 2019).\n- The simulation study is not reproducible because the authors do not include values for sample size $n$ and dimension $p$. \n- The simulation study and real data analysis are very small scale.\n- Unsupported claims: In the introduction the authors claim that their method allowed for faster hyper-parameter tuning than competing methods. However, they do not provide any evidence to back-up this claim.\n\nAdditional comments:\n- Poor notation: For example, on page 3 $\\Omega$ is defined as a single vector, on page 4 the authors draw samples from $\\Omega$, in Algorithm 1 on page 5, $\\Omega$ denotes the dat set.\n- The exposition is unorganized and contains many abbreviations which are hard to keep track of.\n- The language is unnecessarily pompous and riddled with grammatical mistakes to an extend that makes it difficult to follow the authors ideas.",
            "summary_of_the_review": "The paper does not contain new methodology or theoretical result. The simulation study and real data analysis are weak.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper provides a generalized causal forest (CF) by replacing the partial linear model in the CF as a kernel DML estimator. Empirical evidences show that the proposed generalized CF (GCF) outperforms the existing estimators. ",
            "main_review": "I like the idea of generalizing the CF because it's impactful by allowing users to leverage any flexible modern machine learning tools. However, I think the paper is hardly understandable to most readers, and the theories are not deep enough. \n\nHere are some details. \n1. \"However, these methods largely focus on binary ...\"  may mislead, because this statement is only for a few recent estimators developed by Chernozhukov et al., Zhao et al., and Kunzel et al. For example, a BART estimator developed by Hill (2011) can deal with continuous treatments. I suggest conducting a more extensive literature review to provide more clear motivation. \n2. What is the meaning of \"local splitting\" in the 3rd paragraph in Section 1? Please don't assume that the authors are familiar with the details of the causal forest algorithm. \n3. This paper doesn't answer or motivate why we have to use the proposed GCF instead of the Kernel DML (Colangelo and Lee, 2020) or the doubly-robust estimator provided by Kennedy et al. (2017). It would be great if authors can provide more extensive literature review on the HTE estimators for the continuous treatment case. \n4. In Section 2, what is the meaning of the \"global\" estimator? \n5. I think the most important prerequisite to understanding the paper is the notion of the causal tree and causal forest, developed by Athey et al. Please add this in Section 3. Without it, it's indeed impossible to understand the paper. For example, what is the meaning of 'honesty' in Algorithm 1? \n6. It's not clear what the contribution of the paper is. Even if I wrote the contribution as \"replacing the partial linear model to the kernel DML,\" I only guessed so because it's not explicitly written. \n7. I expected that the GCF would achieve the doubly robustness property, following the properties of the kernel DML. Please consider adding the formal convergence analysis exhibiting the doubly robustness property. Showing the empirical performance is not sufficient to promote the proposed estimator. \n8. In the experiment, why don't you compare with the Kernel DML (Colangelo and Lee, 2020)? \n9. There are typos -- \"Table ??\" on page 9, and \"First, We\" on page 2. ",
            "summary_of_the_review": "- This paper implicitly assumes that readers are familiar with the details of the causal forest algorithm. Without this prerequisite, it's impossible to understand the paper. \n\n- This paper doesn't motivate to use the GCF over existing estimators (Kernel DML, or the one of Kennedy et al. (2017)). \n\n- No theoretical analysis on the convergence or bias analysis for the GCF.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}