{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "I agree with the reviewers that this work is not well-presented, and it seriously lacks rigor and experimental support. The writing of this work also needs significant improvement. The authors made many claims without offering rigorous proofs, and hand-waved their argument throughout without strong empirical support. In the end, the authors' response did not address the reviewers' concerns satisfactorily and no one is excited enough to defend the current draft. Please consider revising your draft according to the reviewers' comments."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper does a theoretical analysis of various gradient approximations for the non differentiable sign function. They investigate 3 gradient estimators based on the STE assumption, namely (identity) STE, ReLU STE, and clipped STE (cReLU). They show theoretically that a gradient estimator which breaks the scale symmetry (clipped STE) in the network is more likely to achieve a local minimum than one that keeps the symmetry.",
            "main_review": "Strong points:\n* A novel approach on how to analyze various gradient estimators.\n* Interesting new theoretical insights into various flavors of the STE gradient approximation.\n* Detailed derivations in the appendix.\n\nWeak points:\n* No experimental results, even not on MNIST or a toy example to show that the theoretical findings correlate with empirical observations.\n* The straight through estimator (STE, identity function) is know to be a biased estimator. In the paper this discussion is not coming back, but I think it would be interesting to discuss whether the other estimators (ReLU, cReLU) are biased and whether one is more biased than the other. Is there a trade-off to be made between being more biased vs having a higher chance to converge to a local minimum?\n* The optimization problem defined in 2.1 (eq 1 & 2) is non-convex. For a non-convex optimization problem it is unclear to me based on the manuscript how the gradients in stationary points are important and increase the chance to get to the true global minimum (page 5, below eq 23). Some further explanation on the reasoning could be helpful to understand for the reader and to verify that these claims are true.\n* A discussion on whether or not this analysis also holds for 1) weight quantization and 2) multi bit quantization would be interesting. Especially for the latter case it is common practice that the gradient is clipped due to the clipping in quantization (STE is applied to the rounding operation, thus the gradient of the full quantization operation is clipped due to the gradient of the clipping, see Esser et al. 2019).\n* The clarity of the paper could improve and there are some misconceptions:\n    * The straight through estimator is clearly defined in literature (see Bengio et al. 2013) as back propagation through a function as it would have been the identity function (thus gradient of 1 in the backwards path). However, in the manuscript any approximate gradient is defined as a STE gradient. This misuse of terminology makes it extra difficult and confusing to understand the work, especially since it is of theoretical nature. (“In this paper, we refer to the proxy of the gradient as the STE gradient.”)\n    * It is a bit confusing that sigma is defined as step function. But based on the text (binary activations) and later equations it seems that it is a sign function. Would be good to be clear and maybe mathematically describe the exact function.\n\n",
            "summary_of_the_review": "Interesting and novel perspective on various gradient estimators for the sign function used in binary activations. The main drawbacks are that there is no empirical data to support any of their claims, several aspects are unclear based on the manuscript and the paper could improve in its overall clarity. In the current version and my understanding of it the drawbacks outweighs the strong points.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Training quantized neural network by the biased Straight-Through estimator (STE) is a common practice in the NN quantization literature. However, due to the biased nature of STE, there is no principled guidance in making algorithmic choices for such training methods. This work focuses on the choice of surrogate differentiable functions in STE, and looked at three common choices: identity, ReLU, and clipped ReLU. Through analysis of the scale invariance of gradients in STE, the paper suggests clipped ReLU might pick up local minima degenerated in scales, due to its property of breaking scale symmetry among the three. The authors further analyzed the stationary points of a simple misspecified model with Gaussian inputs to confirm their observations.",
            "main_review": "\nI have read the rebuttal. I'd like to thank the authors for the revision, although it does not resolve my concerns.\n\n==========================\n\n## Summary\n\nTraining quantized neural network by the biased Straight-Through estimator (STE) is a common practice in the NN quantization literature. However, due to the biased nature of STE, there is no principled guidance in making algorithmic choices for such training methods. This work focuses on the choice of surrogate differentiable functions in STE, and looked at three common choices: identity, ReLU, and clipped ReLU. Through analysis of the scale invariance of gradients in STE, the paper suggests clipped ReLU might pick up local minima degenerated in scales, due to its property of breaking scale symmetry among the three. The authors further analyzed the stationary points of a simple misspecified model with Gaussian inputs to confirm their observations.\n\n## Comments\n\n* The clarity of the paper is very poor. Currently the paper is more like a technical note than a polished conference paper. There are deep stacks of equations everywhere and many of them lack clear explanation. To support my claim, here are a few examples:\n\nEquation (1) is confusing as a convolutional neural network because the hidden layer output is a simple matrix vector product, I figured it out after some thought that you restructured the input by replicating its features in a window to form each row of Z. However, you mentioned this nowhere and it is even not stated what $m$ and $n$ is.\n\n* The main claim is not rigorous, which makes it very difficult to understand the implications of the results and the potential impact of the paper.  Many arguments are hand-wavy and when approximation is used, there is no characterization of the error, to list a few:\n\n\"the cRelu STE gradient for |w| << 1 is approximately identical with the Relu STE gradient\"\n\n\"this relation implies that the Relu gradient is approximately embedded in the cRelu STE gradient\"\n\n\"may not be zero\" \n\n\"there is still a chance to be zero\"\n\n\"using cRelu STE is the most likely to achieve the true minimum in the three STEs\"\n\nAll these make the conclusion less convincing and the lack of empirical evidence makes it worse.\n\n## Overall suggestions\n\nI vote for rejection mainly due to the questionable analysis (lack of rigor) and the difficulty to interpret the relevance of the result. Also the work does not go beyond one-hidden layer network as it claimed in the abstract.\n\n## Questions and minor points\n\n* Is my understanding correct that the hard $g_j$ has no scale invariance (it only has scale invariance in the loss)? \n* I'm always curious, have the quantized neural network community ever looked into other unbiased gradient estimators developed by the Monte-Carlo gradient estimation people? Or is it true that STE empirically outperformed all of them so it is still the default practice?\n",
            "summary_of_the_review": "I vote for rejection mainly due to the questionable analysis (lack of rigor) and the difficulty to interpret the relevance of the result. Also the work does not go beyond one-hidden layer network as it claimed in the abstract.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper considers a network with one hidden layer with binary valued neurons sharing the same weight vector but having non-overlapping receptive fields. The authors analyse the stationary points of a particular MSE loss when a straight through estimator (STE) is used for the loss gradient. They analyse three variants for the STE: (1) derivative of the identity function, (2) derivative of ReLU and (3) derivative of a piecewise linear squashing function. Assuming a simple independent distribution over the inputs and a particular loss, the authors show that the first two variants are biased and only the third STE variant can be unbiased. The authors conjecture that the reason for this is that the third STE variant breaks the scaling symmetry w.r.t. the weights of the first layer.",
            "main_review": "**Strengths:**\nThe authors propose a clear, concise and closed form analysis of the true gradient (i.e. the gradient of the expected loss) and its stationary points. This is possible due to the simplicity of the model and the assumed data distribution. The same is done for each of the three STE variants. This proves that the first two STE variants are biased in general. Moreover, the authors prove in particular that none of the global optimisers of the expected loss is a stationary point for these STE variants. (Notice that the globally optimal weights of the hidden layer form a ray due to scale invariance). On the other hand, they show that at least one of these global optimisers is also a stationary point for the third STE variant.\n\n**Weaknesses**\nThe same model has been studied in the previous work Yin et al. (2019) for a different loss. It has been shown there that the first STE variant is biased, while the second and third where found to be unbiased in this case. The loss studied there was the MSE loss w.r.t. a ground truth model with the same architecture. The loss considered in the submitted manuscript is the MSE loss w.r.t. a ground truth model that uses ReLU activations in the hidden layer. The submitted manuscript thus also rules out the second STE variant as biased. \n\nThe authors cite the previous work Shekhovtsov & Yanush (2020) which derives STEs as a linearisation of an unbiased gradient estimator with low variance. In particular, it is shown there that the properly derived ST estimator is unbiased for models studied in the submitted manuscript, albeit for a different type of loss. It is clearly seen that their derived unbiased ST estimator breaks the scale invariance. I am pretty sure that the derivation given there can be generalised for the model & loss combination studied in the submitted manuscript.",
            "summary_of_the_review": "The paper provides a convincing in-depth analysis of STE properties for a particular simple combination of model and data distribution. However, the novelty is limited in view of the previous work Yin et al. (2019). Its significance is restricted due to the very specific assumptions on the model, data distribution and loss.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper investigates analytically the scale invariance properties of \ndifferent straight-through estimators used in training neural networks with binary activation and weights. \n\nThe main claim is that the cReLU STE, among the 3 considered, is the one that is more likely to give zero gradients in the minima of the population loss. This is (weakly) supported by theoretical analysis on a little variation of the shallow setting analyzed in Yin et al. (2019), where the teacher now contains ReLU activations.   \n",
            "main_review": "The paper contains little novelty compared to Yin et al. (2019)\nAlso, it is not well written. The claims are weak and it is not clear if they are specific to the specific setting chosen.\nThere is no numerical validation.\n\n- Claims such as\n\"\"\"\nConsidering the scale invariance of Relu STE gradient, this relation implies that the\nRelu gradient is approximately embedded in the cRelu STE gradient, so that if the back-\npropagation using Relu STE converges to a point, the back-propagation using cRelu STE\ncan also converge to that point.\n\"\"\"\nare not entirely clear nor sufficiently motivated.\n\n\n- The main claim reads\n\"\"\"\nIn other words, because of the breaking of the scale symmetry, the cRelu STE may pick up one of local minima,\nwhich degenerate in all scales, while because of keeping the scale symmetry, identity STE and Relu\nSTE may not. Therefore, we conclude that the back-propagation using cRelu STE is the most likely\nto achieve the true minimum in the three STEs.\n\"\"\"\nIn other words, the authors claim that some minima of the population loss of the original model *may* also be minima of the surrogate model with cReLU STE if the weights are appropriately rescaled. This claim has only the weak analytical support\ngiven by the analysis of the Gaussian input case of the next Section 3. No numerical support is provided for\nconvergence of the training dynamics in this toy model, let alone deeper networks and structured datasets.\n\n- In Section 3, the authors show that the cReLU STE has an additional stationary point (the global minimum) compared to the\nReLU STE. This is an interesting result, and also somehow natural given that a non-invariant gradient has more degree-of-freedoms\nfor satisfying the stationary point condition. The single setting explored, the one with the ReLU teacher, doesn't seem enough to support general claims though. Can the author check analytically a few other cases, e.g. tanh activation. Or at least experimentally (possibly also\nin deeper networks)? \n\n- Reviewing the paper was made more complicated by the poor writing, \nstarting from the very first sentence of the abstract  \"In the quantized network, its gradient shows either vanishing or diverging.\"\n",
            "summary_of_the_review": "While there is some novelty to the paper, I don't think the importance of the results and the novelty level\nare high enough for the ICLR standards.\nMoreover, there is a complete lack of experimental validation and some poor writing. \nI cannot recommend the manuscript for publication on ICLR. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}