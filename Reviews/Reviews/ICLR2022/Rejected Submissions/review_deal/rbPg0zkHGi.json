{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The submission considers a new acquisition function for active learning. The method considers the sensitivity of the prediction for a given datapoint with respect to parameter perturbations. Points with the largest variance under these perturbations are selected for labelling.  The method is simple and the empirical results are reasonable. Some weaknesses are the clarity of writing, and somewhat limited experimental comparisons.\n\nThe discussion was useful and helped improve the clarity. Additional experiments also helped improve the paper, although some reviewers still felt the experimental comparisons were lacking, including using entropy as a baseline acquisition function. Despite these improved scores, the overall average score remains below threshold I'm afraid.\n\nI feel this is a useful paper, but perhaps needs a little more polishing in the writing and some additional experiments. As such, it just falls short of the acceptance threshold."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper provides a new method for solving the active learning problem, i.e. choosing from a set of unlabeled examples that should be labelled and added to the training set. The proposed method makes use of 'noise stability' of inputs–specifically if the model output corresponding to an input changes to a large extent when small amounts of noise are added to the model parameters, then this input should be labelled. The authors compare their method to a range of existing AL methods and draw theoretical connections to predictive variance reduction.",
            "main_review": "## Strengths\n\n* **Relevance**: The active learning problem is an important one in today's age of 'big data' and deep learning. Deep neural networks are particularly data-hungry and we wish to be able to train them with as few data points as possible. However, as noted by the authors, deep neural networks are also pathologically overconfident in their predictions. Thus, predictive entropy (for example) cannot be reliably used as a proxy for the informativeness of unlabelled examples. Therefore, methods for AL with deep neural networks are of high relevance and importance to the field. \n\n* **Simplicity**: The proposed method is simple to understand and implement. Many state-of-the-art AL methods rely on complicated Bayesian approximations and are difficult to understand/implement. A simple method that works well is a good method!\n\n## Weaknesses\n\n* **Clarity**: I found the paper hard to understand in many places and there were many grammatical errors. Overall, I think a lot more effort needs to be put into making the ideas presented easy to understand and follow. I'll provide some specific examples here, however, this is a general comment for the paper. I encourage the authors to spend more time thinking about how best to present their ideas. I also encourage them to use a tool like Grammarly which will flag problematic sentences. \n    1.  \"This is also aligned with the pre-requisite of semi-supervised learning.\" – What is the pre-requisite of semi-supervised learning? What is the point of this sentence? \n    2. \"... namely uncertainty based method, aims to select a portion of the most uncertain or representative data ...\" – uncertain is not the same as representative so it is not clear exactly what is meant by this sentence. \n    3. What is the \"interlayer cushion\"? This seems like a pretty specific term so it should be explained. \n    4. What does \"tends to be easy to recognize future examples\" mean? Is this to say that the model is good at determining which examples should be labelled?\n   5. \"Specifically, we introduce a simple algorithm of uncertainty estimation by computing the distance between outputs of the clear and perturbed **input** ...\" (emphasis my own) – this sentence makes it sound like the inputs are also perturbed, when in fact it is only the model parameters that are perturbed. \n    6. \"Our method is easy to implement and free of customized auxiliary models. Therefore, ...\" – I think a bit more context is needed for what kinds of auxiliary models other AL methods require, in order for this statement to make sense to the reader. \n    7. \"pool of unlabelled data\" – I believe the standard term is \"pool set\".\n    8. \"labelled pool\" – this is just the train set.\n    9. I don't think the set update equations are needed to understand the moving of labelled examples from the pool set to the train set. If anything the equations make this seem more complicated than it actually is.\n    10. In section 3.2 the authors switch notation for the Jacobian of the output w.r.t the parameters. This doesn't seem necessary to me and makes things more difficult to follow. \n    11. \"Note we omit the notation of \\theta in J and A to avoid ambiguity\" – presumably the authors mean that the omission is okay because there is no ambiguity? \n    12. Where does equation 7 come from? Some elaboration would be helpful.\n    13. In equation 10, in the last line the numerator contains a J_\\hat{y}(x)^T. Should this be u(x)?\n\n\n* **Experimental evaluation**: while the experimental evaluation provides a good start (the ablations provided are especially appreciated), it is not of the standard required for an ICLR paper, in my opinion. I have concerns with:\n    1. **The narrow range of underlying tasks.** The tasks considered in the paper are all image classification tasks of various kinds. Regression tasks and non-computer vision tasks should be added. \n    2. **Missing baselines.** In particular, batch-aware acquisition strategies such as BatchBALD (Kirsch et al.) and sparse subset approximation (Pinsler et al.). The BALD acquisition function  (Houlsby et al.) is also missing and would likely improve the strength of the Dropout baseline.\n    3. **Experiments with *smaller* step sizes.** The chosen step sizes (e.g. 5% of MNIST = 3000 images) are very large. Most active learning works present results for step sizes of 1, 10, 50, etc. (see the experiments in the references below). With such a large step size a random (or near random) acquisition can easily perform well because there are already enough examples to train a decent model. With smaller step sizes the choice of acquired examples becomes much more important for good model performance. \n    4. The adaptive noise magnitude \\eta. An ablation, showing the performance when using a constant value for the noise magnitude (e.g. a value that does not depend on \\Delta\\theta^{(i)}, should be added to verify that this is an important part of the proposed method. \n    5. No runtime comparisons. In the text there are brief discussions of the runtime for the proposed method and some of the baselines, however there are no empirical results to go with the discussion. Runtime comparisons should ideally be provided for all baselines and the proposed method.\n\n* **Connections to other work.** The proposed method seems very closely related to doing active learning with a linearised Laplace approximation of the BNN posterior (see Immer et al. and Daxberger et al. for good descriptions of the linearised Laplace approximation). However this is not discussed at all. I suspect that this link could provided a more concrete link to the predictive variance (the provided connection being somewhat weak in my opinion.) \n\n\n## References\n\n### Active Learning \n\nAndreas Kirsch, Joost van Amersfoort, Yarin Gal:\nBatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning. NeurIPS 2019: 7024-7035\n\nRobert Pinsler, Jonathan Gordon, Eric T. Nalisnick, José Miguel Hernández-Lobato:\nBayesian Batch Active Learning as Sparse Subset Approximation. NeurIPS 2019: 6356-6367\n\nNeil Houlsby, Ferenc Huszár, Zoubin Ghahramani, and Máté Lengyel. Bayesian active learning for classification and preference learning. arXiv preprint arXiv:1112.5745, 2011\n\n### Linearised Laplace\n\nAlexander Immer, Matthias Bauer, Vincent Fortuin, Gunnar Rätsch, Mohammad Emtiyaz Khan:\nScalable Marginal Likelihood Estimation for Model Selection in Deep Learning. ICML 2021: 4563-4573\n\nErik A. Daxberger, Eric T. Nalisnick, James Urquhart Allingham, Javier Antorán, José Miguel Hernández-Lobato:\nBayesian Deep Learning via Subnetwork Inference. ICML 2021: 2510-2521\n",
            "summary_of_the_review": "While the proposed method is simple to understand and solves an important problem, I do not think the paper is of the standards required for ICLR primarily due to the weak experimental evaluation and lack of clarity in the writing. \n\n++++ Updated review ++++\n\nThe authors have provided many clarifications and additional experimental results which do strengthen the paper. As a result, I have increased my score from 3 to 5. Unfortunately, the experimental evaluation is still a little weak which prevents me from increasing my score further. Similarly, I am not sure that the updated presentation is quite clear enough.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a single-model active learning method for classification and segmentation. The key idea is to add noise to the parameters in order to generate multiple outputs and be able to estimate the uncertainty of a sample. The paper presents results for classification and segmentation in different datasets.",
            "main_review": "Strengths:\n- The idea is simple and easy to implement. \n- Results seem to generalize to different datasets.\n\nWeaknesses:\n- Links to calibration are confusing and not empirically supported.\n- Results are not convincing:\na) Large scale is needed (Imagenet for conclusive results)\nb) Ensembles are not compared (and MC Dropout results are somehow questionable). See below\nc) Compute cost is not provided. ",
            "summary_of_the_review": "The idea behind the paper is interesting. Instead of using MC Dropout of Ensemble methods trained with different seeds, use a single model and do multiple forward passes adding a perturbation to the parameters for computing the output distribution needed to estimate uncertainty.\n\nAs this is basically a multi-model approach (using variations of a single model), would be interesting to see a comparison with ensembles. In the end, ensembles are the best approximation to Bayesian Uncertainty estimation.  \nFor MC Dropout, I wonder about the parameters. I do believe the results are taken from one of the references, but details there are needed. MC Dropout easily outperforms single models like Entropy when the number of forward passes is large enough. How many models are used here? Would be good to see, at least 50-100 forward passes. It is actually surprising that MC Dropout performs on par or worse than random on CIFAR. \n\nSame with the proposed method. How does the method vary as a function of the number of forward passes? How does that compare to the same ensembles? A similar comparison was done (for classification) in: Chitta et al. Training Data Subset Search with Ensemble Active Learning arXiv 2020. The number of forward passes needed is larger compared to models trained with different seeds. \n\nThe paper talks about uncertainty calibration but there is no empirical evidence. That would be very interesting to see. \n\nI would also be interested in understanding how the noise is applied to the layers and if noise in all layers is needed. That would have an impact in the inference time needed for computing uncertainty. An ablation study here would be interesting. \n\nSame for cityscapes. I am missing Random as comparison. \n\n\nFor all the experiments, I would suggest adding Entropy as a baseline for comparison. Entropy is straightforward and provides very solid results. \n\n\n\n",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "A method for batch active learning for DNNs is proposed in the paper. The main idea is to consider robustness to parameter perturbation as an uncertainty measure. In other words, prediction deviation when adding a small noise to the parameters is used as a score for selecting data to be labeled in the active learning loop. The authors provide some theoretical analysis for the method to connect it to variance reduction. Experiments on several image classification and semantic segmentation datasets show that the method performs better than several SotA baselines.",
            "main_review": "+:\n\n+ The method is simple and does not involve auxiliary models.\n+ Experiments are performed on several tasks and datasets\n+ The paper provides some theoretical connection to variance reduction (they are straightforward though).\n\n\n-:\n- The clarity of the presentation can be improved in several places. For example:\nIt seems that in the current presentation, the assumption is that epsilon (noise magnitude) is the same for all parameters. It is not clear if this is implemented. Also, is this a valid assumption in terms of noise being relatively “small\"? Is the noise added to all parameters? One or more baselines are missing from some of the results figures without any explanation.\n\n- Since the proposed method directly works on the task model as opposed to some of the baselines, it would be helpful to see more models/architectures tested for each task.\n\n- The proposed method does not seem specific to image data, so evaluating it on tasks in other domains would be beneficial to validate its applicability across different domains.\n",
            "summary_of_the_review": "Overall, the paper proposes a simple method for active learning, provides some theoretical connection with existing literature, and shows its effectiveness where it improves SotA. While there is room for improving the paper, I tend towards acceptance. \n\n--Update--\nI think the authors have addressed most of the concerns and comments, and I remain positive about the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}