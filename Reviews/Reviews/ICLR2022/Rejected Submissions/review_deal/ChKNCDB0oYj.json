{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper received 3,3,1 as reviews. All reviewers have the consensus on the weaknesses, i.e. limited technical novelty and weak boost in performance in datasets that may not be the state of the art anymore. The authors have submitted a rebuttal however the rebuttal did not improve the score of the reviewers. Following the reviewers recommendation, the AC recommends rejection."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a solution for the imbalanced classification method.  Learning the CNN traditionally on training data, Finding the classes with low accuracy and generating/augmenting the samples from those classes aiming to improve the accuracy of those classes by fine-tuning the model on it, is the proposed solution.  Several experiments on datasets such as CIFAR10 is done. Results the feasibility and perfection of the idea.  For generating samples from classes with low accuracy,  a GAN is used. ",
            "main_review": "-The paper tries to adress an ongoing challenge in machine learning.   The idea is simple and efficiently works. However, I do not believe the method is novel. For example, for incremental class tasks with no samples from the previous classes, GANs are widely used to generate the new samples for those classes. Or even the GAN is widely used for augmentation. It seems this paper just exploited that method for a new task. \n\nBesides, as I understand, the low accuracy will be associated with the class with a limited number of samples. How do you train a GAN on such tiny samples? \n\nAlso, there are several methods for dealing with the imbalanced training set, for example, downsampling the small classes or upsampling the large classes. Did this method work better? If so, please put the result on the paper. \n",
            "summary_of_the_review": "-The paper is not novel. The idea is previously exploited for other tasks. \n-Experiments is not completed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies mistake-driven image classification, with the idea of applying data augumentation to classes with lower classification accuracy. Emphasizing the training on classes with low accuracy boosts the overall performance of trained CNN classifier. \n\nThe proposed method is quite straightforward, and the paper is clearly written. It first evaluates the classification accuracy of each class on the validation set, then selects the categories with lowest accracy for data augumentation with GAN, and add the augumented class into the training set for fine-tuning. Experiments are tested on 5 dataset, which demonstrate improved performance. ",
            "main_review": "Strength:\n1. The proposed method make sense and boosts the performance. \n2. The paper is clearly written and easy to repeat.\n\nWeakness:\n1. The proposed method in this paper can be regarded as an engineering trick on training procedure of CNN models. The adopted algorithms are existing ones or based on well-studied methods. This makes the method show limited technical novelty.\n2. The performance enhancements are quite marginal on those tested datasets. For most of cases, the improvement is less than 1%, which is not impressive, considering to cost of generating extra images and extra fine-tuning step.\n3. The experiments are not convincing. The baseline algorithms have already achieved good performance on five tested datasets, e.g., mostly higher than 95%. It is more convincing to conduct experiments on more challenging datasets and datasets with larger-scale, e.g., the Imagenet. \n4. This paper also lacks necessary ablation studies on some important parameters, like the nWPC.\n",
            "summary_of_the_review": "To summarize, this paper is clearly written, and demonstrates an engineering trick for training cnn classifiers. It boosts the classification accuracy at costs of larger computation and memory consumptions. The experiments are also not convincing enough.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper presents a method that uses a pertained model for classification in order to choose classes with low accuracy, then performs a GAN-based augmentation to those classes, and retrains the classification model with the augmented dataset. The claim is that this procedure improves the classification performance. There are several known components in the proposed architecture, and recent works such as SpinalNet, and FastGAN are utilized as a classifier and as a lightweight GAN synthesizer, respectively.",
            "main_review": "Strengths:\n- The paper proposes to engineer a way to augment low-accuracy class data through a recent lightweight GAN model.\n\nWeaknesses:\n- The novelty is limited, the combination of a pertained classifier and its output to evaluate low accuracy classes is followed by separate GAN generators for each of those classes.\n- Five relatively old datasets are used to evaluate the method. However, in the experimental results, it is not clear which and how many classes in those datasets were selected by the model as the lower accuracy classes, hence were augmented into the dataset, and how that affected the performance.\n- Also, it is not possible to judge the quality of the FastGAN-based  augmentations for the imbalanced (or low-accuracy?) classes, as they were not provided.\n",
            "summary_of_the_review": "My opinion is that the idea in the paper is at best a nice engineering idea, which aims to improve on the class imbalance problem, which typically lead to lower accuracy for those classes. \n\nThe novelty of the paper is limited, due to reasons listed above.  ICLR is not the appropriate venue to present this idea.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}