{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper experiments with a combination of Sparse MoEs and Ensembles on the Vision Transformer (ViT), showing improved performance. To efficiently combine Sparse MoEs and Ensembles, the paper presents Partitioned Batch Ensembles (PBE), where the parameters of the self-attention layers are shared, and an ensemble of Sparse MOEs are used for the MLP layers of the Transformer blocks.\n\nWhile reviewers agree that the proposed approach is interesting, they also point out several weaknesses, such as the limited novelty of the proposed method (a simple combination of existing techniques) and small experimental gains. They also pointed out several weakness related to the experimental part. While the authors responded in a very detailed manner to several of these points and presented several additional experiments, I feel this paper will benefit from consolidating all these new results and going through another round of reviews."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors show empirically that Sparse MOEs and Ensembles have complementary features, and suggest that combining the two should lead to improved performance. Authors build on the Vision Transformer (ViT) for their experiments. To efficiently combine Sparse MOEs and Ensembles, the paper presents Partitioned Batch Ensembles (PBE), where the parameters of the self-attention layers are shared, and an ensemble of Sparse MOEs are used for the MLP layers of the Transformer blocks.\n\n",
            "main_review": "1. PBEs share the attention layers and use an ensemble of sparse MOEs for the MLP layers of Vision Transformers. Any reasons for this design choice, and not going the other way, i.e. sharing the MLPs and using Sparse MOEs for the self-attention layers ?\n2. Results in Table 2 suggest prediction level ensembling can be beneficial for uncertainty calibration. The Table reports the results for K = 2 and K = 4. Does this claim hold true for K > 4 as well ?\n3. The multi-head MoE presented in Section 3.2 stacks the K selected expert predictions. Are these K predictions averaged in order to compute the NLL and Error scores reported in Table 2 ?\n4. Fig 2 a: For every value of M, the log likelihood score increases till K=5, and drops at K=6. Any reasons behind this ?\n5. The idea of Batch Ensembling and Tiling isn't new, and has been used in previous works [A].\n\n[A] Yeming Wen, Dustin Tran, and Jimmy Ba. Batch ensemble: an alternative approach to efficient\nensemble and lifelong learning. In ICLR, 2019.\n",
            "summary_of_the_review": "I only have minor concerns with the paper, and I have stated those concerns in the Main Review. Other than that, the paper is novel and the experiments demonstrate the usefulness of the proposed Partitioned Batch Ensembles (PBE).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper investigates the benefits of combining (Sparse) Mixture of Experts (MoE) and ensembling. Sparse MoE’s employ conditional computation to reduce computational and environmental costs of DNNs while maintaining (or increasing) performance. On the other hand, ensembling models has been shown to achieve the highest robustness in the presence of dataset shift, e.g., higher accuracy and better estimation of uncertainty.\n\nThe present submission empirically demonstrates that Sparse MoE’s can be ensembled to attain the benefits of both techniques simultaneously, i.e., conditional computation (with its implications to scalability) together with robustness in the presence of dataset shift. The main components of the approach are: 1) Disjoint MoE’s as ensemble members. 2) Tiling of representations which enables all ensemble members to compute the output for a batch in a single forward pass.\n\nA number of experiments compare predictive performance vs computational cost for the proposed approach (pBE), vision Sparse MoE’s (V-MoE) and Vision Transformers (ViT). The results show that, under some conditions, pBE displays the known benefits of Sparse MoE’s and ensembling, which leads to performance gains w.r.t. ViT and V-MoE under metrics like accuracy, negative log-likelihood (NLL) and expected calibration error (ECE).",
            "main_review": "Strengths:\n\nThe paper provides confirmation that MoEs and ensembles have complementary (or additive) benefits. Moreover, the proposed approach (partitioned batch ensembles), which combines MoEs and ensembling, achieves the benefits of both techniques simultaneously. \n\nThe paper provides the first study of the robustness of vision sparse MoE’s (parallel submission).\n\nWeaknesses:\n\nThe proposed approach is incremental. In the words of the authors, the method “introduces two changes to V-MoEs: (a) the partitioning of the experts and (b) the tiling of the representations.”\n\nThe paper focuses on the comparison of specific architectures and training procedures. In this regard, conclusions one may be able to draw from the paper are too constrained. For example, can the benefit of “static” ensembles be achieved by “adaptive” ensembling? While this paper’s results suggest perhaps not, I don’t think this is conclusive. [The distinction between static and adaptive may be generalized in such a way as to enable finer control on aspect and degree of adaptation with implications to, e.g., the diversity and uncertainty calibration at the output of the ensemble.] Further, e.g., (Wen et al. 2020) claim that “diversity of initialization entirely determines the diversity of ensembling system.” The present paper notes ways to achieve diversity given a particular distinction between static vs adaptive ensembling, and training procedure. There are other dimensions (e.g., initialization, example balancing) which have direct relevance to diversity and thus, uncertainty calibration, which are not explored in this paper.\n\nSection 3.1 is an ablation study just like section 4.2 is. I find the discussion going back and forth to be detrimental to clarity and efficiency of communication.\n\nSection 3.2 compares a naive multi-head method -- without specifying how it is trained -- with a “standard” vision (sparse) MoE. I don’t find the results there clear enough to be compelling. For example, whether the multi-head variant is able to provide diverse predictions is more likely to do with the training procedure than with the architecture -- this is related, e.g., to the “load balancing” during training used in (Riquelme et al. 2021). Besides, the claims in section 3.2 are conflicting: we are told naive multi-head improves ECE but also that a different strategy is required for uncertainty calibration.\n\nThe experimental results (section 5) are simply stated. For example, most of the time (static) ensembling helps but when it doesn’t (e.g., out of distribution performance) the paper provides no discussion or insight.\n\nSome acronyms are used without introduction (the introduction appears after the first usage), e.g., OOD and NLL in page 4.\n\nIt’s not clear to me how the static ensembling portion of the proposed approach is “efficient” (section 4.1). For example, Batch Ensemble (Wen et al. 2020) generates the family of ensemble weights via element-wise multiplication of a shared matrix and “fast-weights.” In my understanding, in the case of the proposed approach any sharing of parameters is done within the ensemble members.\n\nHow are the dashed lines in figures 4 and 5 pareto frontiers? ",
            "summary_of_the_review": "Overall I lean towards rejection. While the paper presents experimental results likely not found elsewhere, in my perception, that appears to be the sole reason for the paper. Specifically, the paper presents experimental results on the robustness of V-MoE’s (vision sparse MoE’s) and adds ensembling on top of the V-MoE model to increase robustness. However, no further insight or development (e.g., in architecture or optimization) is provided.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper considers the combination of MoE and Ensembles for image recognition tasks to obtain both improvement of classification accuracy as well as stability of prediction. The proposed method is evaluated with extensive expereiments from large-scale image recognition to OOD and the results indicate better performance for the proposed method.",
            "main_review": "The paper is well written and the experiments are comprehensive. The main concerns about this paper are as follows.\n1. The paper lacks novelty. It just combines the MoE with Ensemble in a trivial manner. The tiling technique is quite simple and it is just the classical way of doing ensemble across models.\n2. The improvement is quite limited. In Table 3, I find although the proposed method achieves gains across different metrics e.g. NLL, Error, and ECE, the improvement in terms of accuracy is less than 0.5% which is quite small and cannot justify the effectiveness of the method. Furthermore, when compared to ViT and V-MoE in Figure 4,5,6, I can only find the curve for NLL instead of Error, which I think is more critical. \n3. The number of parameters should be reported and compared to ViT, since MoE is a parameter-consuming method and we cannot ignore this factor.\n4. The FLOPs cost by tiling and partitioning is another concern. More details about how to compute FLOPs for the proposed method and V-MoE should be clarified. For V-MoE, it is possible to first compute the coefficient for each expert and then synthesize the experts before extracting features. Therefore, it can save a large scale of computational costs. But due to tiling and partitioning, each branch should cause its own computation, which I think will cost a lot of extra computation and let me doubt the efficiency of the proposed method. I wonder whether the author computes the FLOPs with the optimal way.",
            "summary_of_the_review": "The paper still has a big space for further improvement. I recommend rejecting it this time.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}