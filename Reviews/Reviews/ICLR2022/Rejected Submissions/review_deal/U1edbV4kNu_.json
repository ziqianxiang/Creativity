{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "Overall, the reviewers thought this paper suggested an important problem.  However, there were many concens.  Particularly, the multiple reviewers felt it was unclear when the new approach is better than prior work. The reviewers had difficulty connecting the experiments to the paper's main claims."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper describes a novel parallelism strategy for training large neural networks: compute devices are grouped into “swarms”, pipeline parallelism is applied within each swarm, and pipeline parallelism is applied between swarms. Communication between swarms is randomized and swarms are periodically rebalanced, making this approach amenable to systems with heterogeneous and/or unreliable devices of different sizes and capacities. The authors discuss two techniques to reduce the communication overhead: increasing the compute volume (since it tends to grow faster than the data volume) and modifying the model architecture (compression-aware architecture) to reduce the data volume at pipeline boundaries. The authors empirically investigate the effect of these techniques when training Transformer models and they apply SWARM parallelism to train in a cloud environment with heterogeneous GPUs.",
            "main_review": "The authors present an elegant approach for training neural networks at scale without a dedicated HPC system. It addresses an important need in the community and is likely to have a significant impact. SWARM parallelism is essentially a generalization of pipeline parallelism, with similar benefits (efficiently increasing model capacity) and downsides (communication overhead, need for micro batching, bubble overhead). The dynamic swarm rebalancing does alleviate some of the load balance issues in pipeline parallelism.\n\nThe communication overhead is especially painful in environments with high-latency, low-bandwidth communication. Reducing the proportion of time spent in communication by increasing the model size will improve the FLOP throughput, but then the authors must justify that this doesn’t hurt the truly important metrics: learning quality and time-to-solution. The baseline model in Section 4.2 is not convincing: it is about 2x larger than GPT-2 Small but achieves worse learning quality. This is further confused by the different compute budgets when training the two models. That said, it is a nice result that compressing the pipeline boundaries with maxout only resulted in modest degradations to learning quality. It will be nice to see if the authors could get comparable results to an existing Transformer model only using the maxout trick.\n\nFinally, the large-scale training in a cloud environment is a good proof-of-concept for the effectiveness of SWARM parallelism. One can estimate the authors achieve 61% of peak throughput when training with 400 T4s and 56 A100s. However, as discussed above, the authors should demonstrate that the final model achieves a competitive learning quality in a reasonable time frame. Also, more thorough scaling studies, with and without device heterogeneity, would be helpful in understanding this algorithm’s behavior at scale.\n\nMinor issues (mostly typo and grammar errors):\n1. .....as each machine can be leave training abruptly....\n2. Repeated \"that that \" on page 6\n3. Repeated \"memory memory \" on page 16",
            "summary_of_the_review": "SWARM parallelism is an elegant approach to train large neural networks in non-HPC settings and it has been shown to achieve decent compute throughput in a cloud environment. However, it requires several model modifications to run efficiently, and it has not been shown that these changes do not degrade the learning quality or slow down training.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper investigates the possibilities to train large model-parallel neural networks (e.g. large transformer networks) via pipelining on heterogeneous distributed hardware architectures. The rational behind their work is that full HPC environments and large-scale super-computers are not available to many researchers, especially in developing countries, and hence a method for training on cost-efficient distributed „preemtible“ hardware instances would be favorable in these cases. Specifically they employ model pipelining (distributing layers across nodes) together with random dynamic  pipeline routing and data compression, and evaluate their algorithm on training  GPT-2 on cloud T4 and A100 GPUs.",
            "main_review": "Whilst the research question at hand is definitely valuable and of importance, the study presented by the authors presents with several shortcomings.\n- The background and related work section lacks detail in the field of model-parallelism, see e.g.\nBen-Nun, T., & Hoefler, T. (2019). Demystifying parallel and distributed deep learning: An in-depth concurrency analysis. ACM Computing Surveys (CSUR), 52(4), 1-43.\nOn the other hand, the section on data-parallelism is quite extensive even though not really relevant for this paper. Moreover, some of the work in this section is comparably old and not state-of-the-art any more\n- The authors formulate a Square-Cube-Law to justify their choice for pipelining as parallelisation strategy. They argue that for neural network layers relying on matrix multiplication (MLP, Attention), the computation of the layer scales with O(n^3) (where n is the number of nodes), while communication between layers only scales with O(n^2), hence for large layers, computation will be the bottleneck. However, this is a very theoretical assumption and the employed scaling rates are not true in practice:\n - Matrix multiplication only scales O(n^3) on paper. Modern Algorithms and frameworks are highly optimized for this kind of operation, accelerating it. Moreover, modern highly-parallel Hardware (i.e. GPUs) yields approximately linear scalability with respect to the number of compute units (The A100s they use in their experiments alone provide 8192 nodes each)\n- Likewise their assumption for the communication part with O(n^2) is not correct, as modern communication schemes perform operations like Allreduce (which is  the most commonly used communication for such distributed models) in O(n log(n)).\n- Even if the assumptions on the Square-Cube-Principle were correct, the authors provide not thorough/rigorous experimental validation/proof of this law, but only a single measurement to demonstrate that for a few sample cases, computation is higher then communication (Fig. 1 and 3)\n- One of the main components of their algorithm, the stochastic rewiring of the pipeline connections is neither explained nor demonstrated. How does this work? How do you determine the used weights? Why do you use 300s as time interval for the round robin algorithm? I would assume the length of the computation window is highly problem-dependent. How do you handle adaptive rebalancing, avoiding cascades? One can easily think of situation where all peers in one swarm are idle and hence all leave to an other stage, leaving the swarm empty? How do you avoid such deadlocks?\nAlso in the experiments, the proclaimed superiority of SWARM when confronted with peers leaving or joining is not tested or demonstrated. And while you show convergence in the same number of iterations as the original algorithm, not a single word is spent on actual compute time? How long does training actually run? \n- One of the main motivations of their work is that highly specialized hardware is expensive and hence not available to many researchers in developing countries. Yet all experiments are run on T4 cloud devices and A100s. This is not cheap hardware! The argument is that one could use many cheap devices instead of fewer high-end ones. How many of these devices would I actually need? And what about the connections between those devices? Would the cost of the network not outweigh the saving for compute devices?\n",
            "summary_of_the_review": "The topic studied is definitely of interest, yet the authors do not manage to sufficiently explain their method. Furthermore, they fail to demonstrate the advantages of their method that they claim. The experimental part is lacking substantial thoroughness, and more studies should be conducted in that regard.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper focuses on the model-parallelism setting which is commonly used for training very large neural networks due to its memory efficiency. Today, model-parallelism is mainly used on dedicated HPC clusters which are not available to most researchers, especially for those in developing countries. Existing cost-effective algorithms can significantly reduce the cost of training a neural network, but they currently do not support the model-parallelism setting. Section 2 provides a well-written background on existing techniques in model-parallelism and communication reduction.\n\nTo overcome the limitations of existing algorithms, the paper presents the SWARM algorithm - a decentralized model-parallel algorithm that supports heterogeneous devices and node failure recovery. In SWARM, the devices are dynamically assigned to stages that are load-balanced (adaptive swarm rebalancing) throughout the training. Each device dynamically communicates with a device in the subsequent stage in proportion to their training throughput (stochastic wiring). The forward and backward computations in a stage are computed in parallel across devices. Once enough gradients are accumulated, the devices run an All-Reduce (which can be executed asynchronously in the background) to aggregate gradients within their respective pipeline stages and run a global optimizer step. Finally, a compression-aware architecture modification is presented which reduces the dimension of the edge stage layers. This decreases the communication overhead and is further combined with existing compression algorithms.\n\nThe experiments section, which I found to be the weakest point in the paper, evaluates the SWARM algorithm and the proposed Transformer architecture with compression-aware modifications. Section 4.1 presents the idle time of the different neural architecture where the newly proposed Transformer architecture has the least idle time. However, the comparison is executed in different settings and configurations, such as 12 vs 1 layer per pipeline stage and 8-bit activation quantization which is not a novelty for this work. Section 4.2 compares the perplexity on different NLP datasets. The performance of the new architecture degrades as the number of stages is increased and therefore the solution is limited to a small number of stages. Furthermore, the speedup gains of the different compression ratios are missing, and therefore it is unclear the direct impact of the compressions other than increasing the perplexity. Finally, Section 4.3 presents a real-world large-scale experiment with 400 T4 GPUs on the Pile dataset. However, it is unclear the speedup gains of the SWARM algorithm, nor is SWARM compared to any existing model-parallel techniques\n",
            "main_review": "Pros:\n1. The proposed SWARM algorithm design addresses some key shortcomings of existing algorithms such as fault-tolerance, load-balancing, heterogeneous devices, and low bandwidth.\n2. The paper addresses an important problem that can reduce the cost of training very large neural networks and make the training accessible to more researchers.\n3. The paper is well-written and a thorough background is given. \n4. The code and hyperparameters are detailed in an anonymous GitHub repository including recipes for reproducing the experiments.\n\nCons:\n1. My biggest concern is that there is not a single speedup comparison to existing model-parallel techniques (such as [1]) or ZeRO offloading [2]. \n2. The additional compute time of adding the bottleneck layers should be presented and justified as it might overshadow the benefits of the compression-aware technique. \n3. Table 2 shows that adding more stages increases the perplexity (worse) and therefore the solution is limited to a small number of stages. Furthermore, it is unclear why GPT-2 is not the better choice as it has less than half of the parameters and significantly better perplexity. Finally, the speedup gains of the different compression ratios are missing, and therefore it is unclear the direct impact of the compressions other than increasing the perplexity.\n4. The paper claims to overcome high latency, however, it does not provide a solution or present justifying results. Although the compression-aware technique addresses the low bandwidth limitations, it does not hold for high latency.\n5. The square-cube law was first presented in the introduction section as “a counterintuitive observation that, for some methods, training larger models can actually decrease the network utilization”. However, the given experimental results clearly show the opposite, larger networks have less idle time. In no place throughout the paper have I seen that the first claim been justified.\n\nHaving some speedup experiments and comparison to existing algorithms would help increase the score. There is a misconception about ZeRO in Appendix C: Ring All-Reduce is used in practice for gradient aggregation, and therefore the communications of a single device would be 2GB and not 200GB as stated.\n\nTypos:\n1. Page 1 Line #7: “both these” → “both of these”\n2. Page 4 Line #13: “in Section 4.1 demonstrates” → “Section 4.1 demonstrates”\n3. Page 6 Line 19: “finds the most” → “to the most”\n4. Page 6 Line 34: “where” → “, where”\n5. Page 7 Line 1: “Where” → “where”\n\n[1] Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen, M., Lee, H., Ngiam, J., Le, Q.V. and Wu, Y., 2019. Gpipe: Efficient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems, 32, pp.103-112.\n[2] Ren, J., Rajbhandari, S., Aminabadi, R.Y., Ruwase, O., Yang, S., Zhang, M., Li, D. and He, Y., 2021. Zero-offload: Democratizing billion-scale model training. arXiv preprint arXiv:2101.06840.",
            "summary_of_the_review": "I recommend rejecting this paper in its current form. Although the paper is well-written and I liked the proposed algorithm, I found the experimental section to be lacking crucial experiments to justify the paper’s claims. My biggest concern is that there is not a single speedup comparison to existing model-parallel techniques. Furthermore, several key details regarding accuracy drops and computation overhead negate the paper’s empirical impact. Having some speedup experiments and comparison to existing algorithms would help increase the score.\n\n\n### Update Following Revision #2\nThroughout the rebuttal process, most of my questions and concerns were fulfilled by both answers and paper revisions. I'm still concerned about the proposed compression technique, which I find the weakest part of this paper. However, with that, I think this paper in its current form should be accepted. The paper addresses an important topic with a high potential of impact on the ML community, both as a tool and research direction.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a novel model-parallel distributed training framework named Stochastically Wired Adaptively Rebalanced Model Parallelism (SWARM) to improve the training efficiency of large-scale models using unreliable heterogeneous devices and slow interconnect. This method is based on an observation for pipeline-parallelism methods, which need to communicate activations instead of model parameters/ gradients of parameters. The observation is that training larger models with pipeline-parallel will reduce the relative overhead of communication. Combined with compression techniques, the proposed method can train a billion-scale Transformer language model with high training throughput on preemptible low-power T4 GPUs and <400Mb/s network. ",
            "main_review": "Strengths:\n1. The literature review of this paper is adequate and great.\n2. The proposed SWARM framework is novel and practical. The authors \"square-cube law\" of communication-computation relation in pipeline parallel is convincing. The description of the framework is pretty clear.\n3. The authors verify their idea on both simulated and real scenarios, which makes the result quite solid.\n\n\nWeakness:\nThe authors should provide more details about their methods, including:\n1. How to choose peers to construct a swarm for each stage? Since the framework is designed for heterogeneous devices, which may of different geographic, computation capacities, interconnect speeds, etc., and within each swarm, all reduce operation is required to aggregate gradients, it would be better to describe your swarm construction method and provide some experimental results to show the advantage of your construction method. Additionally, the swarm division is changeable, is it possible to make the pipeline stage division changeable too?\n2. Is gradient compression techniques required during all-reduce?\n3. Could you describe \"Stochastic wiring\" in more detail? Does that mean the device wirings among different stages are fixed during a period of time and modified dynamically after this time slot? If that is true, besides interleaved weighted round-robin algorithm, could you give more details about how to wire these devices, for example, which factors should be taken into account during wiring? \n4. Could you give some experimental evaluation about the effect of stochastic wiring and adaptive swarm rebalancing?\n5. For the compression techniques comparison, besides end-to-end evaluation on ppl, it would be better to compare their reconstruction error additionally.\n6. For experiment part in section 4.1, it would be better to put the configuration \"12 servers for 12 transformer layers\" in this part instead of in supplementary to help authors understand well. And please keep your description consistent in Figure 3, Table 1 and the main body (BERT-base vs base, BERT-large vs xxlarge).\n7. For the results in Table 1, whey the GPU utilization for base model so small even when latency is none? How do you get the conclusion that \"these models maintain most of their training efficiency at 100ms latency\"?\n8. Could you explain the throughput relationship among T4,A100 and T4&A100 ? Why is the T4&A100's throughput much better than others?\n9. For real-world scenario, it would be better to provide a visualization figures about dynamic stochastic device wiring and adaptive swarm rebalancing during training.   \n",
            "summary_of_the_review": "This paper proposes a novel and practical method to do pipeline-parallel training for large scale models. The literature review is adequate and experimental evaluation is convincing. Although some details of the system design is missing, I am more than happy to raise my score is they can add those details to make this paper more concrete and useful. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety"
            ],
            "details_of_ethics_concerns": "In the scenario the authors work on, it seems training data has to be distributed to volunteered training machines, which would increase the data leakage risk.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}