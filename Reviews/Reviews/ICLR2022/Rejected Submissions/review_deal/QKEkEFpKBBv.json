{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "All reviewers concur that the paper has promise, but fails to deliver on that promise.  The idea of learning potentials based on DNNs is appreciated, but the evaluation of the contribution is considered lacking by all reviewers.  In addition, reviewers note that the training is not differentiable, which the rebuttal acknowledges is future work.\n\nI do not reject the paper simply for failing to beat a deep learning baseline, but for having chosen applications which do not even test the paper's hypotheses: reviewers note that the models are tree structured, so loopy BP is not tested, despite the revised paper's claim that \"the inference strategy is compatible with graphs containing cycles\"."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "  The paper proposes an extension of particle belief\n  propagation which allows the factor parameters (e.g.,\n  neural network weights for factors modeled as NNs) to be\n  learned using standard stochastic gradient descent.\n\n  This algorithm is then applied to several\n  continuous-domain state estimation tasks involving\n  articulated objects (e.g., hand pose estimation from\n  images).\n\n  The main contribution of the paper is adapting an\n  existing line of nonparametric belief propagation methods\n  to this setting, which allows (partial) end-to-end\n  learning.\n\n  The learning is only partly end-to-end as the particle\n  resampling stage is non-differentiable. Instead, the\n  proposed approach supervises the belief at every step of\n  the algorithm, but does not backpropagate through the\n  entire inference procedure opting to instead maximize the\n  belief of the GT values for the unobserved labels at each\n  step.\n\n  Experimental results on synthetic and real datasets for\n  articulated object state estimation show that the\n  proposed algorithm is able to learn meaningful NNs for\n  the unary and pairwise potentials while also producing\n  reasonable uncertainty estimates. The proposed approach\n  sometimes outperforms the baselines (e.g., an LSTM), but\n  in the task of parametric human hand tracking, it lags\n  behind the state of the art by a large margin.\n\n",
            "main_review": "## Strengths\n\n  - [S1] The proposed method produces meaningful uncertainty estimates,\n    improving state estimation robustness under\n    uncertainty/occlusion.\n  - [S2] The writing is good.\n  - [S3] The paper potentially opens up future avenues of\n    work that build on the proposed framework, improving\n    learning efficiency, tackling a wider range of\n    problems, etc.\n\n\n## Limitations\n\n  - [L1] Limited empirical performance - the proposed\n    approach seems to struggle to outperform a much simpler\n    LSTM baseline. While the proposed DNBP does provide\n    uncertainty estimates which are unavailable with the\n    LSTM, there are many ways to add, train, and calibrate\n    such outputs for NNs as well. However, such baselines\n    are not evaluated.\n  - [L2] The reasons why Hernando et al. outperform the\n    proposed approach are not discussed at all, in spite of\n    the large gap between the two methods.\n  - [L3] Training is not end-to-end, as backpropagation\n    does not occur through the particle resampling stage.\n  - [L4] The comparisons to related papers which may, for\n    example, also use NN factors but perform inference in a\n    different way are a bit too brief in my opinion. For\n    readers who are not experts in graphical models it may\n    be difficult to establish what this paper does\n    differently, and what its strengths and limitations are\n    w.r.t. the rest of the literature.\n\n\n## Suggestions (Per-Section)\n\n### Related Work\n\n  - Could you please elaborate on the differences between\n    the setting of the current paper and that of, e.g.,\n    Xiong & Ruozzi? That paper also seems to learn the deep\n    potentials end to end. I did not read that paper in\n    detail, but is the main idea that they use a\n    variational method rather than belief propagation?\n\n\n### Results\n\n  - Can you please make the text on Fig. 3 a bit bigger,\n    especially the legend? Currently the legend is\n    difficult to read.\n\n",
            "summary_of_the_review": "  While I think that there is definitely a lot of potential\n  in the paper, there are several areas that remain open to\n  improvement. First, the comparisons to related work are a\n  bit unclear to me, and explaining the trade-offs between\n  different kinds of inference methods, especially through\n  the lens of data-driven factor learning, would help\n  position the paper better in this area of ML.\n\n  Second, the empirical evaluation is not very thorough,\n  and its results don't improve much over simple\n  baselines like an LSTM. A broader evaluation, including\n  on tasks where formulating \"hand-engineered\" potentials\n  is more difficult would strengthen the paper in my\n  opinion.\n\n  Because of this I would recommend rejection for now\n  unless the above issues are fully clarified in upcoming\n  communications. That being said, I am likely to be the\n  least experienced of this paper's reviewers when it comes\n  to graphical models, so please take this into\n  consideration when evaluating my reviews (this is\n  addressed to both the authors, as well as to the ACs).\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/A",
            "recommendation": "3: reject, not good enough",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Supervised learning of Markov Random Fields (MRF) by MLE requires to compute pairwise and unary marginals of the current model estimate at each iteration of the likelihood maximisation. This task is not tractable except for MRFs on trees with finite hidden state spaces. The authors consider MRFs with infinite state spaces, propose to model the pairwise and unary potentials by neural nets and aim at developing an approximated belief propagation (BP) approach for learning these networks. BP is known to be exact on trees but is not tractable for infinite state spaces. The authors apply their method on a challenging task of hand pose estimation in  RGB-D image sequences taken from the first-person perspective.",
            "main_review": "**Strengths:**\n The paper aims at a differentiable approach for supervised learning and prediction in MRFs with infinite state spaces.\n \n **Weaknesses:**\n It is well known that loopy BP inherently fails in estimating pairwise marginals for MRFs on graphs with cycles. The examples analysed in experiments are MRF on trees. The authors do not clearly state that in such cases there is no need for loopy BP.\n \n The proposed BP approach employs a series of approximations which are not concisely explained. Some of them are given in the supplements only. E.g.:\n  - what is meant by pairwise sampling network? (Sec. 4 and line 6 of the Algorithm in the supplement)\n  - The pull strategy approximation is adopted from Desingh et al. (2019). The quality of this approximation is not evaluated.\n  - Further empirical approximations of the BP approach (which is itself an approximation) are explained in the supplement even though they are part of the proposed approach. Again, their approximation quality is not evaluated. \n\nOverall, I can imagine that such \"approximations of approximations\" can be useful if the main goal is to solve a particular application and they are unavoidable. However, this is not adequate if the goal is to develop a conceptual approach.\n\nThe authors mention the work Xiong & Ruozzi, (2020), where the task of computing unary and pairwise marginals is solved approximately by variational inference and then combined with learning of MRF potentials given by neural networks. This approach has the potential to learn MRFs on graphs with cycles. I would have expected at least an experimental comparison with this method.\n\nFurthermore, it is well known that supervised learning of MRFs with finite state spaces can be approached by using the Pseudo-likelihood estimator instead of the ML estimator. This obviates the need of computing unary and pairwise marginals during learning. I would expect that it is much easier to adapt the PL estimator to the case of infinite state spaces.\n\nThe artificial examples studied in the experiments and seemingly also the human hand tracking use MRFs on trees. This would obviate the need  for loopy BP, because in this case a standard two-pass BP can be used instead. \n\nIt seems that the proposed approach is worse (w.r.t. estimation) than the neural network baseline (Fig. 8)? This holds both, for the frame by frame variant and the tracking variant of the proposed approach? It remains unclear to me how the latter tracking variant works precisely.",
            "summary_of_the_review": "The proposed approach for learning MRF parameters by combining neural networks with an approximated belief propagation method is not concisely explained and involves (in my view) too many ad-hoc approximations. The experimental results are not convincing enough to compensate the conceptual weaknesses.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper enables end-to-end learning of the factors of a graphical model for nonparametric belief propagation (NBP) methods by using neural networks. It calls this method \"Differentiable nonparametric belief propagation\" (DNBP). \n\nThe aim is to replace domain-specific hand crafted factors with learned factors, by replacing each factor with a neural network. Compared to vanilla neural net based solutions, DNBP also reports uncertainty.\n\nThe method is evaluated on a couple of toy examples of articulated pose tracking, as well as using hand pose estimation on the FPHAB dataset.\n\nThe method is compared against learned, neural network based baselines.\n",
            "main_review": "Strengths:\nFor readers who aren't sufficiently knowledgeable on BP techniques, the paper does a good job of going over the basics of BP. \nThe paper is clearly written, easy to understand.\n\nWeaknesses:\nThe main issue I have with the paper is that I find the results of the paper weak:\n1) The chosen baseline is LSTM, and it is trained with an extremely small batch size (6). This doesn't provide enough confidence that the DNBP is any better than a simple neural approach. Quantitative differences are very small, as are the toy datasets. \n2) LSTM is not the natural choice for graphical models. Graph Neural Networks (GNN) would be far more appropriate. In fact, GNNs are shown to beat BP techniques significantly in this uncited paper: Yoon et al. \"Inference in Probabilistic Graphical Models by Graph Neural Networks\". \n3) Why is DNBP not compared against NBP or some other BP method? The paper mentioned above does compare against BP for instance. The paper doesn't give a good reason to choose this method over others.\n4) Qualitatively, hand tracking results are significantly worse than any other recent hand tracking method I know, so it is hard to judge how well the method works. The jitter is extremely high, which would be the opposite of what I'd expect from a method that takes uncertainties into account.\n5) I'd expect the uncertainty estimates to be elongated along the bones for the hand tracking results. For some reason DBNP seems to somehow claim that keypoints can be significantly outside of the hand area. Especially for a depth based method, where background pixels are very far away, this doesn't make any sense. Likewise for the toy dataset results with occlusion, DNBP doesn't seem to do a better job than the simplistic LSTM. \n6) Other Neural Net based methods that can measure uncertainty are not mentioned. For instance, Kumar et al.'s \"LUVLi Face Alignment: Estimating Landmarks' Location, Uncertainty, and Visibility Likelihood\" paper uses Gaussian and Laplacian log-likelhood losses, and their uncertainty estimates align much better with the face features, e.g. elongated along the jawline. Here we don't see the same property. Also that paper has a good methodology to check how informative the uncertainties are. I don't see a similar analysis in this paper.\n\n\n\n\n",
            "summary_of_the_review": "While I'm not an expert on BP techniques and haven't verified the math (hence the low confidence), I've found the paper to be weak on experimental results. The LSTM baselines are very weak, and some important citations are missing. Uncertainty visualizations for hands do not seem sensible, and the uncertainty estimates are not analyzed in depth. There are no comparisons with other BP methods, as well as other uncertainty prediction techniques. Even if the approach seems sound, the results don't justify adopting this challenging-to-implement method over any other regular method.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The method (DNBP) proposed in the paper considers a non-parametric belief propagation method where the unary potential functions, the pairwise potential functions and the particle diffusion function are modeled as feed-forward neural networks. It allows them to learn the parameters of these networks using labeled data which is the main contribution of the paper.\n\nDNBP is evaluated on three tasks (simulated double pendulum, simulated articulated spider, first-person hand action). In each application, DNBP is not able to outperform the considered baseline but it is able to provide measures of uncertainty associated with its predictions.\n",
            "main_review": "## Strengths\n\n1. The idea of replacing the handcrafted potential functions with feed-forward neural networks is interesting.\n\n2. The paper is well written and easy to read.\n\n3. The implementation of the gradients is not straightforward, i.e it does not simply consist of building the computational graph and back-propagating through it.\n\n## Weaknesses\n\n1. The main weakness to me, which is also pointed out in the conclusion, is the fact that the graph structure needs to be provided (i.e. it is handcrafted). Moreover, if I am not mistaken, the paper only considers trees (i.e. graphs without loops). These two limitations are (likely) the reason why DNBP is not able to outperform the considered baseline in each application (it is even significantly outperformed by the method Hernando et al. on the hand pose tracking task).\n\n2. In section 4 \"Pairwise potential functions\", it looks like the pairwise sampling network $\\psi_{sd}^{\\sim}$ output is deterministic. Shouldn't it be stochastic (since it is a \"sampling\" network)? Please provide more details about this network.\n\n3. The main benefit of DNBP over classical neural networks, is its ability to associate a measure of uncertainty to its predictions but the three applications that are considered do not accentuate this benefit. I suggest to evaluate DNBP on an application where its ability to associate a measure of uncertainty is very important/useful.",
            "summary_of_the_review": "Currently, the weaknesses outweigh the strengths. Especially the fact that the handcrafted graph structure is probably the reason why DNBP is not able to outperform the considered baselines.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}