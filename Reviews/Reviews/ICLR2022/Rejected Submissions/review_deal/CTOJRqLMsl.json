{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper theoretically studies the convergence of memory-based continual learning with stochastic gradient descent, and suggested several methods based on adaptive learning rates.\n\nThe reviewers appreciated the novelty of the direction, and some of them thought the experimental results are promising.\n\nHowever, most reviewers (3/4) were negative. I think the main reason was the paper presentation and clarity, which they found lacking (and I agree). One reviewer thought the experimental evaluation should be improved, but there might have been some misunderstanding there. Lastly, even the positive reviewer thought the results were somewhat incremental and non-surprising.\n\nI hope the authors improve their paper and re-submit."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors propose a stochastic gradient descent algorithm with adaptive learning rates for continual learning providing an  analysis of convergence. The main idea is to formulate the problem as a finite sum of non-convex objective functions with each component of the sum corresponding to a different task. The authors claim that the use of adaptive learning rates allow them to better control the relative importance of different tasks. ",
            "main_review": "\nStrengths \n\nThe use of adaptive learning rates in SGD provides a novel formulation on the continual learning problem. Experimental results show the merits of the proposes approach as compared to other state-the-art algorithms while a theoretical analysis of convergence is presented.\n\nRemarks/Weaknesses\n- The authors should make more clear the contributions of the current paper as compared to other relevant state-of-the-art algorithms possibly enumerating them.\n \n- The writing in poor in some parts of the paper making it difficult for the reader to grasp the meaning. \nE.g.  \n       a) last 3 sentences  of page 2 . What the authors mean by saying \"..smaller than the loss gap of general SGD...\"\n       b) Section 3, First paragraph : \"We use the convergence rate..., which denotes the IFO complexity ...\"\n- The two figs in Figure 2 are not well explained in the caption provided.\n\n- The overfitting and catastrophic forgetting terms $B_t, \\Gamma_t$ appears rather abruptly in the text and are not well explained.\n\n- Subsection 3.3. First sentence:  \"...the model $x^t $eventually converges to $M\\cup C$, not $P\\cup C$. It is not clear what the authors mean by saying that $x^t$ converges to $M\\cup C$. \n\n- Lemma 3: What does it mean that the convergence rate is ON $M\\cup C$? This looks rather weird, again it is not clear at all what the author mean by that.\n\n\n\n\n",
            "summary_of_the_review": "The paper proposes a  stochastic gradient descent algorithms applied on finite sums of nonconvex functions. The main novelty of the paper lies in the application of  the SGD ideas in the context of the continual learning.  In principle, this seems to be an interesting direction. However the  writing is rather poor in general and the authors fail to give an intuition of theoretical results presented in the paper. There also seems to be some wordings that make it difficult to understand some technical details of the paper (see above). I believe that the paper in its current form does not meet the high standards of ICLR.\n\n--------------------------------------------\nComment after authors' responses:\nI appreciate the efforts of the reviewers to address my comments and the changes they have made. However, I will keep my score to 5 since I believe the paper still lacks clarity of presentation and the significance of the contributions is not sufficiently illustrated. Moreover, the writing needs improvement and I encourage the authors to further work on it and resubmit an updated version of it to a future conference.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the authors analyses the convergence rate of episodic memory-based continual learning methods. The authors formulate the continual learning problem as a nonconvex finite-sum optimization problem. Based on the analysis, the authors propose an adaptive learning rate scheduling methods to adjust the learning rates based on the gradients computed in each iteration. The results on several benchmarks show that the proposed method can achieve better results  than the baselines.",
            "main_review": "Strengths:\n\n1. The authors propose a theoretical analysis of the memory based lifelong learning algorithms which is an important problem since currently most of the memory based lifelong learning algorithms lack  theoretical  guarantee.\n\n2. The authors make a good connection to existing memory based lifelong learning algorithms such as GEM and A-GEM and discussed their drawbacks.\n\nWeaknesses:\n\n1. The writing of the paper needs improvement. There are many unclear sentences and typos in the paper. The notations are also confusing. \n\n2. The main issues of paper are the experimental results. In table 1, NCCL + Reservoir and NCCL + Ring underperforms ER-Ring, could the authors give some discussion on this?\n\n3. The authors argue that \"the adaptive learning rate scheme is to prevent catastrophic forgetting\", but the average accuracy maybe is a more important metric？Otherwise a method which only focuses on improving the accuracy on the memory would perform best for preventing catastrophic forgetting.\n\n\n\nUnclear sentences and typos, just to list a few:\n\ni. This implies that ∆f is not a reason for moving away from stationary\npoints of f by catastrophic forgetting\n\nTypos:\n\ni. gradient descent based algorithm reach -> gradient descent based algorithm reaches\n\nii. an replay memory -> a replay memory\n\niii. but use limited -> but uses limited\n\niv. g_j is not defined in page 2. \n\nv.  we has trained ->  we have trained\n\n",
            "summary_of_the_review": "In this paper, the authors conduct a theoretical analysis of the memory based lifelong learning algorithms which is an important problem. However, the writing of the paper needs improvement and  the experimental results are not convincing enough to support the theory. \n\nAfter response:\n\nThe authors' responses partly address my concerns. But in the current form of the paper, it is not ready to be published. The authors should conduct more experiments to make the theory more convincing. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper provides a convergence rate characterization for a continual learning problem where the objective functions (for two consecutive tasks) are generally nonconvex and have a finite-sum form over data samples (which belong to current and previous tasks). The analysis uses the tools from nonconvex optimization, with a goal to find a stationary point for current and previous tasks. Under the setting of continual learning, the authors consider the standard replay memory based method, focusing on both the episodic memory and the replay memory with sample dropping. For the analysis, the authors first address two important types of errors: gradient estimation bias at time t and the catastrophic forgetting error. They show that for these two memory based methods, with a good initialization of replay memory, the gradient estimation error vanishes. They further show that in term of the convergence rate of previous task, i.e., $f$ in its context, the catastrophic forgetting error seems to be inevitable and can be even unbounded. To address this error, they further propose some adaptive learning rate schemes, which show some effectiveness in several experiments.     ",
            "main_review": "My detailed comments are provided as below. \n\nStrength:\n\n1, Continual learning is a timely and important topic for modern machine learning. This paper provides a good step to understand the optimization properties of this direction. The analysis captures something that has not been encountered in conventional optimization problem, such as the overfitting error term and the catastrophic forgetting term, although they do not provide a quantitative characterization on the catastrophic forgetting error. \n\n2. The design of the adaptive learning rates motivated by the analysis is interesting. It can help to reduce the catastrophic forgetting error and seem to perform well in the experiments. \n\nWeakness:\n\n1. This paper is not well written. For example, it is really hard for me to understand Lemma 1, which is an important result to show how the overfitting error vanish when the initial memory setup $M_0$ is properly selected, because the authors do not do a good job in explaining the episodic memory and the replay memory with sample dropping. Even after I carefully read their proofs, I figure out in the episodic memory case, $M_t=M_0$ for all $t$, whereas for the dropping case, $M_{t}$ samples from previous $M_{t-1}$ with one random sample dropping. Given such settings, it is not very surprising to me to get such a result. \n\n2. The authors do not provide a quantitative characterization on the overfitting error (for the general case without the initialization as in Lemma 1)  and the catastrophic forgetting error. For example, it is more interesting to characterize, even under some extra assumptions, how and when such errors can be bounded. So far I do not find such results. \n\n3. I am wondering whether the authors can provide some figures to show whether their proposed algorithms indeed achieve a faster convergence rate in terms of the iteration number and the catastrophic forgetting error. \n\n4. Since in practice, there is a long sequence of tasks (beyond two as studied in this paper) for continual learning, I am wondering whether the analysis here can be further generalized to this more general case. This is because based on my knowledge, catastrophic forgetting error may also depend on the how far between two tasks. \n\n5. Some typos: \n1) page 4 line 7: \"the the replay memory with...\" ->  \"the replay memory with...\"\n2) page 8 last row but three: \"Table ??\": no quotation number. \n\n\n\n\n\n\n\n\n\n\n\n\n\n",
            "summary_of_the_review": "Overall, I feel this paper provides some initial steps to understand the convergence of continual learning, but given some weakness I list above, I am slightly negative about it. However, I am open to increase my score based on the authors' feedback and other reviewers' comments.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes mini-batch stochastic gradient method of the framework of nonconvex memory-based continual learning, and conducts  a theoretical convergence analysis. Additionally, the paper studies catastrophic forgetting and outfitting in the context of the proposed approach.\n\n",
            "main_review": "Pros:\n - The paper is well-written, clear, and straightforward.\n - The results are new and interesting\n - Theoretical analysis is well-conducted\n\nCons:\n - The guarantees not surprising and maybe a bit incremental \n - Proof techniques are standard\n\nOther than the small issue in Lemma 2: note that \\alpha is allowed to be 0 but then (10) is not well-defined, so it should be strictly larger than 0, I did not find any problems in the proofs in the appendix or in the mathematical writing.",
            "summary_of_the_review": " The paper is interesting, provides new insights and theoretical guarantees, and in my opinion should be accepted for publication.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}