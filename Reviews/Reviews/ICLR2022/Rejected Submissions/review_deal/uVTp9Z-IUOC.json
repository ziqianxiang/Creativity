{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper considers test time adaptation to distribution shift which is a very important and impactful problem. The authors propose an empirical method that has different pieces, the most important ones being input transformation and confidence maximization and using likelihood ratio loss.\n\nThere were various concerns that got addressed during the rebuttal period such as, novelty of the proposed method, ablation study of different parts of the model, novelty and importance of diversity regularizer, choice of optimization. However there are still three remaining concerns that addressing them will improve the paper significantly: First, clear motivation behind the method for the cases when the model is certain but we have data imbalance. Second, analysis in the online setting of batch-by-batch prediction and adaptation. Third,\nestablishing the claim regarding data subset experiment that it enable the model to adapt on a subset of data and later switch to complete execution mode without adaptation for efficient run time and improved throughput. How is the method to know the data distribution has changed, or that it has sufficiently adapted to it when the data distribution is not changing?"
    },
    "Reviews": [
        {
            "summary_of_the_paper": "Studies the problem of test-time adaptation across distribution shift, and proposes i) a new self-training loss with better stability than entropy minimization ii) using a diversity regularizer and iii) an additional “input transformation” module. The approach is found to lead to improved performance on standard test-time adaptation settings.\n",
            "main_review": "Strengths\n\n– The paper is mostly well written and easy to follow\n\n– The proposed approach is intuitive, appears effective, and consistently outperforms competing methods for test-time adaptation\n\n– The paper includes a comprehensive set of experiments\n\n– The paper does well to compare with existing entropy minimization alternatives like MaxSquares and Charbonnier penalty. I would recommend including those results and a more detailed discussion in the main paper rather than appendix.\n\nWeaknesses \n\n– The proposed approach is largely a combination of existing methods – TENT (Wang et al., ICLR 2021), negative log-likelihood ratio loss (Yao et al., 2020), and batch-level diversity regularization (Li et al., arXiv 2020, Prabhu et al., ICCV 2021 [A]), for the test-time adaptation setting.\n\n[A] Prabhu, Viraj, et al. \"Sentry: Selective entropy optimization via committee consistency for unsupervised domain adaptation.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.\n\n– The paper lacks a proper ablation study: while the input transformation module is ablated, what is the individual contribution of each proposed piece?\n\n– The proposed loss does appear to have certain limitations, as it is unbounded and relies on proper scaling via model design (eg. batch norm layers) to prevent logit explosion.\n\n– “the soft likelihood ratio loss creates lower amplitude gradients for low-confidence self-supervision”: this does not appear to match Figure 1 (right), where SLR is slightly larger than HLR for low confidence (<0.2). Further, both SLR and HLR actually appear to have large gradients in this confidence regime as compared to hard pseudolabels – is this not problematic, since that would effectively upweight very low confidence predictions?\n\n\n----post-rebuttal----\n\nThe author response had addressed my concerns about the behavior of the proposed loss. In light of the paper's empirical contributions but limited technical novelty, I recommend a marginal accept. ",
            "summary_of_the_review": "Interesting paper on improving test-time adaptation but I have some concerns (see weaknesses). I will be willing to reconsider my rating based on the author response.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "In the spirit of full disclosure: I have recently reviewed this paper, and several parts of my previous review are still applicable, thus I am copying in these parts when appropriate.\n\nThis paper presents a method for test time adaptation based on several techniques. These include a self-supervised adaptation objective based on log likelihood ratios, an additional regularizing objective to encourage diverse predictions, and an input transformation module that is also trained with the aforementioned objectives. Together, these techniques lead to better performance on ImageNet-C and ImageNet-R compared to Tent, a recent and similar test time adaptation method based on entropy minimization.",
            "main_review": "Strengths\n---\nThe self-supervised log likelihood ratio objective appears novel, as far as I am aware. And, for this problem setting, the combination of the aforementioned techniques is novel and leads to stronger empirical results than what has been previously reported.\n\nThe experiments are generally comprehensive and cover, as far as I can see, the important aspects of evaluation. I appreciate the results presented for both the \"offline\" and \"online\" adaptation settings, as well as the results adding all of the various techniques to Tent to evaluate whether any technique is of paramount importance.\n\nThe paper is generally well written and structured.\n\n\nWeaknesses\n---\nI think that the paper has improved on this point, but the motivation behind the general approach is still somewhat shaky. The idea that the model should extract a self-supervised learning signal from data points it is already very confident for still seems strange to me. Imagine if the model was already very confident for the entire batch of data points, but there is a (predicted) class imbalance in the batch. Would it not be the case that the model would adapt in this case when using the proposed approach, even though it would make more sense to not adapt at all, which for example entropy minimization would (roughly) do? And it is in general just unclear to me how incorporating a stronger gradient signal from confident points would help when it comes to ambiguous points. Perhaps what could be useful here is to actually \"show this in action\", e.g., take a real batch of data during adaptation and demonstrate how the model adapts with the proposed approach vs with Tent. This may provide greater intuition as to why the proposed approach is a good idea.\n\nNegative results are also of interest to the community, and to this end, including results on challenging distribution shift benchmarks such as ImageNet-A and ImageNet-v2, which prior work [1] has shown adaptation to be unhelpful for, would be great. Even just in the appendix, it would still be appreciated.\n\nA final minor nit from my previous review: I would still like to know whether or not a confidence of 0.82 is \"low\" for the corrupted image datasets or other instances of test distribution shift.\n\n[1] Schneider et al, \"Improving robustness against common corruptions by covariate shift adaptation\". NeurIPS 2020.",
            "summary_of_the_review": "Primarily due to my concerns above, I am initially recommending a weak accept of this paper. I am happy to engage in discussion with the authors and other reviewers in order to reach a more confident final recommendation.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a new loss to improve test-time BN adaptation for domain adaptation. The proposed loss consists of two components: the diversity maximization loss and the confidence maximization loss. Specifically, they use a running estimate for the diversity loss based on KL divergence. They propose the hard and soft likelihood ratio for the confidence loss which has large gradients for high confidence predictions.   ",
            "main_review": "Strength:\n\nThe paper introduces an information maximization loss for the test-time BN adaptation on unlabeled target mini-batch data.  The method has been well-motivated by pointing out the limitations in SOTA methods. \n- The proposed formulation incorporates the diversity regularizer to avoid the trivial collapsed solutions on mini-batch data.\n- It presents the analysis of the gradients of different losses for confidence maximization. And, proposes to use the negative log-likelihood ratio loss [Yao 2020] for TTA  which has non-vanishing gradients for high confidence predictions.  \n- It proposes to jointly update the IT module and BN parameters for TTA\n- The paper provides comprehensive experimental results in the paper and appendix. \n\nWeakness:\n- The proposed diversity loss is not very effective on mini-batch data. The results show the improvement is very marginal. \n- The paper should provide examples of failure cases,  and more explanation & discussion about the issues in the last paragraph of section 3.  \n- The novelty of the paper is relatively limited. IM loss, KL based $L_{div}$,  IT module, and the negative log-likelihood ratio are all proposed in the previous works. ",
            "summary_of_the_review": "Overall, the paper presents an information maximization (IM) loss for TTA on unlabeled target data. My main concerns are that the effectiveness of the proposed $L_{div}$ on mini-batch data, and the scale-normalization problem of the proposed logits. Hopefully, the authors can address my concern in the rebuttal period. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Test-time adaptation by entropy minimization can help models adapt to dataset shifts like corruptions without altering training. This work extends tent, an entropy minimization method, by proposing alternative non-saturating losses, adding a diversity regularizer, and adapting the input data along with the model parameters. The input is adapted by applying a convolutional image transformation model between the input and classification model. These extensions do not need more optimization iterations or supervision than the baselines: the method adapts online and efficiently without auxiliary supervision. Experiments on the corruption benchmark ImageNet-C and the newer benchmark ImageNet-R report reduced generalization error. The improvements are there but marginal, and they are consistent across multiple baseline architectures (ResNet, DenseNet, MobileNet, etc.). However, the clean accuracy reduced, so the proposed method does not strictly dominate prior work.",
            "main_review": "Strengths\n+ The joint adaptation of input transformations and model parameters is novel for test-time adaptation. Neither TTT (Sun et al. 2020), test-time normalization (Scheider et al. 2020), nor TENT (Wang et al. 2021) adapt the input. The related work section covers prior projects that learn to transform the input during training rather than testing.\n+ The proposed techniques—the alternative losses, the input transformation model, and diversity regularizer—improve accuracy jointly and separately. For instance, the input transformation model helps the proposed approach as well as TENT, and the diversity regularizer likewise fixes cases where TENT can fail. While these are not wholly new, it is still informative to double-check their effectiveness, ablate their combination, and show small but consistent improvement across multiple architectures and datasets (ImageNet-C, ImageNet-R, and the digit datasets MNIST/MNIST-M/USPS). \n+ The proposed extensions are still online and efficient, so this method seemingly could be deployed as easily as TENT. The only counter to this is that small accuracy drops are reported on unshifted data, where these drops are not seen (TENT, BN) or smaller (TTT) for other test-time methods.\n+ The results for test-time adaptation by optimization (TENT, HLR/SLR) on ImageNet-R are empirically new. Only test-time normalization (Schneider et al. 2020) had reported test-time results on this data.\n+ Design choices are justified with experimental results (main tables), visualization (Figure 1 for losses and gradients), and toy experiments (Appendix A.1).\n\nWeaknesses\n- The novelty of the proposed extensions is diminished by intersection with prior methods. The losses for HLR and SLR are from Yao et al. 2020, although this work is the first to bring them to fully test-time adaptation, and that is worthwhile. (For comparison, note that TENT argued for entropy which is obviously well-established as a loss). The diversity regularizer is a core part of SHOT (Liang et al. 2020), although this work calculates it differently with a moving average. The input transformation model is the most new, as input adaptation of this kind has not been done during testing, but there are close connections to training like ANT (Rusak et al.) and CyCADA (Tzeng et al.).\n- The proposed losses HLR and SLR further restrict the choice of parameters to adapt, as these losses would otherwise cause the predicted logits to grow without bound. Deriving an alternative loss without this restriction would be better to simplify the application of the method.\n- The amount of improvement on shifted data is marginal, at 2-3 points absolute in many cases, while there is harm on the unshifted/standard/clean data. This argues against the motivation for non-saturating losses. This is admitted on pg. 9 under \"Clean Images\" but is not remedied. A new loss or combination of losses that improves shifted accuracy without hurting clean accuracy would be more significant.\n- Input transformation (Sec. 3.1) is not closely studied or ablated. For instance, what if only the channel-wise input changes are included without the network? Do shallower networks do better or worse? Does input transformation help on all corruptions, or can it hurt?\n\nFor Rebuttal\n- Please provide a control experiment for the choice of optimization. How do TENT and TENT+ fare when optimized with Adam, the same solver as HLR/SLR? For TENT/TENT+, does raising the learning rate for SGD or Adam improve results by counteracting the saturation of the entropy loss? The need for non-saturating losses is a key claim so this is worth double-checking.\n- Please provide the results for TENT/TENT+ without freezing the weights of the deeper layers. The need for this freezing is a limitation of HLR and SLR, not TENT, so it is worth knowing if these additional parameters help the baselines.\n- Please comment on the learned transformation models, and in particular the learned transformation weight tau. How much do the scale gamma and shift beta help on their own? As a more novel part of this work, the input transformation module deserves more analysis.\n- Please explain the data subset experiments in more detail (Figure 3). Why does TENT fail as the split fraction reaches 1.0? In the TENT paper, there are generalization results with adaptation on target train and evaluation on target test, and the method still helps in that case. What is the justification for these use cases? Would it not be better to always adapt on all the data that is encountered?\n\nMiscellaneous Feedback\n- [clarity] please summarize results with the mean where appropriate, for instance by including the mean over corruption types in Table 1\n- [clarity] consider including qualitative results of the learned image transformations, for instance in the appendix, to show the types and degrees of transformation.\n- [text] in the related work, change \"domain adaptation train\" to \"domain adaptation methods train\"\n- [text] in the related work, change \"such setting refrain the cost\" to \"such settings spare the cost\"\n- [text] in Sec. 3.2.1, change \"One option are\" to \"One option is\"",
            "summary_of_the_review": "This work makes a reasonable but minor contribution that can inform further extensions of test-time adaptation. A large part of this work is the double-checking of elements of test-time adaptation methods, with only marginal empirical novelty and significance, or the importing of techniques from other scopes, with either no or only marginal technical novelty and significance. It is a pity that the most new part, the test-time input transformer, is not further studied and improved to give this work a more independent dimension of contribution. At the same time, the harm to accuracy on normal data is cause for hesitation to accept the proposed changes. While there is value in this work, rejection is recommended so that (1) the input transformer can be more fully covered and (2) the issue of improving out-of-distribution accuracy at the cost of in-distribution accuracy can be resolved.\n\n**Final Review** The rebuttal thoroughly clarified results and offered additional experiments to empirically justify the contributions and reduce worries about potential issues. In particular, the results with the baselines of TENT/TENT+ now make sense with the clarification of online/offline results (please underline this in the paper), and the proposed method is not so sensitive to the choice of frozen layers. I have raised my score to 6, and I would have considered a 7 if there were such a rating. I did not go higher because the trade-off of lower accuracy on unshifted data for higher accuracy on shifted data remains. Nevertheless there is informative material here, in the main paper and appendix, and so I vote for acceptance so that this work can inform the burgeoning direction of test-time adaptation. Along with the more novel parts, such as input transformation during testing, this work also helpfully confirms and tunes other parts like the diversity regularizer in ways that future work can simply adopt.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}