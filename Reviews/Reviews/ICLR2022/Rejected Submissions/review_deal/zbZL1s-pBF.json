{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "Three experts reviewed the paper and gave mixed reviews. Reviewer BBZL raised their score to 6 in the discussion phase. Reviewer dv5k was not fully convinced by the rebuttal and remained negative. Reviewer oUrr also remained negative. The reviewers were not excited by the proposed method in general and raised questions about both experiments and theoretical results. AC found clear merits in the paper, but the reviewers' comments suggested the work could be strengthened in both experiments and presentation. Hence, the decision is *not* to recommend acceptance at this time. The authors are encouraged to consider the reviewers' comments when revising the paper for submission elsewhere."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposed a training-free robust multimodal learning late-fusion methods via sample-wise Jacobian regularization. The key idea of the work is to minimize the Frobenius norm of a Jacobian matrix, so that the multimodal prediction is stabilized. The paper demonstrate\nthe good efficacy on both adversarial attacks and random corruptions setting on multimoda datasets such as AV-MNIST/VGGSound/RAVDESS.\n",
            "main_review": "Strengths:\n\n- The Jacobian regularization methods is simple and straight forward. \n- The paper provide a theoretical error bound of the proposed robust late-fusion method.\n- The ablation of \\gamma on the necessity of extra modalities via the TwoMoon example is interesting.\n\nWeaknesses:\n\n- In the related work section, the paper claim (robust) late-fusion in multimodal learning is un-explored in the previous literature, However, this is not entirely true, some of the missing references includes [1][2].\n- The paper provide some experimental results, i.e. Multimodal classfication dataset AV-MNIST/VGGSound and Emotion Recogition dataset RAVDESS. However, this is not a extensive in multimodal learning. More real-world tasks and datasets should be considered, e.g. RGB-D Human Action Recognition dataset NTU-RGBD, multimodal classfication dataset Kinetics400/AudioSet.\n\nMore comments:\n\n- In their implementation, the author mentioned one iteration could already yield a sufficient accurate result (tmax = 1), I'm a little bit suprise on this, can you elaborate more on this? \n\n\n[1] MMTM: Multimodal Transfer Module for CNN Fusion, CVPR 2020\n[2] Deep Multimodal Fusion by Channel Exchanging, NeurIPS",
            "summary_of_the_review": "The paper proposed a simple multimodal fusion methods based on Jacobian regularization, However, there are minor issue in related work, and experiments are not extensive, i.e. only valided on smaller Toy datasets.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the authors proposed a training-free late-fusion method for robust multimodal learning. They specifically considering its performance under adversarial attacks and random corruptions which usually confuse the model by introducing noise to the input data. To promote the multimodal prediction robust to attacks, they propose to minimize the Frobenius norm of Jacobian matrix so that the prediction becomes stable to the perturbation of inputs. They also provide a theoretical error bound of their method. The experimental results outperfom other late-fusion methods. ",
            "main_review": "Strengths:\n\n- The idea is interesting and well-motivated\n- The empirical results on various datasets are solid\n\nWeakness/major concerns:\n\n- To make eq(1) more rigorous, I would suggest to note the shape of $W_{A/B}$ and $h_A$. \n- in Eq(2) $freq$ contains the occuring frequencies of each class, calculated from the train dataset. Will it be directly applied to compute $p$ on test dataset? Looks like this approach only invoked at test time, so I think the answer is yes. Here $freq$ denotes the class prior which is crucial for computing $p$ accurately. I think when domain shift, more specifically label shift exsit between train and test dataset, it is inappropriate to estimate the test class prior from train dataset. Does the proposed method can handle this problem?\n- In eq(5), is the objective $\\min_{W_a} L$ or $\\min_{W_a,W_A} L$? According to the Algorithm 1, it minimizes over $W_a$. It would be better to clarify in the equation as well.\n- To me the error bound in Theorem 1 is very loose. Is it possible to compute the value exactly for some experiment settings, for instance biase noise? We can make some moderate assumptions, like set the Lipschitz constant to 1.\n- Is this the work pipline of the algorithm, at training time, we fuse $z_A$ and $z_B$ directly; at test time, we fuse $W_a z_A$ and $W_b z_B$ with optimized $W_a$? Do we have to solve the Sylvester equation for each mini-batch? I would appreicate if the authors can help me to get a better understanding  of their algorithm. \n- For AV-MNIST experiment, would it be more interesting to perturb the image modality? Intuitively, image modal provide more useful information than audio modality. But of course this is not necessary to be true. ",
            "summary_of_the_review": "I would consider to raise my score, if the authors can address my concern.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper propses a late-fusion algorithm for multimodal learning. This algorithm serves to improve the robustness against adversarial attacks and random corruptions. Assuming that which modality is perturbed, this paper propses to leverage Jacobian regularization and conditional independence assumption to fuse predictions from different modalities. Moreover, this paper also provides rigorous error bounds on it error and explain the necessity of extra modality.\n\nIn summary, this paper's main contributions are as follows:\n\n- Propose a late-fusion algorithm based on Jacobian regularization and conditional independence;\n- Derive theoretical error bounds and demonstrate the biasing effect of the extra modality;\n- Conduct comprehensive evaluation on adversarial attacks and random corruptions and outperform baseline late-fusion algorithms.",
            "main_review": "\n## Strength\n\n- This paper proposes to utilize Jacobian regularization to improve neural networks' robustness when one of the modality is corrupted. This method is intuitively sound and also provides rigorous guarantee by *Theorem 1* that the corruption can only models' final prediction to a certain amount.\n- To find the optimal $W_a$ matrix, this paper also proposes an efficient optimization algorithm.\n- In the experiments section, this paper's method could outperform baselines for most of the time.\n\n## Weakness\n\n- This paper makes the assumption that the corrupted modality is known. It would be appreciated if the authors could provide concrete examples about this scenario or references using similar assumption.\n- The authors state that their method is *training-free*. Actually, there involves some optimization operation to identify $W_a$. It would be good if the authors explicitly explain what their *trianing-free* means. If possible, some similar definition in other papers would be appreciated. (The authors state that their method is *the first to propose a training-free robust late-fusion method*. It would be important to check whether the authors' definition of *training-free* is consistent with other paper.)\n- The rows in *Table 1*, *Table 2*, and *Table 3* are not clear. It would be good if the authors could explicitly define *UM*, *MM*, *(0, k)* mean. \n\n## Questions and minor typos\n\n- In the explanation for *equation (5)*, the authors write *the second term in the loss guarantees numerical stability*. It would be good if the authors could explain what the *numerical stability* means here.\n- At the end of Section 4, the authors write *the extra modailty plays a key role*. The *key role* term is vague. Could the authors be explicit about what the role of the extra modality is?\n- In the second paragraph, the authors write *our method invoked even larter than (0,2)*. *larter* might be a typo. \n\n- In *equation (4)*, should $P'P^{',T}$ be $P'P^{'T}$ ?",
            "summary_of_the_review": "\nThis paper proposes a novel late-fusion algorithm based on Jacobian regularization and conditional independence assumption. It also provides theoretical guarantee on its performance. Further, this paper conducts comprehensive experiments to verify their method. However, there are still some concerns about it assumption and claims. ",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}