{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper proposes to create an explanation space to describe the relationships between input data and prototypes (and also between the prototypes themselves). It constructs such a space suing VAEs and conducts experiments to validate the effectiveness and interpretability of the method.\n\nStrengths:\n- The proposed method is interesting and intuitive\n\nWeakness:\n- Novelty of the idea is limited\n- Missing experiment comparison with some important previous work\n- Some claims are not well supported by the empirical results"
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The main contributions of this papa are 1) a particular architecture that combines Variational Auto-Encoder (VAE) that makes the predictions based on the distances between instances and proto-types; and 2) a series of visualizations in the latent space that explains the model’s predictions, i.e. explanation space. In order to verify their assumptions that their networks provide faithful and rich explanations, the authors conduct user studies on MNIST and FashionMNIST datasets.",
            "main_review": "Overall the paper introduces an interesting concept, *explanation space*, and an interesting way of constructing networks that incorporate some constraints, i.e. the distance layer, into the model. As a result, part of the internal components are not black boxes to humans any more. It also comes with an easier way to visualize the internal activations of the model by the decoder part of a VAE. \n\nHowever, even though I appreciate the potential merits in the paper’s idea of building better networks in terms of interpretability, I have several concerns about the motivation of the technique and inadequate evaluations that may not be sufficient to convince the reader. Please find my detailed review as follows.\n\n### Novelty\n\nThe idea of combining distance layer with VAE is novel and the concept of *explanation space* is also first proposed in this paper. These two techniques make this paper stand out from other VAE-based explanations. However, these techniques are not well-motivated to me and the particular application of the proposed visualizations, explanation space, is also missing. To clarify, I have two following questions regarding the motivation of the paper:\n\n1. What is the concrete problem this paper tries to solve? As the authors argue that “However, examples are powerful but not enough” in the introduction of the paper, the sentence seems not to be completed because the most important object is missing -- the example-based explanations are not enough for what (questions)? I am interested in what kind of questions that the authors have in mind that a human user may have but is not able to fully answer by the current explanation approaches. For example, with a classifier that differs dogs from cats, what does the concrete question look like, which requires a set of explanations, i.e. explanation space, instead of examples generated by prior work. Without a clarification like that, I am not sure how the evaluations provided in the result of the paper justify the significance and validate the efficiency of the proposed approach.   \n\n2. The current motivation for using the uncertainty layer seems to be quite weak. The authors do not justify what motivates the use of an uncertainty layer on the top of the network. The only relevant information I am able to find is that by using the uncertainty layer the accuracy of the model is improved by 1%. This is not convincing enough to serve as a strong motivation because :1) the improvement of performance is strongly related to the data distribution, which does not serve as a motivation if using other datasets. For example, 1% increase can be very significant for larget datasets, i.e. ImageNet, but pretty trivial for the datasets used in this paper, FashionMNIST and MNIST. 2) the improvement of the accuracy should instead be the result of employing the uncertainty layer instead of the motivation to do so. The way the current paper justifies the use of this layer does not convince me of the necessity of it. And i am curious to see how the changes in explanations may look like with the 1% decrease of the network’s performance. \n\nA minor point: understanding the model’s internal behavior by the distances of representations learned by VAE or any encoder-decoder networks are not first proposed in this paper . Some related work [1] seems worth mentioning and discussing. \n\n\n### Technical Quality \n\nI have three concerns regarding the analysis and the experiments in this paper.\n\n1. The empirical findings seem to be the main contributions of this paper, however, some descriptions seem to convey the message that the proposed explanation enjoys some nice properties but they are either not clearly defined or proved. As the authors emphasize in the abstract that “... but we propose an inherently interpretable model for more faithful explanations.”, I am not able to locate clear definitions in the paper about “inherently interpretable” and a measurable definition of “faithfulness” to support the argument that the proposed method is **more** faithful. Further, what are the baselines for the authors to derive the conclusion that the proposed method is **more faithful** (I will come back to the baseline issue in the next bullet point). \n\n2. The empirical evaluations are not strong enough to support the claims that the authors make about the contributions. Firstly, most of the experiments are conducted on MNIST and FashionMNIST but these two data distributions are sometimes too simple to generalize to higher dimensions, i.e. ImageNet. It is not new to the community that a lot of algorithms will work perfectly on MNIST and FashionMNIST but fail dramatically on more complicated colorful images as in practice you may not actually need a deep net to achieve a great performance on MNIST. For example, the K-nearest neighbors classifier is both explainbale and accurate on MNIST. I would encourage the authors to expand the empirical evaluations to datasets with higher dimensions in the input features before rushing to make any conclusions. \n\n3. Baselines are missing in the evaluation section. The major part of the evaluation section seems to focus on explore that the proposed method can bring to the user. However, before rushing into the exploration part of the method, there is a missing part of comparing the proposed method with the prior work. Even though the authors emphasize that the proposed method provides a set of examples instead of just one, which does not exist in the previous work. However, it does not seem to be unfair to compare one or several examples samples from the explanation space with some prior work that provides instance-based explanations [2, 3, 4, 5, 6] unless I misunderstand the paper and please help me understand why such comparisons are not useful to show the proposed explanation space provides better explanations. These baselines may not provide an apple-to-apple comparison but are worth discussing and performing some comparisons in the ballpark. \n\n\n### Clarity\nThe writing of the introduction section is a bit confusing to me because I miss the part about what the problem the paper aims to answer and how the results in the evaluation sections help to show that the proposed method solves the problem. Figure 1 and 2 are clear to help readers to understand the proposed architecture and I appreciate that. \n\n### Significance\n\nWith the aforementioned review, the significance of this paper can vary. One the one side, if the authors can help me understand the what is the subject of the explanation, that is the question requiring an answer like the one proposed in the paper and can not be answered by prior work, and why the current evaluation is sufficient to support the claim, I would recommend this paper because the contributions are significant to the explanation community. On the other hand, the current manuscript does not seem to be able to convince me that the contributions are sound and significant. Therefore, I am on the negative side but will decrease my confidence in the score because I am willing to increase it once my concerns are resolved. \n\n\n[1] Yang, Ceyuan et al. “Semantic Hierarchy Emerges in Deep Generative Representations for Scene Synthesis.” Int. J. Comput. Vis. 129 (2021): 1451-1466.\n \n[2] Pang Wei Koh and Percy Liang. 2017. Understanding black-box predictions via influence functions. In Proceedings of the 34th International Conference on Machine Learning - Volume 70(ICML'17). JMLR.org, 1885–1894.\n\n[3] Kim, Been et al. “Examples are not enough, learn to criticize! Criticism for Interpretability.” NIPS(2016).\n\n[4] Yeh, Chih-Kuan et al. “Representer Point Selection for Explaining Deep Neural Networks.” NeurIPS(2018).\n\n[5] Pruthi, Garima et al. “Estimating Training Data Influence by Tracking Gradient Descent.” ArXivabs/2002.08484 (2020): n. Pag.\n\n[6] Goyal, Yash et al. “Counterfactual Visual Explanations.” ArXiv abs/1904.07451 (2019): n. pag.\n",
            "summary_of_the_review": "Overall I recommend for a rejection because the current version of the paper is not well-motivated and the evaluations are not sufficient to support the claims and contributions made by this paper. I am open to any discussions and will increase my score if my concerns are resolved.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper extends existing series of prototype-based DNN for image classification. Their model uses VAE to ensure similar latent representations share similarities in appearance, a problem that many existing prototype-based CNN models suffer; They also include an uncertainty layer which improves the predictive performance. The authors also conducted human evaluations via Amazon Mechanical Turks to validate the interpretability of their model ",
            "main_review": "This paper improved existing prototype-based DNN by enhancing interpretability and while also improving the performance. I enjoyed reading the paper and I think the proposed solutions will benefit other prototype-based DNN models for image classification. I have a couple of questions that I wish the authors could answer.\n\n1) in the original ProtoPnet paper, the authors Li et al designed separation cost and clustering cost in their objective, with the goal to push images closer to prototypes of the same label, and away from ``incorrect'' prototypes. I don't see such designs in this paper. Is there a reason that you don't choose to include these terms?\n\n2) In some prototype-based models, they also include a \\emph{diversity} term, which encourages protoypes to be different, to avoid redundancy. There are no such designs in the proposed model. Do prototypes have redundancy issues? do some of them look very similar to each other?\n\n3) I'm wondering if you can discuss the connection between your explanations for basic explanations and counterfactual explanations, because it seems by changing certain features, following your basic explanations and explanations for basic explanations, you can guide the model to get a different prediction. You did mention that \"Visualized images of our explanations are fundamentally generated through VAE, making it challenging to generate human-recognizable images for complex datasets that are difficult for VAE to generate the observable image.\" Would that be a reason that your model cannot be used to generate counterfactual explanations?",
            "summary_of_the_review": "Overall I enjoyed reading the paper. I only have a few questions, which basically asked the authors to clarify their design choices, especially those that are different from existing works. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I don't see any ethics concerns",
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper extends prototypes-based explanations by proposing what they call explanation spaces describing the relationships between input and prototypes, the relationship between prototypes, and how prototypes are distributed. They construct explanation spaces using VAEs and suggest a way to find the optimal number of prototypes.\n",
            "main_review": "The key issue that explanation spaces try to address is that having similar latent representations does not guarantee that images share similarities in human-discernible ways, so the method uses VAE to regularize the latent representations. This is an interesting idea.\n\nI have a few questions about the experimental results:\n- How was reliability measured (that FactorVAE forms a more reliable explanation space) in Section 5.1?\n- It looks like there are two orange clusters in Figure 3b?",
            "summary_of_the_review": "The idea is interesting but not a lot of novelty, as some existing papers (e.g. Li's Deep Learning for Case-Based Reasoning through Prototypes paper) already provide notions of distances between inputs and prototypes, prototypes and test data points, etc.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper tries to provide an inherently interpretable neural network. Unlike the previous method of (O. Li, AAAI 2018) the paper uses VAE instead of AE. It considers the distance between the distribution of the input and the distribution of prototypes in the latent space of a VAE (the explanation space). The paper also provides a method for determining the number of prototypes using the Bayesian Information Criterion (BIC) for choosing the optimal number of components in the corresponding GMM. \n",
            "main_review": "The continuous nature of the embedding space of a VAE makes it more suitable than an AE. This is an improvement over the previous work of (Oscar Li et al. AAAI 2018).\nTraining VAE may have several issues: it may require many training samples to be able to generate explanation images of adequate quality. In the paper, generated samples are based on toy datasets MNIST or FMNIST. It is not clear how complicated the image generation process would become if the method be applied to more serious datasets. My concern is that at some point the challenge will become high-quality image generation, rather than a classification task.  \nAlso, the decoder of the VAE may degrade the classification performance (C. Chen Neurips 2019). Too much regularization may degrade the classification performance. Too little regularization (VAE) may result in a poor generation of the explanation images.\n \nDo these \"extra\" explanations this method is providing offer anything more than a simple prototype? In other words, if the authors had a comparison study in their mechanical turk experiment with PDL (Oscar Li et al. AAAI 2018), would there have been any benefit over using their method? The paper unfortunately hasn't done an experiment with PDL, only comparing to the \"no explanation\" scenario.\n\nI am not a native English speaker but there are too many grammatical issues in the paper. For example, in pp.2 paragraph 2: “Among our model, (i) relationships …” , Meanwhile, prototypes … are given for explanation.”. “we developed the model that progresses…”. Or in page 9, “it is proper to use …”\n",
            "summary_of_the_review": "I see values in the paper as using a VAE instead of AE is a natural improvement over previous work. Nevertheless, I have major concerns as described above, in particular, the paper is missing a comparison with previous works especially (Oscar Li et al. AAAI 2018). ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}