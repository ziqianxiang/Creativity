{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The authors present a new learning based algorithm for constructing index structure. Existing learned index algorithm use a fixed value, in contrast the authors show that a more refined methods can be used to obtain higher quality solutions for the problem.\n\nThe reviewers, after discussion, found the paper interesting and the experimental results promising but they feel that the paper in the current form is not yet ready for publication.  In particular,\n- in the current form the theoretical motivation and the experimental results are a bit detached\n\nOverall, the paper is interesting and the results are promising but it probably would benefit from significant re-writing before being accepted."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a methodology, pluggable to any learned index method, that adjust the prediction error guarantee ε to data locality so as to achieve a given expected value of ε while improving index performance by some measure. Experiments with real world data show that the proposed approach can be plugged in several existing learned index method with improved results.",
            "main_review": "The paper can be improved in terms of both argumentation and design.\n\nThe paper claim to resolve several tradeoffs at the same time through the proposed ε-learning approach:\nIt claims that the proposed method improves a space-time tradeoff in index performance.\nAt the same time, it claims to improve another trade-off, that of space-error.\nYet it in never clear how these three trade-off quantifies, space, time, and error are not compared to each other for a given ε value.\nIn particular, Figure 2 presents what appears to be a view of the space-error tradeoff. Yet it is not clear what ε values are set for each measured data point in the figure, and it is not clear what querying time they require. Perhaps improvement on one trade-off come at a cost on another.\nSimilarly, Figure 3 presents a good picture in terms of index size and querying time, yet it is not clear what cost that entails in terms of error.\n\nOn the design side, it is not clear why there should be user-define ε at all. If a user is interested on some index feature and presumably goes not wish to have a fixed ε guarantee, then it would be reasonable to do away with a user-specified ε, and use the proposed approach in order to guarantee some observable index feature, e.g., size, while adjusting ε as seems fit. The semantics of a dynamic ε guarantee from the user's point of view are unclear.",
            "summary_of_the_review": "The paper resolves two trade-offs with clarifying the relationship among them, and does not explain the meaningfulness of a dynamic ε",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concern at this time.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper considers learning based methods for constructing index structures, a fundamental data structure.  Recent work has given learning based index structures which guarantee a maximum error of $\\epsilon$ in the predicted index.  At a high level, this is done by breaking up the data set into several segments, and then a linear function is fitted to each segment, while maintaining the $\\epsilon$ error guarantee.  \n\nThe main idea behind this paper is to allow $\\epsilon$ to vary across the different segments.  This is motivated by the fact that answering a query must first find the correct segment via a binary search, and then perform another binary search within the segment using the fitted linear function.  The runtime of the first step depends on the number of segments and the runtime of the second step depends on the prediction error.  By allowing the prediction error of some segments to go above $\\epsilon$, the number of segments can be decreased, allowing for better trade-offs in some cases.  The main conceptual contribution is a method for tuning $\\epsilon$ across different segments, which can be plugged directly into pre-existing learned index methods.    The authors give a theoretical analysis based on a stochastic analysis of the MET algorithm from prior work which is then leveraged to design their method for tuning $\\epsilon$ across different segments.\n\nAn empirical study of their algorithm is performed on standard datasets for this area, comparing to several baselines from recent work in this area which use a fixed $\\epsilon$.  The results show that the proposed $\\epsilon$ tuning method can improve several learned index methods from prior work.",
            "main_review": "The main strength of this paper is the empirical study, which shows some interesting results.  However, I am not sure that the theoretical justification is sound.  Namely, the main justification for using a dynamic $\\epsilon$ seems flawed.  The authors argue that allowing for different $\\epsilon$'s between segments allows the algorithm to decrease the number of linear segments at the cost of increasing the mean absolute error of some of the segments.  In the analysis of the runtime, it seems that what matters is the maximum absolute error in the predicted indexes, not the mean absolute error, which can be much smaller.  Thus it seems natural to me that prior methods guarantee that the maximum error of each segment is at most $\\epsilon$.  Due to this, there seems to be a disconnect between the theoretical explanation of why the proposed method should work well and the method's improved empirical performance.  The proposed method seems to be able to trade-off between the number of segments and the mean absolute error within a segment, but not the maximum error which is what shows up in the runtime analysis.\n\nAdditionally, the overall presentation of the paper could be improved.  The paper could benefit from a more clear description of the $\\epsilon$ tuning algorithm.  Related to the comments above, the motivation in section 3.1 could be improved by directly connecting the running time to the mean absolute error in a segment, as right now it seems to be connected to the maximum error.  \n\nMinor comments:\n-  Use \\log inside of math environments to represent logarithms",
            "summary_of_the_review": "The paper provides some interesting experimental results on using dynamic $\\epsilon$ in learned index structures, but the theoretical justification does not seem to explain the improved empirical performance adequately.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper mainly studies the index structure problem.  Existing learned index methods use a fixed value for all the learned segments. In this paper, the author contributes a deeper understanding of how the impacts the index performance, and enlightens the exploration of fine-grained trade-off adjustments by considering data local characteristics.  Experiments with real-world datasets and several state-of-the-art methods demonstrate the efficiency, effectiveness, and usability of the proposed framework.",
            "main_review": "Strengths:\n1. The paper is well-written and easy to follow. I can easily understand the idea of this paper. The author first gives their intuition which motivates their proposed method.\n2. This paper conducts extensive experiments on real-world datasets from different domains to show their improved performances, which is very reasonable and convincing to me.\n\nWeaknesses:\n1. There are some typos, in Table 3, the authors ignore the positive sign in the last column (compared to other columns),\n2. The author can incorporate more explanations on the experimental results. For example, ‘From these results, we can see that the dynamic versions of all the baseline methods achieve much better error-space trade-off (( 16.48% to 23.57% averaged improvements as smaller AUSEC indicates better performance’, why there are some negative improvements?\n3. There is one related reference, authors need to highlight their difference.",
            "summary_of_the_review": "This paper is well-written and studies on the theoretical analysis of the database field, which focuses on an important and popular problem. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}