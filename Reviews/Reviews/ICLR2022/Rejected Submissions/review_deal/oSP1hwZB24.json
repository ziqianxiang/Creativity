{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "Although scores are somewhat mixed, even ignoring the most negative review the overall score would still be somewhat below the acceptance threshold.\n\nThe authors and reviewers had a robust discussion, mostly about the novelty, experimental setting, and the significance of the results. Although the discussion ultimately did not reach a consensus, I think there are valid points on both sides. E.g. I somewhat disagree with the reviewer that the paper is too application-focused for ICLR, though several other points remain valid. The overall message that the experiments seem not totally convincing was highlighted by multiple reviewers."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This submission is on modeling feature interactions for CTR prediction. It proposes a framework that follows meta-learning. Specifically, to model the interaction between feature F1 and feature F2, it uses a meta neural-network g(F1) that takes F1 and produces the parameter for another neural network f(F2) that takes F2, i.e., the outcome of feature crossing is f(F2) where f's parameter is g(F1). It outperforms the baselines and is deployed online.",
            "main_review": "Pros:\n- The algorithm is deployed in a real-world production system.\n- Many variants are proposed and empirically studied.\n\nCons:\n- Not enough novelty. The idea is almost the same as [1] and [2]. Both [1] and [2] conducts feature crossing by using one feature to generate the parameters of a neural network that takes another feature as input. Note that [1] is a recent work that has been deployed in a large-scale real-world e-commerce system as well and is a well-known work among some industry practitioners, especially in China.\n- More details about the experimental setup may be needed to assess if the setup is fair. For example, please consider reporting the total number of parameters of each baseline, since sometimes performance can be increased by simply increasing the model's capacity. It seems possible that the so-called state-of-art baselines are not well-tuned.\n- The reported +1% improvement in the online A/B test could be meaningless without details about the production systems. For example, +1% improvement in an early-stage business with a weak baseline is not as impressive as +1% improvement in a well-developed business with a strong baseline.\n- The writing can be improved.\n\n[1] CAN: https://arxiv.org/abs/2011.05625\n\n[2] A Meta-Learning Perspective on Cold-Start Recommendations for Items. NeurIPS 2017.",
            "summary_of_the_review": "The idea is almost the same as the existing works, especially [1].",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposed a module DPO for CTR prediction to enhence the explicit and implicit information. The authors claimed that they provide the first attempt to extend the dynamic neural networks to CTR prediction, and experiments show that DPN (Dynamic Parameterized Network) significantly outperforms other state-of-the-art methods. ",
            "main_review": "Pros: \n1) This paper is organised well and clearly written. 2) The idea of behavior modeling is novel to me. 3) Detailed theoretical analysis.\n\nCons:\n1) Even though I think the proposed method is technically sound, the results cannot convince me. For the experimental results shown in Table 1, some baselines run worse than their normal performances, and the result of Dynamic Parameterized Network seems not good enough, e.g. compared with the results in [1]. \n2) Eq.5, how to demostrate the rationality of using the low-rank strategy.\n3) few typos, e.g., section 2.5, actions.to\n\n[1] Field-wise Learning for Multi-field Categorical Data, NeurIPS 2020.",
            "summary_of_the_review": "The idea is somewhat novel, but the result is not good enough.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposed a new method to handle feature/context interactions within the eCTR prediction neural networks. Details are provided in different scenarios. Experiments are conducted for offline and real-world experiments with promising results.",
            "main_review": "Strengths:\n1. A good way to model and utilize feature/context interactions in order to better predict CTR.\n\n2. Promising offline/online results and detailed discussion of offline results.\n\n3. Well written paper. Clear illustration of the methodology as well as discussions about the relations with existing methods\n\nWeaknesses:\n1. Table 7 is way too simple, and so are related descriptions for the experiments. Are there more metrics to demonstrate the superiority of the new method? Is it affecting different segments of queries/users evenly? More details would be helpful to make the A/B test results stronger. Also, it seems CPC also increased. Any intuitions?\n\n2. Why TP99 jumped a lot? Is it purely from the network structure change? How about TP50/90/99.9?\n\n3. Page 4 \"Relation to FM\" would be more interesting to have details. The current high-level discussion is confusing. What is \"self-excluded version\"?\n\n4. In section 2.1 Preliminary, why is it helpful to separate traditional/sequence-base CTR prediction? \n\n5. To distinguish from \"a random paper which tweaks the neural network structure for better performance\", I would suggest more intuitions or discussions on why the added interaction component helps with the CTR prediction accuracy.\n",
            "summary_of_the_review": "It's a methodology paper working on an important problem (CTR prediction). The proposed method is not extremely innovative but the idea is natural and makes sense, so are the results positive and promising. The novelty is a subjective matter but personally I see enough contributions to clarify and materialize some of the vague commonsense in the industry. One minor concern is about its relevance to \"ICLR\" which focuses on representation learning.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}