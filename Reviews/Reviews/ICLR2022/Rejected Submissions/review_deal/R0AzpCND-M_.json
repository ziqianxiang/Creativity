{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper introduces a \"model agnostic\" attack to evaluate the robustness\nof machine learning models, specifically focusing on adversarial training\nstyle defenses. The attack uses a RNN optimizer and meta-learning to achieve\nthis goal.\n\nThe reviews are mixed. The two more positive reviewers appreciated the\ntechnical ideas of the attack, found the paper well written, and noted\nthe results were stronger than prior attacks.\n\nHowever the more negative reviewer raises valid concerns around (a) the\nmagnitude of the contribution compared to prior attacks, and (b) to what\nextent this attack will be useful more generally.\n\nStarting with the first point (also raised by the other reviewers) the\ntotal contribution of this paper is to improve attack success rates by\n~0.1% compared to the best prior attacks. This is a fairly limited total\ngain, especially because this technique requires much more sophisticated\nattack techniques.\n\nMore fundamental is the question if this attack is useful to the community.\nI tend to agree with reviewer 8AXD here that this contribution is rather\nlimited for two reasons:\n1. As the authors acknowledge, the attack is most effective for adversarial\n  training techniques, or others that don't make the gradient hard to\n  optimize. This limits the attack to a smaller subset of defenses.\n2. Complexity complicates attack evaluations. It's hard enough to get an\n  attack working in the first place, and this paper has to use some fairly\n  sophisticated tools to just get the attack marginally better than\n  prior attacks that are (much) simpler. It's not clear that we would\n  expect authors of future defenses to be able to get as good results,\n  when prior methods are much simpler to apply.\n\nAnd so on the whole, this paper's main contribution is improving attack\nsuccess rate by a small amount, with a fairly complex method, that only\napplies to a class of defenses that don't make gradient descent difficult.\nSo while there is nothing outright wrong with this paper, in its current\nform it seems to be adding unnecessary complexity to achieve something\nthat can already be done with existing techniques."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes an automatic approach for attacking classifiers, by approximating a possible adaptive attack that can take place on a newly-published defense. The methodology (MAMA) applies meta machine learning techniques, by training it on different defenses to let it grasp what might be a good signal to follow when attacking an unknown classifier. The authors then test this strategy and a naive version of it (that is, the one that is only trained on the attacks, and not fine tuned on a test set) against other attacks proposed in literature.\nResults suggest a modest improvement against defenses hardened with adversarial training.",
            "main_review": "Pros:\n+ the usage of meta learning techniques is interesting to create better attacks\n+ preliminary step ahead for the research in creating better adaptive attacks\n\nCons:\n- results suggest a too modest improvement w.r.t. the computational effort for training the attack\n- only adversarial training is considered\n\nMinor:\n- confusion between MAMA and BMA\n\nThe creation of automatic adaptive attacks is an important step ahead for the adversarial machine learning community.\nHowever, the proposed technique is hindered by the following key issues.\n\n**Results suggest a too modest improvement w.r.t. the computational effort for training the attack**\nThe results proposed in the paper show that MAMA and BMA reduces the robustness accuracy of the strongest automatic attack (AA) by a very low delta (Table 5), but the table do not show all the effort that is needed for optimizing the optimizer.\nHence, I am doubtful about the overall technique, since it behaves similarly to other strategies that do not need to be trained on any models.\nAlso, the deltas shown in Table 5 are computed against the original evaluation, and not the output of the best attack against the considered defense. This is misleading, since the discussion should revolve around the efficacy of the proposed attacks against the other ones, and the table seems to suggest an improvement that is not real.\n\n**Only adversarial training is considered**\nWhile it is true that adversarial training (AT) is becoming widely used, it is not the only defense that has been proposed, or that it will be proposed in the future.\nHence, how this method can adapt to a technique whose loss landscape is very different from an adversarially-trained one? Intuitively, more defenses should be added to the set used in the algorithm, but more experiments should be conducted to understand their effect on the meta learner. Or rather, this should be stated as limitations (that are not discussed).\nFinally, the fact that this strategy only addresses AT should be mentioned in the abstract, since it is now misleading the reader into believing that this method can adapt itself to every defense.\n\n**Minor comment: confusion between MAMA and BMA**\nThe authors might need to better describe BMA and MAMA, maybe by creating a better visual plot.\nAs it is now, the paper starts by describing BMA, and later MAMA, but the paper title is centered on MAMA. So, the discovery of BMA is a bit misleading while reading.\n",
            "summary_of_the_review": "I opt for a rejection, since I struggle to see a clear contribution w.r.t. the literature. Other attacks that do not need any optimization have comparable efficacy to the ones scored by BMA and MAMA, but without all the effort that is needed for training the meta learner. Also, the table is reporting misleading results in the delta column, since it is computed against the original evaluation, and not w.r.t. the strongest attack against a particular defense.\nAlso, these techniques only adapt themselves on adversarial trained models, limiting the scope of the meta learner efficacy.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a model-agnostic meta-attack and achieves promising adversarial attack performance compared with state-of-the-art adversarial attack algorithms. The proposed algorithm overcomes many issues, such as the vanishing gradient problem, training instability problem, generalization to other defense models. The detailed experiments support the proposed method.",
            "main_review": "I think the proposed adversarial attack algorithm makes a nice contribution to the adversarial machine learning community, and this method can be considered as the main evaluation for a newly proposed defense model. \n\nThe strengths are listed as follows:\n\n1: The proposed adversarial attack has less computational cost compared with other state-of-the-art attacks (As shown in Table 4).\n\n2: The proposed attack has outperformed other strong attacks under 12 robust defense methods.\n\n3: The proposed attack can be applied to a large-scale dataset (ImageNet as shown in Table 5)\n\nThe weakness of the paper is the lack of novelty. Meta-learning-based adversarial attacks have been considered before and using RNN as an optimizer has also been considered. The contributions of this paper are those improvements made to improve the performance.\n\n",
            "summary_of_the_review": "In the perspective of proposing an adversarial attack algorithm, the author has conducted detailed experiments to support the method. However, the contributions of this paper are the improvements to the existing framework.\n\nHence, I support a marginal acceptance of this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes both a Basic and a Model-Agnostic Meta-Attack, designed to automatically produce more effective update directions during adversarial example generation than standard or hand-crafted evasion attacks against deep neural networks.",
            "main_review": "The presentation of the paper was very good. Clearly laid out and comprehensively presented, with thorough experiments to back up the claims.\n\nI am not an expert in this particular area, and so my comments may be misguided or have missed key issues (my Confidence score reflects this).\n\nMy main reservation is that the improvement in performance is only minor. That said, since it comes at little extra computational cost, it is certainly not an argument to not use it.\n\nRelated to the above comment, the deltas reported in Tables 4 and 5 are against the Reported accuracies from the source rather than the author’s discovered accuracies from their experiments. This seems to show their method(s) in a better light and so is a bit disingenuous.\n\nSome minor comments:\n-\tRemove “the” (“the maliciously ...”) in first sentence of Introduction. Same at the start of the Ethics Statement\n-\tIn Algorithm 1, n is undefined\n-\t“be integrating” should be “by integrating” in caption for Table 2\n-\tAre there missing bold highlights on some of the data in Table 3?\n-\tThe Ethics Statement section is not an ethics statement. It is more of a summary of the paper.\n-\tIn the Ethics Statement: a few grammatical corrections\n    o\t“effects of” should be “effects on”\n    o\t“and may promote new adversarial attacks” (i.e. add “and”\n-\tThe second line of the two line equation in Annex B is corrupted (norm)\n-\tIn Annex C, discussion of the MT attack refers to 9 classes. More general to say “all other classes” of course.\n",
            "summary_of_the_review": "The paper provides only small improvements in performance. But since this is at minimal computational cost, this is not necessarily an issue. Given the importance of effective adversarial example generation in properly assessing the risk in usage of ML solutions, any contribution to improving the state of the art is welcome.\n\nThe authors draw a distinction to a similar Learning to Learn approach from 2020, establishing that though their approach is not therefore strongly novel, is distinct and has merit in its own right.\n\nHence I would recommend this paper for publication.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}