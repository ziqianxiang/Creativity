{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes a sampling technique for unnormalized distributions. The main idea is to gradually transform particles by following the gradient flow of the relative entropy in the Wasserstein space of probability distributions. The paper tackles an important problem and provides an interesting new perspective. However, even putting aside the concerns on the theoretical analysis raised by the reviewers, the experimental evaluations does not seem sufficient to demonstrate the benefits of the proposed approach."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper suggests a method for approximating the 2-Wasserstein gradient flow for the relative entropy. The proposed particle-based method uses a neural network function approximation-based approach to estimating the necessary density ratios. Experiments verify reasonable performance compared to MALA and ULA.  ",
            "main_review": "The paper appears to miss the fact that the 2-Wasserstein gradient flow for the relative entropy defines a Markov process, which is exactly the Langevin dynamics; this can be seen by comparing Eq. (4) to the Fokker–Planck equation for an Ito diffusion (e.g., Eq. (4.1) in [1]; see also section 3.5 in [2] for a relevant discussion from the ML literature). Indeed, the Fokker–Planck equation determines the diffusion uniquely because the differential operator in the Fokker–Planck equation is the adjoint of the infinitesimal generator of the diffusion. Thus, the proposed algorithm is a rather unorthodox way of approximating a Langevin distribution. \n\nThe paper makes the said approximation more difficult than it should be by using a particle approximation that requires estimating density ratios, a notorious tricky problem. The algorithm ends up having some ability to handle multimodality because the use of weighted particles and density ratio estimation allows the algorithm to effectively compute the relative volumes of different modes of the distribution. The proposed method for estimating the density ratios appears to be the same as Geyer’s reverse logistic regression method [3], with a neural net replacing the inner product. Thus, I expect similar results could be obtained more directly, and with only a single round of volume estimation, by (1) running MALA many times and (2) then estimating the relative volume of each chain (note there are numerous other methods for doing this other than reverse logistic regression). Such an approach should work well on the kinds of low-dimensional examples considered in the numerical experiments. \n\nFurther issues arise in the experimental evaluation. First, the experiments seem to show very similar performance to MALA, all within the standard errors when provided (e.g., in Table 2). Second, I’m concerned about the quality of the MALA implementation, for which code was not included. The lack of convergence in one example suggests MALA was not run with appropriate step size adaptation targeting the optimal acceptance rate [4,5], as is standard in the literature. If so, then the comparison is not appropriate. Third, for a fair comparison, the MALA chains should also be reweighted based on volume estimates for each chain, as described above. \n\nIs it possible there are some gains from using the proposed method on multimodal distribution? Yes. But I remain skeptical. Moreover, if the goal is prediction, I expect combining MCMC with stacking will be more effective [6,7]. \n\n\n[1] Pavliotis, G. A. Stochastic Processes and Applications. (Springer, 2014).\n\n[2] Liu, Q. Stein Variational Gradient Descent as Gradient Flow. In NeurIPS (2017).\n\n[3] Geyer, C. Estimating normalizing constants and reweighting mixtures. Technical Report (1994). \n\n[4] Roberts, G. O. & Rosenthal, J. S. Optimal scaling of discrete approximations to Langevin diffusions. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 60, 255–268 (1998).\n\n[5] Roberts, G. O. & Rosenthal, J. S. Optimal scaling for various Metropolis-Hastings algorithms. Statistical Science 16, 351–367 (2001).\n\n[6] Yao, Y., Vehtari, A. & Gelman, A. Stacking for Non-mixing Bayesian Computations: The Curse and Blessing of Multimodal Posteriors. arXiv.org, arXiv:2006.12335 (2020).\n\n[7] Yao, Y., Vehtari, A., Simpson, D. & Gelman, A. Using Stacking to Average Bayesian Predictive Distributions. Bayesian Analysis 13, 917–1007 (2017).",
            "summary_of_the_review": "The paper seems to have a fundamental misunderstanding of the Wasserstein gradient flow for the relative entropy, and the experimental evaluations may not be appropriate. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper considers the problem of sampling from an unnormalized distribution. The unnormalized target distribution can be regarded as a stationary point of the Wasserstein gradient flow of the corresponding relative entropy functional, which can be equivalently identified from a microscopic perspective by defining a time-varying velocity field of the particles. While the exact time-varying velocity field is not exactly available, the authors propose to estimate such a quantity by approximating the corresponding logarithmic density ratio through minimizing the Bregman score. Such an approximation requires only samples from the variable distribution which can obtain by simulating particles following the estimated velocity field.",
            "main_review": "Wasserstein gradient flow has proved to be a useful tool for sampling from an unnormalized distribution. Section 2 to 4 of this work follow the standard derivation of the work along this research line and well explain the particle evolution strategy. Since the underlying velocity field of the Wasserstein gradient flow requires the access to the variable distribution $q_t$ which is in general not available, a key step in methods along this research line is to estimate such a quantity. To estimate such a quantity, this work proposes to estimate the log density ratio between the potential function and the variable distribution $q_t$ by minimizing the Bregman score which is described in Section 5. \nHowever, I find Section 5 difficult to understand. It would be vary helpful if the authors could explain the intuition of the Bregman score. In fact, I think there should be an individual section in the preliminary that describes the Bregman score and all the statements below equation (15) so that the reader can follow this very important step. \n\nI think section 5 is the part that differs this work from previous work like [1] and is where the novelty of this paper lies. It need to be very clearly explained.\n\n\n[1] Degond, Pierre, and Francisco-José Mustieles. \"A deterministic approximation of diffusion equations using particles.\" SIAM Journal on Scientific and Statistical Computing 11, no. 2 (1990): 293-310.",
            "summary_of_the_review": "This paper leverage the microscopic equivalence of the Wasserstein gradient flow of the relative entropy to sample from an unnormalized distribution, but the derivation of the key step in the proposed approach is not well explained. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a novel way to sample from unnormalized distributions. This is helpful when calculating or estimating the normalizing constant is untractable.\n\nThe main idea is to track the gradient flow of the relative entropy in the Wasserstein space of probability distributions. It is known that the flow converges to the target distribution and the paper introduces a variational characterization of the discretized steps. The main benefit of this characterization is that it bypasses the need to know the normalizing constant as well as being amenable to estimation by using a combined particle evolution.\n\nThe benefits of the new algorithm are demonstrated through several numerical simulations.",
            "main_review": "I enjoyed reading the paper. It is well written, the motivation is clear and it is easy to follow the main ideas.\n\nHowever, I find it hard to assess the actual contribution of the paper.\nOn one hand, while the proposed algorithm makes sense, there is no guarantee in the paper, either for the sampling accuracy or even for the fact that the algorithm will converge to the target measure. For example, is there any guarantee that the discretized flow does not add bias to the obtained measure?\nThere are many tunable parameters in the algorithms, $s$ the discretization step, $K$, the time horizon, $n$, the number of particles. What is the interplay between those parameters, given some distribution, how should I choose those?\nI would have expected to see a bit more of the underlying theory behind this algorithm.\n\nOn the other hand, from the perspective of actual results, I find that the numerical experiments are somewhat restricted and artificial. Coupled with the computational overhead, it's not clear to me when one will actually prefer to use the new algorith,\n",
            "summary_of_the_review": "The idea is elegant interesting but the paper lacks evidence for its usefulness, both from theoretical and applied perspectives.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper addresses the issue of sampling from an unnormalized distribution. The sampling problem is cast as the numerical simulation of the gradient flow associated with the KL divergence between the target unnormalized distribution and the approximating distribution. The challenging part is to estimate the density ratio that appears in the gradient term. The authors propose to use a deep neural network to estimate the density ratio. Numerical results show the usefulness of the proposed method.",
            "main_review": "Overall, the paper is well written: contributions are clearly stated, relation to previous is presented, the proposed method is well explained and a somewhat extended comparative numerical evaluation of the proposed method is given.\n\nI find the overall approach interesting - the difficulty of sampling being cast as a density ratio estimation problem. This new problem, however, is not easy to solve and your approach of using a neural network to estimate the density ratio seems to work, at least in the considered examples.\n\nThat being said, I do have some issues with the paper. You mention in the conclusion that you hope to establish the convergence properties of the proposed method. I understand that it is not trivial to establish convergence. However, not presenting at least some intuition about the convergence of the algorithm is a strong drawback. The way I see it is that there are two sources of error that could hamper convergence: the discretization error of the numerical implementation of the continuous gradient flow and the approximation error of the density ratio. I didn't find anything about either in the paper. If not a proof, at least some intuition about how they affect the results, about how they interact, etc.\n\nIn the numerical experiments section, you present a fair amount of examples which show that the proposed method is capable of outperforming the competing algorithms. The results are interesting, however, there are no results to show how the performances of the proposed method vary with the different parameters, notably the number of considered particles, and the choice of distribution w. Such results would cast some light into the inner workings of the proposed method and would be useful for anyone interested in using it.\n\nIn section 6.4 you do mention that the improved performances of the proposed method come with a higher computational cost. However, you do not perform any analysis of the trade-off between computation time and performances. It would have also been interesting to compare the performances of the different algorithms for the same computational budget.\n\nAnother aspect that struck out to me is the choice of competing algorithms. The algorithms that you choose as competitors are valid, however their choice is questionable. My first remark was why didn't you choose the SMC algorithm as a competitor? It also uses particles in order to estimate the unnormalized target distribution. Also, HMC could have also been considered.\n\nI spotted some typos here and there, for example chians instead of chains on page 8 in the bottom paragraph \"... denote the ULA and MALA with k chians\", repeats instead of repeat on page 9 \"We repeats the random partition 10 times.\". \n\nAnother small issue is with figure 4, it's hardly readable. I understand that there is a limit on the page count, however, that's not a justification for having figures that are hard to read. More so, as there are some redundancies in the text that could have been eliminated, for ex. equations (11) and (13) are the same, is the presence of both necessary for understanding the idea that is presented in section 4?",
            "summary_of_the_review": "I find the approach interesting. However, there are some issue with the paper as it is, both theoretical and empirical. From a theoretical point of view, there is no discussion about conditions for convergence of the algorithm. From an empirical point of view, the numerical experiments are not complete enough, with respect to the comparison analysis that is carried out, but also with respect to compensating missing theoretical analysis. Overall, the paper is interesting, but in its current form is not ripe enough for publication.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}