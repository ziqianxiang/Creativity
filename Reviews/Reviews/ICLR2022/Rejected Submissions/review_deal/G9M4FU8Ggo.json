{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "All reviewers recommend rejection, and I'm following this recommendation."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes an efficient concrete novel Neural Architecture KD one-shot NAS algorithm that uses a diversity-based sampling algorithm to assemble outstanding ensemble-based teachers for Knowledge Distillation. They demonstrate their method on Imagenet with several different search spaces and significantly improve the SOTA accuracy for KD one-shot NAS. They demonstrate in ablation studies that the diversity sampling algorithm improves both the accuracy of the teacher network used in distillation as well as the performance of the final derived architectures.",
            "main_review": "Strengths:\n- This paper explores and tackles an useful area of improving the teacher network in self-distillation augmented NAS.\n- They propose a concrete algorithm to create stronger teachers using ensembling of subnetworks and and demonstrate that they can achieve higher performance on Imagenet compared to similar methods in a several albeit similar search spaces.\n- They introduce a novel diversity sampling algorithm to improve the diversity of models used to build the ensemble and demonstrate its affects with ablations. They show that it it improves final model performance and allows the construction of a stronger teacher with less training.\n\nWeaknesses:\n- While they achieved significant results on Imagenet, the paper could be significantly improved by inclusion of significantly more empirical results. It would be especially useful to demonstrate that the one-shot network can still transfer to other datasets. It would also benefit from results on the NAS benchmark datasets to better characterize the ability of the algorithm in generally characterizing the architecture space. Are the results you show from only one run of the algorithms?\n- The current wording and presentation of the paper make important details of the paper and the algorithm difficult to follow. It is unclear if all of the claims are strongly supported.\n- The algorithm itself could be much better understood with significantly more ablations of the different hyperparameters like the number of subnets chosen, and the weighting between the distillation loss and the Cross entropy loss.\n\n\n\nQuestions:\nIn the abstract and results you claim, \"Eventually, compared with existing works, on\nthe real-world dataset ImageNet, EnNAS improved the top-1 accuracy of architectures searched out by 1.2% on average and 3.3% at most.\"  I'm not sure based on table 1 compared to CREAM if that is accurate? I believe CREAMS average = 77.86% EnNAS average = 78.47%\n\nWill the code be released for reproducibility?\n\n\"After training the supernet for several iterations, we follow the process described in Section 3.3 to update the probability distribution of operators in the supernet correspondingly. Since the operator diversity does not change drastically, updating operator diversity per epoch does not affect the diversity of subnets sampled.\"\n\nWhat are the details for updating the probability distribution? It does not seem to appear in the algorithm psuedocode. Is it updated per epoch? Since the distribution is dependent on the previously selected subnets, how doe that work? Does this bias the one-shot network towards a particular architecture distribution? How many images are used to determine this diversity sampling distribution?  In section 3.3, you claim that the method both biases toward diverse outputs and every operator is trained equally. Could you explain further? I believe I am misunderstanding.\n\nIs this method unsupervised? The paper seems to claim that this paper, and the CREAM paper are unsupervised, but I believe both train on the labels of Imagenet. It might be a bit confusing to overload the meaning of that term. Calling it self-distillation may be less confusing?\n\nLooking at Figure 1 and 2, it seems like the gap in performance of the algorithms and diversity of subnets with and without diversity sampling significantly drops towards the end of training? Would they converge if you continued training longer? How many runs did you do in the abaltions\n\nDoes there happen to be a particular theoretical grounding for the specific derivation of the diversity-based sampling based being linearly proportional to the cosine similarity?\n\nMinor:\nensebmling in the conclusion",
            "summary_of_the_review": "This paper explores a useful area of improving the performance of the teacher network in self-distillation augmented NAS and shows that this can empirically improve NAS performance. Unfortunately, I currently recommend rejection of this paper since it does not have sufficient empirical results to demonstrate the robustness of the proposed improvement and the robustness of their method on different datasets and as a one-shot method. The paper's presentation is also unclear and some of the results may be stated incorrectly so it is unclear if all the conclusions are well justified.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper focuses on knowledge distillation for NAS, which improves the student networks by the ensemble teacher models. To generate better ensemble teacher logits, the authors proposed to sample operators that predict diverse feature maps to form subnets. \nExperiments shows the proposed method achieves better results compared to state-of-the-art KD-based NAS methods.",
            "main_review": "- Strength \n1. Well written. Easy to understand.\n2. The motivation is clear.\n\n\n- Weakness\n1. Inconsistent experiment results. The authors claimed the proposed method ``achieved 1.2% higher than the average top-1 accuracy compared to state-of-the-art methods''. However, I checked Table 1 and found there is only about 0.5% higher than Cream. Also, I checked the revision history and found the authors claimed a 1.71% improvement on their first version. \n\n2. The idea is kind of straightforward. Using the ensemble teacher has been widely used in previous knowledge distillation works and semi-supervised learning methods (e.g., mean-teacher, co-training). As for the diversity-based sampling strategy, it is quite trivial of the current design.\n\n3. For the diversity-based sampling strategy, the authors sample operators sequentially, which might not be a good choice. In this way, the first selected/sampled operator(s) would have a huge impact on the following sampling process, which doesn't make sense. This may greatly  affect the final performance since the sampled set would be totally different given different random initializations/seeds.\n\n4. The authors did not try other sampling strategies as baselines. For example, always choosing subnets with most different connections might be a strong baseline.\n\n5. What is \\lambda just below Eq. (3)? Is it a typo or something I missed. \n\n6. What is the impact of the hyperparameter \\tau? Is it very sensitive?\n\n\n",
            "summary_of_the_review": "The solution is somehow trivial (e.g., diversity-based sampling strategy). The experiments are not sufficient. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "In this paper, authors propose to dynamically construct a teacher network from the ensemble of multiple student networks, and then use it to guide the training of super-net via knowledge distillation. The ensemble of student networks is built with the diversity criterion explicitly considered to improve the teacher network's accuracy. After the super-net is well trained, the optimal network architecture is determined by evolution search.",
            "main_review": "Strengths:\n1. The proposed method avoids pre-selecting a sufficiently accurate teacher network to guide the training of super-net. Instead, the teacher network is dynamically constructed and will have higher accuracy as the super-net is gradually trained to its optimum.\n2. The diversity-based criterion for selecting student networks seems to be a reasonable choice for constructing a high-quality ensemble as the teacher network.\n\nWeaknesses:\n1. Comparison with some more recent weight-sharing NAS methods are missing. For instance, AlphaNet [1] also aims at improving the training of super-net by replacing the commonly used KL-divergence with alpha-divergence in knowledge distillation. It seems that they achieved a better accuracy-FLOPs trade-off than this paper (~79% @ 300 MFLOPs and ~80% @ 500 MFLOPs).\n2. For constructing the ensemble, student networks are sampled based on the diversity criterion. However, the prediction accuracy of each student network is not considered. Will this lead to a sub-optimal teacher network, if the search space is not well designed (e.g. contains many low-quality operator choices)?\n3. Section 3.2, last paragraph. What does $\\lambda$ stand for? Besides, why there is a negative sign in the knowledge distillation loss in Equation (3)?\n4. Section 3.3, Equation (4). The cosine similarity ranges from -1 (completely opposite) to 1 (identical), which means that the diversity metric defined here ranges from 0 to 2, instead of \"between 0 and 1\".\n\n[1] Dilin Wang, Chengyue Gong, Meng Li, Qiang Liu, Vikas Chandra. AlphaNet: Improved Training of Supernets with Alpha-Divergence. ICML 2021.",
            "summary_of_the_review": "My major concerns on this paper includes:\n1. The comparison between some recent weight-sharing NAS methods, e.g. AlphaNet, is missing.\n2. The diversity-based sampling method may impose additional restraints of the design of search space of network architectures.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}