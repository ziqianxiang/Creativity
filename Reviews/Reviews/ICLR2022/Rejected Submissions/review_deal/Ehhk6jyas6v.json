{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper considers the question of whether recent concept-based learning algorithms, as well disentangled representation learning algorithms, result in high-quality representations. In particular, the authors consider what high-quality should mean in terms of the relationship with ground truth concepts and the ability to make accurate predictions for a downstream task. To this end, they propose two main metrics for representations that are explicitly or implicitly encouraged to encode concepts. While the premise of this paper has been appreciated by the reviewers, some concerns about the details of the metrics proposed and experimental results which have been raised by the reviewers remain post rebuttal. Given this, we are unable to recommend the acceptance of the paper at this time. We hope the authors find the reviewer feedback useful."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors have put decent effort to bring concept-based representation learning and disentanglement learning together under one umbrella in terms of the quality of generated concepts in presence as well as absence of ground truth concept labels. Some related metrics were proposed for evaluation of the quality of concepts for both these methods. Based on these studies, presented in the paper, some important conclusions were made based on requirements of concept supervision and their effects on final concept quality as well as the predictive performance of the model.",
            "main_review": "This paper introduced metrics to evaluate the concept quality generated by both methods i.e. concept-based representation learning and disentanglement learning in the scinerio of concept supervision and correlation of concepts. For this purpose, mainly three family of methods were considered i.e. concept-based representation learning both with and without concept supervision and semi-supervised disentanglement learning method. Based on this metrics, some recommendations were made regarding requirements of explicit concept supervision for concept-based representation learning methods and weak supervision for disentanglement learning methods.\n\nThis paper has below weaknesses/clarifications/suggestions:\n1)It's not very clear how the concept property (ii) is related to the contributions made based on concept correlations and availability of ground truth cocnepts. The property is definitely a desirable property for the concepts, but how the proposed metrics capture whether a set of generated concepts follow this property, specially when the ground truth concepts are not available.\n\n2)How each entry of the purity matrix is calculated? Do all the data points are required to calculate each matrix entry? Please elaborate the line \"In practice, we parameterise the family of functions Ïˆ j by training, via gradient descent, a ReLU MLP with 32 hidden activations\" in this context.\n\n3)A very important work (https://arxiv.org/abs/1806.07538) is missing from the paper, which can be a very good representative of unsupervided concept learning methods. This work has used both label supervision and decoder based network for better concept learning. Extending in the same line, What is your opinion on (possibility of) merging these two types of methods (CL and DGL) for better concept learning? If you think it's possible, then how would the proposed metrics would be helpful here?\n\n4)Figure 8 shows the effect of network capacity. On top of that, it would be interesting to see the individual effects of capacities of encoder and the decoder. To be precise, improving the capacities of both the encoder and the decoder would result low impurity and higher cocnept accuracy, but it's important to check if capacity of the encoder has more effect on concept quality than capacity of the decoder.",
            "summary_of_the_review": "This is an interesting work that tried to unify concept-based representation learning and disentanglement learning and proposed corresponding metrics that helped to find some important conclusions. Answering/commenting on the points, that I posted under the main review, can significantly improve the quality of this work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors consider the question of whether recent concept-based learning algorithms, as well disentangled representation learning algorithms, result in high-quality representations. In particular, they consider what high-quality should mean in terms of the relationship with ground truth concepts and the ability to make accurate predictions for a downstream task. To this end, they propose two main metrics for representations that are explicitly or implicitly encouraged to encode concepts: 1) a score that captures how well the learned representation preserves the relationships between concepts (which may be correlated), and 2) a score that captures how well concepts can be split into groups that are useful/useless for predicting particular label dimensions.",
            "main_review": "Concept-based explanations are a relatively new idea; the first papers on this topic, including TCAV and its extensions, were post-hoc methods that did not require specialized model architectures. More recently, the concept bottleneck idea suggested training a model that explicitly encodes concepts at an intermediate layer (enabling interventions, etc). In parallel, as the authors say, there has been a rich literature on unsupervised/semi-supervised disentangled representation learning, where models are trained that may or may not capture semantically meaningful, decorrelated concepts in their latent spaces. It is natural to wonder to what extent these various methods can result in representations that are useful from a concept-based model explanation perspective. \n\nBefore getting into the authors' contributions, it's probably worth saying that the intuitive answer here (for me, at least) is that concept supervision is key, and without it we can't expect a model to learn a latent space that is well-aligned with concepts that we don't tell it about. Ultimately, this appears to be the takeaway from the experiments (see figures 2, 4, 5), but it's not a particularly exciting or surprising result; it's showing what people would expect to begin with, but with a little bit of rigor.\n\nBelow, I'll summarize and comment on the authors' two proposed metrics.\n\n### Oracle impurity score\n\nThe proposed OIS score basically assesses to what extent the learned concepts predict the ground truth concepts equivalently to how the ground truth concepts would. Some previous work suggests that concepts should not be predictive of one another because this would constitute \"leakage,\" which is problematic for experiments with concept interventions. To remedy that, because concepts in fact are related (this can likely be seen in the ground truth annotations for most datasets), the authors consider how well each ground truth concept can predict another (using AUC, for example) and then ask whether the learned concept achieves the same predictive performance.\n\nMore specifically, the authors build a matrix of predictive accuracies for all pairs of concepts, using both the ground truth and learned concepts as predictors. They then take the Frobenius norm of the difference between these two matrices and apply a couple constant factors.\n\nThis is fine, it seems like an improvement on previous metrics for measuring leakage. It also seems related to the idea of measuring how well the learned concepts can predict the ground truth concepts in a 1-1 manner, which is one of the main metrics in the CBM paper. While that metric effectively assesses the mutual information between matched pairs of learned and ground truth concepts while ignoring inter-concept relationships, this one measures (roughly) the delta in mutual information across all pairs.\n\nA couple questions and concerns about this:\n- The bounds used to guarantee that the score is in [0, 1] seem to require the use of AUC as a performance metric. What if we want to use something else, e.g. because the concept is multivariate or real-valued?\n- If you want to quantify the mutual information for a discrete concept, why not use cross entropy loss? This would approximate $H(c_i | c_j)$ and $H(c_i | \\hat c_j)$, whose difference is equal to $I(c_i | c_j) - I(c_i | \\hat c_j)$. This seems more aligned with what you're talking about than AUC.\n- The use of a Frobenius norm on the difference in matrices of prediction accuracies seems a bit heuristic. It's fine, it just seems a bit arbitrary and I wonder if there's a better way to do this depending on what the particular accuracy metric is (e.g., if we had exact mutual information values).\n- The notation in definition 1 is a pretty odd way of saying you train a model $\\psi_j$ to predict one concept from another. I would not have been able to understand this without the explanatory text below. Please consider rewriting this a different way.\n\n### Niching scores\n\nThe next metric tries to provide some measure of concept-based representation quality without requiring access to ground truth concepts. This is quite a leap, because it requires resorting to some notion of quality that departs from how previous work has assessed concept-based representations. \n\nThe authors suggest that for a good concept-based representation, we should be able to identify a small number of concepts that are predictive of each output dimension, and that the remaining concepts should not be predictive (because this would constitute leakage). I read this part of the paper a couple times and I just don't buy this argument, I don't see how the ability to \"niche\" concepts into groups that are useful/useless for each output dimension is a measure of the representation's quality. This is not what I would think of as a measure of whether my learned concepts capture what they're supposed to capture. And the ability to do well on these scores seems dataset-dependent: what if there are output dimensions where every concept is useful, or where no concept is useful? It's a nice idea to try to make a metric that doesn't require access to ground truth concepts, but I don't quite agree with the premise of this particular approach. (Though I'm curious to hear if other reviewers disagree.)\n\nSome other questions and concerns:\n- I'm not so sure about framing the fact that previous metrics require access to ground truth concepts as a weakness. First off, if you're using some of these methods (e.g., CBM) we know you have access to ground truth concepts, so it's not an unrealistic assumption. Second, existing notions of high-quality concept-based representations are defined specifically based on learned representations capturing known concepts, so access to those concepts is clearly necessary; you haven't gotten around that need, you've just opted for an orthogonal notion of representation quality.\n- Isn't a high NPS and low NIS virtually guaranteed by the construction of your concept niche? You're effectively doing feature selection to ensure that the predictive concepts are in one group and the useless ones are in another (for a given output dimension). For any representation at all, it should not be surprising if NPS is high and NIS is low. Or am I missing something here?\n\nAs explained above, I have some qualms about this metric, but given its current state here are a couple other comments:\n- When defining your \"concept nicher\" (i.e., a way to decide which concepts are relevant to a given output dimension), there are a couple problems with your use of correlation. First off, it's worth noticing that correlation is essentially a measure of the MSE from a fitted linear regression model (check the math on this for the specific relationship). With that in mind, there's a better option than taking the max score when you have multivariate learned concepts: derive the MSE from a fitted multivariate linear model that takes in all the concept dimensions. But why not allow a nonlinear model? Also, what if the label is not real-valued but a classification label? Rather than using correlation, wouldn't it be better to fit a classifier and check the cross entropy loss? Yes, this requires fitting a model, but that step is only required once per method evaluation.\n- How are we supposed to choose beta? Different beta values will result in very different outcomes. For example, if I set beta close to 0, I expect we'll see great NPS/NIS scores (because everything will be deemed relevant to every output dim). \n- Overall, there's a lot of wiggle room in your definition of \"concept niching\" and it would be nice if there were a better justified default option. On the other hand, this aspect of your paper is essentially feature selection, and that's a hard problem with approximations of varying quality and computational cost.\n\n### Experiments\n\nThe experiments section uses the proposed metrics to compare some SOTA concept-based and DGL algorithms. As mentioned at the beginning of my review, these experiments overall felt a bit too much like sanity checks (particularly figure 3), because the outcome was mostly what I would have expected to begin with. Also, it could have been nice to use some non-toy datasets.\n\n### Other comments\n\n- While you're at it evaluating various methods, why not include any unsupervised (rather than semi-supervised) DGL algorithms? It would be interesting to see how much worse these do than the other approaches (which is perhaps to be expected because of the works cited).\n- The OIS score appears to require a known alignment between learned and ground truth concepts, is that right? Am I correct in understanding that this is somehow known for semi-supervised DGL approaches but not unsupervised ones? \n- The definitions in this paper are not easy to read. They're too verbose and too long, please try to make them more concise.\n",
            "summary_of_the_review": "It's a good idea to assess whether unsupervised and semi-supervised DGL algorithms can learn high-quality concept-based representations, and in doing so the authors developed two metrics that they suggest can be used to compare concept-based representations both with and without access to ground truth concepts. However, the results from their study do not seem very surprising or impactful, and I was not convinced of the rationale behind these metrics (for the reasons described above). For that reason, my rating is that this work is marginally below the acceptance threshold.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose new metrics for methods in disentanglement learning and concept learning,  which have some nice properties, including robustness to correlations in the underlying factors, and experimentally probe the properties of these metrics on a number of different proposed methods in the disentanglemnt/concept learning literatures.",
            "main_review": "\nStrengths\n\n- metrics which work for correlated underlying factors are really valuable and I think a big blind spot in this space, glad to see this is a focus of this paper\n- proposed metrics have some nice underlying intuitions and seem sensible to me\n- experiments seem quite extensive, comparing many prior methods from the literature in both CL and DGL\n\nWeaknesses\n\n- paper could greatly benefit from improved clarity throughout: there are a number of points where I struggle to understand exactly what is being communicated\n- In particular, clearer descriptions of methods and how those differences manifest in the results section would be helpful - I don't have a deep understanding of all the related work discussed and I have a lot of trouble knowing what the takeaways from the results section should be. Some figures which accent the particular points in question in each section could help, or more analysis of the overall concepts in question for each experiment\n- it's not entirely clear conceptually how we're supposed to think about CL and DGL in this paper. Is the paper intended to unify the methods? Contrast them? I don't fully understand what the authors see as the main differences or similarities, or what they want to communicate to the reader on this front. It would be good to spend more time developing a framework and some common language to describe the two approaches\n- I give many more specific notes below\n\nOther\n\nSec 1:\n- the acronym DGL for disentanglement learning is strange - I'd personally prefer something like DEL but up to you\nSec 2:\n- a working example at the top of Section 2 explaining what each concept might represent would be very helpful, especially to someone like me who is familiar with the overall literature but not on the bleeding-edge methods\n-  not sure why we need both g and \\phi in this framework?\n- can you clarify the role of the downstream task here? it's never really made clear - are there many tasks? just one? do we expect to know it at training time?\n- would be useful to define TCAV\n- you state that if a function g contains sufficient statistics to perform well on a downstream task f, then g must be invertible; I don't see why this is true, since g may reduce the input \\phi to just the sufficient statistics, and much of \\phi may not be recoverable\n- you list the operating assumption of DGL, what is the analogous assumption for CL?\n- you focus on weakly supervised disentanglement, why not weakly supervised CL as well?\nSec 3:\n- some sloppiness in language at the top - I'm not sure what the \"quality of their learnt concepts\" is meant to refer to: only CL (since you used the word \"concept\")? or both, since you refer to all the \"approaches above\". This is where I think it would be useful to clarify a particular framework for how you relate these two literatures\n- may be beyond the scope of the work, but I agree that the no-leakage assumption is unrealistic - however there are some cases where we want it (the two concepts are truly related) and some cases where we don't (if it's a relic of sampling bias). Just an interesting thought, not sure if there are ramifications in your paper\n- it would be good to discuss the CUB example in the main body since it is an important point for the rest of the paper\nSec 3.1\n- Def. 1: what does it mean for two representations to be \"aligned element-wise\"?\n- Def. 1: the argmax notation loses me - is the idea that the concept should be categorical? I don't think this was specified earlier\n- I don't understand the usage of AUC here - are you assuming binary concepts?\n- Implementation concern which should probably be discussed: your estimate of \\pi depends on calculating a \\sup over functions \\psi - this will probably be only approximate, and if this is a related function class (or less powerful) than the encoder in question g, then you might systematically bias your estimate of \\pi and OIS\nSec 3.2\n- I'm not totally sure the analogy with evolutionary biology is necessary here\n- there's an assumption implicit in niching which is that all tasks you care about will necessarily rely on a specific subset of individually predictive dimensions - worth motivating this assumption, it's a little bit different from the standard DGL assumptions\n- Def. 6: technically N_j is not an input to NPS, just clean up formalization here to clarify what you mean by \"the NPS of \\not N_j (v_f)\"\nSec 4.2\n- \"in order to have datasets compatible with both CL + DGL ... construct datasets full described by ground truth gen. factors\" - I'm not sure what this means, can't you use any dataset with either of these methods?\nSec 4.3\n- clarify exactly what the metric \"Mean Concept AUC\" refers to - I'm not quite sure how it's calculated\n- why is the non-oracle impurity baseline matrix using 1/2 on its off-diagonals - wouldn't the baseline be 0 on the off-diagonals to mimic the situation where we assume no leakage?\n- you say that the non-oracle impurity \"misleadingly\" shows more impurity as dependency increases. However, it's not clear this is misleading, it's possible that the methods are truly doing worse on that data (even relative to oracle). It would be nice to have something external to tell us which is the case\n- suggestion: indicate in figures which methods are supervised, this will help interpreting them a lot\n- the last sentence of the \"oracle impurity demonstrates\" paragraph seems to contradict the first: the first says that explicit supervision \"encodes purer individual concepts\", the last says explicit supervision \"does not translate to purer concepts\"  - I'm confused which should be true\n- the reference to \"concept loudness\" should be explained to those not familiar with the literature\n- the point at the bottom about robustness to concept inter-dependencies seems really important - I would love to see this fleshed out more\nSec 5.\n- (iii) you discuss the \"weak supervision provided by DGL methods\" but aren't there supervised, weakly, and un-supervised DGL methods?\n- why should we prefer niching-based metrics?\n- this point about efficiency goes over my head, would be good to have a quick discussion in the paper\n\n",
            "summary_of_the_review": "The authors propose metrics which address a very important problem in the relevant literatures (in particular, underlying factors which may be correlated), but the communication of the ideas and experimental results are not currently clear enough for me to accept the paper as is.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}