{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposed a speech-to-speech translation (S2ST) model. The model is trained end-to-end from speech to speech, along with an auxiliary speech-to-phoneme task. Experiments firmly support multiple claimed improvements to the previous model.\n\nHowever, most reviewers argue the novelty and clarity of this paper, making this paper cannot be accepted by ICLR-2022. We hope the authors can modify this paper accordingly."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper advances the previously proposed direct speech-to-speech translation (S2ST) model Translatotron.  Direct S2ST has many benefits but has quite limited research effort. The major contributions of this work can be summarized as follows. Translatotron 2 addressed the following major problems of Translatotron: 1)  translation quality, naturalness and robustness of predicted speech are significantly improved and now comparable to a cascaded system (i.e., speech-to-text translation followed by TTS).  2) Similar to Translatotron, the model retrains source speaker’s voice in predicted speech, but Translatotron 2 achieves this without relying on any explicit speaker embedding or speaker ID, hence Translatotron 2 will not be able to generate speech in a different speaker’s voice. This will address some ethical concerns for deployment. Translatotron 2 achieved these improvements through the following approaches. Compared to the previous Translatotron model,  1) Translatotron 2  uses  the output from the auxiliary target phoneme decoder as an input to the spectrogram synthesizer;  2) the spectrogram synthesizer is duration-based, while still keeping the benefits of the attention mechanism.",
            "main_review": "The main strengths of the paper include the follows.\n(1)\tCompared to other direct S2ST works which mostly focus on translation quality, this work also aims to improve perceptual quality such as naturalness of the translated speech, which is also very important to the overall quality and user experience of a S2ST system.\n(2)\tAlthough Translatotron 2 also uses an encoder (conformer)-decoder (a stack of LSTMs) with attention modules, which is similar to a ST model, the difference is that instead of predicting subword units as a standard ST model, Translatotron 2 uses this framework to predict target phonemes, and then uses the phonemes as input to the spectrogram synthesizer. Note that the original Translatotron uses both source and target side phoneme predictions as auxiliary training tasks but Translatotron 2 only predicts and uses target side phonemes, yet achieving a better BLEU than Translatotron, especially on small dataset.  \n(3)\tExperimental results showed Translatotron2 improves BLUE and naturalness (MOS) and robustness (UDR), compared to Translatotron, on conversational and smaller datasets, and also significant BLEU improvement (on ASR transcripts) on the multi-lingual CoVoST 2 dataset.\n\nThe main weaknesses of the paper include the follows.\n(1)\tThe innovations from Translatotron 2 over Translatotron are not very strong. The major innovations are the use of the encoder(conformer)-decoder(LSTM) with attention framework to predict target phonemes, instead of using both source and target phonemes, and use phonemes as input to the spectrogram synthesizer.  The synthesizer in Translatotron 2 replaces the attention module with a duration-based upsampler, which yields more robust synthesized speech. But this approach is similar to the previous published works such as (Shen et al., 2020). And, the synthesizer in Translatotron 2 is simplified as it only used the L2 loss on the total predicted duration of the entire sequence.  The more principled approaches such as using per-phoneme duration labels for training the duration predictor and applying FVAE alignment, as in (Shen et al., 2020), would be important to be investigated in this work, but they were left for future work.\n\n(2)\tSome experimental results need to be further discussed. For example, the effect of ConcatAug in Table 2, Table 3, and Table 4 is not discussed clearly. For example, ConcatAug hurts BLEU, MOS and UDR for the relatively larger conversational dataset, and it is useful to provide analysis for it.\n\n(3)\tAlthough there is a significant improvement from Translatotron 2 over Translatotron on translation quality, naturalness and robustness of predicted speech, as can be seen from Table 2, there is still a relatively significant gap between Translatotron 2 and cascaded (ST->TTS). It would be useful to provide analysis and provide insights on more focused future directions on direct S2ST.\n",
            "summary_of_the_review": "This paper extends the previous direct S2ST Translatotron model with Translatotron 2 model, which significantly improves translation quality (BLEU), the naturalness (MOS) and robustness (UDR) of predicted speech compared to Translatotron,  through a few modifications. The paper also proposed an approach to train targets in source speaker’s voice and prohibits generation in a different speaker’s voice to mitigate spoofing audio artifacts.  Experimental results on S2ST datasets of different sizes (relative large and small) and multilingual S2ST demonstrate the significantly improved performance from Translatotron 2 over Translatotron, and also the effectiveness of the proposed approach to retrain the source speaker’s voice in the translated speech.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a setup to perform Direct Speech To Speech Translation from one language to another. As such, we could view this as a voice conversion setup, but with the additional task of rendering translated voice in a different language. It seems to be created on the same lines as Translatotron, but with improved performance from different modeling choices. The architecture is encoder-decoder, with the decoder side consisting of two parts - a spectrogram synthesizer, and a phoneme/language translator. The phoneme translator is needed because it also needs to translate to a different language. \n\nThe experimental evaluation is quite good, consisting of cases to translate with the same voice as source (voice retention); samples with speaker turns (augmented dataset with two different speaker utterances concatenated together), and cross language translation. The paper shows improved results over Translatotron in all cases. \n\nAside from the results, the contributions seem to be incremental. They also emphasize a modified training setup that does not use speaker embeddings to avoid antispoofing. ",
            "main_review": "The paper gives a good picture of current developments in speech synthesis and voice conversion, especially those that are derived from the Tacotron family of models. The setup seems to be directly derived (at least in philosophy) from the Translatotron. In Translatotron, the main components were an encoder and two types of decoders - one to synthetize target spectrogram and the other to classify phonemes in source/target languages. The language decoders in Translatotron were treated in the manner of multi task learning - to add extra supervision to the setup rather than to be used directly for the task. In the current work, the language decoders seem to be performing a similar role, but the paper does not go too much in detail about the philosophical motivations. \n\nThe paper is generally very well written and results (also, samples) look good. However, I have some misgivings about \n\n1) The novelty of concepts. It seems that most of the ideas are already there in Translatotron, and this paper modifies architecture (and does not give much insight into these changes).\n2) I did not understand the presentation in section 4.1 - of retaining voice. The paper proposes an alternative way to produce source speaker's voice without conditioning with a speaker embedding, which they claim could be used for spoofing purposes. They also point to other works and especially, state that they can produce target voice with the help of a speaker encoder network and TTS. \n\n\"We modified the PnG NAT (Jia et al., 2021) TTS model by incorporating a separately trained speaker\nencoder (Wan et al., 2018) in the same way as Jia et al. (2018), and trained it on the LibriTTS corpus\n(Zen et al., 2019). ...\"\n\nI find that this could be explained in a clearer manner. Do they convert to text and then to speech? Moreover, the use of a speaker encoder seems to suggest that the embedding will add on to the context as in other works, but then wasn't the aim of the work to avoid using a speaker encoder for antispoofing purposes? Please clarify.\n\nOverall, I feel that the current work does not have sufficient novelty, and details are missing. ",
            "summary_of_the_review": "Not enough novelty.\nSome components not explained clearly,",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposed a speech-to-speech translation (S2ST) model that is and improvement to a previous work. The model is trained end-to-end from speech to speech, along with an auxiliary speech-to-phoneme task. The relevance of the two tasks is further exploited beyond parameter sharing, by feeding phoneme decoder's hidden layer output to spectrogram synthesizer's input, and the synthesizer is duration based. To retain speakers' voice from source language to target language, two data-centric approach is proposed. First, a zero-shot voice-transfer TTS model is trained, to transfer target speech into source speaker's voice. Second, to enforce local voice similarity within an utterance, two samples from two distinct speakers are randomly selected and concatenated to create new training samples. Experiments are conducted on speech-to-text translation (ST) datasets with TTS synthesized target speech, and are measured by both objective (BLEU) and subjective (MOS) metrics. The proposed model's translation quality is much closer to the cascaded ST+TTS oracle than baseline S2ST models, for both bilingual and multilingual S2ST tasks. Its generated speech is also more natural and more resembles the source speech.",
            "main_review": "The paper proposed several non-trivial improvement to a previous S2ST model, which significantly improves translation quality and speech naturalness, as well as mitigates audio spoofing concerns in production. Although some improvements are apparently inspired by existing work, they can still be considered somewhat novel and significant contribution.\n\nMost of the claimed improvements are substantiated by experiments. However, some new questions arise from the experiment results themselves.\n1. The reason to translation quality improvement (5.1, 5.5) is not discussed, and is not clear from the proposed approaches. Some ablation study would be helpful here.\n2. Voice retention in speaker turns (5.4.1) seems somewhat tailored towards ConcatAug, and its relevance to real-world S2ST application is not clear. (How often will a third person use S2ST to translate dialogue between two other persons?) Instead it would be more interesting to see experiment scenarios related to anti-spoofing as discussed at the beginning of the paper.\n\nThe paper is generally written clearly and is easy to understand. But some important technical details are completely left out as reference to previous work, such as 3.3 and 4.1. It makes readers not closely following latest research in speech quite difficult to understand. In other words, the paper is not self-contained. In addition, requiring reviewers to search for these references during a double-blind review can easily reveal authors' identity or affiliation.",
            "summary_of_the_review": "Contribution and novelty are moderately significant. Experiments firmly supports multiple claimed improvement to the previous model. Key technique are completely left out as reference to other work, making out-of-domain readers difficult to follow.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Translatotron2, a speech-to-speech neural based translation system is proposed. The work is a modification of Translatotron (also a speech-to-speech translation system) and tries to address some of the  issues in Translatotron:\n\na) Translation quality fairly below a cascaded baseline\n\nb) synthesized translated speech suffers from robustness issues, such as babbling and long pauses\n\nc) Voice retention relies on explicit speaker embedding which can potentially be misused for generating spoofing audio with arbitrary content\n\nCompared to Translatotron, following modifications are made in Translatotron2. The experiments suggest that Translatotron 2 significantly outperforms Translatotron, and is comparable to a cascaded system, in terms of translation quality, speech naturalness and speech robustness.\n\na)  Output from the auxiliary target phoneme decoder is used as an input to the spectrogram synthesizer \n\nb) Conformer Encoder with SpecAugment is used, which is known to improve the speech to text performance\n\nc) Duration-based  Spectrogram synthesizer is used. Specifically, architecture and hyperparameters similar to Non-Attentive Tacotron spectrogram synthesizer is used, which is known to improve the robustness issue in Tacotron2 \n\nd) To retain speakers’ voices across translation, Translatotron2 is trained on parallel utterances with the same speaker’s voice on both sides. This enables the trained model to restrict generation to only the source speaker’s voice, mitigating the risk of potential misuse for creating spoofing audio artifacts. Parallel utterances are synthesized using a TTS model with cross lingual voice transfer capacity.\n\nIn addition, authors also trained Translatotron 2 with examples that contain two speakers’ voices in both the source and the target. The experiments suggest that Translatotron 2 has capability to retain voice identity when the input contains speaker turns.  For multilingual experiments, Translatotron 2 was trained with 4 high resource languages  and BLEU scores suggest Translatotron2 outperforms Translatotron in multilingual settings as well.\n\n",
            "main_review": "Strength:\n\n1) With the use of Conformer encoder with Spec Augment and giving auxiliary target phoneme decoder output as input to the spectrogram synthesizer , the results in Table2 indicate the significant improvement in transcription quality\n\n2) The voice retention capability of Translatotron 2 is significant\n\n3) The ability of Translatotron2 to retain voice in case of speaker turns without additional diarization systems is interesting\n\n4) The multilingual capability of Translatotron 2 is significant\n\nWeakness:\n\n1) While results are impressive, however, novelty is limited. For example: a) Conformer encoder and SpecAug are known to improve ASR performance; b) Using auxiliary target phoneme decoder output as input to the spectrogram synthesizer makes the architecture similar to cascade system; c) Robustness issue of Translatotron is addressed here by using Non-Attentive Tacotron spectrogram synthesizer \n\n2) In both Table 2 and Table 3,  two results for the Translatotron model are shared, one of which is reported from the original paper. For the second results that are reported, it appears that the authors trained their own Translatotron model for the experiments, however, this should be stated explicitly in the paper \n\n3) In Section 4.2, pause duration used to concatenate two consecutive speakers is missing in the description of  ConcatAug method. This information will be useful to replicate the experiments. Besides, it will justify the rationale of using 1.6 sec leading and trailing segments to measure voice retention in speaker turn experiments.\n",
            "summary_of_the_review": "The paper is well written.  The experiment results of  improvement in  translation quality , naturalness and robustness are convincing. The voice retention capability in the speaker turn setting of Translatotron 2 is appealing.  However, the overall  paper’s novelty is limited mainly because the paper brings in the best systems available in literature together to improve speech-to-speech translation systems performance. Thus, the paper might be more appropriate for a speech technology focused conference. \n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper describes an updated version of Translatron which purports to be a speech to speech translation system trained end-to-end.  The new system now includes the ability to control the target voice to match the input voice without providing an explicit speaker id vector and the translation performance of the updated system now comes close to that of a cascade speech to text + TTS system, although it does not match or improve on it.  \n",
            "main_review": "Please list both the strengths and weaknesses of the paper. When discussing weaknesses, please provide concrete, actionable feedback on the paper.\n\nThis paper describes an updated version of Translatron which purports to be a speech to speech translation system trained end-to-end.  The claimed benefits include the ability to use with languages without a written form and reduced error-compounding relative to a cascade system.  However, I find no support for these claims in the paper.  The system is not strictly end-to-end since it relies on an auxiliary loss function based on a phoneme transcription and nowhere does it explain where this transcription comes from (the original Translatotron paper is also quiet about this).   Since the experimental data was derived from text corpora my guess is that the phoneme transcription used in the experiments was generated by a standard G2P system.  Given the difficulty of automatic phone transcription, its not at all clear how you could in practice realise the claimed benefit of using with languages with only speech as training data and no text (or well-trained ASR system)  \n\nThe results presented for translation quality come close to those achieved by an ST+TTS cascade system but do not match it.  Given that deriving an accurate phoneme transcription is not much easier than deriving a text transcription, I fail to see the benefits of this system.  \n\nAlso the so-called Conversational dataset is mis-leading because this is actually crowd-sourced speakers reading conversational texts.  Read speech is quite different from spontaneous conversational speech. \n\nThe architectural changes between Translatotron and Translatotron 2 are described but not fully justified.  The paper contains various evaluations of voice quality and similarity but does not provide any ablation studies to show why the new architecture is better than the old and the relative contribution of each change. \n\nOne particular difficulty is understanding the BLEU results since this is measured automatically by applying ASR to the output.  I presume the reason why improving the output synthesis quality improves the BLEU score is not because the underlying translation is any better, but simply because the ASR made fewer errors?\n\nThe authors include an augmentation option in training to include more than one speaker turn with the goal of getting the target speech to track the changed input speech instead of being an average of the two.  The results show that without the augmentation, the target voices are indistinguishable, whereas with it there is a clear differentiation although the absolute similarities are quite poor.\n\nOne of the many difficulties that i had in understanding this paper was the lack detail on the training data and how the results would apply to a real-world use-case.  Firstly, I cant find any information on the relative sizes of the train and test sets.  Are speakers in the test set included in the training set - ie is this evaluated as a speaker independent translation system or a speaker dependent system.  For the speaker dependent case,  if I wanted to train a Translatotron for a single voice, how much speech data would I need?  \n\nOverall, there is much interesting stuff in this paper, but far too many unanswered questions.\n",
            "summary_of_the_review": "The paper describes an incremental improvement over a previous paper on the same topic.  However, the experimental details are not adequately described and the claimed benefits relative to a cascade system are not justified.\n\nI remain somewhat sceptical of the practical utility of the overall framework.  Nevertheless, following the authors response to questions and updated paper, I have increased my score to \"marginally above\" the threshold.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}