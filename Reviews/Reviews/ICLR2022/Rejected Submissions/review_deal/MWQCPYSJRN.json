{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper suggests a new technique to utilize generative replay for continual learning. Specifically, the authors claim that even though the generated samples are imperfect (thus cannot be used as positive samples for old classes), they can still be used as negative samples for the current class. 3 reviewers are negative and 1 reviewer is positive. The main concerns of negative reviewers are (a) non-ablated effects of baseline and proposed components, (b) insufficient analysis of negative replay, and (c) no assessment of generated data quality. The rebuttal provides an additional experiment to address the issue (a), but the reviewers and AC think the experiments should be better polished. Also, AC believes the issues (b) and (c) should be better analyzed. The rebuttal claims that issue (c) is not applicable as they generate samples on the latent space. However, the main motivation of the paper is the low quality of generated samples, and the paper should provide a quality measure to support their claim. For example, an update of the feature extractor may move the latent space generative replay to the wrong class (i.e., low quality), and thus one should not use it as positive but only as negative, as suggested in this paper. Here, the negative replay would increase the margin of current and old classes, enhancing the accuracy of the current class. To analyze the source of benefits (old vs. current classes), the authors could report the task-wise accuracy trends, not only the overall accuracy. It would be a nice addition to the issue (b). Due to these unresolved concerns, AC tends to recommend rejection."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The key idea proposed in this paper is to use generative replay as negative samples to improve performance in class-incremental scenarios of continual learning. The authors argue that samples from a generative model have a lot of artefacts due to challenges in training/adapting a low resource generative model. Hence, its use as a positive sample for future experiences fails. But, these imperfect samples can still be relied on as negative samples for classes being trained in the current experience to prevent \"learning in isolation\" problems. The proposed approach is compared to positive replay, negative replay and no-replay baselines for two complex datasets in both NC and NIC scenarios.",
            "main_review": "### **Strengths**\n- The key idea of using generative patterns/samples only as negative samples is simple, yet effective. The weight updates by the generated imperfect samples can be derogatory for the classes learnt in the past experiences. But these samples are still important for the new classes being learnt in the current experience due to the \"learning in isolation problem\".\n\n- Empirically, this approach is found to significantly outperform both positive generative replay and no-replay paradigms. Also, when compared to the PR-OD (Positive replay with original dataset), which is an upper bound for continual learning with replay approaches, the generative negative replay comes reasonably close in performance\n\n- The datasets used in this paper such as ImageNet-1000 indicate that this approach could be scalable to more complex or real-world scenarios of continual learning for classification.\n\n- The paper is generally well written and is an easy read.\n\n### **Weaknesses** and **Suggestions**\n- One of my concerns with that approach is that splitting the model into classification and feature extraction heads adds another hyperparameter. How does one decide where the split will be? Readers might want an answer to this question before using this approach.\n- It is a little surprising to see the accuracy on the ImageNet-1000 decreasing with the number of experiences, while the accuracy of CORe50 increases with the number of experiences. Is it because they are calculated differently? Maybe for CORe50, the accuracy is calculated across all classes even though they haven't been trained yet, while this is not the case for ImageNet-1000.\n  - If yes, it would be better to use the same way to calculate the accuracy, otherwise it is quite confusing\n  - If no, then it would be interesting to understand why this is happening?\n\n- Is PR-OD the same as Cumulative scores (Pelligrino et. al. 2020)? One of the goals of continual learning is to learn models (that are incrementally exposed to all the training data) that are comparable to a model that was trained on the full dataset. Hence, instead of PR-OD cumulative scores would probably be a better upper bound. \n- For experiments on CORe50, the MobileNetV1 was first pre-trained on ImageNet-1000 (to probably get a good feature extractor). But what about experiments on ImageNet1000; was the ResNet-18 pretrained? If yes, on which dataset? \n  - Another question that could be raised here is the choice of pretrained model as the feature extractor. Does it matter how the pre-trained model was trained? For example, if an unsupervised/self-supervised approach is used that uses negative examples and learns linearly separable embeddings, will it reduce the need for generative negative replay for continual learning?\n- The definition of the generative model $g_\\Omega$ is slightly confusing. In eq 5, the input to $g_\\omega$ is $z$, while in equation 6 the input now becomes a sample from the input space $\\mathcal{X}$.\n\n### Typos\n- Table 1: PR-OD instead of PR-OG\n",
            "summary_of_the_review": "Overall the key idea proposed in the paper is sound and reasonably well justified with experiments. That being said, I have some concerns regarding some of the design choices and is the approach robust to these design choices. Hence, I have given a borderline rating.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In Continual Learning (CL) scenarios, storing data from previous experience is helpful to mitigate catastrophic forgetting. However, privacy issues or storage overhead makes replay methods impractical. Aware of this problem, generative models have been proposed in previous work to generate data that represent previous experiences. Still, these methods suffer from low performance because of the continual training of the generating model and/or the generation of high dimensionality data. In this paper, the authors propose Generative Negative Replay. Instead of using the generated data as a positive example of previous classes, they used it as a negative example for the classes present in the current experience. While training with the negative examples, the authors propose to freeze the weights of the classification head corresponding to the previous classes, similar to what happens in CWR. This solution is particularly relevant in situations where there are not as many classes from experience.",
            "main_review": "Strengths:\n- Using generated data as only negative examples is interesting. It reminds me of SVM Examples [1], where they train N binary classifiers (where N is the amount of data). To train each classifier, this method uses only 1 positive example and the others as negative examples. In Exemplar SVM, the model learns very well what not to classify. I understand that the idea of Generated Negative Examples is different, but maybe it could be helpful.\n- I find it an ingenious idea to use CWR to train the sorting head. The model only focuses on making controlled changes, ignoring changes that can generate unwanted noise.\n- I liked how the paper is structured, introducing the current problem of methods that use generated inputs. Then, how this can be alleviated by using negative replay as negative examples.\n\nComments and Questions to the authors:\n- Acknowledging that one of the problems for not using data from past experiences is data privacy. Instead of generating negative examples, you could use the same proposed technique but actual data (extracted from other parts without security problems). Since you only train the columns of the classifier of current experience, there is no supervision needed. Maybe this brings the results a little closer to the real examples, but without security problems?\n- On page 5, just below Eq 7. What do you mean by “patterns” is similar to a data point?\n- In Figure 4, why do you think that after the 350 experience, the accuracy of the PR-OD starts to drop, but using NR-GD it remains stable? Do you think it could be due to the poor representation of the data from past experiences during the training? Is this drop due to the number of items from past experiences in the batch?\n- How does the proportion of elements generated in the batch affect it? I imagine that if you place fewer elements, the performance drops, but how sensitive is it to this number?\n- I understand that this method only works when we have little data in each experience. This is since there would not be enough data to correctly train the columns of the classifier of the classes that are not present in the experience. Could this technique be used in other scenarios? Not necessarily when we have few classes from experience. Perhaps using techniques similar to ACL to train the adversarial model?\n\nTypos:\n- Table 1 and 2, said NR-OG. I think it should have said NR-OD\n\n[1] Malisiewicz, Tomasz, Abhinav Gupta, and Alexei A. Efros. \"Ensemble of exemplar-svms for object detection and beyond.\" 2011 International conference on computer vision. IEEE, 2011.",
            "summary_of_the_review": "The authors do not present a new idea but a very ingenious solution for a recurring problem. Having little data to train the tasks does not impede training well if you have a little imagination. By generating data that cannot be used directly as positive examples (because of the limitations of generation methods), they use this data as negative examples for other tasks. Although I still doubt if this can be extended to other scenarios, for example, when tasks are more similar or different. I think the authors present surprising results.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a novel learning scheme for generative replay methods in continual learning. Different from the original generative replay method, the generated examples are treated as negative examples for new classes. Experimental results on CORe50 and ImageNet-1000 demonstrate the effectiveness of the proposed method.",
            "main_review": "Strengths:\n1. The paper is well-written and states the problem setting quite well.\n2. Using generated examples as negative examples is interesting and novel (for generative replay based continual learning methods).\n\nWeakness:\n1. The main method is based on AR1, is there a particular reason of using this AR1? Could the method still work well under the case that only generative replay is allowed (no additional components as in AR1)?\n2. To my understanding, the main purpose of the paper is to learn new data better, instead of reducing forgetting on the old data. However, according to Figure 2 (c), the gradient will also flows back to the feature extractor (when it is not frozen). This may leads to forgetting issue, when no other techniques (Synaptic Intelligence or Experience Replay using real data) are applied. \n3. There are actually many tricks for learning a better classifier in the continual learning literature, see Labels Trick [1], Separated Softmax [2], etc., which also works without any replay techniques. It would be idea to compare with those methods.\n4. (Minor) The novelty and practical usage of the method is limited by the fact that a generative model is required.\n\n[1] Zeno et al. Task agnostic continual learning using online variational bayes. Arxiv 2018.\n[2] Ahn et al.  A simple class decision balancing for incremental learning. Arxiv 2020.",
            "summary_of_the_review": "The idea is interesting, but the novelty is limited, and some details and comparison experiments are missing.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a method to make generative replay for continual more effective even if generated data quality is not perfect. The method uses replayed data only as negative samples for current tasks and not as training samples to remember.\nThey apply their method on one model on Core50 and ImageNet data.\nThe replay process is realized in the latent space.",
            "main_review": "\nThe paper presents an interesting method that could improve how to use replay in continual learning.\nThere is a high amount of experiments with various baselines.\nUnfortunately, the baselines are not run to estimate the interest of the replay strategy but to select only one model that will use the replay strategy.\nIt would make more sense to run the different baseline approach with and without the replay strategy to estimate the interest of the proposed method.\n\nThe authors beat the state of the art in ImageNet1000 and Core50 with a combination of their method and a model inspired by several papers.\nBeating state of the art is good; however, it does not provide a good evaluation of the replay strategy.\nThe sota experiments would be interesting as a conclusion experiment after evaluating negative replay on several baseline strategies.\n\n\nMasking past output neurons while applying backward pass for generative replay is new, however, it has already been experimented in continual learning papers in other contexts such as \"Continual Learning in Deep Networks: an Analysis of the Last Layer\" Lesort et al. \nThe related works should also mention how the proposed masking strategy is related to existing works.\n\n\nOther Remarks:\n\n- the approach is designed to make low-quality replayed data useful. However, no assessment of the replay quality is presented. The authors compare the results of replay with generated data, real data, and random data without actually evaluating the data quality.\n- the approach for Core50 and Imagenet is not the same (one pretrained feature extractor and one feature extractor trained only on task 1), making it difficult to assess the results.\n- the paper should be clear from the beginning that the replay process is realized in the latent space.\n- in conclusion : \"using negative replay largely improves\" is maybe a bit over enthusiastic (+2.95% for Core50, +0.83% Imagenet)\n- Table 3: The accuracy reported from [Masana et al] is not EWC but from EWC-E, which is a version of EWC with Replay. \n- Figure 2: The mask should appear more clearly and annotate into the figure. Why the backward pass of new samples goes to past classes? New data does not bring any information about past data.\n- The related works section presents a bibliography of generative replay but do not provide perspective on the link between the presented approach and the bibliography. The context described in the related works section should be linked to the paper approach.",
            "summary_of_the_review": "Negative Replay looks like a promising approach to continual learning with approximate generative models. The papers propose interesting results with several baselines.\nHowever, the experiments are not designed in a convincing setup to evaluate negative replay. The evaluation of negative replay would benefit to be applied on several classical approaches before being applied on a specific/complex training procedure.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}