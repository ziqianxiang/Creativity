{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper proposes to interpret point cloud data in Euclidean space as samples from some underlying probability distribution. Thus a set of point cloud data can be given the structure of a statistical manifold with a Riemannian metric structure, namely the Fisher Information Metric.\nApplications to point cloud autoencoders are then studied.\n\nReviewers generally agree that the idea of equipping point cloud data with the Fisher information metric is interesting and has potential. However, there are concerns that the theoretical properties of the proposed framework have not been explored in depth. Furthermore, the experimental work should be enhanced to demonstrate the practical utility of the proposed formulation."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes to represent point cloud data as samples drawn from statistical distribution, and project the data to an underlying statistical manifold on which various analyses can be conducted in a manner that is more favorable than doing so in a regular Euclidean space. \n\nThe authors thoroughly describe how points are converted into samples from a probability density function, and explain the metric that determines the coordinate system on the statistical manifold, which describe the changes to the distribution of point clouds. \nTo introduce this metric to represent point clouds, the authors propose to use this to find minimal geodesic curve between two data in a latent space. They also propose to add a novel constraint to the error metric of autoencoders, so that the encoded latent space possesses the Fisher information metric.\n\nThe effect of this proposal is firstly analyzed by two synthetic experiments. First, the authors show that using the proposal, they are able to find the geodesic curve along the manifold that consists of data with the same label. Then, they demonstrate that the latent space of the constrained autoencoder successfully captures the Riemann metric, as samples of many classes are placed along a single line, or are cleanly separated. \nFinally, the authors transfer the learnt representation between dataset to demonstrate that the proposed metric space is suitable for representation of various point cloud data.",
            "main_review": "Strength: \nI personally find that incorporating the proposed constraint to achieve Fisher information metric in the underlying latent space is a very interesting idea.\n\nAs the authors proposed, the underlying latent space created by the autoencoder does seem to be endowed with the desired metric. The experiments regarding the decision boundary is also interesting, as the data seems to be cleanly separated. \n\nThe task of point cloud classification does not really involve usage of pointwise correspondence, therefore may have worked to the advantage of the authors' proposal.\n\nWeakness:\nThe contribution of the paper is very limited, as it lacks a thorough review of the geometric/statistical representation of point clouds in other fields.\nThe description of representing points as a distribution is accurate, yet has already been explored in registration. The proposal in section 3 itself lacks novelty in that regard.\n\nIn the field of image/point cloud registration, there has already been decades of discussion regarding the usage of geometric and statistical representation, and the advantages of using Euclidean metric and Fisher information metric to compare the sets of points.\nIn the case of point set registration, it is widely known that geometric methods excel at cases where outliers are included, while statistical methods are robust to noise.\n\nSuch discussion has been completely omitted in this proposal, which I find disappointing.\nHere are some of the prior works, but there are many more:\n\nH. Chui and A. Rangarajan. \"A new point matching algorithm for non-rigid registration,\" CVIU 2003\n\nA. Myronenko and X. Song, “Point set registration: Coherent point drift,” TPAMI 2010\n\nB. Jian and B. Vemuri, \"Robust point set registration using gaussian mixture models,\" TPAMI 2010\n\nZ. Zhou et al. “Robust non-rigid point set registration using student’s-t mixture model,” PloS one, 2014\n\nThere has also been discussion regarding merging the two approaches:\n\nF. Li et al, \"Toward a unified framework for point set registration,\" ICRA 2021\n\nThe novelty of the paper lies in section 4, which I did find interesting. However, as it lacks a thorough review, the contribution of this section is insufficient. \nFor example, as stated above, each method is known to have some advantages and disadvantages in registration. I wonder if, in this proposal, the same can be said of cases under a different setting, such as point clouds with added noise, occlusion, etc.\nThe overall novelty lies only in the proposal of embedding the metric in the latent space, therefore thorough analysis is definitely necessary for its contribution to be sufficient.",
            "summary_of_the_review": "Overall, I find the paper to be lacking background analysis, and thus also in novelty. \n\nThe autoencoder with the Fisher information metric as the metric of the latent space is an intriguing proposal. However, as this is the only contribution of the paper, the authors are required to thoroughly compare the geometric and statistical approaches. There has been decades of research that analyzed the advantages of the two representations, especially in the field of image and point set registration. Therefore, such knowledge had to be used in the analysis. As the paper lacks such analysis, I have to lean towards rejection.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper concerns data sets of point clouds in Euclidean space and proposes to analyze those as samples from underlying probability distributions. Each point cloud being represented by a probability distribution can now be seen as a point in a statistical manifold on which the Fisher information metric provides a natural Riemannian structure. The authors presents two experiments in this setup where they train autoencoders to represent point clouds, interpolate between point clouds using geodesics, and where the latent representation is trained so that straight line are close to geodesics.",
            "main_review": "The idea of representing point clouds as probability distributions and considering those as points in a statistical manifold equipped with the Firsher information metric seems good and well-founded. I am not aware of any previous work that has exactly this take on point cloud analysis.\n\nOverall, I find the idea novel, the presentation clear and interesting.\n\nSome points that the authors can consider:\n\n- a weakness of the method is the estimation of the density. The result is of course very dependent on the chosen kernel density estimator and parameters of the kernels. As far as I could read, the authors use fixed values for the bandwidth without justification for the chosen values. A more comprehensive discussion of the effect of the density estimator and e.g. the bandwidth parameter would strengthen the paper.\n\n- I was at the first read confused about the statement that the mapping h:X\\to S being 1-1, and subsequently that the representation is invariant to permutations. I believe the manifold \\mathcal X can equivalently be considered the set of equivalence classes of point sets under the permutation group so that permutation invariance comes by construction, but perhaps this could be described more explicitly around Proposition 1.\n\n- Proposition 3: Isn't the linear independence assumption satisfied on all reproducing kernel Hilbert spaces? Perhaps the result can be stated much more generally.",
            "summary_of_the_review": "I believe the paper presents a novel take on analysis of point clouds.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper studies 3D point cloud data. The core message is that one can regard a point cloud as a collection  of samples from an underlying distribution and, hence, one can imagine a manifold of point clouds and construct an associated distance metric via the Fisher information. That distance can then be used for certain manipulation and analysis tasks, e.g., to interpolate (\"morph\") between point clouds along the manifold, or to regularise representation learning. A central assumption of the paper is that it is easier to choose a meaningful probability distribution than to make other a-priori assumptions \"ad hoc\" about the point cloud. However it is not discussed how to actually choose a suitable distribution, the paper limits itself to Gaussian kernel density (seemingly also \"ad hoc\" for mathematical convenience).\n",
            "main_review": "Strengths:\n- the proposed framework has potential, it may indeed sometimes (although certainly not in general) be useful to think about point clouds in this way\n- the framework definitely has a certain mathematical elegance\n- experiments in toy data like parametric primitives, Shapenet and Modelnet indicate that for \"perfect\" point clouds the proposed kernel density model is a valid regulariser\n- actually the most promising bit of the proposed view for me: an interesting way to construct a regulariser for (global) point cloud representations, although the paper only scratches the surface and does not deeply explore that aspect\n\nWeaknesses:\n- the work is in my view interesting, but not as \"rigorous\" and momentous as claimed; actually, behind the information-theoretic smoke screen it largely boils down to the popular and obvious standard assumption that a point cloud is a set of samples from an underlying surface, perturbed by (with the proposed kernel, Gaussian) random noise\n- the view as a distribution may be useful in some situations - typically when requiring a \"rough, global\" descriptor of the point cloud, for instance to recognise well-discriminated shape categories; it is almost certainly not a suitable view for many other (arguably more realistic and relevant) tasks - for instance the kernel smoothing will harm any local feature extraction or description; and it will likely also be challenged by point clouds that cover the object of interest incompletely or with varying density - in practice any point cloud that has been sensed, rather than rendered synthetically from a surface model (i.e., from an \"explicit version of the distribution\")\n- nothing is said to substantiate the claim that choosing the distribution is easier than making other reasonable assumptions; I am not sure how to make a reasonable choice for realistic distributions (say, autonomous vehicle Lidar)\n- the kernel density flavour suggests that the method might not scale all that well to realistic point cloud sizes. Perhaps it does, but the experimental setting with <2100 points per object evades the topic\n\nComment (neutral, I would not want to call this a strength or a weakness):\n- the paper sits in a strange place between a theory paper and a practical, algorithmic machine learning paper: a theory paper can be very valuable, but for my taste the theoretical \"contribution\" of the paper is fairly basic, it would have been nice to push further and see what more advanced theoretical analyses and conclusions the point cloud manifold view leads to; on the other hand, the practical \"evidence\" is limited to last-decade toy point clouds - nicely discriminative objects without any background, sampled with perfectly regular, even coverage and low, isotropic noise. If one claims practical utility, then one should nowadays really show at least \"semi-real\" point clouds (e.g., Scannet, semantic3d.net, KITTI, ...)\n",
            "summary_of_the_review": "A rather theoretical paper, the proposed view is in some sense elegant, but not as revolutionary as claimed. Interesting theoretical or practical developments could perhaps build on the proposed view, although the paper does not push far enough in any directions to get to a real impact. Still, I have no objections against putting the idea out to the community, if the authors tone down their claims and acknowledge that they are throwing out a cute, raw idea; while neither exploring the theoretical implications in much depth nor engineering it into a strong algorithmic tool.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes treating the point clouds as probability distributions and equipping them with a Fisher Information Metric, providing a Riemannian geometric structure that can be used as regularization in AutoEncoders for training and latent interpolation. The experiments are performed on ModelNet and a toy dataset of toy shapes (cones, cylinders, ellipsoids). ",
            "main_review": "===STRENGTHS===\n\nNOVELTY\n\nThe proposed approach seems novel, and I am not aware of other works that propose a similar approach to equip a point cloud of a Fisher Information Metric. \n\nCLARITY\n\nThe paper is clear, and I had no trouble understanding it. The only parts a bit obscure to me are the paragraphs of Section 4; I would suggest giving the applications a bit more space. For example: how is implemented and solved the optimization problem of \"Geodesic Interpolation\"?\n\nSOUNDNESS AND REPLICABILITY\n\nBy my understanding, the method seems sound; while I have not performed an in-depth check, the passages seem reasonable to me. The appendix provides information to help the replicability of the results, explaining the implementation details.\n\n\n===WEAKNESSES===\n\nEXPERIMENTS\n\nMy major concern about this paper is the experimental setting for the following reasons:\\\na) Quantitative results are provided only on one experiment of shape classification; in this experiment, the comparisons with other methods are \"adopted from references\", and hence they are not directly comparable. I am convinced that the proposed method work in a similar level of performance, but at the moment, some sentences seem over-claims not well-grounded (e.g., \"the classification accuracy of our method is higher in most cases\"). The improvement over the vanilla baselines seems tight; can you provide some elaboration on the \"best\" cases, i.e., in which cases using the regularization produce the greatest improvement?\\\nConcretely: Given the tight margin between the proposed method and the ones taken from other papers, I think that it would be important to reproduce at least two SOTA methods (e.g., PointCaps and Multi-Task) in the exact same setting.\\\nb) The toy dataset is really interesting from the point of analysis, but only qualitative examples are provided. I would also suggest providing some quantitative results in this case that are highly controllable and naturally in correspondence, providing visualization of the reconstruction error. This would give insight into the proposed metric's behavior on different geometrical features (e.g., different curvature and sharpness).\\\nc) Only rigid objects have been shown, all with the same distributions, while one of the paper's claims is that defining probability distribution is \"likely quite natural and intuitive to the user\". I would suggest proposing at least some qualitative examples on different domains and different probability distributions. Are there issues in considering non-rigid objects? Which are other reasonable choices for probability distributions that respect the given assumptions?\n\nCONTEXT\n\nThe paper does not offer a proper context around the work. I would suggest providing at least two sections for related works: one on point clouds as a probability distribution (as stated in the text, this is not a novel idea), and one for defining metrics for point clouds and their latent representations (examples are [1] and [2]). I think that the correct positioning of the paper in the literature would help to clarify the novelty of the work and what it is actually proposing.\n\n\n===MINOR ISSUES AND QUESTIONS===\n\n1) the \"info-Riemannian\" name is introduced twice (page 2 and page 4)\n2) to me it is not completely obvious the term \"velocity\" on page 4; could you give me a better intuition\\definition of it?\n3) I would suggest coloring the point clouds of Figure 4 with a non-constant color to better track the movement of the points; I see the overall structure is better \"preserved\" in IR metric, but I would better understand their movements.\n4) the computational aspect seems untouched; can you provide some timings of your computation, e.g., comparing it with the geodesic metric computation over a mesh? \n\n[1]: Limp: Learning latent shape representations with metric preservation priors, Cosmo et al., 2020\n\n[2]: Point-set Distances for Learning Representations of 3D Point Clouds, Nguyen et al., 2021",
            "summary_of_the_review": "I like the overall idea, and some qualitative results seem promising. However, some aspects require better experimentation and remain a bit obscure, particularly the effectiveness of the method, the positioning among other SOTA approaches, and the behavior in different conditions (e.g., different domains, probability distribution). ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}