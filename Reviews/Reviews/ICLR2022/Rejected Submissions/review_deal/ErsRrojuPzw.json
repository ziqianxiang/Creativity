{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper exposes a method to reduce the training cost of once-for-all networks.\nOverall this paper is well written and easy to follow, and the experimental section shows a clear reduction of training time on the examples used.\nHowever, the reviewers point out that the experimental section could benefit from adding more design spaces, and have a better explanation of the results. More importantly, three out of four reviewers agree that the novelty of this work is too low for the submission to be accepted, with the fourth one only giving a score of 6 (and also noting the lack of novelty). I therefore recommend reject for this paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This work proposes new methods of Once-for-all(OFA) network training methods, which significantly reduce the training time as compared to the existing OFA methods. Specifically, this work:\n- Eliminate the need of pretraining the teacher network with in-place distillation\n- Develop an upper-attentive sampling method which always sample the full-sized sub-net at each iteration to help co-train the rest sub-networks\n- Use an upper-attentive warmup technique which trains only the full sized sub-net for a few epochs before co-training to further improve the performance\n\nWith these optimizations, this work can decrease the number of sampled sub-networks in each iteration of training, further reducing the total training cost.\n",
            "main_review": "Strength:\n- Overall this paper is well written and easy to follow.\n- The reduction in training time is indeed decent\n\nWeakness:\n- Limited novelty.\n- Lacking the deeper explanation of the improved results.\n",
            "summary_of_the_review": "- **Limited novelty**.\nIt seems to me the two major contributions in-place distillation and upper-attentive sampling are already proposed by the existing works. It is a little unclear what further modifications the authors did other than stitching these two techniques together. Would be great if the authors can give some stress here considering the improvements in the results.\n\n- **Lacking the deeper explanation of the improved results.** \nIn the results part, it seems to me that the proposed methods just magically improve as compared to on the baselines. Not clear the underlying reasons and insights for them. Would be nice for the authors to point out as I might miss them.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This manuscript aims to reduce the training cost of once-for-all networks. The proposed method is built upon CompOFA that reduces the number of sub-networks within the OFA network to 243 by only considering networks whose dimensions are coupled. This paper introduces several techniques to further reduce the training cost, including in-place distillation, upper-attentive sampling, and upper-attentive warmup.",
            "main_review": "Strengths\n1. The training cost saving is clear and significant while maintaining the same level of accuracy as OFA and CompOFA.\n2. The proposed method can potentially benefit real-world deep learning applications. \n\nWeaknesses:\n[major]\n1. Limited novelty. Although this paper presents good results, the novelty of the proposed method is a bit weak. For example, in-place distillation is not new. Upper-attentive sampling and upper-attentive warmup are more like engineering tricks. \n2. This paper only presents results on one design space (mobilenetv3). But, in practice, MobileNetV3 is not supported or not efficient in many cases (e.g., on FPGA, GPU, etc). Showing the generalizability of the proposed method on other design spaces (e.g., MobileNetV2) is important. \n\n[minor]\n1. On ImageNet, it seems that only supporting 243 sub-networks is sufficient for NAS. But it is unclear whether this strategy also works under the transfer learning setting. \n",
            "summary_of_the_review": "This paper presents several simple methods to reduce the training cost of once-for-all networks, showing clear and significant training cost saving without losing accuracy. But, I find the novelty of the proposed method is a bit weak, and the generalizability of the method on other design spaces and transfer learning is unclear. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this manuscript, the authors propose a new framework for training the once-for-all (OFA) networks. This framework uses an in-place distillation schema to replace the teacher training in the former OFA training fashion and makes the overall training time faster. The paper is overall well-written with good performance.",
            "main_review": "In this manuscript, the authors propose a new framework for training the once-for-all (OFA) networks. This framework uses an in-place distillation schema to replace the teacher training in the former OFA training fashion and makes the overall training time faster. The paper is overall well-written with good performance.\n\nMy concerns are as follows.\n\n- The novelty for this paper is limited: this paper claims four contributions they made, but the idea for training without the teacher network borrowed from the in-place distillation paper without much modification. The upper-attentive schema simply samples the biggest network in each phase, which can be regarded as \"merging“ the training for \"teacher network\" and \"elastic network\" in one phase, but the author does not show the performance boost for the schema (compared to without using upper-attentive).\n- The OFA aims to alleviate search cost across different hardware platforms, but the comparison in the experiment only include little platform. The author should prepare more experiments across different platforms (mobile, CPU, GPU, FPGA, etc...).\n- It is not clear how each component in this paper contributes to the final performance for the reduction of training time, the author should do an ablation study to justify this.\n\nI will raise my vote if the concerns are addressed.",
            "summary_of_the_review": "The paper is overall well-written, but the novelty for this paper is kind of limited. The author should prepare more experiments to justify the performance and address the technical contribution for this paper.\n\nI will raise my vote if the concerns are addressed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not applied.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed a method to train a once-for-all network, where one network can run at different resource constraints. The method is based on previous methods, and the author further improved the training speed by around 1.5x - 1.8x without loss of performance. The method is evaluated on ImageNet classification.",
            "main_review": "Strength:\n1. The proposed method improved the training speed without loss of accuracy.\n2. The paper is easy to follow.\n\nWeakness:\n1. The proposed method is of very limited novelty and technical contribution. Looking at the contribution part, (1) in-place distillation is used in Slimmable Network. (2) The upper-attentive sampling is basically Sand-wich rule in Slimmable Network, the only difference is that the author doesn’t sample the smallest sub-network. (3) The warm-up training is also a very incremental and commonly-used technique. The proposed method is basically a combination of previous techniques.\n2. The results seem to be not consistent with that reported in the paper. For example, in Table 3, the performance of OFA seems to be lower than that reported in the original paper.\n3. Some missing related works: \\\n[1] NestedNet: Learning Nested Sparse Structures in Deep Neural Networks \\\n[2] MutualNet: Adaptive ConvNet via Mutual Learning from Network Width and Resolution \\\n[3] Dynamic Slimmable Network \n",
            "summary_of_the_review": "Overall, I think the paper has limited technical contribution, and the training speedup is not surprising (less than 2x). If it is a combination of techniques but significantly improved the training speed (say 5x-10x), which makes such network training easily-doable for everyone, then I would say it is a large contribution. Currently, I think the author need to further improve their method.\n\n======== Post Rebuttal ========\n\nThank the author for the response. My concern about the novelty and significance of the method and results still remain. I agree that combining existing techniques may need more tuning. But this is hard to be considered as enough contribution, especially given that the performance is not significantly improved. Therefore, I will keep my score.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}