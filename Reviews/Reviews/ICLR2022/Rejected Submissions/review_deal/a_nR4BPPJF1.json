{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper aims to explain the pretraining effectiveness of masked language model, based on the concept of  diversity of classes. They empirically study how a diversity regularizer, based on this theory can improve model performance, as an empirical support.\n\nBefore rebuttal, reviewers consistently found the empirical study rather preliminary, while authors, through rebuttals, argue the theoretical study should be highlighted as their main contribution, and expressed concerns that the lack of empirical rigor should not be a ground to reject. We agree with these concerns, but rebuttals and discussions failed to convince reviewers that assumptions and evaluations are proper for connecting the proposed theory to potential impacts in pretrained language model scenarios. Revising to make this connection clearer would address the reviewer disagreements in the future."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors present a new statistical analysis aiming to explain the success of (masked) language model pretraining for NLP. Specifically, they focus on the *diversity of classes*, which they claim is similar to the *diversity of tasks* for multi-task pretraining (e.g., Tripuraneni et al. (2020)). Based on their theory, the authors then propose a *diversity regularizer* to improve model performance.\n",
            "main_review": "This paper presents a new statistical analysis aiming to explain the success of pretraining for NLP. The theory the authors present focuses on the *diversity of classes*, parallel to the *diversity of tasks* for multi-task pretraining (e.g., Tripuraneni et al. (2020)). The authors then propose a *diversity regularizer*, which they evaluate in empirical experiments, with the goal to improve model performance.\n\nStrengths: \n- The authors propose a theory aiming at explaining the success of pretraining, which is something not frequently explored systematically in NLP. Theoretical insights into the reasons behind the benefits of pretraining are (at least in my opinion) strongly needed.\n- The authors aim at drawing practical conclusions from their theory by introducing the *diversity regularizer*.\n\nWeaknesses:\n- It is not directly clear to me that Assumption 1 holds.\n- The empirical results aren't very strong. In fact, I would argue that the author's claim \"Our empirical results show great potential of this diversity regularizer.\" is probably an overstatement.\n\n",
            "summary_of_the_review": "The authors investigate an important topic: the success of pretraining in NLP. Some of the statements in the paper are too strong or unclear to me; I would appreciate more explanation from the authors.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper presents a theoretical view on how diversity of classes can help in language model pre-training, and then backs up with experiments consisting of improvements in the downstream tasks due to introduction of a class diversity loss during pre-training. The paper provides extensive theoretical justification of why diversity in classes - which is diversity in the linear output layer in case of multi-class pre-training - can help in better performance in downstream tasks.",
            "main_review": "## The good\n\nThe paper provides a comprehensive theoretical justification of introducing diversity in the last linear layer of pre-training. The paper follows prior work in establishing the theoretical framework, and then proceeds to define their contribution in establishing a guarantee of an upper bound of transfer learning risk. Then, the paper explains how a diversity parameter controls the lower bound of the downstream fine-tuning. Concretely, the authors propose to tune the class diversity by increasing how spread out the vectors of the last linear layer are. The authors empirically show their method outperforms the baseline in the GLUE benchmark. The setup of the diversity parameter makes sense intuitively. The authors do observe some marginal improvement over the GLUE benchmark, which shows that in general a more diverse classifier in pre-training helps in providing a better initialization for fine-tuning.\n\n\n## The bad\n\nWhile the paper is heavy in the theoretical aspect, it raises few questions on the assumptions made in the setting. Firstly, the assumption that the representation function is fixed after pre-training (Page 4, first paragraph) is not true for the general fine-tuning case where models are typically fine-tuned such that weights of the representation function are also updated. Thus, Assumption 1 (Realizability) does not hold for general settings, that is representation function $h$ is typically not optimal for both pre-training and downstream tasks (after optimal pre-training, downstream tasks typically overwrite and change the representations). I would love to hear an explanation of why this assumption is required in this setting.\n\nThe empirical results, however, are only restricted to BERT which is a Masked Language Model. Since the authors made a general claim of this method improving any pre-training task, it would make sense to investigate an auto-regressive pre-training model (such as BART) in the same setting as well. The diversity factor in last linear layer classification during pre-training might entirely depend on the type of pre-training objectives used. The authors could, on the other hand, note clearly that this theoretical result and experiments are specific to masked language modeling itself, in order to avoid a potentially costly set of experiments.\n\n",
            "summary_of_the_review": "The paper introduces a comprehensive theoretical framework to establish the need of diversity in classes during pre-training. They support their theoretical results with that on BERT fine-tuning on GLUE benchmark. Overall I find the theoretical contributions solid, but I don't know how general it is as we only see experiments in masked language modeling setup. Empirically the results do support the theory somewhat, but I suspect its mostly due to authors investigating an overly saturated benchmark such as GLUE. Additional results on either pre-training or on different fine-tuning tasks should be able to make it a stronger contribution.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper analyzes the effectiveness of pre-trained models on\ndownstream classification task, showing that the \"class diversity\"\n(characterized by the least singular value of the last linear) in\npre-training is important for the target classification tasks after\nfine-tuning. The paper also suggest a diversity regularizer for\nimproving the downstream task efficiency. The paper is mainly\nconcerned with the formal analysis of the effects of class diversity\n(in practically relevant settings). However, a demonstration of the\neffects of proposed diversity regularizer is also demonstrated\nempirically on a well-known benchmark database for 7 multi-class NLP\nclassification tasks.\n",
            "main_review": "Starting with a disclaimer: I could not carefully verify the proofs\nwith the time I can afford for this review.\n\nThe paper investigates a question that is relevant to the wider field \nformally, and the outcome leads to suggestions for practical \nimprovements. Although more empirical evidence would be useful, the\nexperiments provided also support the usefulness of the proposed\nimprovements (hence the effect of diversity).\n\nAs evidenced by the sources cited in the paper, the general idea of\nusefulness of diversity is not novel. I am also not fully convinced \n(by the paper) that 'class diversity' is categorically different than\n'task diversity'. However, in my opinion, the paper has enough \nnew/interesting content that would be beneficial for the community.\n\nExcept above notes, I only have some minor remarks on the format of\nthe paper:\n\n- It would be a good idea to indicate the difference of the reproduced\n  BERT-base results are from the state-of-the-art results from the\n  earlier literature.\n- Many of the citations use wrong form. For example, \"Devlin et al.\n  (2019)\" on page 1 should be in parentheses, while  when citation is \n  used as a name as in \"...  proposed in (Zou & Adams, 2012)\" (p.3),\n  it should not be in parentheses. This is a general issue in most of\n  the citations, a through check is recommended.\n- Footnote marks should be placed after punctuation, and there should\n  not be space before them. Also, I strongly recommend delaying the\n  placement of footnote marks to a phrase/sentence boundary,\n  especially when they are in the neighborhood of mathematical\n  expressions (e.g., like the footnote mark 2, which would be better\n  at the end of the sentences).\n- First paragraph of 4.2: \"F^pre and F^down are *sets of* linear \n  functions\"?\n- First paragraph of 5: \"...  capability,in ..\" space is missing after\n  comma \n- References has some case (normalization) issues, e.g., \"rademacher\", \"bert\"\n  (not a complete list, a careful check is recommended)\n",
            "summary_of_the_review": "The paper investigates a question that is relevant to the wider field \nformally, and the outcome leads to suggestions for practical \nimprovements. Although more empirical evidence would be useful, the\nexperiments provided also support the usefulness of the proposed\nimprovements (hence the effect of diversity).\n\nAs evidenced by the sources cited in the paper, the general idea of\nusefulness of diversity is not novel. I am also not fully convinced \n(by the paper) that 'class diversity' is categorically different than\n'task diversity'. However, in my opinion, the paper has enough \nnew/interesting content that would be beneficial for the community.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies the question of why the pre-trained language model is effective on downstream NLP tasks. Through mathematical analysis, the authors conclude that the diversity of the model parameter matrix at the linear output layer in pre-training has a significant impact on the transfer capability in the sense that the larger the diversity parameter, the smaller the risk. Therefore, the authors explicitly add a diversity regularizer to the linear output layer to increase diversity to improve the effect of pre-training.\n\n\n",
            "main_review": "\nStrength:\n\n1. Very full and detailed mathematical derivation and analysis.\n\n2. With the help of theoretical analysis and the use of regularizers to improve the effect of pre-training, a slightly better effect than the baseline was obtained on some NLU benchmarks, which supports the conclusion of the analysis (though not very strongly).\n\nWeakness:\n\n1. The authors mentioned that multi-task pre-training is the reason why the pre-training language model is effective, but the author uses the diversity of multi-class for replacement. Is there any enhancement difference between multi-task and multi-class diversity? \nAfter reading the paper, we still don't know which one should be used, multi-task pre-training or increasing multi-class diversity.\n\n2. The theoretical part of this paper is relatively small compared to (Du et al., 2021, Remark 6.2), and the method adopted is also proposed by (Zou & Adams, 2012). The contribution is relatively limited, it is only a detailed derivation and combination with pre-training.\n\n3. This work only tried one type of regularizer, the experimental part is slightly thin, and other diversity regularizers are not compared in the paper. Are they also in line with the conclusion of the paper?\n\n4. Whether multi-task pre-training and increasing multi-class diversity can be used at the same time and their combining effect is not explored. For example, according to the \"don't stop pre-training\" training method, pre-training and downstream tasks constitute multi-task training. Does the diversity regularization method proposed in the paper can still work? Can increasing multi-class diversity be used in the finetune stage? Does the BERT baseline in Table 1 continue to be pre-trained for fair comparison? In addition, since the pre-training of BERT itself is multi-task pre-training (MLM and NSP), why not choose the simpler RoBERTa as the baseline for ablation study?\n\n5. I find it more puzzling as mentioned in footnote 6. Regularization only changes the pre-training process and should have nothing to do with downstream tasks. Is BERT only suitable for classification tasks after use? I would like to see this part of the experiment added. If the task generalization ability of the pre-trained language model is sacrificed, then some of the improvements obtained are of no value. This improvement can be directly achieved by designing pre-training tasks related to the task.\n\n6. The theoretical analysis is somewhat wonderful and convincing, meanwhile, the more the  diversity is, the better the performance is, which is an obvious and universal phenomena in various machine learning operations.  However, the experimental part is obviously flawed, inadequate and unconvincing. Too few experiments to support its theory. If such theory provided in this paper holds, to my best knowledge, I believe it will have great influence on CV pre-training tasks / transfer learning tasks, where the “class diversity” also holds. Also, I think experiments on more NLP pre-training models, like RoBERTa, ELECTRA are needed. Whether the proposed regularization technique has general effects to all pre-training tasks in all directions is unknown. Even for NLP, there are only very weak improvements. The glue result is only performed on the development set but not the test set, making the results less convincing. The effects of the proposed method cannot even surpass the effects of changing random seeds, meaning the improvements is non-significant. Meanwhile, for all settings of the diversity parameter in Table 1, it seems that there is no one single setting that surpass the baseline on all downstream tasks. \n",
            "summary_of_the_review": "\nAlthough this paper provides good explanation for the success of pre-training and mathematical proofs for its theorems, I still think it’s not good enough for ICLR. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}