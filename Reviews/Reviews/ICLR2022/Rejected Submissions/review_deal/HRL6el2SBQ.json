{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes to use intra-class mixup supplemented with angular margin to improve OOD detection. \n\nStrengths:\n+ Simple idea\n+ Experiments on multiple datasets (although mostly focused on image benchmarks)\n\nWeaknesses:\n- Justification for the idea could be improved. It'd be nice to understand when we expect this to (not) work.\n- Differences from prior work \"Angle-based outlier detection in high-dimensional data\" could be better explained.\n\nWhile the paper has some interesting contributions, the reviewers and I feel that the current version falls short of the acceptance threshold. I encourage the authors to revise and resubmit to a different venue."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors propose adding angular distance in the OoD score and additional training samples via intra-class mixup.  Angular margin/distance is defined as the angle between the weight vector from a class and an instance (ie. arccos of the dot product of the unit weight vector and the unit vector for a instance.  Intra-class mixup generates samples by interpolating between two samples of the same class.   Angular spread is the standard deviation of the angular margin for a given dataset.  Intra-class mixup reduces angular spread in in-distribution data and increases angular spread between in-distribution and OoD data.   Angular separability is defined as the squared difference of in-distribution angular margin and OoD angular margin, normalized by the sum of their standard deviation.\n\nWith four datasets, they show that intra-class mixup increases angular separability in 3 datasets.  With four datasets and 3 different OoD techniques, intra-mixup is more accurate in AUROC and FPR95, but not AUPR.   Adding the cosine of angular margin in the OoD scores generally improves performance over using OoD scores alone.\n\n\n\n",
            "main_review": "Strengths:\n\nUsing intra-class mixup to increase angular separability between in-distribution and OoD and hence OoD performance.\n\nAdding angular margin to the OoD scores improves OoD performance.\n\nWeaknesses:\n\nThe improvement in performance is not significant.\n\nTable 3: the purpose and analysis of Table 3 could be further discussed.   The \"baseline accuracy\" on In-distribution and/or OoD data?  It seems to be in-distribution data.  Accuracy of Intra-mixup is lower in 3 out of 4 datasets, any suggesions on reasons?\n\nEquation 8: theta_o and theta_i -- are they averages of all the samples in the in-distribution and OoD datasets?\n",
            "summary_of_the_review": "The ideas of using intra-class mixup to increase angular separability and adding angular margin to the OoD scores are interesting.  \n\nHowever, the presentation can be improved.\n\n\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes to use intra-class mixup to train OOD detectors. Adding intra-class mixup, the separability between in-distribution and out-of-distribution data is improved. The methods are evaluated in multiple OOD benchmark datasets, showing improvement of 4-6% over the methods without intra-class mixup. \n\nThe paper also interestingly shows that the cosine of the angular margin is also a useful measure for OOD detection. It can be added to other regular OOD measures to further improve the performance. \n\n",
            "main_review": "Strengths: \n\nThe paper is well written and easy to follow. The method of intra-class mixup makes sense to me. The results (including both tables and figures) are clearly presented.\n\n\nWeakness:\n\nThe results show that adding Cos(\\theta) on top of the model with intra-class mixup is helpful. I wonder if a regular model without intra-class mixup can also get improvement by adding Cos(\\theta) to the OOD metric. \n\nThe method has been compared with simple baselines MSP, ODIN, and energy score. It would be great if the method can provide supplementary improvement for other recently developed methods such as Bayesian NNs or Gaussian Process.\n\nThe method is evaluated mostly on far-OOD benchmarks (see Table 4 and 6, 7, 8). I wonder if the proposed method can work well for near-OOD benchmarks such as CIFAR-10 vs CIFAR-100. \n",
            "summary_of_the_review": "The paper proposes an interesting idea of using intra-class mixup to improve separability between in- and out-of-distribution data. The paper also shows that cos(\\theta) can be a useful measurement for OOD detection, and can be added to other OOD measurements like MSP and ODIN. It would be great if more results can be provided",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Inspired from inter-class mixup (Zhang et al, ICLR 2018), where data augmentation is used to train models more robust to adversarial samples, this work proposes intra-class mixup to reduce the variance of in-distribution samples, i.e. the training set, and improve the capacity of a trained model to detect out-of-distribution samples at inference time. The key difference between the two works is that the here proposed does the mixup within samples belonging to the same class. \n\nIn addition to this, the work propose to use the angular margin, i.e. the angle between the normal of the decision boundary of a neural net (obtained from the weights of the last layer) and an unmixed sample, to detect OOD samples. The cosine of such angle shall be coupled with an OOD method to perform OOD detection. ",
            "main_review": "Strengths\n- Overall the paper is clearly written, although the experiments and results could be improved. \n- Good coverage of the related work\n- Very relevant topic\n\nWeaknesses\n- This work is presented as an alternative to standard empirical risk minimization (ERM) or the setup of mixup (Zhang et al, ICLR 2018), where ERM is replaced by vicinal risk minimization (VRM, Chappelle et al, NIPS 2000). What learning strategy is used here, in opposition to these two works, is never stated. \n- The experimental setup is not clear, which limits reproducibility and makes difficult to interpret the obtained results. What exactly is considered as in-distribution and what samples are OoD and how are they generated? (see detailed comments)\n- The proposed comparisons against ERM and mixup are not necessarily relevant. ERM is the standard approach to train a model and mixup proposes a data augmentation strategy to make a model more robust to adversarial samples. This work, instead focuses on the detection of OOD samples. As such, it should compare itself with methods addressing the same problem and not directly against ERM and mixup (table 1).\n\nDetailed comments\n- Please acknowledge previous work on angle-based outlier detection [1], as it closely relates to this work.\n- The angular margin is estimated w.r.t the decision boundary (see eqs. 3 and 4). Therefore, there is an error in the illustration in Fig. 1a.\n- Differently from mixup, in this work \\lambda does not follow a Beta distribution. Moreover, no details are provided on how it is chosen. Please comment.\n- If the label should not change, second line of Eq. 2 could be omitted. Otherwise, there is no guarantee that \\hat{y} will have the same value. This would only hold when \\lambda=1 or 0 and y_i=y_j=1 or 0.\n- Eq 5 implies that new data is being generated, i.e. data augmentation. Is this the case? What happends with the original samples? I suppose they are undesired since they have high variance.\n- Could the authors motivate why the angular margin needs to be used coupled with another metric (eq. 7) and not on its own?\n- The experimental setup is not clear. The paper misses to clearly establish what is an OoD sample/set on each of the experiments. In which way Gaussian and uniform noise are used for this purpose?\n- At inference time, when is a sample considered OOD?\n- The AUROC is not a good measure in OOD detection problems, since usually the majority class dominates. AUPRC should be favored. Interestingly, it is mixup that fairs best in that scenario, despite not being a method designed for OOD detection. Please comment \n\nMinor\n- There are typos in the plots in figure 1b,c,d\n- Table 2 is unnecessary and may be omitted\n\nReferences\n[1] H.-P. Kriegel, M. S hubert, and A. Zimek. Angle-based outlier detection in high-dimensional data. In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’08, pages 444–452. ACM, 2008.\n",
            "summary_of_the_review": "This work proposes intra-class mixup coupled with an angular mesaure for OOD detection. The idea of an angle-based measure has been explored in the past (not mentioned in the work) and it is interesting to propose it in the context of deep nets.\n\nThe work, however, misses some key elements in the presentation of the method and has a weak experimental setup.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The authors propose to use intra-class mixup (i.e. linearly interpolating images from the same class) as a form of data augmentation. They further define the angular margin and propose to add it to out-of-distribution detection scores in order to improve the performance. They evaluate their method's OOD performance on three different OOD scores and show improvements.",
            "main_review": "A strong point of the paper is that they evaluate their method in many different settings, i.e. on 3 OOD methods on 4 different in-distribution. They also report AUROC, AUPR and FPR95, which is good. They do not fit special hyperparameters for each specific out-distribution, making their comparison fair in my opinion. Furthermore they test their OOD detection performance on many different out-distributions. \n\nThat being said, one OOD detection task that they should definitely add is CIFAR10 vs. CIFAR100 and vice-versa, as this task is known to be particularly difficult.\n\nOverall, the paper's use of the angular margin is not well-motivated. Especially the creation of a new OOD score by simply adding the angular margin to other OOD scores is completely ad-hoc. On top of this, it should be noted that their angular margin score depends on arbitrary bias values in the last layer. If one adds some bias shift $b$ to every bias in the last layer then it shifts the origin and thus modifies the angular margin. However, the resulting network's loss would be identical to the original and thus there is a-priori no reason that training should favor one network over the other. All their OOD performance measures would also remain invariant except for the angular margin score. There should at least be a discussion about why despite all this, the angular margin score is still helpful.\n\nI am not sure about the strength of their baselines. Comparing for example with the results from [1], they used the same architecture but their simple MSP model beats even the best results in Table 9 (on all but one of the datasets and metrics that both report). This doesn't even mention the fact that even if this wasn't the case the paper would still only be comparing their results to weak baselines. Basically, the authors argue that one can improve all three of the baselines that they study using their technique. But even the improved version's performance is worse than, for example outlier exposure [Hendrycks et. al. 2019] or Mahalanobis distance [Lee et. al 2018], so why use the weak techniques in the first place?\n\nOne point of confusion for me was that the standard deviations in Table 1 were seemingly computed across the dataset, whereas in all other tables they were computed across different random seeds. I really think this could be clarified explicitly in the Table's caption (especially given that the authors had lots of space left).\n\nI was also confused how exactly the standard deviations of the averages were computed in Table 4, Table 5 and in the appendix. Are all runs aggregated and then the standard deviation across all of them is computed? Or is the average computed for each seed and then the standard deviation of that is reported?\n\nMore issues:\n- In Table 1 the caption reads \"Intra-class mixup has consistently has better separability\" (small typo here!), even though it clearly isn't true for CIFAR-10. \n- Table 2 is fairly useless to the paper and appears to have been added to pad out the length.\n- In Table 3, we see that Inter generally performs better in terms of accuracy (the gap being quite large on CIFAR-100). This should at least be acknowledged and discussed by the authors. Arguably, this issue is strong enough as an argument against using their method just by itself, so it cannot be glossed over.\n- In Table 4, there are 6 instances of wrong boldfacing, making the authors' results look better than they are.\n\nMinor comments:\n- The citation style is very hard to read. Presumably the authors used \\citet (instead of \\citep) even for citations that are not directly referenced in the sentence.\n- \"Angular\" is misspelled in all axis labels in Figure 1.\n- All Table captions should be above the tables, not below.\n- All equations are missing punctuation.\n- Wenn formatting $\\cos$ or $\\arccos$ in mathmode one should use \\cos and \\arccos (instead of $cos$ and $arccos$). Similarly, I would recommend to use \\mathrm for worded subscripts, i.e. $R_\\mathrm{expected}$ instead of $R_{expected}$.\n- Several figure and table captions end without punctuation. \n\n[1] Towards neural networks that provably know when they don't know, Alexander Meinke, Matthias Hein, at ICLR20",
            "summary_of_the_review": "I recommend to reject this paper. The theoretical contribution is marginal and mostly unmotivated and the empirical results only compare to relatively weak baselines. On top of this, the paper clearly lacks polish. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}