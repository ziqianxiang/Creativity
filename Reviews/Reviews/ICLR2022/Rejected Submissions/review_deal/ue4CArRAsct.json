{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes a method for structured representation learning using autoencoders. The method has two primary ingredients: (i) encourage independence in latent blocks by feeding different blocks of the latent representation to different depths of the decoder by injecting noise in an Ada-IN-inspired block, (ii) a so-called hybrid sampling, that samples each block from a fixed learned set of k latent vectors, similar to the codebook used in VQ-VAE (Oord et al 2017). The method is claimed to result in higher fidelity reconstruction and generation while also learning representations that are more disentangled compared to VAE and $\\beta$-VAE. \n\nSome limitations that came up in the reviews and later in the discussion among the reviewers are (i) lack of comparison with more advanced disentangled VAEs, which would be helpful in establishing the claim of the paper on better reconstruction but comparable disentanglement performance to regularization based methods (ii) high-level similarity to VLAE and other methods that also use hierarchical latent variables that limits the claims on novelty. Current draft also emphasizes disentanglement which the reviewers found lacking in justification and rigor. The paper is currently not suitable for publication at ICLR but taking into account the comments from reviewers on the presentation aspects will help improve the paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper structures how latents are used in an autoencoder to improve its performance. They are motivated by causal structure and independence of latent variables. They also sample using a sort of discrete mixup between latent codes from pushing forward data, and show various improvements there.",
            "main_review": "Strengths:\nThe paper is fairly easy to follow, the motivation is generally good, the problem is important (generation and structured unsupervised latent structure / representation learning). The experiments are well-motivated and support their story.\n\nWeaknesses:\nThe main idea of the paper, independence of the latent variables, only seems empirically supported, but it's hard to find convincing motivation in the paper that the precise way they use the latents in generation / algorithm *should* lead to independent latents. This is a minor point I think, but there are some arguments in the paper I either didn't follow or didn't convince me that the results *should* make sense.\n\ne.g., S2.2: \"This architectural asymmetry... encourages statistical independence...\" Can you can explain this more clearly?\n\nSimilarly, with the hybrid sampling, could you clarify: \"This is consistent with the causal perspective of the latent variables as independent noises...\" Do you mean that because we observe this sampling works so well, that this supports the idea that these latents are independent?\n\nOther comments / clarifications:\nSo the latents are a flat vector output of the encoder correct? When you split this latent vector, how do you distribute it amongst the layers which generating when the splits are different? Do you give every layer a latent or if there are less splits do only some get latents?\n\nIs AdaAE your method? From what I can tell, you are just using all of latent vector (not splitting) at every layer? Since this works so well, I'm curious if it's just a matter of hyperparameters that make it inferior sometimes to SAE.\n\nAny intuitions about the ordering of performance of SAE-X? For instance, in Rec FID, SAE-6 performs worse than SAE-12 and SAE-4. Why is this?\n\nThe FID scores are distributional, generated (e.g., using hybrid sampling) vs test, correct? I wonder how close the train distribution is to the samples in FID. Is there any possibility, because you're sampling latents using the training examples (and random mixing) that the generated examples closely resemble training instances? I feel like this needs to be tested.\n\n",
            "summary_of_the_review": "I'm leaning accept, but there are many points of clarification that would need to be sorted out before acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposed a new structure call SAE (structural autoencoder) along with a new sampling technical, named hybrid sampling, to train a encoder-decoder generative model. The authors conducted extensive experiments to show the proposed model and sampling method lead to much better results for generating higher quality images and get more meaningful/smooth images in experiments involving feature disentanglements and extrapolations.  ",
            "main_review": "Good points:\n\n* The proposed model gets good empirical results. The authors did extensive experiments and ablation studies. The results and explanations look promising.\n* Overall, this paper is well written and is easy-to-follow. The authors explained the motivation and the techniques clearly.  \n* The authors provide their codes in the supplimentaries. At a glance, the code have a clear structure and there's a good README that provide comprehensible instructions.\n\nMinor issues:\n\n* I feel like Figure 3 can be presented in a better way. There are just too many bars with the same color and it takes a lot of time for me to tell the bar annotations ('X', 'O').\n* Similar issue for Figure 5. I had to go back and forth between the figure and the first paragraph in section 4.1 multiple times to understand the message the authors wanted to deliver. It could be probably better if the texts and figures are close and there are some text annotations or marks in the image to make it easier for readers.\n",
            "summary_of_the_review": "This paper is well written and its claims are well supported by the experiments, analysis, discussions. I believe this paper will bring values to the broader researcher community.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a mechanism to train disentangled generative models without relying on regularization losses. It aims to enforce independence in blocks of the latent representation of a VAE by 1) corresponding different blocks of the latent representation to different depths of the decoder by injecting noise in an Ada-IN-inspired block 2) sampling each block from a fixed learned set of k latent vectors, similar to the codebook of VQ-VAE (Oord et al 2017).",
            "main_review": "The goal of this paper is important: having architectures that, without carefully engineered regularization losses, learn diverse disentangled representations would be a significant contribution to the literature. Moreover, the approach is fairly simple to implement and natural. However, there are several aspects of this paper that leave me somewhat unconvinced that this work actually accomplishes that goal.\n\nFirst, and most crucially, while the paper emphasizes the goal of disentanglement by its architecture, there is no part of the paper meant to convince the reader that the method accomplishes this aim. Specifically, there is no theoretical justification for why this model should result in the disentangled representations that it advertises. As noted in Subsection 2.2, there is no guarantee of correctly disentangling the true factors of variation (something that’s generally been accepted with unsupervised models following Locatello et al. 2018). Which precise definition of disentanglement is the paper operating under, what is its relationship to the causal ordering in this paper, and how does this method accomplish it? \n\nSecond, while the evaluation is very comprehensive in some aspects (five disentanglement metrics, six (distinct) alternative models, four sources of datasets, with each model trained several times), there are several aspects that take away from this evaluation. \nThe disentanglement metrics presented are not especially convincing. Notably, the MIG scores reported appear to disagree with prior work - for example, 0.107 for $\\beta$-VAE is substantially lower than the performance reported in Khrulkov et al. 2021 (~0.2). Little detail is given about the specific hyperparameter choices for the models which are used for comparison, so it’s difficult to assess whether the comparison is fair. Moreover, the primary proposed model (SAE) is not the best-reported model in terms of MIG for any datasets besides the MPI3D toy and simulated datasets, which is somewhat surprising as Locatello et al. (2018) suggested that DCI-D and MIG are broadly very correlated.\nThe metrics which are most emphasized in the paper seem to be FID and Reconstruction FID. Figure 6 is the only place outside of the appendix where any non-FID metric is shown or discussed. However, FID is broadly inappropriate as a disentanglement metric and is less applicable the less similar a dataset is to ImageNet, as noted in Section 4.2.1 of Barratt and Sharma (2018). FID has been widely criticized by many VAE papers, such as in Section 5.2.1 of Razavi et al 2019, which makes the choice of emphasis on this metric more difficult to understand. Furthermore, in terms of FID, the model proposed by this paper appears visibly less performant than others such as StyleGAN, VQ-VAE, VQ-GAN, BigGAN, LOGAN. \nFID but not disentanglement metrics are reported on Celeb-A or RFD, which are the only datasets evaluated in this paper not consisting of 3D shapes.  There are reasonable arguments which one can make for not evaluating these metrics on these datasets, but I feel like a sentence on this point would be helpful. In addition, if they are excluded due to the general difficulty in interpreting their underlying factors of variation, a disentanglement dataset such as Cars3D (Reed et al. 2015) could be useful in bridging that gap.\nThe models from prior work which are compared against are VAE (Kingma and Welling 2013), $\\beta$-VAE (Higgins et al. 2017), VLAE (Zhao et al. 2017), and WAE (Tolstikhin et al. 2018). However, notably excluded are FactorVAE (Kim and Mnih 2018) or $\\beta$-TCVAE (Chen et al. 2018). Seeing as many of the metrics used here can be calculated on GAN models, it could also have been interesting to see InfoGAN. \n\nThird, the paper discusses Variational Ladder autoencoders (VLAEs) as an inspiration, but the main differences appear to be a different way of injecting the embeddings hierarchically (using Ada-IN layers) and the alternative sampling method/prior - why is this so important? The current argument, as I understand it, is that the variational loss is among the undesirable “aggressive regularization” methods. However, I do not believe that it is traditionally thought of as a disentanglement loss, so this seems fairly unconvincing. I would be interested to see a precise motivation for why this provides an improvement, beyond the general structural advantages of AdaIN as shown in other models. Moreover, there are many ways to enforce causal dependence on an input, many of which are already widely used in the literature. One approach, causal attention masking, appears to also enforce the desired causality constraint and is widely used in the transformer literature (e.g. 3.2.3 of Vaswani et al. 2017). Some variant of causal attention masking has also been applied to generative models, such as “Diagonal Attention and Style-based GAN for Content-Style Disentanglement in Image Generation and Translation” (Kwon et al. 2021) which leveraged a “diagonal spatial attention” layer for hierarchical control of content embeddings. Overall, I’d like to see a much more thorough discussion of the motivation behind the features proposed in this paper. \n\nFinally, I have some concerns over the scalability of this approach. Namely, as the datasets become more complex and necessitate higher-dimensional encodings and deeper models, I would expect the disentanglement benefits of this method to decrease. Specifically, while a causal structure may technically exist here for deeper models and larger embedding spaces, the ability to condition on many prior embeddings may limit their usefulness. The choice to use only 12 and 32 dimensions for the experiments appears to reinforce this. This alone is not a reason to reject this work, as the paper intends to propose a novel architectural approach to disentanglement and further advances would clearly be necessary to make this competitive with the state-of-the-art in generative models in terms of fidelity and diversity - still, it would probably offer an improve understanding of this approach to see a scaling analysis. \n\nA few nitpicks on style: there are many places throughout the paper where references that appear to be intended to be parenthetical are actually in-text (such as Section 1.1, the caption of Table 1, etc. They are easy to find by searching for “(20”. In addition, the paper extensively emphasizes the lack of regularization terms in the loss in the proposed model but it’s not clear exactly what loss was used.",
            "summary_of_the_review": "The paper has an ambitious goal and some potentially novel ideas but there seem to be serious issues with\n1. Precision: while certainly not every paper in the field needs to be theoretically driven, the work makes claims that would likely benefit from a more rigorous discussion - there is no formal discussion of the intended meaning of disentanglement and no precise argument for why this method would benefit that definition of disentanglement. However, while some additional precision would be helpful, I am mostly evaluating this as an empirical work.\n2. Evaluation:  within the disentanglement (non-FID) metrics and the datasets where a quantitive evaluation of disentanglement is provided, the paper does not consistently show that the proposed approach provides disentanglement advantages over VLAEs. The main text primarily focuses on FID and reconstruction FID which is inappropriate for the datasets used and the way in which the paper presents its contribution. Finally, it would have been helpful to have also seen other standard models like FactorVAE or TCVAE.\n3. Novelty: the main proposed advantage over VLAEs (described as an inspiration) appears to be the lack of variational regularization, but the authors don’t appear to argue that this is a disentanglement loss. It’s unclear to me exactly what argument is being made for the advantage of this approach.\n4. Scaling: it seems that the independence properties should enforce a gradually weaker constraint with deeper models and larger input dimensionalities. Some analysis of the performance of this method as a function of larger input dimensionalities could be beneficial.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a hierarchical latent variable model and accompanying sampling procedure for learning disentangled representations. Rather than a latent feature map/vector, the latent variables are used to condition affine transforms in the decoder. The authors combine this with a ‘hybrid’ sampling strategy, effectively sampling from the aggregate approximate posterior. Comparing with several baselines, the authors compare their model in terms of FID and various disentanglement scores. The authors also investigate generalization.",
            "main_review": "**Strong Points:**\n\nThe empirical evaluation has several strong aspects. The authors evaluate their proposed structural autoencoder (SAE) using multiple image datasets of varying complexity. The authors also compare with a wide range of baseline models of varying techniques and latent depth. Experimental results evaluate multiple aspects of the model, including reconstruction and generation performance, disentanglement (via latent traversals and quantitative metrics), and generalization in both the encoder and decoder to unseen data examples. In total, these results demonstrate that SAE obtains a high degree of disentanglement while also yielding high-quality reconstructions, as compared with relevant baselines.\n\nWhile I do not see the extrapolation experiments as particularly essential in demonstrating SAE, these experiments are somewhat novel in systematically evaluating generalization in various components of autoencoder architectures. To the best of my knowledge, these types of experiments do not exist in the current literature. It’s somewhat surprising that the disparity of generalization performance is so large between training the encoder versus the decoder on unseen examples. I see this as an interesting and potentially useful result.\n\nI also see the performance comparison of hybrid and prior sampling (Figure 3) as somewhat interesting and novel, providing another useful analysis for future works.\n\nFor the most part, the paper is clear. Much of the model is described well, results are presented clearly, and the appendix appears to be thorough.\n\n**Weak Points:**\n\nMy largest concern is regarding the validity of some of the design choices in the model. I understand that formulating a proper probabilistic model may not be the authors’ main purpose in developing SAE, however, it’s somewhat difficult to reconcile the presentation of probabilistic models in Section 2.1 with the design choices made in the model. For instance, from what I can tell, the authors use deterministic noise variables without any form of prior distribution. Likewise, the reconstruction is optimized using binary cross entropy, when this is only valid for binary variables, not the RGB values in the image datasets used. To be clear, the authors’ results are still valid. Rather, I’m simply discouraged that the model is not a proper probabilistic model, as this makes it difficult to assess the space of valid design choices.\n\nAnother concern is the novelty of the authors’ insights and the validity of the conclusions drawn. Multiple previous works, some of which the authors cite, have shown that hierarchical latent variable models are capable of learning disentangled, hierarchical representations, e.g., Zhao et al., 2017. Thus, it’s already clear that model architecture can bias representation learning. The authors’ specific claim, then, appears to be that some aspect of the latent independence and structural transform layers is helpful for disentanglement. However, it’s not clear why these techniques necessarily need to be linked, as these seem like independent design choices. One could also feasibly combine SAE with priors, either via fixed or learned priors. Thus, while it’s clear that the presented SAE model performs well, it’s not entirely clear which aspects of the model are responsible or whether techniques from previous works would improve performance further. I remain somewhat skeptical regarding the conclusions drawn by the authors.\n\nSome aspects of the model were not entirely clear to me. Figure 1 presents a helpful diagram of the structural decoder. I had hoped that the authors would include a similar diagram depicting the encoder, as it was not clear whether the $U$ variables were stochastic or deterministic.\n\nThe authors motivate their approach in terms of structural causal models (Section 2.1). However, I don’t see what this adds beyond standard directed graphical models. Motivating the approach from the perspective of causal modeling, in my opinion, only serves to complicate the presentation.\n\n\n**Additional feedback:**\n\nSection 3.2: It’s not clear how the extrapolation sections fit with the rest of the paper. This seems like an orthogonal contribution from the proposed model.\n\nFigure 6: $\\beta$-VAE is lower than VAE on all of the metrics. If $\\beta$ was tuned, as the authors claim, then I wouldn’t expect this to be the case.",
            "summary_of_the_review": "This paper has both strengths and weaknesses, as outline above. While aspects of the model presentation, model formulation, and conclusions could be improved, the strength of the empirical evaluation somewhat makes up for these shortcomings. I’m currently borderline, with a tendency toward acceptance. With revisions, I could be convinced to accept this paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}