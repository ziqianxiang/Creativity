{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper studies the \"shortcut\" learning phenomenon in CNNs and proposes a simple and effective strategy (white paper) to alleviate specific shortcut patterns (e.g. \"black squares\" in the image). The proposed scheme is verified empirically and shown to improve over some existing solutions. All reviewers appreciate the simplicity of the idea, which allows its quick implementation and reproduciblity. However, reviewers y5Su and C42n believe the notion of shortcuts as studied in this paper are not only very limited, but also artificial. Consequently, they raise doubts about practical relevance/significance of the method for real world datasets with natural shortcuts. Based on these concerns, I suggest authors to identify a real setting (non-artificial data) where, alongside their synthetic shortcuts, can show the practical effectiveness of the proposed can."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a method called White Paper Assistance that serves as a data/task agnostic regularization to prevent shortcut learning. It conducts a set of empirical experiments to show the usefulness of this technique.",
            "main_review": "Strengths:\n1. The paper is very well-written, especially the first three sections (introduction, motivation, etc.).\n2. Authors conducted a very comprehensive set of empirical experiments and ablation studies to show the usefulness of WPA.\n3. The idea is novel.\n\nWeakness:\n1. The idea is a bit simple -- which in of itself is not a true weakness. ResNet as an idea is not complicated at all. I find it disheartening that the paper did not really tell readers how to construct a white paper in section 3 (if I simply missed it, please let me know). However, the code in the supplementary materials helped. White paper is constructed as follow:\n```python\nwhite_paper_gen = torch.ones(args.train_batch, 3, 32, 32)\n```\nIt offers another way of constructing white paper, which is \n```python\nwhite_paper_gen = 255 * np.ones((32, 32, 3), dtype=np.uint8)\nwhite_paper_gen = Image.fromarray(white_paper_gen)\nwhite_paper_gen = transforms.ToTensor()(white_paper_gen)\nwhite_paper_gen = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))(white_paper_gen)\n```\nThe code states that either version works similarly and does not affect the performance.\nI wonder if there are other white papers as well, for example `np.zeros((32, 32, 3))` -- most CNN models add explicit bias terms in their CNN kernel. Would a different white paper reveal different bias in the model? I don't think the paper answers this question or discusses it. \n2. Section 4 \"Is white paper training harmful to the model?\" -- the evidences do not seem to support the claim. The evidences are 1). Only projection head (CNN layers) are affected but not classification head (FCN layer); 2). Parameter changes are small. None of these constitute as a direct support that the training is not \"harmful\" to the model. This point can simply be illustrated by the experimental results\n3. Section 5.1 and 5.2 mainly build the narrative that WPA improves the test performance (generalization performance), but they are indirect evidence to support that WPA does in fact alleviate shortcut learning. Only Section 5.3 and Table 6 directly show whether WPA does what it's designed to do. A suggestion is to discuss the result of Section 5.3 more.\n4. It would be interesting to try to explain why WPA works -- with `np.ones` input, what is the model predicting? Would any input serve as white paper? Figure 2 seems to suggest that Gaussian noise input does not work as well as WPA. Why? Authors spend lot of time showing WPA improves the test performance of the original model, but fails to provide useful insights on how WPA works -- this is particularly important because it can spark future research directions.",
            "summary_of_the_review": "The paper proposes a simple idea and conduct a wide range of experiments that seem to focus on producing SOTA results instead of illustrating how and why WPA works. However, given the amount of effort that goes into this paper, I'm on the fence of whether to recommend accept or reject.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a novel method, White Paper Assistant (WP), to prevent CNNs from utilizing spurious input-out correlations, the so-called shortcuts, in classification. The main idea is to intermittently update the CNN to predict uniform distribution over classes for white image inputs. Through careful and extensive empirical studies on various datasets, the paper shows that this simple regularization can prevent the CNN from excessively focusing on shortcuts, thus learning more generalizable features and improving the overall accuracy on the test set.",
            "main_review": "**Strength:**\n\nThe paper is easy to read and understand, and the arguments are mostly clear and concise. The prior works are explained well and the current work is well motivated and very relevant. The proposed method is simple and effective, the right combination for wide adoption, and the insights from Section 4 on how WP affects different parts of the CNN classifier can help improve the overall understanding of these models in the community. The experiments are well-designed and clear in what they try to illustrate, and sufficiently analyze the performance of the method, on its own and in conjunction with other related regularizations, as well as the contribution of different hyperparameters.\n\n**Concerns and Questions:**\n\n1- In Section 2, the paper argues that “instead of leveraging laborious and expensive supervision, our method utilizes the common sense by leveraging the white paper to detect the dominant patterns”, however, I cannot find any experiment/figure backing this up. Reporting some cost comparison (training time, computation, etc.) could help justify this claim.\n\n2- Table 1 is very helpful in illustrating the effectiveness of WP, however when it comes to testing the method using other input types (e.g. noise or ice-cream), the paper only provides the overall performance on CIFAR100. A more clear and helpful comparison would be to also include these different input choices in Table 1.\n\n3- Table 4 is confusing to me: why does the first row read “+White Paper Assistant” and then has “-” under “w/ WP”? Are these methods being stacked as we go down the rows of the table (for example is the last row using all the above methods together, or just Label smoothing w/ and w/o WP? I suspect it is the latter case, in which case I think “+” should be dropped from the rows to avoid confusion). \n\n4- In a closely related line of work, adversarially-trained CNNs have been shown to be less reliant on shortcut features and focus on more “human-expected” features (see for example Figure 2 and 3 in [1]). In particular, these models can reduce error of classification on data limited regimes (such as the small datasets used in this paper). Including a study of whether WP achieves the same performance boost on adversarially-trained CNNs, and moreover, whether WP can be used as an alternative to adversarial training for robustness, would be very helpful for the community. But at the very least, I recommend referencing such works and commenting on potential effects.\n\n5- An important consideration when deciding to use WP is the time/accuracy trade-off, that is, how will increasing the number of WP iterations (M) from zero to 50 or 500 affect training time, and in turn accuracy (this can be achieve by adding a training time plot to Figure 6 of Appendix B).\n\n6- How will the size of the dataset affect the helpfulness of WP (as most of the datasets used in the paper seem to be on the smaller size)? This is an important point to consider since methods that try to avoid shortcut connections tend to degrade performance on larger datasets (see [1]).\n\nSide note: Figure 3e should be 3d.\n\n[1] Tsipras et al. Robustness may be at odds with accuracy. ICLR (2019).\n",
            "summary_of_the_review": "I enjoyed reading this paper, and I think its proposed method is mostly effective and can be valuable as a new regularizer for CNNs. Moreover, the insights into how different parts of a CNN are affected by such a regularizer strikes me as a very helpful piece of knowledge for the wider research on neural net architectures. As such, despite having some concerns listed previously, I think the paper merits acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The present work introduces an approach to tackle Shortcut Learning by CNNs, called White Paper Assistance (WP). After motivating and introducing the method, the authors evaluate it on computer vision datasets with synthetic inserted shortcuts, ie. black pixels in the corners of the images. The authors show that the WP approach reduces the learning of so-called shortcuts.  ",
            "main_review": "The paper tackles and illustrates an important issue of not only CNN's but general deep learning models. As the authors describe this is not a novel insight and several approaches were introduced to reduce and even prevent shortcut learning by deep neural networks. However, I major line of related work seems to be missing: Teso et al. Explanatory Interactive Machine Learning (AIES 2019), Ross et al. Right for the Right Reasons (IJCAI 2017), Schramowski et al. Making deep neural networks right for the right scientific reasons by interacting with their explanations (Nat Mach Intell 2020), to name a few.\n\nMore importantly, the paper is not well written and even looks unfinished in parts (e.g first paragraph on page 5). This makes it very hard to read. \n\nSince, in the vision domain, shortcut learning is already a well-explored field (see above) benchmark datasets were already introduced (Decoy MNIST, ColorMNIST, CLEVR-Hans) cf e.g. https://arxiv.org/abs/1703.03717, https://arxiv.org/abs/2011.12854 and https://www.nature.com/articles/s41467-019-08987-4. \n\nInstead of designing or synthetic adapting datasets, I recommend an evaluation of already introduced and well-known datasets. Further, a comparison to previous approaches is missing in the present work.",
            "summary_of_the_review": "Unfortunately, I do not believe that this paper meets the standard for publication. Besides that, the paper is not well written and therefore hard to understand its experimental evaluation is missing a comparison to previous approaches.\n",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The work proposes a method for reducing the extent to which a model learns to rely on \"shortcuts\" by introducing a new form of regularization encourages the model to make uniform predictions for white images. The proposed method is evaluated across a range of model architectures and datasets and evaluated in combination with a variety of existing techniques.  \n",
            "main_review": "Strengths:\n-  The proposed method is intuive, simple to implement, and very general.  \n-  The problem of preventing \"shortcuts\" is an important one.\n\nWeaknesses:\n-  The experiments have a fundamental flaw that prevents the results from being meaningful:  the results reported are the \"best test accuracy achieved across the entire training process\" instead of the standard \"test accuracy of the model with the best validation performance across the training process.\"  This conclusion is based on Figure 3 and an inspection of the code.  \n\t-  This might be related to the conflict between the stated intuition that \"predicting uniform probability for a white image should help the model\" and the empirical results that \"predicting uniform probability for real images (eg, CIFAR images) also helps the model.\"  \n-  The connection to the \"shortcut\" literature should be expanded upon. \n\t-  The related work is missing many references which include, but are not limited to: Shetty et al \"Not Using the Car to See the Sidewalk--Quantifying and Controlling the Effects of Context in Classification and Segmentation\";  Rieger et al \"Interpretations are useful: penalizing explanations to align neural networks with prior knowledge\";  Teney et al \"Learning what makes a difference from counterfactual examples and gradient supervision\"; Singh et al \"Don't Judge an Object by Its Context: Learning to Overcome Contextual Bias\"\n\t-  It is unclear what shortcuts this work detects, which is something that many methods in the \"shortcut\" literature specify. \n\t-  The evaluation is run on a testing distribution which matches the training distribution, which is unsusual for methods in the \"shortcut\" literature (as using the \"shortcut\" usually makes it easy to acheieve high accuracy on the training distribution, so showing improvements in this way by preventing \"shortcuts\" is very challenging).\n\t-  While setup correctly, the example using \"black squares\" and the CIFAR dataset is a very simplistic (and artificial) example of a \"shortcut\".  See the aforementioned references for some other examples.  \n\t-  Generally, it seems that the proposed method is closer to a standard regularization technique than a method for preventing \"shortcuts\".\n\n",
            "summary_of_the_review": "On its own, the methodology used to gather the results disqualifies the paper for publication at this point.  If the reviewer is mistaken about this, then all of the sub-scores would increase.  \n\nHowever, even if the reviewer is mistaken about this methodological error and the results are valid, it does not seem that the paper is ready for publication because the connection to the \"shortcut\" literature needs clarification (which impacts how almost the entire paper is written).   \n\n# Update based on author responses\n\nThe author's responses have addressed some of my concerns (eg, using testing data as a validation set, the lack of overlap between CIFAR10/100).  However, the paper still needs to be revised to emphasize that it is not focused on \"perceptible patterns\" as shortcuts (eg, cows are usually in pastures) but rather on \"imperceptible patterns\" (eg, high frequency signals in an image).  So I've changed my score from a 1 to a 3.  \n\nI'm very familiar with the work on \"perceptible patterns\" and this paper is not up to par for that area because  it does not identify specific shortcuts and then demonstrate that it has reduced them.  I'm much less familiar with the work on \"imperceptible patterns\".  So I've changed my confidence from a 5 to a 3.  \n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}