{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "Dear Authors,\n\nThe paper was received nicely and discussed during the rebuttal period. However, the current consensus suggests the paper requires another round of revisions before it gets accepted.\n\nIn particular:\n\n- There were still some gray areas regarding comparison to simple techniques. E.g., one reviewer raised the question how it compares to simply stopping based on validation accuracy for example. The reviewer was missing the justification why stopping at the loss of Ramanujan graph property is preferable in comparison to other criteria. \n- Several reviewers found the general idea interesting, but all felt that more reasonings about the impact/insights/relationship of Ramanujan graph property with pruning need to be found to get accepted.\n- Reviewers appreciate that the authors corrected many parts of the submission (see increased scores). However, reviewers felt that the paper requires more data/evidence to get accepted at this level, based on the discussions made during the rebuttal period. \n\nBest AC"
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper revisits the Lottery Ticket Hypotheis (LTH), where aggressively pruning a neural network in a smart way allows it to retain its convergence properties, at a lesser computational cost.\n\nThe authors advance an explanation relating this LTH to expanders, graphs that have the property that information circulates really well between the vertices, and then to Ramanujan graphs. They then propose an algorithm built on this premise, that prunes the graph as long as the ramanujan property is still satisfied.\n\nFinally, a wealth of experiments is shown, showing the relation between the Ramanujan property and the performance of the pruned network. The authors also test the performance of their Ramanujan pruning algorithm against the same datasets.",
            "main_review": "Strengths:\n- as far as I know, the connection from the LTH to expander graphs is both novel and very interesting\n- the exposition and algorithm definition are clear and easy to read\n- judging from the provided experiments, the analysis seems to be well-supported by data\n\nWeaknesses:\nThe main weakness is in the experiments section, which is very hard to parse. Problems include:\n- very small graphs requiring constant zooming in and out, especially since some curves are only distinguished by line and marker style\n- too many curves per graph, some being borderline unreadable (e.g. Figure 5.d)\n- the color schemes are confusing: the same colors are used for distinguishing Lenet layers (Figure 3.a, b) and noise levels (Figure 3.c)\n- the loss of the ramanujan property should be present on the testing accuracy graphs\n- the legend scheme for the curves should be explained more clearly: as of now, they are quite hermetic and require good knowledge of the studied networks' architecture.\n\nRemarks:\n- in some cases (e.g. below Figure 1), $p$ switches between the proportion of edges pruned and the proportion of edges kept\n- I understand why you wouldn't want a rigorous definition of a universal cover, but you should add a reference\n- the theory of Ramanujan graphs is only stated for positive weights, yet this is not the case for neural networks: how is the ramanujan bound defined?\n- typo below def. 3: it is the Alon-Boppana theorem",
            "summary_of_the_review": "The idea of the paper is novel, interesting, and clearly exposed. However, a wealth of clarity issues in the experiments section hinder the clarity of the paper overall.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The work proposes to utilize the spectral gap of the bipartite network connectivity graphs between neural network layers to terminate an iterative weight pruning algorithm.",
            "main_review": "\nStrengths:\n- The paper builds upon a nice premise by introducing expander graphs to describe connectivity between neural network layers.\n- Choosing the pruning termination criterium based on on the extended spectral gap (Algorithm 1) is a good idea and novel to my knowledge.\n\nWeaknesses:\n- The experiments are not able to convince me that the chosen bound leads to good pruning results. In some experiments they seem to fit, in others they do not.\n- There is also no solid theoretical justification, why the termination criterium should fit (except for the already known correlation between the spectral gap and density).\n- The work is lacking comparisons to previous pruning methods, showcasing the usefulness and significance of the proposed algorithm.\n- The experiment section is confusing (see below)\n\nIssues in Experiments section:\n- I have trouble following the figures. The labels in the figures are not consistent with the names in the text (lambda vs t, L1-L3 not defined). Minor: the figure labels are way too small.\n- UE, UD, WE, WD only defined in a Table and the text is lacking clear discussions and conclusions for the results in the different scenarios.\n- It would help alot if the test accuracy figures would show the point in which Algorithm 1 would stop the pruning.\n- If I understood correctly, it should be easy to apply Algorithm 1 to the networks and present the resulting test accuracy in comparison to previous pruning techniques. However, that is not done.\n",
            "summary_of_the_review": "All in all, while I like the general idea of utilizing spectral graph information of bipartite network connectivity graphs for pruning decisions, I find the paper is lacking a clear message and the required experimental evidence to back it up. There are no comparisons to previous pruning techniques, which would allow to showcase significance. Therefore, I tend to vote for rejection.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors state the hypothesis that the Ramanujan graph property is necessary or at least beneficial for pruned networks to serve as lottery tickets. Motivated by this claim, they propose a stopping criterion for iterative magnitude pruning, which is based on the two largest eigenvalues of the weight (or adjacency) matrices of neural networks and should (approximately) preserve the Ramanujan graph property. \n",
            "main_review": "While the connection of Ramanujan expander graphs to lottery tickets is interesting, the experiments are not convincing and leave doubt that this is even a relevant property for network pruning. The practical relevance of the stopping criterion is doubtfull. Detailed points of critique follow below.\n\nStrengths:\n+ The idea to measure the potential of a pruned network to serve as lottery ticket based on the eigenspectrum is novel and interesting.\n+ The idea that the Ramanujan graph property could be beneficial for sparse network structures could be also useful (but this remains open to show). \n\nWeaknesses:\n- The introduction into Ramanujan graphs is lengthy and only motivational, as the results on maximality hold only for regular graphs and not for the lottery tickets (which are usually irregular).\n- The authors claim that they introduce a new pruning algorithm, while in fact they only propose a stopping criterion for iterative magnitude pruning (IMP), which requires the repeated computation of the two largest eigenvalues of potentially large weight matrices. The original IMP is computationally expensive already, which leaves the question what the added value of the eigen-bound based stopping criterion should be?\n- As a stopping criterion for pruning, it is not relevant, as the validation set performance (or even training set performance) is in fact more indicative.\n- Is the contribution should be the insight that graphs with the Ramanujan property are better suitable as lottery tickets than other structures, I would expect different experiments that test this hypothesis. \nFor instance, I would expect that random graphs are sampled or otherwise constructed that have comparable properties (like density, degree distribution, flow preserving, etc.)\nwhich either have the Ramanujan graph property or not (but are otherwise comparable). Or through rewiring of edges the Ramanujan graph property can be gained or lost...\nFor both these sets (Ramanujan or not) one could then test whether they are suitable lottery tickets (or at least preserve information flow).\n- For real practical impact, I would then expect an algorithm that repairs a pruned network to gain back the Ramanujan graph property (by edge rewiring that preserved the overall network density) and then would want to see experiments that suggest a superior performance of this pruning algorithm with repair.\n- Different from the made claims, the Figures 3, 4, 5 do not even seem to indicate a strong link between the loss of the Ramanujan graph property and a loss of accuracy.\nI would expect to see a sharp phase transition and complete loss of network accuracy at the point where the Ramanujan graph property is lost if the made claims were true.\n- Or is simply the connectivity of the graph the relevant property?\nI would expect at least a comparison with methods like Synflow or layerwise pruning approaches that try to prevent layer collapse and preserve information flow through the network or orthogonal repair as in \"A Signal Propagation Perspective for Pruning Neural Networks at Initialization\", ICLR 2020.\n\nPoints of minor critique and open questions:\n- Definition 3 for irregular graphs: Two replacements of the same quantity (the regularity r) are proposed, which is contradictory in this form. This could be explained more precisely.\n- Multiple symbols, abbreviations, figure legends, etc. are nowhere defined. For instance, \\sigma shall probably refer to the standard deviation of the applied Gaussian noise?\n- Figure 7: It is impossible to compare which pruning algorithms perform best. They all look very similar.\n- No error bars are reported anywhere.\n",
            "summary_of_the_review": "As the merit of the proposed eigen-bound based pruning is unclear, I believe the work is not suitable for publication at ICLR in this form.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the authors study the properties of the subnetworks in lottery ticket hypothesis (LTH) from the perspective of the spectral graph theory.\nThey argue that the pruned network in LTH remains a Ramanujan graph.\nThe performance of the subnetworks begins to degrade with the loss of Ramanujan graph property.\nTo this end, they propose an eigen-bound based network pruning algorithm to preserve the graph property.\nExtensive experiments are conducted to demonstrate the effectiveness of the proposed method.",
            "main_review": "**Pros**: The proposed pruning method is based on the spectral graph theory, which is novel and interesting.\n\n**Cons**:\n1. **Significance of the method**.\n    - What is the practical significance of the proposed pruning method in this work? \n    - Whether the proposed method can effectively improve the pruning performance, such as alleviating the performance degradation? \n    - Whether the winning lottery ticket can be found more effectively?\n    -  For pruning the neural network, what is the significance of robustness against the noise in the dataset? Maybe we should focus on pruning instead of robustness.\n\n2. **More comparisons are needed**.\n    -  This paper does not show the performance of other state-of-the-art pruning methods, such as IMP or gradient-based pruning.\n    -  How robust are other pruning methods to noise?\n    -  Authors should also make more ablation studies to demonstrate the roles or impact of different components of the proposed method.\n\n3. **Other issues**.\n    -  The experiment section in this paper is too single. The results shown in Figures 3, 4, and 5 are all similar, except that different datasets and models are changed.  This section should include more important results to demonstrate the proposed arguments. Please put the insignificant results or curves to the appendix.\n    -  The experiment section frequently shows the curves of eigen factors. What does this indicate? What kind of enlightenment does this changing trend provide us?\n    -  Why does the proposed method establish robustness against the noise?",
            "summary_of_the_review": "In general, the paper is well written. I hope the authors should first answer the aforementioned questions cautiously and carefully.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}