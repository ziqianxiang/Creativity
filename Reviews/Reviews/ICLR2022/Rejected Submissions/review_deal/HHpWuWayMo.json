{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "Although the problem studied in the paper is interesting, all the reviewers believe that the current draft has limited technical contributions. Moreover, there are serious issues with the writing and presentation of the work. Also the experiments are rather limited and their results are not significant. I strongly recommend the authors to take the reviewers' comments into account and improve different aspects of their work for future conferences."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "Proposes an adversarial attack on continuous cooperative multi-agent settings. Given a target low performing state, the adversary chooses an observation perturbation by trying to minimize the distance from the next state (as predicted by a dynamics model) to the target state via projected gradient descent. They also experiment with optimizing for and choosing a limited number of agents to perform the adversarial action at each timestep. There experiments are in multi-limbed mujoco tasks such as Walker, HalfCheetah, and Ant, and show that their method causes the agent to receive lower reward than a random adversary.",
            "main_review": "I like the idea of using a learned dynamics model to reduce the need for environment interactions when computing proposed attacks.\n\nI'm not so sure about the constraint that you need to prespecify a target state that leads to low performance. In some sense, if you have to pre-specify this, you have to predeterimine the type of attack you want your adversary to make, as opposed to letting it learn what are the best types of attacks given the domain. The paper against which you compare, Lin et al. (2020), seems more general to me in that they try to minimize the reward of the team. This is a broader approach in that your reward function could simply be I(s ==s_target) if you wanted your adversary to reach a certain state as in your work.\n\n\"Note that we cannot directly apply methods such as the fast gradient sign method (FGSM) or JSMA (an attack using saliency map) as in Lin et al. (2020), since those methods require a “target” action which is not available in continuous action space.\" — why is the case? If I understand correctly, the target action in Lin et al. (2020) is proposed by a RL agent policy, which could easily be a continuous action policy no? To that end, it's unclear to me why their method wouldn't work in a continuous action space if you substitute policy gradient for q-learning. That being said, it could be interesting to compare to their method with and without a learned model to replace the need for interactions with the environment.\n\nWhy is victim selection important? What the real world instances where you'd have an adversary be able to select a different agent at each timestep but not be able to simple perturb all of them?\n\nThis method would be more interesting to me if it allowed for and experimented with discrete or mixed continuous-discrete action spaces.\n\nSmall comments:\n\nOverall I found the grammar and writing to be somewhat poor and distracting.\n\nTo me, using language \"dynamic model\" is more unclear than \"dynamics model\". The latter I think is more commonly used.\n\n\"neural network of target policy\" → \"neural network of **the** target policy\"\n\n\"another line of research tackle\" → \"another line of research tackle**s**\"\n\n\"problem on DRL with continuous action space**s**\"\n\n\"where A is the set of concatenated states\" — I think this should be \"concatenated actions\".",
            "summary_of_the_review": "Overall I felt this paper was below the acceptance threshold primarily because I felt the experiment domains rather limited and disappointing that the only baseline was random noise (and that the method seems to do only marginally better than random noise). I also felt that prior methods could work in continuous action spaces if very slightly modified, so I felt this paper could be much stronger if it experimented with and compared to those variants.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work focuses on attacking cooperative multi-agent reinforcement learning using a model-based method. First, a model is trained to predict the next state based on the current state and action. Then PGD is used to find a small perturbation on the current state so that the next state is closed to a target state. The target state is selected using heuristic methods and they are used as a surrogate objective for minimizing the team reward. A victim agent selection method is also proposed for selecting the best target action.\n",
            "main_review": "The following are some of my concerns and questions:\n1. The main issue is related to the novelty of this work. Previous works (Gleave et al. 2020, Lin et al. 2020) have looked at attacking MARL. Thus it is no surprise that MARLs are vulnerable to perturbation of the agents’ observation. In fact, as shown in the evaluation section, random noise seems to have comparable (or sometimes better) performance compared to the proposed method, which further emphasizes the instability of the cooperative MARL algorithms. In fact, the brittleness of cooperative strategies is richly documented in game theory literature, which is not addressed in this paper. \n\n2. From a practical point of view, this work also has a few shortcomings. The attack method proposed in this work is a white-box attack, which requires the attacker to have access to the model’s parameter, which limits the applicability of the attack.\n\n3. The proposed method in fact does not directly minimize the team reward, but the users need to manually define a target state as part of the attack objective. Defining the target states can be challenging for some use cases, and hence it is in fact non-trivial to evaluate the robustness of the MARL algorithm.\n\n4. Gradient-based attack methods such as FGSM and JSMA are not compared in this work. Although the authors mentioned that the target action is not available in the continuous action space, one can actually back prop from the centralized critic toward to actor policy with an objective to minimize the team reward. How do those methods compare to your proposed method?\n\n5. One of the contributions in this work is the victim selection during the attack. However, it seems the attacker needs to have observations of all agents and be able to modify the observation of any agent. This can be challenging during the execution phase of MARL where the agents are physically distributed. Can you provide some concrete applications of this attack? What are the assumptions of the attack?\n\n6. This attack requires training a model-based state prediction model. How much data is needed to train such a model? How much time is required for training? Does it assume the attacker has access to the underlying transition function?\n",
            "summary_of_the_review": "Although this work is looking at an important problem related to the robustness issue of MARL, its technical contribution fail short in term of the theoretical analysis, novelty, and evaluations. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a method to attach cooperative MARL strategies on tasks with continuous action spaces. A dynamics model is trained, based on which agents’ local observations are perturbed so that agents select actions that lead to the least-rewarding states.",
            "main_review": "***Soundness***\n\nThe main concern of the reviewer is about the novelty and applicability of the proposed method.\n\n1) The proposed attacking method depends on a pre-trained dynamics model. Training such a model may be costly in multi-agent settings. \n\n2) The most frustrating point about the proposed method is that the target state needs to be predefined, which largely limits the applicability of the proposed method.\n\n3) For selecting the victim agents, the proposed method needs local information of all the agents. This assumption is less realistic in multi-agent settings. Moreover, together with 2), this assumption simplifies the difficulty of attacking MARL algorithms.\n\n4) The presented results are not surprising because the instability of MARL has been well studied. As reported in the paper, random noise and randomly selected victim agents can also reduce performance significantly.The reviewer thinks that perturbing local observations is not a promising research direction. The reviewer holds that a simple attacking method where the gradients of observation with respect to value function is used to perturb the observations can have significant influence.\n\n5) The proposed method has access to the policy of all agents. Such a white-box attacking method is less applicable.\n\n***Evaluation***\n\nThe baselines are not powerful. Please show the results of the baseline described in Soundness (4).\n\n***Clarity***\n\nThis paper is not well written, and there are various grammatical errors throughout the paper. For example, in section 3.2, above equation 1, “$A$ is the set of concatenated states.” I guess this should be actions? ",
            "summary_of_the_review": "The problem under research is interesting, but the proposed method can access parameters of all agents’ policies, can modify observations of all agents, and depends on pre-defined target states. These assumption hurts the novelty and applicability of the paper.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "The reviewer does not see obvious ethics concerns.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presented a novel method for cooperative MARL to evaluate the robustness under the adversarial states. Specifically, the authors leveraged a model-based approach to perform adversarial attacks on states for MARL with continuous action and state spaces. To represent well the environment, they used a deep neural network to predict the future states. Subsequently they solved an optimization problem with the learned dynamics to produce small perturbations. They also discussed a few strategies to optimally select the victim agents for the attack. To validate the proposed method, the authors implemented numerical experiments on multi-agent Mujoco tasks to show the effectiveness of the proposed method. The investigated topic in this work is quite interesting yet challenging, which remains an active research area. Overall, the paper is easy to follow and well written. However, the authors should pay attention to a few points I have raised in the following to make the paper more technically solid and sound.",
            "main_review": "1.\tThe method proposed in the paper has marginal novelty. It looks like the authors combined different existing method, such as model-based attack, MARL, and PGD to come up with a novel framework for evaluating the robustness of the cooperative MARL. While such a combination has not been explored, the work presented in the paper did not really show sufficient and significant contributions, either theoretical or empirical. In terms of theory, this work is lack of some analysis that can guarantee the optimality or convergence. It is okay to not have such analysis, which means that the effectiveness of the proposed scheme highly depends on the empirical evidence. However, the numerical results shown in the paper to me are not very convincing. \n2.\tDue to the DECENTRALIZED MARL, typically, method would require communication protocol among various agents. However, in the paper, it looks like the author only summed up all agents and solved the optimization problem to attain the optimal solutions. That way, how to guarantee the optimality?\n3.\tIn the paper, the author leveraged the model-based attack. While a learned model for the environment is helpful, how does the model accuracy affect the performance of the proposed approach? Though using deep net to represent the dynamics may be decent for the state prediction, while the nonlinearity could significantly affect the optimization problems. How to evaluate such effect and empirically solve such a potential issue?\n4.\tIn the numerical results, the authors showed different results for Mujoco environment. However, the baselines to me look simple and in the introduction they have mentioned some existing works, so they should have used one or two in their comparison. Additionally, it is unknown to me whether the proposed method can always outperform those two baselines under different constraint budgets. Since I have only seen 0.2 from the supplementary materials. We can also see that the authors only showed a few agents. What about 20? 30? Even more? Though with more agents, it could be difficult to scale in the Mujoco environment. The authors could use the Gridworld to show that. Moreover, what about topologies? For a decentralized method, it is critical to see the performance with different topologies. While this is missing in the work.\n5.\tThe author mentioned in Algorithm 1 the target state. How to determine the target state in theory and practice?\n6.\tWhat is $\\mathcal{D}_{random}$ and why did the authors need that to form the buffer?\n7.\tAt the beginning, the authors said that each agent observes its own local state. While in remark 3.1, the authors assumed each agent had access to the other agent’s observation. Why not just assuming at the beginning each agent could observe the global state?\n\n****************************************\nAfter carefully reviewing the authors' responses and comments from other reviewers, I decide to keep the score. I think the current draft is still below the acceptance bar and it requires more work to make it technically solid and sound, particularly in how to determine the target state in a more efficient way, mathematically analysis for the framework, and the quantitative impact of dynamic accuracy on the performance.\n",
            "summary_of_the_review": "Overall, I think the paper still requires substantial work to make sufficient contributions. Particularly, the authors need to address some aforementioned issues, such as marginal novelty and weak numerical results, to make the paper technically solid and sound. Hopefully my comments can help out.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}