{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper proposes a new distributional assumption and a new algorithm for learning convolutional neural networks. However, the reviewers reach a consensus that this paper's assumptions are not natural and may not be satisfied in real-world domains. The meta reviewer agrees and thus decides to reject the paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, the authors consider learning the function class of one-layer convolutional networks. While multiple results show that learning neural networks is NP-hard in general, meaning there is no efficient algorithm that is guaranteed to succeed at learning the function class of neural networks. Hence, in order to close the gap between theory and practice, and guarantee tractability, distributional assumptions should be introduced. In this work, an abstract distribution is considered, where only the covering number is assumed to be bounded. With this assumption, the authors show that a learning algorithm with an unsupervised clustering procedure creates features that are then used in a linear classifiers, learn a class of one-layer convolutional networks, with sample complexity that is tight for the number of supervised samples in terms of $nN$.",
            "main_review": "The paper is satisfyingly well-written and somehow clear and easy to follow. I am not very familiar with the literature on hardness results for neural networks and I am not able to judge the novelty of this work. It seems to be well motivated with a simple tractability result with a natural assumption for PAC learnability. I have a few comments:\n\nI don’t think the section 3.1.1 is necessary and could be replaced by a short paragraph in order to make space for more high level discussions about the assumptions and the related literature. In particular, the relation between the covering number assumption and the settings considered in previous work (low-dimensional manifolds etc). \n\nI am also not entirely convinced that the dependence in the covering number is the right measure here. Especially, the algorithm would require to sample from every covering ball, which does not seem to be practical. Furthermore, this covering number is on all patches at the same time, and might be impractically large. I don't think the simulations are particularly convincing.\n\nOn the other hand, some function class are tractable without requiring small covering number (by for example, restricting the GCF to have all its functions in a small RKHS ball, and taking a convolutional kernel as the learning method).",
            "summary_of_the_review": "This paper  give a simple example of tractability under the natural assumption of covering number (somehow necessary for PAC learnability). Even if it is a simple example, it is not trivial that this lower bound is achieved by an efficient algorithm, and the fact that the semi-supervised method presented in this paper works is not obvious. However, the author falls short of convincing that this covering number could a feature of real dataset that ensure tractability. For these reasons, my recommendation is marginally below the acceptance. If the authors can convince me that this covering number assumption has strong theoretical or empirical backing, I would be willing to raise my recommendation!\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Motivated by the empirical successes of CNNs for classification tasks on\nnatural image data in spite of certain known computational hardness barriers,\nthe authors consider a model for semi-supervised learning of CNNs under certain\ndistributional assumptions. They consider a classification setup given samples\n$(x, y)$ from a distribution $\\mathcal{I}$, assumed to be separable with a\npositive margin (in population) by a \"generalized convolution function\" $F$\n(which is a linear function of a bounded Lipschitz function acting on patches),\nwhich only depends on certain low-dimensional projections of the images $x$ (in\nvision parlance, the \"image patches\"). The authors study a two-stage\nsemi-supervised algorithm for classifying such data, originally given by Coates\n(2011): the first stage takes in unlabeled data samples $x$ and creates a patch\ndictionary $D$, and the second stage uses this patch dictionary $D$ together\nwith a training set of i.i.d.\\ labeled examples from $\\mathcal{I}$ to learn a\nlinear classifier via solving a hard margin SVM problem with features obtained\nfrom the dictionary $D$ (the embedding is something like a nearest-neighbor\nembedding: given an input image, the image is decomposed into patches, and a\none-hot vector indicating the patches in the dictionary falling within a\nspecified radius of each input patch is output (and then normalized by its\nsum)). The authors' analysis proves that this model PAC learns this family of\ndistributions given a sufficiently expressive patch dictionary $D$: the sample\ncomplexity ultimately depends on the covering number of the set of all patches\nthat can be observed, and the authors argue that this indicates that when the\nset of all possible patches has low-dimensional structure, we avoid the\naforementioned hardness results and learn efficiently. A lower bound is\nprovided to prove certain dependencies on the number of patches and the\ncovering number is unavoidable in the PAC learning setting. Some experiments\nare provided to demonstrate the algorithm on MNIST and Fashion MNIST.\n\n",
            "main_review": "### Strengths\n- The paper is well-referenced, well-written, and well-motivated. It studies\n  several timely issues: theory for data-driven embeddings applied in deep\n  learning-esque setting (following empirical results of Thiry et al.\n  mentioned), learnability of CNNs on natural data distributions.\n- The mathematical formalism is precisely presented and the arguments explained\n  with a great combination of technical precision and intuition, making it easy\n  to understand the authors' algorithm and technical claims. I believe the\n  results obtained are reasonable given these arguments, although I have not\n  checked the proofs in the appendix line-by-line.\n\n### Weaknesses\n\nI would appreciate clarification around how the authors see several modeling\naspects of their setup, relative to the stated goal of understanding\nlearnability of CNNs under practical data distributions. In general, I feel an\nexplicit discussion of some of these aspects in the paper would be helpful:\nsome of this material could be worked into section 3.1.1 (although the\nresults there are helpful to see when thinking about $M$ and $L$, I believe\nthey are standard and the lemmas could be placed in the appendix).\n- The GCF target model is significantly more general than CNNs, while also\n  missing certain aspects of practical CNN architectures. E.g. the GCF is\n  presented like a \"linear probe\" of a convolutional network's intermediate\n  features, but I do not think modern convolutional networks use bounded\n  activation functions (as in assumption 3.2); the GCF does not allow for\n  hierarchical interactions between intermediate features for each patch,\n  implying that representing complex visual relationships in this model will\n  require $k$ to be very large (and potentially $f$ to have a large Lipschitz\n  constant) -- so although $f$ can be a deep CNN, the overall classifier does\n  not have some standard depth benefits that one expects in practical CNNs.\n  These make me have some caution when interpreting what the theoretical\n  results imply about practical, deep CNNs.\n- Assumption 4.2 and the later $m_u$ condition assuming i.i.d. samples from the\n  marginal image distribution seem to require the algorithm to have access to\n  an unlabeled training set of all possible patches one will observe (at a\n  sufficient resolution).  This might be reasonable depending on the patch\n  structure in practice, but it might be helpful to discuss this a bit in the\n  paper, and possibly cite prior works that argue for similar points.\n- The definition of $P_{\\mathcal{I}}$ is distribution-specific, but the\n  algorithmic results on PAC learnability in section 4 eventually require to use \n  one set $S_u$ for all possible distributions: it therefore seems like what is\n  being used here via Assumption 4.2 is a definition of $P$ that does not\n  involve a specific function $f$. I see that definition 4.6 attempts to handle\n  this issue for the later results, but Corollary 4.7 still references\n  assumption 4.2, and it is not clear to me that access to iid samples from the\n  marginal distribution over images (I assume this is what is being used in\n  Theorem 4.5, although it is not stated, based on the subsequent discussion) \n  leads to a covering of $P_{\\mathcal{I}}$ here, given that the function $f$\n  does not seem to be involved in the marginal distribution.\n\nIn general, it seems from section 4.2 that the authors' argument for how their\nanalysis relates to the stated aim of giving theory that explains how neural\nnetworks may be efficiently learnable in practice amounts to the fact that\nlearning Lipschitz functions over $d$-dimensional data has a\ncursed-in-dimensionality sample complexity $\\exp(d)$ (whether this $d$ is the\ndimension of the ambient space, as in the hardness results of Livni et al.\ncited, or the dimension of a low-dimensional manifold). I am under the\nimpression this is a well-known fact in computational learning theory;\nmoreover, the literature contains numerous results *specific* to neural\nnetworks trained with (S)GD that prove how these dependencies can be achieved:\nfor example [1-5] discussed below (there are also non-algorithmic results on\napproximation by deep networks, e.g. [6-9] below). The insight/contribution\nhere would be enhanced if the authors discussed ways in which their result on\nGCFs provided insights beyond the standard result in learning theory (perhaps,\nas discussed below, in terms of contributions to learning theory through their\nanalysis of the embedding algorithm and comparisons of their result to existing\nones).\n\nAfter Corollary 4.4, the authors state that their algorithm runs in polynomial time. This description may not be appropriate, given that the parameter $N$ is not a true input parameter, but rather a derived one\n  from $r$ and the properties of the set of all patches $P$ (and without\n  further assumptions on $P$, one cannot guarantee a priori that this is not\n  exponential in $d_P$).\n\n[6] http://arxiv.org/abs/1908.01842\n\n[7] http://arxiv.org/abs/2008.02545\n\n[8] http://arxiv.org/abs/1908.00695\n\n[9] https://openreview.net/forum?id=BJ3filKll\n\n### Related work\n\nIt would be helpful in assessing the authors' contribution if more reference\nwas made to prior art in the learning theory literature that might be relevant\nto the authors' algorithm and its analysis. Given the amount of study that has\nbeen devoted to sparse coding/nearest neighbor embedding/dictionary learning\nprocedures in the literature, it seems to me that analysis similar to the\nauthors might exist. If so, it would be helpful to compare the authors' rates\nto the rates in these algorithms, or point out other novelties of the authors'\nresults; if not, it would speak well to the novelty of the authors' analysis,\nwhich I would like to credit. In general, the related work section is helpful,\nbut it spends most of the time discussing learnability results for neural\nnetworks: it seems to me that the authors' analysis is applicable to a\nsignificantly more general bounded Lipschitz learning setting, and lacks some\nnuances associated with direct learning of NNs by GD/SGD (as mentioned above),\nand so it would be appropriate to discuss more general learning results to aid\nin the interpretation of the contribution.\n\nIt might be relevant to point out certain prior works that have been motivated\nsimilarly as the authors: e.g. [1-5] below. These works imagine that the data\nunder consideration have a low-dimensional manifold structure, and provide some\nalgorithmic results for training deep networks to classify them (some\nasymptotic).\n\n[1] http://dx.doi.org/10.1103/PhysRevX.10.041044\n\n[2] https://openreview.net/forum?id=O-6Pm_d_Q-\n\n[3] http://arxiv.org/abs/2107.14324\n\n[4] https://arxiv.org/abs/2106.04156\n\n[5] http://arxiv.org/abs/2006.13409\n\n\n### Minor issues/corrections\n- page 5 \"unsupervised stage\": reference should be to figure 2 or algorithm 1\n- definition 4.1: script D instead of script P?\n- theorem 4.5: should this reflect what is stated in the discussion below\n  ($S_u$ is composed of iid samples from the marginal over inputs?)\n- bottom of page 6: $\\hat{w}$ equation seems indexed incorrectly (the $v$'s\n  should index from $1$ to $N$ and the $u$'s should index from $1$ to $n$)\n\n\n",
            "summary_of_the_review": "This is a well-written theory paper, but I am unclear on the extent to which it accomplishes its stated goal of providing insights into how CNNs can be efficiently learned in practice relative to the existing literature (on neural network training and on learning theory), given the very general target function model and the fact that the final bounds essentially depend on a covering number for the entire space of input patches one will observe. The CNNs that can be modeled by the authors' target function model are essentially \"shallow\" models with a linear probe evaluation: no hierarchical structure is captured.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose a semi-supervised learning method, that learns a linear classifier over data-dependent features obtained from unlabeled data. Under some distribution assumption on the images based on their patches, the authors claim that the algorithm provably learns CNN. Experiments are conducted on MNIST and FashinMNIST aiming to verify the importance of covering number of the patches.",
            "main_review": "** Strength **\n+ The paper is well-written and well organized. It is easy to read.\n+ The distribution assumption, namely Assumption 3.2, is natural, and the authors have verified the realizability of such assumption if a dataset can be classified using shallow/deep CNN.\n+ Theoretical analysis seems to be sound.\n\n** Weakness **\n- The proved result seems to be \"underwhelming\" compared to the author's claim. The learning algorithm is constrained to learn a linear classifier on top of a pre-defined patch embedding function based on patches mined from the unlabeled data. However, the learned model is not necessarily a \"standard CNN\". More specifically, the patch/input-embedding function $\\Phi$ is not necessarily realized by convolutions.\n- The authors stressed the importance of covering number in the paper. However, no theoretical analysis or empirical study has been provided on an estimate on it. In particular, the experiments in section 5 is especially underwhelming. The authors hand-wavingly explain that noisy FMNIST appears hard to learn because it has large covering number, but no estimate has been provided, making the theoretical analysis unverified.\n- Also, the scaling of the proposed algorithm is questionable. The unsupervised learning mines patches from the entire patch domain, which might be costly.\n- The performance of the proposed algorithm is also not convincing. On the easy MNIST and FashionMNISt dataset, it underperforms even shallow CNNs. I would suggest test the algorithm on more complicated datasets, and report the runtime.",
            "summary_of_the_review": "Based on my review above, I am leaning towards rejecting the paper based on the utility and scalability of the algorithm, and the lack of empirical study to verify the theoretical contribution of the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work studies from the PAC-Bayes perspective the learning of predictors that consists in an aggregation of patch detectors. The main principle is to claim that if a low-dimensional structure is present in the patches, then a function of the data (e.g., a function to regress) inherits this low-dimensional structure too. From my perspective, all the proofs are direct and rely on straightforward applications of the PAC-Bayes theory and the proposed classifier isn't really studied from a game-changer perspective.",
            "main_review": "Section 3.1.1 is not really useful, as most of the Lipschitz bounds can be found in the literature (e.g., https://arxiv.org/abs/1312.6199) - I’d prone to remove this section, also because the considered architecture is not Lipschitz (it’s not even continuous)\n\nBasically, this work relies on the assumption that the patches (implying the training data) are low dimensional. As stated in this work (sec 4.2), this remains to be proved, at least numerically, and this is here tractable. Secondly, if experiments can be conducted on MNIST, they should also be conducted on datasets such as CIFAR-10. These are two major flaws of this work: I believe that the idea that if some data are low dimensional then it's easier to learn is not really new and specific to this method - it needs to be shown that this is actually the good underlying model of the data.\n",
            "summary_of_the_review": "This paper tries to address the difficult question of understanding CNNs through a simplified model. While this is its main focus, I believe more works needs to be done to get to this result. Indeed, if the authors could certify that their assumptions are correct and are reflected by the data, then I’d be inclined to raise my score. In other words, that the method proposed here is actually working and employs fully the structure of some (complex) data, and is not vacuous.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}