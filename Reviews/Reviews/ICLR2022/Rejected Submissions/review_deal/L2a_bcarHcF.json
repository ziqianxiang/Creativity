{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper demonstrates that transformer architectures can be trained to compute solutions of linear algebra problems with high accuracy. This is an interesting direction and, as the the reviews and the discussion show it is a \"good data point and insightful\", as one reviewer puts it. I fully agree with this but also agree with one other reviewer in that this is \"yet another\" application of a known transformer architecture. The author should place the model into context and provide some perspective. Without, the motivation behind solving the specific set of linear algebra problems considered is a bit unclear. For instance, could a transformer now learn to solve corresponding ML problems? Moreover, the dimensions of the considered matrices are rather small, and the generalization to larger dimension appear to be tricky."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper “Linear algebra with transformers” studies the application of seq2seq transformers to matrix operations. It studies their performance across different encodings of floating point numbers, different sizes of matrices, different operations, and different (synthetic) data distributions. The main findings are that transformers work surprisingly well on various matrix operations (addition, multiplication, eigenvalues, inversion, SVD, …) for small matrices (e.g. 5x5), and that generalization to OOD problems is not symmetric (I.e. generalization from one distribution to another does not imply the other way round).",
            "main_review": "Strengths:\n- The authors present a detailed study of many important questions. This is a fairly comprehensive work on the idea.\n- Thought-provoking application of transformers.\n- Very well written.\n\nWeaknesses:\n- I would have loved to see more than just L1 distances\n- The paper studies the question only on random matrices. In other symbolic domains we have seen that insights gained from machine learning approaches trained on random data do not necessarily carry over to “real-world” distributions. I would have loved to see a study that includes a wider variety of training and evaluation data.\n- The models used in this paper have sometimes rather odd (small) hyper parameters. E.g., for many experiments the models have only 2 layers. I’d love to see larger models and see if they improve the results (and the exact number of parameters).",
            "summary_of_the_review": "This work explores a wonderful idea: to solve linear algebra with transformer models. While these models use way more compute internally than the problem they are applied to requires to solve, it is an intriguing question whether these computations can be learned from scratch without further biases. The surprising answer is that this works relatively well for small matrices.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper consider the problem of approximating algebraic computations over matrices using transformers.  \nExperiments with different encodings are presented, investigating the use of transformers for approximating a number of algebraic operations.",
            "main_review": "While I found the paper well written, I didn't find it very impactful. The authors are not proposing a novel technique for addressing the problem, but only report some experiments with different encodings for the matrices and a standard transformer architecture. I was hoping to get more insights after reading this article, such as what architectural choices are beneficial for each specific problem and why. Instead, the proposed approach is training an off-the-shelf model with randomly distributed examples, and the experiments do not consider alternative techniques.\n\nIn my opinion, the motivation for this approach is also lacking, since the reported experiments only consider very small problems that can be solved exactly. I wish that the paper considered instead cases that need to be approximated, or possibly prove that indeed transformers trained on smaller problems can generalise to much higher dimensions. Since the learning algorithm has access to an oracle (numpy) that can provide exact supervision, an interesting problem is how to select the most informative input instances to be solved during training. Another aspect that is worth investigating in my opinion is how to cope with noise in the training data, allowing the training of transformers with an approximate oracle.",
            "summary_of_the_review": "- There isn't a novel contribution besides some experiments with an off-the-shelf model\n- The motivation is not supported by the experiments, which only report results in low-dimensional settings\n- The experiments do not consider any other technique besides transformers ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors train generic, dense transformers to perform several standard linear algebra computations, ranging from simple tasks like transposition to complex nonlinear tasks such as matrix inversion. They restrict themselves to relatively small matrices due to the practical limits of the dense, quadratic attention mechanism. The main result of the paper is that transformers can perform fairly well on all tasks, meaning that they can usually produce outputs that are correct upto relatively small tolerance. The paper also shows that some forms of out-of-distribution generalization are possible, and that this phenomenon is sensitive to the details of the training distribution.",
            "main_review": "I found this to be an interesting paper overall.\n\n- Framing\n\nI found the following claim to be problematic: 'Our results on out-of-distribution generalization provide justification to the idea that models trained over random data can be used to solve \"real world\" problems'. First, the authors only evaluate on other synthetic distributions. Second, in most \"real world\" problems, matrices are gigantic relative to the tiny context windows of dense transformers. Third, since traditional methods are always perfectly accurate on all distributions, I think the claim carries with it some burden to elaborate on why such noisy (and much less scalable) methods might prove useful in practice. Note that even if the potential practicality cannot be argued for, I think the experiments are interesting enough to stand on their own. \n\n- Sparse data reporting\n\nI would have liked to see much more data collected from the experiments, especially train-loss, validation-loss, and correctness-upto-tolerance curves over a range of architectures. The curves would also make it clear how many samples had been trained on for each measurement, which would be useful for understanding the relative performance of the different encodings. Note: it is not always clear from the current tables which encoding is even being used. It would also be interesting to see some analysis/visualization of the attention patterns, at least for tasks with relatively simple ground-truth algorithms like transposition and addition. Some experimental results also seem to be omitted; for example, S4.3 claims that \"deeper decoders are needed\" for matrix-matrix multiplication, but Table 5 does not include enough data to defend this claim.\n\n- Out of distribution findings seem unsurprising\n\nIt is great that the authors assess out-of-distribution, and I appreciate the negative result of generalizing from Wigner to matrices with positive eigenvalues. However, although the details of the subsequent out-of-distribution experiments are interesting, I found it generally unsurprising that models trained on non-Wigner matrices would generalize better, and I thought that the authors tried to make too big a point of this finding.\n\n- Co-training?\n\nAlthough not essential, I would be interested to see how co-training on many of the tasks at once affects sample efficiency.\n",
            "summary_of_the_review": "Despite the concerns listed above, I think this paper does contribute to our emerging understanding of transformers, and that many people in the community will find it worth engaging with.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper describes several experiments where transformers are trained to perform real-valued linear algebra calculations (matrix transposition, addition, multiplication, eigenvalues & eigenvectors of symmetric matrices, SVD, inversion). In-distribution accuracy is generally very high, whereas care is needed in order to obtain out-of-domain generalization.",
            "main_review": "The paper carries out a very thorough set of experiments on linear algebra calculations with transformers, using four different encodings of input matrices. In addition, the authors are aware of the importance of out-of-distribution generalization, varying both the matrix size and the distribution of input matrices of a given size. Results appear to be complete, and the conclusion drawn from them are generally sound.\n\nHowever, the problem tackled in this paper does not appear to be particularly useful. In my opinion, the conclusions and findings of this paper are only interesting on a theoretical level (perhaps they can help understand what transformers can or cannot do), rather than being directly applicable in a meaningful way. After all, we do have algorithms for all linear algebra problems considered, and they work with 100% accuracy, perfect out-of-domain generalization, and faster run time.\n\nAs the authors note in the discussion, at the current stage transformers have quadratic complexity in the number of tokens, which translates into $O(n^4)$ complexity for $n \\times n$ input matrices, and this is asymptotically slower than the exact algorithms we have. A potentially interesting future direction (which perhaps can be advertised more by the author, to strengthen the claim that this paper is useful) is to investigate linear-time transformers on tasks where the exact algorithm requires more than $O(n^2)$ time, so that perhaps transformers can be used to perform approximate computations with less time.\n\nI also have the following minor comments.\n- First line of page 2: $m \\times n$ should be in a math formula.\n- Section 5, fifth line: what does “for small values of n” mean here? The statement is very precise, so it is hard to believe that it is true up to a certain small number (say, 5) and false for a larger n.\n- End of page 7, “This confirms that out-of-distribution generalization is possible when particular attention...”: actually, you showed that it is *necessary* to pay particular attention, not that it is *sufficient*. So I would write “out-of-distribution generalization requires particular attention...”.",
            "summary_of_the_review": "This paper provides a thorough and well-written investigation of the use of transformers to perform linear algebra computation. However, this does not appear to be particularly useful.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}