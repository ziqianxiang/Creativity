{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper proposes a pre-training technique for improving the logical abilities of pre-trained language models.\nReviewers point to many issues with clarity and experimental evaluation. No response was given by authors."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The goal of the paper is to incorporate logical relations into pre-training of language models to solve the reliance of existing reasoning-enabled language models on external knowledge bases. This is done in a self-supervised way - facts (tuple of 2 arguments and a predicate) are parsed using dependency parsing, and then a logical graph is created to denote relationships between coreferents, and between predicates and arguments. Three pre-training objectives are presented over the facts and logical graph: logical connective masking, logical structure completion and logical path prediction.\n\nThe author's claimed contributions are the following:\n- 3 new pre-training objectives: logical connective masking, logical structure completion, logical path prediction\n- The model Prophet which \"achieves significant improvement over various logic reasoning involved NLP and NLU downstream tasks\"\n- An analysis that verifies how Prophet is using the context for logical reasoning",
            "main_review": "Strengths\n- Good and clear ablations to verify each pre-training objective, along with an assessment of longer term dependencies by context length\n- Approach and findings are important for the community in that this work uses no external knowledge bases for incorporating logical reasoning, positioned well relative to previous work\n\nWeaknesses\n- The emphasis on self-supervision could be stronger in the paper to make clear that it is a self-supervised approach to logical reasoning\n- The submission has areas where the technical explanation could be clearer (see questions below), and even the naming convention of the proposed pre-training objectives is not consistent\n- Several methodological choices were not justified (see questions below)\n- The attention matrix heatmap analysis is not convincing due to certain methodological choices\n\nQuestions\n- How exactly do you convert the dependency parse to the triplet of {$A_1, P, A_2$}? Please illustrate with an example\n- Why do you choose to model the logical graph as edges defined over relations between argument-predicates and between coreferents only? Are other relationships possible? Why did you not choose them?\n- In equation 3, there is no indication that the final training objective is weighted? Was a weighted sum used? What weights did you try and why?\n- In 4.3, it is unclear which model is pre-trained for 500k steps and which is fine-tuned from BERT for 200k steps. \n- If the baseline model is fine-tuned for 200k steps, why is that a sufficiently fair comparison with your pre-trained model with 500k steps?\n- In table 2: why are PROPHET's results clearly better than BERT large on LogiQA but perform worse / about the same on ReClor?\n- Why do you only choose the first head of the last attention layer for the attention matrix? What do you see for the other heads or other layers? I'm not convinced by this methodological choice\n\nPresentation concerns\n- Spelling of stabilized in Figure 1\n- If you specify what V is in 3.2, it is also good to specify what E is\n- The names used in Figure 2 for pre-training objective should correspond with those in 4.2\n- In 5.2, advise to keep Table 2 results separate from the list of Table 1 results\n- Suggest you show the BERT baseline in Table 4 for comparison with the ablations\n- In Table 6, are the figures from vanilla BERT or Prophet?\n- In Figure 5, typo \"nentailment\"\n",
            "summary_of_the_review": "Overall, I think this paper is very marginally below the acceptance threshold. I like the self-supervised approach to logical reasoning in pre-training. My major concern is the clarity of the paper, which hinders some explanation of methodological choices and proper understanding of the technical approach. I am willing to move this paper above the acceptance threshold if the authors can address my questions appropriately.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a new pre-training technique to induce a logical prior in the language model representation. Concretely, they propose pre-training on facts, represented as knowledge base triples (source, sink, relation) (knowledge-base completion) and link prediction, alongside traditional masked language modeling objective. Their proposed method achieves some improvement over downstream tasks, including a subset of GLUE benchmark and a couple of relation prediction datasets.\n\n",
            "main_review": "## The good\n\nThe paper attempts to build logic-aware models by pre-training on relation prediction and link prediction objectives. Since natural language corpuses are not annotated with facts (triples of source, sink and relation), the paper proposes a dependency parser based extraction mechanism. The paper investigates their method on GLUE benchmark, and also investigates it on two additional datasets which require logical reasoning. The paper also provides an ablation study and attempts to glance at the inner working of the model by qualitative analysis of attentions. The proposed model appears to work better on long contexts, which is also a plus.\n\n## The bad\n\nIt is clear that the paper required multiple revisions in the writing process before submission, as it is rife with many grammatical issues and weird constructions. Even if I choose to ignore that fact, a big issue in the framing of the paper is its complete lack of awareness of relation prediction and link prediction literature. A fact introduced is essentially a triple of source, sink and the relation, which is extensively studied in the Knowledge Base (KB) and Graph Neural Networks community. The related work misses to relate to this large body of work. Preliminaries also miss out on important nomenclature of the logical facts being triples and how it relates to having a knowledge base. In fact, the logic aware pre-training tasks are also variations of popular knowledge base reasoning tasks, such as relation prediction (\"Logical Connective Masking\" in the paper) and link prediction (\"Logical Structure Completion\"). The authors should acknowledge this vast area of research, and relate their tasks with the existing literature appropriately.\n\nThe proposed pre-training technique appears to work well in downstream tasks, but the paper raises more questions of the entire experimental results. Firstly, it is not clear whether the authors pre-trained the whole model on their pre-training objectives, or just continued pre-training on a publicly available checkpoint. If it is the latter, then the claim that their pre-training objectives are able to inject logical reasoning awareness is significantly weakened, as the downstream performance could be just an artefact of regularization through continual training. The presented results are also incomplete, as no information is provided on whether its the mean or median (typically downstream experiments are reported on a median of five runs, see Liu at al 2019 https://arxiv.org/abs/1907.11692 .) The authors tend to investigate primarily on GLUE benchmark, but it has been shown extensively in the literature that many tasks in this benchmark does not require logical reasoning, as the data consists of annotation artefacts which are leveraged by the model during training. (Gururangan et al 2018 https://arxiv.org/abs/1803.02324, Poliak et al 2019 https://aclanthology.org/S18-2023/, Tsuchiya et al 2018 https://aclanthology.org/L18-1239/, McCoy et al. 2019 https://arxiv.org/abs/1902.01007) Interestingly enough, recent research indicates these tasks might not even require the knowledge of word order, either during fine-tuning (Gupta et al 2021 https://ojs.aaai.org/index.php/AAAI/article/view/17531, Pham et al 2021 https://arxiv.org/abs/2012.15180) or in the pre-trained representation (Sinha et al 2021 https://arxiv.org/abs/2104.06644.) Thus, authors could have instead doubled down on more tasks that require explicit logical reasoning, such as CFQ (https://openreview.net/pdf?id=SygcCnNKwr), CLUTRR (https://arxiv.org/abs/1908.06177), TextWorld (https://arxiv.org/abs/1806.11532), etc.\n\nThe analysis presented in the paper (Attention Matrix Heatmap and Case Study) is not convincing and instead raises questions on the motivation and methodology used. Both are cases of extreme cherry picking, which results in a weak argument and does not answer the question of whether the proposed model really imbibes logical reasoning abilities adequately.\n\n## Questions & Comments\n\n### Section 3.1\n\n- Does the fact only represent the two notions \"who-did-what-to-whom\" and \"who-is-what\"? There are more categories of relations in knowledge base triples.\n\n### Section 4.2\n\n- \"apparently presents in sentences\" -> what does this mean?\n- The authors had previously defined a fact in Section 3.1 (T={A_1, P, A_2}), yet they do not re-use them in the text (\"Argument-Predicate-?\" could have been T={A_1, P, ?}). Thus, it does not make sense when new notations are introduced ($m^{a}$ and $m^{p}$)\n\n### Section 4.3\n\n- This section really puzzled me for a while. The authors note \"We pre-train our model for 500k steps\", and in the next to next sentence they say \"Initialized by pre-trained weights of BERT_{base}, we continue training our models for 200k steps.\" Which one is true? Do the authors just used a pre-trained model (BERT_{base}) and then trained on their logic pre-training objective for 200k steps? If so, then that severely impacts the contribution of the paper as then the benefit of the pre-training tasks are masked by the initialization!\n\n### Table 1, Table 2, Table 5\n\n- It is not clear what these numbers represent - is it mean or median of multiple runs? Or is it a single run on the fine-tuning scores? The authors can consult Liu et al 2019. (RoBERTa) for a good reference on how to report the numbers.\n\n### Section 5.2 Results\n\n- \"continual trained for 200K steps for a fair comparison\" - this isn't clear. Does the authors mean they trained the baseline BERT_{base} for 200k steps?\n- Performing better on GLUE benchmark does not indicate a models better ability for reasoning, as these tasks have been extensively studied in the literature to contain annotation artefacts which the models exploit (Gururangan et al 2018, McCoy et al 2019, etc).\n- The claim that the model is robust as it gains consistent improvements on small and large datasets is also not clear to me. Robustness is typically measured by the model's ability to comprehend on different adversarial examples.\n\n### Section 6.2\n\n- Yet another introduction of a new notation, without re-using the ones already defined\n\n### Section 6.3\n\n- I have severe issues with this analysis. First of all, its a cherry-picked result of a single sentence. Secondly, attentions behave differently in different heads of the originally pre-trained BERT model. From the same paper authors cite (Clark et al 2019), the last layers of BERT typically act as a \"no-op\" operation. It does not make sense to compare the last layer with that of the proposed model to claim the model pays more attention to discourse level information, as due to new training objectives the attention maps might be simply re-arranged in different layers. Attention map comparison of a multi-headed BERT model to display linguistic prowess is also incorrect - there exists other methods to quantify the same, such as the probing literature.\n\n### Section 6.4\n\n- \"slope of the dashed line is more gentle\" -> how is it quantified? Is it purely based on visual inspection? A metric would be more convincing to quantify the rate of decline.\n- Similarly as of Table 1 and 2, the results of Figure 4 cannot be parsed properly as they lack error bars.\n\n### Section 6.5\n\n- The case study is also an instance of cherry-picking. It is good to see the proposed model being more robust to negation and entity change. However, it is hardly convincing as it is a single instance. If the authors want to establish this as a strength of the model they should perform rigorous examination of the entire evaluation set, using these intervention methods.\n\n## Grammatical issues\n\n### Abstract\n\n- \"logic reasoning ability\" -> \"logical reasoning ability\"\n- \"knowledge basis\" -> \"knowledge bases\"\n- \"logic reasoning\" -> \"logical reasoning\" (lots of instances in the whole paper)\n- \"We evaluate our model on a broad range of NLP and NLU tasks\" - NLU is a subset of NLP, so this statement doesn't make sense. Either \"broad range of NLP\" tasks or \"broad range of NLU tasks\"\n\n### Section 2.2\n\n- \"multi-granularity pre-training\" -> \"granular pre-training\"\n\n### Section 3.1\n\n- Is \"actees\" even a word?\n\n### Section 6.2\n\n- \"performance is hurt a lot\" -> usage of colloquial english is generally discouraged in scientific literature. Consider replacing by \"perform is hurt significantly\"\n\n",
            "summary_of_the_review": "Overall, the paper requires extensive re-writing and better result presentation to be considered for acceptance in the conference. While the paper introduces a nice pre-training technique, it raises way more questions than it answers. Ultimately, the empirical results (and especially the presentation of those results) are not convincing enough to support the claim that the proposed pre-training techniques at all induces logical reasoning capabilities to BERT model. At this stage, unfortunately I cannot recommend acceptance.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "To enable pre-trained language models to capture logic relations contained in natural language, this paper presents a pre-trained language model, PROPHET, which aims to encode logic relations (facts) with three pre-training objectives. Experiments show that the proposed method outperforms its baseline BERT-base in a broad range of NLP tasks.",
            "main_review": "This paper is intended to inject knowledge into pre-trained language models without relying on external knowledge. Instead, it proposes to leverage knowledge in the corpus itself via dependency parsing and pre-trains a new language model with 3 designed pre-training tasks. \n\nThe authors should be cautious when using big words such as “logic reasoning”. The writing in sections 1 and 3 gives the feeling of overclaiming. It does not specify the exact meaning of the word \"logic\". The related work section is far from being exhaustive, missing a large portion of PrTM with syntax information. The pretraining tasks lack some details. Is LSC another modified MLM? Does it consider context? This work is based on BERT-base, which has limited capacity. It is not convincing of where the improvement is from, and it is unlikely that the proposed base-size model will have a broader impact. In short, I find the overall performance of the method not impressive. Additionally, in the analysis section, the attention maps are clearly selected biased over the multi-heads. It is not difficult to select the desired heat map from multiple heads. \n\nStrengths: \n1. This paper proposes a new method to enhance the logical reasoning ability of PrTM via syntactic parsing that avoids complex knowledge injection. \n2. Experiments show the effectiveness of the method, and the writing is clear and easy to understand.\n\nWeakness:\n1. The proposed method is not novel enough when compared with other works using syntactic parsing to enhance PrTMs.\n2. It is likely that there are only very limited logic relations contained in texts, making PROPHET not substantially beneficial to downstream NLP tasks.\n3. The experimental results are not convincing enough. Firstly, this paper only conducts experiments on BERT-base, which is an undertrained model. It is worth at least reporting both BERT-base and BERT-large to demonstrate the scalability of PROPHET.\n\n\n",
            "summary_of_the_review": "Injecting knowledge into pre-trained models without resorting to external knowledge sources is interesting, and the effect is confirmed by empirical results. However, my concern is the limited novelty of the proposed model.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to add the dependency parsing information of sentences as part of the pre-training objectives to the pre-training of BERT. In particular, it designs three pre-training tasks: logical connective masking, logical structure completion, and logical path prediction, to introduce the dependency parsing information.",
            "main_review": "I think this paper has the following weak points:\n1. The word \"logic\" is misused. Obviously, the paper considers the additional information of dependency parsing rather than logic. Secondly, the information in this paper is not \"fact\", they should be called word dependency. Of course, the naming is not a very serious issue. I just hope that the author's naming will not mislead the readers.\n2. The main drawback of this paper is that it does not convince me that dependency parsing is a very important additional information for pre-training language models. According to previous studies [1], the pre-training of BERT actually already contains morphology, syntax, and semantics information. And the current state-of-the-art dependency parser is even built based on pre-trained language models. This is what I suspect motivates the further introduction of dependency information. In particular, this paper does not specify what exactly the introduction of dependency parsing information solves for the original masked language modeling task. For reference, [2] introduces span information (simpler morphology information than dependency), thus making pre-training more reasonable and difficult.\n3. Equation (1) should be rewriten. Since ma and mp are blanks, why can they be used as conditions?\n4. The logical connective masking proposed by the authors is very strange. It is irrelevant to their motivation based on fact and logical graph as stated in the previous sections.\n5. I do not think that the authors' experiments support their motivation very well. First, with reference to SpanBERT, which introduces span information, the effect of PROPHET is degraded. Second, the authors do not experimentally verify what exactly the newly introduced dependency parsing solves. The example in Figure 5 does not give an in-depth insight to me.\n6. In the experiments of Figure 3, it is not reasonable to compare the attention matrix of token level only. Because some dependency information may be represented by higher levels of BERT.\n7. in equation 3, the authors did not consider the original masked language modeling as the loss, which I think should be helpful.\n\n\n[1] Bert rediscovers the classical nlp pipeline\n[2] SpanBERT: Improving Pre-training by Representing and Predicting Spans",
            "summary_of_the_review": "In summary, I think the motivation for adding additional dependency parsing has not been adequately articulated. And the experiments have not fully verified what the dependency parsing information actually solves. The simpler SpanBERT is in fact better than this paper. Therefore, I think this paper does not reach the threshold of ICLR.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}