{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper introduces sparse modeling-inspired regularizations to improve deep neural network-based image generators. Experimental results on both (low-resolution) image synthesis and deep image prior-based inverse problems are used to validate the proposed method.\nThe majority of the reviewers were against the acceptance of the paper. As summarized by Reviewer tsoA: \"There are shortcomings in the overall concept as well as its evaluation. The findings suggest that this might be a promising avenue of research, but it would need to be taken further. At present, the paper boils down too much into simply adding a simple regularizer at the end and observing that it somewhat improves some metrics in a limited number of scenarios. Due to the limitations of the evaluation, it remains unclear whether the proposed improvement carries over to state of the art models and datasets. Similarly, the promised elucidation of the purpose of the feature values never really materializes.\" The AC agrees with that summarization and recommends rejection."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This work proposes a sparsity enforcing structure for GANs by splitting layers into the generator for the sparse vector and for the final image and by training with sparsity enforcing regularizer in the latent space between them. The proposed method yielded improved performance over conventional methods in FID, IS for generators and PSNR for denoising with DIP.",
            "main_review": "Strengthes:\n- The proposed method is neat and did improve the performance of other GANs with the proposed structure and the loss.\n\nWeaknesses:\n- It is not easy to see clear differences between the work of Mahdizadehaghdam and the proposed work. Even though the authors argued that they are different, but there is no clear supporting arguments theoretically and experimentally. CSC / ML-CSC can be seen as an advanced version of other dictionary based methods, but it has been used in other deep learning related works (see LISTA-CPSS, ALISTA) and employing CSC / ML-CSC into the framework does not seem like a major contribution.\nSee also the following work:\n[Zhou] Zhou et al., SPARSE-GAN: SPARSITY-CONSTRAINED GENERATIVE ADVERSARIAL NETWORK FOR ANOMALY DETECTION IN RETINAL OCT IMAGE, IEEE ISBI 2020.\n- It was claimed that \"We propose a novel sparse modeling interpretation of image generators for better understanding them.\", but the work of Papyan et al., 2017b does not have to be limited to non-generators, so it is hard to see this as a contribution of this work.\n- Results on CIFAR-10 seem too weak considering recent advanced in GANs for high resolution images. It is not easy to be convinced that the proposed method works well since images are too small (is the small image sparse?)\n- DIP is not a good method for denoising and usually requires multiple realizations for ensembling to boost the performance. Moreover, DIP requires early stopping for good denoising performance, while the sparsity constraint may affect this issue instead of purely enforcing sparsity. There are also prior works on DIP using other regularizers such as the following:\n[Liu] Liu et al., IMAGE RESTORATION USING TOTAL VARIATION REGULARIZED DEEP IMAGE PRIOR, IEEE ICASSP 2019.\n[Mataev] Mataev et al., DeepRED: Deep Image Prior Powered by RED, ICCVW 2019\nSo, for fair comparisons, it should be compared with these instead of vanilla DIP.\n\nComments:\n- How is the sparsity guaranteed during testing?\n- It is recommended to show images for visual comparisons.",
            "summary_of_the_review": "The idea of this work is to enforce sparsity in CSC domain in the latent space of GANs and this work showed the effectiveness of it. However, this idea is very close to the work of Mahdizadehaghdam and there are other works to promote sparsity in latent space such as [Zhou]. Moreover, it was tested with very small images, so it is not easy to see this work working well and the experiment with DIP seems to contain some potential issues in terms of iteration number and fairness.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes viewing CNN's through the lens of sparse coding. In practice it mostly boils down to encouraging sparsity in the last layer activations. To this end the paper proposes three different mechanisms that promote sparsity. This is shown to improve the FID scores somewhat in one dataset with a range of different GAN architectures. The ideas are also applied to another generator-like DIP scenario.",
            "main_review": "Overall the paper is mostly well readable and understandable, and I enjoyed reading the background section on the sparse convolutional models. Perhaps I would include some more technical detail (sizes of layers and filters, regularization weight, etc) in the appendix. The FID gains also seemed potentially promising, but on closer thought I have some significant reservations about the paper, that prevent me from giving a higher score at this time.\n\nThe first half of the paper is framed around understanding and revealing the meaning of the internal activations of deep generative CNN's. As far as I can see, the findings fall short from this goal. They don't address the intermediate activations beyond the last ones, and for those, the reader doesn't really get an understanding of whether and how the meaning has changed or been elucidated. It is not enough to simply argue that sparse coding finds more meaningful features and filters, and therefore this must also happen when they are combined with CNN's. Did it actually happen and how would we tell? Can we more clearly attribute this or that meaning to different feature channels now?\n\nThe expecatation set up in the first five pages was that it would be revealed that generators have been secretly doing sparse coding all along. However this was not really the case, and it seems that the proposed \"interpretation\" is more like a specific way to decompose the last layers so that a sparsity prior can be inserted. No sparsity seems to be really happening by default (other than maybe via relus zeroing out values, but it's arguable if this is genuine sparsity). From there on the paper mostly focused on showing that adding the regularization improves the metrics.\n\nInserting the point of sparsity specifically at the end of the network seemed a bit arbitrary to me, and even if this were justified from some principle, it may be significantly limiting in practice. For a modern GAN with a reasonably high-resolution dataset (unfortunately the only experiments are on the extremely low-resolution CIFAR, more on which later), the meaning of the last layer is arguably rather minor, at least from the standpoint of sparsity of the local neighborhoods. It no longer affects any of the global structure of the image, and effectively just overlays different versions of the same image that have already been built earlier in the network. The filters themselves have very little opportunity to affect anything beyond the finest sharp detail in the full resolution. For this reason it is somewhat unclear if any \"meaning\" can be imposed on that end of the activations, and what exactly the role of sparsity should be there.\n\nIn the DIP experiment, as far as I can see, sparsity is applied all through the network. Why is this not something one would consider with GANs also? It's also not clear why the sparsity should apply specifically to the encoder and not the decoder -- and if it were to apply to the decoder, why wouldn't it apply to the similar looking generator also? And again, are the internal activations easier to interpret in the proposed DIP compared to the original? The metric improvement is an interesting effect, but the promised understanding of the network internals is forgotten in practice.\n\nFigure 4 seems to indicate that there is a significant amount of sparsity even in the baseline model with no regularization. I assume this is because the relus set part of the values to zero? Given that there is such a strong source of apparent (and in my opinion, questionably meaningful) sparsity in the system already, the paper should clarify the relationship between these things. Also, what would happen with some pre-activation that didn't zero out any values, e.g., leaky relu? Wouldn't clamping the lowest ones just end up mostly cutting away the leaked tails, to little effect? What about tanh that produces both positive and negative values?\n\nThese shortcomings would be more acceptable if the results were striking in the end. It is true that some consistent improvement to the FID has been achieved in some cases, which is potentially promising and could warrant a closer examination. Unfortunately the evaluation is solely centered on the CIFAR dataset, which is a reasonable benchmark to include in a GAN paper, but hardly enough if it is the only one. Findings on such toy datasets do not always generalize to cases of actual interest. In this case, CIFAR has specific properties that may hide some issues in the proposed method. The very low resolution may work in favor of this approach, because now even the final filters have a fairly wide footprint, and they can still make meaningful contributions to the overall content of the image. But will this effect be diluted away with realistically sized images, where the filter footprints are on the order of a hundredth of the image size? (See my more detailed comments on the intermediate vs final layers, above.) Also, some very relevant methods have been excluded from the analysis, in particular the entire StyleGAN line. It would be important to know if the advantage persists there, as these are in widespread use.",
            "summary_of_the_review": "There are shortcomings in the overall concept as well as its evaluation. The findings suggest that this might be a promising avenue of research, but it would need to be taken further. At present, the paper boils down too much into simply adding a simple regularizer at the end and observing that it somewhat improves some metrics in a limited number of scenarios. Due to the limitations of the evaluation, it remains unclear whether the proposed improvement carries over to state of the art models and datasets. Similarly, the promised elucidation of the purpose of the feature values never really materializes.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a CNN-based (specifically GAN-based) solution to tackle image synthesis using sparse coding concept. The proposed method utilises the generator from a generative adversarial network to synthesise a sparse representation, i.e. the sparse code, for image synthesis. This method proposes to split original ML-CSC into two consecutive function learning. It claims that the two functions are easier to learn. In addition, this paper also proves and adds image regularisation such as DIP into the framework. Extensive experiments demonstrate that the proposed methods outperforms state of the art image synthesis methods including traditional sparse coding methods and deep learning based methods. \n\n",
            "main_review": "Strengths\n1). This paper has pointed out the inefficiency of existing multi-layer convolutional sparse coding models, which is to decompose the image synthesis function into two consecutive yet independent processes. This finding is solid and backed with proper proofs. \n2). Extensive experiments demonstrate that this method outperforms the state of the art baseline methods as suggested in the claims of this paper. \n\nWeakness\n1). There is one key assumption in the sparse coding that the latent code between backbone network and output is sparse when the output image is synthesised at the best performance. However, it may not be necessarily this case. The authors are suggested to prove first that the latent code layer must be a sparse vector (or  tensor)\n2). Usually, PSNR or SSIM are also used for evaluating the image restoration tasks. It would be great to provide the scores of these two traditional metrics in the paper in order to have an all-around evalutions.\n3). The paper does not clearly disclose the detailed parameters of the proposed method, for example, what is the selected threshold  $\\lambda$ value in L0 norm? And the reason to use these threshold ? What are the weighting terms of the three different normalization constraints?",
            "summary_of_the_review": "This paper generally proposes to improve the existing multi-layer Convolutional sparse coding frameworks by splitting the tasks into independent and consecutive components. The idea is novel and is supported by proper experiments. However, the authors are suggested to address the concerns in the weaknesses. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "There  are no ethics concerns about this paper. ",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper describes image generation as a sparse coding reconstruction process. The authors formulate the sparse coding as a core block of a convolution image generator and test several different approaches for enforcing sparsity in the trained network. The experiments show that the method can improve FID scores of common GAN networks and it can also increase PSNR of image reconstruction using deep prior when compared to the same network architectures without sparsity objective.\n\n",
            "main_review": "1) The paper is overall well written. I had no problem following the text, the exposition is clear and I detected minimal amount of grammar or typo issues.\n\n2) The method is relatively simple and it could possibly be widely adopted if the experiments in the paper generalize to a wider range of problems. The paper only tested two scenarios be it with a range of baseline architectures. The paper does not discuss any downsides of the technique which I find a bit unexpected. Optimization with multiple objectives will typically introduce some kind of trade-off. In this case we are getting \"better\" (based on some metrics) images but at what cost? Did the sparsity change the convergence rate? Did the behavior of interpolation in the latent space change? Or is everything just strictly better?\n\n3) The simplicity of the method could potentially be viewed as a downside of the paper since it turns up to be just a regularization scheme applied to standard architectures. E.g. Figure 5 shows standard U-Net where additional losses and/or L0 filters are applied where necessary. Since both L1 and L0 metrics are not new it makes the paper look a bit more like an experimental survey than a completely new technique.\n\n4) I miss any qualitative comparisons of the synthesized images. The authors claim that the ML-CSC scheme leads to a significant improvement but from the tables 1,2 alone it is quite impossible to say if it is really a significant improvement or a minor improvement. Not to mention the statistical implications of the term \"significant\" without statistical analysis.\n\n5) The sole reliance on PSNR as a metric without providing visual examples is even more troublesome in the DIP experiment. PSNR (as much as L2 loss) tends to be biased towards blurry results and it is generally not very well correlated with human perception of image quality. It would be fitting to use a more sophisticated image quality metric such as LPIPS or VDP. However, showing an example result would surely be the easiest option. Judging from the numbers in Table 4, I would expect the differences to be quite small, perhaps even indiscernible. \n\n6) The paper does not provide details on how each of the baseline architecture was modified to incorporate the (ML-)CSC technique. I cannot see a supplemental document that would fill these gaps but the authors promise to provide code for all the experiments later. It is, however, not available at the time of review.\n\n\nMinor editing issues:\n- Abstract: \"leading to state-of-the-art performance\" - performance in what?\n- \"a meaningful such regularization\" -> remove such\n",
            "summary_of_the_review": "The paper is exploring a simple but elegant idea of formulating the typically black-box process of CNN image generation as a process of sparse dictionary decoding. I like this concept and I like that it very simply improves performance of established GAN methods. The paper is also easy to read which should make this trick accessible to the community. On the downside, the technical contribution does seem boil out to a nicely packaged sparse regularization scheme. The authors could have spent more effort analyzing the learned coding. What dictionary atoms were extracted and how meaningful they are? The experimental section covers the main check-boxes including relevant baselines and ablation of the technique but it falls short in presenting the results. The judgment of the performance is limited to a very narrow scopes of quantitative metrics and it is hard to make any conclusion about the actual magnitude of the improvement that is introduced. Furthermore, the authors also do not discuss any limitations. \n\nOverall, after weighing all the cons and pros I am very slightly leaning towards positive outcome. It is mainly because the idea is clear, elegant and at least to the presented extent working. The evaluation is limited in scope but it appears technically valid. I would also expect the authors to address some of the concerns in the final revision.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}