{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This work aims at giving a systematic evaluation of different unsupervised domain adaptation methods on time series classification tasks under a fair setting. By providing extensive experiments on various datasets, competitive baselines, and model selection approaches, this paper has the potential to facilitate future research on this topic if the mentioned concerns are well addressed.\n\nAfter rebuttal and discussion, the final scores were 3/5/5/5/5. AC considered all reviews, author responses, and the discussions, as well as reading through the paper as a neutral referee, and reject the paper based on the following concerns:\n+ *Model Selection Criterion*: As stated by the authors, employing labeled target data for model selection will violate the fundamental assumption of unsupervised domain adaptation. However, the proposed Few-Shot Target Risk (FST Risk) also requires labeling a few target domain samples. If it is possible, why not directly conduct semi-supervised domain adaptation?\n+ *Experiment Details*: As a benchmark paper, it is extremely important to carefully design the experiment details to attain promising results. Among these details, a suitable network backbone for time series classification (Is CNN or ResNet-18 the best choice?  Or TCN mentioned by Reviewer f7Xp), large-scale datasets with considerable domain gap, and evaluation metrics are the first consideration to attain insightful findings.\n+ *Novelty or Interesting Findings*: As pointed out by reviewers, it is obvious that the technical novelty is limited but it may be okay for a benchmark paper if solid/interesting experimental results are observed. However, some of the findings are also fragile and the experiments should be carefully conducted to make them more solid.\n\nIn summary, this paper studies a promising research direction of domain adaptation, but the work cannot be accepted before addressing the reviewers' comments. The weaknesses mentioned above will have a high probability of being asked by the reviewers of the next conference. So the authors need to make sure that they substantially revise their work before submitting it to another venue."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This work proposed a systematic framework of (unsupervised) domain adaptation for time series data, with various model selections and strategies.",
            "main_review": "Strength:\n- include extensive baseline for the experiment\n- open-sourced the code\n\n\nWeakness:\n- did not clarify what is the specific challenge in time series data beyond static data, and how to remedy them as contributions?\n- does not cover other main time series applications: time series forecasting scenarios.\n- did not cover or compare with the recent work (DAF) of domain adaptation for time series by Jin et al. https://arxiv.org/abs/2102.06828 \n--  did not justify well why domain alignments are important well. In DAF, it mentioned having both domain specific and invariance features are critical.\n-- what if using different backborns, such as attention-based ones?\n- hard to understand why many criteria are proposed. what is difference between SCC and DEV risk? when to select one of them?\n- difficult to interpret the experiment results\n-- what is visual UDA in the table 3? \n-- why were average values reported?\n- the observations are not much informative \n-- one on imbalanced data is not connected to model selection criterions. \n-- one of backborn selection is kind of implying non-systemtic aspect. What is the real take-away out of this? may it recommend to use other possible backborn, e.g., attention-based one?\n- hard to understand why authors emphasize fair and realistic procedure. Are any of other methods not fair or realistic and why?\n- ",
            "summary_of_the_review": "The paper is not well-written especially experiments parts, and the message of our them are not clear in the line of overall paper take-away.Therefore, the contribution seems to be not clear beyond aggregating all existing DA methods and systematically deploying them. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a systematic evaluation framework named ADATIME, which systematically evaluates different unsupervised domain adaptation methods on time series data. The whole framework consists of a feature extractor, a classifier, and a domain alignment component. The paper conducts large-scale experiments adapting the state-of-the-art visual domain adaptation methods to the proposed framework on time series classification tasks. The findings based on the experimental results reveal the key points of applying UDA to time series data.",
            "main_review": "Pros:\n1. The paper comprehensively adapts the state-of-the-art UDA methods to time series classification tasks. \n2. The paper proposes a novel UDA framework for time series data, which may contribute to the community.\n3. The idea of this paper is novel to some extent.\n\nCons:\n1. My main concern is the motivation. The paper only talks about how the framework is designed, while not elaborating clearly on why the framework is designed like this.\n2. The selected datasets are relatively too small and simple that the backbone network can only be a 1D-CNN in order to avoid the overfitting phenomenon. The experimental results on these toy time-series datasets are unconvincing.\n3. The organization and writing of this paper should be improved. The authors should pay more attention to the motivation and the details of the framework. \n4. It would be better if the authors explain some symbols like $X_{train}^{S}$, $X_{test}^{S}$, $Z_{train}^{S}$, $Z_{test}^{S}$, ..., etc. in Figure \n5. Some related works are missing: \n\n[1] Ruichu Cai, Jiawei Chen, Zijian Li, Wei Chen, Keli Zhang, Junjian Ye, Zhuozhang Li, Xiaoyan Yang, Zhenjie Zhang:\nTime Series Domain Adaptation via Sparse Associative Structure Alignment. AAAI 2021\n[2] Xiaoyong Jin, Youngsuk Park, Danielle C. Maddix, Yuyang Wang, Xifeng Yan. Domain Adaptation for Time Series Forecasting via Attention Sharing. arXiv:2102.06828\n\nMinors:\nOn page 2, “the the following questions\" -> \"the following questions\"",
            "summary_of_the_review": "The findings are interesting and may contribute to the community. However, considering the motivation and the unconvincing experiments, I vote for \"5: marginally below the acceptance threshold\".",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper explores the unsupervised domain adaptation of time series data (TS-UDA). And it focuses on the benchmark construction. By standardizing the base model, datasets, and model selection, this paper provides a good benchmark of TS-UDA. This benchmark can facilitate future research. Also, the paper proposes some findings.",
            "main_review": "## Strengths\n(1) This paper evaluates the previous domain adaption algorithms under a fair setting. Extensive experiments are provided. Also, the experiment results present some competitive baselines to the TS-UDA area. This benchmark will be helpful to future research.\n\n(2) Based on the experiment results, this paper provides some findings or analyses.\n\n(3) The paper is well organized and with clear clarification.\n\n## Weaknesses\n(1) The main concern is the technological novelty of this paper. This paper adopts previous methods in the time series datasets, which is technologically trivial. Especially, the proposed AdaTime framework is also trivial. Can you further clarify the difference between your AdaTime framework and the standard domain adaption protocol in image classification? It is hard for me to distinguish them. That is why I think the AdaTime is not novel. More elaborations will be very helpful for my judgment.\n\n(2) Some of the findings are also fragile. \n- Performance Gap diminishes with a sufficient amount of data: this clarification is not persuasive for me. Your conclusion is obtained from the comparison among three different datasets. It would be helpful if you controlled the variables. For example, you can keep enlarging the size of one fixed dataset and record the change of results.\n- Model selection has a significant effect on performance. I think your experiment results are also affected by the limited data. The three experiment datasets are too small to provide a robust result. Larger datasets should be included, such as the HHAR dataset used in CoDATS.\n\n(3) I am not sure about the contribution of this paper when the backbone is just CNN. Firstly, as shown in Figure 3 of the main text, different backbones result in quite different results. Not only the numerical values but the relative performance is also changed. So I think the findings of your paper may also be changed under other base models. Secondly, as your mentioned, the time series area does not have a consistent backbone. Thus, the experiments are lacking in the backbone aspect. In conclusion, I think more baselines are needed if you want to obtain a general conclusion, such as the TCN.\n\n(4) As for the metric, F1-score is a well-established convention in the time series area, such as anomaly detection and recommended system. Maybe F1-score is not popular in vision, but I still think the metric finding is too trivial.",
            "summary_of_the_review": "\nThis paper provides a useful benchmark for the TS-UDA task. Extensive experiments are included. But because of the concern of technology novelty and the unconvincing evidence of some findings, I would like to reject it.\n\n--- after discussion---\n\nThe author addressed part of my question. But I still think this paper is below the expectation. I would like to arise the rank to 5.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper introduces , a standardized framework to systematically and fairly evaluate different domain adaptation methods on time series data.",
            "main_review": "“Deep learning has achieved a great success in time series classification tasks, assuming access to a vast amount of labeled data for training” I am not totally convinced that this is true. The improvements over simple nearest neighbor models etc is not great. \nIn the related domain of anomaly detection, a recent paper makes a forceful claim that most of the apparent success of Deep learning for time series anomaly detection is nonsense [a]\n\nThe paper contains a nice “bake off”, but the main finding “Visual UDA methods achieve comparable performance to TS-UDA methods on time series data.” is unsurprisingly.\n\nI am very curious about how well one could do with a much simpler methods.\nFor HAR the class are WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING. It is easy to tell LAYING from any of the dynamic classes, simply by the fact that the variance of LAYING is less than one tenth of the dynamic classes. It is easy to tell LAYING from the other classes because G (the acceleration due to gravity), sifts from one axis to another etc. \nI understand that simply maximizing accuracy is not the full point of the paper, but I am still curious of we really need deep learning here.\n\n\n“In addition, we find that model selection plays a key role and different selection strategies can significantly affect performance.” It would be surprising of that was NOT true.\n\nHowever, I do think the experiments as detailed and forceful, and the community may find them useful\n\n\n[a] Renjie Wu, Current Time Series Anomaly Detection Benchmarks are Flawed and are Creating the Illusion of Progress.\n",
            "summary_of_the_review": "While the novelty is low, the experiments as detailed and forceful, and the community may find them useful\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents an empirical approach for unsupervised domain adaptation of time series data. The paper points out some of the drawbacks of existing approaches due to inconsistencies in evaluation schemes, datasets, model selection rules, and base neural network architectures. The paper then presents adaptations of visual domain adaptation methods for time series data. Experimental results using ten state-of-the-art methods on three benchmark datasets spanning fifteen cross-domain scenarios are presented.",
            "main_review": "Strengths: A systematic experimental approach  for time series domain adaptation using multiple datasets and baseline architectures.  Comparisons between visual domain adaptation methods and time series-unsupervised domain adaptation methods are given.  Guidelines for future research are given. ",
            "summary_of_the_review": "While the authors' efforts in systematic experimental evaluation of existing visual domain adaptation algorithms and time series-unsupervised domain adaptation algorithms is commendable, there is no theoretical understanding at even the most basic level. hence the conclusions drawn from the experiments may be specific to datasets and baseline architectures used. The conclusions do not generalize.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}