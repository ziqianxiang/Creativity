{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper initially received negative reviews; the authors did a good job during the response period: two reviewers have updated their scores to 6. The AC has carefully read the reviews, responses, and discussions, and agreed that the authors have also mostly addressed the concerns of reviewer gsUt as well. It is unprofessional for reviewer gsUt to not engage in discussions after multiple requests.\n\nThe AC however also agrees with reviewer seqp that the new changes are major, and submissions are supposed to be evaluated in their initial form. Further, neither of the positive reviewers would like to champion the paper. \n\nThe final recommendation is to reject the paper. The authors are encouraged to further improve and flesh out the paper based on the reviews for the next venue."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper build on recent progress in implicit object representations and where the representations are trained(or fine-tuned) along with a feature head on down stream manipulation tasks. Experiments are conducted by using this trained representation within an LGP formulation solved with Gauss-Newton optimizer to find plans for manipulation problems.",
            "main_review": "Strengths\n+ The idea is interesting and a natural progression from recent progress in 3D vision/representation learning and is a welcome change of pace where a vision problem is contextualized in a robotic task and developed with the intent to be a part of the full system.\n+ The related work is well covered and the motivation is setup clearly i.e. we need vision backbones that work for manipulation tasks.\n\nWeakness/Comments\n- The interesting task-specific and task-agnostic dynamic is brought up but not really explored within the paper in the context of representation learning. Seems like a missed opportunity.\n- While the broad strokes are clear (Fig 1, 2, Eq 4, etc) it was a bit hard to parse how the various components came together, in particular when instantiated given a particular task. At inference time, does the planning problem just take one image as input? is the image ego-centric? what are other inputs and do any of them fall under assumptions (like a 3D environment representation). \n- Some more algorithmic details would be helpful. How to choose what set of points are used to condition PIFO and what if this choice changes between tasks? Could something like Mask-RCNN work for object segmentation vs Eq 5?\n- Some more experiment/result details would be helpful. Why 2 view task performance drops compared to (1 view?) PIFO? In the Zero-shot imitation what's 'unseen' in test setting, new object geometry, anything else?",
            "summary_of_the_review": "Interesting idea and application, results are promising, but the write-up could be improved to understand the approach clearly.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies the problem of learning representations for robotic manipulation tasks. The authors developed a method that represents objects as neural implicit functions. To method is trained in a data-driven fashion that learns the training pipeline from camera images to interaction features end-to-end. At each time step in a motion planning task, the implicit function is queried at interaction points that are attached to the robot frame. With the pixel-aligned representations, the representations at a certain spatial location correspond to the associated pixels of the images. Experiments are conducted on a mug hanging task. Videos in simulation environments show the effectiveness of the proposed method.",
            "main_review": "Strengths:\n1. The motivation from neural implicit functions is interesting.\n2. The videos show that the method works in the mug hanging task.\n\nWeaknesses:\n1. This paper is very difficult to follow.\n2. It is very difficult to understand the framework by looking at Figure 1. Specifically, there seems to have 3 blocks in Figure 1, but in Section 3.1, the authors stated that the proposed framework consists of two parts. This is confusing and makes readers hard to follow.\n3. In the introduction, what does \"key interaction points which are rigidly attached to the robot frame\" means? How can a point attach to a frame?\n4. The related work section is too long (more than 1.5 pages).\n5. The method is only trained on different mugs. It is unclear how well the method would perform if given data of other categories.\n6. Since this paper studies a robotics problem, it is not convincing without showing video results using real robots.\n7. In Figure 1, I do not understand how to compute forward kinematics for a given robot pose. Forward kinematics are typically computed when given joint angles for deriving poses. I believe this is a clear technical error.\n8. The authors mentioned in Section 4.1 that the posed image data consists of 100 images. The authors should include an ablation study that analyzes how many images are needed. Is 100 sufficient? \n9. How are ground-truth SDF values computed? The authors did not mention nor providing references in Section 4.1.\n10. Since the method is only trained on 131 mesh models (as mentioned in Section 4.1) which is a very small dataset, I wonder if there is a possibility that overfitting happens. ",
            "summary_of_the_review": "Although the problem studies by this submission is interesting, this paper is very difficult to follow. There are some details missing and some technical errors in the paper. There are also some unclear details and missing experiments mentioned in the above section.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The method proposes an implicit-field-function-based representation, which can be directly inferred from the camera images and be used for robot manipulation. The proposed method infers the object representation by querying about the implicit features of some pre-defined key points. The architecture projects the 3D query point into the 2D images and then exploits the pixel-wise features. ",
            "main_review": "Strength:\n1. The overall idea of combining pixel-wise image features, the implicit function over the 3D space, and task guided learning scheme makes sense to me. However, the separate idea of each component may not be very novel.\n\n2. The authors present some experiments (with videos) to illustrate their method.\n\nWeakness & confusions:\n1. The authors only compared their method with some ablated versions (i.e., global features, SDF representation) and didn't compare with other methods:\n    (a) methods also propose object representation for robot manipulation. Besides the works mentioned (but not compared) in related work, SORNet[1] is also a work needed to cite and compare.\n    (b) traditional baselines: pose estimation + heuristic landmarks + motion planning + (RL) + (grasp point proposal). There are lots of strong baselines for the pick and place style tasks. What's the advantage over them, and why not compare with them?\n\n2. It's not clear how the learned features are incorporated into the LGP framework. What are the detailed examples of h_path, g_path, h_switch, and g_switch (equation (4)) for the grasp and hang experiments?\n\n3. The current representation only cares about the relationship between the object and the pre-defined key points (e.g., gripper and fixed hook). It doesn't capture the relationship between the objects (e.g., whether object A is on top of object B, whether object A will collide with object B).\n\n4. It's not clear what will happen when the query points are out of images. What features shall we get?\n\n5. It's not clear about the runtime of the LGP framework. Is it efficient to optimize or time-consuming?\n\n6. The definition of \"learned features\" is not clear. Grasp feature and hang feature are pose distances between the current pose and the sampled success pose? Distance in SE3 or joint space? What about the collision feature?\n\n7. The definition of the \"feasible rate\" (in table 1) is not clear.\n\n8. For the \"zero-shot imitation\" experiment, where are the grid points placed? \"centers of the target and the actual bounding balls\" is not clear. Only the ball center is used? \n\n9. For POSEICP, what's the result of only using Euclidean distance without feature space.\n\n\n\nMinor comments:\n\n1. Please adjust the presentation near equation (1) to make it look neat.\n\n2. Page 5: \"Similarly to the aforementioned image data augmentation\", but data augmentation is not mentioned yet.\n\n3. The location of the cameras is not clear. Where are they placed?\n\n4. In table 1, why is PIFO (2 views) worse than PIFO?\n\n\n[1] Yuan, Wentao, et al. \"SORNet: Spatial Object-Centric Representations for Sequential Manipulation.\"  ",
            "summary_of_the_review": "The authors propose a new method for capturing object representation directly from images and used for object manipulation. The idea of each component is not very novel. More importantly, the experiments miss baseline algorithms. I am not very convinced of the practical contribution or benefit of the proposed method. Can it solve more problems or outperform the existing method? ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a method that integrates neural implicit functions (NIFs) with planning methods for robot manipulation. A neural implicit function that represents geometry with SDFs, grasp scores, and hanging scores is learned. This is then integrated with a planner based on Logic-Geometric Planning (LGP). Experiments are run to test mug hanging in a few different scenarios, including hanging a single mug and hanging a mug with handover.\n",
            "main_review": "Strengths:\n- Neural implicit functions are still quite new, and investigating their use for robotic manipulation is a valuable direction. In particular, I think the idea of incorporating NIFs with LGP is worth exploring.\n\nWeaknesses:\n- There are not enough quantitative results in the paper to convince the reader that this method is better than others. Also, hanging mugs only is a relatively narrow set of experiments (as the paper is titled \"... for robotic manipulation\").\n- There are no comparisons to other works. Since the introduction mentions that traditional methods (that use traditional representations such as meshes or point clouds) has limitations, a comparison should be made with such a method. Further comparisons should include the Omnihang paper (You et al.). Additionally, the ablations are not that meaningful. It is well known that dense features will give better results than a single global vector (or a single-vector representation of an object). \n- The exposition could be more clear. For example, PIFO predicts graspness scores and hang-ness scores, however this is not mentioned in the introduction. The introduction is rather vague. Additionally, it is not clear how exactly the NIF is used in Equation 4. This seems to be a major contribution of the paper, yet it lacks detail.\n\n\nQuestions/Comments:\n- In P1 of Section 1, a stated problem with traditional approaches is that \"the representations have to be inferred from raw sensory inputs like images or point clouds\", however this contradicts the fact that the proposed method infers the NIF from images. Additionally, the last line of P2 also directly contradicts this.\n- \"Interaction features\" should be defined somewhere, it is not clear what this means in the introduction.\n- The introduction is lacking citations. For example, NeRF (Mildenhall et al.) should be cited when discussing it.\n- In Section 3.5, there has been no discussion of data augmentation up to this point. This is confusing, please clarify this.\n- In Eq. 6, what is w? This is not explained in the main paper nor the appendix.\n- It is unclear how the graps/hang interaction points of the gripper/hook are used with the neural implicit function. How is the implicit function used to evaluate a grasp or a hang? Please clarify this.\n- In 5.1, the ablations (global latent, vector repr, SDF repr) have not yet been introduced. Please re-arrange sections for better flow of the text.\n- Some graphics of the baseline networks in the appendix would be helpful for the reader to better understand the baselines and what is being ablated.\n- The handover experiment for hanging is complex and interesting. It would be interesting to expand on those experiments and show that other baseline methods cannot solve such a complex task.\n- typos:\n    - P2 of 2.2: \"trowing\" -> \"throwing\"\n    - Eq. 5: I believe it should be Mn(u,v)=1\n    - P2 of 5.1: \"interaction\" -> \"intersection\"\n- Incorrect Citations\n    - Barron et al. (MIP-NeRF) is ICCV, not CVPR.\n    - Trevithick & Yang (GRF) is ICCV, not CVPR.\n    - Jiang et al. (Synergies Between...) is RSS, not ICRA. \n    - I would suggest double checking all citations.\n- Missing related works:\n    - Henzler et al. \"Unsupervised Learning of 3D Object Categories from Videos in the Wild\", CVPR 2021.\n    - Reizenstein et al. \"Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction\", ICCV 2021.\n    - Van der Merwe et al. \"Learning Continuous 3D Reconstructions for Geometrically Aware Grasping\", ICRA 2020.",
            "summary_of_the_review": "While I like the direction the paper is investigating, I do not think the work is mature enough yet to warrant publication at ICLR. More experiments, comparisons, and clearer exposition would definitely make this paper a better candidate for publication.\n\n---- UPDATE -----\nRaised score from 3 to 5.\n\n---- UPDATE -----\nRaised score from 5 to 6 in light of comparison to hand-engineered methods.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}