{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The authors study the training settings that may affect active deep learning performance, including code/warm start, leveraging unlabeled data, and initial set selection, for each active learning strategy. The findings on several data sets help understand AL more, with some pieces of insights to inspire future research.\n\nThe reviewers were at best lukewarm about the work prior to the rebuttal. Some turned more positive but none were willing to strongly champion for the paper's acceptance, even after the authors provided a decent rebuttal. This leaves the paper to be a borderline case, and the recommendation comes from carefully checking the latest revision and calibrating its score with other submissions.\n\nThe reviewers are generally positive about the breadth of the study, the potential impact of the codebase and the systematic study that can inspire future works. Some clarified issues include comments on future research directions and the labeling efficiency plot (which is, however, not analyzed deeper in the main text), and results on additional settings like transfer learning (somewhat preliminary). In the end, two remaining concerns surround whether the technical contribution and the conclusions are sufficiently solid, including\n\n* limited insights: Some reviewers comment that the insights are on the lighter side. The authors identify several issues that may affect the performance of the underlying tasks of active learning, and find that the best setting differs across different active learning strategies. But given that the paper offers at best \"best practices of training models on actively-queried labels\", it is not clear whether the authors achieve their claimed goal of \"compare different strategies in a fair way\"---in particular, the conclusion for this particular comparison seems to be missing (e.g. which is recommended in practice, BADGE or LL4AL or others?). Also, given that only three data sets (5 after rebuttal) have been studied in this work (see item below), the \"generalization ability\" of the conclusions in this paper cannot be clearly established. While the authors provided some additional pieces in the rebuttal, the pieces can use more study to be fully conclusive. Some reviewers are also concerned that the conclusions are rather scattered.\n\nFrom a practical perspective, it appears to be a chicken-egg problem on whether to fix the active strategy first (and then train the model with the best setting/practice), or fix the training setting first (and then select the best strategy). The authors may want to add more arguments on why they focus on the former rather than the latter.\n\n* limited experiments: several reviewers point out that the few data sets used could not fully justify the \"best practice\", and demand data sets like ImageNet. The authors offered some new results on TinyImageNet and CIFAR100, but those are not studied as deeply as other data sets at the current point. A more careful study on the two (and other) data sets are thus strongly recommended."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, the authors provided a study on how to effectively utilize the existing active learning (AL) methods. Specifically, the authors conducted experiments on the performance of different AL approaches with different training strategies and data manipulations. \n",
            "main_review": "Pros:\n1) The paper can be used as a guidance to utilize AL methods. \n2) The conclusions drawn in the submission can be used to design and improve the performance of AL methods. \n\nCons:\n1) The paper is more like a technical report where different experiments are conducted and corresponding results are reported. Minimal analysis or insights are given to discuss why the results are obtained. For example, the authors give a conclusion where LL4AL should use a warm start. But what exactly affects the warm start to be effective for different AL methods and datasets? What results could be expected on other datasets that are not included in this paper? \n2) Can the conclusions be generalized to other AL methods? If so, it would be interesting to see the authors using the conclusions to construct an AL system to achieve better performance.\n\nMinor:\n1) The font size is too small for the figures to be read. Specifically, please enlarge the font size in Fig. 1,2,3,4,5,7,8.\n2) Please consider to put different shapes or use different line types for the methods in comparison for better presentation.",
            "summary_of_the_review": "To sum up, the paper can be employed as a practical guidance for researchers to conduct AL for image classification. However, minimal analysis or insights are given for the results to be generalized to other datasets or methods. Thus, my initial rating on this submission is borderline reject.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper provids a codebase for fair comprisons of existing Active Learning methods, and performs a lot of comparative studies on the various influence factors for Active Learning. Some new observations are yielded and help figure out the optimal practices of evaluating AL methods.",
            "main_review": "The experiments are extensive and valuable for the better understanding the existing strategies of training Active Learning models. Compared to previous codebase, this work seems to show some different observations and insights. Meanwhile, the conclusions about  various factors in AL methods are insightful and useful for guiding the development of AL strategies.\n\nSome concerns:\n1. Is it possible to extend these experiments into more real and complex image datasets, like ImageNet?\n2. The conclusions or observations are straightforward, but scattered. Is it possible to provide an overview of the importance of various setting, like training setting, dataset configuration?\n3. It would be better to discuss the potential research direction, considering the properties of existing AL methods and datasets.\n\n",
            "summary_of_the_review": "Overall, the experiments are extensive and produce many findings that could help understanding of existing pool-based AL methods. The new codebase could provide solid benchmarks for making fair comparisons. Although some adjustments may make a better contribution, this paper is marginally above the acceptance threshold.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper conducts an extensive analysis of state-of-the-art Active Learning (AL) methods (Coreset, BADGE, LL4AL, JLS, and WAAL) and studies the effect of different training settings (Backbone architecture, Initializing backbone weights, Optimizer, and learning rates, with and without data augmentation) and their effect on AL evaluation for image classification. It also highlights the main factors that can influence the performance of AL methods: the construction of an initial training set following a certain strategy instead of random sampling, and pretraining the backbone of the network in an unsupervised manner. In addition, it provides solid benchmarks to compare new with existing methods in sections 5 and 6.",
            "main_review": "This article is focused on an extensive analysis of popular AL methods and studies the effect of different training settings typically used in the literature. In addition, it discusses the merit of recent trends in AL such as using unlabeled data, initial set construction, and unsupervised pre-training of the network. The work is topical and on a topic that should be of interest to readers. There are a few parts of the article that might benefit from some attention in revision:      \nThe greatest concern is with the assessment of classification accuracy. The article is lacking detail on this important issue. Most AL works (Wei et al., 2015; Sener & Savarese, 2018; Ash et al., 2019; Killamsetty et al., 2021) present their results comparing the test accuracy with respect to the number of labeled points, to highlight the benefit of the selection algorithm compared to their baselines. In contrast, this paper does not explicitly show the labeling efficiency of selection algorithms with respect to random sampling. In addition, it is not clear how many fewer labels a selection algorithm needs in order to achieve that test accuracy with respect to random sampling. I doubt this problem can be removed by the use of labeling efficiency plots and motivate the need for studying labeling efficiency in future works as a means to more holistically understand AL selection algorithms.  \nI suspect that the codebase of this article would be a useful resource for others to build on this work and help give this article impact on the community.\n",
            "summary_of_the_review": "This paper reproduces state-of-the-art results of popular AL methods with different datasets and settings typically used in the literature. It also highlights some approaches (sections 5, 6, and 7) that can influence the performance of AL methods by comparing only test accuracy with respect to a number of labelled points. It is not clear how many fewer labels a selection algorithm needs in order to achieve that test accuracy with respect to random sampling. They also provide a new Pytorch codebase that will allow future researchers to evaluate and compare different strategies in a fair way. This article lacks details on the labeling efficiency of different methods and their quantitative evaluation as done in Beck et al., 2021.\n",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "\nThe paper provides benchmarking of some of the popular active learning methods on CIFAR10, SVHN and FashionMNIST datasets. Effects of factors such as choice of backbone, data augmentation, optimizers, learning rate, cold vs warm starting are studied and the conclusions are provided as best practices. Analysis is also performed w.r.t using unlabeled data, choosing the initial labeled pool (random vs K-Means++/K-Center), and unsupervised pre-training of the backbone.\n",
            "main_review": "Strengths:  \n\n- Recent active learning methods show results using different settings, and this paper tackles the important job of evaluating these methods in a consistent manner. \n\n- The analysis w.r.t using additional modules for complementary tasks, and unsupervised pre-training is interesting and useful. \n\n- The paper is clear to understand and the most results are presented effectively.  \n\n\nWeaknesses: \n\n- When comparing different optimizers, the optimal learning rate for different optimizers can be different. Hence, the argument of using the learning rate which is better for one optimizer (SGD), also for the other optimizers (Adam and RMSProp), is not precise. This is especially concerning because the relative ordering of methods might change, and the conclusion w.r.t optimizer might not hold. \n- Similar concern as above for experiments with different backbones, as different backbones might need different learning rates to obtain the best performance.   \n\n- Other popular methods to use unlabeled data in terms of semi-supervised learning of the classifier should be compared ([1], [2]).\n\n- It would be valuable to see how the performance of these methods would change with datasets having different number of classes, which is another important factor. The three datasets studied here all have 10 classes. \n\n- It would be good to explicitly mention the active learning settings (i.e. initial pool size, budget size) in the experimental setup. \n\n- The effect of supervised pre-training of the network on other larger datasets (e.g imagenet) in section 7 would be useful and make the analysis more complete.  \n\n\n[1]: Liao et. al. Towards Good Practices for Efficiently Annotating Large-Scale Image Classification Datasets  \n[2]: Mittal et. al. Parting with Illusions about Deep Active Learning\n\n------------------------------------------------------------------------------------------\nSome typos:\n\n* Introduction, paragraph 1: it exists a large unlabeled U â†’ there exists a large unlabeled set U \n* Introduction, paragraph 2: include the for training â†’ include them for training \n* Introduction, paragraph 2: divers way â†’ diverse way \n* Section 7, unsupervised pretraining of the network: In such as setting â†’ In such a setting \n* Section 7, unsupervised pretraining of the network: Figure 10a show â†’ Figure 10a shows  \n",
            "summary_of_the_review": "I believe more thorough experimentation is needed for backing up conclusions in some parts like the effect of optimizers and backbone. It would be very valuable to compare popular semi-supervised methods in the section on using unlabeled data.  \n",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}