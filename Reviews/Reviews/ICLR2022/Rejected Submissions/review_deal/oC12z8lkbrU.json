{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "To overcome the challenge of lacking task-specific unlabeled data in semi-supervised learning (SSL) or knowledge-distillation (KD) tasks, this paper presents a new framework called \"generate, annotate, and learn (GAL)\" that uses unconditional language models to synthesize in-domain unlabeled data to advance SSL and KD. Extensive experiments on both NLP and tabular tasks demonstrate positive results of the proposed method.  \n\nReviewers generally agree on several key strengths of the paper, e.g., the paper is well-written, literature review is comprehensive, experimental results are generally positive (the improvements over the standard baselines on GLUE benchmark looks solid despite not very significant). On the negative side, some reviewers did raise some major concerns about the novelty of the proposed framework and the lack of strong baselines for comparison. For example, the proposed GAL framework doesn’t seem particularly novel as neither of the proposed components is new, and the key value of the work seems on the contribution of evaluating the large LM's ability to generate good in-domain unlabeled data (as agreed by authors). Therefore, it is very important to compare with other existing data augmentation baselines in the empirical studies. While authors did try to add one round-trip-translation (RT) data augmentation baseline for comparison during the rebuttal, more stronger SOTA data augmentation baselines should be compared. \n\nOverall, this is a good paper which is worthy of publication in near future but it still needs some more work on more extensive comparison of more baselines and improvements on the writing of novelty and contribution claims."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper identifies challenges in self-training as a lack of in-domain data (input x). To overcome this challenge, the paper proposes to \"generate\" in-domain data using large self-supervised LMs. The rest of the process, which are annotation and learning, follows typical self-training and this paper takes the approach of learning with soft-target as (Du et al.)\nThe paper examines the proposed approach (GAL) on GLUE benchmark on KD (table1), SSL on full data (tab2,3), and prompt-based few-shot (tab4), and also examines GAL in tabular tasks (tab 7). On KD, GAL usually shows the largest improvement on 1st iteration and shows minor fluctuations of performance during more iterations (Tab2, 7).",
            "main_review": "## Strength\n\n- Clearly written, Appendix also has extra descriptions of task and side experiments on image task and fine-tuning GPT2.\n- Has good survey of related work in self-training.\n- Extensive experiments on KD, self-training, and few-shot + tabular task.\n\n## Weakness\n\nThe main challenge that this paper points out is in generating (or coming up with) the in-domain data.  Given this challenge,\n\n- I am not entirely sure whether this GPT-2 fine-tuning or GPT3 generation is very novel for this paper to be accepted as ICLR paper.\n- As *generation from fine-tuned large LM* is the main contribution of this paper, I suggest the authors to compare with other possible ways of collecting this in-domain data (such as kNN or generation without fine-tuning) to contrast and highlight their scientific contribution.\n\n\nThinking about other possible ways to generate in-domain data,\n- This paper points out that it is difficult to collect in-domain data such as pair-wise sentences and tabular data is hard to get.\n    - Q) For tasks that simply require single sentence as an input, what happens if you just use NYT, wikipedia corpus or if you do kNN given training set? \n    - Q) For the pair-wise input (x1, x2), what happens if you simply do nearest-neighbor with x1, x2 given popular unlabeled texts?\n    - Q) For the tabular dataset, I am not sure if pre-training of GPT2 really adds value, what happens if you just train separate LM for the tabular data.\n\n**Other questions/comments**\n\n- Have you considered using adapters instead of finetuning whole GPT2 which seems too expensive.\n- What would be the result like if you don't really finetune GPT2 and do prompt-based generation? (not for few-shot)\n- How about using g(x) notation in figure 1.\n- Do you require such a large pre-trained model such as GPT2 for something like tabular dataset? I am not sure how GPT2 pretraining helps the generation of feature-like datasets such as tabular data.\n- Maybe inappropriate to say outperforming XGBoost on 2 out of 4 tasks? (I think showing gains are fine over transformers, but besides connect-4 in Table 7, I am not sure the comparison to the XGBoost is that meaningful. The gaps between XGBoost and RoBERTa are remaining similar)\n- Table 2, \"We see consistent improvements with more GAL iterations,\", I am not sure if MNLI and RTE show that?\n- What are the two metrics (metric1/metric2 format) for MNLI, MRPC, STS-B, QQP? I recommend that the authors denote metrics for all their tables.\n- Any reason that table 6 has part of GLUE only?\n- How big is one iteration of GAL? Does it correspond to running through all 40x unlabeled data?\n\n**Grammar**\nIf we had access to the oracle generative process, we were able to obtain the best KD and SSL results —> we could / we would be able to (?)",
            "summary_of_the_review": "I want to first thank the authors for writing a clear paper and conducting extensive experiments. \n\nThe experimental results show performance gains, however, I am not convinced that this approach is novel as it has been shown in previous work that self-training can improve performance and as we know that GPT-2, 3 can generate quality examples.\n\nTo appreciate the result more, I recommend authors to actually focus on discussing challenges in finetuning generator given training data. If they could improve the quality of generation in a novel way, maybe the paper could become more appealing. At the current state, I feel like GAL is very straightforward self-training with the unlabeled data generation with large-LM generation.\n(Additional comments:\nIn case I missed some contributions of this paper, If we remove the good quality of GPT2,3  what are the benefits that GAL is bringing as a framework?  Could you elaborate on why generating in-domain data is a novel idea?)",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a new framework of data augmentation based on generative models. The idea is to generate some new samples and then to classify them in an unsupervised manner to improve a student model. The authors provide an extreme experiment where the sentences are composed of numerical data raw extracted from UCI datasets. The authors give a very clear formalization & discussion to bridge between data generation and semi-supervised learning.\nThen, the authors provide a large experimental section that show the interest of the GAL approach. In particular for small models but also for larger ones. The experimental section is very strong, investingating several interseting ablation. The last section (5.5) seems very promising for the future, in particular for data2text applications.",
            "main_review": "++ Clear & well written\n\n++ relevant combination of different language models\n\n++ strong & relevant experimental part\n\n++ great last experiment on UCI tabular dataset\n\n-- (probable) high computation cost\n\n-- anonymization issue\n\nThe fact that GAL appears in the GLUE learderbord associated with the name XXX is a major issue if XXX is indeed the author of this article. As the GLUE leaderboard is mentioned a lot of time, this anonymization leak is important.\n\n==\n\nFew detailed comments:\n\nSection 3 about distillation is very clear.\n\nOn Fig 1., authors could mention explicitely and/or graphically that GAL correspond to the bottom path (and not to the refining of the discriminative path).\n\nDiscussion of section 4.2 is clear and valuable.\n\nAppendix are numerous and very interesting. They contain all information required to reproduced those experiments.\n\nThe fact that GAL appears in the GLUE learderbord associated with the name XXX [to be discussed privately] is a major issue if XXX is indeed the author of this article. As the GLUE leaderboard is mentioned a lot of time, this anonymization leak is important.\n\nThe authors do not discuss the computation cost of combining GPT2 & RoBERTa-large modeling.\n\n",
            "summary_of_the_review": "The proposed framework is simple, well described and very powerful. Experiments are numerous and relevant. I propose to accept this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper studies semi-supervised learning via generated synthetic data. More specifically, the authors handle the challenge of lacking task-specific unlabeled data and use unconditional language models to synthesize in-domain unlabeled data, showing the effectiveness of semi-supervised learning, knowledge distillation on NLP, and tabular tasks. ",
            "main_review": "The authors study how to handle the challenge of lacking task-specific unlabeled data in a semi-supervised learning (SSL). The problem is important and well-motivated.\n\nStrengths:\n*   The paper introduces a clear flow and comprehensive studies in the experimental section. The authors show many interesting findings. It is also surprising to see GPT-j could generate quality data with given few-shot labeled data.\n\n\nWeakness:\n*  First, the authors explicitly mention the small amount of labeled data in Figure 1.  Most of the experiments are conducted under full supervision except Table 4.  It is not very true that only a small amount of labeled data is available considering MNLI includes 400k examples and RTE also has more than thousands of labeled examples. This claim matters since data augmentation is much more challenging and important for low-resource scenarios. \n\n* The proposed solution is to leverage data augmentation to generate task-specific unlabeled data. Considering that this idea is widely explored in the existing literature, I am more interested in the most distinguishing part: the authors use the unconditional autoregressive lanaguage model.  However, this claim is only investigated in Table 6. This still leaves many unanswered questions, like whether this observation is only validated for autoregressive models? especially considering that the previous papers[1] show class-conditional LM also brings sizable gains.\n\n* Even though authors review many related works, they did not include other data augmentation works as baselines. Since this area is widely explored, it is important to understand how much benefits can be brought by this method contrasting other data augmentation options. The model with data augmentation and without augmentation is only ablation studies. Considering a GPT-2 is incorporated additional for data generation, it is not surprising to observe improvements. Comparison with other state-of-the-art data augmentation is needed. Besides the large language model as a data generator[1], many works like EDA[2], UDA[3] also explore this idea. \n\nI have several additional questions:\n1.  In table 1, what data is used for knowledge distillation in DistiRoBERTa + KD? Since KD is usually based on unlabeled data, I am not sure how the unlabeled data is achieved? If KD is based on trained labeled data, this seems not a fair comparison. This can only show that using additional LM for data augmentation is useful and such a claim is provided in many existing papers. I will suggest adopting other simple ways to generate unlabeled data like UDA[3] and still KD loss on them\n\n2. I am not sure about the claim on class-conditional generative models. Since class-conditional generative models still generate noisy data, how about using pseudo labeling on class-conditional generated data? The negative effect seems to be brought by noisy labels instead of conditional generated data quality since codntional-class generation is also shown to be effective in a recent paper [4] with a more challenging setting.\n\n\n\n\n[1] Jason W Wei and Kai Zou. 2019. Eda: Easy data augmentation techniques for boosting performance on text classification tasks\n\n[2] Kumar, Varun  and Choudhary, Ashutosh  and Cho, Eunah Data Augmentation Using Pre-trained Transformer Models. worshop AACL 2020\n\n[3] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, Quoc V. Le Unsupervised Data Augmentation for Consistency Training. NeurIPS 2020\n\n[4] Zirui Wang, Adams Wei Yu, Orhan Firat, Yuan Cao Zirui Wang, Adams Wei Yu, Orhan Firat, Yuan Cao\n",
            "summary_of_the_review": "The authors study how to generate data for semi-supervised learning. The main claim is that the unconditional autoregressive language model is able to provide good quality data. However, this claim is still validated based on comparison with other state-of-the-art data augmentation methods. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper provides a novel framework for advancing self-supervised learning, Knowledge distillation and few-shot learning for NLP and Tabular data. The framework focuses on self-training and knowledge distillation by generating synthetic data based on an unconditional generative model. They conduct extensive experiments spanning NLP, Tabular data and Computer vision datasets, discussing the benefits and limitations of their proposed methods. They also conduct and extensive literature review for each of the components used in their framework.",
            "main_review": "First, I want to thank the authors for the effort in writing this paper. It is extremely well-written and I appreciate the section 2 and 3, where key concepts are detailed and formally presented, very easy to follow. I wish more papers dedicated this much space and effort for adding context for the reader. I also appreciate the fact that you share situations where the models fail, showing limitations of the approach.\n\nThe model seems solid and justification for decisions made are clear. Improvements are not large but, given they are being compared to very strong baselines, it is fair to say they are relatively solid.\n\nI do not have many weaknesses to highlight. Adding some of the things that could improve the publication below:\n\n1 - For figure 1, a NIT comment is that adding the unlabeled data to the same level as the models made it a bit confusing at first glance. Maybe moving the \"open domain unlabeled data\" cloud more to the left might make it more clear where the whole process actually starts (maybe something similar for the \"small amount of labeled data\" part.\n2- As you present the GLUE results on table 1, you split the scores with a \"/\". Since this is a paper that spans several areas of AI, it might be good to give more explanations on why this is the case and what are the metrics being considered in all your tables.\n3 - For table 1 and 2, some standard deviation for the multiple runs or significance testing might be useful to compare results that are similar to each other.\n4 - In section 5.3, a way to evaluate the impact of the quality for synthetic examples is to manually generate them by hand. I do think that is an important component for the paper that was not covered as well as the rest. Understanding the limitation of current generative models and understanding \"how far we can go if we improve them\" based on curated high quality data might make your point stronger.\n5 - In table 5, you might want to add a baseline comparison without GAL so it is easier to compare the improvements.\n6 - For tabular tasks, I think that section was more confusing than helpful. There was not a lot of analysis on why you decided to encode features the way you did and the impact it had on the two datasets where XGBoost outperforms your methods. It might be an interesting analysis to add or at least to discuss further.",
            "summary_of_the_review": "This work has extensive literature review and provides enough context for the reader to follow their thought process and the decisions made by the authors. Results are convincing and span multiple NLP tasks and research areas (tabular and CV data). There is in-depth discussions about limitations and benefits, and about the decisions made for the framework. My opinion is that this is a clear accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}