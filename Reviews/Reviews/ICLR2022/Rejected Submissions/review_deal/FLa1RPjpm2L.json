{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "One of the major challenges in model-based RL is the learning of accurate world models. This work proposes a method that can learn to decompose the dynamics of the world into several sub-dynamics, which is postulated to lead to better prediction accuracy (which in turn should lead to better policies/downstream performance). The proposed method clusters the sub-dynamics in latent space, and can be combined with any existing MBRL method. Here it is specifically combined with Dreamer and MBPO and evaluated on the deepmind control suite/mujoco.\n\n**Strengths**\nThis work addresses an important problem, and on a high-level the problem/approach is well motivated\nThe proposed algorithm is \"simple\" (which can be a really good thing) and very general in that it can be combined with seemingly any MBRL method\n\n**Weaknesses**\nThe manuscript was lacking in clarity on a technical level (partially addressed during rebuttal)\nOn average the experimental results are not very convincing (yet)\n\n**Rebuttal**\nThe authors clarified a few misunderstandings the reviewers and also updated the manuscript accordingly\n\n**Summary**\nI agree with the reviewers that the experimental results are not fully convincing. When looking at the model error plots, the y-scale is very small, and it looks like there is no significant improvement. Also in the \"downstream tasks\" only for the humanoid do we seem to see a significant improvement. Overall this seems promising, but the authors should investigate why there seems to be a clearer benefit for the humanoid, and show more such results. Furthermore, there are some concerns/clarity issues with respect to what your approach learns - I would recommend you take a small-ish toy example to introduce the intuition of your approach, and maybe visualize learned sub-dynamics.\nOverall, while promising, in it's current state this manuscript is not quite ready yet for publication"
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents a model-based RL algorithm that provides decomposed dynamics models when the state and action spaces are defined as a multi-dimensional Cartesian space. \nWe can divide the method into two parts:\n* The one for partitioning the action coordinates, and\n* The NN architecture, one for each partition and then combining the outcomes ($h^i_t$) of partitions into the final one ($h_t$).\nThis paper also shows a clustering based on the correlation, human-provided partition, and full/random partitioning algorithms.\n \nThe experiment shows the results on Mujoco-based environments that have orthogonal coordinates per position and velocity, so\nthe action coordinate partitioning can be readily applicable.",
            "main_review": "This paper proposed to build multiple world models, one per action space partition, and average it to improve the model error.\nThis idea is generic to other MBRL approaches, and the paper implements the method in \"Dreamer\" and \"MBPO\" and shows improvements in the model error.\n\n#### Strengths\n* The idea is simple, and the experiment results showed improvements in model error or value in some domains\n\n#### Weakness\n* If I understood correctly, this action space partitioning can only be applicable when there is a clear decomposable structure in the action space. \n* The result shows some marginal improvements on the five random seeds.\n\n#### Questions\n* Have you experimented with this dynamics decomposition approach to other domains like ATARI?\n\n* Could you provide a side-by-side comparison of NN on ED2-Dreamer and simple Dreamer?\nWill the total number of parameters remain the same or similar? Does the performance changes per the choice of hyperparameters?\n\n* In Figure 6, there is a gap in the model errors between the two approaches. If we run on more sample interactions, will this gap disappear? Or remains the same?\n\n* The main difference is that ED2-Dreamer usually shows lower variance over the five random seeds than Dreamer. Is this due to the averaging effect of the dynamic decomposition? In Figure 5, ED2-Ensemble takes the whole action space, learns multiple models, and averages the results. We can see that the blue and the red curves are less similar in the cheetah and hopper domains. Would it imply that the decomposition doesn't bring improvements in those domains?\n\n* When combining the latent states, the paper proposed to use averaging the outcomes. Have you tested other choices like max or putting an additional layer?\n\n\n",
            "summary_of_the_review": "This paper shows MBRL methods that utilized the decomposition of the action space and showed experiment results on the control domains.\nIn some domains, we see improvements in the model error or higher values. But the overall improvement is marginal, and the method can only be applicable to the problems having readily decomposable action space.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concerns",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a clustering method to decompose world models for model-based RL into sub-models reflecting the dynamics of the task. The method appears to give small to moderate improvements on most of the presented 4+4 benchmarks. \n",
            "main_review": "I have some issues with the technical approach, raising doubts over how this method will generalize. In particular in light of the small to moderate improvements in the experiments sections and small selection of benchmarks. \n\nTechnical approach: \n- The authors make a strong and seemingly very limiting assumption in that they only partition actions, all sub-models get fed the same state input. Then they average the output of all submodels to get the new state, also a strong assumption that the state can be additively decomposed this way. I assume this means their submodels output the full state vector as well, almost like an ensemble approach for the latent state with an extra clustering step to filter out just the action inputs for each sub-network.\n\n- The clustering approach itself also seems to make a very strong assumption in clustering bon linear correlations, and of the action dimensions vs. the state dimensions. I am not sure why this makes sense because linear correlations strongly depend on how you encode the state and action space. For example, it is signed so if you just negate one action dimension, shouldn't that change which other actions it gets clustered with it, even though it will be functionally the same? Further, as dynamics models, shouldn't actions be clustered based on how they predict the next state, not how they are correlated with the current state? In general, the Feature Extraction and Cluster Criterion sections are crucial to this paper but the theoretical basis of the design choices made there are not clear to me. \n\nExperiments:\n- The Dreamer and MBPO papers you compare against have 20 and 6 benchmarks respectively, it is unfortunate that you only present a subset of 4 from each. This becomes relevant because the improvements aren't large to begin with and some questionable assumptions were seemingly made in the method section. As you allude to in Appendix F, the method also appears to only improve results on problems that are easily decomposable, which seems reasonable.\n\nLanguage: \n- Readable but some bad grammar. I recommend another pass. In particular, there is frequent overuse of \"the\", as in \"by combining the optimal control algorithm\" when you mean \"by combining [with] optical control algorithms\"\n\nMinor:\n- Is it necessary to describe it as a POMDP? The paper by Hafner et al 2020 that this paper builds on does not. Your state space definition is seemingly much more complex than theirs, but as I understand it you use the same benchmarks.",
            "summary_of_the_review": "The problem of learning decomposed world models is interesting and the authors do show a small improvement on most of the presented benchmarks.\n\nUnfortunately, the clustering method, which is the main contribution of the paper, makes a number of strong and unexpected assumptions that make me question how well this approach will generalize. The paper also suffers from poor clarity in the method section on the theoretical motivation for these design decisions. The empirical results are not very strong to begin with, and were only presented for a smaller subset of the benchmarks in the papers they compare against. \n\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose an environment dynamics decomposition (ED2) framework to decompose the environmental dynamics into multiple sub-dynamics. Empirical results show that ED2 improves the performance of several model-based reinforcement learning (MBRL) algorithms. The paper is well-written and easy to read, and the motivation is clear.",
            "main_review": "However, I have the following concerns:\n\nQ1. The sub-dynamics model is to output an embedding vector regarding sub-actions and state $s$, but not to predict the next state $s’$ and reward $r$. The sub-dynamics model is therefore not a dynamics model. I don’t think it is precise to claim that ED2 is a dynamics decomposition method. \n\nQ2. As the underlying idea of ED2 is very simple, I believe it is possible to theoretically analyze the advantage of ED2, which can improve the paper’s contribution significantly. \n\nQ3. Doya et al. [1] also leverage the idea of dynamics decomposition to model the environmental dynamics but decompose the dynamics into multiple domains in state space and time. Although their method is different from ED2, I suspect that it is important to discuss the difference between the two studies because both are based on the idea of dynamics decomposition.\n\nReference:\n\n[1] Doya, Kenji, et al. \"Multiple model-based reinforcement learning.\" Neural computation 14.6 (2002): 1347-1369.\n\nQ4. The introduction section mentions many MBRL algorithms, such as ME-TRPO [1] and SimPLe [2], but the authors empirically compare ED2 with only two MBRL methods in the experiment section.\n\nReferences:\n\n[1] Kurutach, Thanard, et al. \"Model-ensemble trust-region policy optimization.\" ICML. 2018.\n\n[2] Kaiser, Łukasz, et al. \"Model Based Reinforcement Learning for Atari.\" ICLR. 2019.\n",
            "summary_of_the_review": "Although the paper is well written, the current contribution is not enough for me to recommend acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}