{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The authors have proposed a new consistency loss for improving model robustness to common corruptions. With a student-teacher training setup, only the student network uses batch normalization at training time. Improvements are shown on small scale corruption datasets (CIFAR-C), a single domain generalization dataset (VLCS), and RobustPointSet.\n\nThough, positive feedback were given on the quality of the story telling, and on an interesting motivation by a few toy examples, some concerns remained among the reviewers.\nIn particular applicability of the method as model and data sizes increases, e.g., on ImageNet-C, was questioned.\nAfter Additional results were provided by the authors, the method seems to break as scales increases.\nThe way relevant baselines from previous work was also judged light and should be improved.\nHence, the paper could be improved to include more comparisons and more convincingly showing advantages of the method."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors study the effects of BatchNorm layers on model robustness.  They demonstrate that models trained with BatchNorm are likely to rely on low-variance features which are performant in-domain, but are not helpful for out of domain data.  Leveraging this observation, they propose a student-teacher method (Counterbalancing Teacher) that creates a copy of the same model with all BatchNorm layers removed.  The demonstrate that this leads to very competitive performance with other techniques for reducing the performance hit of out domain evaluation: self-supervised methods and data augmentation methods.\n",
            "main_review": "**Strengths:**\n\n- I love the clarity of exposition, especially section 2.1 where the insight of data-dependent versus independent norms is presented.  Also the experiments in section 4, which begin with a very clear description of the hypotheses to be tested.\n\n- The authors perform an extensive set of experiments for different models, datasets (and types).  Particularly impressive was the comparison of CT to other data augmentation-based methods\n\n**Weaknesses:**\n\n- Section 4.1 is a bit oddly presented.  In particular, the red vs blue experiment.  The third paragraph states:\n  > \"Ideally, a classifier should not rely only on the dominant features to\npredict the class. However, as shown in the second row of Figure 2, the network trained with batch normalization quickly picks up those low variance clues, which in this case is either the red or the blue\nsquare. However, the same network without normalization takes into account other features as well.\".\n\nI find it hard to draw a conclusion from this experiment.  Have the authors really controlled all the sources of variation in this experiment?  Are the synthetic features shown really the low(est) variance features that disambiguate 2 from 3 in MNIST?  I imagine there are many light / dark pixels that are also low-variance & predictive.  A histogram of variances would be helpful here in establishing the validity of the design.\n\n- Figure 4 is oddly presented.  The X-axis is categorical (different normalization for layers of the wide ResNet), and are not ordinal; presenting this as a line plot suggests that they are.  A grouped bar-plot or box-whisker plot (or violin plot) would be more direct.  Another issue is that the results of Figure 4 are discussed in section 4.2  with a condition not depicted:\n> As shown in Figure 4, the model with no normalization (NN) outperforms all the three types of normalization when evaluated on corrupted data\nNo data for (NN) is presented in Figure 4.  Was the name changed?\n\n- The central insight of the paper is that normalization of features leads to an over-reliance on low-variance features.  Yet after presenting the $L_{ct}$ regularizer, the authors state that they $l_2$ normalize both $r_{G_{\\theta}}$ and $r_{F_{\\zeta}}$.  It seems very strange to me that a corrective measure against normalization must still require normalization.  Is there no other way to account for the different scales of learned features?  *Should* the method account for them?  I'd like to see a version of the CT loss where different losses for equation (6) are used.\n\n- Table 2 was my favourite part of the whole paper, but I was sad to read that the authors only reported point estimates of mCE.\n\n- Given how the student-teacher regularized loss is defined in equation [7], it seems odd that the authors do not present an ablation study where they reduce the regularization effect tuning $\\lambda * \\mathcal{L}_{ct}$ as $\\lambda \\rightarrow 0$\n",
            "summary_of_the_review": "The authors present evidence that BatchNorm leads to a preference for parameters that minimize a data dependent norm, and demonstrate a simple student-teacher training method to correct for these effects.  There are a few alterations I'd really like to see, though I still believe that, given the required corrections, there is enough value in this particular insight and the experiments to see it published.  I would have dearly loved to see more emphasis on other measures of uncertainty for the networks (cf. https://github.com/uncertainty-toolbox/uncertainty-toolbox) rather than looking strictly at accuracy.  If my other criticisms can be addressed, I will increase my score.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a new consistency loss for improving model robustness to common image corruptions. They use a student-teacher training setup where only the student network uses batch normalization at training time. Improvements are shown on small scale corruption datasets (CIFAR-C), a single domain generalization dataset (VLCS), and RobustPointSet. ",
            "main_review": "The paper is mostly well written with a clear story, and nicely motivated by a few toy examples. The results outlined in the paper look very promising.\n\nHowever, especially after the strong focus on batch norm adaptation techniques in the introduction, I expected to see full results and improvements on ImageNet-C, -R, (-A) as common in the referenced papers. In contrast, the experimental results are quite sparse, and comparision to the most relevant competitors (techniques that leverage augmentations during training time) are limited to Table 2 (CIFAR scale). In-depth analysis of the properties of the method, along with ablation studies, is also missing. \n\nIn the following I will discuss the most important additional limitations/weaknesses of the paper.\n\n* Methods: The method section in 3.2 lacks clarity. Especially for batch norm, there are a lot of caveats in the exact implementation. Could you please add a precise description how the statistics are estimated, how the student / teacher update works, how student -> teacher update works, etc.? Given the simplicity of the method (which is a huge plus), it should be possible to re-implement the technique based on the information in §3, and I feel like this is not the case currently.\n* Motivating examples (§4.1): Especially on such small scale experiments, I find it important to strengthen results by running multiple seeds, and reporting error bars. Could these be added to all experiments in Figure 3, both for the barchart and the line plot? In addition, it could be interesting to add results for an “on the fly” normalization layer such as GroupNorm or LayerNorm to the set of models in Figure 3 (optional). Apart from this, I really like this experiment and how it is used to set the stage for the method.\n* Figure 4: The lines should not be connected, there is no obvious relationship between the entries on the x-axis. I suggest a simple scatterplot (one point per method) along with error bars is more suitable in this case.\n* Experimental results from §4.3 onwards: It is unclear to me which experiments were re-implemented, and which results are cited from previous work. Could you please let me know for tables 1, 2, 3 and 4 whether you ran all baseline implementations yourself, or whether any of the numbers in the tables is cited? From the appendix, it seems the contrastive learning numbers are reproduced, while the domain generalization results are cited? If that is the case, did you verify any of the baseline results to make sure the experiment setups match?\n* Self-supervised learning (Table 1): How do you justify this experiment? If all the labels are known, and the models are fine-tuned, why is it interesting to use self-supervised learning for obtaining the embeddings in the first place? Generally, I think this is a minor result, and should be less prominently mentioned in the paper.\n* Instead, It would strengthen the paper to extend Table 2, and add more methods. According to Robust Bench, the [state-of-the art in CIFAR10-C](https://robustbench.github.io/#div_cifar10_corruptions_heading) right now are much better than the reported 17.4%. On a WideResNet-40-2 model, Augmix implemented in the benchmark achives ~11-12%, while 13.5% are reported in the paper. How do you explain the difference? The same applies to CIFAR-C, where Augmix is reported with 35.89%. It would also be good to cite [Diffenderfer](https://arxiv.org/abs/2106.09129) who are state of the art now, although their method is clearly concurrent work (just accepted to NeurIPS'21).\n* VLCS: [Gulrajani and Lopez-Paz](https://arxiv.org/abs/2007.01434) showed that on domain generalization benchmarks like VLCS, after sufficient hyperparameter tuning, and by using more suitable and larger models (like a ResNet50), ERM is a very strong baseline. Results in Table 3 show exactly the issue described by Gulrajani and Lopez-Paz. A much stronger result would be a full evaluation on the DomainBed benchmark, if the authors would like to do a strong claim about Domain Generalization. (Also note that all of the cited methods were published before Gulrajani and Lopez-Paz).\n* Minor Point: Positioning within the related work: It is known (and already shown on ImageNet-C scale, cf. your references regarding BatchNorm adaptation) that adapting to “shuffled” corruptions (your “adapt-all-test-all” setup) does not work with batch norm adaptation, so this is not a new observation. This should be made clear in §4.2.\n\nA few questions:\n* Table 2 suggests that larger models (?) like the WideResNet narrow or even revert the effect of your technique. When moving to full scale data, models get even larger (ResNet50, ResNext101, etc.) and even better augmentation techniques than Augmix (SOTA right now is DeepAugment + Augmix augmentation combined) are used. Could you comment on the limitations of your method and your expectations? For instance, batch norm adaptation fails to work as models and datasets get larger--do you expect that a similar limitation will apply to your technique, that is not visible from the presented results?",
            "summary_of_the_review": "The technique is simple, well-described, and seems to be effective on the considered toy datasets. What I am missing right now is compelling evidence that it actually works in practice, and in practically relevant settings. For this, several key results are missing:\n\n1. Full scale ImageNet-C (and -R) evaluation with relevant larger models (ResNet50, ResNext101 etc, following prior work). This will make the paper much more interesting for a wider audience interested in corruption robustness. In addition, I suggest runnign on ObjectNet --- BatchNorm adaptation does not work on this dataset, and it would be interesting to see whether CT is effective at mitigating this issue.\n2. Comparision to more recent domain generalization techniques, and especially reporting results on more datasets beyond VLCS. To make a claim that the method improves domain generalization, the authors should consider the DomainBed evaluation setting, or a similar, more comprehensive experiment protocol that avoids the issues discussed by Gulrajani & Lopez-Paz (2020). Alternatively, the authors might drop this claim, and move the table as a proof-of-concept to the supplement.\n3. I am missing a statement how hyperparameters were tuned. The supplement simply states a few values, but not how the authors arrived at them. In particular, they should be picked according to the *validation error on the training domain*. Ablations over a few crucial parameters will help here (for example, the learning rate). It is unclear why these were not added, given that the results are obtained on fairly small scale datasets.\n\nI think it is feasible and reasonable to expect these kinds of results in an ICLR paper. Point (1) only requires to train models on ImageNet (3 seeds, ideally) *once*, getting scores for the requested setups is easily doable afterwards.  I find it possible that on full ImageNet-C, more recent techniques like DeepAugment+Augmix on larger models (ResNet50) will outperform the proposed technique. Even then, I agree with the authors comment (cf. Table 2) that a huge advantage of the method is the removed need for well-designed data augmentations. However, I feel that this might come at the expense of only being effective in settings where batch norm adaptation is working. Since the technique is effective on corruptions, a great test would be the ObjectNet dataset, where batch norm adaptation fails to work.\n\nRegarding (2), this is not a strong requirement, the authors could equally decide to drop the claim regarding domain generalization.\n\nPoint (3) is again crucial---especially Table 2 indicates that the baselines in the paper were not sufficiently tuned.\n\nIf the authors manage to add these results (minimally 1, comment on 2, and clarifications on 3, along with addressing the detailed comment in my review), and show a realistic comparisions of tradeoffs to state of the art augmentation based models, I will increase my score. Right now, in my opinion the paper does not meet the bar for ICLR, as the number of truly relevant datasets, and comparisions to the state of the art is limited.\n\nI am happy to address questions/clarify the exact scope of points 1-3 above if this is needed.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper argues that Batch Normalization and similar normalization layers can make neural networks more fragile to data distribution shifts and proposes a distillation-based regularization method to ameliorate this issue while maintaining in-domain accuracy.",
            "main_review": "The paper theoretically analyzes Batch Normalization on linear models, showing that it introduces and inductive bias that makes the model rely more on low-variance features. They argue this high reliance on low-variance features can cause the model to become brittle to distributon shifts where such features are disrupted, while non-normalized models tend to rely to all the input features in a more balanced way, which makes them more robust at the cost of a lower in-distribution accuracy.\n\nThe paper proposes a training method where a teacher model is first trained without BN, then a student model with the same architecture but with BN layers is trained both on the task loss and on a regularization loss that pushes its last hidden representation towards the one of the frozen teacher model. They show that this approach preserves in-distribution accuracy of BN while achieving better out-of-distribution accuracy on various tasks.\n\nQuestion for the authors: are the teacher and the student initialized with the same weights?\n\nOverall the paper is clearly written, the arguments are sound and the empirical evidence is solid.",
            "summary_of_the_review": "\nOverall the paper is clearly written, the arguments are sound and the empirical evidence is solid.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper aims to address the problem of out-of-domain generalization with batch normalization layers. It first identifies the reliance on low-variance features in batch normalization, and proposes a counterbalancing teacher approach to distill from a BN-model to a non-BN model. Experiments verify the hypothesis of the paper and show that the proposed approach outperforms the various baselines on robustness to data corruption and domain generalization. The proposed approach is also evaluated on a 3D point cloud dataset.",
            "main_review": "Strengths:\n1. The paper is well-written and well-motivated. The proposed approach is simple, effective and does not require access to the out-of-domain distribution.\n2. The proposed approach is empirically evaluated against a number of baselines, including covariate shift adaptation methods, self-supervised methods, and data augmentation-based methods.\n\nWeakness:\n1. The theoretical analysis focuses on the limitation of batchnorm, but it is not clear how distillation resolves this problem. If a new model is distilled, one can argue that the effects of batchnorm, along with its limitations, are transferred to the new model, although it does not include the BN layer explicitly—analysis and insights are missing in the proposed counterbalancing teacher in how it address BN's limitation exactly.\n2. The authors should clarify why relying on low-frequency features are undesirable. This is an implicit theory in this paper without sufficient justification. ",
            "summary_of_the_review": "I think the simplicity and effectiveness of the proposed approach out-weights the limitations. This approach has potential for further improvements because it does not rely on access to the target data distribution.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}