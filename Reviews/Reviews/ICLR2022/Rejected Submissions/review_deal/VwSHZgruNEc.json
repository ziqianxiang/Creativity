{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper presents tackles the problem of finding strategies that are -- unlike Nash which is safe -- both safe (non-exploitable, to some extent) and able to exploit the opponent. The proposed solution is a convex combination of exploitation and safety that is efficient to compute. Overall, the paper is borderline. Given that the objective and its analysis are not especially surprising, a lot rides on the thoroughness of the empirical results, which could be improved."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a method of subgame resolving in zero-sum extensive form games, blending in elements of theoretical safety, as well as more classical approaches based on opponent modeling. The method is fairly straightforward, utilizing a modified objective balancing between the safety objective (used in Moravcik et al) and a best-response like objective use in opponent modeling. To facilitate compatibility with modern solvers such as CFR, the authors propose gadgets transforming the refinement step into another zero-sum extensive form game.",
            "main_review": "This paper’s motivation is easy to follow and does a good job of introducing subgame search and opponent exploitation. I have not found any glaring errors in derivations or proofs. In terms of significance, this paper answers the natural question of how opponent weaknesses can be exploited without overly sacrificing safety. Experiments are sufficient.\n\nOne area this paper could do a lot better in terms of clarity. For example, the definition of CBR at the end of section 3.1 could be much more precise---the argument of CBR need only depend on player -p, and \\sigma_p in the definition should really be CBR_p(\\sigma_{-p}). In other areas, the authors could be more precise, e.g., in section 4.1, “under the assumption that the reach probabilities *remain the same*”, could be changed to “is \\hat{p}(...)”. This is to disambiguate between \\tilde{p} and \\hat{p}. The gadget game presented seems correct to me, and Figure 1 was very useful in illustrating it. However, for readers unfamiliar with gadgets, the textual representation could be insufficient. For example, P2’s infosets should stretch over both the left and the right branches (which is briefly alluded to in point 1).\n\nTheorem 1 provides a natural extension of the bounds of Moravcik et. al. One concern I have is how useful these bounds are in practice, in particular for selecting an appropriate value of alpha. In larger games (e.g., FHP), it is very likely that some information sets are never reached in practice even over many rounds of play. Hence, tau is not known, or may not even be estimated using data (this admittedly is a problem for any type of opponent modeling). The authors’ claim at the end of section 5: “In case of a bad estimation, we can always choose smaller \\alpha to ensure safety”. In practice, how do we know we have a bad estimation if there are no samples to compare with?\n\nThe authors present experimental results with varying epsilon (estimation error of opponent strategy). What was the procedure in which this error was introduced?\n\n",
            "summary_of_the_review": "I vote for this paper to be accepted. Apart from a lack of polish, there are no glaring errors in the paper, and the key ideas in the paper are easy to follow and appreciate. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a technique to smoothly interpolate between computing a Nash equilibrium strategy and computing a best response. The technique is presented in the form of search for an extensive-form imperfect information game. Experiments on two poker variants are presented showing that the technique can indeed interpolate between the Nash equilibrium strategy (\"safety\") and the best response (\"exploitation\").",
            "main_review": "To my understanding, the technique presented by the authors to interpolate between a Nash equilibrium strategy and a best response (say, for Player 1) simply boils down to the following:\n- introduce a parameter $\\alpha$ and optimize the objective $f(x) = (1-\\alpha) \\cdot \\min_{y\\in Y} \\\\{x^\\top U y\\\\} + \\alpha\\cdot x^\\top U \\bar{y}$, where $x$ is a strategy of Player 1, $Y$ is the set of strategies of Player 2, and $\\bar{y}$ is a \"predicted\" strategy for Player 2;\n- solve the above optimization point (that is, solve $\\max_{x \\in X} f(x)$ where $X$ is the set of strategies of Player 1.\nThe presence of search seems just an implementation detail that obscures the above, straightforward idea.\n\nIf my understanding were to be confirmed correct, I find the paper very light theory-wise, and I definitely think that the idea should be stated for what it is instead of interspersing it with the search formalism, which ends up obscuring the idea and making it looks significantly more complex than it is.\n\nAgain assuming I didn't miss anything about the fundamental underlying idea of the paper, the technical results (Theorems 1 & 2) are not surprising at all, and just amount to bounding the two terms in the definition of $f(x)$ independently.\n\nGiven the limited technical contribution I would be significantly more satisfied with the paper if it had shown more compelling/insightful experimental evidence of the benefit of the approach. However, the paper only experiments on one domain (poker), and the experiments do not seem to show any particular interesting insight. For example, the experiments find that if the prediction $\\bar{y}$ is accurate, then for large $\\alpha$ the maximum of $f$ will be a best response to the strategy of the opponent, and if the prediction is not accurate, then one would be better off by picking a lower $\\alpha$. I find that completely expected, but perhaps I missed some more subtle point?",
            "summary_of_the_review": "My biggest concern with the paper is about the significance. I believe that the idea behind the paper is simply optimizing a convex combination of functions, and that the search formalism that was imposed on top is mostly an implementation detail from a theory point of view. I also find the experiments unsurprising and not particularly insightful. Other than that, I do not believe the paper to be technically flawed or problematic in its claims. However, I find the significance concern is serious enough to prevent me from recommending acceptance. I look forward to a robust discussion with the authors on that point.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The submission proposes performing safe opponent exploitation via decision-time planning using a novel gadget game. At the top of the gadget game, there is a chance node that goes to the maxmargin gadget game with probability (1 - alpha) and goes to the estimate posterior over private information with probability alpha. The submission proves that, for small enough alpha and posterior error, this gadget game produces a safe (non-exploitable) strategy. The submission presents experiments on Leduc Hold'em and Flop Hold'em Poker.",
            "main_review": "The main idea of the submission is somewhat obvious. However, I think it nevertheless offers value to the community, as opponent exploration is both an important and an under-explored research area (at least in my opinion). There are some small problems with the submission (see below) that I would like to see addressed. More generally, I like to see the authors put into the quality of the writing. As it currently stands, much of it is sloppy as does not read like a publication-ready conference paper.\n\n### Exploitability Theory\nI think the exploitability results are a bit confusing as currently written. The term \"exploitability\" is not generally used in a way that depends on the opponent's \"true distribution\". Exploitability defines a worst case loss. Unless I am misunderstanding something, the submission is talking about a worst case loss, subject to a constraint on the private information. I am not sure it is appropriate to refer to this quantity as exploitability --- SES is not, in fact, safe against arbitrary opponents.\n\nThe submission provides a nice example illustrating why its approach is superior to simply mixing between a safe strategy and an exploitation strategy. That being said, it seems to me to be perfectly possible to provide theorems analogous to Theorem 1 and Theorem 2 for the \"mixing between a safe strategy and an exploitation strategy\" approach. I think it would add value to the submission to work through these results, as it may be interesting to compare them to Theorem 1 and Theorem 2.\n\n\n### Baselines\nThe submission dismisses Johanson 2007 and Ganzfried 2015 as baselines. However, I am not sure that this dismissal is well justified. The method in the submission already uses an estimate of the opponent strategy. It would be relatively to substitute this estimate for the \"known\" component of the strategy required by restricted Nash response (RNR) and test the exploitability. It may be that RNR performs perfectly well in this case. Also, while I do not dispute it is difficult to scale LP solvers to large games, the games considered in the submission are not that large. Therefore, it seems like both Johanson and Ganzfried offer reasonable baselines for the algorithm proposed in the submission.\n\n### Small Problems\n\nPaper cites benchmark Hanabi paper for superhuman Hanabi performance?\n\nPaper cites Tian for superhuman bridge performance?\n\n“Monte Carlo tree search (MCTS) can be viewed as a powerful policy improvement operator.” \nThis sentence appears to be (hopefully accidentally) plagiarized from the AlphaGo Zero paper: “MCTS may therefore be viewed as a powerful policy improvement operator”.\n\nPoorly constructed “They both have sounded monotonic improvement guarantees in theory, and show tremendous performance promotion empirically.”\n\n“In fully-cooperative imperfect information games, search algorithms proposed in Tian et al. (2020b) and Lerer et al. (2020) are also proved to never be detrimental to the current policy”\nThis is only half true. Lerer et al.’s “never detrimental” is only asymptotic. After any finite amount of time (ie in all practical cases) there is no such guarantee.\n\n“However, it may be overly conservative confronted with opponents with limited rationality and fail to take advantages of their weaknesses to obtain higher rewards.”\nIt has nothing to reference (should be they) \nAdvantages is wrong grammatical number\n\n“We construct a new gadget game to optimize this objective, which enable”\n\n“Search techniques are proven to be essential in developing strong AI strategies”\nThis claim appears overly strong.\n\n“Libratus (Brown et al., 2018) is the first superhuman poker AI which is considered the milestone in the research of imperfect information games”\nThis sentence is both grammatically incorrect and unnecessarily contentious. Does it add value to the paper to make a claim about which poker AI algorithm (between Libratus and DeepStack) was the first and which is considered the “milestone”? Also, is it appropriate to not even cite DeepStack here?\n\n“search algorithms for subgame refinement have also shown promise in improving joint strategies in cooperative imperfect information games such as Hanabi (Bard et al., 2020)”\nThe paper being cited here does not involve sublime refinement.\n\n“In that paper, Johanson et al. (2007) assumes that the”\nassume\n\n“But Ganzfried & Sandholm (2015a) relies on LP solver”\nrely\n\n“It is worthwhile mentioning that there also have been extensive research”\nhas\n\n“However, this paper only only focuses”\nfix\n\nNotation Section:\nIt’s bad form to start sentences with symbols.\n\nIt’s pretty unusual for EFGs to use \\sqsubseteq the way the submission defines it.\nHaving read further it appears that the submission uses \\sqsubseteq the way it is normally used for EFGs. So the definition is incorrect.\n\nIn the matrix game shown in Table 1, there are two NEs.\nAny interpolation of the two NE the submission gives is also a NE.\n\nThroughout the paper Safe Subgame Exploitation is improperly capitalized.",
            "summary_of_the_review": "I would like the authors to address the comments above. Should they do so, I am willing to reconsider my score.\n\n---\n\nAfter discussions with the authors, I raised my score from a 5 to an 8.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper introduces Safe Exploitation Search (SES), which achieves a balance between maxmargin solving (guaranteed to be safe, but non-exploitative) and unsafe subgame solving (unsafe, as the name suggests, but exploitative of the opponent's initial distribution at S_top) by mixing their objective functions. The paper provides both theoretical and experimental evidence that the method does a good job of balancing the two objectives. ",
            "main_review": "Strengths:\n\n1. The paper is clean and easy to read.\n\n2. Both the theoretical results and the experiments clearly support the main claims of the paper.\n\nWeaknesses: \n\n1. The main contribution of the paper is only a minimal step on top of known ideas (a convex combination of unsafe and maxmargin subgame solving). The theoretical and experimental results are not at all surprising either.\n\n2. Related to above: I'm not completely satisfied with the comparison to the past safe exploitation works [Johanson et al '07, Ganzfried & Sandholm '15]. As far as I can tell, the present paper simply uses a different parameterization of the Pareto frontier of exploitation vs exploitability--it uses the mixing amount $\\alpha$; the others use the exploitability, formulating the objective as \"maximize exploitation such that exploitability is at most $\\varepsilon$\". These are not very different. Further, LP solving in games is not that slow, especially with recent advances [Zhang & Sandholm '20]; in particular, the subgames in experiments seem tractable with LP.\n\n[Zhang & Sandholm '20] Sparsified Linear Programming for Zero-Sum Equilibrium Finding",
            "summary_of_the_review": "Well-written, but in my view not very novel/just incremental work over past work on subgame solving. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}