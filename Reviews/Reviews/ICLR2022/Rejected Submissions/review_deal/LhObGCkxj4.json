{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper proves a global convergence rate of a newly proposed algorithm finite sum problem under some assumptions. While the proposed algorithm provides some interesting ideas to solving the finite sum problem using intermediate proxy solver, the current assumptions are too strong and I'm afraid that this can make the result essentially trivial:\n\nFor example, assumption 3 assumes that \n$ H_i * v \\approx \\nabla_z\\phi_i(h(w, x_i))$ for every i. This simply implies that \n$ \\|\\nabla_w\\phi_i(h(w, x_i)) \\|_2 $ is as large as $\\|\\nabla_z\\phi_i(h(w, x_i))\\|_2^2 $ as long as the norm of v is small, since \n$\\nabla_w\\phi_i(h(w, x_i)) =  \\nabla_z\\phi_i(h(w, x_i)) H_i $.\n\n\n\nHence the assumption simply assumes that \"If the loss is not small, then the gradient of the objective is not small (using the convexity of $\\phi$, so $|\\nabla_z\\phi_i(h(w, x_i))$ has to be large)\" -- This would imply that gradient descent can also work (and arguably having the same convergence rate) under this assumption.  Note that \"the smallest movement that can decrease the objective the most\" is indeed following gradient descent direction -- So gradient descent would not move the weights more than this algorithm as well. \n\nTherefore, I am not sure that there is a clear benefit to using this algorithm compared to the standard (stochastic) gradient descent. In particular, I would suggest the authors at least show one example where under the current set of assumptions, gradient descent does not work as efficiently compared to the proposed algorithm -- This will make the proposed algorithm much more justified."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors formulate a way to transform finite-sum optimization problems in a proxy strongly convex problem, and prove that it converges to a global minimum in a number of gradient steps that scales inverse quadratically with the tolerance.",
            "main_review": "Strengths: the paper is fairly clear, and the proposed transformation is certainly a novel way to solve a common class of problems in the machine learning community.\n\nWeaknesses: I'm finding it extremely difficult to evaluate the significance of this work.  I respect that there's value in providing a new perspective on an old problem (with correspondingly different asymptotic bounds on its performance), but unless this proposed method is demonstrably useful at solving a concrete problem, I'm not sure if it's significant enough for ICLR.  Specifically, the version of this paper that I would probably feel good about accepting would kick all of sections 3, 4, and 5 to appendices, would succinctly state Algorithms 1 and 2, and then would spend the rest of the paper evaluating these algorithms on real problems.\n\nAt the same time, I don't want to discourage this kind of work, so I would honestly be satisfied seeing this proposed algorithm applied to _any_ problem, but as it stands, I don't think I can accept.\n\nedit: raising score to 6 after author's updates---still would like to see empirical analysis.",
            "summary_of_the_review": "The authors propose an interesting and novel algorithm for machine learning, but it's unclear how significant it will be.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The submitted paper considers a composite formulation of machine learning loss functions, where both the inner and outer functions are required to be smooth. Two algorithms are proposed for the considered formulation, with corresponding convergence results. ",
            "main_review": "The submitted work presents some interesting result, such as using the composite structure for machine learning loss functions, and approximations approach to determine the descent direction. However, I have some concerns,\n1. The formulation is not new. This is a quite well studied formulation in the optimization community. For instance, the authors can check the work of Lewis and Wright 2016. For their result, non-smooth outer functions can be allowed. So the proposed approach is not novel, and the authors should discuss these existing work.\n2. The obtained complexity result is not strong, as it is in the sense of\n$$\\min_{0 \\leq t\\leq T-1} [F(w^t) - F_*] \\leq \\frac{1}{T} \\sum_t [ F(w^t)-F_* ]  = O(\\epsilon) . $$ Given the good properties of the problem. \n3. It would be great if the authors can provide numerical experiments, to demonstrate the performance of the proposed work.\n\nThe author addressed my concerns fairly, as a result i raised my score by 1 to 6. ",
            "summary_of_the_review": "The paper provides certain interesting result, however the novelty of the paper and advantage of the proposed algorithm need further justification. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper provides a new gradient-based algorithm. The algorithm is based on the observation that a loss function for a single sample can be written as composition of two functions (the logits and the actual loss function). It computes the direction by means of solving a quadratic MSE problem. They provide a convergent analysis of the algorithm (there is a version where the quadratic problem is solved explicitly through a closed form expression and a version where gradient descent is applied to solve the problem approximately). There are no computational experiments. \n\nThe main contributions are in the algorithms themselves and the accompanying analyses. It's unclear how novel are the proof techniques since everything resembles second order algorithms. The authors also claim the actual reformulation to be a novel contribution however such a formulation is straightforward and used in many contexts (some of my lecture slides from years ago show the formulation used by authors as a possible formulation for the overall loss function). Despite of this, the design of the algorithm should get credit. ",
            "main_review": "Strengths: the design of the algorithms based on the particular formulation used; the underlying analysis\n\nWeaknesses: While this is a theory oriented paper, the theoretical portion is not strong enough to compensate lack of an experimental study. I am unconvinced about the novelty of the analyses. The authors should provide more convincing arguments that the entire material is unrelated to derivate-free second order algorithms and for example, BFGS.\nRegarding Theorem 1, something is strange with respect to \\beta. If beta is large (goes to infinity), it increases the number of iterations, yet the right-hand-side in (18) goes to infinity. It should be the other way around. \nTo approximately solve (15) or its original version, I wonder why the authors don't consider conjugate gradient descent. It works well for quadratic problems. ",
            "summary_of_the_review": "I give authors credit for the algorithmic development and to a certain degree analyses. They are over-selling the work by using 'novel' too often (in particular in connection with the formulation). The role of \\beta in Theorem 1 makes me worried about the significance of the result. \n\nIn short, this is nice work but not impressive. Addressing some of the weakness would definitely improve the paper. \n\nOther remarks:\nRemark 1: w^t stay in a bounded region: This is often not the case in practice. Gradient clipping is often needed to circumvent this. The reLU issue is addressed in an ad-hoc manner. \nAssumption 3: On the first read, it felt like such a \\hat{v}_aster^t always exists. Then I realized that it is \\hat{v}_aster^t(\\epsilon). The authors should probably emphasize this dependency. \n\nMinor comments: \nThe \"Loss Functions\" subsection should be take out. It's trivial. Definitions 2 and 3 should be removed as it is safe to assume that a reader not knowing these concepts would not encounter this paper (even less being able to understand it). \nNew Algorithmic Framework\nPeriod missing at the end of (7)\nIt is weird to write H (matrix) \\cdot \\eta (scalar) \\cdot v (vector). I suggest \\eta H v\nThe assumptions in Theorem 2 don't have to be repeated. One can simply state \"under the same assumption as in Theorem 1 plus additional assumptions ...\"\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "There are no concerns. ",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper presents a new optimization method for finding global minima\nof nonconvex finite sum problems. In particular, the summands are\nfunctions of the form $\\phi_{i}\\circ h$ where $\\phi_{i}$ is convex\nand Lipschitz smooth, while $h$ is nonconvex. Each iteration of the\nmethod consists of solving an auxiliary regularized least squares\n(RLS) problem, followed by a gradient step. Additional analysis is\ngiven for when the RLS problem is solved inexactly. Finally, a claimed\n$\\tilde{O}(\\varepsilon^{-3})$ complexity is established under strong\nboundedness assumptions on various solution sets.",
            "main_review": "*Strengths*: Overall, the paper is well-written, the problem\nis well-motivated, and the background material is sufficient. \n\n*Weaknesses*: Unfortunately, I will have to recommend rejecting\nthis paper due to a critical mathematical error, namely, **the\nconstant $V$ used in the proof of Theorems 1-2 does not have the\nproper dependence on the tolerance $\\varepsilon$**. To elaborate,\nnotice that the stepsize $\\eta^{(t)}$ in Theorems 1-2 is $\\Theta(\\sqrt{\\varepsilon})$\nand that Assumption 3 (essentially) states that $V$ is a scalar where\n$$\n\\|v^{(t)}\\|\\leq V,\\quad\\|\\eta^{(t)}Q^{(t)}v^{(t)}-b^{(t)}\\|={\\cal O}(\\varepsilon),\\quad t\\geq1,\n$$\nfor matrices $Q^{(t)}$ and vectors $\\{v^{(t)}\\}$\nand $\\{b^{(t)}\\}$. Even under the generous assumption that\n$\\{Q^{(t)}\\}$ and $\\{b^{(t)}\\}$ are bounded, the\nassumption that $\\eta^{(t)}=\\Theta(\\sqrt{\\varepsilon})$ must imply\nthat, at the very least, $V=\\Omega(1/\\sqrt{\\varepsilon})$. It then\nfollows the right-hand-side of the bound in equation (18), and similarly in Theorem 2, is no longer $\\Theta(\\varepsilon)$\nbut rather $\\Omega(1)$ due to the $V^{2}\\varepsilon$ term. As a\nconsequence, **the convergence analysis in the paper is no longer\nvalid as it can no longer be shown that $\\min_{s\\leq T}[F(w^{(s)})-F^{*}]=\\Theta(\\varepsilon)$**\n\nBesides the above major issue, I have a few minor issues with some\nof the other material in the paper:\n\n- [p. 4-5] Lemmas 1-2 are classical results and should be replaced\nby citations to avoid giving the impression that these are new contributions.\n- [p. 5, 6, 14] The use of  \"apparently\" in several important\nstatements in this paper implies that these statements have some degree\nof ambiguity to them. This kind of wording should be removed if the\nauthors are making a formal argument, e.g. Appendix C.2, or the authors\nshould be explicit about their lack of knowledge.\n- [p. 6] What is the definition of \"smooth\" in Remark 1? If\nit means continuously differentiable, then clearly $\\|J_{w}(\\nabla_{w}h_{j}(w^{(t)};I))\\|$\ncan be unbounded even if $w^{(t)}$ is bounded, e.g., consider the\nunivariate function $h_{j}(w;i)=w^{3/2}$ on $[0,1]$.\n- *Minor typos*:\n1. [p. 3] ... this paper **focuses** on deep neural networks ...\n2. [p. 9] ... for non-convex **problems** (Ghadimi & Lan, 2013) ...\n3. [p. 14] ... this result can be **generalized** for larger ...\n\nEDIT: The authors have uploaded a revised version of their manuscript that has addressed my main concerns. The only issues that prevent me from assigning a higher score to the paper are: (i) a lack of numerical experiments and (ii) the use of non-standard assumptions (in particular Assumption 3) that are difficult to verify in practice.",
            "summary_of_the_review": "Despite being a well-written and well-motivated paper, I have to recommend\nrejection due to a critical mathematical error. However, if this error\ncan be resolved in a revision, then I would be open to increasing\nmy final review score.\n\nEDIT: The authors have uploaded a revised version of their manuscript that has addressed my main concerns. I now \nrecommend a score of 6 for the paper. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}