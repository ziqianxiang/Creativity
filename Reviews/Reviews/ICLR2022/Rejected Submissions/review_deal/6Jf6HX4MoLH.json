{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes a planning framework that uses a transformer-based architecture as an attention mechanism that guides the search of a traditional sample-based planner (e.g., RRT*). More specifically, features extracted from a sliding window over the 2D search space serve as input to a transformer that produces a mask indicating where to draw samples from. By constraining the search space for the sample-based planner, the method reduces the time required for planning. The method is compared to both traditional and learning-based planners on different 2D navigation tasks and found to improve sample complexity (and, in turn, computation time), while also being capable of generalizing to unseen and real-world maps.\n\nThe manner by which the method combines the advantages of sample-based planning with an attentional mechanism as a way to constrain the sampling process is interesting. As the reviewers emphasize, the experimental evaluation shows that this approach results in performance gains over both traditional (sample-based) and learning-based planners, while also being able to scale to larger maps as well as better generalize to out-of-distribution settings (compared to learning-based methods). These results support the value of both the overall approach as well as the architectural components (e.g., the transformer and the use of positional encoding). The reviewers initially raised a few concerns with the paper, the most notable of which are the need to include preprocessing in the overall computation time, the accuracy of some of the claims in the paper (e.g., with regards to generalizability), generalization to higher-dimensional domains, and the performance on the Dubins car domain. The authors responded to each of the reviews and updated the submission to address many of these concerns. However, questions still remain regarding whether or not the approach can be adapted to state/configuration spaces with more than two dimensions, something that traditional planners are readily capable of, and the unconvincing results on the Dubins car domain.\n\nOverall, the paper proposes an interesting approach to an important problem that is relevant to the robotics and machine learning communities. The paper makes promising contributions to improve the efficiency of planning, however the significance of these contributions needs to be made clearer."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a learning-based method to solve planning problems. The approach first identifies regions on the map using transformers to provide attention to map areas likely to include the best path, and then applies local planners to generate the final collision-free path. Experiments show that the proposed method can achieve performance comparable to the traditional planners with lower planning time for smaller maps, but the performance drops significantly as the map sizes increase.\n\nNote that I previously reviewed this paper for NeurIPS 2021. The authors have made a few changes since the NeurIPS version and I have updated my review accordingly.",
            "main_review": "The paper tackles an important problem of learning to plan which is of interest to the machine learning community and has important applications in robotics. The writing quality is good and the paper is easy to follow.\n\nThe authors conduct experiments with large map sizes and show effectiveness of the proposed approach. The performance of the method drops significantly when map sizes are increased, which indicates that the method is not scalable. This weakness in itself is not very significant as prior work has experimented with much smaller map sizes.\n\nThe bigger weakness is that the proposed method is not differentiable, which is one of the main benefits of prior learning-based planning methods such as VINs over traditional planners. Differentiability allows the planning models as a component in larger end-to-end models (for example in CMP (Gupta et al., CVPR 2017)). It seems that the only benefit of the proposed approach over traditional planners is that it is faster. However, the lack of differentiability combined with poor scalability means that the proposed approach is only useful for real-time planning on small maps, and even in this case, the proposed approach performs slightly worse than traditional planners (100% vs 99.2%). This makes the significance of the approach very low in my opinion. Traditional planners would be still be preferred over the proposed method when maps are large or when performance is more important than runtime.\n\nThe use of Transformers for planning is not well motivated in the submission. Prior work has utilized convolutional neural networks and convolutional LSTMS for this problem, but the authors have not provided empirical comparisons to those methods. If the motivation is to tackle variable map sizes, why not use recurrent neural networks? Comparison with recurrent and convolutional neural networks is important to establish the significance of Transformers for this problem. The authors have added a convolutional network-based baselines (UNet) in certain experiments but there are no details available on how this baseline is implemented. \n\nThe authors only conduct experiments for 2D navigation tasks. It is unclear whether the approach would scale to manipulation tasks with higher degrees-of-freedom.",
            "summary_of_the_review": "Although the paper tackles an important problem, it has several weaknesses in terms of motivation and missing baselines. Lack of differentiability and poor scalability severely limits the applications of the proposed approach.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a transformer based trajectory estimator for 2D navigation. The proposed method is shown to not be limited by input size as other learning based approaches. In the experimental section, the authors show that their estimated trajectories result in speed up and performance boost for planning in comparison to traditional and other learning approaches.",
            "main_review": "Strengths:\n+ Reduced planning time over traditional approaches (there are some issues here that I talk about in the weaknesses).\n+ Improved performance over learning based approaches.\n+ Method does not need fixed input size as other learning based approaches.\n+ Well written paper.\n\nWeaknesses / Comments / Questions:\n\n- Time to pass through MPT:\nThe key weakness in this work is not counting the time it takes to MPT as the total processing time. The authors argue that this can be seen as a preprocessing step, however, I think this may not be a valid argument. The entire method should be timed in order to compare to the other baselines to have a fair comparison. I would like to see these times to conclude whether this method in fact pushes the envelope against traditional methods. If the baselines have pre-processing steps, these times should be included as well for fairness.\n\n- Using the hybrid approach with learning based baselines:\nThis author proposes the use of a hybrid approach to addressed any trajectories that miss reaching the goal. The authors use this with their proposed method, however, it is not used with the other learning based approaches. I think this experiment is necessary for completeness to see whether the MPT is a key component in the performance and it could not just be replaced, say, with the MPnet or UNet baselines.\n\n\n- Sampling k during training:\nI don't quite get the reasoning behind sampling k for the positional encodings other than to make the training noisy, and thus, difficult for the model to memorize the positions in the grid. If this is the case, I would encourage the authors to investigate this further and maybe find a more mathematical explanation for this type of regularizer which obviously seems to work.\n\n\n- Method marginally outperforms baselines in the \"Dubins car model\" experiments (Table 4).\nThis goes back to measuring the time it takes to run MPT. What happens if this time is included? Is the baseline faster than the proposed method?",
            "summary_of_the_review": "I personally like this approach, but I feel that there are key missing questions that the paper does not answer in order to determine whether the claims are successfully substantiate or not. I am willing to increase my score depending on whether the author's response address my questions.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents a Transformer-based planning approach that attempts to use an attention mechanism to reduce the search-space of a traditional planner such as RRT*. Approaches such as Informed RRT* constrain the search space of the planner, which allows much faster convergence time. In a similar way, the authors present an approach that uses attention on a 2D map to create a \"mask\" for a standard planner to draw samples from. The proposed Motion Planning Transformer (MPT) works by using sliding window on the original map, creating features that are then positionally encoded (which helps with generalisation) and then put through a transformer encoder and classified. The result is a prediction, for each patch, as to whether a planner should draw samples from it. When combined with a planner it is able to achieve state-of-the-art results in a reduced time.",
            "main_review": "The paper is well written, clearly presented and well-motivated. The proposed approach combines learning-based and sampling-based planning approaches using the strength of each approach to solve the task it is best suited to. The authors present several important contributions, from using transformers to ensure the planner can attend the entire map in deciding which areas to sample from, to using positional encoding in a way that allows them to generalise to unseen maps. Furthermore, the approach is presented in a way that makes it reasonable to assume it will generalise to significantly larger maps.\n\nThere are a couple of concerns. Firstly, the evaluation presents 2 planning scenarios: point-robot and Dubins car. The results on the point-robot are well presented and thorough, however I was less convinced by the Dubins-path results. It seems like what the MPT has to learn is very similar in both cases, and there is very little evidence that the MPT network is learning any kinematic-specific strategies. It would be interesting to explore this area further, perhaps with maps that have both kinetically feasible and unfeasible trajectories. Ideally, a point-robot MPT would choose trajectories regardless of kinematic feasibility, whereas the Dubins car model would choose the kinematically feasible one. Secondly, the approach seems to be limited to 2D. While I understand that this is a large domain in-and-of itself, it is important to notice that RRT* and its variants are designed for higher-dimensional spaces and can be outperformed by simple A* in 2D grids. It would be interesting to see if the approach can generalise to higher dimensions, making it feasible for robotic arms and more complex planning spaces. It seems like the windowed approach should be extendable to higher dimensions? Finally, the authors do not compare against learning-based planners because they did not perform well, however it would be interesting to see the results for context.\n\nA few minor things:\n-Page 4: “For maps larger than then the” -> remove then\n-Page 5: “For Dubins Car Model” -> “For the Dubins Car Model”\n-Figure 5, consider re-moving RRT* to make the other approaches more easy to read. RRT* is not constrained in sampling, so it makes sense. \n-Page 6: Figure 4 is referenced before 3, might be worth re-ordering figures. and 4 are referenced in",
            "summary_of_the_review": "The paper presents a solid approach to pathplanning that solves the problem in an interesting way. There is a good amount of novelty and  experimental evidence and the approach is technically sound.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a transformer based method for 2D motion planning. The aim of the method is to work in any resolutions of the 2D map and the attention of the transformer enable to get the best possible regions through which the desired path should exist. A  traditional planners is then used given the extracted region to generate the final collision-free path. The results shows the efficacy of the transformer guided path planning using RRT* in different resolution of maps.",
            "main_review": "There are two important aspects of this method. First is to get a region so that a sampling based method can derive the path within that region to save time in the path planning. Second due to the transformer this method can scale to different resolution of map. \nThe neural guided RRT is not new.  Like \"Neural RRT*: Learning-Based Optimal Path Planning\" also aim to get a guidance. Similarly \"Path Planning using Neural A* Search\" which uses different planner but similar philosophy. It would be better to position the advantage of the current method for a fix resolution against these similar existing method which share same philosophy. \nThe resolution invariant property is nice. It will be good to see the generalisation of this method as it uses a classifier to train. In the paper it is mentioned that the training used both maze and random forest and test on both. To understand what the network learns can we see the result trained on maze and test on random forest or train on both and test on different map with different densities? If the maze has higher density  (means very narrow path/free space) does the method without further training work? Any classification inaccuracy may lead to a completely different and longer path which the author also shows in appendix. So a study on the generalisation ability out of distribution would be good given the IRRT* performance.\nIn the Table 1, the time exclude the MPT planner which I think is not correct because MPT planner is the key to reduce the RRT* time. The time should be the total time of MPT based mask generation + RRT* planning. Also the path length should also be compared across method. A comparison of time and path length with \"Learning to Plan in High Dimensions via Neural Exploration-Exploitation Trees\" will be better in a single resolution.\nThe authors mention about a few choices of the position encoding of the transformer. This is an important choice to take care and a detail ablation of this with the resulted path can be useful to the reader to understand what is happening there.",
            "summary_of_the_review": "The method is good as a pipeline but it needs few careful experiments to position the claims with respect to state-of-the-arts. While part of the method's philosophy is already existed like neural guided path planner, the other part where the resolution invariant is being claimed is good. Some of the comparisons are important to position the entire pipeline against SoA  and few metrics needs to be shown (like the time) to understand the computational complexity.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}