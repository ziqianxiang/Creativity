{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "First, I would like to thank all the reviewers for their efforts in reading and understanding this paper. I tried to read the paper as well and I also find it's really difficult (if possible) for me to understand the ideas presented here. The most important task in writing a paper (as Reviewer Svha also suggested in his/her review) in the field of machine learning is to explain to your peers what is the problem you are trying to solve and how you solve (or partially solve) that problem. I think there is a consensus among the reviewers that the paper did not do a great job of that. I am not questioning the quality of the idea or the research here, but I think the paper here will need to do a significantly better job here in explaining the idea before it can be a good ICLR publication."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "It **seems** that this paper tries to propose a general neural network architecture that is composed of multiple layer types based on a tree structure.",
            "main_review": "Generally, I don't (and shouldn’t) reject a paper because of writing.\nHowever, the writing of this paper is too hard to follow.\nAmong hundreds of papers I have reviewed, it is the most distracting one.\nEven if there are significant technical contributions, I still cannot accept this paper because of the writing.\nHowever, the idea of applying a tree-like structure to generalize neural networks is still interesting.\nI am sorry that I cannot complete the reading because of the writing.",
            "summary_of_the_review": "Writing is too hard to follow.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N.A.",
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper introduces a new neural network architecture called a \"Graph Tree Neural Network\". This architecture is inspired by several properties of the human brain: 1. it's multi-modal; 2. it allows for cross-input reasoning; 3. it can adapt its internal structure to the current input; 4. it can adapt the amount of computation per input; and 5. the human brain is larger than most neural networks.\n\nThe architecture itself relies on mode-specific representation components: later, in the experiments, they introduce targeted architectures for images, speech and text. These representations are then consumes by a convolutional logic that allows full communication within the different components of the network. Finally, these components can be recursively combined to yield a full graph tree neural network.\n\nThis architecture is then evaluated on several domains covering speech, vision and nlp datasets, showing consistent improvement over the selected baselines.",
            "main_review": "From what I understand, I really like the research idea and direction of the paper. Unfortunately, I had extreme difficulties understanding it because the paper lacks a good structure and a fluent narrative (details below). Therefore, I cannot properly comment on the paper's strengths and weaknesses. Instead, I list the questions I had while reading the paper.\n\n* Apart from having a new architecture, I do not understand the exact problem setting of the paper: is it supposed to be multi-modality? Multi-task learning? An analysis of the relationship of brain-like learning and artificial learning? This is important, because a proper problem setting would determine the choice of baseline architectures and experiments. As things are, the paper feels \"ungrounded\".\n* In the paper's claims, several related works are not mentioned. In particular regarding dynamic computation graphs and dynamic resource allocation, the work around conditional computation (mixture of experts; more relevant, the \"Routing\" literature (Routing Networks and the Challenges of Modular and Compositional Computation) come to mind)\n* Terminology throughout the paper is strange; instead of \"inputs\" and \"outputs\" or \"predictions\" we have \"start\" and \"endpoints\". I understood this eventually, but it created some confusion for me in the beginning.\n* Somewhat related to the first point, I'm missing a \"big picture\" overview of what the proposed architecture tries to achieve, and why the proposed architecture is good at achieving this. This is particularly relevant for section 2: I wish for an overview at the beginning of each subsection (2.1-2.5), describing what the respective component is mean to achieve, how it does so, and why it was designed in this particular way.\n* Unfortunately, my confusion continues in the experimental section. I suspect that the experiments are non-standard, but I cannot actually tell, because the experimental setup is not described anywhere in detail. \n* I am not perfectly sure on the experiments. However, I still want to point out that the baselines are out of date. That may be fine, but then the choice of these baselines needs to be motivated.",
            "summary_of_the_review": "Overall, I like the goal and the ideas behind the paper. Unfortunately, the paper is extremely unclear in its presentation: I could not put individual sections into context, I do not know what the goal of the paper is, and I don't understand the results. Therefore, I cannot recommend this paper for acceptance.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the authors propose Graph Tree Neural Network (GTNN) a new learning model that is structured as a graph tree (i.e., a tree with links between siblings) where each node has to process the data coming from its children. GTNN can take as input data of various types where each node will be processing the data differently according to the data type. The authors describe how (de)convolution and recurrent neural networks can be achieved within GTNN. Experiments were conducted on some classic deep learning datasets of three different data types: images (MNIST), language (IMDB), and sound (Speech Command). The goals of those experiments were two see whether GTNN can process various datasets of different types and if GTNN can produce an output vector that retains all the information coming from the different data types.",
            "main_review": "## What I like about this paper\n\n- The idea of unifying different architectures (RNN, Convolution, MLP) into one big neural network organized as a graph tree.\n\n- Dealing with multiple types of input modality is an important research direction.\n\n\n## Concerns\n\n- The paper is hard to follow. Several sentences are not well-formatted. I found it hard to extract the relevant contributions and claims from this paper.\n\n- I'm not convinced by the empirical results presented in section 3. All the numbers look very close to each other and might be within the error margin. I would have liked to have the source code, to test it myself.\n\n- On page 7, the authors mention the image GT has three nodes since \"the occipital lobe is the farthest from the frontal lobe in the human brain\". I find that justification dubious as longer axonal connections don't mean more processing. Citations would be needed to back up this claim.\n\n- A related work section is missing? I doubt there is no other work that can be related to this one. What are the limitations of let's say Graph Neural Network compared to Graph Tree Neural Network?\n\n- P.1: what does it mean to \"freely integrated\"?\n- P.1: there's a claim about \"existing networks are designed ... layers are fixed\" which is not true. Several papers describe approaches to adapt how information is processed in dynamic ways, e.g. ResNet [1], Adaptive Computation Time [2], etc.\n\n#### References\n- [1] He, Kaiming et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016)\n- [2] Graves, Alex. “Adaptive Computation Time for Recurrent Neural Networks.” ArXiv abs/1603.08983 (2016)\n\n### Minor\n\n- This paper has heavy usage of acronyms. For the sake of clarity in the paper, I'd use the full names rather than the acronyms when talking about GT, GTN, GTNN, GTR, and GTC. Otherwise, I'd try to reduce their usage as much as possible.\n\n- I'm concerned by the name used to refer to the proposed approach. To me, Graph Tree Neural Networks means a neural network capable of taking graphs shaped as trees as input.\n\n-----\n### Typos\n- A lot of sentences don't start with an uppercase letter.\n- Missing whitespaces before several opening parentheses.\n",
            "summary_of_the_review": "I believe designing novel architectures inspired by the human brain is an interesting and important research direction. However, in its current state, this paper was not able to convince me that the GTNN architecture was one worth pursuing. To be honest, it wasn't clear to me what were the actual contributions of the paper, and how the experiment results are backing up the claims made in this paper. For those reasons, I recommend rejecting this paper for ICLR.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}