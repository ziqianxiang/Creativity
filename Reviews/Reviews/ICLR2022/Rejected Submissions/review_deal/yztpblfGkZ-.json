{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes a graph convolution operator (BankGCN) to be used in graph neural networks. The reviewers mainly raised concerns about the limited of novelty in the light of numerous previous works that are similar or address similar problems as well as lacking evaluation. While the rebuttal addressed some of the concerns, the overall impression is that the paper is not of sufficient methodological or experimental significance for the conference."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The manuscript proposes a novel convolutional operator dubbed BankGCN, that given a node input signal, first projects it in different subspaces using a group of projection functions (one for each subspace), and then applies to each projection an adaptive filter. \nAs projection functions, the authors use Linear projections. The model was tested on the Graph classification task.",
            "main_review": "The manuscript proposes an interesting point of view on graph convolutional operators. The proposed idea is clearly defined and theoretically grounded. The empirical evaluation of the operator shows interesting results, but some points about the experimental methodology have to be clarified since the comparison with the baseline models seems not completely fair.\n\nIn the introduction, the authors discuss the spectral convolution networks, and they claim that the lack of proper sharing schemes between filters makes these models redundant and even prone to overfitting. To me, this point is not completely clear, and in particular the criticism about the sharing scheme. Since it is one of the motivations on which BankGCN relies, in my opinion, this point it should be extended and further discussed.\n\nIn section 4.2 the authors state that f_{[p]} are defined as linear projections. Moreover, all the functions project the input in subspaces that have the same dimension. In the performed experiments how similar are the projections computed by the various f_{[p]} have the same dimension? Does their similarity influence the expressiveness of the model?\n\nThe definition of the filters (section 4.3) recalls the one defined by Wu, Felix, et al. in  \"Simplifying graph convolutional networks.\" (SGC) and the LGC proposed in (Navarin, Erb, et al. “Linear Graph Convolutional Networks.”), that exploits a normalized Laplacian instead of the Chebyshev polynomial. Considering that these models are very similar to the BankGCN with p=1, they should be also considered in the discussion and the experimental comparison.\n\nFor what concerns the experimental comparison to me it is not clear how the various models were validated, and so it is difficult to assess whether the reported comparison is fair or not. Indeed in section 5, the authors state:\n“The network in the experiment consists of four convolution layers, one graph-level readout module and a linear classier.” \nwhile in appendix C they state: “The trained model with the best validation performance is selected for test.” and on the next page: “...the learning rate is 0.001 and the batch size is 64. The number of training epochs is set as 500, and early stopping is employed with patience 30. Finally, we obtain the following optimal “hyper-parameters through grid search: weight decay ∈{0,1e−5,1e−4}and \\gamma ∈{0,0.1,10}.”\nThe fact that the model with the best performance on the validation set was selected is of course correct, but my understanding only the weights decay and \\gamma were selected through a grid search process while the other hyper-parameters seem to be set without performing any validation procedure.  Moreover, it is not clear whether the validation procedure was also used to select the hyper-parameters of the baselines models. Considering models that are all validated following the same validation procedure is crucial to perform a fair comparison. Indeed, as shown in (Errica et al.  “A Fair Comparison of Graph Neural Networks for Graph Classification”') the policy used to validate the hyperparameters of a model highly impacts the model's performance. The authors also state that “For a fair comparison, the results of baseline models are obtained with the same configurations as BankGCN using the public versions provided in the pytorch-geometric package”. In my opinion, it is not a fair way to compare the models using the configuration selected for the BankGCN,  mainly because each model has to be validated independently, in order to select the hyper-parameters that ensure the model to achieve the best performance (on the validation set). \nIn section 5 “For a fair and complete comparison, we consider two cases, (i) the same number of features, i.e., all of the models with the same number of feature maps per hidden layer, and (ii) the same number of parameters, i.e., models with nearly the same number of free parameters per hidden layer”. Comparing the models that exploit the same number of parameters is a very interesting analysis and highlights the expressiveness of the models but it does not show how good each model is in the graph classification task (as said before, this evaluation would require validating each model independently).\n\nThe results reported in Table 1 show that, in many cases, the accuracy gains obtained by the proposed method seem within noise levels due to high variance. Therefore it would be better to perform a statistical significance test,  to identify which improvements are significant. Note that the methodology followed to obtain the results and validation policy influence also the variance of the accuracy detected for each method.\n\nMinors:\neq (6): the last parenthesis is missing\nthe acronym IGFT is used without a prior definition of its meaning\nFor what concerns the paper organization, putting the notation in the appendix makes it a bit complex for the reader to follow the mathematical definition of the various components. in my opinion, the notation definition has to be present in the paper before the start of the theoretical discussion.\n",
            "summary_of_the_review": "The manuscript proposes an interesting point of view on graph convolutional operators. The proposed idea is clearly defined and theoretically grounded. The empirical evaluation of the operator shows interesting results, but some points about the experimental methodology have to be clarified since the comparison with the baseline models seems not completely fair.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors introduce BankGCN, a graph convolutional network that learns (chebyshev) polynomial filters over the graph. BankGCN includes an initial subspace projection, and a cosine similarity regularization to encourage diverse filters. BankGCN is compared to a number of other graph networks on standard whole graph classification tasks.\n\nThe authors claim that most architectures of MPGCNs are limited in that they (1) focus on low frequency information and (2) lack a proper sharing scheme between filters, and that BankGCN addresses these two issues. ",
            "main_review": "Strong points: The ablation studies are quite comprehensive and give a good sense of the effects of the K, s, and \\lambda parameters. \n\nThe Figures are clear and informative, the notation and writing is clear.\n\nWeak points: \n\nBankGCN tackles a by now well characterized problem, that of oversmoothing in the original formulations of graph convolutional networks. I find the claim that BankGCN is more powerful than \"most\" MPGCNs to be somewhat misleading, while true for those mentioned in the introduction, there are many new MPGCN architectures that address this problem, (mentioned below) as well as Scattering GCN and GPR-GNN (as mentioned in section 2). Comparison to at least some of these methods, or justification as to why these methods are not comparable is a must in an empirically driven paper like this.\n\nWhile the experiments are well designed, although code is not provided, the improvements are relatively minor as compared to the variation inherent in these datasets (as seen in similar work). I would encourage the authors to try different datasets where more substantial differences between methods may emerge, as on these benchmarks improvement is difficult to show.\n\nThe \"sharing scheme\" amounts to a linear layer before message passing, I believe this has been used in a number of earlier works (although I could be convinced otherwise).\n\nQuestions:\n\nIt is mentioned that section 5.2 compares models with the same number of free parameters per hidden layer, how is this done and why was this done on different datasets (Cifar / molhiv)?\n\nOther points:\n\nI find the way the authors use the term MPGCNs a bit confusing, as claims are made about BankGCN in contrast to MPGCNs where BankGCN. is a message passing GCN architecture. Could this be clarified further in the writing?\n\nWhile the notation is well explained, some of it is a bit unusual, for example \\mathbf{\\alpha} is a vector of \\alpha coeffs, but R is a vector of \\mathbf{r} features. T_k are chebyshev polynomials while T_\\Omega(G, Y) is the cross-entropy target.\n\nI believe the reference to Johannes Klicpera et al. (2019) in section 2 paragraph 2 is incorrect, you might have meant to reference the work:\n\nJohannes Klicpera, Aleksandar Bojchevski, and Stephan Gunnemann. Predict then propagate: Graph neural networks meet personalized pagerank. In International Conference on Learning Representations, 2019\n\nRather than \n\nJohannes Klicpera, Stefan Weißenberger, and Stephan Gunnemann. Diffusion improves graph learn- ing. In Adv. Neural Inf. Process. Syst. 32, pp. 13354–13366, 2019.\n\nIn fact the work that you reference employs a learnable filter bank contrary to your claim in section 2.\n\nAdditional recent work in \n\nHao Zhu and Piotr Koniusz. Simple Spectral Graph Convolution. In ICLR 2021.\n\nAlso tackles the problem of oversmoothing and learns polynomial filters of the Laplacian using a slightly different formulation.",
            "summary_of_the_review": "\nWhile the experiments are well designed, given that there are very few comparisons to similar works that claim to solve the problems of oversmoothing, and limited theoretical contribution I lean towards rejection.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes BankGCN, which simplifies spectral graph neural networks by utilizing an adaptive filter bank to extend the capabilities of GCNs beyond low-pass features. Compared to existing spectral graph convolutional networks, which have numerous free parameters, the proposed method reduces parameters by sharing learnable filters. The empirical study validates that BankGCN shows good performance on the graph classification task.",
            "main_review": "**Strengths**\n\n(1) This paper addresses the limited flexibility of message passing convolutional networks (MPGCNs) because of their low-pass properties, which are one of the most important issues of Graph Neural Networks.\n\n(2) This paper is well written and structured.\n\n(3) This paper provides comprehensive experiments. In particular, the discussion of the number/order of filters and regularization is well presented to understand.\n\n**Weakness**\n\n(1) The related work is not detailed. There are many graph neural networks to deal with low-pass properties of MPGCNs [1], [2]. \n\n(2) I think the novelty of this paper is limited. The adaptive graph filter is a not novel method. For example,  GPR-GNNs [3] proposes adaptive polynomial graph filtering, which is similar to the proposed method. Please comment the difference.\n\n(3) The evaluation for the proposed methods is not sufficient. It is highly recommended to conduct node classification experiments to validate BankGCN. There are node classification datasets, which are dependent on high-frequency information not low-frequency information. *i.e.,* texas, wisconsin, and cornell. [2],[4]\n\n*[1] Scattering gcn: Overcoming oversmoothness in graph convolutional networks, Min, Yimeng et al., NeurIPS 2020.*\n\n*[2] Beyond low-frequency information in graph convolutional networks, Bo, Deyu, et al., AAAI 2021.*\n\n*[3] Adaptive universal generalized pagerank graph neural network, Chien, Eli, et al., ICLR 2021.*\n\n*[4] Geom-gcn: Geometric graph convolutional networks, Pei, Hongbin, et al., ICLR 2020.*",
            "summary_of_the_review": "Overall, I recommend marginally below the acceptance score. My concern is about the limited novelty and not enough experimental results. Hopefully, the authors can address it.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a new variant of GCN model, namely BankGCN, based on a novel graph convolutional operator. The main idea of BankGCN is to the sharing scheme among different filters to adaptively capture information from different frequencies. Through detailed discussion, BankGCN is claimed to be equivalent to the learnable message passing mechanism across K-hop neighborhood. Experiments on graph classification tasks over a collection of graph benchmark datasets demonstrate that BankGCN could outperform many state-of-the-art spectral-based GNNs.",
            "main_review": "Strengths:\n- The paper is easy to follow with clear motivation and is well written. The design of BankGCN is closely rooted in Graph Signal Processing, which has merits.\n- Due to the common assumption on the homophily of underlying graphs, it is of great importance to go beyond low-pass filter types of GNNs.\n- Empirical observations on the filter bank and its response to the proposed model are interesting.\n\nWeaknesses:\n- The novelty of the proposed BankGCN is limited. BankGCN is closely related to polynomial GNNs with regularized and learnable coefficients. Thus, several designs towards simplifying polynomial GNNs that were proposed recently [1,2,3] might have relatively the same power of expressiveness, since the learned filters could be very similar. The authors are suggested to mention them, add discussions, and especially have an empirical comparison with them to demonstrate the superiority of BankGCN. Meanwhile, several other attempts that try to integrate adaptive frequency response filters into the design of GCNs such as [4,5] are missed and should be added in the discussion of Related Work.\n- As one main highlight of the design of BankGCN is to use the filter of K-order polynomial function space instead of computational-intense eigendecomposition, this sacrifice should be discussed more in detail, such as the comparison of the flexibility of learned filters and the degree of acceleration through computational analysis.\n- For experiments, the concerns include:\n    - The diversity spectral information of the chosen benchmark datasets, such as the average homophily ratio of all graphs, should be quantitatively analyzed since this is the main contribution of the proposed model, and how the learned filters correspond to specific graphs would demonstrate more convincingly.\n    - Most graphs in the benchmarks seem to be small and it is a known problem with those small and noisy datasets. Thus, instead of simply averaging several runs, cross-validation should be conducted and report the confidence interval.\n    - More recent baselines that also aim to use K-order polynomial function to go beyond low-frequency in graphs should be added. I understand that the design of BankGCN is inspired by the graph signal processing, but the proposed architecture is so similar to these methods, and only comparing with spectral-based GNNs is not adequate.\n\n[1] Polynomial Graph Convolutional Networks, OpenReview 2020\n\n[2] Stacked Graph Filter, arXiv 2021\n\n[3] Simple Spectral Graph Convolution, ICLR 2021\n\n[4] Graph Neural Networks with Adaptive Frequency Response Filter, CIKM 2021\n\n[5] Spectral Graph Attention Network with Fast Eigen-approximation, CIKM 2021\n",
            "summary_of_the_review": "My concorns are mainly from two aspects:\n- The novelty of the proposed method is limited. The contribution of this paper needs further discussion between BankGCN and other efforts on designing filters from K-order polynomial function space. \n- The current experimental analysis is insufficient and can be further improved to make it more convincing.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a graph convolutional networks where, at each layer, the graph signal is decomposed with an adaptive filter bank.",
            "main_review": "Strengths:\nGraph neural networks are timely and of great interest to researchers.\n\nWeaknesses:\n\nIntegrating filter banks in graph neural networks is not new. There has been many studies, such as the work of Tremblay et al. (design of graph filters and filterbanks) and Bianchi et al. (ARMA-based filtering in graph neural networks), as well as related work like Thanou et al. (parametric dictionary learning for signals on graphs). Besides these cited papers, there are other papers on the same topic (in filter-adaptation for graph convolutional networks), such as:\n- Li, S., Kim, D., & Wang, Q. (2021). Beyond Low-Pass Filters: Adaptive Feature Propagation on Graphs. ECML/PKDD.\n- Wu, Z., Pan, S., Long, G., Jiang, J., & Zhang, C. (2021). Beyond Low-pass Filtering: Graph Convolutional Networks with Automatic Filtering. ArXiv, abs/2107.04755.\n- Bo, D., Wang, X., Shi, C., & Shen, H. (2021). Beyond Low-frequency Information in Graph Convolutional Networks. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No. 5, pp. 3950-3957).\nOnly the latter was cited in the submitted paper. While these papers are new, they do cite older papers on the same topic as this work.\n\nRoughly, the proposed method consists of integrating FIR filter banks, with the ARMA-based filtering of Bianchi et al. considers IIR filter banks.\n\nExperiments need to be extensive. While the authors consider several conventional graph convolutional networks in the comparative analysis, only the ARMA-based filtering of Bianchi et al. seems to be comparable in terms of adaptive filter banks. The authors need to provide extensive experiments by comparing to other related methods, considering at least the aforementioned ones.\n\nThe authors need to provide a thorough analysis of the computational complexity of the proposed method. While there is one sentence on computational complexity \"It achieves linear computational complexity with O(K|E|d) and constant learning complexity, similarly to most existing MPGCNs\", this needs to be demonstrated. In practice, the proposed method requires including a filter bank per layer and estimating its parameters can be cumbersome for large-scale graphs and multiple channels. The authors need to provide some experimental analysis of both the memory usage and the computational time, and comparing them to other methods in the literature.\n-------------------------\nWe thank the authors for addressing our concerns.",
            "summary_of_the_review": "Interesting work. However, there are several related methods in the literature.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}