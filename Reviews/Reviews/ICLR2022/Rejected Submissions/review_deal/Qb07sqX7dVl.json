{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper presents an approach to weak supervision to address the possibly low-coverage of rule-based labeling functions, by assigning similar labels to similar instances (where the similarity is computed in feature space).\n\nThe reviewers main concerns were the presentation, as well as the experimental protocol and results. Several directions for improvement have been identified by the reviewers and acknowledged by the authors, but in the current state of the submission the consensus is that the paper is not ready for publication."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes an extension to the data programming / weak supervision method of Ratner et. al. 2016, in which heuristic functions called labeling functions (LF) are used to label training data.  In the proposed extension, called \"reinforced labeling\", points that are close to those labeled by a labeling function in feature space (as determined by a heuristic \"gravitation\" algorithm) are also labeled, thereby augmenting the set of programmatically labeled examples.",
            "main_review": "Strengths:\n- (S1) The proposed approach tackles a relevant area of the space with appealing top-level motivation and intuition.\n\nWeaknesses:\n- (W1) Lack of adequate comparisons to related work: There are many weak supervision approaches that have been proposed since the original Snorkel paper referenced and compared to in the experimental results; some of which are referenced in the paper, but never actually compared to.  These more recent papers address many of the proposed issues- incorporating data features, boosting generalization with joint learning, etc.  Given the motivation of the paper, and claims in the paper that the proposed approach is better than these other approaches cited, it seems to be a major gap that no comparisons are made to these other methods in the experiments.\n- (W1a) Additionally, this work is very similar to classic label propagation techniques in semi-supervised learning, and should have been compared to these as well.\n\n- (W2) Cherry-picked experimental results: There are several aspects of the experiments that raise significant questions as to the protocol with which they were conducted, and the representativeness of the experimental configurations; for example:\n  * (W2a) In the main results table (Table 1), Snorkel is reported as having an average F1 of 0.03 on the YouTube dataset (this is a score so low as to be actively difficult to achieve in most settings!).  The YouTube dataset, as noted in the paper and appendix explicitly, is *from the Snorkel OSS intro tutorial, in which Snorkel achieves a 94+% accuracy* (see snorkel.org/use-cases/01-spam-tutorial).  Yet somehow, the results here are reported as 0.03 F1; according to the appendix, because of an arbitrary subselection of LFs.  In fact, in Fig.5(a) you can see that with more LFs, the score goes up significantly and either matches or outperforms the proposed approach (see next point for more here).  It therefore seems extremely misleading to have presented the scores as presented in Table 1 as main results.  If the objective was to show that in certain low-LF situations, the proposed approach can offer benefits, this should have been messaged clearly upfront when reporting main results.\n  * (W2b) The proposed approach is (like most/any clustering-based approaches) highly dependent on a threshold parameter, the so called \"IQR factor\".  In the main results, this is apparently \"chosen by a data scientist\".  This raises significant questions as to experimental protocol\n \n- (W3) Lack of sufficient ablation and/or formal motivation for proposed approach: The proposed approach is presented in a way that makes it seem fairly arbitrary compared to existing approaches for e.g. weak supervision, clustering, label propagation, etc.  However, minimal ablations of the approach are performed (just the threshold factor and the distance metric) to actually motivate or defend the heuristic approach taken.  This makes it hard to gain insight from the paper about why this approach should work and why it should work better than other approaches- esp. given the lack of other methods compared to.",
            "summary_of_the_review": "This paper lacks sufficient comparison to other more contemporary weak supervision work, lacks sufficient motivation and/or ablation w.r.t. the heuristic design choices taken, and has potential issues with experimental protocol.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper describes a method to improve the coverage of labeling functions in weak supervision. Labeling functions in weak supervision can abstain if they do not vote on a particular point. The method replaces abstentions with a gravitation-based method based on the other points that the labeling function has voted on. The method uses similarities between features of points for the gravitation-based method. The authors claim performance benefits over Snorkel.",
            "main_review": "This paper discusses an interesting method and is well-written and clear to follow. However, there appear to be major problems with the evaluation, and the method is not novel.\n\nEvaluation:\nThe authors only compare to one baseline, Snorkel, and report significantly worse performance on benchmark datasets than reported elsewhere in the literature. Consider the YouTube dataset -- this is a standard weak supervision dataset and is used as a tutorial for the open-source Snorkel library.\n\nIn the tutorial itself (https://www.snorkel.org/use-cases/01-spam-tutorial), Snorkel achieves an accuracy of 94.4%. However, the authors report an accuracy of 54%, with an F1 of 3% in Table 1. This suggests that the authors did not run Snorkel correctly. In fact, the authors' reported performance on the YouTube dataset (75% accuracy) significantly underperforms the tutorial notebook. This discrepancy is because the authors did not use all the labeling functions in the YouTube dataset. This is a non-standard evaluation procedure, and makes the results hard to compare and believe.\n\nIn future iterations, the authors should also consider comparing against the baselines in the WRENCH benchmark (https://arxiv.org/pdf/2109.11377.pdf) as well, to ensure that their evaluation is fair and standard. (This paper was released <1 month before submission, so I don't count it against them -- but it's a useful datapoint nonetheless).\n\nI am less familiar with the wine and weather datasets, since I have not seen them in other WS datasets, but performance is suspiciously low for those as well (F1 of 8% for Snorkel on Weather, which suggests that Snorkel was not tuned correctly).\n\nNovelty:\nThe method itself is almost identical to that in Chen et al 2020 (https://arxiv.org/abs/2006.15168). The authors cite this paper but do not compare against it. Chen et al also focus on improving labeling function coverage by replacing abstentions based on similarity in feature space to other points. Both methods feature a hard threshold after which they no longer transfer labeling function labels. The one new point is that this paper discusses a method for automatically adjusting the threshold (section 3.2), whereas Chen et al tuned the thresholds manually. I suggest anchoring more strongly on this contribution in the future.",
            "summary_of_the_review": "This paper is clearly-written, but it has significant flaws in its evaluation, and the method is only marginally novel. Therefore I recommend reject.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The authors proposed a new approach called reinforced labeling to enhance the data programming weak-supervision approach. Labeling functions in data programming can only produce pseudo-labels for part of the dataset, and thus the authors propose to use similarity between two data features to find the lacking pseudo-labels, and thus enhance the existing data programming methods.",
            "main_review": "Strength:\n1.\tThe research problem of weak supervision is very important and worth studying.\n2.\tThe proposed approach for data programming can improve the performance of weak supervision.\n3.\tThe proposed approach alleviated two limitations of existing data programming methods: (1) coarse information, and (2) lack of generalization compared to existing Snorkel framework by taking data features into considerations.\nWeakness/concerns:\n1.\tThe technical novelty of this work is limited as the proposed reinforced labeling is similar to KNN with pseudo-labels. How these two methods differ in underlying motivation, method design, and experiments is not properly explored.\n2.\tThe term ‘generative’ would seriously confuse readers as it was not referring to data generation but to label generation, it would be better if the authors could use ‘label generative’ instead of ‘generative’.\n3.\tThe experiments are not convincing. What is the supervised learning method the authors used to compete with their weak supervision method? Is there any improvement for the supervision performance if the authors incorporate their weak supervision approach (or pseudo-labels) into a supervised setting?\n4.\tThe labeling functions seems to be rule-based functions that maps the features to the pseudo labels. I’m not sure how this can bring in new information to the downstream classifier when it is powerful enough. How do you avoid the error propagation problem? Also, if the pseudo labels are good enough, why do you need a downstream classifier as the pseudo labels are already the upper bound of the classifier?\n",
            "summary_of_the_review": "The work may be interesting for the automatic label annotation area, but the idea novelty is a little trivial and experimental results are not convincing.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper tackles the problem of designing data programming, which is a practical approach in weak supervision. The authors state that prior effects neglect to utilize data features during the generative process, and therefore have suboptimal performance. Based on this, this paper proposes to exploit the sample similarities to augment the outputs of labeling functions. Empirical evaluations are provided to verify the effectiveness of the proposed method. ",
            "main_review": "**Strengths**\n- This paper focuses on a practical problem. The motivation is good. \n- The experimental results are promising. In almost all cases, we can see that the improvement on F1-Score is clear. \n\n**Weaknesses/Questions**\n- The organization of this paper is a bit confused. I understand the core idea and algorithm flow of this paper after I read it a few times. \n- The writing is a bit illogical and makes readers confused. For example, this paper tends to augment the outputs of labeling functions for better generalization. Perhaps, it is expected to see that the intuition about why the proposed method works and how it handles the issues. However, the explanation is a bit incomprehensible. \n- Figures 2 and 3 are not informative. Also, the explanations are not sufficient. \n- In Equation 1, there are some hyper-parameters. How to control them in experiments?\n- I am confused about the auto-adjustment of $\\epsilon$, which is dependent on the dataset. However, it seems that we still need to determine $\\xi$ artificially to control $\\epsilon$?\n",
            "summary_of_the_review": "This paper tackles a practical problem. The idea is relatively novel. However, the writing of this paper needs to be improved to reach the acceptance line. The current version is hard to follow. Therefore, my score for this paper is negative. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}