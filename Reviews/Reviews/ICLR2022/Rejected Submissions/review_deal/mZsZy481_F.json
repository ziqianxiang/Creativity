{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes few-shot robust (FROB) model for classification and few-shot OOD detection. While the paper has some interesting contributions, all the reviewers felt that the current version falls short of the ICLR acceptance threshold and the consensus decision was to reject. I encourage the authors to revise the paper based on the reviewers' feedback and resubmit to a different venue.\n\nAs Reviewer r838 pointed out, that this paper uses TinyImages dataset which has been since retracted. I appreciate that prior work used TinyImages, but please see \"Why it is important to withdraw the dataset\" https://groups.csail.mit.edu/vision/TinyImages/ and consider not using the TinyImages dataset for future revisions."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper introduces a Few-shot OOD classification model, named FROB. Using self-supervised learning and generative model for enhanced confidence on class boundary, they achieve SOTA results compared to other similar OOD detection methods.",
            "main_review": "Strengths:\n - The proposed method achieves new SOTA results for OOD detection\n - The use of a generator for class boundary confidence training is novel/interesting\n\nWeaknesses:\n - The paper is quite difficult to read. Many lines are repeated in slightly varied ways; 20% of the lines could be removed to enhance clarity and to be more concise.\n - The proposed method is not well described in the main text. Specifically, equation 1 appears incorrect, as the numerator in term 2 is a vector of class logits \"f(Z_m)\". The model and training setup for O(z) is hard to follow and also appears incorrect as well. Equation 2 term 1 has \"j!=i\" yet i does not appear in the equation. z_j is not defined, is it the feature representation of the j-th outlier? The intuition of terms 2 and 3 is unclear, more details on exactly what each of the 3 terms are providing to the O model would increase clarity significantly.\n - Two of the primary metrics, AAUROC and GAUROC, are not defined. Both of those metrics are designed around adversarial perturbations, which is not clear how it applies to this work.\n- Table 1 has misreported metrics. Cifar10 with OE outperforms FROB on each individual dataset when looking at table 4 in the appendix.\n- The outlier dataset size appears to be randomly selected/cherry picked. How did the authors choose the 1830 SVHN examples or 73257 80M examples? \n- The final results of using various Outlier datasets are very strange. The results in table 12, C10 vs. C100 are non-intuitive to me. Why is it that using 100 SVHN examples gives the exact same AUC as 70k examples from 80M?\n- The main discussion of the results has several pointers to large appendix tables, making comparison of results difficult and must remind the authors appendix reading is supposed to be optional, where the main text is self-supported.\n- Finally, the overall results are not a fair comparison between methods. Specifically, how does OE perform when it only accesses the same number of outlier examples as FROB? Without data like this, I cannot evaluate the efficacy of FROB.\n\nAdditionally, the paper often uses 80M tiny images dataset, which has been retracted by the original creators.\n\nPost rebuttal:\nI do not feel as though many of my concerns have been addressed, and I will be keeping my original score.\n- I still don't understand the model and training setup for O(z). I do not even know if O(z) is an MLP or CNN.\n- The focus on AAUROC and GAUROC is misplaced. Hein's work with these metrics is somewhat related, but I do not think they are appropriate for evaluation with FROB.\n- Too much information is being used from the appendix as primary reference material\n- It still seems normal outlier exposure is a better choice on average than FROB, that is, I am still not sure what the guidelines are for choosing FROB over OE.\n- 80 Million Tiny Images is inappropriate to develop new algorithms with. Other conferences (NuerIPS) will desk reject any papers using this dataset.",
            "summary_of_the_review": "While this paper offers an interesting angle into OOD detection with few shot examples, several flaws regarding writing/methodology/results must be addressed before this work is ready for publication.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns",
                "Yes, Potentially harmful insights, methodologies and applications"
            ],
            "details_of_ethics_concerns": "This paper heavily uses the retracted 80M tiny images dataset.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes to generate the support boundary of the normal class distribution and combine it with few-shot Outlier Exposure (OE) to improve the OOD detection performance. The proposed method is evaluated at multiple benchmark datasets, and is compared with a few baseline methods. ",
            "main_review": "Strengths: the proposed method provides an improved version of outlier exposure (OE) method, by combining the self-generated boundary with the general outliers like 80M tiny images. \n\nWeakness:\n1. The main method is not clear to me, in particular, \n\nIn Eq (1), it is not clear which class is the f(Z_m) in the second term refers to. \nIn Eq (2), it is not explained what are i and j.\n\n2. The results are not clearly presented.\n\nIn Figure 2, the curve for CIFAR-100 starts from a AUROC number below 0.5, i.e. when there is no outlier exposure, the model trained on CIFAR10 cannot distinguish between CIFAR-10 test and CIFAR-100 test. That’s strange because the current SOTA AUROC based on a wide resnet model is already around 0.95 [1]. Similarly, the current SOTA AUROC for SVHN is at least 0.91 (see Table 7 in [2]), while the value indicated in Figure 2 is only around 0.6. \n\n[1] Hongjie Zhang, Ang Li, Jie Guo, and Yanwen Guo. Hybrid models for open set recognition. European Conference on Computer Vision, 2020.\n[2] Hendrycks, Dan, Mantas Mazeika, and Thomas Dietterich. \"Deep anomaly detection with outlier exposure.\" arXiv preprint arXiv:1812.04606 (2018).)\n\n3. The usage of the term few-shot is not well suited.\n\nThe paper claimed their method is for few-shot OE. But the results show that \"The FROB performance decreases with reducing number of SVHN fewshots. A small AUROC of 0.5 is reached for approximately 800 few-shots for CIFAR-100..\",  “for few-shots less than 1800, the modeling error of OE covering the full complement of the support of the normal class is high”. In my opinion, 800/ 1800 images for training should no longer be called as few-shot. \n\n4. The conclusion needs to be adjusted.\n\nI found the mean AUROC value in Table 4 for FROB was computed wrongly. It seems that OE, CEDA, and ACET all have higher mean AUROC than FROB. Also, in particular for near-OOD benchmark CIFAR-10 vs CIFAR-100, FROB only has 0.76 AUROC, much lower compared with 0.924 for OE.\n\n5. The effect of adding the generated boundary points O(Z) is not clear. \n\nIn Table 2, it would be good if you can provide OCC {FROB, FROB W/ OE SVHN} {w/ O(z), w/o O(z)} for direct study the effect of OE and O(z). \n\n",
            "summary_of_the_review": "The paper proposes an interesting method for combining generated OOD that is customized to the training data with the general natural outlier images for outlier exposure. However, the paper needs to be further polished such that the method and results can be clearly presented. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper addresses an important issue of Out-of-Distribution detection in a few-shot setting. The authors propose to generate negative samples in an adversarial way to increase the OoD performance. Additionally they augment the loss function with additional term for OoD samples. They perform experiments of 2 benchmark datasets: CIFAR-10 and SVHN.",
            "main_review": "The paper is generally well written and addresses an important issue of OoD detection combined with few-shot scenario.\nThe task is definitely very practical, of importance to the ML community, and with real world applications.\n\nThe proposed loss function. The only novel aspect would be the second term minimizing the softmax predictions for OoD samples. It would be good to compare it to existing, very similar loss functions in the OoD community such as [1, 2] where the intention is to penalize high softmax scores for OoD samples. What is the differentiating factor here?\n\nGeneration of adversarial examples. I am lacking here a deeper comparison to existing methods utilizing adversarial examples in a similar fashion (as close to border negatives) [3,4]. It is possible that the proposed approach is significantly different, however lack of comparisons and description does not allow to assess that.\n\nIn the reference section I am also missing a general discussion on adversarial methods and open-set approaches.\n\nSince the authors pursue the few-shot OoD task, I believe it would be crucial to underline the difference from few-shot open-set classification/recognition (such as [2]). Quick scholar search shows multiple papers in the topic that I believe to be of high relevance here. They also establish benchmark datasets to be the datasets from few-shot classification such as miniImageNet, CUB or tinyImageNet rather than CIFAR-10 and SVHN which are good as a proof of concept, but require a more modern and difficult dataset to back them up. \n\nThe experiments and ablation studies seem interesting and well thought, however I am lacking some deeper explanation between non-obvious numbers present in the paper describing chosen amount of training data (e.g., 915, 1830, 73257). There might be an easy explanation there, but without it they seem quite random. \n\n[1] Dhamija, Akshay Raj, Manuel Günther, and Terrance E. Boult. \"Reducing network agnostophobia.\" Proceedings of the 32nd International Conference on Neural Information Processing Systems. 2018.\n\n[2] Liu, Bo, et al. \"Few-shot open-set recognition using meta-learning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n\n[3] Kim, Jaehyung, Jongheon Jeong, and Jinwoo Shin. \"M2m: Imbalanced classification via major-to-minor translation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n\n[4] Kozerawski, Jedrzej, et al. \"BLT: Balancing Long-Tailed Datasets with Adversarially-Perturbed Images.\" Proceedings of the Asian Conference on Computer Vision. 2020.",
            "summary_of_the_review": "The paper touches interesting problem and proposes a valid solution, however lack of comparison with adversarial and few-shot open-set methods raises some questions as to novelty and quality of the experimental results. Additionally, experiments on more modern datasets should be performed, since CIFAR-10 and SVHN are quite simple.\n\nPost-rebuttal: The lack of deeper discussion on related work, the lack of comparisons and modern datasets still causes me to keep my initial score. I believe more work needs to be done in order for it to be accepted. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper focuses on classification and Out-of-Distribution (OoD) detection in the few-shot setting. They propose the Few-shot ROBust (FROB) model for classification and few-shot OoD detection. ",
            "main_review": "This paper is generally well-written and well-structured. The experimental results are sufficient. ",
            "summary_of_the_review": "Generating low-confidence samples from random vectors is a common practice. All the terms in Eqn. (2), which is the key component of the proposed method, are trivial. The technical novelty and contribution of the proposed method is somewhat weak. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}