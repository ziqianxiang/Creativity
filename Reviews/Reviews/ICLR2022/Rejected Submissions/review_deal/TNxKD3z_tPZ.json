{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper got uniformly strongly negative reviews.  The issue of estimating or bounding generalization accuracy from performance on the training set has a huge history and literature.  After considerable discussion the reviewers uniformly find this paper lacking in making a contribution to that literature."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper uses distances between the Persistent diagrams of states of neural networks and observe that during training there is a high correlation with the corresponding validation accuracy of the model. The authors tested their method on a variety of datasets.",
            "main_review": "I think the subject of the paper is interesting itself and using PH techniques to discover insights about deep learning models is indeed an interesting topic. However, I find the paper to be still in early stage of development and still requires work. Specifically, It think the authors need to really support  their claim for the title. Generalization is probably the hardest problem in deep learning model models and the claim that PH truly captures that is not explained well at least at the beginning of the paper. What is the metric for such a claim ? The authors mentioned \"that there exists a high correlation with the corresponding validation accuracy of the model.\" I do not think think that is enough reason to justify the claim of the paper.\n\nThe contributions are not very novel in my opinion. For instance :\n\nBased on principles of Algebraic Topology, we propose measuring the distances (Silhouette\nand Heat) between the PH persistence diagrams obtained from a given state of a neural\nnetwork during the training procedure and the one in the immediately previous weights\nupdate.\n\nMeasuring the distance between two weights of neural network is not significant contribution, unless you do something with it--which you are claiming next but this particular point is not a valid contribution in my opinion.",
            "summary_of_the_review": "I am sorry to say that I have to reject the paper. The paper is still in its early stage and requires more careful presentation especially in the beginning to make the contribution more clear and crisp ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents some empirical observations about the relationship of a persistent homology-based measure of learning dynamics and validation set error, during the training of deep neural nets. The paper opens with an introduction on persistent homology, then introduces an approach to study the structure of deep nets using topological data analysis (TDA) tools. Three case studies are then presented, for which a measure of change in topological structure of the network during training is compared with the validation set error. The argument of the paper is that the two measures are correlated, and therefore it may be possible to use the topological measure (which depends on the structure of the network only, and not on data) in place of the validation set error to assess the generalization performance of a deep net.",
            "main_review": "One merit of the paper is, to put it in very broad terms, the idea of\napplying tools from topological data analysis to the problem of\nunderstanding generalization in deep neural networks. In principle,\nthis is an interesting problem from a theoretical standpoint, and the\nidea of applying TDA is timely in the sense that many fields are now\nin the process of \"discovering\" topological techniques, and it is good\nto see some exploration of what they can be useful for. Unfortunately\nhowever, in my opinion the strengths of the paper stop here. I will\nnow go in detail over what I consider to be its major weaknesses.\n\nEmpirical studies can be valuable, but the complete absence of any\ntheoretical justification (or even just an intuitively plausible\nstory) for the main argument means that the experimental evidence\npresented carries all the burden of convincing the reader. My main\nconcern with the paper is that, unfortunately, the analyses presented\nare nowhere close to the standard of rigor and thoroughness one would\nexpect in this case. In particular:\n\n1. Unless I missed it, the paper never mentions the evolution of the\n   training error. For all I know by reading the paper and looking at\n   the plots, the tested networks never overfit the training data, in\n   which case I expect the training error to correlate to the\n   validation set error even better than the proposed metric,\n   something that would completely invalidate the claim of the paper\n   (i.e., that the topological measure is telling us something\n   nontrivial). In other words, in my opinion the interesting quantity\n   to understand the learning dynamics is not the validation set error\n   per se, but the generalization gap (or, in other words, the\n   validation set error should be considered together with the\n   training error, and not in isolation).\n2. As far as I understand (although the paper is quite confusing in\n   this respect - see below for more on this), the complex machinery\n   of TDA is used to characterize how much the network changes during\n   training. By looking at the plots in the paper, this quantity of\n   interest (rate of change) is typically large at the beginning of\n   the training, and small towards the end. Because this metric goes\n   down quite quickly, its cumulative value during training takes on a\n   saturating trend. This trend correlates with the validation set\n   error, which also saturates. The main claim of the paper is that\n   this correlation suggests a conceptual link between the two\n   quantities. But there are many quantities that presumably exhibit\n   similar trends during training! For instance, how about the norm of\n   the weight update vector (which could be conceptualized as a much\n   simpler way of measuring change in the network, and therefore could\n   be a natural comparison to the proposed metric), or even just the\n   learning rate, which is dynamically adjusted by RMSprop in the\n   proposed case studies?  Similarly to my first point above, these\n   are examples of quantities that could trivially correlate with the\n   validation set error in the specific examples chosen, just as well\n   or better than the proposed measure, without of course carrying any\n   interesting information on the generalization gap.\n3. Besides the issues above, the main quantitative piece of evidence\n   proposed in the paper is the table with the correlation\n   coefficients. To be honest, after reading the paper I am not sure\n   what is being correlated with what here. On page 7 I read that \"*For\n   each experiment (e.g., layer size in MNIST), we plot both the\n   evolution of the PH diagram distance and the validation score\n   (accuracy). The plotted values are the corresponding means of the 5\n   repetitions with different seeds. In addition, we compute the\n   Pearson correlation for these values.*\", and I deduce that the\n   correlation must be between the validation score and the normalized\n   cumulative topological distance (actually the plots contain also\n   the non-cumulative distance, but I guess the correlation is\n   computed with the cumulative one because the non-cumulative doesn't\n   seem to be correlated with the validation score). However, later on\n   in the same page I read \"*...there is strong\n   correlation. Intuitively, this is also observed in the plots,\n   although once the distances are normalized it is not as clear to\n   visualize.*\". Does this mean that the correlation was computed on\n   the unnormalized values, that is, not on what is in the plots,\n   contradicting the statement above?\n4. The paper does not seem to connect well with the existing\n   theoretical literature on the generalization gap in deep\n   learning. For instance, it completely ignores the effort to extend\n   information-theoretic criteria to be applicable to deep\n   networks. For instance, https://arxiv.org/abs/1906.07774v1 discuss\n   how a technique introduced by Takeuchi in the '70s (itself a\n   generalization of the Akaike Information Criterion) can be used to\n   predict the generalization gap. The introduction of that paper also\n   gives several references to multiple strands of the literature that\n   seem relevant for anybody interested in proposing a new technique\n   to estimate generalization (flatness etc). Other works that are\n   relevant from a theoretical standpoint are recent approaches\n   exploring an implicit simplicity bias in DNNs (see for instance\n   https://arxiv.org/abs/1805.08522, https://arxiv.org/abs/1812.10156,\n   and more recent literature citing those).\n\nMinor points:\n1. There are several passages in the paper that I completely failed to\n   parse. For instance, \"*Corneanu et al. (2020) try to estimate the\n   performance gap between training and testing using PH\n   measures. They claim. However, one can observe some caveats.*\", \"*We\n   study the relation between the evolution of the PH diagram\n   distances with the one of the validation score with the cumulative\n   values of the distance between homologous persistence diagrams\n   because this value seems much more stable. The information of the\n   distance between the persistence diagrams has been normalized\n   [...]*\".\n2. I don't understand why the y axes in most plots are labelled\n   \"distance difference\" but the word \"difference\" never appears in\n   the text of the paper - only distances are discussed.\n3. The architecture and the pre-training of the convolutional network\n   is entirely unclear to me. It is defined as \"*In the case of CNNs,\n   the pre-trained model is defined as 3 convolutional blocks with\n   kernel size 3 (starting with 32 channels), interleaved with max\n   pooling (its linear layers are thrown away after the\n   pre-training)*\". What does \"starting with 32 channels\" mean? This\n   sounds like the authors imply that there is a standard way of\n   changing the number of channels from one layer of a conv net to the\n   next (this is not the case, as far as I know). How is max pooling\n   performed? How many, and how large, linear layers are trained and\n   then \"thrown away\" during pre-training? When does pre-training\n   stop?\n4. I do not agree with the following statement: \"*If the measured\n   distances are, indeed, related with the learning process of neural\n   networks, these variations should not have any noticeable effect.*\",\n   when discussing the various network and training settings that were\n   examined in the experiments. How is changing the learning rate\n   expected not to have a noticeable effect on a metric related to the\n   learning process? And more generally, why would one expect to\n   change things like the size of the network in such a drastic way\n   (going e.g. all the way down to 4 units per layer in size) and see\n   no effect on the learning process?\n5. When commenting on Comeanu et al 2020, the paper states that the\n   method proposed there is \"not usable in practice\". I haven't read\n   that paper, but I wonder to what degree this is just the opinion of\n   the authors of the present work. If it is, it should be justified,\n   and not passed on as a fact. Moreover, regardless of its merits (on\n   the apparent lack of which I have commented in the Major Concerns\n   section), the method proposed in the current work is enormously\n   expensive from a computational standpoint (taking a week to run on\n   a machine with 2 V100 and 1.5TB of RAM to analyze networks with a\n   few hundred neurons on problems such as MNIST), so it doesn't seem\n   like practical usability is a key area of focus of the authors.\n6. The precise definition of the metric, and some of the choices\n   behind it, are not clearly specified. On page 5, \"*For each weighted\n   directed graph associated with the state of a neural network, we\n   link a directed flag complex to it. The topological properties of\n   this directed flag complex are studied using homology groups H n\n   . We calculate the homology groups up to degree 3 (H 0 -H 3 ).*\"\n   What is a flag complex? It is mentioned elsewhere in the paper that\n   flag complexes are used in the other (unpublished, AFAICT) paper by\n   the same authors, but the method proposed here should be\n   understandable without having to go and read that other\n   paper. Also, what motivates the choice of using homology groups up\n   to degree 3?\n",
            "summary_of_the_review": "Despite investigating the application of a promising set of techniques\nto an interesting problem, this paper fails to sufficiently support\nits claims. The main conclusion rests upon the correlation of a\nproposed measure of change in deep nets during training with\nvalidation set accuracy, but no effort has been made to control for\npossible confounds that could explain such correlation\ntrivially. Moreover, multiple important passages in the paper are\nextremely hard (or indeed impossible as far as I'm concerned) to\nfollow, and noticeable gaps are present in the references to related\nliterature.\n",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper analyses the training of neural networks from a topological\nperspective, presenting a pipeline that can measure (pseudo) distances\nbetween the network's weights during training. Such information is then\nemployed to study the generalisation error of a neural network.\n\nIn contrast to existing methods for estimating this error, this paper\ndoes *not* require a specific hold-out data set, as topological features\nof the neural network are monitored during training. This frees up\nadditional data for fitting, which can be highly relevant in the sparse\ndata regime.",
            "main_review": "I enjoyed the ideas presented in this paper; the analysis of\ngeneralisation performance is a highly relevant and timely, and indeed\nconstitutes one of the largest obstacle toward employing machine\nlearning techniques in the wild. Understanding generalisation without\nrequiring additional data has the potential to improve machine learning\ntechniques to a substantial extent.\n\nThat being said, the current paper suffers from some issues, which\nprevent my endorsement at this point:\n\n1. Clarity: while background information on topological data analysis is\n   provided (which I appreciate; in particular in light of the fact that\n   TDA is still a rather novel occurrence at ML conferences), the method\n   itself could be described in more detail. In particular, some aspects\n   should be discussed in Section 4:\n\n    - The use of additional representations for topological features\n      needs to be clarified. At present, the main paper does not contain\n      any description of the challenges in calculating distances or the\n      need for additional representations. At the very least, a brief\n      description of these topological representations is required, so\n      that readers may better understand the remainder of the paper.\n      I find it particularly problematic that no stability guarantees of\n      these representations are discussed; they are not *just* drop-in\n      replacements for the bottleneck or Wasserstein distances.\n\n    - Overall, this section should be rewritten with a clear 'roadmap'\n      in mind. What is the problem you want to tackle and how do you\n      intend to tackle it in this paper?\n\n2. Delineation to existing work: The ICLR paper by Rieck et al. (2019b)\n   appears to already contain a large amount of the material proposed in\n   this paper. A cursory reading shows that Rieck et al. even discuss\n   generalisation performance in an early stopping setting (more about\n   this later). It is therefore critical that the contributions of this\n   paper are more clearly delineated from existing work. From my\n   understanding of both papers, I would say that the current submission\n   improves on the following aspects:\n\n    - Choice of filtration for such neural networks\n\n    - The use of other topological representations (whereas Rieck et al.\n      only use a summary statistic of persistence diagrams).\n\n   These differences (and potentially all others that I missed) should be\n   briefly discussed; the advantage would be that the paper can refer to\n   the previous publication as a justification of the method itself!\n\n   The previous work by Rieck et al. also proposes a set of experiments\n   that would be very fitting to perform here: the experiments proposed\n   in this paper could be substantially strengthened by the addition of certain\n   (simple) monitoring baselines that serve to showcase the added value\n   of employing topological information in the first place here. Rieck et\n   al. compared their topological measure to a 'patience criterion' based\n   on validation loss, thus assessing the difference in performance\n   and epochs after the early stopping criterion was applied). In my\n   opinion, this is most important experiment here when assessing\n   validation performance. Also, it would be interesting to add simple\n   baselines measures to check that these are *insufficient* to be used\n   alone. For instance, a non-topological baseline that could be assessed\n   in this context would be a distribution of weights; it should not be\n   possible to assess validation performance with this baseline.\n\n2. Experimental depth: the experimental section is lacking depth, in\n   particular given the specific goals of this paper. The depicted plots\n   and correlations are useful, but don't serve to highlight the\n   benefits of the proposed method. I would therefore suggest a more\n   thorough experimental setup such as the one shown in the supplement\n   of Rieck et al. (2019b): as mentioned in the previous point, their\n   proposed measure of network complexity is shown as additional early\n   stopping criterion (without an hold-out validation data set as well).\n\n   A post-hoc experiment of this sort would be extremely useful in\n   demonstrating the utility of the method, and it would even enable the\n   quantification of certain measures. The correlation is also useful in\n   this context, but since the goal is to assess the generalisation\n   error, an experiment in which the new measure is directly applied\n   would be extremely worthwhile.\n\nPlease see below for detailed comments.\n\n## Detailed comments\n\n- The first contribution reads a little bit like a sentence fragment to\n  me. I would suggest to use the terms 'silhouette distance' and 'heat\n  kernel distance' instead of plain 'silhouette' and 'heat'.\n\n- The terminology 'homological convergence' is slightly misleading\n  because the paper studies persistent homology, as far as I understand.\n\n- The introduction of topological concepts in the background section\n  jumps between the 'geometrical' and the 'abstract'; I understand that\n  both perspectives are valid, but I would suggest to choose only\n  a single one.\n\n- The discussion of chains and boundaries requires more explanations (or\n  could be partially relegated to the supplementary materials). For\n  instance, please clarify the 'signed combination.' However,\n  definitions need to appear in the main text, not only in the\n  supplementary materials.\n\n- In figure 4, the $3$-simplex should consist of four $2$-simplices, but\n  only two are shown. Either clarify this in the captions or update the\n  figure accordingly.\n\n- The introduction of filtrations is not motivated; the 'nested family\n  of simplicial complexes' could be introduced by building more\n  intuition here.\n\n- The related work discussion should, as mentioned above, delineate this\n  paper from Rieck et al. (2019b).\n\n- The heading 'Algebraic Topology object' is rather confusing to me;\n  I would suggest to rewrite it as 'directed flag complex', for\n  instance.\n\n- As outlined above, the distance calculation lacks information about\n  stability and other guarantees. There is also a broken reference on p.\n  5, which needs to be rectified.\n\n- In the experimental section, the use of the MLP should be explained in\n  more detail. What does it do and how does it work exactly?\n\n- I do not understand the 'input order' experiment; does it refer to\n  ordering of samples (i.e. on the batch level)?\n\n- The use of additional topological descriptors makes the use of the\n  word 'distance' imprecise: the bottleneck and Wasserstein\n  distances are metrics in the mathematical sense; while there *is*\n  a well-defined metric between, for instance, persistence landscapes or\n  silhouettes, this is only (to a certain extent) an approximation to\n  the bottleneck/Wasserstein metrics. This distinction should be made\n  clearer in the paper.\n\n- Figure 6 should add standard deviations of the curves. An additional\n  clarification of the content of this figure would also be appreciated\n  by readers; I found the discussion to be slightly confusing because\n  I did not get the relevance of the cumulative distance. The statement\n  'the evolution of the homological convergence [...] seems to be very\n  similar to the one of the validation score' also needs some\n  clarification. Except for Figure 6b, where I observe some oscillation\n  behaviour and maybe the tendency to converge, I don't observe\n  convergence anywhere else.\n\n- The same applies to Figures 7–9. Adding some more details here will\n  result in a clearer description of the method. Maybe some of the\n  curves could also be relegated to the supplementary materials?\n\n- I would also suggest to investigate the use of energy distances or\n  energy correlations, as these measures are more flexible and capable\n  of assessing more than just linear dependencies between two\n  variables. See [*Energy statistics: A class of statistics based on distances*](https://www.sciencedirect.com/science/article/abs/pii/S0378375813000633)\n  for more details.\n\n## Terminology and style\n\nFor the most part, the paper is written well. I have some suggestions\nconcerning the style:\n\n- Please use `\\citep` and `\\citet` consistently when using `natbib`. At\n  present, citations are directly appearing in the text without any\n  enclosing parentheses. For instance, in the related work section,\n  the sentence 'for improving the training procedure of the models Hofer\n  et al. (2020); [...]' should use `\\citep` in order to obtain a proper\n  parenthetical citation.\n\n- can't --> cannot (likewise for other contractions; I admit that this\n  is a personal preference, so feel free to ignore it)\n\n- 'contained simplicial complex' --> 'nested simplicial complexes'\n\n- The paper employs non-standard terminology in certain places. The\n  diagram, for instance, is called 'persistence diagram', no\n  'persistence homology diagram'. Please ensure consistency with\n  existing papers here. The same applies to capitalisation; I personally\n  see no need to write 'Persistence Diagram', but if this spelling is\n  used, it should be used consistently.\n\n- 'complex clique' --> Use the term 'clique complex'\n\n- 'module $Z_i(K)$' --> 'modulo $Z_i(K)$' (the paper is discussing the\n  quotient operation)\n\n- 'non-cumulative homology': I think the word 'distances' is missing\n  here.\n\n- The use of 'Means mean' and 'Deviations mean' is slightly confusing;\n  I would suggest to show follow the format $\\mu \\pm \\sigma$, with $\\mu$\n  being the mean and $\\sigma$ being some measure of variance, such as\n  the standard deviation.\n\n- For the references list, I would suggest to carefully check which\n  version of a paper is being cited. Numerous papers mentioned in the\n  bibliography have already been published as book chapters, papers,\n  etc.\n\n- The preprint by Guss and Salakhutdinov is cited twice.\n\n",
            "summary_of_the_review": "While I am very excited about topology-based approaches that aim to\nunderstand the training or testing behaviour of neural networks,\nI cannot endorse this paper for publication yet.\n\nThe current write-up is suffering from several issues, which need to be\nrectified in a **major revision** before reaching the quality standards\nof ICLR. These issues include:\n\n1. Lack of clarity: an improved introduction to topological concepts is\n   required and the proposed method needs to be compared more with\n   existing topology-based approaches (in particular since it does not\n   meet the requirements of a metric in the mathematical sense, an\n   analysis of approximation guarantees is crucial).\n\n2. Lack of experimental depth and delineation to existing work: since\n   the express goal of this paper is to analyse generalisation\n   properties of neural networks based on their topological properties,\n   a comparison with existing work (Rieck et al., 2019b) and an improved\n   experimental suite (containing previous work *and* non-topological\n   baselines) is critical for corroborating the claims of this paper.\n\n**Updated after rebuttal**: A lot has been discussed during this rebuttal period. I would strongly\nrecommend to pick up some of the suggestions of reviewers in order to improve the paper. A recent paper\nby [Birdal et al.](https://papers.nips.cc/paper/2021/hash/35a12c43227f217207d4e06ffefe39d3-Abstract.html)\ndemonstrates how to successfully assess generalisation performance (disclaimer: I am *not* one of the authors\nof said paper). I hope that this may serve as a partial inspiration.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Unprofessional behaviors (e.g., unprofessional exchange between authors and reviewers)"
            ],
            "details_of_ethics_concerns": "Authors were bordering on passive--aggressive and behaved in an abrasive manner with all reviewers. Instead of updating/revising the paper, much was spent on nit-picking irrelevant details. Overall, I have the feeling that the hours I put into reviewing this paper and engaging with the authors were rather wasted. I wish the authors would take some of the comments to heart—my summary with a link to a recent NeurIPS paper demonstrates how a successful experimental setup could look like; I honestly don't understand why the authors appear to be so adamant and hostile in these exchanges.",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper experimentally investigates the (cor)relation between the validation accuracy achieved by a neural network and a variation of a topological descriptor built on top of the network: the persistence diagram of a directed flag complex built on top of the network. \n\nThe main claim is that there is a high correlation between the validation accuracy and this \"topological distance\", so that the later may be used when a validation set is not available. ",
            "main_review": "# Strengths and Weaknesses\n\n## Strengths: \n- The paper is well written overall (aside for few typos, see Minor comments).\n- Experimental report is extensive given the amount of complementary plots in the supplementary material. \n- The methodology proposed by the paper is original and interesting. The construction of topological descriptor for networks has interesting properties, theoretically improving for instance on the approach proposed by Rieck et al. (at the price of computational efficiency though). \n\n## Weaknesses :\n\n- My main concern with the paper is that its conclusions seem too optimistic. From my understanding, saying that the \"*proposed measure strongly correlate with validation accuracy*\" (Section 7) and that \"*the generalization error of a neural network could be intrinsically estimated without any holdout set*\" (abstract) would mean for it to be useful that : \n  - (a) If the network starts overfitting, that is the validation accuracy starts decreasing (while the training accuracy presumably keeps increasing), this should be reflected in the curve of (cumulative) distances between (vectorization of) PDs. On Figure 7, b, for instance, there is---as far as I understand---no way to infer from the line-curve (PD distances) that the network is overfitting. \n  - (b) To be able to predict the generalization gap, one would expect a change of behavior when the network properly learns (e.g. MNIST) vs when it does not (e.g. CIFAR-10-MLP) ; (here, I assume that the training accuracy is going high---it as not been reported from what I can tell so we cannot be sure what the generalization gap is. Hence I assume that low validation accuracy $\\Leftrightarrow$ high gap.). From Figure 8.a and 8.b for instance, how is one supposed to infer that one network is learning something useful while the other does not? \n\nI am worried that what is interpreted as a strong correlation may only be a consequence of the networks parameters optimization/convergence. During optimization, as the step size goes to zero using RMSprop, it is not unlikely that the network weights variations (hence successive diagram distances) are getting smaller, yielding this concave-shape for the plots of cumulative diagram distances and for the *training* accuracy---thus for the validation accuracy when the network learns properly (i.e. validation accuracy $\\simeq$ training accuracy). \n\nFurthermore, when looking at plots in the supplementary material (for instance Figure 53), it seems that the reasonably correct correlation coefficients obtained are in great part due to the correlation *before* network parameters convergence (bottom-left quadrant: low topological cumulative, low validation score), which are not very interesting. On the top-right quadrant (when the network is likely to be \"ready\"), things get messier. \n\nIn the same vein, the sentence \"*(...) validation accuracy depends on the specificity of the data sampled in the validation subset, while the homological convergence is independent of the validation data*\" (end of section 6) is somewhat contradictory with the claim of the paper that homological properties of the network \"*captures the generalization of neural networks without a validation set*\". \n\nAll in all, and unless I have missed something crucial, I feel that the empirical results (which are quite numerous, thanks!) provided by this work are, unfortunately, not convincing.\n\n- A second important concern is the intersection with the Anonymous concurrent work (available in the supplementary material). With few exceptions, everything is the same up to the end of section 5 : same construction of topological descriptors (flag complex, filtration by Eq (1)), same diagram vectorizations, same datasets, same changes of architecture (number of layers, layer widths)... So that, roughly, what differs in the two paper is the experimental measurement: this paper studies what happen at training time for a given network, while the concurrent one makes cross-networks comparisons. Though the two papers cover different objectives, it diminishes the intrinsic contributions of the current one, which are now limited to the experimental results which are, as said above, insufficiently convincing in my opinion. \n- As acknowledged by the authors, this method does not scale and is only applicable to reasonable MLP; even though requiring high-end hardware and a lot of time. \n- The paper is not supported by any theoretical result that would motivate the use of their approach.\n\n\n# Minor comments\n- (Typo) p2, give-->given\n- (Typo) End of Section 3 : \"They claim. However\". It seems a sentence is missing. \n- (Typo) Start of p5 : an smoothing --> a smoothing\n- (Suggestion) Eq 1 : use \\max and \\left( \\right). \n- (Suggestion) Numbering cases with \"1. blabla... 2. blabla\" hinders readability. Perhaps using something in the vein of \"*(i)* blabla, *(ii)* blabla\" would be better. \n- (Clarity) The transition between filtrations of simplicial complex and persistence diagrams may be a bit too short (I am referring to the single sentence \"The sequence of homology groups is calculated by varying the parameter $\\epsilon$ to obtain the persistence homology diagram.\")\n- (Typo) p5 : a reference is missing (\"as in ?\").\n- end of p7, \"the homology does not seem to converge\". I do not understand this claim. If the network (parameters) converge, so does the persistence diagrams. If my understanding is correct, RMSprop should always yield convergence (it uses a decay factor for the step size); so I guess more epochs are needed. \n- p9 : \"our approach computes the exact PDs distance\", I think the exact PDs are computed in first place (not the distances). \n- Results in Table 1 for Heat and Silhouette are the exact same. Is it an unfortunate copy-paste?\n- Supplementary material, Figures 42-53 : how can it be that the topological cumulative decreases (given that the validation score seems monotonic, I guess that the axes labels have been switched?)? Also, why does the validation score reaches 1, while it is supposed to be much lower on some model? (I guess a normalization has been applied). \n- The paper claims that \"*in CNNs, the correlation are (...) still usually above 0.8*\", but unless I misread the tables 5,6, 9, and 10, this seems to be exaggerated. CIFAR100CNN + number of layers $\\geq 4$ is one the only instance where this seems to hold. ",
            "summary_of_the_review": "Though I appreciate the ambitions of the work, I feel that the empirical results are, unfortunately, not convincing. The absence of theoretical results and the very strong similarities with the concurrent submitted paper diminishes it's impact as well. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}