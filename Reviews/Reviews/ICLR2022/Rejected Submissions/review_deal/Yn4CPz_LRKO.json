{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes a conditional generative adversarial network with an auxiliary discriminative classifier for conditional generative modeling. The auxiliary discriminative classifier can provide the discrepancy between the joint distribution of the real data and labels and that of the generated data and labels to the generator by discriminatively predicting the label of the real and generated data. Experiment results are provided to demonstrate the effectiveness of the proposed idea.  The current paper receives mixed ratings after rebuttal (5, 6, 5, 8). Except that one reviewer (the Reviewer uPwH) will champion the paper with a score of 8, the concerns of the other three reviewers remain. To be specific, even though Reviewer ebJs assigns a score of 6, he/she doesn’t champion the paper because additional experiments requested are not provided by the authors, including (i) training on more datasets or higher resolutions, (ii) visualizing feature norm and grad norm as done in ReACGAN, (iii) experiments on ADC-GAN without unconditional GAN loss. The Reviewer DPgR pointed out that the paper might have a novelty issue because it bears some similarities with other works but it lacks a discussion in the revision. Additionally, Reviewer mZT7 pointed out that the authors didn’t provide a revised paper during the rebuttal, thus leading to a difficulty to assess the quality of the final paper. As a result, AC thinks that the paper is not ready to publish at the current stage and recommends a rejection.  The AC urges the authors to revise their paper according to the comments provided by the reviewers, and resubmit their work in a future venue."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper is about improving conditional GANs. To be specifically, it also aims to resolve the bias issue of ACGAN by proposing a discriminative classifier. The discriminative classifier is a hybrid model of discriminator and classifier, where it has to not tell real or fake, but also the class.  Preliminary analysis of the proposed method are provided. Experiments are conducted on the standard benchmarks. ",
            "main_review": "Strength:\n\n* The proposed discriminative classifiers seems interesting to resolve the biased issue of ACGAN. \n\nWeakness:\n\n* The key results seems to be Thm 2, which is based on Prop. 2. However, the proof shows pm(x, y, l) = pm(x, y, 1) + pm(x, y, 0) = 1/2 p(x, y) + 1/2q(x, y). at the very beginning. How do you get the second equation?  I guess it may not be a fatal error, but I can't tell the correctness at the moment. \n\n* The main criticism of AC-GAN is being generator-agnostic, which I think it's not fully appropriate. A common practical implementation of AC-GAN is also using generated data to train classifier, which I think it's a straightforward idea. Under this, whether it can simply resolve the generator-agnostic issue? However, by just doing so, the performance seems not as competitive as the reported numbers of the proposed method.  Could you comment on it? \n\n* The reported numbers looks not quite consistent with other works to me.  \n\n  - For CIFAR10 results, the reported FID are all below 7, however, it's not the case for most of the existing works. For example, all the models reported here https://github.com/POSTECH-CVLab/PyTorch-StudioGAN#cifar10-3x32x32 are all with FID > 7. \n\n - For CIFAR100, the numbers reported in TAC-GAN seems better than the 11.37 reported in the paper for the TAC-GAN and 7.98 for the proposed method. \n\n- For ImageNet, again the reported BigGAN is worse than it should be.  For example, see Table 2 in https://arxiv.org/pdf/2111.01118v1.pdf which provides a nice comparison. \n\n* Some descriptions are not accurate. For example, there are multiple sentences about \"GANs are notoriously unstable to train\", which is true back to 2014. However, there are already \"tons\" of papers working on it to resolve the issues. The authors should at least cite those and make a fair description.  Secondly, the below Eq (6), the authors mention they proposed method can be \"unbiasedly optimize\", which is not true under alternative and minibatch setting. Again, there are lots of works (check all the works on GAN optimization) discussing this issue. The authors should remove this incorrect claim. \n\n* Proposition 1 seems trivial. Also, Theorem 1 is quite similar to the analysis in TAC-GAN. I would highly suggest removing Proposition 1,  which you don't have to pretend to be a theoretical paper. Also, cite TAC-GAN for the analysis. We should not copy or redo the analysis from the predecessor. \n\n* I have the concern of the analysis. Most of the results rely on assuming sth components are optimal. However, in reality, they are not hold in reality, and there is no convergence analysis provided.  Could the authors comment on it?  \n\n* The biased issue of ACGAN is known.  For example, \n\n  - AC-GAN Learns a Biased Distribution, 2017\n - Unbiased Auxiliary Classifier GANs with MINE, 2020\n\n There are many more. The authors should provide a better overview for the progress of this direction.\n\n\nQuestion:\n\n* In Table 1, PD-GAN is optimizing JS(P||Q) while the proposed ADC-GAN is optimizing KL(P||Q).  To me, there should not much difference. Any insights why the proposed ADC-GAN, which optimizes an asymmeric loss, should be better? \n\nSuggestion:\n\n* There are some very recent works in NeurIPS and also highly relayed, \n  - Rebooting ACGAN: Auxiliary Classifier GANs with Stable Training, NeurIPS 2021\n  - A Unified View of cGANs with and without Classifiers, NeurIPS 2021\n  Although they are posted online after ICLR deadline, I would strongly encourage the authors comment on the similarities and differences between the proposed work and these two, because they will be on public for a when the paper decision of ICLR is out anyway. Note that I won't judge the paper decision based on this, but I think it's great to have for the community. It's fine even though the ideas are overlapping. ",
            "summary_of_the_review": "The main concerns are first on the analysis, which I couldn't tell the correctness at the moment. Second issue is the empirical results, which seems not consistent with other works. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a new conditional GAN model that employs a discriminative classifier that predicts in the joint space of label and real/fake domain. The theoretical analysis shows the proposed ADC-GAN can minimize the reverse KL between joint $Q_{X,Y}$ and $P_{X,Y}$.",
            "main_review": "## Strengths\n1. The paper presents interesting analysis of AC-GAN, TAC-GAN, PD-GAN. Especially the Theorem 3 reveals potential drawbacks of TAC-GAN.\n2. The experimental results on synthetic and real datasets demonstrate the superiority of ADC-GAN on conditional generative modeling tasks.\n\n## Weaknesses\n1. I don't fully agree with some claims made by the authors:\n   1. Page 5, footnote 2, it might be true that term (a) is \"ignored\" or set to zero, but it is not sufficient to say this inductive bias is a mistake.\n2. Equation 8, this is not the original form of TAC-GAN. The original TAC-GAN is built upon AC-GAN, so term (c) in Equation 3 in the TAC-GAN paper is missing. I notice that Equation 8 is consistent with the actual implementation of TAC-GAN, but I guess it is good to state this clearly in the paper.\n3. Implementations of ADC-GAN and TAC-GAN are the same? As I checked the provided code in supplementary, I think the proposed ADC-GAN is very similar to TAC-GAN (as defined in Equation 8): In fact, if spectral norm (SN) and bias are not used in the linear classification layer, they are exactly equivalent. This is because the weight of $C_d$ is just $C$ and $C_{mi}$ stacked together. I would consider this as an implementation difference. Note that Theorem 2 and 3 are different, I doubt the superior performance of ADC-GAN might come from the difference in SN or a different choice of hyperparameters. In such case, it would be helpful if the author could provide code for MoG experiment, which is cleaner, simpler, and no SN applied (if the code is borrowed from TAC-GAN). Please correct me if I am wrong, and I'm happy to amend my score accordingly.\n4. It would be helpful if the author could provide results of ADC-GAN on ImageNet at 256 resolution.\n5. Table 2, why not use the reported numbers in TAC-GAN paper? I checked Table 1 in TAC-GAN paper, and their FID on CIFAR100 is 7.22 which is lower than the reported 7.98, any explanation?\n6. It is nice to see ADC-GAN worked even without GAN loss, I am curious how the model performs (w/o GAN loss) on challenging datasets such as ImageNet? It is also surprising that in Supplementary Table 4, Hinge loss does worse than no GAN loss: if Theorem 2 holds, the solution set of ADC-GAN is a subset of (unconditional) GAN (say with hinge loss), so adding GAN loss wouldn't affect ADC-GAN training. Can the author explain this?\n7. Comparing Theorem 2 with a plain cGAN which minimizes $JS(Q_{X,Y}||P_{X,Y})$, does the reverse KL tend to cause mode collapse? (in theory it seems that JS is better than reverse KL? a typical yet imprecise point to be made is that KL causes mode averaging and reverse KL causes mode collapse. [1])\n\n[1] Zhao, Miaoyun, et al. \"Bridging Maximum Likelihood and Adversarial Learning via α-Divergence.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 04. 2020.",
            "summary_of_the_review": "I think the paper presents an interesting analysis of TAC-GAN, AC-GAN, and other cGAN methods. My concern is that the proposed method has the same actual implementation as existing method (TAC-GAN). The paper also lacks results on high-resolution generation. I am willing to raise my score if my concerns are resolved.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes the Auxiliary Discriminative Classifier GAN (ADC-GAN) to eliminate a contractionary objective and conditional entropy in ACGAN generator training. Specifically, the authors mathematically demonstrate that training ACGAN without a discriminative label classifier causes minimizing an undesirable divergence (KL(q(x)||p(x))) which conflicts with the joint distribution matching (KL(q(x,y)||p(x,y))). Also, they insist that the lack of intra-class diversity of ACGAN results from the absence of generator guidance for training the discriminator. To resolve all these issues, they devise a new classifier, the auxiliary discriminative classifier and deploy the new classifier directly on the ACGAN framework. Experiments demonstrate that ADC-GAN can successfully learn the joint distribution whose conditional marginals have non-negligible support overlap using MoG dataset. In addition, they show the effectiveness of ADCGAN compared to ACGAN, projection discriminator, and TAC-GAN on four benchmark datasets (CIFAR10, CIFAR100, Tiny-ImageNet, and ImageNet) using IS, FID, iFID metrics. ",
            "main_review": "Strengths:\n\n(+) The paper exactly points out the primitive problem of ACGAN from the optimization perspective. Since ACGAN is widely adopted in the machine learning area, analyzing problems of ACGAN is necessary and valuable.\n\n(+) The proposed auxiliary discriminative classifier is reasonable, and easy to implement. Also, ADC-GAN does not require much computational burden.\n\n(+) Section 4.2 is very interesting and the explanation of why projection discriminator fails to approximate the joint distribution in Figure 2 is reasonable.\n\nWeaknesses:\n\n(-) It seems that Theorem 1 has already been covered in TAC-GAN paper (paragraphs below eq.4 of the TAC-GAN paper [R1]). Although mathematical formulations are different from each other, the arguments of Theorem 1 and the paragraphs seem to be very similar. I think it is essential to clarify differences between two arguments.\n\n(-) It seems that all experiments were conducted once. It would be better to conduct experiments several times since GANs have been known to have a large performance variance. \n\n(-) The contribution that ADC-GAN can generate diverse images compared to ACGAN and TAC-GAN is not fully demonstrated. Although FID has been a widely used metric to measure fidelity and diversity of generated images, I think It is not enough. I recommend the authors to utilize the improved precision and recall [R2], classification accuracy score [R3], or density and coverage [R4] to quantify the ability of generating diverse images of ADC-GAN.\n\n(-) In section 5.1, the authors conducted the distribution learning experiment using one-dimensional conditional gaussians whose supports are overlapped. I accept that ADC-GAN can learn the joint distribution which consists of the one-dimensional conditional gaussians better than PD-GAN, AC-GAN, and TAC-GAN. However, what about a joint distribution which consists of conditional gaussians with disjoint supports? Can ADC-GAN learn the joint distribution better than other cGANs?",
            "summary_of_the_review": "The authors propose a new type of ACGAN named ADC-GAN to address an improper optimization process of ACGAN. They apply adversarial training not only for the discriminator but also for the auxiliary classifier to eliminate a contradictory divergence and conditional entropy in ACGAN training. In the experimental results, they prove the effectiveness of ADC-GAN using synthetic datasets and various benchmark datasets. However, I think Theorem 1 has already been addressed in TAC-GAN paper and experimental results do not fully demonstrate the effectiveness of the proposed method. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "- This paper aims to solve the low intra-class diversity on generated images of AC-GAN, a classifier-based cGAN. \n- As far as I know, this is an important issue that limits classifier-based cGANs (the counterpart is the projection-based cGAN, i.e, PD-GAN).\n- The authors point out that the reason is that the classifier of AC-GAN is generator-agnostic and minimization of conditional entropy decreases the intra-class diversity. \n- The authors propose ADC-GAN (auxiliary discriminative classifier) to solve this problem, and theoretical analysis is also presented.\n",
            "main_review": "Strengths:\n\n- The problem of low intra-class diversity of classifier-based cGANs is important. \n    - Reasons: The projection-based cGAN, PD-GAN, does not suffer from the low intra-class diversity problem, but it converges more slowly than classifier-based cGANs (see Omni-GAN, arXiv:2011.13074).  The classifier-based cGANs converge faster but suffer from low intra-class diversity. Therefore, it is of great value to improve classifier-based cGANs so that we can completely abandon the PD-GAN converging slowly in practice. \n- This paper provides a theoretical perspective for analyzing the loss function of different cGANs. \n- The proposed ADC-GAN is very simple to implement without additional computational overhead.\n\nWeaknesses:\n\n- Please detail in the paper how the FID in Table 2 is calculated (for example, how many generated images are used, whether the training set or the validation set is used, and whether the inception model is from PyTorch or tensorflow). In addition, I also recommend including IS in Table 2.\n- In Equ. 5, the notations of $C_d(y,1|x)$ and $C_d(y,0|x)$ are a bit confusing. After checking the code in the supplementary material, I understood the meaning of the equation. In fact, $C_d(y,1|x)$ and $C_d(y,0|x)$ are implemented using a fully connected layer with output dimensions of num_classes * 2. I suggest that the author use much clearer notation to make it easier to understand. The author can refer to the notation of equation 8, which is clear. \n- In Figure 4e, I am surprised that as a classifier-based cGAN, ADC-GAN did not suffer from mode collapse. The author also does not seem to apply weight decay for the discriminator, as Omni-GAN does. I am not sure if the author has used other regularization techniques to stabilize the training. As far as I know, if do not add regularizations such as weight decay, other classifier-based cGANs will collapse earlier, such as AC-GAN, Multi-hinge GAN, and Omni-GAN. It would be better if the author could explain this phenomenon. \n- I know that there is an improved version of AC-GAN (ImAC-GAN), as discussed in section 3.3 of the Omni-GAN paper. ImAC-GAN has a clear performance gain compared to AC-GAN. It would be better if the author could discuss the relationship between ImAC-GAN and ADC-GAN. \n- In Figure 4 (c) and (f), the results of T-SNE visualization are not very convincing. In my opinion, the author should not use the discriminator to extract feature representations, because the PD-GAN discriminator is less supervised than ADC-GAN. I suggest that the author use a pre-trained classification model to extract features for a fair comparison.\n- In the caption of Figure 4, the author says that the T-SNE uses training data, but in the last paragraph of section 5.3, the author says that the T-SNE uses the validation data. \n",
            "summary_of_the_review": "The author proposes a simple but seemingly promising classifier-based cGAN. I hope the author can answer my questions above in detail. I will improve the score based on the author's answer. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}