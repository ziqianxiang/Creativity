{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes to learn embeddings into complex hyperbolic space. This is an extension of the popular hyperbolic-space embeddings which have shown success on graph-like and tree-like data. Reviews and discussion mostly centered around the lack of clear motivation for the work (why complex hyperbolic spaces?) and the lack of a clear advantage over other manifold embedding methods that have varying curvature. The reviewers mentioned many questions and points that they thought the work should cover. There was also concern about the baselines against which the method was compared. There was not a consensus that the paper should be accepted, and no reviewer argued strongly for acceptance, even after the author response. As a result, I recommend that this paper not be accepted at this time. I expect a new version of this paper, incorporating this reviewer feedback and especially improving the explanation of the motivation, will be a good submission for a future conference."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors propose a new approach for representation learning on hierarchical data. They propose to embed hierarchical data not into the commonly used models of hyperbolic space (the Poincare and Lorentz models), but into complex hyperbolic space instead. ",
            "main_review": "- Hyperbolic spaces have gained a lot of interest for representing hierarchical data due to their relative simplicity and computational tractability – both in terms of computing the embeddings and for designing algorithms for downstream applications in such spaces (see, e.g., (Ganea et al., NeurIPS ‘18), (Cho et al., AISTATS ’19), (Weber et al., NeurIPS ‘20)). What implications has the switch to non-constant negative curvature? In comparison with embeddings spaces that are products of constant-curvature manifolds or that don’t have a constraint on the sign of curvature, how simple and tractable are the respective tools? Does this merit the additional representation power that we gain compared to the commonly used models of hyperbolic space?\n- How scalable is your approach, especially in comparison to the established hyperbolic embedding approaches you compare against? Some of the real-world data sets that you analyze are larger in size, but I did not see a comment on the computational cost of your approach.\n- In your experiments you only compare against two hyperbolic embedding approaches (Nickel and Kiela, NeurIPS ‘17 + ICML ‘18), but not against more recent methods. In particular, it is known that when learning representations in the Poincare model, data points get often mapped to points close to the boundary, which impacts the downstream performance. This can be mitigated by regularizations. It would also be good to include a comparison with embeddings into products of constant-curvature manifolds (e.g., Gu et al., ICRL ‘19).\n",
            "summary_of_the_review": "The paper proposes an interesting approach for embedding hierarchical data into complex hyperbolic space. By choosing complex hyperbolic space as the target embedding space, the approach tries to gain additional representation power. I have some conceptual concerns and also think that a more comprehensive experimental comparison with existing approaches would improve the paper (see comments above).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper introduces an extension of real hyperbolic embeddings to the complex hyperbolic space [A]. The exploited geometry is an extension of the Poincaré ball that contains complex vectors (instead of real vectors) whose norm is smaller than 1. The resulting manifold is of nonconstant negative curvature, which the authors expect to be favorable for embedding various hierarchical structures. \nFollowing the optimization framework of [B] and since the complex hyperbolic space is a Riemannian manifold, Section 4.3 presents a standard Riemannian optimization framework to learn nonparametric embeddings. The proposed manifold shows (slightly) superior results compared to real hyperbolic embeddings proposed in [B,C] in the graph reconstruction and link prediction tasks.",
            "main_review": "The paper is well written in general, and the optimization framework is clear although not novel since it follows standard Riemannian optimization.\n\n- One major weakness of the paper is that the motivation of using the complex hyperbolic space instead of more recent approaches such as mixed curvature representations [E] or ultrahyperbolic embeddings [F] is not clear. In Section 2, the authors argue that it is impractical to search for the best manifold combination of [E] for each graph. Therefore, ref [E] is ignored from the baselines in the experimental section and the only relevant baselines are standard hyperbolic approaches [B,C] that are known to be outperformed by most of their variants. The improvement over baselines [B,C] does not seem significant so it is difficult to justify using the complex hyperbolic space. At least, ref [E] should have been used for comparison.\n\nMoreover, even after reading the appendix and the experimental section, I do not understand the type of relationship that the distance function in Eq. (9) is supposed to describe. In [B,C,E,F], the motivation of the geodesic distances is given (e.g., high-level nodes tend to be closer to the origin in [B,C]). In the submission, there is no illustration or toy dataset/analysis to understand the embeddings that are learned. \n\nMost of the details (e.g., description of the dataset, optimization framework) could have been moved to the appendix to give a better intuition of the learned representations to the reader.\n\n- Another weakness of the paper is the fact that the learned representations are nonparametric embeddings. For instance, hyperbolic embeddings have been extended to hyperbolic (graph) neural networks, just like ref [E] was extended to graph neural networks [G]. How easy is it to extend the exploited manifold to (graph) neural network and perform inference of test examples?\n\n[A] Goldman, Complex Hyperbolic Geometry, Oxford University Press, 1999\n\n[B] Nickel and Kiela, Poincaré embeddings for learning hierarchical representations, NIPS 2017\n\n[C] Nickel and Kiela, Learning continuous hierarchies in the Lorentz model of hyperbolic geometry, ICML 2018\n\n[D] Absil et al, Optimization algorithms on matrix manifolds. Princeton University Press, 2009\n\n[E] Gu et al., Learning Mixed-Curvature representations in product spaces, ICLR 2019\n\n[F] Law and Stam, Ultrahyperbolic representation learning, NeurIPS 2020\n\n[G] Bachmann et al., Constant curvature graph convolutional networks, ICML 2020",
            "summary_of_the_review": "The idea looks interesting but the paper lacks providing the intuition of the learned representations, and baselines are lacking.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In most real-world hierarchically structured data such as taxonomies and multitree networks have varying local structures and they\nare not trees, thus they do not ubiquitously match the constant curvature property of the hyperbolic space. The authors explore the complex hyperbolic space when EMBEDDING HIERARCHICAL STRUCTURES.",
            "main_review": "## Strengths\n The unit ball model-based embeddings have a more powerful representation capacity to capture a variety of hierarchical structures. Through experiments on synthetic and real-world data, the author's work shows promising empirical results.\n\n## Weakness\n**1. The motivation is not clear to me. How do complex hyperbolic spaces solve the varying local structures?**\n\nThe network structure in the real world is complicated, and the use of one space with a constant curvature is likely to cause distortion. However,  the curvature of hyperbolic space is global. How does it deal with different local structures in the same network? For example, there are both tree local structures and ring local structures in a network. When using a global curvature -1, it is still difficult to take into account the two local structures simultaneously.\n\n**2. What kind of graphs are suitable to be embedded in the complex hyperbolic space is not clear**\n\n(More questions could be raised in the following discussion.)\n\n",
            "summary_of_the_review": "The unit ball model-based embeddings have a more powerful representation capacity to capture a variety of hierarchical structures. Through experiments on synthetic and real-world data, the author's work shows promising empirical results. However, there are two key problems causing my concerns.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose to learn complex hyperbolic embedding, which outperforms the real hyperbolic embedding in graph reconstruction and link prediction tasks. The authors argue that the complex hyperbolic space can have variable negative curvature, which can better capture the varying local structures in real-world data.",
            "main_review": "## Pros:\n\n(+) The idea and intuition of introducing complex hyperbolic embedding are novel and clear.\n\n(+) The experiment shows the great performance of the proposed method.\n\n## Cons:\n\n(-) Although it is intuitively correct that variable curvature can better suit the real-world data, it is not well demonstrated in the paper besides the performance metric in the experiment. I hope there will be a figure to illustrate at least one such example.\n\n(-) In the experiment section, the authors only compare with the standard (real) hyperbolic embedding method. Why not compare with the product space with multiple hyperbolic signatures?\n\n## Detail comments\n\nOverall, I like the paper where their key idea is interesting and inspiring. The extensive experiments on different tasks and datasets (including both synthetic and real-world) are appreciated. However, I have some slight concerns about the paper.\n\nFirst, I get that intuitively we want variable curvature spaces to account for varying local structures in data. However, I wonder what is the exact effect of such “variable curvature”? For example, is it possible to show (either by figure or experiment) that certain graph structures induced embeddings concentrate more at the place with smaller/larger curvature? I think one such example can greatly improve the overall clarity, as we do see a direct effect of variable curvature in the complex hyperbolic space.\n\nSecond, the authors keep mentioning that their embedding is different from the product space of two n-dimensional real hyperbolic spaces. While I can get this through the analysis provided by the authors, it is then natural to ask whether it is better or not. I find that the authors do not conduct such experiments and thus I think it is unsatisfactory. For example, given dimension 8, can the 4-dim UnitBall embedding outperform embeddings from product spaces of 4H^2 and 2H^4? Note that according to the experiments in [1], these products of small dimension hyperbolic spaces give much better results compared to H^8 (single large dimension hyperbolic space). See also the similar findings in terms of classification downstream tasks in [2]. Also, I do not fully understand why the authors do not compare UnitBall with neural network based embedding methods such as mixed-curvature VAE [3]. Even if restricting the signature to be hyperbolic is good enough. Please either provide more experiments or elaborate on why such an experiment is not done.\n\nFinally, I find the related work section can be slightly improved. Although neural networks are overwhelming nowadays in hyperbolic learning. Classical methods such as perceptrons and SVM [4,5] should not be ignored, as they come with convergence guarantees.\n\n## References\n\n[1] LEARNING MIXED-CURVATURE REPRESENTATIONS IN PRODUCTS OF MODEL SPACES, Gu et al. ICLR 2019.\n\n[2] Linear Classifiers in Product Space Forms, Tabaghi et al. ArXiv 2021.\n\n[3] Mixed-curvature variational ´ autoencoders, Skopek et al. ICLR 2020.\n\n[4] Large-margin classification in hyperbolic space, Cho et al. AISTATS 2019.\n\n[5] Highly Scalable and Provably Accurate Classification in Poincare Balls, Chien et al. ICDM 2021.\n",
            "summary_of_the_review": "Despite the slight concerns mentioned above, I still think the idea and the methodology of the paper is good enough for ICLR. The proposed complex hyperbolic spaces can potentially inspire more advanced method in the field. Hence, I lean to recommend accept of this paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}