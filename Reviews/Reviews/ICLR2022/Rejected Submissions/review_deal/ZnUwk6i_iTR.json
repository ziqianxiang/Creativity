{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper introduces a theory of mind benchmark. \n\nThis paper certainly improved during the discussion period. However, the paper is still incomplete. The authors are working related work (paper was not updated in this regard). The experiments still need significant work. The original submission used only 3 runs (very, very low). Although the authors bumped up the # of runs, the learning curves in the appendix feature very large and overlapping error bars, and the main table of results presented in the paper contains no measures of certainty---those a reported in a separate table in the appendix making comparison tedious. The paper has a fairly informal approach to dealing with hyper-parameters that should be discussed and improved. The reviewers pointed out (in their reviews and dialog with the authors) several ways the experiments should be extended. \n\nThe contribution of the benchmark is evaluated primarily via experiments; much work needs to be done before acceptance."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a multi-agent environment to develop and test theory of mind capabilities for agents. The environment is one where all agents speak, listen to and move around. Agents share information and the goal for each agent is to get all the information and then release. The paper suggests some architecture that avchieve some improvement over agents without theory of mind capabilties. Ultimately though, the task is unsolved and posed as a research question\n",
            "main_review": "\nThe paper introduces the concept of theory of mind well and does a good job at motivating the papers direction.\n\nThe proposed environment is well motivate dwith respect to the target of theory of mind. Although, I do think it's important to make sure to qualify the term theory of mind a bit more - as the original concept in psychology does include additional knowledge such as beliefs, desires, intentions and emotions. Most of which are not captured in this paper at all. I'd also really suggest to stay away from misleading terms like \"awareness\". E.g. I have trouble with the definition or model non-ToM. Theory of mind is about other agents not oneself. It's also not true that the agent is not \"aware\". It's just that he doesn't have the knowledge in the first place.\n\nMulti-agent settings typically suffer strongly from varied outcomes. It would be important to have more random seeds and also report measures of dispersion of run outcomes. Otherwise, I am not sure these results actually hold true in rigorous statistical testing.\n\nThe discussion of potential tests for ToM in 7.3 is interesting. It's not clear though why you don't just apply some of these, in order, to get good statistical data about the performance of trained agents besides accumulated reward. \n\nWhy did you choose MADDPG and RMADDPG?",
            "summary_of_the_review": "The paper presents an interesting new challenge that could give rise to a number of new research ideas. The challenge is well motivated from the literature\n\nAuthors explore the new task with existing methods with some obvious expansions.\n\nThe proposed baselines should be explored more and specific tests outside of reward should be part of actualy analysis\n\nPRO\n* paper is written well\n* the challenge is well described  \n* paper presents some sensible baselines\n\nCONS\n* no technical contribution\n* potentially shaky results (not clear if these results are significant)\n* evaluation relies on reward only. other evaluations are discussed but not really explored in detail\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper focuses on establishing the framework of symmetric TOM. Paper identifies such decision-making settings as being characterized by four different properties, namely, symmetric action space, imperfect information, observation of others, and information-seeking behavior. The paper then introduces a sample task that instantiates the symmetric TOM setting and then formulates some solution approaches for the setting that builds on an earlier multi-agent actor-critic framework, with an unmodified version being the baseline. The proposed method, particularly the GreedyEncounter variant, seems to perform better than some of the other baselines, but not as well as the heuristic version.",
            "main_review": "The problem of imbuing systems with a theory of mind is an interesting one and one that is getting attention in multiple areas in AI. Many works argue for not only modeling the beliefs and knowledge of other agents but also argue that in many scenarios where the agent is interacting with humans, it would need to explicitly take into account what the human expects from the agents (the authors could check works done in explanation in deterministic planning setting for some related work [1]). While the general direction is interesting, I don’t think the paper is ready for publication as the current paper doesn’t make a convincing argument for the novelty or significance of the current results.\n\nUnfortunately, the current version of the paper doesn’t contain a related work section. This is a particularly glaring omission for a paper that is looking at as widely studied a problem as modeling of other agents for multi-agent decision-making settings. Apart from works that frame such modeling of other agents as building theories of mind, which generally happens when there is a human in the setting or if the work is inspired by the psychological phenomena, many (if not most) multi-agent planning itself allows some such form of modeling (even ignoring all the various game-theoretic formulations). Not to mention, there is a pretty mature and well-investigated sub-area of reasoning and planning called epistemic reasoning that has looked at the problem of modeling beliefs and knowledge of other agents for a long time (authors can check [2] for a useful starting point).\n\nA related work section helps the reader compare the contributions of a specific paper against all the previous works that have been done in related areas. For example, in the paper, there was no discussion on how the current problem being proposed compares to multi-agent planning frameworks like DEC-POMDPs [3] or I-POMDPS [4]. Particularly since one of the baselines used in the paper is specifically a solution method for DEC-POMDPs. To the best of my understanding, the specific scenario discussed in the paper seems to be a special case of a DEC-MDP if you allow the facts that the agent collects to be modeled as part of the state. If that is not the case, then the paper should explain why it is not so.\n\nThis brings me to the specific problem being studied in the paper. Why is this problem of particular interest? Does it correspond to any particular practical problems? The methods introduced here leverage specifics of the problem and as such, the usefulness of methods are also limited by the utility of the setting. Additionally, the paper doesn’t provide any complexity analysis of this particular problem type or an optimal solution. It was also not clear to me why forgetting information is particularly important for information-seeking behavior. If a given agent has partial observability, the task is the infinite horizon and its utility depends on the other agent’s state and actions, you could always create a setting where the optimal strategy for the agent involves seeking out the information about other agents' state.\n\nAlso, the beginning of the paper makes some connections to human-agent interaction, if this is one of the problems the authors are interested in, then they are generally incompatible with any centralized planning mechanisms. Since in most cases you can’t plan for the human. It might make more sense for the authors to base their framework on decentralized planning frameworks like I-POMDPs or some game-theoretic formulation.\n\n[1] Sreedharan, Sarath, Tathagata Chakraborti, and Subbarao Kambhampati. \"Foundations of explanations as model reconciliation.\" Artificial Intelligence 301 (2021): 103558.\n\n[2] Fagin, Ronald, et al. Reasoning about knowledge. MIT press, 2003.\n\n[3] Oliehoek, Frans A., and Christopher Amato. A concise introduction to decentralized POMDPs. Springer, 2016.\n\n[4] Gmytrasiewicz, Piotr J., and Prashant Doshi. \"A framework for sequential planning in multi-agent settings.\" Journal of Artificial Intelligence Research 24 (2005): 49-79.",
            "summary_of_the_review": "As of right now I won't argue for accepting this paper, as the paper doesn’t make a convincing argument for the novelty or significance of the current results.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a multi-agent environment and task, termed SymmToM, for analyzing machine theory of mind emerged from multi-agent RL training. In SymmToM, agents can see and act in a 2D grid world as well as share information about the world state through communication. Crucially, all agents have the same physical characteristics, hence, \"symmetric.\" In this task, agents gain rewards by hearing or sharing novel knowledge of the 2D grid world, so that the optimal policy would be cooperatively seeking and sharing information among the agents. The proposed approach extends MADDPG to explicitly model the knowledge of each agent and how it would be affected by the shared information. The experimental results show that this extension improves over the generic RNN based extension to MADDPG (i.e., RMADDPG), but is still not performing as well as the simple heuristics-based baseline.",
            "main_review": "=====Strengths=====\n\n1. Testing RL agents or any kinds of machine agents' ToM ability is an important and yet understudied problem. When we evaluate the success of multi-agent policies, it is important to test the true social intelligence that comes with the policies. Naturally, that includes ToM reasoning. So this is certainly a welcomed contribution in the area of multi-agent RL in my opinion.\n\n2. The explicit modeling of agents' knowledge is an interesting extension to MADDPG. It makes sense to use domain knowledge in some cases to improve the RL training if there is a discussion on the limit or possible improvement.\n\n3. The two kinds of tests proposed in the discussion are very interesting and are to some extent the most important aspect of this study -- evaluating and analyzing the true ToM ability of agents trained in SymmToM that goes beyond reporting a single reward value.\n\n=====Weaknesses=====\n\n1. The literature is insufficient. There has been a rich history of computational modeling of ToM (e.g., [1,2,3,4]). There have also been multiagent communication and cooperation tasks / environments proposed before (e.g., the particle environment proposed in the MADDPG work, and [5,6]). There should be a more thorough discussion and comparison.\n\n2. It is also unclear to me why it is necessary to have a symmetric setting. This setting is emphasized in the title and in the main text, but I have not seen the motivation for it.\n\n3.  I do not see a systematic and quantitative evaluation of the two kinds of tests proposed in the discussion section. Am I missing something here? Is it more of a proposal than a completed evaluation?\n\n4. There should be a discussion on how general and scalable the explicit modeling of knowledge is as an extension to MADDPG or similar multi-agent RL approaches.\n\nReferences:\n\n[1]  T. D. Ullman, C. L. Baker, O. Macindoe, O. Evans, N. D. Goodman and J. B. Tenenbaum (2010), Help or hinder: Bayesian models of social goal inference. In NeurIPS.\n\n[2]  Netanyahu, A., Shu, T., Katz, B., Barbu, A., & Tenenbaum, J. B. (2021). PHASE: PHysically-grounded Abstract Social Events for Machine Social Perception. In AAAI.\n\n[3] Zhu, H., Neubig, G., & Bisk, Y. (2021). Few-shot language coordination by modeling theory of mind. In ICML.\n\n[4] Shu, T., Bhandwaldar, A., Gan, C., Smith, K. A., Liu, S., Gutfreund, D., ... & Ullman, T. D. (2021). AGENT: A Benchmark for Core Psychological Reasoning. In ICML.\n\n[5] Das, A., Gervet, T., Romoff, J., Batra, D., Parikh, D., Rabbat, M., & Pineau, J. (2019). Tarmac: Targeted multi-agent communication. In ICML.\n\n[6] Jain, U., Weihs, L., Kolve, E., Rastegari, M., Lazebnik, S., Farhadi, A., ... & Kembhavi, A. (2019). Two body problem: Collaborative visual task completion. In CVPR.\n",
            "summary_of_the_review": "I think this paper has a lot of potentials but is incomplete in its current form due to 1) a lack of interactive review, 2) insufficient evaluation (missing systematic evaluation of the proposed tests), and 3) a lack of discussion of how scalable and generalizable SymmToM and the approach are. Please see specific concerns in my main review.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper suggests a fully symmetric multi-agent environment based on the Theory of Mind, SymmToM. In the SymmTom, agents can remember its information, infer behaviors of other agents, and estimate the probability of others. Interactions between agents are modeled by a partially observable Markov decision process. Each agent decides an action(move, speak) upon its state which consists of partial observations. The authors admit that the proposed SymmTom cannot be completely solved. They adopt MADDPG for performance evaluations.",
            "main_review": "This work considers a broader range of tasks by removing a pre-defined role of the agent. From the symmetric setting, every agent can play all roles without being biased towards a specific role.\n\nEven though the paper has merit, the reviewer has the following concerns. \n1)  The paper adopts the partially observable Markov decision process. But in section 7.1, the observation space is defined by a set that consists of the position of **all** agents, **all** recharge bases, **every** agent’s first-hand information, etc. It does not seem to be a “partial” observable setting. In addition, the assumption that the agent has full vision seems not realistic in a large grid world. How can every agent know the initial knowledge of the other agents? \n\n2) Recharge bases seem to be important in the proposed environment. However, the process that happened in the recharge bases is not fully explained. Why all the information gathered are removed? It would be better the information pieces are removed depending on the importance, or time.\n\n3) The paper must include an exact definition of reward. The reward function is not provided in the paper. \n\n4) The experimental results in Table 1 were written with only 3 runs, which is a very insufficient number of trials to explain the claim. In addition, it is not a good method to judge the performance of a specific algorithm through the average of the results performed under different experimental conditions.\n\n5) The values in Table 1 do not have enough explanations. Does it represent a ‘per agent’ reward?\n\n6) The paper should be reorganized. For example, Fig. 2 does not have specific captions(a/b/c), so it is hard to follow. In addition, Fig. 3 has 4 images with different sizes and #1(line 3 in the caption) is not explained.\n\n7) No parameter tuning and hyperparameter setting methods for experiments",
            "summary_of_the_review": "The multi-agent reinforcement learning with the Theory of Mind is an interesting topic to be discussed. The proposed SymmTom aims at the researchers in the Theory of Mind and MARL field. But overall problem setups and experimental results are not described enough.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces SymmToM, an environment aimed at benchmarking agents' Theory of Mind (ToM) capabilities. In motivating the construction of this environment, it outlines criteria.\n\n- It first defines ToM agents in terms of knowledge/prediction of each agent's own hidden state,\n\n- It then defines \"Symmetric Theory of Mind\" environments as having certain properties: symmetric action space, imperfect information, observation of others, and information-seeking behavior (really, the task-relevance of it). This is motivated by attempts to move away from more traditional ToM situations, e.g. where one agent is a passive observer or agents have one of several different designated roles.\n\nWith this, it defines the environment, a simple grid environment with several modifiable parameters.\n\nIt then defines several reasonable baselines: an oracle (that knows info states of all agents but must learn what to do with this via MADDPG -- so, not necessarily an upper bound), an heuristic (consisting of a simple coordinated strategy between agents that is also not an upper bound, but represents decent coordinative behavior), and several hand-crafted agents that explicitly track knowledge estimates (these have a great deal of domain-specific architecture).\n\nIt then tests each of these baselines on several versions of the environment (varying the aforementioned parameters). In all but the simplest versions, the heuristic method dwarfs the performance of everything else (including the oracle), thereby demonstrating room to improve.",
            "main_review": "Strengths:\n\nThe topic covered in the paper is extremely timely: ToM, and MARL in general (specifically centralized training with decentralized execution (CTDE) MARL such as MADDPG), are topics receiving a great deal of interest, with many workshops and conference publications related to this work.\n\nThe paper makes a compelling case for SymmToM with both the motivating discussion and the results. The definitions of \"Theory-of-Mind Agents\" and \"Symmetric Theory-of-Mind environments,\" while perhaps having some debatable components (e.g. is it important that each agent _always_ get some information about every other agent, as seems to be implied?), are reasonable and in my mind cover many interesting cases. The definition of SymmToM follows cleanly from this. It is a very simple environment that seems to contain minimal aspects critical for it to count as a Symmetric Theory-of-Mind environment. Finally, the results make a compelling case that this is a useful benchmark. In particular, by tuning the number of agents and info pieces, we easily reach a regime in which Heuristic substantially dominates several reasonable baselines. \n\nThese baselines are extremely hand-crafted for the environment, which would detract if the point of the paper were to demonstrate their utility. But that is not the point: the point is that (1) there is a substantial delta between these and Heuristic, and (2) that they are so hand-crafted that I cannot think of a way in which more generic baselines with more sophisticated methods could reasonably be expected to do better. This means that there is demonstrably considerable room to improve.\n\nThe paper is well-written and easy to read, with useful figures.\n\nWeaknesses:\n\nIt is important to think critically about whether SymmToM provides a robust test for ToM. Intuitively, several things are challenging about this environment: the need to estimate the belief/knowledge states of others, coordination (generating mutually beneficial behaviors between agents -- as Heuristic does in a very nontrivial way), and long-range planning (collecting all these info pieces before going to the recharge bases.\n\nWith regard to the need to estimate the belief/knowledge states of others, one critique is that this is a drastically simplified parametrization of belief/knowledge. That is, belief/knowledge is not about embodied/spacial/actionable knowledge about the environment, but rather it is abstracted away into discrete info pieces and thus lacks real-world nuance (e.g. one might try to estimate the location of something and know roughly where it is, and roughly where others think it is, but not have an exact sense of either; or one might be attempting to estimate others' goals). It is thus a bit unclear just how much this is capturing in our intuitive, psychological notion of theory of mind.\n\nCoordination is a massive challenge in CTDE MARL. In a sense, the Heuristic baseline largely solves a coordination problem: with this sort of simple behavior, ToM estimation and long-term planning is seems relatively easy. What might be difficult is not necessarily ToM estimation but rather simply achieving this level of coordination between agents. Failing massive amounts of coordination, planning in this environment seems to be very challenging as well, easily being difficult for long-range planning algorithms simply because of the number of subgoals one needs to achieve in order to get the recharge reward.\n\nA few things (not necessarily in the scope of this paper) could help clarify this.\n\n(1) Perhaps easiest, a direct analysis of the baselines' capacity to estimate K, as well as a ceiling for K estimation, would be useful. If it turns out that the models are already estimating K well, then this is perhaps more a benchmark for other challenges, not ToM.\n(2) It would be helpful to see what humans do on this task. One would be to see how often humans, playing together for a while, converge to simple, effective cooperative behaviors such as Heuristic.\n(3) The other would be to see how well a human does when the other agents are not particularly coordinated. One possibility is that the amount of things needed to be kept track of in the higher a, c environments simply make for not particularly human-doable tasks.",
            "summary_of_the_review": "In sum, the paper puts forth a compelling benchmark: one that is simple and well-motivated but appears to demonstrate a considerable delta between an heuristic \"upper bound\" (not necessarily actually the upper bound) and reasonable baselines. I feel that these merits outweigh the critique that this might not be isolating critical aspects of ToM capacity in people. As such, I think it would be worthwhile to get this to the broader community.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}