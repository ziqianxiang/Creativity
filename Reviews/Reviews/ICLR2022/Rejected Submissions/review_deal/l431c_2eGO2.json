{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper proposes a new regularizer, based on entropy maximization of samples near the decision boundary, to improve the calibration of neural networks while maintaining their accuracy. \n\nThe method seems simple, sufficiently novel, and has promising results. However, based on the review process (described below), I feel the paper needs to significantly improve its evaluation and presentation before it can be accepted.  \n\nThe review process summary:\n\n* Two reviews were eventually weakly positive about the paper: without major concerns, but not enthusiastic.\n\n* One review (L8Yz) was not sufficiently informative. \n\n* One review (ESue) raised many points. I disagreed with most of these points, following the authors' discussion. However, a few points seemed valid, such as the not-so-impressive performance for OOD detection, which the authors did not address.\n\n* I therefore asked for an additional review (iva2). The review concluded the paper is interesting and potentially useful, but requires another round of revision before it can be accepted, mainly because of its clarity and missing comparisons. I agree with these conclusions."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors propose a method for regularizing DNN classifiers during training by encouraging the classifier to have high entropy on embeddings of mixed samples (from two different classes) as a proxy for the unknown portion of the data distribution (i.e., the portion of the in-distribution that is not represented by the training samples). The objective of the method is to produce better deterministic uncertainty estimates without compromising accuracy.\n\nThe suggested approach is similar to the previously proposed Mixup strategy, except regularization is explicitly applied to the mixed samples and labels while the optimization objective of clean samples is unchanged. Moreover, the distribution of $\\lambda$ is set to produce values close to ~0.5 (i.e., mixture farther away from the class clusters).",
            "main_review": "Strengths:\n1. Empirical results demonstrate meaningfuly improved accuracy in both clean and (non-adversarially) corrupted data compared to the baseline methods.\n2. The method can be applied without modifying the network architecture, and it seems to perform well without being sensitive to HP tuning.\n\nWeaknesses:\n1. Limited evaluation to support claims (\"Mix-MaxEnt consistently provides much improved classification accuracy, better calibrated probabilities for in-distribution data, and reliable uncertainty estimates when exposed to situations involving domain-shift and out-of-distribution samples\"):\n\n1.a. In the context of classifier calibration, the comparison to Mixup seems to be limited given the similarity between the methods and the calibration results in the paper. Specifically, ECE/AdaECE results of C100-C and C10-C for Wide-ResNet (Tables 1 and 2) show conflicting results (this appears to persist in ResNet50 experiments as well). Moreover, the authors report the average results over all corruption types and intensities, however, Figure-10 shows that Mixup ECE is more consistent when facing severe corruption (>3), this could suggest a tradeoff between the two variants. Finally, the ablation study should be improved to clearly emphasize the importance of the proposed modifications including a clear summary of the empirical results. I would suggest to dedicate a section of the paper for a deep analysis comparing the two methods instead of scattering results and discussions in different sections.\n \n1.b. Evaluation is limited to ResNet architectures and small image classification datasets. As a general regularization method, the authors should provide a wider range of architectures and datasets to demonstrate the method is broadly applicable (at least in the context of supporting better accuracy/calibration claims).\n\n2. OOD results are not very impressive compared to the baselines (e.g., AUROC is comparable to vanilla DNN in Tables 1 & 2). The authors should also justify why they do not compare against other methods that have trained on proxy distributions to increase uncertainty on OOD [e.g., Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier\nexposure, 2018, Weitang Liu, Xiaoyun Wang, John D. Owens, and Yixuan Li. Energy-based out-of-distribution\ndetection, 2020 ]. The authors also do not provide a sufficient explanation to avoid comparison to Mukhoti, Jishnu, et al. \"Calibrating deep neural networks using focal loss 2020.   \n\n3. Some aspects of the method are overlooked. For instance, the importance/impact of regularization strength is not discussed in the paper (i.e., the balance between CE and ERL, there is an implicit balance when setting a specific ratio of mixed and clean data in a batch). Also, the use of beta distribution is not properly motivated when preferring $\\lambda$ values that are close to 0.5.\n\nGeneral remark:\nThe paper can improve segnificantly in terms of readability, flow and focus. For example, some paragraphs contain highly technical details or remarks that can be moved to the appendix. Moreover, some important results are deferred to the appendix and presented in an inconvinient way and with limited discussion (e.g., missing a summary for ablation study results, appendix G. tables are missing baseline results etc.). In contrast, some content that made it to the main paper is arguably beneficial to readers (e.g., SR,SRN given the results are comparable to vanilla DNN).",
            "summary_of_the_review": "The general idea, insight from empirical analysis and results appear promising. However, important aspects of this work are not properly addressed to support the main claims (limited evaluation, comparison to relevant work, general writing quality). The authors are encouredged to consider revising the paper accordingly and re-submit.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The author proposed a new method for improving uncertainty estimation in deep neural network output. The idea is similar to the mixup paper, but rather than augmenting the data by making a new sample interpolated from two samples having the same label; it creates a new sample that interpolates two samples with different labels. The hope is that these new samples serve as proxies to out-of-distribution data. A new loss function is then proposed by the author that augments the standard cross-entropy loss with an entropy regularization term that encourages the prediction of the new augmented samples to be as uncertain as possible (maximizing the entropy of the prediction). The author then performs experiments to demonstrate the effectiveness of the proposed approach.",
            "main_review": "Strengths:\n- The paper is clear and easy to understand.\n- The proposed technique is simple and can be easily integrated into other NN architecture. \n- The performance of the proposed model is competitive compared to the baselines.\n- The author conducts experiments on multiple use-cases with multiple datasets.\n\nWeakness:\nI have a few comments, concerns, and questions regarding the paper:\n1) The authors start the paper with an analysis of some of the observations they found on CIFAR and SVHN data. How much of these observations are applicable to other datasets?\n2) The authors made an observation that the OOD and data-shifted samples are mapped by NN to the hypersphere S. Since we are working on high dimensional feature space, a hypersphere is likely to cover more volumes than the area where the data lies. Therefore, this observation should not come as a surprise, shouldn't it?\n3) The technique focuses on developing proxies for OOD samples that lie between different clusters of data. However, most of the OOD samples should lie in the area that is not in between two clusters (at least based on the areas). Has the author considered this case? For example, create experiments on this type of OOD and see if the proposed approach could cope with it.\n4) The closest previous work to this paper is the DUQ paper. However, the author did not properly compare the model with the DUQ (with code base as the reason). All the DUQ numbers mentioned in the paper are based on directly taking the numbers from the DUQ paper. This may cause problems in assessing the fairness of the comparisons, since the setup in the DUQ paper may be a little different from the experiment in this paper.\n5) The comparison of several metrics for evaluating OOD in section 5.1.3 is a bit confusing. What \"metric\" is used to evaluate the performance of the metrics so the author can say that one is better than the others?\n6) As presented by the author, the methods create a high-entropy barrier between two class labels. What if in the classification problems, the transition from one label to another is more or less smooth. For example, in the case of digits 1 and 7 in handwriting recognition. Is applying the proposed approach counter-productive in these cases?\n7) Presentation. The tables are too small to clearly see. Some of the results in the main paper refer to a table in the appendix, which makes it a bit inconvenient to read.",
            "summary_of_the_review": "The paper proposes an interesting approach in uncertainty estimation for deep neural networks. The paper has some weaknesses that I hope the author can fix it soon. For now, I recommend a weak accept.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a simple approach called mix-maxent to improve the robustness of neural networks. The main approach is to regularize the network to make an uncertain prediction when the data is a interploation of two images with different classes. Experiment shows the good empirical results. ",
            "main_review": "The main advantage of this approach is simpleness. The proposed algorithm can be easily used to modify the training. My main concern is the significance of the empirical result.\n\n1. Seems in the compared baselines, mixup tends to be the strongest one, which is quite surprising as mixup is not an algorithm specially designed for robustness and uncertainty. I am not an expert in this area and I am concerned about whether the current SOTA approaches are compared.\n\n2. Cifar-100 is still a kind of a small dataset. For this empirical paper, I would recommend conducting larger-scale experiment on Imagenet with some more challenging natural distribution shifts.\n\nMinor:\n1. What if we mix images from 3 or even more classes? Why do we need to use beta distribution other than others to decide the weight?\n \n\n",
            "summary_of_the_review": "Overall, the main advantage is the simpleness but my concern on the significance of the result making me feel this paper is slightly under the acceptance bar.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes to add a regularization term to the loss used to train the neural network. The mentioned term imposes high entropy on synthetically generated examples to improve accuracy, uncertainty estimates, and OOD detection performance. In other words, it proposes an \"entropy maximization regularizer.\" The paper also has the strength of performing a significant amount of experiments that includes uncertainty estimation, domain-shift, and out-of-distribution detection.",
            "main_review": "Increasing entropy to improve OOD detection performance is not very novel. For example, the Agnostophobia [1] and Outlier Exposure [2] approaches start from a similar idea. The advantage of the proposed method is the fact that it avoids collecting additional data as required by the Agnostophobia and Outlier Exposure approaches.\n\nHowever, the Mixup [3] approach also uses data augmentation to improve robustness. Similar to the proposed approach, it also does not require collecting extra data. Despite both approaches being based on data augmentation, the authors present some differences between the proposed approach and Mixup. We partially agree with the authors in this case. This is not our main concern regarding the paper.\n\nThe IsoMax loss [4,5,6] and the Enhanced IsoMax (IsoMax+) loss [7] significantly also uses the strategy of improving the OOD detection performance by maximizing the entropy with the advantage of avoiding data augmentation and also massive hyperparameter tuning. For example, ten-fold cross-validation used to search five hyperparameters values requires training the same model 50 times. Rather than a regularization term, the IsoMax loss and the IsoMax+ loss are full losses used to seamlessly and entirely replace the SoftMax loss (combination of the cross-entropy loss, the SoftMax activation, and the last linear layer). Therefore, considering that the paper is proposing a loss regularization term to improve entropy in some cases, it must cite the IsoMax loss and IsoMax+ loss papers. The IsoMax losses work by simply replacing the SoftMax loss.\n\nAdditionally, it needs to change the following sentence because it is no longer valid after the advent of the IsoMax loss and the IsoMax+ loss: _\"While the literature suggests that classifiers whose predictive distributions increase their entropy the further away the test input gets from the training data are desirable, the implementation of models satisfying such a property is not scalable, and can only occur at the cost of performing crude approximations and by non-trivial modifications to the architecture of the neural network (Liu et al., 2020a; Kristiadi et al., 2020). Such modifications sometimes lead to degraded accuracy.\"_\n\nThe SoftMax loss is an extremely easy baseline for the OOD detection task. Hence, the proposed method should combine the proposed method with IsoMax and Enhanced IsoMax (IsoMaxPlus or IsoMax+). This could be done simply by adding the proposed regularization term that was added to the SoftMax loss. By using the code provided with the IsoMax papers, the suggested experiments could be performed by changing two lines of code. Indeed, combining the proposed term with the IsoMax loss variants (i.e., IsoMax and IsoMax+) would be appropriate to allow us to compare with a much-improved OOD detection baseline. \n\nAdditionally, we would evaluate whether IsoMax variants are better than SoftMax loss when combined with the proposed approach from an uncertainty estimation point of view. Adding these additional experiments using an improved baseline loss rather than the SoftMax loss appears essential. One final reason to include IsoMax and IsoMax+ in the experiments is the fact that they are distance-based losses, which the community is starting to agree is the best approach for OOD detection. Hence, it would be great to add one line called \"IsoMaxPlus\" and other \"IsoMaxPlus+Min-MaxEnt\" to Tables 1, 2, 5, and 6.\n\nFinally, considering that the Tables showing experiments with ResNet50 are as important as experiments using WideResNet28-10, to give a better notion of the real generalization power of the approach, we recommend moving Tables 5 and 6 to the main paper rather than being presented in the appendix. Additionally, section 5.1.4 should also consider the results obtained for the ResNet50 model. Considering the ResNet50 results, we believe the solution is attractive but lacks some generalization power (see ResNet50 results in the appendix). Furthermore, it is extremely costly, as we need to train the same model too many times to perform hyperparameter tuning. \n\nThe approach needs to improve the baseline performance of the IsoMax and IsoMax+ losses when combined with them to be valuable. Otherwise, we could simply use the IsoMax+ and improve the OOD detection performance training just once by avoiding the costly hyperparameter tuning.\n\nThe authors could be a bit more explicit about the fact the proposed approach requires increased training times because we need to add augmented data to the training process.\n\n_\"For all the methods, the ECE and AdaECE are computed after performing temperature scaling (Guo et al., 2017) with a **cross-validated temperature parameter**\"_ and _\"when temperature scaling is applied, **the temperature T is tuned on the validation set**, minimising the ECE (we considered values ranging from 0.1 to 10, with a step size of 0.01)\"_ seem contradictory. Considering _\"Such hyperparameters have been selected performing cross-validation with stratified-sampling on a 90/10 split of the training set to maximise\nAccuracy\"_, we believe the second sentence is true. Please, could you clarify?\n\n[1] Reducing Network Agnostophobia: https://arxiv.org/abs/1811.04110\n\n[2] Deep Anomaly Detection with Outlier Exposure: https://arxiv.org/abs/1812.04606\n\n[3] On Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks: https://arxiv.org/abs/1905.11001\n\n[4] IsoMax loss (first preprint, 2019): https://arxiv.org/abs/1908.05569v1\n\n[5] IsoMax loss: https://arxiv.org/abs/1908.05569\n\n[6] IsoMax loss (journal version): https://arxiv.org/abs/2006.04005\n\n[7] IsoMax+ loss: https://arxiv.org/abs/2105.14399\n\nDavid Macêdo\n\n\n###########################################################################################\n###########################################################################################\n###########################################################################################\n###########################################################################################\n\n\n===== FINAL RECOMMENDATION POST-REBUTTAL ========\n\nNOVELTY\n\nThe paper presents some level of novelty. It is similar to Mixup, but the differences are relevant. As we said before, this is not our main concern regarding the paper.\n\n**Hence, overall, the novelty is regular.**\n\nEXPERIMENTS\n\nThe paper presents many types of experiments such as uncertainty estimation, domain-shift, and out-of-distribution detection. However, the OOD data initially considered were only CIFAR and SVHN. We did not find results for the other methods for the new OOD data. \n\n**Hence, overall, the experimentation design is ok, but it may be improved regarding OOD detection.**\n\nPERFORMANCE\n\nConsidering WideResNet, the proposed approach generally outperforms other approaches regarding uncertainty estimation and domain-shift. **However, these results does not appear consistent if we consider the ResNet results presented in the appendix. Moreover, it usually performs similar to others regarding OOD detection and commonly does not even outperform regular neural networks in WideResNet.** \n\n**Hence, overall, the performance of the approach is not so great, and the results are not very consistent for different types of models.**\n\nCLARITY\n\n**ResNets are much more largely used and relevant than WideResNets. Unfortunately, the results of the proposed approach are worse on ResNets than WideResNets. We see no reason to present the ResNet results in the appendix.**\n\nWe complained about this in our review. However, the authors did not answer.\n\nSCALABILITY\n\n**The proposed approach has a severe limitation: it appears too expensive to be used in real-world projects.**\n\nTo tune hyperparameters, the authors did 10-fold cross-validation and searched for five values. This means they need to train the same network using the same data (with just a different hyperparameter) 50 times. Considering that they use the same amount of augmented data as the number of training examples, the method also doubles the GPU memory requirements or training time.\n\n**Therefore, overall, the proposed solution is about one hundred times more expensive than training a regular neural network.**\n\nHence, the proposed approach is viable for toy datasets like CIFAR. However, for real-world images such as ImageNet, it is essentially inviable. For example, let's say the cost to train a given big Transformer using a large amount of data is about 1 million dollars. If we plan to use the proposed approach, the total cost will be 100 million dollars. Same for CO2 emissions. \n\n**All this for not very clear or consistent performance improvements.**\n\nWe complained about this in our review. However, the authors did not answer.\n\nRECOMMENDATION\n\n**Considering the above, we recommend REJECTION.**\n\nFUTURE\n\nThe main idea is interesting, but we suggest that the authors remove the hyperparameter dependence and improve the OOD detection performance. We believe that combining the proposed approach with the IsoMax+ may help in this investigation.\n\n\n###########################################################################################\n###########################################################################################\n###########################################################################################\n###########################################################################################\n\n",
            "summary_of_the_review": "The proposed method is interesting, but the novelty is regular as it is similar to Mixup and the performance is not too impressive when we also consider ResNet50 results. Moreover, the paper completely ignores the IsoMax loss and the IsoMax+ loss that are significantly related solutions. Hence, considering that the IsoMax loss variants already significantly increase the OOD detection performance without requiring costly hyperparameter tuning, this proposed regularization term would be more valuable if the authors show it also increases the ECE, Domain-Shift and OOD detection performance of the IsoMax loss and IsoMax+ loss when combined with them. Finally, it is possible that combining IsoMaxPlus with Min-MaxEnt may produce state-of-the-art results and also make the proposed approach work better (i.e., producing more stable results) when dealing with the ResNet50 model. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper presents a new penalty function that can be augmented to the common cross-entropy loss, which was shown to improve the accuracy and calibration of deep net classifiers, applied to in-distribution data. This new loss also encourages the model to generate better estimates under domain-shift and better identify out-of-distribution samples.\n\nThe proposed regularizer is tightly connected to mixup. The latter suggests a data augmentation routine that creates new semi-synthetic samples by interpolating two images and their (possibly different) labels. In this paper, the idea is to interpolate between two images from different classes and then maximize the entropy on these samples during training. In a series of experiments, the authors show the advantage of this method over baseline approaches.\n",
            "main_review": "My impression is that the proposed penalty is a slight variation over mixup, which is the major concern I have about the novelty of this paper. The authors explained the differences on page 5, but I found this paragraph hard to phrase and somewhat ambiguous. Below, I entail the comments I have after reading the paper. \n\nAnalysis and observations: Figure 2 (right) displays the histograms of the distances of the embeddings that correspond to CIFAR10 (for which the model was fitted) and CIFAR100. As illustrated, there is an overlap between the two histograms, where the one that corresponds to CIFAR100 is slightly shifted to the left. Now, I do not understand the intuition that the model “will produce stronger activations if the similarity between the features learned on the IND data and the ones present in the input is high.” This should be better explained, including a demonstration. Also, a table that summarizes the statistics of interest (evaluated on the entire test data) would be informative.\n\nI believe that Figure 3 better illustrates this point. However, following Figure 7, mixup has an effect that is similar to the one presented in the right panel of Figure 3, although the “high entropy barrier” obtained by mixup is smoother compared to Min-MaxEnt.\n\nWhile the data sets used are commonly used in this literature, the distribution shift considered is due to various corruptions in the input image, and therefore reflects the robustness of the fitted model to such deviations. I would suggest studying other types of domain-shifts, such as the one proposed in [1].\n \n[1] Recht, B., Roelofs, R., Schmidt, L., Shankar, V.. (2019). Do ImageNet Classifiers Generalize to ImageNet? Proceedings of the 36th International Conference on Machine Learning.\n\nAlso, the OOD experiments are fairly easy, since CIFAR10 and SVHN are very different data sets. It will be great to explore other cases for which detecting OOD samples is more challenging. For example, using synthetic data can be helpful and informative [2]. In addition, it will be illuminating to compare the proposed method to existing techniques, such as [2].\n\n[2] Liu, J. Z., Lin, Z., Padhy, S., Tran, D., Bedrax-Weiss, T., & Lakshminarayanan, B. (2020). Simple and principled uncertainty estimation with deterministic deep learning via distance awareness. arXiv preprint arXiv:2006.10108.",
            "summary_of_the_review": "The paper offers an interesting approach to improve robustness to domain-shift and better detect OOD data. The major concern I have is about the novelty of the proposal and its similarity to mixup, especially because the latter was already studied in this context (calibration, OOD, and adversarial robustness). Here, I believe that the authors should do a better job in distinguishing between the two methods (and other existing extensions of mixup).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}