{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper computes channel attention by considering feature maps across different layers, and named it the previous knowledge channel attention module (PKCAM). The reviewers find the proposed idea too straightforward and naive. Lack of technical contribution is one of the major criticisms. There are also correctness concerns with the submission. The authors have not provided any rebuttal.\n\nWe recommend rejecting the paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This work proposes a previous knowledge channel attention module (PKCAM) that captures channel relations across the different layers to help enhance feature representation. The module can be integrated into the current ResNet series and show reasonable performance improvement over the baseline network on some benchmark datasets.",
            "main_review": "I think the descriptions about the core PKCAM module are unclear and seem to be incorrect. As shown on Page 4,  Y=W\\widetilde(X) and thus Y should be a feature vector with the dimension of RC_0. However, the authors claim that Y is a feature vector with dimension of C_0. Do I miss something? Besides, the authors state to split the function f() into two functions f_1() and f_2(). However, I can not find the definition of f_2(). \n\nThe authors split the PKCAN module into a summation followed by a linear mapping. It can indeed reduce the complexity. But why such simplicity works? More analysis should be provided.\n\nThe authors claim that the module can be easily integrated into different network architectures. However, they merely conduct experiments with the Reset series as baselines. I think the experiments with other architecture s such as mobile net should also be provided. Or the experiments can not support the statements.\n\nMore recent works [1, 2] also adopt channel and spatial attention for feature enhancement. These works should also be included for analysis and comparisons.\n\nI think the work is somewhat a draft for the current version. There are many reference errors throughout the paper, e.g., fig. ?? on Page 3 and fig. ?? on Page 7.\n[1] Hou et al., Coordinate Attention for Efficient Mobile Network Design Coordinate Attention, in CVPR, 2021.\n[2] Zhang et al., ResNeSt: Split-Attention Networks, in arXiv 2020.",
            "summary_of_the_review": "I think the introduction about the PKCAM module is very unclear. The experiments are not sufficient and convincing.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper computes channel attention by considering feature maps across different layers, namely previous knowledge channel attention module (PKCAM).  The developed PKCAM is achieved by two steps: (1) the previous knowledge aggravation module is used to aggregate channel information of multiple feature images; and (2) the global cross channel interaction module models the correlation among channels to compute attention weights. The experiments are conducted on image classification and object detection tasks. ",
            "main_review": "Strengths: \n1. The previous feature maps are used to compute channel attention for the current feature map, showing some differences with existing methods. Besides, the proposed PKCAM is only applied to the end of the last stages, leading small parameters and FLOPs.\n2.\tThe proposed method seems easy to implement. \n\nWeaknesses:\n1.\tThe technical contribution of this work seems limited. The core of PKCAM is to exploit feature maps from previous layers for computing channel attention of the current feature map. For me, it is still not very clear why previous feature maps can help to compute channel attention of the current feature map. In other word, what are the merits brought by the previous feature maps? Furthermore, the core components of PKCAM (i.e., SDA and CDA) show similar with existing works, e.g., SE and ECA blocks.\n2.\tThe experimental results are not very convincing. Specifically,\n(1)\tThe improvement of PKCAM over existing methods is very marginal in terms of accuracy and model complexity, which hardly verify the effectiveness of previous knowledge.\n(2)\tExperiments could be further strengthened. More stronger backbones (e.g., ResNet-101 for classification and Faster R-CNN/ Mask R-CNN for object detection) could be used to further evaluate performance of the proposed method.\n(3)\tSome experimental results need further discussions. For example, why FPS of ECANet increases two times from ResNet-34 to ResNet-50, while others only increase less 1.5 times? Why LCAM+PKCAM is inferior to PKCAM? How to combine LCAM with PKCAM? What are the details of LCAM?\n(4)\tWhy PKCAM is only applied to the end of each stage? Why PKCAM only inserted into the last stage achieves the best results?\n3.\tThe writing could be further polished. Specifically,\n(1)\tIn Section 3.2, the details on how to select the previous knowledge seem missing, i.e., How to select R feature maps? and How about effect of number of R?\n(2)\tWhich method is used for channel dimension alignment?\n(3)\tThe orders of channel dimension alignment and spatial dimension alignment described in Eqn.(2) (from 1 to R) are inconsistent with those in Figure 1 (from 0 to R).\n(4)\tThe dimension of fully-connected layer in Eqn.(1) seems confusing.\n(5)\tThe sum operations in Eqn.(2) and Eqn.(3) are confusing.\n(6)\tMany citations are missing, e.g., Figure ???\n",
            "summary_of_the_review": "In my opinion, the technical contribution of this work seems limited, while experiments and writing could be further polished. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper explores a new kind of attention mechanism, which can be easily integrated into any cnn models.\n\nThe proposed method is called previous knowledge channel attention module, whose aim is to make use of the knowledge from the previous layers.\n\nThe methods are validated on the image classification and object detection tasks. \n",
            "main_review": "This paper proposes an attention module with the knowledge from the previous layers also included. I have two concerns about this idea.\n\nFirstly, the idea to use the previous knowledge is not new. In the works of self-knowledge distillation, they use the previous layers knowledge to do the distillation (i.e. Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation). So, the idea of utilizing previous knowledge is not new and thus the contribution, for me, is not enough.\n\nSecondly, the authors use the previous knowledge to build a better attention module. This idea is too intuitive. I don’t think the attention module has long-term relationship. \n\n\nSome small typos in writing:\n- The last line of page 3, there is something wrong with the figure (actually, all the citations for figures are wrong)\n- The second row of sec 3.2, which rely on…\n",
            "summary_of_the_review": "-\tWhat is the main contribution or motivation of this paper. If it is only about using the previous knowledge, it is not creative enough.\n-\tTry to explain or proof the working mechanism of the proposed PKCAM, which should be made more solid.\n-\tTry to polish the paper.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces an attention module, namely PKCAM for Previous Knowledge Channel Attention Module, which can be integrated into CNNs. The proposed PKCAM mainly aggregates the feature maps of earlier CNN blocks for aggregating previous knowledge. The ablation study is conducted for validating the effect of PKCAM in CNNs as well as its different design. Image classification and object detection experiments are considered for demonstrating the effectiveness of PKCAM in different ResNet architectures.",
            "main_review": "This paper is not well prepared for review with many incomplete and inconsistent expressions, e.g., the Fig.?? in Pages 3 and 7, the EMCA in Figure 2 and Figure 3 but the PKCAM in the captions, the CDA and SDA in Figure 1 without any explanation in the caption and text, the first SDA then CDA operation in both Figure 1 and Algorithm 1 but inverse order in Section 3.2, etc.\n\nThe key contribution of this work is to consider aggregating the previous feature maps from earlier CNN blocks, but this idea has already been proposed for CNNs such as the DenseNet. Besides, the implementation of PKCAM is not special and actually the commonly used spatial and channel attention. Thus the paper has limited technical novelty.\n\nFor the ablation study, most of the analysis is actually related to the current techniques used in attention design (see Table 2). Besides, most of the comparison results show that the PKCAM is not superior to the compared counterparts a lot (refer to Tables 4, 5, 7, and 8 as well as Figure 2). Therefore, the advantage of PKCAM is not obvious, that is, this work is not convincing empirically.\n\nFrom Figure 3, it can be seen that the PKCAM has a big difference with ECA, however, the quantitative comparison shows very small difference between PKCAM and ECANet, while the explanation of the last paragraph in Section 5 is not reasonable, especially how to evaluate the discriminative and representative ability of an attention module. \n\nThe paper slightly exceeds the paper limit of ICLR, and the writing needs to be carefully revised for typos and grammar errors.",
            "summary_of_the_review": "The paper is not ready for review, also the contribution and analysis are limited. Thus, I vote for rejecting.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}