{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper presents the use of Simulated Annealing (SA) for pruning and optimizing the architecture of a neural network. After reviewing the paper and taking into consideration of the reviewing process, here are my comments:\n- The contribution of the paper and the novelty is limited and not well presented\n- The related work is very sparse. It requires a major improvement.\n- The main concern is about the simplistic experiments and the lack of comparison between the results of the proposal and the SOTA methods.\n- Conclusions are not well supported by the results.\nFrom the above, the paper does not fulfill the standards of the ICLR."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper develops a method for neural network pruning using simulated annealing. In the proposed method, the mask matrix indicating the pruning location is optimized by simulated annealing. The proposed method is evaluated on pruning tasks of fully connected neural networks in image classification.",
            "main_review": "The topic treated in this paper is interesting and important. However, the weaknesses of this paper and the reviewer's concerns are as follows:\n\n- Many pruning algorithms have been developed so far, including pruning methods based on black-box optimization like simulated annealing. The novelty of this paper is not apparent compared to the existing pruning methods. \n\n- The effectiveness of simulated annealing for network pruning is not apparent. For example, a comparison to other black-box optimization methods, such as genetic algorithms, should be conducted to claim the effectiveness of the simulated annealing. Also, the state-of-the-art pruning methods should be included in the baseline. \n\n- Random pruning should be included as the simpler baseline.\n\n- In the experiment, the size of the neural network is too small compared to the current trend. The convolutional neural networks, such as VGG and ResNet, should be considered as the baseline networks to be pruned. The following paper is useful for experimental design.\n\nDavis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, John Guttag, \"What is the state of neural network pruning?,\" arXiv:2003.03033 (2020).",
            "summary_of_the_review": "This paper proposes the network pruning method using simulated annealing. The basic framework of the proposed method is the same as the usual pruning methods. The experimental comparison with other pruning methods is not performed. Therefore, the reviewer can recognize neither the novelty and effectiveness of the proposed method.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a simulation annealing (SA) algorithm to prune neural network by randomly dropping and adding links according to SA's acceptance-rejection criteria. Experiment results shows the algorithm is useful in reducing number of weights while maintain model accuracy. ",
            "main_review": "The SA presented in the paper is easy to understand and well studied in traditional optimization community. It has proven convergence to optimality. Applying it for network pruning is certainly an interesting idea. The experiment results demonstrate the algorithm can prune up to 80% of the links without too much accuracy deterioration. The algorithm should be not difficult to implement in practice. I believe it helps small neural networks. \n\nThe paper has following weaknesses:\n1. The algorithm drops or adds a link a time, it can take very long to prune a large network. Why not drop/add a set of links so that the state neighborhood can be larger for large networks? \n2. Some content in Appendix should be included in main paper, such as Algorithm 1. Appendix is not guaranteed to be reviewed. \n3. Need more baseline comparison. How good is it when comparing with the deep compression (Han et a, 2016, Chen et al, 2020)?\n4. There are other network prune studies, for instance ASAP: Architecture Search, Anneal and Prune, by Asaf Noy, etc. What is the difference with them?\n\n\n\n\n\n",
            "summary_of_the_review": "The proposed SA method is interesting, however, I am not convinced it can scale well, and the experiments are using small networks. Need more baseline comparison to show its advantages.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper explores the application of simulated annealing to network pruning. In the proposed algorithm, a fully trained network has some percentage of weights erased and its connections are perturbed to new configurations, with acceptance happening according to a Boltzmann distribution. Two variations of the algorithm are proposed, one where the target percentage of connections is removed in one-shot, the other where the process is gradual.",
            "main_review": "Strengths \n* Interesting application of simulated annealing to solve the network compression problem.\n* Detailed overview and motivation for pruning algorithms and simulated annealing.\n\nWeaknesses\n* The limited scale and scope of the experiments puts the significance of the results into question. Experiments are conducted on MNIST and FashionMNIST and appear to be single seed, with no clear comparisons to baselines. This is a much smaller scale than similar research published e.g. lottery ticket hypothesis papers. Conducting more detailed comparison to baselines would greatly strengthen the paper.\n* The organization of the paper could be improved. There is too much space spent on discussing related work and details of the simulated annealing algorithm, which can be deferred to the appendix. This leaves less room for detailing the actual method and experiments, which come off less clearly as a result. It took me some time to understand that one-shot and gradual pruning \n* There are minor grammatical papers throughout the paper. ",
            "summary_of_the_review": "I recommend rejection of the paper. Simulated annealing is not a new technique, and the limited scale of the experiments and lack of comparisons to baselines limit the utility of this work in its current form. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "  This paper presents a novel simulated annealing method for pruning neural networks. Both one-shot pruning and gradual pruning strategies are discussed in this study. Experiments are conducted based on a simple neural network structure with only two hidden layers. The results show the effectiveness of the proposed method.",
            "main_review": "The idea of using simulated annealing to pruning neural networks seems interesting, but the paper suffers from the issues of 1) insufficient presentation, and 2) limited experiments. My concerns include:\n1.\tThe related work is very sparse. There are many recently published works considering neural architecture search (NAS) and network pruning (NP). But only a few of them is introduced in this study.\n2.\tThe main weakness of this study is the experiment design. The proposed method is only tested on a very simple neural network. Thus, it is difficult to judge whether this method is effective for widely-used convolution neural networks and others.\n3.\tThe performance comparison in this study is also very weak. There are many heuristic algorithms, e.g., evolutionary algorithms, that have been applied on NAS and NP. The proposed method should be compared with those SOTA ones.",
            "summary_of_the_review": " Although the idea seems interesting, this paper is not technically sound. Experimental results cannot fully support the main conclusion of this paper. Without extensive performance comparison, it is also hard to judge whether this new method can make a real contribution to the related community.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}