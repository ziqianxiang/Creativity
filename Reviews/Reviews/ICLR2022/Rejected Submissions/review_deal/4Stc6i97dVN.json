{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper gives high probability bounds on excess risk for differentially private learning algorithms, in the setting where the loss is assumed to be Lipschitz, smooth, and assumed to satisfy the Polyak-Łojasiewicz (PL) condition. The key idea in the paper is to leverage the curvature in the loss (PL condition) and the generalized Bernstein condition. \n\nAuthors show that they get sharper bounds of the order \\sqrt{p}/(n\\epsilon) when the loss is assumed to satisfy the PL condition besides being convex Lipschitz/smooth. Without using some curvature information about the loss function, the best upper bounds we can get are in the order of \\sqrt{p}/(n\\epsilon) + 1/\\sqrt{n} — and this is tight at least in terms of the dependence on n given the nearly matching lower bounds — in fact, the dependence on n is tight as it matches the non-private settings.  \n\nSo, I find it a bit misleading when authors say that they improve over the existing results. That statement is not true in its generality — it is true that we can leverage the PL condition to give faster rates but that is not the setting of prior work. Again, the bounds that authors compare against are for smooth/Lipschitz convex loss functions and without any assumption on the curvature of the loss. \n\nIf we do look at the literature for when and/or how can curvature help, we can compare against the existing bounds for strongly convex losses. The best-known result in the setting that is most closely related is that of Feldman et al. (STOC 2020): https://dl.acm.org/doi/pdf/10.1145/3357713.3384335. As we can check from Theorem 4.9 in that paper, the bounds we get are in the order of 1/n + d/n^2 which is actually better — not surprising since PL condition is a weaker condition. There is merit to the results in this paper but the current narrative is quite misleading and a more careful comparison with the existing literature is needed. The bounds are hard to parse — for example, what is the dependence on the strong convexity parameter (\\mu)? It would also help to instantiate specific loss functions so that we can fix some of the parameters in the bound to have a clear comparison with the existing bounds."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper gives a high probability excess population risk bound for differentially private optimization algorithm under $G$-Lipschitz, $L$-smooth and PL condition with gradient perturbation. This paper also gives a result under $\\alpha$ Holder smoothness, with an optimized bound and a new normalized gradient perturbation scheme.",
            "main_review": "Pros:\nThe authors discussed each assumption in detail, which gives the result under various assumptions.\nThe analysis hints that the generalized Bernstein inequality could provide a new tool for improving population risk bound and other differentially private algorithms.\nThe algorithm is empirically evaluated on a number of datasets and achieved noteworthy results.\n\nQuestions:\nIt seems that mNGP is proposed solely for the sake of bypassing Young’s inequality. However in the empirical evaluation it seems that the normalization works well on various datasets. Could the authors elaborate on this and provide some intuition on why normalization works in practice?\nFor the experiment results, could you report the variance and the number of random seeds used? Will it be open source?\nThe paper shows that normalization helps with improving population risk bound, though the convergence rate of the algorithm is yet to be discussed. Could the authors provide an insight on whether the normalization will affect the convergence rate? If so, to what extent should we expect normalization to affect the rate?\n\nTechnical questions:\nFor Theorem 3, in the step where you get R_n(\\hat{\\theta}_t) - R_n(\\theta^\\ast_n), the authors refer to Lemma 7, which gives us the bound on R_n(\\theta_t) - R_n(\\theta^\\ast_n). This along with R_n(\\theta_n) - R_n(\\theta^\\ast_n) does not give a bound for R_n(\\hat{\\theta}_n) - R_n(\\theta^\\ast_n). How did it go through?\nIn the proof of Lemma 3, why do we need to assume $i=n$? This condition is not used in (4) and the remaining proofs.\nBousquet lemma is referred both as lemma 4 and lemma 7 in the section.\nAppendix A.2 Lemma 3: Consider stating lemma 4 first and then state and prove lemma 3 for the sake of clarity. The same goes for Lemma 5 and 6. It is also unclear to me why ``Bounding’’ is in bold text.\n• For (17), is \\psi is defined prior to this equation?",
            "summary_of_the_review": "This manuscript provides a sharper utility analysis up to some clarity in the proofs. There's also some reservation on the significance of the techniques presented in the proofs.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper achieves high probability excess risk bound with rate O(1/n) w.r.t n for DP models via uniform stability by using Generalized Bernstein condition under G-Lipschitz, L-smooth, and PL condition. Then the authors expand the result to a more general case, only requiring α-Ho ̈lder smoothness, Polyak-Łojasiewicz condition, and generalized Bernstein condition. But the result is worse than before, so in order to get a better result, they propose m-NGP algorithm to achieve O(1/n) high probability bound w.r.t n under α-Ho ̈lder smoothness, Polyak-Łojasiewicz condition, and generalized Bernstein condition. The authors also show the experimental better accuracy results of m-NGP compared to traditional gradient perturbation method on real datasets.",
            "main_review": "I think the problem statement of the paper is interesting and important. The authors aim to solve the important open problem in DP community: to achieve high probability excess population risk bounds with tighter upper bound. The paper is well-written and has clear writing logical.  The authors make solid theoretical contribution and based on the logical in theoretical analysis, they propose the new algorithm to achieve better excess risk bound. It would be good to bring the experiments on real datasets to verify theoretical results. But there are some limitations in the paper. Firstly, the authors do not give existence of function under Assumption 4 and Assumption 5 simultaneously. Secondly, the authors only give the upper bound of excess risk bound and there is no lower bound, so it is unknown that their bound is optimal. Thirdly, in experiment part, the authors do not descript the loss function task and if the function in experiments satisfies these assumptions in theoretical part. Besides, they should give the performance about changing privacy parameter and dimension parameter.",
            "summary_of_the_review": "It is not sure that there is the function satisfying assumption 4 and assumption 5,so the problem setting may be not reasonable. And the authors do not give the loss function task in detail in experimental part. There is no lower bound so we do not know if this upper bound is optimal. And the authors should give more experiments about different parameter. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper analyzes the utility bounds of the gradient-perturbation based DP algorithm. They first provide DP by previous result (it is not the key point in this paper). Then, by applying the Generalized Bernstein condition, they give $O(p^{0.5}/(n\\epsilon))$ high probability excess population risk bound under the properties Lipschitzness, smoothness, and PL condition. Furthermore, under the more general assumption (Holder smoothness), they analyze the utility bound but the result is not so good as before. So they propose an algorithm (called m-NGP) to improve it under the assumption Holder smooth, and it is claimed that the utility bound can be improved to $O(p^{0.5}/(n\\epsilon))$. Their results are sharper than previous analyses in different settings: previous results require convex assumption but this paper requires PL condition. Moreover, experiments are performed to evaluate the accuracy of m-NGP.",
            "main_review": "Strengths:\n1. The problem is interesting and natural. This paper proposes the first $O(p^{0.5}/(n\\epsilon))$ high probability excess population risk bound for DP algorithms under the assumptions Holder smooth, and PL condition (or Lipschitz, smooth, and PL condition). Comparing with previous results ($O(p^{0.5}/(n^{0.5}\\epsilon))$), the improvement is significant, of the order $O(n^{0.5})$.\n\n2. For the provided utility bounds, convexity of the loss function is replaced with the PL condition, so the results can be applied to many non-convex settings.\n\n3. The paper also generalizes the $O(p^{0.5}/(n\\epsilon))$ result to the non-smooth settings, by assuming the loss function to be Holder smooth and introducing normalization to traditional gradient perturbation method.\n\n4. The results of this work are clear, novel concepts (techniques) are introduced to overcome the technical difficulties, and the technical contributions overall seem very solid. In this paper, the algorithmic stability is applied to bound the gap between the generalization error and its expectation, and via the Generalized Bernstein condition, the expectation one is solved. Another important part to achieve the superior theoretical results is to combining the generalization error together with term $R_n(\\theta^*)$. Besides, the optimization error is solved under different assumptions, for smooth loss function, the proof is similar to previous works. But for non-smooth loss function, Holder smoothness is introduced and Young's inequality is applied to get the bound, and an additional term with $\\eta$ exists. So the authors choose $\\eta$ for each iteration carefully in Theorem 2 to get the claimed result. This is also the motivation of their proposed algorithm: to eliminate the additional term. After eliminating it, the proof of the new algorithm is similar to previous works.\n\n5. Experiments on real datasets are performed to evalute their proposed algorithm m-NGP, the results show that the accuracy is better than traditional gradient perturbation DP method.\n\nWeaknesses:\n1. From my perspective, Holder smoothness implies $\\max\\{2HM_\\mathcal{C},H\\}$-Lipschitz if the parameter space is bounded by $M_\\mathcal{C}$ (In Section 3, the authors claim it is $\\max\\{HM_\\mathcal{C},H/2\\}$-Lipschitz). The claim should be explained or corrected.\n\n2. The paper overall read clearly, though the proof sketches should be more clear. For example, in Theorem 1, the authors say that 'the gradient perturbation based DP algorithm is $\\mathcal{O}(T\\eta/n)$ uniformly stable w.t.t n with high probability', but what are the connections between the uniformly stability and the given result? While the algorithmic stability is indeed closely related to the generalization error, the descriptions should be more detailed.\n\n3. There are minor mistakes when analyzing the gap between $\\theta_n$ and $\\hat{\\theta}_n$, although it does not affects the results signifacantly, the mistakes should be corrected.\n\n4. Some of the notations should be defined before used. For example, $R_n(\\hat{\\theta}_{t-1})$ in (1) is formally defined after it, and $\\eta_t$ in (1) is not explained throughout the paper. The authors should check them carefully.\n\n5. Although the understandability of the current manuscript is acceptable, I encourage the authors to correct the typos and the grammatical errors in future versions.",
            "summary_of_the_review": "The results given by this paper are clear, the theoretical analysis seems solid, the organization and the presentation are fine. But the proof sketches should be more clear, the notations, the typos, and the grammatical errors should be checked. Although there are some minor issues, I recommend this paper to be accepted because of the superior theoretical results and the novel technologies.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper extends the results in Klochkov & Zhivotovskiy 2021 to the DP case, proving the first $O(1/n)$ high probability excess population risk bound for differentially private algorithms, assuming Lipschitzness, smoothness, Generalized Bernstein condition and Polyak-Łojasiewicz condition. Then the authors relax the assumptions of Lipschitzness+smoothness to $\\alpha$-Holder smoothness, and propose a new normalized gradient pertubation algorithm that also achieves the $O(1/n)$ bound.",
            "main_review": "Strengths:\n\nThis paper is well-written and easy-to-follow. The presentation of results and proof sketches is clear. \n\nThe results of this paper are significant, improving an $O(1/\\sqrt{n})$ term over previous utility bounds, with the help of introducing the Generalized Bernstein condition and the Polyak-Łojasiewicz condition.\n\nWeakness:\n\nThe proof of Theorem 1 mainly follows Klochkov & Zhivotovskiy 2021, extending their result to the DP case using the same method. Theorem 2&3 have more novelty, but are less important than Theorem 1 in my opinion. The technical difficulties should be discussed in more details, to convince the readers that 1: the two new conditions are not too strong, 2: extending the results in Klochkov & Zhivotovskiy 2021 to the DP setting is non-trivial.",
            "summary_of_the_review": "The presentation of results is clear. Extending existing results to the DP case and getting the first O(1/n) utility bound is significant. The method is mostly based on Klochkov & Zhivotovskiy 2021, thus not very novel.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}