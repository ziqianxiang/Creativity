{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The reviewers agree that the proposed method of joint model-policy optimization using a lower bound is novel and interesting and worthwhile pursuing. But all reviewers find a variety of issues in the paper, such that ratings are just above borderline or below. Given all the mixed feedback, it appears that the paper is still a bit premature for publication and could greatly benefit from improvements in a future submission."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies the the problem of objective mismatch problem in model-based reinforcement learning, which states that the goal of the model is different from the goal of policy optimization. To solve this issue, this paper proposes a single objective to train both the policy and the dynamics model, by deriving a lower bound of the (log of) expected return and optimizing the lower bound jointly. Based on the objective, the paper proposes the Mismatched no More (MnM) algorithm, which use a GAN-style objective to train the model. Experiments show that the proposed algorithm can match the performance of baseline algorithm (MBPO) when exploration is not a big issue, while outperforms it significantly for sparse-reward tasks. ",
            "main_review": "Writing: The paper is written clearly, although it has a few typos. \n\n1. The link to the proof of Lemma 3.2 is wrong. \n2. Eq (8): What is $\\tilde Q_\\phi$? Is it $Q_\\psi$​?\n3. Appendix A.3, (d), the first cancelled term should be $\\log \\pi_\\theta(a_t | s_t)$​. \n4. Appendix A.1, \"Rather, it corresponds to maximizing a sum of the expected return **and** the *variance* of the return\"\n\nNovelty: The proposed method is similar to a prior work,  VMBPO. Both share similar components, with similar method of learning. However, there are a few difference which separates them apart. One of the most important differences is that MnM takes the logarithm of rewards, and derives a lower bound of the log of expected return, while VMBPO derives a variational lower bound of an upper bound of the log of the expected return. Given VMBPO as a prior work, The theoretical result is not so surprising. If the technique of logarithm of reward can be well justified, I think the empirical novelty is good. \n\nOther comments: \n\n> Following prior work (Janner et al., 2019), we learn an additional model to predict the true environment.\nWhat does it mean by \"additional model\"? \n\nIs the dynamics model learned well? From my personel research experience, learning the model using a GAN-like objective leads to an inaccurate model and sometimes produces unrealistic states. The signals from the discriminator are much weaker than the signals from supervised learning. What if we train $C$​ and $\\pi$​ with MnM, but train maximum likelihood model $q$​​? \n\nIn Figure 6: What does it mean by \"For fair comparison, we use Q values corresponding to just the task reward\"? MBPO uses $r$ while MnM uses $\\log r$, so they indeed differ a lot. \n\nAs the new reward function $\\tilde r$ takes the logarithm of old reward function $r$, it seems that (a) it's tricky to apply logarithm to non-positive numbers; (b) The new reward function $\\hat r$  is not invariant to the scale of the old reward function $r$, i.e., shifting $r$ can lead to a very different $\\tilde r$. Do all these $\\tilde r$​ work in practice? \n\nTheoretically, there is indeed a difference whether we transform the reward. But I'm not very convinced that this technique is essential in practice. The authors constructs a 3-state MDP in Figure 3, but some details are missing, e.g., how is $\\eta$​​​​ in VMBPO chosen? I agree that VMBPO optimizes an upper bound approximately, but it can somehow control overestimation by adjusting $\\eta$​, while I don't see how MnM controls underestimation. Moreover, what happens if we don't apply this technique in practice? I expected to see the ablation in a more complex environment. \n\n",
            "summary_of_the_review": "This paper is written well and the idea sounds very interesting and promising. However, as stated in the review, I'm not fully concerned by the log reward technique. Overall, I think there is more merit in this paper than the flaw so I recommend weak acceptance. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper studies the model learning aspect in model-based reinforcement learning (MBRL). Standard Dyna-like MBRL approaches that train the model using the maximum likelihood estimation (MLE) or its variations. In contrast, the authors propose an algorithm called Mismatched no More (MnM) that optimizes model parameters using a lower bound on the agent’s performance. By doing so, the paper mitigates the known objective mismatch of standard MBRL: MLE-optimized model might not necessarily be useful for optimizing agent’s returns, the overall objective of an RL system. Experimentally, the paper presents two studies. First, in 2 tabular environments, the authors compare MnM to Q-learning and VMBPO, an alternative MBRL algorithm that addresses the objective mismatch. Second, the authors compare MnM to an MLE-like MBRL baseline and a model-free algorithm on 12 continuous control tasks.\n",
            "main_review": "Novelty\n\nTo the best of the reviewer's knowledge, the idea is novel. The authors propose a lower bound on the logarithm of expected returns that the policy and the model can jointly optimize against an adversary that tries to distinguish between real and model rollouts. As a result, the MnM model mitigates the objective mismatch yet produces realistic samples. This contrasts the approach with other non-MLE methods (e.g. see [1]) that address the objective mismatch but produce unrealistic rollouts. The authors might want to emphasize the property more in the paper and study the effect in more detail.\n\nSignificance\n\nThe significance of the paper is limited. The main concern of the reviewer is about the empirical results: \n- First, most experiments appear to have a very limited number of runs. The reviewer did not find the details in the paper explicitly articulating the number of random seeds. The reviewer might infer from the plots that MnM and the baselines were run 2-3 times (e.g. Figure 5, `meta-world-door-open-v2`). Given the high variance of the results, it is hard to make conclusions about the comparison of MnM to the baselines. Reporting the results using 10 random seeds might improve the credibility of the results. See [2] for a discussion about the significance of the experimentation in RL.\n- Second, a few important baselines are missing. In tabular experiments, there is no comparison to an MLE MBRL agent. In continuous control, there is no comparison to any MBRL algorithm that addresses the objective mismatch. Why not use VMBPO in these experiments if you already used it in tabular experiments? Furthermore, Appendix A.4 suggests that the optimal dynamics for the lower bound are achieved by weighting the true dynamics with returns. Why not try return-weighted MLE as a baseline? Including other baselines that address the objective mismatch would further increase the significance of the results (see notes about related work).\n- Lastly, while MnM mitigates the objective mismatch, it is unclear from the results what are the benefits of this. Figure 5 suggests that out of 12 continuous control environments, only on `DClawScrewRandom-v0` we can make a conclusion that MnM significantly outperforms the presented baselines. In 3 MuJoCo environments, the performance of MnM and MBPO does not differ significantly. In the other 8 environments, it is hard to make definite conclusions due to the variance of returns.\n\nThe authors provide theoretical analyses associated with MnM. There are several concerns about this part of the paper as well:\n- The proposed method optimizes a bound $L(\\theta)$ on the log expected returns. The authors claim that “this bound becomes tight under certain assumptions”. To support the claim, the paper introduces another bound $L_\\gamma(\\theta)$ and proves that the supremum of $L_\\gamma(\\theta)$ coincides with the log expected returns. While the reviewer appreciates the similarity of the $L_\\gamma(\\theta)$ with $L(\\theta)$, the new bound relies on (a) non-markovian dynamics (b) **time-varying** and **parameterized** discount factor, limiting the justification of the method by this theoretical result.\n- With the above in mind, it is possible to argue that even a standard MLE-based MBRL algorithm optimizes a lower bound on the expected returns (yet this bound will not be tight) using the Simulation Lemma [3]. See section 2 in [4] for an explanation. Since the bound that the practical algorithm optimizes is not tight, it is unclear to which extent the proposed method mitigates the objective mismatch.\n- Lastly, the analyses make several assumptions that might not be entirely justified. The most noticeable one is the assumption about the positive reward. While it is common to assume that the rewards are *non-negative*, the assumption about positive rewards might be limiting. Moreover, the rewards in certain environments (e.g. `HalfCheetah-v2`) where MnM is tested take negative values.\n\nDetailed notes and questions\n\nThere are several smaller concerns and questions that the authors might address:\n1. The literature review could be improved. In particular, relevant references include [1] that characterize the set of models that are optimal for planning; [5] that optimize the policy and the model using the same objective; [6] that learn the model to directly optimize agent’s performance, not a lower bound on it; [7] that learn the model that is helpful for policy improvement using a weighting scheme.\n2. Since $\\Gamma_\\theta(t)$ is a CDF, does it imply that $\\gamma(t)$ is a distribution?\n3. “One detail of note is that we omit the reward augmentation for MnM during these experiments, as it hinders exploration leading to lower returns.” Is it the entropy augmentation of SAC or the proposed augmentation with the log density ratio?\n4. Is the reward function learned as well?\n\nReferences\n\n[1] Grimm, Christopher, André Barreto, Satinder Singh, and David Silver. \"The value equivalence principle for model-based reinforcement learning.\" arXiv preprint arXiv:2011.03506 (2020).\n\n[2] Henderson, Peter, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. \"Deep reinforcement learning that matters.\" In Proceedings of the AAAI conference on artificial intelligence, vol. 32, no. 1. 2018.\n\n[3] Kearns, Michael, and Satinder Singh. \"Near-optimal reinforcement learning in polynomial time.\" Machine learning 49, no. 2 (2002): 209-232.\n\n[4] Farahmand, Amir-massoud, Andre Barreto, and Daniel Nikovski. \"Value-aware loss function for model-based reinforcement learning.\" In Artificial Intelligence and Statistics, pp. 1486-1494. PMLR, 2017.\n\n[5] Schrittwieser, Julian, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez et al. \"Mastering atari, go, chess and shogi by planning with a learned model.\" Nature 588, no. 7839 (2020): 604-609.\n\n[6] Nikishin, Evgenii, Romina Abachi, Rishabh Agarwal, and Pierre-Luc Bacon. \"Control-Oriented Model-Based Reinforcement Learning with Implicit Differentiation.\" arXiv preprint arXiv:2106.03273 (2021).\n\n[7] D'Oro, Pierluca, Alberto Maria Metelli, Andrea Tirinzoni, Matteo Papini, and Marcello Restelli. \"Gradient-aware model-based policy search.\" In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 04, pp. 3801-3808. 2020.\n\n",
            "summary_of_the_review": "The reviewer recommends rejecting the paper. While the idea is promising, the submission in its current state needs substantial improvements. Addressing the outlined concerns might increase the overall score.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper concerns the objective mismatch problem in model-based RL that the model is optimized for prediction accuracy, not a good performance. The authors propose Mismatched no More (MnM) that maximizes a lower bound on expected reward in an adversarial training manner. They also give a global lower bound that holds for any dynamics. Experiments also validate the effectiveness of MnM.",
            "main_review": "Objective mismatch problem is important in MBRL. And the proposed MnM is motivated by the idea of GAN to solve this problem. The whole paper is easy to follow and I also appreciate the theorems in the paper. However, I would like to see more comparisons, both theoretical and experimental, that if MnM performs better than the VAML framework [1,2] and the simple weighting strategy in [3]. Both these works are addressing the same problem, so it would be important to directly compare them, especially considering that MnM is a more complicated algorithm.\n\n[1] Value-Aware Loss Function for Model-based Reinforcement Learning. Amir-massoud Farahmand et al. AISTATS 2017\\\n[2] Iterative Value-Aware Model Learning. Amir-massoud Farahmand et al. NeurIPS 2018 \\\n[3] Objective Mismatch in Model-based Reinforcement Learning. Nathan Lambert et al. L4DC 2020",
            "summary_of_the_review": "The overall paper is motivated and clear, The concern I have is the advantage over previous works that address exactly the same problem with relatively simple mechanisms.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a new model-based RL method by devising a new reward function, which incorporates a term measuring the difference between the learned dynamics model and the true dynamics probability, in addition to the logarithm of the original environment reward. Maximizing this augmented reward function is proved to maximizing a lower bound of the logarithm of the original expected return. The reward function is not practically usable since the true dynamics probability is not known. To practically apply the proposed reward function, a GAN-like classifier is introduced to differentiate the transition generated from the dynamics model and the true dynamics. The experiments show that under an unified objective, the learned dynamics tends to assign higher transition probability for some (s,a,s’) if this transition can assist the policy to achieve a higher return in the true environments.",
            "main_review": "The method is well motivated, and the proposed augmented reward function is novel. The experiments are also well conducted, and the visualization of the learned dynamics provides intuitive illustrations that it tends to help the policy to succeed. The overall performance of MnM is also demonstrated to perform at least as well compared with SAC and MBPO.\n\nDespite these advantages mentioned above, I have some comments below that I’d like to hear from the authors in the responses.\n\n1. The augmented reward in Eq. (3) is informative, while in Theorem 3.1, the equality of the lower bound is hard to hold. In the proof of Theorem 3.1, Jensen’s inequality is applied, twice, to move the logarithm inside the expectation. For Jensen’s inequality, the equality holds when the function (logarithm here) is affine or the variable inside the function (the reward r here) is constant. Obviously, neither is the case here. Therefore, at least for the reward function in Eq. (3), maximizing over its cumulative expectation does not guarantee the optimality in the original expected return.\n\n2. Although, immediately, the following contexts about “Tightening the lower bound” gives a new reward function by employing a learnable $\\gamma_{\\theta}$, I cannot agree that this is really significant. From the proof of Lemma 3.2, the equality holds when the two analytical solutions are satisfied for $\\gamma_{\\theta}$ and the dynamics model, and the two analytical solutions are independent with each other. Then, for the learnable $\\gamma_{\\theta}$, it indeed plays a role of reward shaping that shapes the reward in Eq. (3) probably nonlinearly to reach the logarithm of the original expected return (probably a way to approximate the affine function to remove the Jensen’s gap?). Also, in the experiments, this new lower bound and reward function are not applied. So, I agree Lemma 3.2 demonstrates that there exists some way to modify the reward in Eq. (3) to have a tighter lower bound, while I think this is a benefit from reward shaping, instead of a thoroughly informative new reward function.\n\nMinor:\nIn the proof of Lemma 3.1, line (d) in the equation, the reduced $\\pi_{\\theta}(a_t|s_t)$ missed the logarithm.\n\nOverall, I think the proposed method is novel and the experiments give nice explanations for the learned dynamics in tending to assist the policy learning, and MnM indeed has its superiority compared to prior model-based RL methods.\n",
            "summary_of_the_review": "A good paper that introduces a novel model-based RL method, where the policy learning and dynamics modeling share a unified objective.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}