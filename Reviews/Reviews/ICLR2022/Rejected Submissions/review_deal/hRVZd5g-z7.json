{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper eventually got 5 \"marginally above the threshold\" after rebuttal. Such scores testify to that the paper is a borderline one. By reading the post-rebuttal comments, it is evident that most of the reviewers still deemed that the novelty is incremental. One of the reviewer (vUb9) raised the score simply to \"encourage the authors to think more important problems\", rather than acknowledging the merits of the paper. The AC also read through the paper and had the following opinions:\n1. The paper is actually about DNN compression, based on the \"new finding\" that the weights across layers are low-rank. However, the authors would not write the paper in the way of DNN compression, but put more emphasis on the \"new finding\", which has no theoretical support at all (only some heuristic reasoning). The AC would deem that the \"new finding\" is only an assumption.\n2. Actually the \"new finding\" is not new at all. For example,\n\n[*] Zhong et al., ADA-Tucker: Compressing Deep Neural Networks via Adaptive Dimension Adjustment Tucker Decomposition, Neural Networks, 2019, \n\nused a shared core tensor (which could be regarded as the common dictionary) across all layers for higher compression rates. More recent references that use tensors and consider shared information across layers for compression can be easily found as well.\n\nSo the AC thanked the authors for preparing the rebuttals carefully, but regretfully the paper is not good enough for ICLR."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a scheme for partially sharing CNN parameter across layers.  Specifically, convolutional layer parameters are factored into the product of layer-specific filter atoms (which define spatial filter structure) and a set of shared coefficient that mix features across channels.  Training CNNs reparameterized in this manner yields improvements along the accuracy-parameter tradeoff curve, compared to baseline models on image classification tasks.",
            "main_review": "The factorization approach for convolutional layers is exactly that proposed in DCFNet [Qui et al., 2018].  The difference is in terms of choosing to share channel mixing coefficients between multiple layers.  However, [Qui et al., 2018] focus on another means of parameter reduction -- namely, choosing a fixed basis for the filter atoms, and report accuracy-parameter efficiency improvements.\n\nAs the paper makes use of the same factorization as DCFNet [Qui et al., 2018], I would expect an experimental comparison between DCFNet and the proposed approached.  The paper also cites [Li et al., 2019] as another previous work that uses a filter basis factorization.  Moreover, Section 4.2 of [Li et al., 2019] proposes sharing the same basis across multiple convolutional layers.  Given this similarity in terms of ideas, it also seems necessary to experimentally compare to [Li et al., 2019] and provide further discussion of how the proposed approach and/or experimental results differ from these two prior works.\n\nFurthermore, though the paper does compare results to LegoNet [Yang et al, 2019] in Table 2, that comparison is uninformative as it applies the filter parameterization scheme of LegoNet to VGG, while applying its proposed scheme to Wide ResNet.  VGG vs Wide ResNet is a significant change in the base network architecture, which is a confound for the question of which filter parameterization is better; each method could be applied to either base architecture and the comparison should be made by choosing the same base architecture.\n",
            "summary_of_the_review": "This paper proposes a cross-layer parameter sharing scheme for CNNs that demonstrates some benefits on image classification tasks.  However, the proposed method is similar to filter factorization approaches in several previous published works, for which experimental comparison is missing or incomplete.\n\nUpdate (after rebuttal):\nThe author response addresses some of the experimental concerns from my initial review.\n",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a novel method to decrease redundancy of parameters in a CNN through a joint subspace view. The authors use a low-rank decomposition of the convolutional kernel (A * D) into shared coefficients (A) and atoms (D). The authors propose a variety of proposal methods for their method termed Atom Coefficient Decomposed Convolution. Notably by sharing A across all layers, sharing A across groups of layers and by utilising the idea of filter groups. The authors demonstrate that ACDC-x outperforms baselines at moderate to significant reductions in model size. Further, they show that their method can alleviate issues of model degradation as deeper networks are used.",
            "main_review": "[Strengths]\n\n* The idea to use a low-rank decomposition of convolutional kernels and to consequently use the coefficients as sharable weights throughout a network is really interesting. This is especially true for the filter groups networks as the authors show a significant reduction in parameters and improvement in model performance (Table 1). Performance of the method when compared against a good number of _light_ CNN architectures on CIFAR-10/1000 + TinyImageNet is good and further demonstrates the performance.\n\n[Weaknesses]\n\n* The main motivation for the joint subspace (Section 2.2) is quite confusing in the opinion of this reviewer. It is difficult to understand why exactly one can exploit \"intrinsic, low-rank structure...across layers\" from the explanation and the pilot MNIST experiment. This is an important section that needs to be reworked.\n\n* The idea to share filter basis' across layers is not entirely new some and _shares_ similarites to https://arxiv.org/pdf/2006.05066.pdf. (Deeply Shared Filter Bases for Parameter-Efficient Convolutional Neural Networks). In that paper, they also learn a filter basis that can be shared across layers despite a different methodology. \n\n[Other]\n\n* In the filter groups model, how exactly does ACDC work? Is there a limitation that you need to have equal amounts of filter groups at every layer as the dimension of the shared coefficient is contingent on the groups _g_?\n\n* The decomposition method that is based on Qiu et al. 2018 is presented in Section 2.1. How much does this differ? Why not just use Qiu et al?\n\n* I would double check the notation for all subscripts as some $\\times$ symbols seem to be missing",
            "summary_of_the_review": "The idea to learn a joint subspace view to share weights across layers in a CNN is interesting especially with filter groups. They show good results on a variety of datasets against a solid number of baselines. Despite this, it is to my feeling that the idea is not entirely novel as i) low rank decomposition of convolution kernels has already been performed and ii) the idea to share filter basis/weights across layers has also been proposed. Further, the motivation/pilot experiment for the insight to sharing shared coefficients over layers needs to be worked on. I therefore propose 6 for now.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work first shows empirical observations that there exist obvious correlations in features across layers within a CNN after proper linear transformations. Accordingly, the authors propose coefficient sharing strategies across layers or across groups of filters, which aim to decompose convolution filters for reducing model sizes. The experiments are conducted on four image benchmarks using several CNN backbones, and the results show the effectiveness of the proposed ACDC on image classification and few-shot image classification tasks.",
            "main_review": "Strengths:\n(1)\tThe idea of coefficient sharing across different settings (e.g., the whole networks, the convolution stages, convolution layers, and groups of filters) seem interesting.\n(2)\tThe experimental results show the proposed ACDC is superior or competitive to state-of-the-art methods.\n(3)\tThe paper is clear written.\n\nWeaknesses:\n(1)\tMy first concern is the settings of coefficient sharing. For CNN models, different convolution stages capture various kinds of features. For example, the preceding layers or blocks extract the fundamental cues (e.g., color or boundaries), while the latter layers or blocks learn the high-level semantic features. Although there exist some correlations in features across layers, correlations among different-level settings may be clearly different. Therefore, the best settings (e.g., the whole networks, the convolution stages, convolution layers, and groups of filters) of coefficient sharing may vary in various backbones and tasks. So, could the authors give the guideline or develop an adaptive method?\n(2)\tAs shown in Table 1, coefficient sharing across groups of filters have great effect on the final performance. Could the authors give more detailed discussion? Furthermore, the original backbone models with only groups of filters could be compared as baselines for verifying the effectiveness of ACDC-g. How about the performance of ACDC-g on ImageNet?\n(3)\tThe authors claim that the proposed method can provide better interpretation for CNN. However, it seems not very clear for me. Could the authors give more detailed discussion?  \n\nOther Comments:\n\n1.\tCompared to 2D convolutions, 3D convolutions need more optimization due to high computation complexity. Therefore, do the proposed method adopt to 3D convolutions (spatial and temporal information are coupled)?  Could the authors provide some tiny examples about 3D convolutions?\n2.\tSome typos, e.g., 13.49-> 13.49M and 8.57 -> 8.57M in Table 3.\n",
            "summary_of_the_review": "The idea of coefficient sharing across different settings seem interesting, and a ok experimental results. However, there exist some weaknesses in this work (see main comments above). Therefore, my current rating is borderline accept.  ",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces a joint subspace view between the layers of convolutional neural network. Since the important features across the layers in deep CNN is maintained, the filters can be decomposed into shared coefficients and layer-specific coefficients. Motivated by multi-scale decomposition, they compose the shared coefficient in order to exploit correlations between features maximally. Also, by using group convolution, they reduce the amount of parameters as well as maintain the performance. This method is easily compatible to modern CNN architectures.",
            "main_review": "Pros\n1.\tMotivation is well defined. By suggesting motivating experiment, the method of the paper could be verified theoretically as well as experimentally.\n2.\tThe method is simple but very strong. The method is just decomposing the shared atom coefficient of the layers in CNN, then they convolute images only with layer-specific coefficients followed after matrix-vector multiplication with respect to shared coefficients. Furthermore, by using group convolution, they accelerate the speed of the algorithm as in Tab.4. Even with fast computation, it shows good performance at network reduction, with at least maintaining or improving the error ratio.\n3.\tThe method can be compatible with various case. Since there are additional methods to be combined with the proposed such as block (section 2.3.) and group (section 2.4.) approaches, it can have a lot of variants.\n\nCons\n1.\tThere is a lack of reasons for using maximum value of C_in and C_out in the section 2.3. If you already have ablation study on that, it would be better to have in text, somewhere.\n\nMinors\n1.\tThe caption of Figure 1 is same as the third paragraph of the Introduction (replica). If the authors provide additional description of the figure, it will be clearer.\n2.\tThe authors need to explain about the bottom block in table 2. The description of third paragraph of section 3.1 has no referred table or figure. (The former is likely results about your methods and the latter seems to be an analysis about table 2.)  Also, the author needs to clarify the meaning of y-axis in figure 5.\n3.\tThere is a typo in Fig. 4 : D_{n_1}\\in\\mathbb{R}^{k\\times l\\times l} -> D_{n_1}\\in\\mathbb{R}^{m\\times l\\times l}\n4.\tThe organization of the manuscript is uncommon; a section of related work is located after result section.\n",
            "summary_of_the_review": "Overall, the paper is hard to follow the logic because of unusual organization. However, the novelty of the proposed method is fully convinced with supported experiments and their explanations.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "To reduce the parameters of the deep networks, this paper proposes a joint subspace view to convolutional filters across network layers. Experimental results verify the effectiveness of the proposed method.",
            "main_review": "Depending on the low-rank structure observation, this paper proposes to jointly model filter subspace across different layers by enforcing cross-layer shared subspace coefficients. The key idea is that all convolutional filters are decomposed into a shared coefficient submatrix and special filter atoms. \n\n+ves: \n+ The idea of the shared subspace coefficient for reducing the parameter redundancy seems interesting. \n+ This paper is well-written and is easy to follow.\n\nConcerns: \n\n- The novelty of this paper is weak. In fact, the idea that the convolutional filter is considered a multiplication of two small (low-dimensional) submatrixes has been proposed in [ref-1]. Of course, this paper proposed a shared coefficient submatrix. However, this novelty is not enough for the standard of ICLR.\n- The experimental results are not surprising. For the small datasets (e.g., CIFAR-10, CIFAR-100, and TinyImageNet), they only need a small deep model essentially although the proposed method is to reduce a large number of the parameters. For the large datasets (e.g., ImageNet), the proposed method is to reduce a small number of the parameters. Moreover, many related works (e.g., ShuffleNet-V2,MobleNet-V2) are missing to compared with the proposed method on ImageNet.\n-I am sincerely hoping that the authors deeply think more important problems in the topic of speeding up deep networks.\n\n[ref-1] Speeding up Convolutional Neural Networks with Low Rank Expansions, BMVC 2014.\n",
            "summary_of_the_review": "I recommend the borderline rejection due to the limited novelty and unsurprising experimental results.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}