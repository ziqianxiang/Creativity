{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper develops an approach to learning hierarchical representations from sequential data. The reviewers were very positive about the overall approach, finding it well motivated and interesting with strong potential, and thought that the paper was extremely well written with clear examples throughout. There was a good back-and-forth between the reviewers and the authors, discussing several aspects of the paper and providing constructive suggestions for improvement. In particular, the reviewers suggested improvements in terms of independence testing, comparison to further baselines, further experiments, and other improvements as detailed in the reviews. The authors were extremely receptive of these suggestions, which is to be commended and is very much appreciated, and in a response state that they are planning to take the time needed to revise this paper before publication."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents HCM, an approach for chunking a sequence of data into a hierarchical representation. More specifically, HCM learns a tree with atomic units (ie the low-level inputs, in this case integers representing things like text characters or quantized pixel values) as the leaves and increasingly complex groupings of them higher up the tree. \n\nHCM learns by iteratively parsing the provided data (ie stream of tokens), in each pass computing marginals for the current set of chunks as well as transition frequencies between them. After updating its marginals and transition frequencies, the two chunks with highest joint probability are combined into one. The process continues until all pairs of chunks pass an independence test. \n\nI believe the main contribution of this paper is in that it presents an idea for interpretable grouping based on the principle of grouping by proximity from cognitive science, and a largely qualitative proof of concept for it.",
            "main_review": "**Strengths**\n\n- I believe the paper's main strength lies in its motivation. I believe the core of the presented idea is compelling, and would be of interest to the community.\n- The paper is clearly written, and the method is simple.\n\n**Weaknesses**\n\n- The paper presents primarily qualitative results for the majority of datasets/tasks used. The experiment performed on a text corpora only presents a table of examples with learned chunks, and the visual-temporal experiment only presents a figure with some of the learned visual chunks. It is not clear to me from the presented experiments how to compare this method to alternatives. There is one experiment comparing against an RNN baseline, showing that HCM converges faster — however, RNNs are not the current SOTA in sequence modeling (i.e. why wasn't a transformer model used?).\n- I am concerned that the method as currently defined cannot generalize to real world data. HCM parses chunks from a sequence by matching them exactly to subsequences, which to me means that this method groups segments together purely based on form rather than semantics. My perspective is that the promise of hierarchical representations is that you can decompose complex objects and patterns into their parts (e.g. a person into [head, arms, legs] — head into [eyes, ear, nose], etc...). However, in modalities such as vision the same parts can appear with drastically different color values. The paper alludes to this in its Discussion section, but does not present a solution to this problem, which is something that I think would need to be shown.\n- Related to my point above, I'm not entirely sure I understood the thesis of the paper in terms of the narrative it is trying to convey, and would appreciate hearing the authors thoughts on this. Is this meant to be received as a paper for the cognitive science community, showing an operationalization of grouping by proximity? Or is it being presented for the machine learning community as a representation learning method for use in downstream tasks? If it's the former, I believe this work would be much more appropriately submitted at a cognitive science conference. If it's the latter, I believe much more empirical evidence of the learned representations' usage needs to be shown.\n- The related work section mainly focuses on historical NLP methods, with little discussion over similar methods in computer vision, which I believe is needed given that it presents experiments on visual data. I would suggest works such as:\n    - Normalized Cuts and Image Segmentation by Shi and Malik 2000\n    - Selective Search for Object Recognition by Uijlings et al. 2012\n    \n    as places to start. Additionally, I think work on unsupervised grammar induction could also be relevant here.",
            "summary_of_the_review": "Although I believe the motivating idea is very compelling, I don't believe this paper is ready for publication. In summary, I believe the paper currently lacks: \n\n- More thorough empirical evaluations comparing against other methods.\n- Experiments showing the method's potential for generalizing to more naturalistic data, as well as its usefulness for downstream tasks.\n- A more clearly focused narrative motivating why it's appropriate for a venue like ICLR as opposed to a cognitive science publication, as well as more thorough contextualization among related work (particularly comparing against recent alternative methods for this problem).\n\nI thank the authors in advance for their response, and am also interested in seeing other reviewers' thoughts. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a graph-learning model (HCM) for learning hierarchical chunks from sequential data. The paper first proposes an idealised HCM method, for which the paper provides learning guarantees via a proof by induction, and an online approximation to this idealised method, which is more computationally feasible and which is used to perform experiments in temporal, visual, visuotemporal and language sequential data domains. The paper demonstrates that the online method learns interpretable chunks at multiple levels of abstraction and demonstrates positive (and negative) transfer to other hierarchically structured environments with similar (and different) structures.",
            "main_review": "Strengths:\nThe paper is very well written, with very clear, intuitive explanations for how their method works, and justifications for the authors’ design choices.\n\nThe paper provides several well-considered experiments to demonstrate the HCM method quantitatively and qualitatively. First, purely sequential data is generated from several random (but known) heirarchically-structured graphs and the HCM method is shown to learn this underlying hierarchical structure well, compared to a vanilla RNN. Secondly, the paper verifies that the learned model shows positive and negative transfer to similarly or differently structured heirarchical environments, as might be expected from a chunk learning algorithm. Fianlly, the paper explores how the HCM model performs qualitatively in spatial, spatiotemporal or english-language chunking, with interpretable (although unquantified) results in each.\n\nThe connections to animal chunk learning are well thought through. Interestingly, for the case of spatiotemporal chunking, without considering a priori the spatial proximity of pixels, spatially connected chunks are learned. So it is by virtue of the fact that objects tend to move smoothly in space and time that online HCM will learn to group visual spatial chunks smoothly in the height x width plane too. This has really interesting close ties to theories for animal learning of object permanence (although obviously the implementation is very different), as the authors note.\n\nWeaknesses:\n\nThe paper mentions that this method should offer more interpretable learned representations, but for what sort of task or application is this envisaged? Regarding the transfer of learned chunks to new data sequences, it seems that a human (or other model) would have to know the underlying generative process of the target data sequence in order to know whether the original learned chunking model should work well in the new setting or not (unless of course, the data is generated from the exact same process as the training data). If a human (or other model) knows that then is it not true that you don’t need the model to do the chunking in the first place?\n\n\nIt would have been nice to see quantitative demonstrations of performance for the spatial, spatiotemporal and language-chunk learning experiments. I appreciate its not immediately obvious what the right metric for this performance would be (at least to me), but if the authors were willing and able to find an appropriate one and use this to compare their method to other chunk-learning algorithms it would definitely strengthen the paper.\n\nIn the learning plots vs the vanilla RNN, the paper would also have benefitted from comparisons to other explicit chunk-learning algorithms.\n",
            "summary_of_the_review": "A well-written description of a method for a chunk-learning algorithm, with learning guarantees and qualitative demonstrations of sensible-looking chunks across a variety of domains. Quantification of results was a bit lacking.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a method for learning representations of non- i.i.d. data in terms of hierarchical sets of chunks, inspired by cognitive theories of grouping by proximity. These sets are assembled over time from the initial set of primitive data points by finding correlations between temporally/spatially sequential primitives/chunks and appending to the set. The authors show that this learning method is tractable, has convergence w.r.t. hierarchically-decomposable problems, and learns intuitively and practically reasonable chunk sets.",
            "main_review": "Strengths:\n- This paper is particularly well-written and understandable. I appreciated the intuitive explanations of chunking in cognitive science and its extension to common machine learning use cases like language and visual data. The examples of instances where hierarchical chunk learning could both help and hurt a learned model were well-chosen. The figures effectively demonstrated the training process and the learned representations in each domain. Even the theorems were more interpretable than I typically see, being subdivided and laid out piece by piece.\n- The method is reasonably novel and broadly applicable. The paper shows HCM applied to temporal, visual, visuo-temporal, and language domains. Given a domain with some hierarchical structure, a fairly reasonable assumption, this method is able to find that hierarchy with some guarantees. The learned hierarchy itself, as the authors note in the conclusion, could be applied to down-the-line endpoints such as -causal learning.\n- This method really leans into explainability/interpretability and could thus be more compatible in human-ML frameworks\n\nWeaknesses:\n- While the method is novel and seems to recover structure quite well, the results are not as convincing as I’d like.  To lay this out:\nGiven a toy generative hierarchical model, HCM is able to more effectively predict sequences than a basic RNN, particularly as the levels of hierarchy increased. Not to be too glib, but I should hope so!\n\n- In an environment where the HCM representations overlap with the underlying model, it outperforms a learned-from-scratch HCM, while in the opposite case, it underperforms. The authors suggest that the nature of the HCM (as compared to something like a DNN) allows users to understand a priori whether their pretrained model will work well, which I agree with\n- In toy visual domains with and without temporal correlations, HCM learns reproduces the underlying representations. But how does its ability to reproduce the actual sequences compare with appropriate baselines.\n\n- Finally, HCM is applied to a corpus from the Hunger Games and is able to learn commonly-repeated phrases over time.\n\nMy main concern with all of this is the lack of actual baselines. I agree that the models are interpretable and useful, but they aren’t applied to any previously-used datasets or compared (empirically) to other SOTA methods. HCM doesn’t necessarily need to *win* in performance, given its other advantages, but I’d like to see whether it’s competitive\n\n- On a related note, the authors provide both idealized and online HCM algorithms. Even the “online” algorithm, while theoretically tractable, seems practically quite slow, which I assume is why the chosen domains are simple. While the online algorithm seems to work well for these domains, I would imagine the loss of guarantees is more likely to be impactful in harder domains.\nIt was not clear to me how the chunks were generated until I read the independence tests section in the appendix, and I think that this is too important to push out of the body of the paper. It also introduces the hyperparameter of statistical significance p, which isn’t really discussed.\n",
            "summary_of_the_review": "I like this algorithm and think it has potential. I can see how it can be applied both to standard ML tasks, but also how it could unlock a more symbiotic human-ML collaboration through its interpretability. The motivation and build-up from cognitive science is clear, and all else aside, because of its writing, I felt this paper gave me a lot more valuable insights than most. That said, I’m just not convinced by the current set of experiments. I can’t glean how well HCM will *actually* perform (vs. baselines on standard datasets), particularly the online variant, and I suspect it’s not computationally that practical either. With some of these comparisons added, I think I could accept, but for now it’s a reject from me.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a non neural system of parsing natural language text by chunking sequences to form hierarchical structures. The algorithm strongly resembles classical parsing algorithms. Decisions about when to chunk a phrase into a constituent are based on chi^2 tests of independence, where a pair of chunks that are considered to be dependent are joined into a single constituent. They test this chunking algorithm on natural language data against an RNN,  concluding that the classical parsing algorithm is more sample efficient in achieving a low KL-divergence from the true sequence data. They also provide some examples of how this algorithm can be applied to temporal image data or video.",
            "main_review": "\nThe paper is clear. I rarely had trouble following, although I didn’t understand that the decision to chunk was based on a chi^2 test until I read the appendix, which seems crucial. I enjoyed reading about the relative sample efficiency of the classical algorithm vs the RNN, though I would have rather seen a fair comparison with a TreeRNN or some other system that involves latent tree structure, as well as a comparison to other classical dependency parsers. The application of a classic parsing algorithm to video was a nice adaptation.\n\nThe overall problem I had with this paper was the fact that it is presenting a classic parsing algorithm but contains no citations to any work from the age of classic parsing algorithms. I found this lack of background disturbing because as far as I can tell this algorithm is a statistical stack based parser, and the authors should have looked into whether they were reproducing existing work. The problems they have with efficiency of their own algorithm are resolved by many statistical parsing algorithms. Even allowing partial parses (as they do) is a property in a number of non neural parses such as https://www.cs.cmu.edu/~nschneid/twparser.pdf (A Dependency Parser for Tweets by Kong et al.). Ironically, I also had trouble looking for specific classical parsing algorithms to compare with this while reviewing, because the literature has exclusively contained neural parsing algorithms for so long. The general area of structured prediction is one that has a long history, and the authors seem not have a particular background in the problem space. I recommend reading Slav Petrov’s thesis (https://www2.eecs.berkeley.edu/Pubs/TechRpts/2009/EECS-2009-116.pdf) for a deep background on the topic from the age of classical parsing.\n\nAlthough the paper described the problem of a lack of inductive bias towards hierarchical parse structures, there were no citations to the literature which attempts to resolve this problem (treeRNNs, RNNGs, etc.). There was also no discussion of non-neural hierarchical algorithms for structured prediction on video (e.g., structured prediction cascades), which seems necessary in a paper with experiments on a non neural hierarchical algorithm for video.\n\nBeyond the lack of discussion of the existing field of algorithmic hierarchical parsing, the discussion of limitations does confront the possibility of non projective grammars, which cannot be covered by this sort of chunking (“How to relax the adjacency assumption as a grouping criterion to allow for non adjacent relationships to be chunked together remains an open challenge”), but does not to discuss it in terms that have been used historically in parsing, or acknowledge the existing parsers that cover non-projective cases. I was somewhat confused by the decision to use The Hunger Games as a corpus for training natural language parsers on, as there are a number of more common corpora that would have compared more easily to the existing literature (The Little Prince, PTB, or wikitext come to mind). I was confused by the reference to Teh 2006 alone as an extension of ngram models, given that there was no other discussion of backoff (e.g., Katz back-off, or smoothing) in ngram models, which has a much longer history. \n\nI was not surprised that introducing a parsing algorithm with a strong inductive bias was more sample efficient than using an RNN. This phenomenon is the reason why, for years, NLP did not use neural networks until large quantities of data and compute became easily available.\n\nMINOR\n\nPlease explain how the hypothesis testing works in the main text of the paper, and not just in the appendix, or at least emphasize appendix A in the main text of the paper while describing the algorithm.\n\nTypos:\n“they the way”\nHinton (1979) should be a parenthetical but is instead inline citation.\n\n Questions: \n\nHow does catastrophic interference relate to gradient starvation?",
            "summary_of_the_review": "This paper is missing significant background on classic hierarchical structured prediction. Because it is presenting a classical parsing algorithm without a single citation to pre-neural structured prediction as a field, I believe that it is extremely similar to existing algorithms that are rarely in use today.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}