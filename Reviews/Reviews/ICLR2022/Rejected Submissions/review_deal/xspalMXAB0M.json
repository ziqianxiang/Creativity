{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes a boosting algorithm for RL based on online boosting. The main advantage of the result is that the sample complexity does not explicitly depend on number of states. Post rebuttal, some of the reviewers have changed their opinion on the paper. However, overall the reviewers still seem to be on the fence about this paper. Seems like the paper combines the techniques from Hazan Singh’21 along with a frank-wolfe algorithm to deal with non-convex sets but the reviewers seem to view this as not as significant a new contribution. \n\nI see the paper as being interesting but do agree with some of the comments of the reviewers and am leaning to a reject."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies boosting-like RL algorithms using the most boosting techniques from online learning. It advocates the advantage of the proposed algorithm as the sample complexity's independence of the number of states. ",
            "main_review": "Strengths:\n1. The idea of transferring the most recent online boosting technique to RL is interesting. \n2. This paper gives solid theoretical analysis of the proposed algorithms.\n\nWeakness:\n1. The novelty looks limited as the techniques are very similar to Hazan and Singh 2021. Moreover, this paper does not discuss the major difficulty of designing boosted RL compared to designing boosted contextual bandits, which has already been proposed in existing literature.\n2. The presentation has some problems. If this paper aims to deal with the large state space problem in RL, then it misses some important related work in state compression techniques.\n3. Although this paper is mainly theoretical, some initial experiments will be beneficial to validate the effectiveness of the proposed algorithms. Hazan and Singh 2021 also gives some experiments.",
            "summary_of_the_review": "This paper extends the recent online boosting technique to the RL setting. The current presentation has some problems and the technical novelty looks a little bit incremental.\n\nI recommend the authors to (1) discuss the difference between designing boosted contextual bandits and boosted RL; \n(2) design some initial experiments to justify the effectiveness of the proposed algorithms. \n(3) consider to present the logic of this paper in another way (not motivated by the large state problem in RL). ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the authors study boosting in RL, i.e., how to convert weak learners into effective policies. The authors provide an algorithm that improves the accuracy of the weak learners iteratively, and the sample complexity and running time do not explicitly depend on the number of states. \n\nIn order to overcome the non-convexity of the value function (with respect to the policy space), the authors use a non-convex variant of the Frank-Wolfe method together with recent advances in gradient boosting. ",
            "main_review": "Strength\n\n1. This paper studies boosting methods in RL, which is an interesting topic on its own and has not been explored much from the theoretical perspective yet.\n2. The algorithm uses a variant of the Frank-Wolfe method to overcome the non-convexity of the value function (with respect to the policy space). To do so, the analysis relies on recent investigation of the policy gradient algorithm which identify conditions under which the value function is gradient dominated. The application of  the Frank-Wolfe method in RL seems interesting. \n3. The writing of the paper is well-structured and very easy to follow. The analysis looks rigorous. \n\n\nWeakness:\n1. The authors only consider weak learner that optimizes a linear function over policy space, which seems restrictive. \n2. The analysis does not look very novel given prior work (Hazan & Singh, 2021). ",
            "summary_of_the_review": "Weighing the strength and weakness of the paper, I am recommending weak accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper attempts to deploy the boosting technique in the supervised learning on policy learning in the RL setting. The author suggests a protocol to do policy aggregation from weak policies through the quantity $\\textit{the extension operator}$, and presents the probabilistic convergence guarantee for the learning process.",
            "main_review": "I was called to do an urgent review on this submission and I did not have time to check the mathematical details in full. \nIn general I do not think the paper is well-written, as many notations are directly referred to without definition such as \\mathbb{\\Lambda}, \\mathbb{\\Pi}, d_0, d_{d_0}, d^{\\pi}, R_{\\mathcal{W}}(M). \nIt is difficult to catch the main idea of this paper, and it is dubious if the proposed algorithm can be implemented in practice. Specifically, \n- how should one compute $\\nabla F_{G,\\beta}$ in practice? \n- How does one know that a function $f$ which satisfies the definition 2 (function aggregation) exists? How does one know that a policy $\\lambda$ that satisfies the definition 4 (shrub) exists?\n- In practice, is there a way to estimate policy completeness as defined in Definition 9?\n- In definition 10 (distribution mismatch), is there any assumption made to ensure that $C_\\infty \\ne \\infty$?\n- Any explicit comparison between the proposed boosting-enhanced learning algorithms and no-boosting RL learning algorithms?\n\nThe meaning of notations is inconsistent from place to place. \n- $P$ in theorem 11?",
            "summary_of_the_review": "I suggest the paper be added more clarification of notations and interpretation of main results. The paper is certainly not ready to be published due to its lack of clarity and unrefined narrative.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes boosting (using Frank-Wolfe) weak RL learners to get a strong (in a concrete sense) policy. The main theoretical bound does not depend on the number of states, which indicates that the method should be competitive in very large state-space MDPs.",
            "main_review": "FYI: this is an emergency review that I agreed to do earlier today, so I have not examined the appendix in detail.\n\nThe paper is extremely dense and hard to read in many places. It suffers substantially from notation overload. It is very hard to parse the results and contributions since they are obscured by very dense notation. This is further made difficult by a few typos and undefined notation in some places. The paper reads like a list of mathematical concepts / definitions that are not really strung together in a coherent (to the reader) manner. I have no doubt that these concepts are all important to the results, but the paper would be vastly improved by a) improving clarity and doing more to help guide the reader's understanding of what is going on and why it is important and b) reducing the notation significantly, even at the loss of some generality or at the expense of moving some material to the appendix.\n\nThis paper would also be improved significantly by a numerical example, even on something very simple like a grid world. I was left with no intuition about how this method would perform in practice. In a similar vein, the paper did not discuss when / if one should use the main proposed algorithm. At the start the authors mention working in 'very large MDPs' and 'MDPs with large state space' - but that is not really supported with any discussion or experiment (other than the complexity bound, which is not enough to build intuition).\n\nThe 'two-layer neural network' is mentioned a few times, but I don't really see how it is used / discussed. Where in the algorithm is the two layer NN used?\n\nNot enough discussion is given to the basics of boosting.\n\nI like the inclusion of the proof sketch, but it should be even more detailed and should provide much more intuition.\n\nSeveral claims are made in the paper without citation - eg the sentence \"Training deep neural networks in the supervised learning model is known to be computationally hard\". This requires citations not just because it is a controversial claim, but because you say it is 'known' but you don't provide the reader with a way to learn about it if they are unaware of this fact. This pattern is repeated in several places, eg: \"However, in general no learner can outperform a random learner over all label distributions (this is called the “no free lunch\" theorem).\" Please give the reader a reference to learn about this theorem.\n\nA big benefit of the proposed algorithm is that the final complexity bound does not depend on the number of states, which is a very interesting result. However, I have no idea how that came about. The paper should explain how the authors managed to get rid of the dependence on S.\n\nOne point I am confused about - \"assume the availability of an efficient exploration scheme\" - where / how is this assumption used?\n\nThe paper says: \"we assumes access to a weak learner: an efficient sample-based procedure that is capable of generating an approximate solution to any linear optimization objective over the space of policies\" but also says that the problem over policies is non-convex (ie, non-linear). Do you mean to say 'linear optimization objective over the space of **state-visitation distributions**'?\n\nMinor points and typos:\n\n* 'w.r.t.' - don't abbreviate\n* \"we assumes\"\n* \"weak learner that attains\"\n* Is the initial state distribution mu or d0?\n* Page 4 - What is d_d ? Is s_0 \\sim d_0 or d ? \n* I do not understand the equation in Definition 2 at all\n* \"always slight better\"\n* What is d \\mu in Assumption 1? What is D(s, l).\n* What is R_W(M) in Definition 7?\n* \"weak learning is provided\"\n* bellman -> Bellman\n* Several terms in Alg 1 are undefined (eg. \\eta_2 never defined, \\hat Q never defined). The period is misplaced after line 6.\n* Theorem 11:  \"Algorithm 1 samples T(MN + P) episodes of length O˜(1/1−γ) with probability 1 − δ.\" I do not understand what 1-\\delta is doing there.\n* \"Note that Qπ (s, ·) produced by Algorithm 2 satisfies ||Qπ (s, ·)|| =|A|\\1−γ.\" What norm is this? Where is that justified?\n\n\n",
            "summary_of_the_review": "An interesting result, however I cannot recommend publication due to significant clarity and readability issues.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a new approach for solving RL problems with sample complexity independent of the number of states. Rather than imposing structural assumptions, the authors consider access to weak learners and propose a way to combine these weak learners effectively to generate a near optimal policy. The sample complexity result is competitive and does not depend on the number of states, under the assumption of access to weak learners.",
            "main_review": "Strengths\n\n- The authors propose an interesting and novel approach to solving the RL problem with unknown transition function and large state space. The use of combining weak learners and leveraging boosting ideas from supervised learning seems significant and new. I believe the core idea will be valuable to both RL and optimization communities.\n- The proposed algorithm aims to be agnostic, offering a way to bypass common structural assumptions (e.g. linearity) as long as one has access to certain weak learners. Such a setting has not been explored much previously in RL theory to my knowledge.\n- The technical results needed to achieve the sample complexity guarantee may be of independent interest.\n- The proofs seem reasonable, but I did not carefully check.\n\n\nWeaknesses\n\n- I think an example of a valid weak learner that satisfies either the SL or online assumptions would be very helpful for understanding the applicability of the results. For instance, are there existing algorithms that could generate such weak learners easily? Perhaps the answer is obvious, but in any case this seems like an important missing piece.\n\n\n- The paper is very tersely written and not easy to understand. There is little motivation or justification for both definitions and algorithmic/analytical design choices. Many are simply stated. Even some theorems (11, 13) are just stated. I believe a remark or an interpretation or a comparison with prior work would be enormously helpful. There is no description of any steps in Algorithm 1 besides the algorithm box itself. This issue is also compounded by the fact that there are many notational inaccuracies (see minor comments below for a subset), so a reader trying to parse the approach and results cannot reliably just read the math either.\n\n\nSpecific examples of lack of motivation of *definitions*: “The extension operator… operate overs functions and modifies their value…” Why are we interested in such an operator? Are there other operators we could use in its place? \n“To state the results, we need the following definitions” Why do we need these definitions? Are they being used to sweep certain problems under the rug (i.e. Def 10 for exploration)?\n\nI believe these parts (and others throughout) would significantly benefit from more supporting text explaining the reasoning. \n\n\nMinor comments:\n- It would be helpful to also include more discussion about the known achievable PAC sample complexities for RL and MDPs in the tabular case in order to highlight the difference with the state-space independent results here. For example, generative models (not the same setting but close): “Model-Based Reinforcement Learning with a Generative Model is Minimax Optimal” (Agarwal et al, 2019)\n- Assumption 1 seems strong since it looks like the weak learner is required to be good on any potential distribution induced by the policies. Can you elaborate on this? An example, as suggested earlier, would be helpful.\n- R_W is never defined in the main text.\n- $\\eta_{2, t}$ is undefined in Alg 1 until Thm 13.\n- The input parameters of Alg 1 are not described. One has to look at Alg 3 in the appendix to see what P is even used for.\n- M and P are overloaded.\n- In the definition of M in Theorem 11, there is a little m followed by a tuple. What is this?\n- Little m is undefined in Alg 1.\n- Thm 11: $\\pi$ is undefined. I think this should say $\\bar{\\pi}$.\n- Thm 11 (and elsewhere): From what I can see, the policy class $\\Pi$ is never defined except in some places it is said to be an arbitrary base subset of all policies. When used in the theorem, is this meant to be $\\Pi_W$? How should we interpret the term involving $\\mathcal{E}$ in Thm 11 (depending on the answer to the previous question)?\n- There is no Alg D. I think this should be Alg 5.\n\n",
            "summary_of_the_review": "I believe the results are interesting, decently significant, and relevant to the community. That being said, there are issues with clarity (noted above). I believe fixing these issues is an easy way to significantly strengthen the paper.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}