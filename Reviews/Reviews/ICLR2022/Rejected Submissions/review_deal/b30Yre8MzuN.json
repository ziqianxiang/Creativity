{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes a new method for subgraph similarity search by learning embeddings via a GNN-based approach to reflect the edit distance between subgraphs. Reviewers highlighted that the paper proposes an intuitive and promising approach to an interesting problem and provides a good balance between theoretical and empirical results. However, reviewers raised also concerns regarding the significance of technical contributions, limited analysis (e.g, performance on large-scale graphs, baselines, evaluation) and comparison to related work. After author response and discussion, reviewers did not come to a full agreement with two reviewers indicating weak acceptance and two reviewers indicating (weak) reject. Taking rebuttal and discussion into account, I agree with the viewpoint that the paper is not yet ready for acceptance at ICLR as it would require an additional revision to fully address the raised concerns. However, I encourage the authors to revise and resubmit their manuscript based on the feedback from this reviewing round."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper addresses the problem of graph/subgraph similarity search in terms of the edit distance, termed GED/SED, respectively. Unfortunately, both GED and SED are NP hard, with exponential search space, making the cost prohibitive on even moderately large graphs/queries. Instead, the authors apply a neural network based approach, by designing a siamese graph neural network called NEUROSED. In particular, NEUROSED is able to preserve theoretical properties of  SED/GED, such as triangle inequality, through a purposely designed prediction function. Experiments are conducted on real graph datasets for both SED and GED, which evaluates the effectiveness and efficiency of GED/SED.",
            "main_review": "Strong points:\n\n- The paper is well presented and organized. It is easy to follow with adequate examples.\n\n- The ideas are intuitive and technically sound, although alternative or even better solutions could exist.\n\n- There is a good balance of empirical and theoretical results.\n\nWeak points:\n\n- The paper emphasizes a lot on the ability to preserve theoretical properties. But these theoretical properties are direct consequences of the prediction function, and has nothing to do with the rest of the model (e.g. the siamese network, the GNN layers). In theory, any neural network architecture, followed by the same prediction function, would preserve these theoretical properties.\n\n- The prediction function is manually crafted. Why not make it learnable? In theory, an MLP can universally approximate any continuous function in a specified input range--- which could automatically discover the right prediction function, including the proposed handcrafted function. To improve learning, as a prior, some regularization can be added to guide the MLP toward the theoretical properties.\n\n- Some important neural baselines are missing for SED. \n\n[a] Neural Subgraph Isomorphism Counting. KDD2020.\n\n[b] Subgraph Neural Networks. NeurIPS 2020.\n\nBoth are based on graph neural networks. Although they are not for SED, they are for subgraph isomorphism counting, the difference only lies in the loss function. If they are replaced with the same loss on SED, they can be reasonable baselines.\n\n- The pair-independent advantage is not unique to this method (see [a]). Moreover, it may not be that useful. First, it is only applied at inference time, which is typically fast for neural networks. Second, this only applies to cases where query/graph repeatedly appear in different combinations from a fixed pool. In cases where each incoming query/graph is new, the model has to compute their embeddings on the fly anyway. \n\nMinor issue:\n\n- F are sometimes used instead of \\mathcal{F}.\n\n--- update after rebuttal ---\nI see some valid points in the rebuttal, hence upgrading the score.",
            "summary_of_the_review": "Based on the weaknesses stated above, the handcrafted prediction function is the key but there may be better alternatives. Additionally, other subgraph-based GNN can be easily ported over to solve the SED problem.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a simple and effective model for subgraph similarity search. Two GNNs with shared weights are applied first to get the graph embeddings of the query and target graphs. Then, instead of using an MLP over the concatenated graph embeddings to regress the subgraph edit distance (SED) score, the paper proposes to use the l2 norm of the positive portion of the difference between two embeddings. This inductive bias is proved to preserve SED's triangle inequality property, and shows better accuracy and generalization power for large graphs. Experiments show the proposed model outperforms state-of-the-art neural approaches in both graph similarity search and subgraph similarity search tasks.",
            "main_review": "Strengths:\n1. Simple yet effective approach for estimating the SED from two graph embeddings.\n2. The adopted score function Eq. (6) can preserve SED's properties of nonnegativity, subgraph identity, and triangle inequality, which is theoretically proved.\n3. Solid experiments show state-of-the-art performance and orders of reduction in computation time.\n4. The writing is clear. The paper is easy to follow and demonstrates the key insights very well.\n\nWeaknesses:\n1. Equation (6) seems the key technical innovation of the proposed model, which seems a bit simple and appears to be the same one as in Neural Subgraph Matching [1]. Although theoretical analysis is given to show it satisfies three properties in Lemma 1. However, it should not be the only one satisfying the three properties. Then why is Eq. (6) exclusively better? Is it possible to improve (6) further? A discussion of [1] should also be included.\n2. The title of Table 4 is too close to Table 5, making it look like the title of Table 5. \n\n[1] Lou, Zhaoyu, et al. \"Neural Subgraph Matching.\" arXiv preprint arXiv:2007.03092 (2020).",
            "summary_of_the_review": "Considering the strong empirical performance and the theoretical guarantee, I recommend an acceptance. My only concern is the possible limited technical innovation in Eq. (6), which desires more in-depth analysis and discussion. Besides, it already appears in previous papers and a discussion is lacking.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a supervised model, NeuroSED, to compute Subgraph Edit Distance (SED) (and Graph Edit Distance (GED)). To this end, given two graphs, target and query graphs, NeuroSED uses a shared GNN to encourage embeddings for both graphs representing similar topological features. Then a dedicated learning metric w.r.t. SED is introduced to ensure the learned embeddings hold key properties of SED and the embeddings are indexable. The indexable embeddings further support fast prediction. Extensive experiments show NeuroSED significantly improves SOTA methods.",
            "main_review": "Strengths\n- Efficiently computing SED is a vital problem. NeuroSED is a promising model that supports accurate and fast computation. It is a good contribution to the SED field.\n- The learned embeddings hold key properties of SED, such as triangle inequality. These embeddings also support indexing strategies for fast prediction.\n- Extensive experiments show how NeuroSED is a better choice than SOTA neural and non-neural methods. Sufficient ablation studies on each component of NeuroSED give evidence of why NeuroSED works.\n- The paper is fairly well written.\n\nWeaknesses\n- Technique contribution is limited.\n  * Although the paper argues the model is inspired by Siamese networks, it is indeed just using a shared GNN to encode both target and query graphs. In the field of combinatorial optimization, encoding graphs using GNN is straightforward and not new (Cappart et al., 2021).\n  * Besides, NeuroSED is very similar to Neural Subgraph Matching (NeuroMatch, Lou et al., 2020): both models use a shared GNN to encode target and query graphs, the learning metric of NeuroSED is simply the violation penalty in NeuroMatch. NeuroMatch also studies subgraph relations in terms of subgraph embeddings as in this paper. From this regard, it seems like NeuroSED merely adopt NeuroMatch to compute SDE. Clarifications are needed.\n\n- An advantage of neural methods over heuristic methods is the ability to efficiently predict large graphs. However, it is unclear how to extend the model to predict large graphs accurately.\n\n  * The model is supervised, but obtaining training data for SED in practice is nontrivial. NeuroSED has to train on small target and query graph pairs with labels computed from a mixed integer programming (Lerouge et al., 2017), which is at high costs and not scalable to large graphs. As a result, training NeuroSED on large target and query graphs may be infeasible.\n\n  * If NeuroSED trains on small graphs and predicts on large graphs, the prediction performance can be bad due to the extrapolation capacity of GNNs (Xu et al., 2021).\n\n  * The experiments are limited to small target and query graphs with graph diameters at most 10. Showing the performance on large graphs is necessary.\n\n- The constructed training data may have a different distribution than test data distribution in real-world applications. The training data is a collection of (target graph, query graph, label) triples, where the query graph is generated using BFS traversal on target graphs. In practice, the underlying query graph distribution can be unknown and be much different from these BFS traversal samples. Then the training distribution is different than the test distribution, making the trained model not generalized to test data.\n\n- Pair-independent embeddings condense the target/query graph into a single vector seems to be too simple. It pushes much burden on the expressiveness of GNNs. GNNs are forced to summarize the target graph according to a large amount of query graphs. However, GNNs may not be that flexible, e.g., it is difficult for GNNs to identify simple substructures (Chen et al., 2020). It would be better to understand what features are captured in the trained GNN embeddings and ensure the pair-independent embeddings are sufficient for SED.",
            "summary_of_the_review": "I think there is still room to improve the paper. The model seems reasonable, and the experiments show promising results under specific settings. However, the technical novelty, strategies of generating training data, and the ability to generalize the model to large graphs are not well discussed.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The submission proposed to use GNN to encode graphs and calculate graph edit distance and subgraph edit distance based upon supervised learning with the generated graph embeddings. ",
            "main_review": "My minor concerns are:\n\n1). Some notations are not used after definition. For example, what is Ï€? I can only find the definition in the appendix PROOF OF LEMMA 5. This will make the reader hard to follow. Please check all notations and make sure all usage happens after a clear definition.\n\n2). Missing some related work, such as the following one. [1] uses random walk sampling paths and model the graph as a set of paths to measure the distance between graphs.\n[1] Inductive and unsupervised representation learning on graph structured objects\nL Wang, B Zong, Q Ma, W Cheng, J Ni, W Yu, Y Liu, D Song, H Chen, ...\nInternational conference on learning representations, 2020\n\nBesides, the following paper although is cited, it is suggested to discuss the superiority of the method over the work listed below in the introduction or related work section.\n[2] Rex Ying, Andrew Wang, Jiaxuan You, Chengtao Wen, Arquimedes Canedo, Jure Leskovec NEURAL SUBGRAPH MATCHING \n\nMy major concerns are the draft includes too many typos and grammar errors, I listed some. Please fix all.\n typos:\nL(v) and L(e) denotes==>L(v) and L(e) denote\nsatisfies triangle inequality==> satisfies the triangle inequality\nAn edit can be addition or deletion ==>An edit can be the addition or deletion \nthat G1 is subgraph isomorphic==>that G1 is a subgraph isomorphic\nit satisfies triangle inequality==>it satisfies a triangle inequality\nhigh generalisation accuracy==>high generalization accuracy\ndenote a labelled==>denote a labeled",
            "summary_of_the_review": "In general, the idea is novel and interesting. The authors also provide proof of the properties of the proposed metric. Basically, SUBGRAPH search and matching is an important problem. The work well validated the effectiveness to use GNN for this problem. I weakly champion the acceptance. ",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}