{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This is an interesting paper trying to answer how contrastive learning can be used in multi-label classification. The reviewers however had raised several doubts about motivations, novelty, or the impact of contrastive module on final results. For many of them the authors had delivered satisfying responses, but after a long discussion, we decided that the paper needs revision to improve in these aspects. For example, the authors should make it clear whether the image retrieval application from Section 4.4 is the main motivation of the method. If so, what are the competitive approaches to solve such problem? How to measure the performance of such methods? Answers for the above questions are crucial to find the right motivations for the contrastive module used by the authors. \n\nWe hope that the authors will follow the recommendations and resubmit the paper to another top conference."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors introduce a new framework for multi-label classification that leverages supervise contrastive learning. The framework adds an attention mechanism on top of the image encoder. Thanks to that, per-label features can be obtained to perform label-wise contrastive learning. The proposed method (MulCon) is trained in two steps. The first step performs pre-training by optimizing only binary-cross entropy (BCE) loss. In the second step, the model is finetuned using a combination of BCE and proposed by the authors Label-Lebel Contrastive Loss. The experimental study on four datasets shows that the proposed approach achieves the new state-of-the-art results among other methods based on ResNet-101, on most from many performance measures.",
            "main_review": "Strengths:\n+ The organization of this paper is good, and it's easy to read.\n+ The proposed method seems to be an intuitive adaptation of the contrastive learning formula to multi-label classification.\n+ Experimental evaluation proves the attractiveness of the proposed method. ResNet-101-based MulCon beats other baselines on most of the presented measures. I believe that using only ResNet-101 models is a fair way of conducting such a comparison and four datasets are enough.\n+ The appendix seems to consist of enough details to reproduce these results.\n+ Ablation study (including results from discussion), qualitative analysis and pretty t-SNE visualization.\n\nWeaknesses and doubts:\n- The scope of the contribution is a bit limited, it seems to me that the main novelty in the area of multi-label classification is proposed Label-level Contrastive Loss. The rest is pretty much a simple arrangement of known blocks and techniques.\n- The motivation for the usage of contrastive learning is shallow.\nThe major performance boost comes from the attention block, which, as I understand, has almost the same number of parameters as ResNet-101 used as base-encoder. I lack a comment on a number of parameters of the proposed method and baselines in the paper.\n- There is no information about the maximum numbers of epochs for each MulCon training step. I suspect that two-step training of MulCon may take more time than other benchmarks since finetuning with LLCL that use SGD may require a large number of epochs + MulCon has a lot of additional parameters.\n- Is augmentation described in the appendix is also used with R101 + BCE + SCL, and LLEN + BCE + LLCL variants in the ablation study presented in Table 3? It is not clear from the paper. One of the authors' responses suggests that it was not. It is also not clear which other techniques from the finetune phase are necessary for this framework to work.\n\nNits:\n- It's not difficult to guess (based on results), but for clarity, it would be nice to add resolution used for ablation study.\n- Different ways of writing ResNet-101/Resnet101, it would be nice to unify the form.",
            "summary_of_the_review": "The review was written after the rebuttal phase, and authors' responses to other reviewers' comments were taken into account. I find this paper to be a sold work, a simple idea with good empirical evaluation. In my opinion, the authors also did a good job answering the reviewers' comments. However, I still have some other doubts, so I believe that right now, this paper is marginally above the acceptance threshold - I recommend it for acceptance, but I will not be upset if the paper is rejected.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, authors propose a framework for utilizing the contrastive learning to improve the feature distinctiveness for multi-label classification. The main purpose is to address that the direct application of contrastive learning (similar to the single-label classification case) is unhelpful for improving the multi-label performance.",
            "main_review": "Strengths:\n- Built upon the transformer encoder-decoder structure, authors manage to obtain the label-level embedding for further contrastive learning, of which the label-level embedding among one mini-batch construct a natural training set for contrastive constrains;\n- The in-depth investigation have been made to analyze why and how dose contrastive loss help, including a vivid t-sne illustration;\n\nWeaknesses:\nThe idea of conducting the contrastive learning within the mini-batch among the label-level embedding is interesting. However, the major concern is the overall novelty of proposed method.\n- The attention-based label-level embedding is a straightforward application of transformer-like structure, which have been shown in the recent multi-label classification models;\n- Apart from the scenario of training with network, the bag-of-words kind training policy is also quite common in multi-label classification; holding the contrastive learning as the main contribution of the paper is quite limited;\n- The experimental results are still less advanced compared to the recent sota models, such as query2label, TResNet, MlTr etc., which can be referred at paper with codes (https://paperswithcode.com/sota/multi-label-classification-on-ms-coco). It is known that the multi-label performances are sensitive to input size, backbone, and training strategies etc., authors still need to expand their experiments to fully validate the effectiveness of the contrastive learning;\n- Also only two datasets are investigated, what about VOC07/12, Visual Genome? As the proposed framework is highly related to the label-level embedding, it should be verified on different label-set size to investigate its limitations.",
            "summary_of_the_review": "Overall, the idea of utilizing the contrastive loss during the training of multi-label image classification network is appealing, however the limited novelty and experiments are the main concerns. Authors should carefully address above concerns (weaknesses) during their feedback, the final recommendation will be made upon the feedback.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a contrastive learning based method for multi-label classification. In particular, authors propose to learn \"label-level embeddings\" for an image, thereby the multi-label classification problem can be transformed into a single-label one where contrastive learning can be naturally adapted. The proposed label-level embedding network involves self-attention and multi-head attention blocks that learn label-specific embeddings. Results were demonstrated by the experiments on two benchmark datasets (MS COCO and NUS-WIDE).\n\n",
            "main_review": "Multi-label classification is one of the fundamental problems in computer vision and has a large amount of studies in the literature. This paper  addressed the problem by contrastive learning, which has not been explored for multi-label classification. The proposed solution that learns label-level embeddings is intuitive and reasonable, and the results are promising.\n\nThe paper is well organized and written. It also provides sufficient amount of background to understand the proposed method.\n\nHowever, I have two questions for further clarification:\n\n1. The proposed label-level embedding network is considered one of the main contributions in this work. This network receives an image and returns a set of label-level embeddings. The architecture of this network involves attention blocks, similar to visual transformer and its variants. In fact, Query2Label [1] adopts transformer (taking advantage of cross-attention modules as well) and achieves a similar performance level. What is exactly the difference between the proposed design and those works? \n\n[1] Liu et al., Query2Label: A Simple Transformer Way to Multi-Label Classification, arXiv:2107.10834.\n\n2. The proposed method achieved state-of-the-art performance on both MS COCO and NUS-WIDE datasets; however, these two datasets have different characteristics. For example, COCO contains some \"small objects\" (e.g., spoon, cell phone, etc.) and NUS-WIDE has a few \"concepts\" (e.g., map). Image embeddings learned by contrastive embedding and attention mechanism may help recognition of small objects, but why is it helpful for recognizing concepts that are global, and perhaps somewhat vague? Does the performance gain against existing methods come mainly from the attention module?\n  ",
            "summary_of_the_review": "This paper presents new state-of-the-art results on MS COCO and NUS-WIDE for multi-label classification. It leverages contrastive learning to enhance distinctiveness for great performance in multi-label image classification. It is also well organized and written. \n\nThe proposed label-level embedding network is conceptually similar to visual transformer or other attention variants. I am not sure about the novelty at current point. The results were demonstrated on MS COCO and NUS-WIDE only. I am not sure if it is scalable to larger datasets (like open images dataset).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors introduce the contrastive learning into multi-label classification. Specifically, the multi-label classification problem is first decomposed into a series of binary classification problems with label-level features extracted by the attention mechanism. Then, label-wise contrastive learning is performed on these binary classification problems respectively. Comparative experiment shows the proposed approach achieves the new state-of-the-art performance in multi-label image classification.\n\nHowever, the proposed adaption of contrastive learning for multi-label classification is a trivial generalization of existing contrastive learning for single-label classification, since contrastive learning is simply performed on the binary classification problem of each class label. And it is not a new idea to introduce the contrastive learning into multi-label classification. Besides, extracting label-level features via attention mechanism is a well-established technique in many existing works. Thus, if I had not missed something, the contribution of this paper is very limited.\n",
            "main_review": "Strengths:\n1. The paper is written very well and easy to follow.\n2. The proposed approach is technically sound and achieves the new state-of-the-art.\n3. Interesting qualitative analysis for the properties of learned representations.\n\nWeaknesses:\n1. It is an incremental work and the main focus, i.e. the adaptation of contrastive learning for multi-label classification, is a trivial generalization of existing contrastive learning method.\n2. There are some exist works which introduce the idea of contrastive learning into multi-label classification. Please talk about the relationships between these existing works and the approach in this paper. Experimental comparisons with some of these works would be desirable.\n3. According to the results reported in the ablation study (LLEN+BEN+LLCL vs. LLEN+BEN), it seems that the contrastive learning term may not be a good regularization for multi-label learning. While the proposed MulCon trained with a two-step policy has a much better performance, I am really confused about whether the performance gain comes from the contrastive learning term or the improved training procedure, since many additional training tricks are utilized. More analyses are suggested to uncover the true sources of the performance gain.\n\n\nThere are some related works which introduce the idea of contrastive learning into multi-label classification:\n\n[1] Li, C., Liu, C., Duan, L., Gao, P., & Zheng, K. Reconstruction regularized deep metric learning for multi-label image classification. TNNLS, 2019.\n\n[2] Chen, C., Wang, H., Liu, W., Zhao, X., Hu, T., & Chen, G. Two-stage label embedding via neural factorization machine for multi-label classification. AAAI, 2019.\n\n[3] Liu, W., & Tsang, I. W. Large margin metric learning for multi-label prediction. AAAI, 2015.\n",
            "summary_of_the_review": "Considering the limited contributions and unclear sources of the performance gain, I think this paper is marginally below the acceptance threshold.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}