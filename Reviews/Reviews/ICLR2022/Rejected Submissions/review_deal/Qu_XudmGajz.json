{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "In order to evaluate the evidence lower bound (ELBO), VAEs typically use a parametric distribution-based decoder $p(x|z)$. If the data is continuous, one often considers a Gaussian VAE, where the canonical setting is to assume a diagonal covariance matrix $p(x|z) = N(x; \\mu(z), \\sigma^2 \\mathbf{I})$. In this paper, the authors suggest replacing the diagonal covariance matrix with a structured covariance matrix (low-rank + diagonal). As this only amounts to a minor change to a canonical Gaussian VAE, strong empirical results are expected to justify its acceptance. However, the image generation results presented in the paper are not comparable to the state-of-the-art VAE results (e.g., Arash Vahdat, and Jan Kautz. \"NVAE: A Deep Hierarchical Variational Autoencoder.\" Neural Information Processing Systems (NeurIPS), 2020)."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes to include some structure in the observation model of the standard VAE, typically implemented with a normal distribution with diagonal covariance matrix. The authors follow the work in [Monteiro 2020] which include low rank structure structure in the covariance matrix for the observation model in another generative framework.\n",
            "main_review": "Strengths\n- The authors provide an adequate introduction and summary of the literature, and highlight how their proposed method could be plugged into existing models.\n\nWeaknesses:\n- The approach is not radically novel and the paper could improve further by providing all details to allow the reader to fully reproduce their results. This could be included in an extended supplementary section.\n- Another part that could greatly improve the paper is some analysis of the scale of the variables. \n- The paper would greatly benefit of analysis showing how the approach extended into a more complex VAE framework further improve any downstream task. \n- How should the reader interpret the scales of the Lagrangian multiplier and the slack variable in Eq (2)? In section 4.1 these values are reported for (2) datasets. Adding additional analysis/explanation for these variables would benefit the paper and help persuade any reader about the robustness of  the results presented.\n- Figure 2 would also benefit from further explanation to better understand how these results support the claim of the authors.",
            "summary_of_the_review": "\nThe paper provides an extension of the work in [Monteiro et al. 2020] to the case of VAEs. The paper provides adequate references but it would greatly benefit for more downstream experiments and detailed explanation as suggested in the main review.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors aim at improving the canonical VAE model by replacing the standard iid Gaussian likelihood with a multivariate Gaussian with (low-rank + diagonal) covariance.\n\nIn applications to CelebA and a brain MRI dataset from UK Biobank, the authors compare the proposed structured-observation-space VAE with a canonical VAE, showing that the samples from their model have lower Fréchet Inception Distance (FID) scores than both means and samples from the canonical VAE. The authors then evaluate the expressiveness of the representations learned in the observation space by visually evaluating interpolations in the observation space, and images obtained by rescaling the principal components of the observational covariance matrix. Finally, the authors show how their model can be used for interactive editing—i.e., editing a small number of pixels and using the conditional distribution to infer coherent edits in the remainder of the image.",
            "main_review": "The problem the authors tackle in this paper and the proposed solution are interesting. The submission is technically sound and the paper is well written.\n\nHowever, I have some major concerns about the originality of the work and feel that further comparison with other methods would be needed to show improvements over the state-of-the-art methods. Specifically:\n1) this work is closely related to Monteiro et al. (2020), who used a similar model in a segmentation setting. Is the model proposed here the same considered in Monteiro et al? If not, the authors should highlight methodological differences. If yes, the authors should highlight the conceptual contributions of their work.\n2) Dorta et al. (2018) considered a closely-related model, but making a different modeling choice for the covariance matrix. The authors should compare their results with those obtained with the model from Dorta et al (e.g., comparison of FID scores of generated samples).\n\nOther questions / concerns:\n- I find it hard to judge whether the means from the structured-observation-space VAE look visually more realistic than the mean from the canonical VAE. Can the author comment on this? Also, does not \"fixing the covariance diagonal to a small positive scalar\" artifactual reduce independent noise in the samples? I wonder if a visual comparison with canonical VAE is fair in this setting.\n- Are there other potential use cases for the observational model beyond pixel editing?\n- I think another way to improve upon the canonical VAE likelihood is to consider an iid gaussian likelihood on perceptual features [1]. The authors should consider this method in the related method section and potentially include this technique in the comparison.\n\n[1] Hou X, Shen L, Sun K, Qiu G. Deep feature consistent variational autoencoder. In2017 IEEE Winter Conference on Applications of Computer Vision (WACV) 2017 Mar 24 (pp. 1133-1141). IEEE.",
            "summary_of_the_review": "The paper is sound, well written and tackles an interesting problem. However, more work is needed to demonstrate improvement upon state of the art methods (specially, Dorta et al, 2020, and potentially Hou et al, 2017).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In the standard Variational Autoencoder framework the statistics of the decoder output are assumed to be pixel-independent Gaussian which can lead to problems when sampling from the model when covariances are missing. To overcome these limitations, the authors propose to use the network architecture proposed by Monteiro at al. 2020 in the decoder of a variational autoencoder. In the approacah of \nMonteiro at al. 2020, the output distribution is modeled as low-rank multivariate normal. Qualitative results on the CelebA dataset demonstrate that samples from the proposed modified variational autoencoder capture covariances between different parts of the image.",
            "main_review": "More quantitative results are needed to demonstrate possible advantages of the proposed method. Overall the only contrubution of this paper is to transfer the method of Monteiro et al. into the variational autoencder framework. Therefore this paper should clearly demonstrate the benefits of the proposed method over the established practice of using the predicted mean output vector of the decoder.\nFig. 4 a) and b) compare the proposed method with the established practice of using the predicted mean output vector of the decoder. However in Fig. b) the samples from the proposed method shows some unfavourable color shift. Thus the mean vector creates the more realistic image and would still be the preferable option.\n\nMore quantitative results are needed:\n\n- \"Structured uncertainty\": Is the uncertainty calibrated? Please provide experiments on synthetic data, for example sampled from a multivariate distribution with known mean and variance to demonstrate that the framework actually models the uncertainty of the data distribution.\n- Use of constraints in Eq. 1: Is convergence of the model affected? Are the Lagrangian multupliers maximized during optimization? Does this affect the numerical stabiilty during training?\n- How were the dataset-specific slack variables in Eq, 1 chosen? \n\nReferences\n\n[Monteiro at al. 2020] Monteiro, Miguel, et al. \"Stochastic segmentation networks: Modelling spatially correlated aleatoric uncertainty, Stochastic Segmentation Networks: Modelling Spatially Correlated Aleatoric Uncertainty, Neurips 2020\n",
            "summary_of_the_review": "The contribution of this paper is limited, it merely takes an existing method to capture covariances of model outputs and incoroporates it into the decoder of a variational autoencoder. More quantitative results are needed to demonstrate possible advantages of the proposed method and calibration of the estimated covariances. Further details on parameter optimization and convergence are needed.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to use a low-rank plus diagonal covariance matrix, rather than the usual diagonal ones, in the decoder of Gaussian VAEs. The authors empirically show the advantages of using more expressive covariance matrices in the decoder.",
            "main_review": "This paper is well-written, easy to follow, and well motivated. I agree with the authors that having more flexible decoder distributions, and not just more flexible neural networks (mapping to parameters of simple distributions) is an understudied area in VAEs that deserves work. The paper is very simple and relies mostly on empirical results, but I do think that the authors properly show improvements and added benefits over using diagonal covariance matrices.\n\nOne concern I have though is that it seems like adding this structure to the covariance matrix come with its share of training instabilities. The authors need to add regularizers (eq. 2), and some of the parameters used look incredibly specific (e.g. $\\xi_H=-504750$ and $\\xi_H=-199250$ for different datasets). How were the values of $\\xi_H$ chosen? How robust are the results to changes in this hyperparameter? I think a careful answer to these questions should be presented, at least in the appendix to strengthen the paper.\n\nAlso, while I think the experiments do suggest improvements over diagonal covariance matrices, I think there are relevant additional experiments that are left out. Firstly, as the authors correctly point out, they are not the first to use a structured covariance matrix in the decoder of a Gaussian VAE [1], although this previous work uses sparse covariance matrices rather than low-rank plus diagonal ones. The absence of comparisons against this other structure significantly weakens the paper in my opinion: the baseline should not be diagonal covariance, it should be sparse covariances based on pixel neighbouring structure (as in [1]); and the reader should be convinced that low-rank plus diagonal is a more sensible choice than sparse. In other words, the authors claim \"([1]) limits its ability to capture long-range spatial dependencies\", and they should empirically verify that this actually hurts performance. Some more minor experimental things that I believe are also left out are: The authors should report \"Our VAE (means)\" in Table 1 as well. I imagine these results would be worse that the standard VAE's given for example the grey backgrounds in CELEBA, but I think this is actually interesting! It means the decoder is actually being used as a distribution in the generative model. I also believe that log-likelihoods (or ELBOs) should be reported in Table 1.\n\nI will consider increasing my score if my main issues (hyperparameter instability, and comparisons against [1]) are adequately addressed during the rebuttal period.\n\nMinor things:\n\n-Citation links seem to be formatted in a different way than other papers I reviewed (they appear in blue in other papers).\n\n-The notation S = H x W seems a bit weird: why collapse only height and width, but not channels C into a single variable? That is, S could be defined as S = H x W x C and notation would be simplified.\n\n-Above eq.3, $t \\in [0..1]$ should be $t \\in [0,1]$ I believe.\n\n-The slerp function in eq.3 should be explicitly written down, at least in the appendix.\n\n[1] Structured uncertainty prediction networks, Dorta et al. 2018\n\n========================================================================================================\n\nUPDATE 1 AFTER REBITTAL\n\n========================================================================================================\n\nI have read the author's rebuttal, and while I understand that adding the comparisons against Dorta et al. is a significant amount of work, I am not willing to increase my score until definitive comparisons have been done, even if agree with the authors that the method of Dorta should not add large-scale noise, as ELBO/FID comparisons could remain interesting.\n\n========================================================================================================\n\nUPDATE 2 AFTER REBITTAL\n\n========================================================================================================\n\nI have read the updated author's rebuttal with comparisons against Dorta et al.'s method. I decided to increase my score, and still want to encourage the authors to include ELBO comparisons in an updated version of the manuscript.",
            "summary_of_the_review": "While I think this paper proposes a sensible idea and shows improvements over using diagonal covariances in Gaussian VAE decoders, I have concerns about the stability of the proposed method; and I believe that the authors should compare against a stronger baseline than the one they use for the experiments to be fully convincing.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}