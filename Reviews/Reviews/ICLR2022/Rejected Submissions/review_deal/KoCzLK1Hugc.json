{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "In this paper, authors study adversarial robustness against the union of Lp-threat models. Reviewers had some concerns about this work. They mentioned the paper is not well-organized and the explanations of the novel components should be clearer.  In particular, they suggested authors to study the effects of different components of E-AT and motivate their combinations with fine-tuning. The lack of novelty was another concern. I suggest authors to focus on the fine-tuning part in their revised draft which has more novelty. Given all, I think the paper needs a bit more work before being accepted."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper addresses the problem of multiple perturbation adversarial robustness for attacks subsumed within $\\ell_p$ regions for $p\\in{1,2,\\infty}$. The main contribution of this work is to show how a model robust to a particular attack type (typically $\\ell_\\infty$) can be fine-tuned (at low cost) to be robust against multiple (or alternate) perturbation types. The authors build on prior formalization about the geometry of $\\ell_p$ balls (by Croce et. al.) to empirically demonstrate its effect. The results are convincing and evaluated against AutoAttack which is",
            "main_review": "## Strengths\n1. This paper makes a very strong contribution to the sub-field of multiple norm adversarial robustness by showing that one can achieve this goal by spending a small budget on any pre-trained robust model.\n2. I like the graphs in Figure 4 of the Appendix because they highlight how prior methods naturally do extreme norm sampling in the given setting of CIFAR-10. However, as a word of caution here -- in my understanding, you will not see the same trend in the case of MNIST -- this makes me speculate the performance of EAT on MNIST.\n3. The overall paper is written very comprehensively, and the authors have done an extensive study of various pre-trained models. The authors have already included most experiments that I would have been curious to know as a reader -- such as fine-tuning standard models, using EAT from scratch, performance on Imagenet, comparison with other prior works. I am very satisfied by the presentation.\n\n## Suggestions/ Weaknesses\n1. Can you also see the same results on MNIST? I suspect this is hard because the $\\ell_\\infty$ model is not robust to $\\ell_2$ attacks in MNIST. I believe that CIFAR-10 is just acting as a nice test bench for this kind of work because the problem was biased against $\\ell_2$ attacks (because of how perturbation budgets have been historically chosen in literature). There is no reason why the perturbation budget of interest should be 0.5, especially when you consider that the robustness of $\\ell_2$ models against $\\ell_2$ attacks is significantly more than that of any other $\\ell_p$ robust model against $\\ell_p$ attacks.\n2. Comparison with CCAT or other methods such as PAT would be helpful. Can you also fine-tune models beyond $\\ell_p$ robustness -- to say that this is a general property of adversarial robustness and not particularly hinged on geometry?\n\n\n## Questions\n1. Are you using AutoAttack (plus) or standard? I would suggest using the plus version to be comparable to other works.\n2. How is EAT-unif different from AVG (if $\\ell_2$ was not considered)? Noting that in most cases unif and biased sampling have nearly the same effect.\n3. Do we really need to use EAT to fine-tune models? What about using MAX or MSD as a fine-tuning step? Does this perform better? Since in most results where EAT is independently used, the performance is sub-optimal. I wonder if there is any reason why EAT should be optimal for fine-tuning or if it needed to be introduced in the first place?\n4. What is the impact of the additional data versus running 6 epochs of fine-tuning?\n\n## Post-author response\nI am voting for acceptance of this work. However, I also understand why the biggest contribution of this paper might be hard to grasp for the average reader. The authors spend too much time and paper space to highlight how EAT is novel. I do not think that is the case (it is a heuristic modification that works in practice!), nor do I think this paper needs that to be accepted. The remaining contribution is significant enough for me to vote for acceptance -- and in fact one of the most significant developments in multi robustness after the initial few papers. I would encourage the authors to spend more time to re-evaluate what is the most significant contribution in their opinion, and possibly rethink how to go about the prose.\n\n",
            "summary_of_the_review": "The paper has a very strong contribution to the field of multi-norm adversarial robustness. However, I am concerned if this phenomenon generalizes to other datasets -- based on some observations noted by the authors. I would be happy to raise my scores if the authors can show results on MNIST as well.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a method to produce image classifiers which are adversarially robust against multiple $\\ell_p$ threat models—in particular, against $\\ell_1$, $\\ell_2$, and $\\ell_\\infty$ attacks. The method involves training against $\\ell_1$ and $\\ell_\\infty$ attacks with the hypothesis that this will additionally give robustness for $\\ell_p$ threat models with $1 \\leq p \\leq \\infty$. This hypothesis is supported by prior results that proved that affine classifiers robust to $\\ell_1$ and $\\ell_\\infty$ threat models are also be robust to other $\\ell_p$ threat models. The authors test their method on CIFAR-10 and ImageNet for both training classifiers from scratch and for fine-tuning robust models trained on one $\\ell_p$ threat model to the other $\\ell_p$ threat models.",
            "main_review": "This paper is generally well-written. The motivation for the problem and the E-AT method are clear, and it seems like the experiments were carefully run.\n\nStill, I have some concerns about whether the results support the proposed method. It seems like the main contribution proposed by the authors is the idea of training against only $\\ell_1$ and $\\ell_\\infty$ attacks to get robustness to $\\ell_p$ attacks for other $p$ values. However, when selecting between $\\ell_1$ and $\\ell_\\infty$ attacks randomly, the method performs nearly identically to (or maybe a bit worse than) SAT which randomly selects between $\\ell_1$, $\\ell_2$, and $\\ell_\\infty$ attacks at each iteration. The proposed method is only superior to SAT when using the proposed biased sampling scheme. Thus, it seems like it is actually the biased sampling scheme—not the use of $\\ell_1$ and $\\ell_\\infty$ threat models—that leads to the improvement. As far as I can tell, none of the other experiments in the paper clearly separate the effect of these two changes to SAT-type methods.\n\nIn general, the paper seems like a collection of three separate ideas:\n\n 1. restricting adversarial training against multiple $\\ell_p$ norms to only $\\ell_1$ and $\\ell_\\infty$, which is the subject of the theory part of the paper and the motivation for the name E-AT,\n 2. the biased sampling scheme,\n 3. and the idea of using fine-tuning to quickly impart robustness against a different $\\ell_q$ threat model to a model trained with an $\\ell_p$ threat model for $q \\neq p$.\n\nAs I mentioned above, the effects of 1 and 2 are currently difficult to disentangle, and 3 seems somewhat orthogonal to the other two contributions. In particular, in Table 4 it seems that adversarial fine-tuning (3) is effective even without 1 and 2. The three ideas are mostly just connected by the common problem they aim to solve—producing robustness against different and multiple $\\ell_p$ norms. Even the title of the submission seems split between the different ideas. Thus, it would be helpful if the authors could better motivate the combination of E-AT and fine-tuning in the same paper, or even consider splitting the paper into two submissions.\n",
            "summary_of_the_review": "Overall, the paper could use some more work to separate the effects of the components of E-AT and motivate the combination of it with the fine-tuning contribution, which seems orthogonal. Thus, I do not recommend acceptance, although I am open to raising my score during the discussion period.\n\n**After discussion period:** My overall position is that I would be more supportive of accepting the paper if it focused on fine-tuning over E-AT. Currently, the extensive focus on E-AT detracts from what seems like the more important contribution and could lead that contribution to be overlooked. However, I believe changing the focus would require a major revision and thus I maintain my weak reject score. Nonetheless, I respect that some of the other reviewers have different opinions on whether the paper should be published and I hope the AC weighs all our points to make the final decision.\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper mainly studies the problem of defending multiple norm adversarial perturbations. The authors propose extreme norms adversarial training (E-AT), which leverages different geometry of the $\\ell_{p}$-balls to conduct adversarial training by adaptively alternating between the $\\ell_{1}$ norm and $\\ell_{\\infty}$ norm. They also show that using E-AT fine-tune could turn $\\ell_{p}$ robust model into a model that is robust against the union of $\\ell_{p}$ adversarial perturbations. The authors also provide some theoretical proof for their method. ",
            "main_review": "Strengths:\n\n(1) The proposed E-AT method is computationally affordable since it doesn’t use all types $\\ell_{p}$ adversarial examples to do training, it only alternates between the two extreme norms. \n\n(2) E-AT could quickly fine-tune models that are only robust against one perturbation type into models that are robust against the union of multiple $\\ell_{p}$ adversarial perturbations, and the fine-tune results are shown better than simple $\\ell_{p}$ norm fine-tuning. \n\nWeaknesses:\n\n(1) Although the paper demonstrates its motivation in Sec 3.1, the reviewer is still confused about the geometry theory proposed to defend multiple $\\ell_{p}$ perturbations. It’s indeed correct for a simple one-layer linear classifier that model being robust in both $\\ell_{1}$ and $\\ell_{\\infty}$-ball is also robust w.r.t the largest $\\ell_{p}$-ball that fits into the convex hull of the union of the $\\ell_{1}$ and $\\ell_{\\infty}$-ball. However, when it comes to the deep neural network which is in the high-dimensional manifold, there is no guarantee for the above statement since there is no such model that is absolutely robust (i.e., robust accuracy is 100%) against certain norm types on CIFAR-10 or ImageNet dataset. Being partly robust on $\\ell_{1}$ and $\\ell_{\\infty}$ (e.g., robust accuracy is 65.88%) doesn’t mean being robust within the $\\ell_{p}$-ball that fits into the convex hull of the union of the $\\ell_{1}$ and $\\ell_{\\infty}$-ball. There should be more proof about the certified robustness of the proposed method. \n\n(2) In Fig 1, the paper shows the model adversarial robustness against multiple norm perturbations after E-AT fine-tuning is better than L1 fine-tuning. It may be unfair since the evaluation metric for union robustness is the sample-wise worst-case accuracy, and [Ref1] demonstrates that $\\ell_{1}$ and $\\ell_{\\infty}$ robustness are mutually exclusive so fine-tuning on $\\ell_{1}$ perturbation will largely decrease the robustness on $\\ell_{\\infty}$ perturbation, causing the bad results under the worst-case evaluation metric. More fine-tuning is needed for the baseline methods to balance the robust accuracy on all norm types. \n\nReferences\n\n[Ref1] Tramer F, Boneh D. Adversarial Training and Robustness for Multiple Perturbations[J]. Advances in Neural Information Processing Systems, 2019, 32: 5866-5876.\n",
            "summary_of_the_review": "The motivation and the theoretical effect of the proposed defense should be clarified and demonstrated, and some experimental comparisons need to be improved. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper tackles the problem of robustness against multiple perturbations and proposes extreme norms adversarial training (E-AT) that adaptively alternates between $\\ell_1$ and $\\ell_\\infty$-norm. Furthermore, the paper fine-tunes Gowal et al. (2020) to improve its multi-norm robustness. Finally, the experiments are conducted on CIFAR-10 and ImageNet with APGD for training, showing the proposed method's effectiveness.",
            "main_review": "### Strengths\nThe paper tackles an important problem of training models robust against multiple attacks efficiently. The overall paper is well written and easy to follow. The proposed evaluation compares state-of-the-art single and multi-perturbation robustness methods, and the conducted experiments on ImageNet are impressive.\n\n---\n\n### Weaknesses\n- The paper is limited in novelty. While the fine-tuning aspect is novel, but the similarity between the $\\ell_1$ and $\\ell_2$ perturbation was highlighted previously in Maini et al. (2020) [1], where it was shown that the first two principal components of $\\ell_1$ and $\\ell_2$ adversaries are largely overlapping. Therefore, I would recommend including a discussion about their conducted analysis to clarify the contributions of the proposed method. \n- Further, E-AT is simply a modification of SAT, where E-AT uses the proposed adaptive sampling instead of uniform sampling.  The gains by the proposed method are marginal in robustness and training time compared to SAT. While E-AT gains marginal improvement in union accuracy, SAT obtains higher clean accuracy. Therefore, the effectiveness of E-AT is limited from these experimental evaluations.\n- The theoretical and empirical analysis is restrictive to $\\ell_p$-norm.  The scalability to non-$\\ell_p$ attacks (e.g., spatial attacks, patch attacks, common corruptions, unforeseen adversaries) is unclear. While $\\ell_p$ attacks are a standard in the community, it is essential to show the evaluation to unseen non-$\\ell_p$ attacks to demonstrate the success of the proposed method in practical scenarios.\n\n***Other questions and comments*** \n- E-AT obtains lower accuracy on clean examples and across all the $\\ell_p$-norms in Table 1 compared to the other baselines trained with multiple attacks. Can the authors also report the average metric following Tramèr et al. (2020) and Madaan et al. (2021) to show if E-AT effectively mitigates the robustness tradeoff between multiple attacks compared to prior works?\n- How is the performance for fine-tuning on CIFAR-10 with a single epoch? \n- How does the performance vary on increasing the fine-tuning epochs further in Table 8? Can the authors also report the results for ImageNet?\n- The performance difference between uniform and biased sampling in E-AT is marginal (Table 5 and Table 7). Comment.\n- Adding the numbers for all the bars would enhance the readability of Figure 1.\n\n--- \n### Post Rebuttal\n\nThank you for the response and the additional experimental evaluation conducted during the rebuttal. In light of the clarifications and additional evaluations, I have increased my score to 6.\n\n---\n\n### References:\n[1] Maini et al. (2020). Perturbation Type Categorization for Multiple  Bounded Adversarial Robustness.\n",
            "summary_of_the_review": "I think this paper tackles an important problem and conducts an exhaustive experimental evaluation. However, I would like to clarify the points raised in my review and would be happy to raise my score if the authors can successfully address my concerns in the discussion period.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}