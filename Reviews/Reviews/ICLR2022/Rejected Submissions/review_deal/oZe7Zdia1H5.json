{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "### Summary\n\nThis work demonstrates that it is possible to identify lottery tickets with some manner of structural sparsity. The work finds success through refilling (perhaps better termed infilling) and regrouping, two techniques that have found previous homes in other parts of the literature. \n\n### Discussion\n\n#### Strengths\n\n- Tackles an interesting problem. \n\n- At face value, I believe the paper achieves its claimed goal, though not with full clarity as written.\n\n#### Weaknesses\n\n- There is room to clarify the atypical form of structure here for readers. The suggested title change would appropriately set expectations. However, refinements in the text as well would be welcomed.\n\n- As I discuss below, the claims should be settled with respect to the strongest baselines.\n\n- Random reinitialization should play a primary role in the presentation of the text. As the authors note, the original lottery ticket paper did not require besting the performance of random reinitialization. However, the random reinitialization results are central to the main figures of the original paper and demonstrate that the result is not merely happenstance. This paper should follow that practice.\n\n### Recommendation\n\nI recommend Reject and I do not do so lightly, given the scores. The work here is promising because finding a path to better-performing lottery tickets remains an open challenge. However, Reviewer YWUh has voiced reasonable concerns about the evaluation methodology. \n\nI've read the detailed authors' responses and agree with the authors that the presented results may depend on choices in the hyperparameters and training strategy. Having said that, it is critically important for the paper to include in the primary text the strongest baselines available, despite this dependence. On such baselines, the results are worse than originally reported and hence, the primary claims must either be revised to be these results or revised to include these results, with the primary claims providing a range of results. Though the authors offer to make revisions for the camera-ready, the required revisions here are substantial enough to require additional review, which is out of the scope of the current process.\n\nAn additional oversight in this methodology -- at least as reported -- is that the primary target of the lottery ticket hypothesis is sparse training, not sparse inference. Evaluating solely the inference performance -- rather than training performance, which includes both the forward and backward pass -- is, therefore, inconsistent with the purpose of lottery tickets. This methodological error will too need to be repaired in the final version of this work."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This work proposes a method to effectively find structurally sparse winning tickets. It consists of some post-processing techniques that can be added to each round of standard iterative magnitude pruning (IMP) methods. Starting from unstructured sparse sub-networks, the method uses a \"re-filling and re-grouping\" manner to enforce the formation of structural sparsity.",
            "main_review": "# Strength:\n\nThe structural sparsity and unstructured sparsity are not new. However, in previous works about the lottery ticket hypothesis (LTH), the winning tickets are typically unstructured. Therefore, the idea of finding structural winning tickets is meaningful, and the existence of structural winning tickets fills in a crucial piece of the puzzle for the LTH. In this work, the authors propose a method to find structural winning tickets. Extensive experiments on various datasets and network structures show the method is effective and efficient.\n\n# Weakness:\n\n1. In some experiments (such as Figs. 3 and 5), the authors report time-saving when using a specific GPU and batch size. The time saving might vary when other GPUs or batch sizes are used. Therefore, I believe it would be better to also include the number of FLOPs in the results.\n2. IMP-Refill and IMP-Refill+ generate different results. Why only IMP-Refill is included in Fig. 8?\n3. In Figs. 3, 4, and 5, it seems different methods stop at different percentages of sparsity. Why don't they all prune to the same percentage?",
            "summary_of_the_review": "This work demonstrate that lottery tickets can not only be unstructured but also be structural, which makes it a novel and interesting work. There are minor issues in experiments.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Nothing.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents an exciting new finding – by properly reorganizing elements during the IMP process, it’s possible to achieve structurally sparse winning tickets at high sparsity levels that can be easily hardware accelerated.",
            "main_review": "The authors explored hardware-friendly structural sparsity (including channel-wise and group-wise patterns) to find lottery tickets. They propose new refilling techniques based on the classical iterative unstructured pruning process to create channel-wise structural sparsity by growing back the pruned elements within the most important channels and abandoning the rest. \n\nOn top of that, they introduce another regrouping algorithm based on hypergraph partitioning (Rumi et al., 2020) to establish group-wise structural patterns which are more amenable to pruning due to the shape flexibility of grouped blocks. \n\nThe authors reported many experiments on CIFAR-10, CIFAR-100, TinyImageNet, and ImageNet, using various backbones like ResNets, VGGs, and MobileNets. Overall, the structural winning tickets are shown to achieve 53.75% ∼ 64.93% GPU running time savings at 45% ∼ 80% channel- or group-wise sparsity. \n\nOne should note that the sparsity level discussed in this work is not as the usual high as in unstructured tickets (often >95%), but the hardware acceleration is way more apparent. The ablation studies and visualization are thorough. \n\nI have just a few clarifications or curiosity questions:\n- It seems the acceleration effect on the structural ticket (especially after regrouping) requires some specific hardware accelerator and efficient implementations such as GEMM. It is then unclear whether the comparison with vanilla LTH is fair or not: could unstructured tickets also benefit from those? How much performance gain of the reported numbers comes from hardware specialization (Rumi 2020)?\n- Whether the same techniques can be applied to lottery tickets found not from random initialization, e.g., how about the pre-trained model lottery tickets as in (Chen et al., 2020b)? Will the refilling/regrouping rely on random weight initialization anyhow?\n- How much is the extra overhead of refilling/regrouping compared to the IMP cost?",
            "summary_of_the_review": "The paper is in a good shape that displays the first set of positive results bridging the gap between the lottery ticket hypothesis and practical accelerations on real-world hardware. Algorithm details, as well as results, are sufficient and convincing.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "LTH (lottery ticket hypothesis) was mainly proposed for unstructured pruning. This paper shows it can also be validated on structured pruning, for the first time. The key for them to achieve so is the newly proposed post-processing techniques, refilling(+) and regrouping. They show by these techniques, structural winning tickets can be found, with up to 6.67x on hardware platforms.",
            "main_review": "Strengths\n\n●\tUp to date, LTH has not been shown valid on structured pruning (I mean, filter pruning or channel pruning). This paper is meant to bridge this gap, which could be a significant step forward.\n\n●\tThey propose several post-processing techniques, which deliver practical speedup. Potentially, they can be very useful.\n\nWeaknesses\n\n1.\tOne contribution this paper keeps emphasizing is that they find structural winning tickets for the first time. However, some important concepts, baseline results are either not established or (potentially) flawed. I do not think the presented results can faithfully support their claimed contribution.\n\nFirst, one important property of LTH is that, if the subnet is trained using randomly re-initialized weights instead of the winning tickets, the subnet cannot achieve full accuracy. This paper does not have this kind of comparison at all. For example, Fig. 3, for channel sparsity (refill and refill+, not regroup), what are the accuracies when you train the pruned network from scratch (I mean, fully randomly reinit all the weights in the pruned network)? Without this comparison, we cannot tell if the initial weights can be identified as “winning tickets” in the first place.\n\nSecond, the presented results are (potentially) flawed. Many baseline accuracies are far below the average reported by other papers. E.g., in Fig. 3, ResNet50 on TinyImageNet, others typically can get ~67% accuracy (see [1*]), while this paper only got a little bit over 53% at sparsity 0. More than 10% gap.\n\nSimilar issue: Fig 3, ResNet50 on ImageNet. The typical top-1 accuracy is around 76.1% (see torchvision models), while this paper reports 75.2%. Nearly 1 point top-1 accuracy drop on ImageNet is quite big. Fig 4, their WRN-32-2 on CIFAR100 only achieves 72%, while even WRN-16-2 (much smaller than WRN-32-2) can achieve 73.26% (see [2*]). Fig 5, their VGG16 on CIFAR100 only archives less than 72%, while VGG13 can achieve over 74% (see [2*]).\n\nI thus deeply doubt the validity of these experiments -- Note, the accuracy of dense models is important in validating LTH because it is the baseline accuracy that subnets are going to compare against. Using a lower baseline accuracy, of course it would be easy to achieve high sparsity “without performance drop”. \n\n-- I think this is also why in many plots, the accuracy of pruned models can strangely surpass their dense counterparts even at a large sparsity.\n\n2.\tThey do not really achieve what they claimed.\n\nIn pruning literature, structured pruning typically narrowly means filter or channel pruning,  while this paper actually also refers to a more fine-grained sparsity structure (see the “Group-wise Structural Sparse Mask” in Fig. 2). This actually make it much less interesting, because this kind of sparsity still needs customized implementation (“a more sophisticated GEMM-based efficient implementation (Rumi et al., 2020) is needed to accelerate better our refilled / regrouped structural patterns”). How to get practical speedup is actually unknown from this paper (they defer many important details to (Rumi et al., 2020)). \n\nThen, only look at the channel sparsity. Fig. 3, ResNe50 on ImageNet, Refill and Refill+ start to drop accuracy even at a marginal sparsity. This means they do not find winning tickets. They also admitted this in the paper: “There is almost no difference between the performance of IMP-Refill and IMP-Refill+, and both can not find channel-wise structural winning tickets”. \n\nWhat attracted me at the first sight of this paper is that they validate LTH on filter/channel pruning. Unfortunately, they do not really achieve what they claimed. \n\n3.\tFrom the methodology perspective, the proposed techniques are “post-processing”. Most structured pruning methods are independent from unstructured pruning. While here, they blend them together with “post-processing” instead of proposing a really independent technique to find winning tickets for filter/channel sparsity. I would say, it doesn't look like an elegant neat scheme (although “elegant” may not be really important).\n\n4.\tMinor issue. Presentation: Fig. 3, radar plots. It is pretty hard to read the radar plots. In Particular, some dots are overlapping and pretty small. They are barely recognizable in print. They can be presented in a better way.\n\n[1*] https://openaccess.thecvf.com/content_CVPR_2020/papers/Yuan_Revisiting_Knowledge_Distillation_via_Label_Smoothing_Regularization_CVPR_2020_paper.pdf\n\n[2*] https://arxiv.org/pdf/1910.10699.pdf, ICLR 2020\n",
            "summary_of_the_review": "Important baseline results are missing. Experiments are potentially flawed. They claimed what is not really realized in the paper.\n\n",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This work demonstrates the existence of structural winning tickets for the first time by leveraging post-processing techniques.",
            "main_review": "It is well known that lottery tickets so far are only successfully found with unstructured pruning. This paper challenges this commonsense, and its positive finding could be of broad interest and impact. Instead of directly adopting structural pruning, the authors propose the postprocessing techniques to accompany IMP, namely, the refilling/regrouping algorithm to form channel-wise and group-wise structural sparsity, respectively. These algorithms are simple and intuitive, yet novel and convincing.  The paper for the first time achieves high sparsity (up to 80%) with structure in the lottery ticket literature and shows practical GPU hardware runtime savings at around 50-60%.\n\nI suggest the authors to tone down their claims. There definitely exist similar works in relevant problem domains, such as sparse training with structures: for example, “Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch” ICLR 2021, should be discussed and compared. \nBesides the scientific interest, I am not sure if I understand from a practical point of view, why we really need particularly pursue a structurally sparse lottery ticket, in contrast to directly using a structurally pruned NN. \nAlso, how this work compares to using NAS to search for channel numbers?\nI don't understand for ImageNet, why the current results can show \"our picking rule (i.e., channel weights l1 norm) provides a great estimation for channel importance\"\nNo theoretical insight on why the proposed algorithm can work is provided. But for an empirical pilot study, this is fair and acceptable.\n",
            "summary_of_the_review": "The contribution of this paper is solid. I hope the authors could clarify the weak points.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}