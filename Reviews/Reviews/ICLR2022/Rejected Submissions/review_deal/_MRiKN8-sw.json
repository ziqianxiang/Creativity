{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper presents a study of methods for tabular data imputation. In particular, the authors compare deep learning methods with k-NN based approaches. Experiment results demonstrate k-NN to be competitive with deep learning methods.\n\nReviewers have concerns about the characteristics of datasets used in the experiments and hyperparameters used for evaluation, which I agree with. They also think that the main contribution of comparing k-NN and deep learning methods is not strong enough for acceptance to ICLR. I recommend rejecting this paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper considers the problem of tabular data imputation to handle missing numeric data. It compares the RMSE of recently proposed GAN-based methods (GAIN and mis-GAN) with the classical KNN method (either through uniform or distance-weighted imputation). It considers 9 datasets, two are simulated by a mixture of Gaussians and 7 are real datasets, 5 of them used in the GAIN paper. The datasets have 500-30000 samples and 5-57 features (most are below 20). It also considers three different mechanisms of missing data (MCAR - completely random, MAR - random based on the observed data, MNAR - based on unobserved data), each with one or two rates of missing data (more rates are tested for MCAR with the simulated data). The hyperparameters of all the models are tuned once on the simulated data (1000 samples), and scaled based on the number of samples for each dataset (scaling the number of neighbors for KNN, or epochs for the GANs). The models are applied to each imputation case 20 times, presenting the mean and standard deviation for each dataset in each scenario. The paper shows that mis-GAN, originally developed for filling missing image pixels, usually performs much worse than KNN. Furthermore, KNN generally performs roughly on-par compared to GAIN, depending on the dataset and exact scenario (in many cases KNN is better). Based on the presented figures, the RMSE differences are usually within 0.02, but may reach 0.05. The compute required by KNN is much lower, and the paper calls for using the cheaper option, avoiding waste of resources.",
            "main_review": "This paper considers the problem of data imputation, which is important in handling tabular data, and shows interesting results regarding imputation methods, with some potential practical implications regarding the efficient methods to use. However, there are some concerns regarding the presented results:\n- The authors state that they did not manage to reproduce the results of GAIN from the original paper, although they used the same datasets (in one case it was much better in their measurement, in the others worse). They also do not provide reference results for any of the other classical algorithms that appeared in the original paper (such as MICE and MissForest). It is therefore unclear whether their setting was correct and whether this comparison was valid. \n- Since datasets behave differently, it is unclear whether setting the hyperparameters based on one small dataset is the right approach. Note that GAIN is better than KNN on this dataset (my_data2), possibly due to some over-fitting, but not as good on the other datasets. It may be better to set the hyperparameters based on several datasets with different mechanisms of missing data. Maybe there could also be some fine-tuning on a subset of the relevant dataset (although it has missing data), rather than assuming a linear scaling based on the number of samples. Also, it should be noted that more hyperparameter options were checked for KNN than for GAIN.\n- The difference between the algorithms is shown in figures, but it is not quantified. The paper should state what is the average difference between GAIN and each of the KNN methods, per dataset, per scenario, and overall.\n- The paper uses a small number of datasets, and all the datasets used have a small number of features. KNN is known to suffer from the \"curse of dimensionality\". It would therefore be interesting to see if this result holds for a couple of datasets with hundreds of features or more. It would also be interesting to see the results for a dataset with a larger number of samples.\n\n",
            "summary_of_the_review": "The paper considers an interesting problem and presents interesting results that encourage trying KNN. However, due to the concerns mentioned above regarding the correctness and representativeness of the results, and since it does not have a significant technical contribution other than comparing existing methods, it is below the bar for ICLR.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper compares two generative deep learning based data imputation methods (GAIN and MisGAIN) with KNN based imputation. The authors conducted experiments on seven real-world and two synthetic datasets in  all missingness scenarios (MCAR, MAR, MNAR) and multiple missingness rates.",
            "main_review": "**List strong and weak points of the paper. Be as comprehensive as possible.**\n\n**Pros:**\n\n* All missingness scenarios (MCAR, MAR, MNAR) are taken into account.\n* The (real-world) datasets' shapes range from 12 to 57 attributes and 569 to 39644 examples and, therefore, cover a wide range of scenarios.\n* All datasets are fully-observed and the missing values are artificially induced. This helps to eliminate interfering effects.\n* Structure of the paper is clear\n\n**Cons:**\n\n* The paper claims to be \"the first to compare state-of-the-art deep-learning models with the well-established KNN algorithm\", which seems like a bit of a bold statement given the number of publications on empirical work in that space in the past years:\n  * papers that present imputation methods sometimes use KNN as baseline (see: <https://arxiv.org/pdf/1808.01684.pdf>)\n  * there is a number of papers at a recent ICML workshop (see <https://artemiss-workshop.github.io/>), where several papers presented empirical work on KNN imputations as well as more recent generative deep learning approaches\n  * there there is recent benchmark that compares both: <https://www.frontiersin.org/articles/10.3389/fdata.2021.693674/full>\n  * there is a recent paper comparing deep learning approaches with AutoML imputations, and KNN is one of the models used in that AutoML solution <https://arxiv.org/abs/2106.11189> - interestingly that publication finds that if regularized properly, deep nets can actually outperform more classical discriminative methods (such as KNN)\n* The algorithms hyperparameters are optimized on a synthetic dataset and then “scaled” to the dataset’s size. Hyperparameters should always be tuned on the same that that the models are applied to. In the very setting investigated by the authors, it is fair to assume that KNN, with fewer hyperparameters, has an advantage due to that procedure\n* an important experimental setting is whether the imputation model was trained on fully observed data (a relevant but rather academic/lab-conditions setting) or on data with missing values (which is more relevant in real-world applications). \n* Minor:\n  * Section 7.2 abbreviation \"CNN\" should be introduced\n\n\n**Clearly state your recommendation (accept or reject) with one or two key reasons for this choice.**\n\nI vote for reject since there are already comparisons between KNN and state-of-the-art models. Further, the hyperparameter optimization can explain the better performance of KNN. \n\n**Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment.**\n\n* Although, the method used to induce MAR missingness is clear to me, I do not understand the column “MAR column” in Table 1.\n* Is there a good reason why experiment 1+2 differ from the third and fourth?\n\n**Provide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment.**\n\n* Given an overview of the datasets' attributes data types would help to get more insights. Perhaps you should consider to take them into account for the discussion.\n* Add information about the data split or cross-validation used to find hyperparameters would increase the credibility.\n* Figures 3+4 and 5+6 could be side-by-side with the same y-axis; in my opinion, figures 1+2 too to support readability.\n* I personally prefere extensive figure captions.\n* Lastly, adding more \"simple\" imputation methods, e.g. random forests, neural networks, would be interesting, since there are some empirical evaluations that come to different results\n* the authors are right in that there are few peer reviewed papers at the big ML conferences that directly compare latest generative deep learninig methods with classical methods, like kNN. This is however in part true because in my experience these papers do not get accepted. I have been reviewing imputation papers for the past couple years and I've seen several cases where an empirical evaluation of this kind, even though the experimental setting was absolutely flawless, did not make it. ",
            "summary_of_the_review": "The experimental setting has a significant limitation, the hyper parameter optimization is not conducted on the right data set, which can explain the superior performance of KNN. The authors claim that they are the first to report these results, which is not exactly true. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This manuscript presents a nice empirical comparison between the classic KNN imputation with two state-of-the-art GAN-based imputations for tabular data imputation. The experiments are performed on both simulated and publicly available real data. The results, overall, show that the KNN, despite its simplicity, provides very competitive results when compared to its more complex and computationally expensive alternatives. In summary, this paper has a few important messages for both method developers and reviewers in the missing data community: i) first try the simplest; ii) more complex does not mean better; iii) understand your data before data imputation;  ",
            "main_review": "Strength:\n- The text is clear; the experiments are robust; the messages are important.\n- This paper has also an important message for the new method developers within the community suggesting to compare first their new methods with simple standard baselines. The authors might want to emphasize more this point in their summary.\n- This study presents a nice guideline for imputing missing values in tabular data.\n\nWeaknesses:\n- The experimental design can be improved. Currently, the authors mainly focused on the effect of missing data mechanisms (MCAR, MAR, MNAR) on the performance of data imputation strategies. This is while there are also other important avenues to explore. For example, the authors could also evaluate the effect of different hyperparameter settings on the performance of GAIN and KNN. Currently, we have such analysis in Appendix C but only for simulated data and a fixed number of samples (1000). What about the effect of hyperparameters when imputing real data?  I suggest focussing on one real dataset as a case study and evaluating the effect of hyperparameters (K in KNN and epoch number in GAIN) on the performance of imputation. My hypothesis is that K in KNN is less sensitive to sample size compared to the number of epochs in GAIN. It is also interesting to see that how the optimal value for the hyperparameters changes from one dataset to another. Furthermore, I wonder why the authors decided to include the misGAN in the main text (section 3.2) while its results are below the bar and only presented in the appendix. I suggest saving some space for extra analysis by moving all information about misGAN to appendices. \n- The introduction consists few strong (of course valid) arguments and sentences without proper referencing. For example 1) \"This solution, although straightforward to implement, has two main disadvantages: it can significantly reduce the size of the dataset and induce a bias [Ref.?].\" 2) \"It allows to preserve the whole dataset for subsequent analysis but requires careful handling as it can also introduce a bias in the imputed dataset [Ref.?].\" 3) \"In particular, only ”well-behaved” values can be imputed and missing values of outlier observations will get overlooked [Ref.?].\"\n- While the argument is to compare the recently introduced complex GAN-based imputation with their simpler classic alternatives, I wonder why the authors did not include the MICE method in their analysis. MICE performs in some ways similar to GAIN and would be interesting to see a comparison. Of course, MICE does not apply to the MNAR scenario (due to its MAR assumption).\n\nMinor comments and questions:\n- The last paragraph on page 1 is not informative and can be removed in favor of more space for extra analysis.\n- \"We argue that all real-world datasets have missing values distributed in MNAR settings; MCAR and MAR are convenient assumptions to facilitate the treatment of incomplete datasets.\" This is an unsupported argument that should be supported either by some empirical evidence or by including some references.\n- Sec 5: the data is scaled to interval [0,1]: is it only for outputs or includes also inputs? Should be clarified in the text because this rescaling might not be the best choice for the input features. I assume this is done only for outputs to make the resulting RMSE comparable across datasets. In that case, again I would suggest skipping rescaling and using instead standardized MSE (SMSE). It is a very minor comment though.\n- It is not clear how the other GAIN hyperparameters ($h$ and $\\alpha$) are decided. The authors say after several trials they have decided to use the defaults in the original study. Would be nice to include those trials in the appendix.\n- Page 3: \"parameters are trained trained following...\" => \"parameters are trained following...\"\n- Page 3: \"After several trails, ...\"=> \"After several trials,...\"\n- Some aspects of this study overlap with recent work at https://doi.org/10.3389/fdata.2021.693674 . I wonder why the authors did not mention this study.\n \n",
            "summary_of_the_review": "This is a nice study with important messages for the community. The experimental section can be enriched with a more in-depth evaluation of the effect of hyperparameters on the performance of data imputation approaches.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}