{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper applies a reinforcement learning (RL) approach to a medical diagnosis dialog task. Motivated by a large action space, the authors utilize a hierarchical model where the higher level model triggers a lower level model comprising of symptom checkers and disease classifiers. They evaluate their approach on real-world and synthetic data sets.\n\nPros\n+ The application (societal relevance) and the hierarchical approach (large action space) are motivated well\n+ The paper is presented relatively clearly (with caveats: see reviewer comments) and improves performance over reasonable baselines (with caveats over one metric: why longer dialog is better?)\n\nCons\n- The novelty of the work was not entirely clear, other than the application to a new task\n- Lack of examples make it difficult to gauge the complexity of the task\n- Ablation studies would also have provided better insight into task and the proposed model\n\nThe reviewers have several concerns about the work described in the paper. But the authors did not provide any response unfortunately."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper introduces hierarchical reinforcement learning (HRL) into automatic disease diagnosis, which reduces the action space and improves training efficiency. Besides, the authors also expand an existing public dataset and build a synthetic dataset for evaluation. The Experimental results show that their proposed hierarchical framework achieves higher accuracy and symptom recall in disease diagnosis than existing several baselines.\n",
            "main_review": "Strengths:\n1. The task of automatic disease diagnosis that this work focuses on is worth exploring and has a very potential beneficial impact on society. \n2. The paper is well organized and easy to understand\n\nWeaknesses:\n1. My main concern is the technical novelty of this work. This work is more like a combination of existing technologies. HRL, User Simulator, and Reward Shaping are all adopted in previous RL-based methods for disease diagnosis. The author's contribution on methodology is quite limited.\n\n2. In section 4.3 ablation studies, more components (e.g., Reward Shaping) need to be considered to make the experimental results more convincing.\n\n3. It would be helpful if the authors could provide some examples to understand how their systems behave.",
            "summary_of_the_review": "This work focuses on a meaningful task and constructs datasets for better evaluation, but the limited contribution at the methodological level makes me inclined to think that it has not yet reached the bar for a top conference.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "##########################################################################\n\nSummary:\nThis works proposes dialogue policy learning by incorporating a hierarchical policy. The proposed hierarchical reinforcement learning-based approach has two models, i.e., one master model that instantiates low models. The low models have several symptoms checkers and disease classifier. Experiments are conducted on both real and synthetic datasets to demonstrate the efficacy of the proposed work. \n \n##########################################################################",
            "main_review": " \n##########################################################################\n\nPros:  \n\n1. The paper takes one of the most important task, i.e., automatic disease diagnosis. However, the proposed approach only works at a semantic-level and is not an end-to-end approach. That is, it does not consider the NLU and NLg modules of the typical goal-oriented dialog systems and the paper is not clear about it. It took me a long time figure out what they are trying to do.\n\n 2. Experiments are provided to show that the proposed methods work well. However, I am concerned about whether they are actually comparing against state-of-the-art systems?\n\n3. The proposes a nice application of the hierarchical reinforcement learning.\n\n \n##########################################################################\n\nCons: \n\n1. The paper claims that in the conventional RL tasks, the action is not big, whereas the action space in disease diagnosis huge. However, there exists several works in RL where action space is huge, way bigger than the proposed work. Examples include generating natural language text, where action is equal to the vocabulary size. How does this work differ than those from the technical point of view.\n\n2. Regarding comparison with the SOTA systems, I am familiar with at least one paper (there must be many out there) from EMNLP 2020. The title of the paper is: MedDialog: Large-scale Medical Dialogue Datasets. \n\n3. From a technical point of view, I am not sure what is contribution of the paper.\n\n4. In the experiments, the paper shows that having large number of turns as a positive sign, whereas in the goal-oriented dialog systems literature, the less number of turns in a conversation is better (as it is efficient to get job done quicker). However, the open-domain dialog systems literature is different where the goal is have longer conversations. \n\n########################################################################## \n\n\n",
            "summary_of_the_review": "########################################################################## \n\nReasons for score: \n \nOverall, I vote for rejection. I like the application side of the paper. But, technically, I am unable to see significant contribution. Moreover, the paper lacks in comparison with the state-of-the-art systems in the relevant domain.\n\n\n########################################################################## \n\nQuestions for rebuttal:\nFeel free to answer to all the concerns.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper tackles the problem of automatic disease diagnosis through reinforcement learning under a setting of task-oriented dialogues. The authors proposed to integrate hierarchical policies with two levels (one high-level master model, and one low-level policy) into the dialogue policy learning. Experiments on both real-life and synthetic data suggest the proposed approach is effective.",
            "main_review": "Strength:\n1. The proposed HRL approach achieves better performance compared to the existing baselines.\n2. New synthetic data was proposed for the task and can be useful for future study.\n\n\nWeaknesses:\n1. The idea of hierarchical RL is not particularly novel. It seems from Table 2, the existing best baseline is FIT, which is a non-RL approach, so it's not clear to me why RL is necessary to solve the automatic disease diagnosis. Since a non-RL approach achieves the best result, maybe we should develop on the non-RL approach?\n2. It's worth showing some dialogue examples for the audience to understand what automatic disease diagnosis works under a task-oriented dialogue setting. It's not clear how the language generation part is handled in both simulated setting and real-life settings.\n3. Many metrics are missing from the baselines to get a fair comparison in Table 2.\n\n\nQuestions:\n1. Is the \"disease discriminator\" the same thing as \"disease classifier\"?\n2. What does \"user goal\" mean under the task context? Are the users trying to get some treatment?\n3. What exactly is the match rate of the dialogue? Is average turns the higher the better? It seems it's better if the diagnosis can be solved in reasonable amount of time instead of the longer the better?\n\n\nTypo: \n2.1.5, \"base on\" --> \"based on\"\n",
            "summary_of_the_review": "Overall I think the paper addresses an important problem, but it needs a bit more justification on why RL is necessary for this type of tasks, and some details of the paper can be clarified further.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety"
            ],
            "details_of_ethics_concerns": "Will the datasets contain the patient's personal information? ",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper applies Hierarchical Reinforcement Learning (HRL) to automatic disease diagnosis in task-oriented dialogues setting. The authors argue that applying RL to automatic disease diagnosis is challenging because the action space (i.e., symptoms) is very large.\nTherefore, they propose to learn a hierarchical dialogue policy where the high level policy is for categorizing patents into different groups and the low level policy is for checking symptoms and classifying diseases within a group.  Experimental results on multiple datasets show that the proposed HRL strategy achieve higher disease diagnosis accuracy compared to existing RL systems.\n\n\n\n\n",
            "main_review": "Pros:\n- This paper is well organized and easy to follow.\n- The proposed method is well motivated and achieved strong empirical results in terms of disease classification accuracy and recall of symptoms.\n\nCons:\n-  I am not very familiar with RL literature so I might be wrong. But to me, the proposed method looks like a straightforward application of HRL. Therefore, the technical contribution of the work is weak.\n- Regarding the experimental results, I found that the proposed method requires significantly higher *Averaged Turns* for disease diagnosis compared to other baselines (as shown in Table 2). I don't understand why the authors bold the highest *Averaged Turns* in Table 2. But in task-oriented dialogues, accomplishing a user goal with fewer turns is better. Otherwise, the system could have checked all the possible symptoms one by one before making a final decision.\n",
            "summary_of_the_review": "Overall, this is an OK work with limited novelty. I have some concerns about experimental results (see Main Review) which need to be clarified in the author's response.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}