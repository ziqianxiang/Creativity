{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "In this paper, the authors studied reinforcement learning applications that have access to both online and offline data (with limited online interaction though).  In order to handle the mixture of online and offline data efficiently, the authors proposed a new paradigm called adaptive Q-learning, which treats offline and online data differently (as reflected by whether pessimism is implemented or not). The effectiveness of the proposed paradigm has been tested empirically. The reviewers have raised concerns about the sufficiency and significance of the experiments conducted in the paper, and pointed out that the proposed algorithmic idea is a somewhat incremental change over existing ones. The changes the authors promised to make will make the paper stronger."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents a framework called adaptive Q-learning that integrates the advantage of offline learning and online learning. ",
            "main_review": "Poor writing. Inadequate experiments design. This paper, at its current status, is not ready to be submitted to conferences like ICLR. \nActionable feedback for the authors might be rethinking the storyline of this work.",
            "summary_of_the_review": "see above. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a mixed offline-online RL approach for which they design an algorithm. They propose to maintain 2 separate replay buffers, one for online data and one for offline data, to allow them to sample either an online or offline batch of data when doing an update step, and tailor the loss function based on the batch's provenance. In addition, the authors propose using a CQL variant which uses an ensemble of action-value function as done by REDQ to learn in this setting. They conclude by showing some empirical results on the D4RL Mujoco benchmark domains.",
            "main_review": "There are two distinct contributions in this paper. The first is in the proposed training with experience replay approach, and the second is the propose algorithm which combines elements of CQL and REDQ. Unfortunately, I'm not sure the experimental results adequately explore either of these contributions. What the results show is their method doing better in some cases when using their training approach, but this approach itself is not validated by these results. Why not compare to other methods mentioned in the intro, such as Lee et al. 2021, using the training procedure they were designed for? The authors highlight differences between what they propose and the related work, but not all of those differences preclude comparing between the mentioned offline-online methods and the propose method. The results here don't allow us to determine whether this approach improves on existing approaches to offline-online RL.\n\nAs for the other main contribution, it's equally difficult to assess the value of this new CQL/REDQ hybrid because it is only compared using the proposed training method which might disadvantage the methods used as comparison. This wouldn't be an issue if the training method was shown as an important or superior approach for doing offline-online RL, but, as I argue above, I don't believe that the authors have provided any evidence for this. For instance, looking at REDQ's results in table 1, it seems like this training regime causes it to underperform based on the results the REDQ paper report given the same number of interactions (I could be mistaken given the results in this paper use a normalized score who definition is unknown to me). \n\nIn terms of writing, the paper contains many typos and incorrect word choice. I don't think this prevented me from understanding what was meant but this paper would benefit from some careful proofreading. Otherwise, the paper is well structured and mostly easy to follow.\n\nMy concerns about the experimental results make me doubt that this paper is ready for publication. The authors should make sure that their claim, that this training approach is a superior way of doing offline-online RL, is properly validated by the experimental results. It isn't sufficient to just propose a method that works better when using this training approach.\n\n# Questions:\n\n- Is my understanding correct that newly collected samples are added to both the online and offline buffers?\n\n- Is the initial offline dataset not used to initialize the offline buffer?\n\n- p. 7, “we report the max average score as performance”, this is ambiguous. As I see it, we have different points along the x-axis and different seeds. What are taking the maximum over? What are we averaging over?\n\n\n# Comments:\n\n- p. 4, it’s difficult to appreciate this illustrative example without some more context. How was the offline dataset generated? How are the offline and online datasets used, are they just concatenated? How many seeds were used and what are do the shaded areas represent? Why would it be fair to say that CQL failed if it was able to improve, albeit slowly? Why is this not due to the starting performance being already quite good? What kind of performance should we be expecting in this domain?\n\n- p. 5, the term C should be defined here. I could not confidently find it in the cited work, and, regardless, it is central enough that is should be restated here to avoid any possible ambiguity.\n\n- p. 5, “we randomly select two Q functions just following the REDQ’s setting”, this is unclear. Eq. (4) shows only a single action-value function or index. What is the meaning of the hat over this action-value function? One of the major contributions is proposed combination of parts of CQL and REDQ. I expect this terms to be explicitly defined here.\n\n- p. 6, eq. (6), this isn’t a proper definition for a function. As I understand it, nothing prevents identical state-action tuples from being present in both buffers. The output of this function given that (s, a) is indeterminate. This function definition requires another argument.\n\n\n\n# Minor comments and nitpicks:\n\n- p. 1, \"pre-existing offline data can prevent agents from converging prematurely due to the potential diverse”, the potential diverse? typo?\n\n- p. 2, \"the agent can achieve an expert policy using very few online interaction steps regardless of the quality of the offline dataset”, how can this possibly be true? Wouldn’t a degenerate dataset mean that all the learning needs to be done online, which would like require more than “very few” interactions?\n\n- p. 3, later in the paper, policies are treated as probabilistic, e.g., eq. (3). The policy definition provided here is a deterministic mapping of states to actions.\n\n- p. 3, \"a corresponding value function”, this probably should be called the action-value function, not the value function as the term is usually used to refer the state-value function.\n\n- p. 3, “off-policy RL algorithms maintain a growing replay buffer”, this is true only at the start of learning. Buffers are typically kept at a fixed size max size.\n\n- p. 6, algorithm 1, the variable t is not defined and its value is never changed.\n\n- p. 7, \"The entire offline-gradient-update step is about 1 million, and the entire online exploration step is around 90K”, 1 million what? What are we counting, what are the units?\n\n- p. 7, table 1, what do the +/- ranges represent?\n\nPost rebuttal\n==============\n\nThe authors have improved the empirical evaluation and have mostly addressed my biggest concerns. I have increased my scores to reflect this.\n",
            "summary_of_the_review": "The authors shows that the proposed method performs better under their proposed training regime but don't provide any supporting evidence for this training regime. This raises some concerns about the significance of the empirical results, and the usefulness of the proposed regime. These concerns prevent me from recommending acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes GCQL, a new RL algorithm that is trained on a mixture of offline data and online interactions. The novel component of the algorithm is a reweighting that balances acting pessimistically on offline data and greedily on online interactions. The authors propose a mixture replay buffer that consists of both offline and online samples. For online samples, the policy is trained as in the REDQ algorithm; meanwhile, for offline samples, the policy is additionally trained on a CQL-like value penalty. Finally, the authors show that GCQL outperforms existing SOTA offline RL algorithms that simply fine-tune on online data. \n",
            "main_review": "I think the paper studies an important problem of offline-online RL where the algorithm has access to both offline data and a limited amount of online interactions. The authors empirically show the disadvantage of using a traditional approach of simply fine-tuning on the online data, as most offline RL algorithms like CQL will behave too conservatively. The approach, while simple, makes sense intuitively as it has the desired property of behaving pessimistically according to the offline data but greedily on the online data. \n\nThe proposed algorithm GCQL combines two specific offline RL algorithms CQL and REDQ. The central premise of the paper, to only act conservatively on the offline data, is a general concept that is agnostic to the specific training objective. I feel that the paper could be made more general by considering other forms of pessimism. For example, I would be interested in seeing if similar performance gains can be obtained by using a policy constraint penalty i.e. the penalty in TD3 + BC, rather than the value penalty in CQL. That way, the paper could be made more general by giving the practitioner the flexibility to use the best base learning algorithm for their specific problem domain. \n\nOne concern is that the performance gain of GCQL over GCQL-wg (which I interpret as effectively being CQL with ensemble Q-learning) appears to be very marginal. The only exception is when the offline dataset is random or highly suboptimal. In practice, it is common to assume that the dataset was generated by a decently-performing policy, so it is unclear if the novel training objective will yield much improvement. I think the paper would benefit from a more nuanced characterization by measuring the performance gap as a function of the suboptimality of the behavior policy.",
            "summary_of_the_review": "Overall, though the proposed algorithm seems like an incremental change over existing ones, it clearly is preferable to use over existing methods that simply fine-tune on the online data. I believe that the realization that using the same training objective for both offline and online data yields overly pessimistic policies is an important contribution, and opens the door for future work that considers different objectives for offline and online data as this paper does. Because of this, I recommend that the paper be accepted. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper tackles a variation to the offline RL setting, where the agent is allowed some limited number of online interaction steps after learning offline. An algorithm, CGQL, is proposed for this setting and uses the idea that online and offline data should be used in different updates. Experiments on a common benchmark show that this approach can be more effective than standard batch RL methods and ablation studies are also included. \n",
            "main_review": "\nMain points:\n- The topic is interesting and I find that combining both offline and online training is a fruitful direction of research, which is potentially relevant in many practical situations. \n- The main intuition for adaptive Q-learning make sense to me---treating the offline and online data differently, with modified updates for each. \n- In terms of clarity, some parts need to be explained further, such as how the algorithm uses the replay buffers.  See the section below for more detailed questions.\n- The experiments seemed to be done well generally and the performance of the proposed algorithm is substantially better than the baselines in certain settings e.g. medium-replay. It's also interesting to have experiments where the online interactions with the environment are limited. Other ablation experiments and sensitivity tests to hyperparameters are included, which are welcome additions. The ablation studies seem to indicate that the proposed algorithm could be simplified though and I've included a few questions below.\n\nQuestions and suggestions:\n- The description of the online-offline replay buffers and section 4.3 are unclear. What exactly does each replay buffer contain? The online replay buffer says it contains \"online exploration experience\" but then the same thins is said about the offline buffer, \"offline replay buffer consisting of the newly generated online exploration data\". I'm confused about this distinction. Does the offline replay buffer contain more than just the initial dataset?\n\n- Why is it that REDQ fails completely on these tasks? \nOther variants of Q-learning seem to work in batch RL such as the distributional RL algorithms or Random Ensemble Mixture (REM) from \"An Optimistic Perspective on Offline Reinforcement Learning\" (Agarwal et al.). REDQ seems to be fairly similar to REM so one could expect it to work decently well in the offline setting too.\nAlso, looking at the learning curves in the appendix, there doesn't seem to be any improvement even in the online part for REDQ. \nIs the offline training a hindrance to the algorithm? Would it perform better if it was simply initialized from scratch without the offline training? I think this would be a useful baseline to include too.\n\n- CGQL-wo \"without the online replay buffer where we only sample from the offline buffer\". Does this mean that all the experience gets pooled into a single replay buffer? Could clarify the mechanism?\nAlso, does that also include removing the conservative term? The full GCQL would set W=0 when sampling from the offline buffer.\n\n- Related to the previous point, how important is it that the online buffer is much smaller than the offline buffer? \n\n- To clarify, is the difference between REDQ and CGQL-wc (without the conservative term) only that the replay buffers are separated into two? Are all the other hyperparameters the same? Would this explain why REDQ fails but CGQL-wc can achieve decent performance?\n\n- Do you have any idea what happened to CQL in halfcheetah-medium? In this environment, it seems like training more online actually leads to decreased performance over time, strangely enough. \n\n- Looking at the ablations, it seems like the setting CGQL-wg does quite well overall. This would suggest that being more conservative the entire time is not a bad strategy. I'm a bit curious how these results compare to CQL. \n\n- In the ablations, CGQL-we does about as well as the full CGQL in every setting except walker2d-random. In that case, even full GCQL doesn't improve that much over training, only reaching an average performance of around 30 by the end. This would suggest that the algorithm could be simplified while retaining almost identical performance. I'm not very convinced that adding ensembles\nis crucial here. \n\n- Section 5.4. The results should be included in the main text, even if it is summarized. Currently, there are no results to look at. Also, to present the different hyperparameter settings, I would just write that T_off was tested with 2e4 and 5e3 instead of describing the legend for the learning curves. Same for T_initial and p. In the current draft, the legend is out-of-place since the plot isn't in the main text. Also, I would add the label descriptions with the learning curves in the appendix for easier reading. \n\nMinor points:\n- \"Moreover, how to evaluate the policy under the offline setting is also a challenging problem.\" While this is mentioned in the abstract, the paper does not tackle this problem so I think it would be best to remove it.\n\n- I'm not sure if \"framework\" is the proper word to use to describe adaptive Q-learning. I usually expect \"frameworks\" to be more general and adaptive Q-learning seems like a fairly specific template for an algorithm. Personally, I found this a bit confusing and had different expectations when reading the abstract.\n\nVarious typos and awkward sentences are scattered throughout and should be proofread. I'm assuming these can be easily fixed so I haven't counted them as a negative point in my review. Some examples:\n- p.1 \"or by combination with the imitation learning\" -> \"or in combination with imitation learning\"\n- p.1 \"pre-existing offline data can prevent agents\nfrom converging prematurely due to the potential diverse, ...\". Missing words?\n- p.2 \"a well-performed policy \" -> \"high-performing\"/\"strong\"/\"good\"\n- p.2 \"In general, Online RL include the on-policy and off-policy RL\" -> In general, online RL includes on-policy and off-policy RL\"\n- p.5 \"CQL (Fujimoto et al 2019)\". Wrong citation.\n- p.5 \"As explained in section 1, We take the online-offline buffer equally important\". Awkward phrasing.\n",
            "summary_of_the_review": "The problem the paper tackles is interesting, offline RL with limited online interactions. I think this is a nice direction of research that merits further investigation. The proposed approach makes intuitive sense to me and the algorithm, CGQL, has good performance on the benchmark environments. I do have a number of clarification questions and the paper would require some editing to make these clear. Overall, I am currently leaning towards acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}