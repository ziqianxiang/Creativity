{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "Unfortunately, reviewers unanimously agreed that this paper does not meet the ICLR acceptance standards, citing generally unpolished experiments. I would recommend substantially expanding the experimental results in the future."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper attempts to study the effect of different transformations of input datasets, and the performance of the resulting models thereof. In particular, authors consider time-series data which is then represented in (1) time-series format, (2) vectorized form, (3) tensorized form. For each of the representation, a different neural network is trained and the performance is compared. Experiments are shown on MNIST dataset.",
            "main_review": "The authors find that different representations and models give different performance, which is not surprising. This is a known fact - in practice, domain specific models are generally trained tailored to the task at hand. These models encode implicit biases needed for the task. For vision tasks, convnets are trained and for NLP, transformers are used. Researchers are aware of this, and the conclusions of this paper add no value in my opinion. Furthermore, the experiments are performed only on MNIST with minimal architectural choices, and the results are not conclusive in any form. I recommend strongly rejecting the paper.",
            "summary_of_the_review": "The conclusions of the paper add no value, and the experiments are not performed well.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "- Draft attempts to understand the effect of converting dataset format while applying an ML technique. It argues that converting the target data into a format on which the specific ML technique was shown to be successful is suboptimal. Via simple experiments it shows that the data sample format conversion need not be advantageous.",
            "main_review": "- The draft fails to support the claims strongly. For instance, they do not consider data from CPS, instead they consider well known datasets from computer vision on which various DNN architectures have already been explored. Instead, authors should have worked with a different domain as claimed.\n\n- Also, the datasets MNIST and Fashion MNIST are similar in some of the important aspects: both gray scale, same number of classes, and number of samples (complexity), etc. This does not support the draft's claim that it works with diverse dataset types.\n\n- Experimental analysis presented in the paper is very weak. Only a couple simplest datasets are considered to train simple CNN and MLP classifier architectures. This kind of analysis is not very convincing to generalize from.\n\n- It is not very clear why authors consider adversarial robustness to investigate the disadvantage of the dataset conversion.\n\n- The take-away (according to the abstract) is that  the data conversion is not always needed or beneficial. This is not very informative or novel, since it is understandable that the data formats need to be understood and domain knowledge also is required to avoid any structure or information loss while processing,",
            "summary_of_the_review": "- Draft is incomplete in multiple aspects. It promises much in the abstract (or goals) but fails to deliver it. It is supposed to be a comprehensive empirical analysis, but the provided experiments fall short of it.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper aims to study the effects of different transformation methods when processing a dataset for supervised deep learning tasks. The authors conducted one experiment to show that different data arranging methods leads to different vulnerability for adversarial attacks. In the experimental procedure, an optimal transport based method is utilized to measure the distance between a dataset and its perturbed version. ",
            "main_review": "Pros:\n\n- In general, the authors clearly deliver their ideas and experimental procedure. \n\n- While people are introducing ML methods to more and more real-world problems, the topic covered in this paper is certainly becoming more attractive. \n\nCons and feedbacks:\n\n- The problem is not well formulated. The idea of studying different input formates could be helpful when solving practical problems, but I would suggest the authors formulate this problem by starting from even simple NN models and assumptions on datasets. Is it possible to use an objective to describe this discrepancy caused by data transformations?\n\n- Lack experiments. The authors are inspired by ML problems on CPSs datasets, however, the only experiment in this paper is on the MNIST and Fashion-MNSIT datasets, and the experiment on these part (MNIST, fashionMNIST) are already provided by public codebases [0]. Experiments on datasets from power systems, robotics, or autonomous driving are expected. There are multiple public datasets from CPSs mentioned above [1, 2, 3]\n\n- The definition of “domain” and “adapt”. This paper defines the domain as a broad area of applications such as CV, audio recognition, and others, and the term “adapt” means changing the data formate. However, in many existing studies, the term domain is usually associated with the distribution of data [4]. Consider an autonomous driving case where we collect two datasets (image and vehicle trajectory) for each weather (sunny and raining) and thus have four datasets in total, how should we divide these four datasets into different domains? \n\n- The usage of the Optimal Transport Data Distance. While this is a promising prior study on dataset geometric distances, one assumption is that the label-feature distributions are Gaussian. Does that still hold for the CPSs dataset that may contain continuous signals? I would like to hear more from the authors on this point.\n\n- It is a good idea to use adversarial attacks to evaluate the transformations. It would be interesting to utilize the “Wasserstein adversarial attack” [5] and see if this method correlates with the optimal transport data distance. \n\n\n[0]: Link: github.com/microsoft/otdd\n\n[1]: Link: www.kaggle.com/robikscube/hourly-energy-consumption\n\n[2]: Geiger, Andreas, et al. \"Vision meets robotics: The kitti dataset.\" The International Journal of Robotics Research 32.11 (2013): 1231-1237. \n\n[3]: Mandlekar, Ajay, et al. \"Roboturk: A crowdsourcing platform for robotic skill learning through imitation.\" Conference on Robot Learning. PMLR, 2018. \n\n[4]: Courty, Nicolas, et al. \"Optimal transport for domain adaptation.\" IEEE transactions on pattern analysis and machine intelligence 39.9 (2016): 1853-1865.\n\n[5]: Wong, Eric, Frank Schmidt, and Zico Kolter. \"Wasserstein adversarial examples via projected Sinkhorn iterations.\" International Conference on Machine Learning. PMLR, 2019.",
            "summary_of_the_review": "In general, this paper raised an interesting problem and studied the obstacle when people want to apply ML methods to CPSs data. The authors conducted experiments with existing methods on OT, deep learning, and adversarial attack. However, I feel the results and analysis are still in their preliminary form. This paper could meet the acceptance requirements with a clear problem formulation, more extensive experimental results with CPSs data, and theoretical analysis. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I do not find ethics concerns. ",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}