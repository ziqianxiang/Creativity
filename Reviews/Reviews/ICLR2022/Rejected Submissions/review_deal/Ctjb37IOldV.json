{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The authors make an experimental case that dropout aids generalization by\npromoting \"flatter minima\".\n\nThe reviewers felt that the work reported in this paper makes a useful step\nforward on a question of central interest.  The consensus view was that the\ntotal weight of evidence presented was not sufficient for publication in\nICLR.  The paper could be strengthened was more extensive and varied experiments\nand/or theoretical analysis."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper tries to study the effect of dropout by studying the trajectory of optimization via dropout. It is claimed that dropout can help find flatter minimizer.",
            "main_review": "I am not quite familiar with this area. But I think the topic is quite interesting. \n\nThe main concerns are:\n\n1. It seems most of the claims are not theoretically supported. I tried to find something in the supplementary material but only code is found.\n2. It is observed that regularization also helps and find flatter minimizer and there is work to bridge the dropout and reguarliation: Dropout Training as Adaptive Regularization. More discussion is encouraged.\n",
            "summary_of_the_review": "See main review.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper uses the tools from different prior works to study the flatness brought by dropout. The paper shows that the noise induced by the dropout has a similar structured introduced by SGD that leads the training to find flatter minima.",
            "main_review": "In general, the paper is interesting; however, the novelty and contribution are limited. Most of the tools are from the existing work, and neither insights nor empirical results are strong enough to make this paper stands out.\n\nThe paper may miss some important discussions regarding the relationship between dropout, regularization, and generalization bounds, as discussed in [1, 2, 3]. Adding the discussion and extra experiments would increase the value of the current draft.\n\nThe paper divides the training process into two phases, i.e., fast convergence and exploration phase. However, this phase division is relatively rough and has no support (authors are encouraged to cite some prior works to support their justification).\n\nThe computation method for H on neural networks has never been introduced in the main text, making the current empirical results questionable. The computation cost of $H_i$ remains unclear; this comment also applies to the quality of hessian estimation, which may significantly impact the final numerical observation.\n\nIt is a bit hard to identify the (inverse) relation between variance and the flatness in Figure 2 and Figure 3a, unlike the clear pattern in Figure 3b. It would be great if the authors could polish the figures and/or explain the current observation.\n\n## Reference\n1. Dropout Training, Data-dependent Regularization, and Generalization Bounds\n2. On Convergence and Generalization of Dropout Training\n3. A PAC-Bayesian Tutorial with A Dropout Bound",
            "summary_of_the_review": "In general the paper is interesting; however, the novelty and contribution is limited. Most of tools are from the existing work, and neither insights nor empirical results are strong enough to make this paper stands out.\nAdding the discussion related to the generalization bound (as pointed in the provided literature) and extra experiments would increase the value of the current draft.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper provides an empirical study on how the dropout can lead to minima within a flatter landscape (hence a better generalization performance). Based on the definition of minimizer flatness (i.e., eq. 5) and random trajectory data (parameters from a optimization path and gradients when loss is stably small), the authors first show that dropout can help to find flatter minima, and also find a inverse relation between the flatness (the length $F_v$) and the algorithmic variance (i.e., in area with sharper direction, algorithm with dropout has a larger variance), and further find that for areas with larger eigenvalues (i.e., with sharper landscape), the variances of the algorithm along the directions of their eigenvectors are also large. Such empirical results together demonstrate that the dropout can play a good role in improving the generalization of existing algorithms by introducing certain level of noise. ",
            "main_review": "My detailed comments are given as below. \n\nStrength: \n1. This paper is well written and the motivation is clear to me. Dropout is a widely used technique for improving the generalization performance of various learning algorithms, but there is limited understanding how this scheme helps to achieve this better generalization. This paper provides some empirical justification showing that dropout can lead to flatter minima. \n\nWeakness:\n1. Some parts need to be explained more clearly. For example, I have a hard time understanding Section 3.3., where interval flatness is defined as the width of the region where $L_v(\\delta \\theta)<2L_v(0)$. I am wondering why $2L_v(0)$ is chosen here, and whether the factor $2$ is sufficiently general to characterize the landscapes for all settings. I am not familiar with this part, but I hope the authors should explain it a little bit.  \n\n2. My second concern is the novelty of the results. So far I see that some important tools, e.g., flatness definition, PCA, follow from existing studies, although there is some new treatment in obtain the variance. In addition, the empirical results are not that surprising to me (but I agree that it is not clear why the noise induced by dropout plays some similar role as SGD), and a more interesting part may be to provide some theoretical justification even in some simplified settings. \n\n3. Experiments can be strengthened. I see that all experiments are conducted over small datasets (e.g., CIFAR-10) and small models. To make a more convincing conclusion that dropout can lead to flatter minima, I believe experiments on modern datasets and models (e.g., ResNet on ImageNet) need to be run. \n\n\n\n\n\n",
            "summary_of_the_review": "I think this paper provides some empirical explanation on why dropout is useful to achieve a flatter minima. However,  I feel it is more interesting to rigorously explain this phenomenon given that many works already prove that SGD can reach flatter minima. In addition, the novelty of this paper is not that high based on my understanding. Therefore, I am slightly negative about this paper, but I am open to adjust my score based on the authors' feedback and other reviewers' comments.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper empirically found that Dropout noise could help the neural network search for the flat minima, by showing that the covariance of parameter and gradient align well with the loss curvature and have inverse relationship with interval flatness. ",
            "main_review": "\nTo my best knowledge, this is a first work that shows the relationship between Dropout and the flatness of minima. It could provide another perspective for understanding the effectiveness of Dropout. \n\nHowever, my main concern about this manuscript is the complete missing of theoretical analysis, which makes the current version of the paper not strong enough for publication. I strongly suggest the authors conduct theoretical analysis over the covariance introduced by Dropout noise and loss curvature, at least on linear cases. \n",
            "summary_of_the_review": "Complete missing of theoretical analysis. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}