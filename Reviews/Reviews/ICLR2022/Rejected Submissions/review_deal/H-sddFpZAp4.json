{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper presents a novel architecture, ModeRNN, for unsupervised video prediction by learning spatiotemporal attention in the latent subspace (slots).  ModeRNN effectively learns modular features using a set of mode slots and adaptively aggregates\nthe slot features with learnable importance weights. The paper has promising results on several benchmark video prediction datasets. \n\nDuring the post-rebuttal discussion, the reviewer Wt6k and VMMf responded to the authors' rebuttal, but there was no discussion among them. The consensus is that even though the paper is a very strong engineering effort, it was not clear how the proposed architecture addresses the spatiotemporal mode collapse problem.  T-SNE in Fig. 3/10/13 is insufficient to show disentangled feature space. In fact, PhyDnet was designed to disentangle different factors (physical vs unknown), hence not a good baseline.  [Hsieh et al 2018] is a better fit. In addition, synthetic data examples would be helpful to explain the underlying mechanism of the model and provide more insights for the video prediction community.\n\nBased on this reason, I recommend rejecting this paper as it is now and encourage the authors to revise the draft and submit to future venues.\n\nHsieh, J. T., Liu, B., Huang, D. A., Li, F. F., & Niebles, J. C. (2018, January). Learning to Decompose and Disentangle Representations for Video Prediction. In NeurIPS."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The manuscript proposes a novel architecture with a slot-based decoupling-aggregation framework for unsupervised sequence prediction. The model is motivated by preventing spatio-temporal mode collapse, which affects many existing methods. The experiments clearly show that the model addresses this issue and performance comparisons across three commonly used datasets are presented.",
            "main_review": "Strong points:\n- Overall well motivated novel architecture for addressing the issue of mode collapse in unsupervised predictive training. \n- The proposed decoupling-aggregation framework is demonstrated to learn a highly diverse set of mode-factors which are combined to cover a diverse set of output modes with high accuracy.\n- The analytical experiments show that the performance of ModeRNN consistently (across datasets) benefits from increased diversity of spatio-temporal modes, whereas the performance of other methods often diminishes when trained on more diverse data.\n- The description seems sufficiently detailed for reproducing the experiments.\n\nComments/Questions:\n1. Inconsistencies between text, equations and figure:\n- Eq. 3 is missing the shared FFN for dimensionality reduction\n- Text should explicitly state that the slot bus input $g_t$ is tanh activated unlike the gates. I’d also name it something like slot bus input instead of input modulation “gate”, since the input gate $i_t$ is usually considered to do the modulation of the cell input.\n2. Motivation for implementation details in adaptive slot fusion missing.\n3. For clarity: Is $\\sigma(I_t) \\cdot (W_\\text{fuse}^0 * I_t)$ what you call the residual connection? I’m not exactly sure about the terminology, but I believe residual connections are usually linear.\n4. I think instead of $slot_{t-1}^*$, you could just write $Q_{t}^*$, since these queries do not correspond to the spatio-temporal slots of the previous time-step. Using the same notation is confusing.\n5. Since the figure doesn’t have a descriptive caption, I’d consider placing it on the same page as the description of the architecture.\n6. A question about the ModeRNN (i.e. the stacked ModeCell): Do the higher layers receive only the cell output $\\mathcal{H}_t$ as input or do they also have access to the slot bus $\\mathcal{B}_t$?\n7. Typo: Figure 5: Ture should be True\n8. Did you also calculate the A-distance using the cell outputs instead of the memory states?\n9. The t-SNE visualization of Figures 3 and 5 for KTH and the Radar Echo dataset would be interesting. Are the modes as clearly separated?\n10. There’s a broken figure reference in Appendix B.3\n11. I might have overlooked it, but I didn’t see a comparison of ModeRNNs with different numbers of ModeCells. I can imagine you played a bit with this hyperparameter, I’d be interested to know your observations.\n12. The method for selecting hyperparameters is not described.\n13. How do training times compare to ConvLSTM, RIM, and other architectures?\n14. No discussion of limitations\n15. Do you have initial results on datasets with a larger diversity of spatio-temporal modes (e.g. Human3.6M or KITTI)?\n\nI have updated my score as my concerns are mostly addressed.",
            "summary_of_the_review": "Overall, interesting work. However, for stronger recommendation of acceptance some of the points above should be addressed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper defines a phenomenon, spatiotemporal mode collapse in the training of unsupervised predictive models. They propose an RNN-based approach to learning structural hidden representations in temporal data. The proposed idea was experimented with and compared with respect to the convolutional LSTM baseline and several other temporal modeling methods (i.e., RIM or Conv-TT-LSTM).",
            "main_review": "**Strengths:** \n* The paper presents a novel temporal model to capture spatiotemporal structures in the data and perform better video prediction. The quantitative experiments were performed on three datasets, mixed moving MNIST, KTH action, and Radar Echo datasets.\n\n**Weaknesses:**\n* The cell of the proposed ModeRNN is quite similar to an RNN cell with multi-head attention. One relevant work using a similar idea was not cited. Further, it performs better or on par with the proposed approach in the KTH action dataset.  \n   Lin et al., \"Self-Attention ConvLSTM for Spatiotemporal Prediction,\" AAAI 2020, https://doi.org/10.1609/aaai.v34i07.6819\n\n* In generative models, mode collapse means the generated samples being identical or very similar to each other. It may be caused by the imbalanced distribution of the training set or dependent structures and bias in the data. The motivation of spatiotemporal mode collapse is valid, however, the examples and used datasets are not enough to represent. For instance, in moving MNIST or other datasets do not contain any imbalanced or entangled factor of variation that will cause mode collapse.\n\n* Even though it was argued on the contrary in the related work, the proposed work is highly related to feature disentanglement. PhyDNet (Guen & Thome 2020) is cited but did not included in the moving MNIST or other dataset results.\n\n * How were the previous methods in Table 2 and 4 compared? Did you retrain/implement or they were taken from the reference papers? There are several confusing points: for instance, PredRNN's MSE on Radar dataset was reported 44.2 in the reference paper, but it is84.2 in Table 4. \n\n* What is \"mixed moving MNIST\"? In reference works, mostly moving MNIST (either freshly rendered or the version in Shrivastava et al. 2015) is being used.\n\n* Several typo issues in the entire text (for instance, all titles \"Resuls\"->\"Results\").\n ",
            "summary_of_the_review": "I think the proposed problem of spatiotemporal mode collapse was not covered and described clearly. This makes it difficult to understand where the contribution of the proposed method comes from and what it improves. Spatiotemporal slots idea reminds the attention modules on the temporal data and leads to a weighted fusion of temporal weights. It is very similar to self-attention ConvLSTMs. Mainly the introduced problem is not clear and also approach is limited in novelty. These are the points affected my decision.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a mechanism to reduce spatiotemporal mode collapse in unsupervised predictive learning. To achieve this goal, the proposed method is built upon the idea that different latent modes in the same data domain should share a set of hidden representation subspaces, which can be represented with various compositional structures based on the features in each subspace. Experimental results show improvement over simpler baselines such as PredRNN or ConvLSTM.",
            "main_review": "- The first property mentioned in Introduction section seems a bit simple. Why one would expect each sequence correspond to a single spatio-temporal mode? While this may be true for very simple scenarios, e.g., moving mnist or KTH, for more realistic scenarios, which may not hold.\n- Authors made a claim multiple times that they have demonstrated the effectiveness of their approach in dataset with highly mixed visual dynamics. However, I disagree that KTH or Moving MNIST represent such scenarios. Moreover, the training set of such dataset is very small given the complexity of the proposed method.\n- The baselines authors compared their approach to don't represent the SOTA or even rather old but strong approaches, such as (Kalchbrenner et al., 2017). Comparing against ConvLSTM or PredRNN is far from enough for ICLR 2022.\n- Regarding the method, I'd see it as a good engineering effort of combining existing methods rather than proposing a novel one, with new insights.\n",
            "summary_of_the_review": "Although this approach seems to be a good engineering effort to come up with an effective model that works reasonably well on very small-scale datasets with relatively low complexity, I believe the novelty of this method is very limited. Moreover, comparison to SOTA and evaluation on larger, complicated datasets are missing.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}