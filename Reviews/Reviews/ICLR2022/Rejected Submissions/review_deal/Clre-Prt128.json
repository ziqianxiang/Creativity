{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The reviewers in general agree that the proposed complex valued DP method is interesting and novel. However, there are two key concerns due to which the paper might not be ready for publication at ICLR: a. the key technical contribution of the work is not clear, as the methods seem relatively straightforward extension of real valued DP methods to complex valued domains. \nb. More importantly, the experimental results (and hence the motivating applications) are not convincing and do not strongly support the claims of i) complex data provides more flexibility and hence provide better model, ii) proposed method is accurate. \nFor example, the accuracy numbers for SpeechCommands even without DP seem quite low. For example, standard methods like matchboxnet for keyword detection have accuracy numbers in the range of 97%. While the work considers a subset of keywords, but it would be important to show how the standard methods work on this dataset. If the gap is this large, then the case for using complex valued datasets itself is weak. \nSimilarly, on CIFAR10 it seems like that the considered architecture is quite poor as the accuracy is just ~80% while most standard architectures get >93% on the dataset. So the experiment claims of the paper might not hold for practically relevant architectures."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a complex-valued DP-SGD mechanism, named \\zeta-DP-SGD, to train privacy-preserved neural networks on complex-valued data.  In particular, the authors introduce a complex Gaussian mechanism with DP and Renyi-DP properties, \\zeta-DP, to extend DP to the complex domain. \\zeta-DP allows the re-use of prior theoretical results and software implementations. ",
            "main_review": "Pros: This paper is well structured and its motivation is clear. The experimental results show that the proposed algorithm is possible to achieve high utility under tight privacy guarantees.  \n\nCons: More significant results are needed. My main concerns are as follows:\n\n- On the theory side, the idea seems too natural. It would be better if authors can provide a tighter bound for the privacy loss.\n- On the experiment side, to make the results more convincing, please provide more comprehensive experiment results, such as accuracy for differen",
            "summary_of_the_review": "Although the privacy-preserved complex-valued DL is an interesting problem, Iâ€™m not sure this paper reports enough contribution for pushing the development of this research field. The theory for complex Gaussian Mechanism seems to re-use the prior theoretical results. The used gradient computation method based on Wirtinger calculus is similar to the previous works on complex-valued deep learning.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a new framework, which extends differential privacy to complex-valued functions. The authors name this framework $\\zeta$-DP and introduce their main privacy mechanism, the complex Gaussian mechanism. The authors also show how to adapt the private gradient descent algorithm into a private algorithm to minimize real-valued functions, defined on the complex plane.\nIt is then shown, experimentally, that the new algorithms and notions perform very well on a variety of tasks related to signal processing (and hence to complex-valued functions).\n",
            "main_review": "I found the paper to be well written and the problem of  \"complex DP\" well-motivated.\nThe idea of implementing DP-SGD with the complex Wirtinger notion of derivative is particularly elegant and appealing.\n\nHowever, I would like to contend that there the introduced ideas $\\zeta$-DP and complex Gaussian mechanism are not inherently complex.\nIndeed, by considering the identification $\\mathbb{C}^n = \\mathbb{R}^{2n}$. The complex Gaussian mechanism reduces to the standard Gaussian mechanism when the dimension is doubled. The same is also true for $\\zeta$-DP and 'vanilla' DP in real spaces. This is reflected, for example, in the proof of Theorem 1, which is essentially identical to the proof in Balle & Wang (2018) (except the dimension has doubled).\nI would be very happy to be proven wrong on this point and I expect the authors' response.\n\nIn light of the above though, it seems that the main contribution of the paper is to suggest an alternative to DP-SGD, at least when the dimension is even. Since the functions to optimize are necessarily real-valued, otherwise, it's not even clear what is being optimized, the authors suggest using the conjugate of the complex derivative. For real-valued functions, the conjugate is aligned with the regular gradient. Again, I don't find this setting particularly 'complex-valued'. Other than making the gradient smaller by a constant what are the benefits of using the Wirtinger derivative? \nI would at least expect to see some experiments which show that $\\zeta$-DP-SGD on $\\mathbb{C}^n$ outperforms 'vanilla' DP-SGD on $\\mathbb{R}^{2n}$, when optimizing the same function, with the same privacy budget.\nCurrently, the experiments only demonstrate that $\\zeta$-DP-SGD well on a variety of tasks. As the authors note this can be due to the extra information contained in the additional dimensions.\n\nOne final point, which is a matter of personal taste. The title of the paper \"COMPLEX-VALUED  DEEP  LEARNING...\" is not appropriate, and even misleading, in my eyes. Nothing in the suggested framework is specific to deep learning. While the authors do use neural networks (none of which are particularly deep) in their experiments, it is not the main focus of the paper.\n\n\n\n",
            "summary_of_the_review": "The authors suggest a new framework for differential privacy in complex spaces, which works well with complex notions of derivatives.\nHowever, as far as I have gathered, this is not particularly different than the existing notion of differential privacy in real spaces and I was not convinced there is an actual benefit to using the complex derivatives.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a novel and comprehensive direction in private deep learning, namely the complex-valued private learning. A new notion of privacy--$\\xi$-DP, is defined and carefully analyzed. For this new privacy, a new Gaussian mechanism allows novel DP optimizers to be developed. Extensive experiments demonstrate good performance by combining differential privacy and complex-valued deep learning.",
            "main_review": "Overall, the paper is very well-written and insightful. The contents are complete, covering all important aspects of DP deep learning. Especially the complex-valued Gaussian distribution, the choice of L2 over L1 sensitivity, the seamless connection between complex-valued privacy accountant and existing ones, the choice of Wirtinger calculus, etc. are very clear and exciting.\n\nThe definition of $\\xi$-DP is exactly $(\\epsilon,\\delta)$-DP but extended to the complex domain. So this is moderately interesting. The true highlights of this paper are the new Gaussian mechanism (as well as the new DP optimizers) and good motivation of complex-valued deep learning.\n\nAs for the weakness, I think there are 2 points:\n(1) this paper under-sells its strength in generalizability. Although the author discussed other privacy accounting like f-DP, I believe more other accounting like Gaussian DP, Fourier accountant and others should be mentioned. Another key part is the optimizer. If there is no fundamental difficulty, the authors should discuss $\\xi$-DP-Adam/Momentum/Adagrad and new clipping method like global clipping. If there is a challenge to design new complex-valued DP optimizers, the authors should be state clearly the reasons.\n(2) the experiments are exciting but can be improved. First of all, MNIST is too simple even for DP learning. I would expect to understand the performance on CIFAR10. Notice that while DP learning can get 99% on MNIST, it at most gets 70% on CIFAR10 without transfer learning. Also Table 2,3,5 look incomplete to me. Why MNIST has accuracy but these tables have ROC? Please include accuracy and possibly other metrics in all tables.",
            "summary_of_the_review": "The paper is very well-written and insightful. It provides new DP notion, new Gaussian mechanism, new DP optimizer and exciting experiments. Minor weakness exists: the extensions are not well-discussed and experiments can be enhanced.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}