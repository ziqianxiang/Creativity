{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper proposed a flow-based approach FCause to Bayesian causal discovery that is scalable, flexible, and adaptive to missing data. Reviewers were split on this paper and could not reach a consensus during the discussion, and no reviewer pushed for acceptance. After taking a closer look myself, I agree with several of the reviewers that while the core ideas here are interesting and novel, there remain too many unresolved issues that require another round of revision.\n\nI encourage the authors to carefully take in account the reviewers' comments and re-submit this promising work to another ML venue."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a flow-based causal discovery method that is supposed to be scalable (in the experiments d=16,32,64) and able to impute missing values. ",
            "main_review": "The paper builds up on Carefl (Khemakhem et al 2021), which fits an autoregressive flow model using the variables' causal ordering, extending it to use a flow-based model for inferring the causal structure. \n\nOne of the issues I have is that it is supposed to provide a more scalable alternative to better understood methods, e.g. constraint based methods like PC or FCI. Both of these methods can easily scale up to hundreds of variables (also since they do not exhaustively search over the space of all DAGs but work on equivalence classes) and provide theoretical guarantees. Moreover it does not provide any identifiability result, showing that the learnt graph is indeed causal. Given the approximated posterior, this cannot be directly derived from standard score-based methods.",
            "summary_of_the_review": "The paper seems to be a relatively incremental contribution, that is not embedded in the related work on causal discovery and does not provide any theoretical guarantee that one would learn a causal graph. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to combine the continuous optimization-based causal discovery approach from notears with flow-based function learning. An extension is given to data missing (completely) at random. Also, a unifying framework is presented facilitating the comparison and exchange of ideas between different continuous optimization-based causal discovery methods, viewing all in terms of flows.",
            "main_review": "Strengths:\n\n- The method presented in the paper is scalable and empirically gives good results in various settings.\n\n- All aspects of the method are founded on theoretical developments in many earlier papers.\n\n- The developments are also used to state a framework that makes it much easier to state the relations between different algorithms in prior work.\n\nConcern:\n\n- notears turned out to have a particular weakness to different noise variances. Could you discuss possible weaknesses that your proposed method might have?\n\nMinor points:\n\n- Final sentence in section 2, \"better properties\" - better than what?\n\n- page 3: double \"to\"\n\n- page 5: double \"to\"\n\n- page 9: \"practitioners\" -> \"practitioner's\"",
            "summary_of_the_review": "I think this is a strong paper, both for the main result as well as for the unifying framework in section 5. My only concern is that the proposed method has many \"moving parts\" making it hard to see how it might or might not succeed in particular situations. But the empirical results are very strong, so that I recommend acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This  paper  focuses on causal discovery,  especially  on learning DAG using continuous optimization methods. \\\nIn the nonlinear additive noise setting, the authors propose a flow-based method, called FCause, to optimize the log-likelihood of the observed variables.  ",
            "main_review": "The authors consider a fundamental problem of learning directed causal graphs with nonlinear continuous data.  They propose a scale causal discovery method based on the flow, variational inference, GNN, and Notears.  Their method can handle high dimensional and missing data.   \n \nHowever, there are some concerns / suggestions:\n\n1. It's not clear to me whether the proposed optimization formulation (Equation 6 or 10) can always render the highest score? I notice that it does not restrict the distribution of noise z. Is there any theorem to support it? Please clarify it.\n2. I strongly recommend that the authors should give a summary, such as an algorithm, to show the optimization process.  \nRegarding the unified flow-based formulation (lemma 2). \n3. As far as I know, in the additive noise setting, the log-likelihood of observed variables can be converted into the log-likelihood over the noise estimations, such as in the following paper. This is not surprising to me though the author provides another strategy to prove it.\n \nCai R, Qiao J, Zhang Z, et al. Self: structural equational likelihood framework for causal discovery[C]//Thirty-second AAAI conference on artificial intelligence. 2018.\n \n4.  The authors should explain why the metrics of the other methods cannot perform well. For example,  Notear perform incredibly bad, why?  \n5. More experiments are needed, report based on only four data sets is unconvincing.   \n\nminor typos:\n\nEquation 3: no symbol description for tr,e.\n \n \nIf the authors sufficiently address the mentioned concerns, I am happy to change my assessment.",
            "summary_of_the_review": "The authors address an important and hard problem in learning DAG. The method is a novel and interesting contribution to the continuous optimization literature \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a general flow-based approach to learn DAGs from data which provides a unified view of existing continuous optimization methods for structure learning. As a side benefit, the authors demonstrate that the proposed method could naturally be modified to handle missing data. The authors provide empirical studies to show that their proposed method outperforms the other baselines.",
            "main_review": "### Pros\n- The paper is well written and easy to follow.\n- The paper develops a unified fiew of existing continous optimization methods for learning DAGs, which provide insights into how those methods relate to one another.\n\n### Cons\n- In terms of methodology, the idea of normalizing flow has been explored by [1]. I would encourage the authors to include explanation about the difference between their work and [1].\n- Also, the methods of using gumbel-softmax and binary matrix have been explored by [1, 2], which may be worth mentioning and comparing to.\n- Regarding the experiments, it would be better to include more nonlinear/nonparametric baselines, such as GES with generalized score [3], PC with kernel-based test [4], DAG-GNN [5], CAM [6], and [2].\n- The results of GraN-DAG seem to be much worse than those reported in the original paper. Did the authors use the preliminary neighborhood selection and pruning steps (Appendix A.3 in [7])?\n- It would be better to include the structural intervention distance [8] that measures the accuracy of interventional queries and could be more informative.\n- The proposed $f_i(x)$ with graph neural network seems to be restrictive, since it assumes a causal additive model (i.e. elementwise nonlinear relationship $g_j$) with a \"post-nonlinear\" operation $h_i$. However, it is surprising that this function works well with the Gaussian process dataset considered in the experiments, although the functional assumption is not met. Could the authors provide explanation on this?\n\n### Other comments\n- $h$ is used for different purposes, i.e. neural network and algebraic characterization of DAGs.\n- I am not sure if it is appropriate to claim about causality without further conditions. As the authors know, previous works [5, 7, 9] focus on learning DAGs instead of claiming about causal discovery. Perhaps could the authors include some explanation on this?\n- The graph neural networks used in Section 4.1 seem to be similar to [5, 10] which fall under the proposed unfied view; the authors could include comparison with them in Section 5.\n\n### References\n1. Differentiable Causal Discovery from Interventional Data, 2020.\n2. Masked Gradient-Based Causal Structure Learning, 2020.\n3. Generalized Score Functions for Causal Discovery, 2018.\n4. Kernel-based Conditional Independence Test and Application in Causal Discovery, 2011.\n5. DAG-GNN: DAG Structure Learning with Graph Neural Networks, 2019.\n6. CAM: Causal additive models, high-dimensional order search and penalized regression, 2014.\n7. Gradient-Based Neural DAG Learning, 2020.\n8. Structural Intervention Distance (SID) for Evaluating Causal Graphs, 2015.\n9. DAGs with NO TEARS: Continuous Optimization for Structure Learning, 2018.\n10. A Graph Autoencoder Approach to Causal Structure Learning, 2019.",
            "summary_of_the_review": "The paper develops a unified fiew of existing continous optimization methods for learning DAGs, which provide insights into how those methods relate to one another. However, I have some concerns about the experiments, and am willing to increase my score if the authors could address my comments.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a flow-based causal discovery method to learn nonlinear causal DAGs with potential missing values. The proposed approach is based on variational Bayes and continuous optimization approach. ",
            "main_review": "Strengths:\n\n•\tFirst paper to combine VB with NOTEARS\n\n•\tEmpirically, it does well in simulations\n\n•\tIt can handle missing values and allows nonlinear functional relationships\n\nWeaknesses:\n\n•\tThe motivation of using DAG is unclear. DAG is a special case of directed cyclic graphs (DCG). One important advantage of DAG over DCG is its simple factorization, which leads to more efficient computation than DCG. With the NOTEARS framework, this advantage seems largely lost because during the training, at least in the early phase, the graphs may be not be acyclic. Then why one even makes an often unrealistic assumption of acyclicity to begin with? In a typical run of the proposed algorithm, is the DAG factorization ever exploited? If it is exploited in iterations when the graph is acyclic, then typically in how many iterations the graph is acyclic? In summary, acyclicity is, in my opinion, a modeling restriction, not a feature. Is the proposed algorithm faster than the same algorithm without the h(A) penalty (which is both simpler computationally and more general in terms of modeling)? \n\n•\tThe proposed approach seems doing very well in various simulations but doesn’t do well in the real data, which questions the real practical advantage of the proposed method. Perhaps including more real data analyses/comparison can help support it. \n\n•\tIt would be helpful for readers if computation/algorithm/implementation details are given. \n\n•\tNot clear why Carefl is relevant to this paper.  Maybe I missed something, but the only place Carefl is mentioned in the method description is around Equation (6). But it looks to me just a standard definition of a nonlinear SEM? See e.g., Equation (4) in “On Causal Discovery with Cyclic Additive Noise Models”. \n\n•\tScalability is claimed but not tested: the largest dimension investigated is 64. \n\n•\tAlthough the proposed method can handle missing data, missing at random is a rather strong assumption. Under this assumption, practically any Bayesian methods can easily impute the missing values. \n\n•\tIs Lemma 1 a well-known result for any DAG? The Jacobian can be transformed to an upper or lower triangular matrix with unit diagonals by permuting the nodes and hence the determinant is trivially 1. I might have missed something here. \n\n•\tWhat is the rationale of imposing prior on A but not on theta. This choice loses some uncertainty in addition to that lost by using Variational Bayes. ",
            "summary_of_the_review": "In summary, while the paper has some interesting idea of combining variational Bayes algorithm with NOTEARS (continuous optimization) for nonlinear DAG learning, the significance of the contribution is not yet strongly supported theoretically or empirically. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}