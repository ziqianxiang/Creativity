{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper presents a gradient-based hyperparameter optimization method, wherein a differentiable reparameterization is proposed for various popular CNN hyperparameters including kernel size, number of channels and hidden layer size.\n\nAll reviewers have pointed out the lack of novelty (such reparameterizations are standard) and lack of convincing experiments.\n\nThe authors didn't write any rebuttal.\n\nOverall, there is a large consensus among the reviewers that this paper is not ready for publication at ICLR."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "\nThis paper introduces a method for hyper-parameter optimization (HPO) for deep neural networks. Its main idea is to replace hyper-parameters by trainable parameters, which can be included in the training process of the network itself.\nThe proposed method is applied to hyper-parameter tunning for two types of networks, namely CNN and FNN on two datasets MNIST and SVHN. It is also compared with several baselines.\n",
            "main_review": "Overall, this paper is a contribution in the right direction and has potential to cut down the hyperparameter optimization time greatly. The paper provides an elegant way to determine the optimum hyperparameters via a differentiable scheme.\n\nHaving said that, I have several concerns about the experiments.\n1.\tAnother method which is closely related is Darts (Liu et al (2019)). Why it is not selected as a baseline in the comparisons?\n2.\tTwo datasets MNIST and SVHN are used for the experiments. Both of them are similar and contain images of digits. It would be useful if the authors could show the performance of their method on other diverse datasets like CIFAR-10 or CIFAR-100.\n3.\tSection 4.2.3: the authors report the accuracy of optimized networks, but it is not clear if that is on the validation set or on the test set.\n4.\tTable 1: I don’t see much the benefit of the proposed method DHPO. Take the version DHPO+ as example. Compared to BLANK (i.e. train the network without hyper-parameter tunning), DHPO+ is just slightly better than BLANK, there is no significant difference. For example, 98.71% vs. 98.27% for MNIST and 85.05% vs 84.95% for SVHN.\n5.\tI am not sure why the authors use the version DHPO* (running 10 times) of their method when all the other methods are run only 3 times? Is this a fair comparison? If all of the methods including DHPO+ run the same 3 times, DHPO+ is not better than the baselines on datasets SVHN, Wine, and Car.\n6.\tFinally, it would be nice if the authors could apply their methods to neural architecture search using standard benchmarks e.g. NAS-101 etc. This will truly show the effectiveness of their method.\n\nMinor Comments:\nThe paper seems to be written in a rush. The text and presentation could be significantly improved. \n- the citation of Goldberg in the introduction section is not correctly done (missing publication year).\n- space missing after BO and the following parenthesis, HB and the parenthesis, DHPO and the parenthesis.\n-random search is cited incorrectly as (ran, 2012)!\n- One page 2: incomplete sentence “While Darts can select the hyperparameters …”\n",
            "summary_of_the_review": "To summarize, this paper addresses the HPO problem by transforming the hyper-parameters to trainable parameters and including them in the training process of the network. This approach can speed up the optimization time and allow the hyper-parameter optimization to happen in one single training session. Overall, the idea is interesting but the experiments are not convincing.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors proposed a differentiable method to optimize neural networks and hyperparameters simultaneously like DARTS. And, they used a new parameterization to represent the original discrete hyperparameters. They claimed that it could eliminate the impact of rounding.",
            "main_review": "Strengths:\n1. On MNIST and SVHN, the proposed method has the best top1-error and the lowest STD. It also has the fastest convergence rate.\n\nWeaknesses:\n1. The authors claimed that they proposed the first model to solve hyper-parameter optimization in only one training session. However, they also said Darts could select the hyper-parameters in a single training session. Even if their method handled the argmax problem of DARTS, SNAS [1] has also considered this problem by the Gumbel-softmax trick. Hence, using differentiable methods is not their contribution.\n\n2. The proposed theorems are trivial and meaningless. The authors try to handle the cases where multiple candidates occupy similar weights in DARTS. So, the theorem should prove that. At least, it is necessary to prove the three key characteristics in Section 3.1.3. \n\n3. The paper didn't explain their method well. They just introduced their reparameterization. What is its differential? It is not a trivial problem [2]. Why it can solve the DARTS problem? It cannot be explained just in examples. Moreover, there are many writing issues and typos. For example, \n(1) Formula 1 should be written in a sentence.\n(2) The sentence \"It is selecting an activation for the fully connected layer and supposing Relu is the best choice.\" is weird.\n(3) What does the sentence \"Firstly, we define found sufficient conditions as a new relation between \\alpha and \\theta.\" mean?\n\n4. The experiments only used small data. The performance of the proposed method is not clear on the big data, like ImageNet. At least, CIFAR10 and CIFAR100 are needed. Further, this method should also work for other tasks not only for classification tasks. \n\n5. The baselines are old and not enough. The baselines of Bayes optimization should include the first prize of Black Box Optimization competition in NeurIPS 2020, HEBO [3]. The SOTA of multi-fidelity methods should include BOHB [4] or BOSS [5].\n\n[1] Xie, S., Zheng, H., Liu, C., & Lin, L. (2018). SNAS: stochastic neural architecture search. arXiv preprint arXiv:1812.09926.\n[2] Franceschi, L., Donini, M., Frasconi, P., & Pontil, M. (2017, July). Forward and reverse gradient-based hyperparameter optimization. In International Conference on Machine Learning (pp. 1165-1173). PMLR.\n[3] Cowen-Rivers, A. I., Lyu, W., Wang, Z., Tutunov, R., Jianye, H., Wang, J., & Ammar, H. B. (2020). Hebo: Heteroscedastic evolutionary bayesian optimisation. arXiv e-prints, arXiv-2012.\n[4] Falkner, S., Klein, A., & Hutter, F. (2018, July). BOHB: Robust and efficient hyperparameter optimization at scale. In International Conference on Machine Learning (pp. 1437-1446). PMLR.\n[5] Huang, Y., Li, Y., Ye, H., Li, Z., & Zhang, Z. (2020). An Asymptotically Optimal Multi-Armed Bandit Algorithm and Hyperparameter Optimization. arXiv preprint arXiv:2007.05670.\n\n",
            "summary_of_the_review": "As mentioned in Main Review, the contribution of the paper is not enough. The novelty is not strong, since it is not the first model. The theorems are trivial and meaningless to describe the reparameterization, since the specific mapping is given. The experiments are not comprehensive enough considering the dataset and baselines. Moreover, the method itself is not explained well. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes to tune hyperparameters in a differentiable way by using a modified version of softmax function. It covers the tuning for three kinds of hyper-parameters as examples: channel size, kernel size and hidden layer size. Experiments on MNIST and SVHN (which are small scale datasets by modern DNN standards) show improvements on previous methods. The proposed method itself is reasonable, however, this paper misses many previous works on this topic, which makes it hard to appreciate its novelty.",
            "main_review": "Using a modified version of softmax to select channel size and kernel size seems novel which is not achieved by DARTS. However, there exist previous works on neural architecture search which can do this (e.g., Wan, Alvin, et al. \"Fbnetv2: Differentiable neural architecture search for spatial and channel dimensions.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.). Without comparison with these previous works, it is hard to appreciate the contribution and novelty of this proposed method. \n\nBesides, the experiments are conducted only on MNIST and SVHN which are small scale by the standard in this topic. Considering that most papers in neural architecture search report performance on  ImageNet, it is important that the authors also report the performance there.",
            "summary_of_the_review": "In summary, the proposed method is ok but this paper ignores important previous works, which makes it hard to appreciate the contribution.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents a gradient-based hyperparameter optimization method, wherein a derivable parameterization is proposed for various popular CNN hyperparameters including kernel size, number of channels and hidden layer size. Experiments show promise in terms of convergence speed when evaluated on simple benchmarks such as MNIST and SVNH, however this is irrelevant because a variant of this method already exists in the literature and nothing new is proposed.",
            "main_review": "Paper strengths:\n+ The reparameterization of hyperparameters to make them derivable seems good. The idea is a good idea, even if already existing in the literature.\n\nPaper weaknesses:\n- This paper is not novel, a lot of existing work already tested this idea and showed that it already works. See the following references:\n\nFranceschi, L., Donini, M., Frasconi, P. & Pontil, M.. (2017). Forward and Reverse Gradient-Based Hyperparameter Optimization. Proceedings of the 34th International Conference on Machine Learning, in PMLR 70:1165-1173\n\nStamoulis, D., Ding, R., Wang, D., Lymberopoulos, D., Priyantha, B., Liu, J., & Marculescu, D. (2019). Single-path nas: Designing hardware-efficient convnets in less than 4 hours. arXiv preprint arXiv:1904.02877.\n\nZhang, C., Ren, M., & Urtasun, R. (2018). Graph hypernetworks for neural architecture search. arXiv preprint arXiv:1810.05749.\n\nTwo of those references involve hypernetworks, with the same idea as proposed in this paper, which is define a larger network containing all subnetworks embedded within the hyperparameter search space.\n\n- The selected benchmarks are outdated.\n- Going back to the first point, the methods selected for comparison with the proposed approach are also outdated.\n- The paper is poorly written, hard to read and the ideas are not clearly stated.\n",
            "summary_of_the_review": "This paper proposes ideas that are already existing in the literature, therefore it should be rejected. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}