{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This article proposes a novel uncertainty quantification method, formulating the problem as a Bayesian inference problem. Instead of training multiple ensemble models through MAP optimisation, as in ensemble methods, the proposed approach tries to learn a mapping function between the prior distribution and the posterior distribution of model parameters. This avoids the complex training of ensemble models and achieves better efficiency. \n\nThe approach is novel, and the problem of importance. The paper however suffers from a number of weaknesses:\n* Some theoretical results would need to be made mathematically more rigorous\n* The presentation is unclear and confusing in some places\n* Empirical results are not reproducible due to the lack of details\nAlthough the authors clarified some of the points raised by reviewers in their response, the paper in its current form is not ready for publication, and I recommend rejection."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "NN ensembles are useful models to estimate epistemic uncertainty in the predictions. Since their computational cost is generally high, they introduce Generative Posterior Networks (GPNs) as a generative NN model to approximate the posterior distribution of the inference process. Under not very restrictive assumptions, GPNs work as well as previous methods, if not better. This is shown through extensive experiments, especially in classification problems.",
            "main_review": "This paper introduces a new proposal to make use of ensemble NN methods, GPNs. With the approximations imposed, GPNs perform well in the experiments shown. Although some points about the text could be improved and there are certainly some potentially important drawbacks, it seems an interesting contribution to the field. There seems to be a couple of tests that need to be done, but the results point in the right direction. \n\n# Pros:\n\n* Allow to obtain samples in a more scalable fashion from NN-ensemble-like models. \n* The results obtained support the proposed method in a solid manner when compared to previous methods. \n\n# Cons:\n\n* The conditions imposed on the method makes its predictions behave in a Gaussian-like manner. This could be fine in cases where data does not exhibit complex behaviour, but could be a problem for cases with multimodality, heteroscedasticity, etc.  \n* Notation can be improved to make the description of the method clearer. \n\n\n# Other questions / comments:\n\n* It could be argued that, contrary to what is pointed out in the introduction, VI or GP-based methods can be easy to work with. In fact, if some assumptions are feasible or the data behaves in a certain manner, they can be pretty illustrative and informative models. I would suggest rewriting this part of the introduction, pointing out more clearly why NN ensembles may be more interesting in some cases. \n\n* GPs are a model class, not an inference method. \n\n* I would suggest conducting an experiment on the scalability of the method, measuring convergence times with big datasets. This would help us gauge correctly how well does the method perform in this matter as well.  \n\n* While it is true that basic BNNs degrade their performance in higher dimensionality settings, there have been important advances in the last years to prevent this from happening, especially when using deeper or wider networks. This can be said for the function-space-based optimization systems, such as those of [1,2].\n\n* As a mere suggestion, I find the notation of the data-likelihood and parameter-likelihood PDF unnecessarily complex. I would either simplify this to improve the clarity of the text or explain the notation further (not just relying on referenced sources). In general, through the paper, the notation could be clearer. \n\n* Figure 3 can be improved. I suggest trying alternative visualizations s.a. combining different boxplots into one using colours, or at the very least increasing the font (in the CIFAR-10 case) to better show the results.  \n\n* Have the authors conducted any tests in regression datasets? It could be very interesting to see the resulting behaviour in those cases. \n  \n ## References\n \n [1] Ma, C., Li, Y., and Hernández-Lobato, J. M. (2019). “Variational implicit processes”. In: International Conference on Machine Learning, pp. 4222–4233.\n\n [2] Sun, S., Zhang, G., Shi, J., and Grosse, R. (2019). “Functional variational Bayesian neural networks”. In: International Conference on Learning Representations.\n ",
            "summary_of_the_review": "The contribution is novel and significative, although there are a couple of things that should be addressed in the revised version of the paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No potential ethics concerns.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper built upon Pearce et al 2020 that shows regularizing parameters to prior distributions in ensemble of NNs results in the approximate Bayesian inference, aiming to improve computational efficiency of the previous method. Specifically, in the method in Pearce et al, sampling one parameter from posterior requires to find a MAP solution for some anchor point, which can be computationally demanding. Therefore, the authors propose to learn MAP function from an anchor point to MAP solution, which removes the required MAP optimization procedure. \n",
            "main_review": "Pros:\n- The paper deals with an important area of research that provides an efficient way to estimate epistemic uncertainty. \n- The method provides somewhat improved results compared to previous methods. \n\nCons:\n- Even though this work is built upon Pearce et al 2020, it largely hinges on the previous work in the manuscript. Specifically, it is hard to understand without knowing several definitions and results in Pearce. So, I’d recommend authors to provide self-contained backgrounds and results in ch.3. Also, the assumed relationship that parameter-likelihood is equal to data-likelihood (Eq 3) seems to be a special case, but there is no comments on that. \n- It is unclear why the authors exclude the input variables X in the data/parameter-likelihoods. \n- Theoretical results and mathematical states in the paper seem not rigorous. Specifically, in the statements and proof of lemma 1 and theorem 1, the authors simply assume equality between two probability distributions in the limit. This can have many potential problems, such as limit/integral interchangability. It would be better to write and develop their theoretical results in terms of convergence behavior. \n- I have also concerns regarding experiments. First of all, the experiment section lacks important experimental details that make reproducing the results in the paper hard. Also, while the primary motivation for this research is to remove the MAP optimization procedure, there is no comparison to baselines in terms of computational complexity. The last paragraph in page 7 is not reasonable since having many unlabeled samples in practice cannot be the reason why the authors have access to test samples during training. In addition, the CI-ALL measure (that requires the same answers to all members of ensemble) seems unnatural since the power of ensemble comes from diversified predictions from individual members and mostly their majority voting results are of interest. Finally, I cannot understand how the authors could compute the classification accuracy for OOD, unknown class samples given fixed architecture. \n",
            "summary_of_the_review": "The paper is not self-contained. Also, even though their motivation for the improvement is to improve the computational efficiency, the experiments did not provide any results on that. There are some issues in the mathematical statements and proof thereof. \n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this work, the authors introduce generative posterior networks (GPN). GPN is a single network that approximately produces samples from posterior over neural network parameters; this is in contrast to the standard approach of training N individual neural networks.",
            "main_review": "# Strengths\n\nThis paper provides a nifty alternative to ensembling, which significantly reduces the computational complexity as only one network needs to be trained now. Moreover, as opposed to trying to get the posterior over parameter space the authors compute the posterior over function space; this drastically reduces the number of dimensions the GPN needs to predict, making the problem much easier to learn.\n\n# Weaknesses\n\nWhile I am a fan of the approach conceptually, I think there are a number of weaknesses. \n\nFirstly, I think the related works section is lacking. I think most of the discussion on MCMC and VI could have been cut out. More time should have been focusing on other prior works on ensembling neural networks.\nAs the approach can be seen as a form of hypernetwork[https://arxiv.org/abs/1609.09106], I think this should have definitely been discussed by the authors. There is also other work--similar to Pearce et al--showing how to approximately sample from the posterior of neural networks using the NTK which is also pertinent to discuss[https://arxiv.org/abs/2007.05864].\n\nNext, I am confused by section 4.1 as a whole. To be specific and ensure that I am on the same page as the authors, I am going to rewrite the problem formulation (i apologize in advance for the lack of boldface. for some reason it isn't working properly for me). \n\nWe have training data, \n$$ (x_{obs}, y_{obs}) = \\{ (x_{obs}^i, y_{obs}^i) \\}_{i=1}^N $$\n\nwhere the following relationship is assumed\n$$ y_{obs}^i = f(x_{obs}^i; \\theta) + \\varepsilon_i, \\quad \\epsilon_i \\sim \\mathcal{N}(0, \\sigma_\\epsilon). $$\n\nIn supervised Bayesian learning, the goal is to compute the following posterior where I will first write the posterior over the parameters\n\n$$ p(\\theta \\vert x_{obs}, y_{obs} )  \\propto P_{prior}(\\theta) \\prod_{i=1}^N P_{like} (y_{obs}^i \\vert x_{obs}^i, \\theta)  $$\n\nOne can also compute a posterior over functions as well. \nLet \n$$ \\tilde{y}^i_{obs} = f(x_{obs}^i; \\theta) $$\nthus we can express the relationship between the inputs and outputs as \n$$ y_{obs}^i = \\tilde{y}^i_{obs} + \\epsilon_i. $$\n\nLet $ \\tilde{Y} = (\\tilde{y}^1_{obs}, ..., \\tilde{y}^N_{obs}) $, we now want to compute the following posterior\n$$ p(\\tilde{Y} \\vert x_{obs}, y_{obs}) \\propto P_{prior}(\\tilde{Y} \\vert x_{obs}) \\prod_{i=1}^N P_{like}(y^i_{obs} \\vert \\tilde{y}^i_{obs}) $$\nIn Lemma 1, the authors prove that in function space, the likelihood is normally distributed with respect to $\\tilde{Y}$. This is trivially true by construction  i.e., $P_{like}(y_{obs}^i \\vert \\tilde{y}^i) = \\mathcal{N}(y_{obs}^i \\vert \\tilde{y}^i, \\sigma_\\epsilon)  $. When operating in parameter space, the likelihood is complicated but the prior is easier; when moving to function space, this switches.\n\nBesides that, Lemma 1 also makes no sense to me. In the lemma, they take a discretization of the input space but I see no reason to do that as the likelihood is only defined on the inputs provided in the training data. This is also true for theorem 1 as well. In my opinion, section 4.1 could have been entirely removed, which would have left more space for more experiments.\n\nConcerning the experiments section, it was disappointing to not see an experiment similar to Pearce et al, comparing the true posterior to the posterior obtained by GPN on a simple toy dataset. As many assumptions were made, I think it is very important that the approach was empirically verified in a setting where the ground truth can be obtained. Next, I don't understand experiment 2. Regardless of the learning method used, the posterior studied in this paper revolves around using supervised data. I have no idea how the authors were able to use the unlabeled data nor does it seem fair to me.",
            "summary_of_the_review": "While I like the idea, I think more work needs to be done on the structure of the paper and the experiments. I think section 4.2 could be cut, allowing for more space to do experiments and a more fleshed-out discussion section.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper develops an uncertainty quantification method as an extension of the Bayesian ensemble method. Instead of training multiple ensemble models through MAP optimization, the proposed method tries to learn a mapping function between the prior distribution and the posterior distribution of model parameters, which avoids the complex training of ensemble models and achieves better efficiency. Due to the high-dimensional parameter space, the authors propose to learn the mapping function in the low-dimensional output space with theoretical justifications. The experiments include image classification and Bayesian optimization, where authors compare the proposed method with Bayesian ensemble and MC-dropout in terms of uncertainty estimation.",
            "main_review": "Strengths:\n \n(1)\tThe proposed method proposes a novel idea to build a mapping function from prior to posterior. It avoids ensemble training and achieves reasonable uncertainty quantification results. \n\n(2)\tThe proposed method addresses the high-dimensional parameter space issue by constructing the mapping function in low-dimensional output space.\n\nWeakness:\n\nFor the methods: the theory part is not well-written. Some equations are confusing and may have possible mistakes.\n\n(1) The derivation of Eq. 13 and Eq. 14 is confusing. Why the second term $\\log P_{anc}(g(x_{sample,\\phi,\\theta_{anc}}))$ in Eq. 13 includes the posterior sample $g(x_{sample,\\phi,\\theta_{anc}})$ in the prior distribution? When looking into the details in Appendix.A.3, there might be some mistakes. For example, why $\\log P_{anc}(g(x_{sample},\\phi,\\theta_{anc}))$ and $\\log P_{like}(y_{obs}|g(x_{sample},\\phi,\\theta_{anc}))$ both equals to $\\sum_{i} \\log N(y_{obs}^i | g(x_{sample}^i,\\phi,\\theta_{anc}),\\sigma_{\\epsilon})$\n\n(2) The proposed method does not clearly explain how to obtain the parameters of the prior distribution $P(\\hat{Y})$, i.e.,  $\\mu_{\\hat{Y}},\\Sigma_{\\hat{Y}}$.\n\n(3) When we use low-dimensional $z$ for sampling in Eq. 15, there is no explicit equation for $L_{reg}(z_1,z_2,...,z_k)$. The details of such regularization should be provided and there is no theoretical justification of why we need to add $L_{reg}(z_1,z_2,...,z_k)$ to keep $z_1,z_2,...,z_k$  roughly normally distributed.\n\n(4) No training details are provided.\n\nFor the experiments:\n\n(1) Could you explain why the ensemble 10 methods are significantly worse than the proposed method as the proposed method is supposed to only improve efficiency of the Bayesian ensemble method?  It also does not make sense that the MC dropout method outperforms the ensemble method.  Are there any theoretical justifications for such dramatical improvement in accuracy compared to anchor-regularized neural networks ensembles (Pearce et al., 2020)?\n\n(2) For the image classification tasks, the paper evaluates the proposed method based on the out-of-distribution prediction accuracy and the confidence interval. The confidence interval for classification tasks is confusing. The authors should provide more analyses of why those metrics can be used for evaluating uncertainties.\n\n(3) The author should provide some analyses for the convergence of the model. The model may be hard to train especially when the mapping function $g$ is not powerful enough. Additional ablation studies should also be provided such as the sensitivity of hyperparameters.\n\n(4) No experiments for comparing the efficiency and complexity of different methods. From the introduction section, it seems that a major contribution of the paper is to improve efficiency. \n\n\n\n",
            "summary_of_the_review": "Overall, this paper proposes interesting ideas by building a mapping function from the prior to the posterior. However, the authors did not explain their ideas well and there might be some mistakes in their formulation. The author may also need to provide more detailed empirical results and corresponding analyses.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}