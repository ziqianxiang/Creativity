{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper considers input-dependent randomized smoothing to obtain certified robust classification. The main contribution is the derivation of necessary conditions on how the variance of the smoothing distributions (assumed to be spherically symmetric Gaussian distributions) has to change to achieve certified robustness. All reviewers like this result, as it provides guidance on designing input-dependent smoothing, which is an interesting result for the community and certainly helps future research.\n\nOn the negative side, the smoothing method derived based on the theory provides little (if at all) improvement in practice, it cannot be scaled to higher dimensions, it does not address the problems it claims to address (the \"waterfall\" effect, as also admitted by the authors in the discussion), and the presentation should be significantly improved. \n\nThe paper received mixed reviews. While I think that the presented theoretical results are useful and interesting, the problems mentioned above make me to side with the negative reviewers and suggest rejection of the paper at this point (although this was not an easy decision).\n\nWhile this is only lightly touched in the reviews, I strongly recommend the authors to make the presentation of the theoretical results more comprehensible. It is quite hard to follow the paper as notation is introduced continuously in an ad-hoc and confusing way (e.g., in the proof of Theorem 2, $a$ denotes $\\delta$ and $\\|\\delta\\|$), and things are often not adequately defined (e.g., the certified robust radius is not defined formally; in Lemma 1, $x$ is undefined and used for $x_0$ as well as a free parameter, $\\chi_N^2$ is only implicitly defined, etc.)"
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies the input-dependent randomized smoothing technique - it adds isotropic Gaussian noise with different variance on different input x to give robustness certificates.  The hope is that this can perform better than applying a uniform noise.  The main theoretical claim in this paper is that when adding input dependent isotropic Gaussian noise, the variance of the noise should not change too much among different inputs. Based on this, the authors propose an input dependent smoothing mechanism based on k-nearest neighbor and evaluate its performance on MNIST and CIFAR datasets.",
            "main_review": "Strengths:\n\n1. The main theorem looks reasonable to me. Despite all the complex equations, I think the intuition is that the worst-case classifier f* when using input dependent randomized smoothing has a spherical structure, and its volume shrinks in high dimensional space.\n\n2. The proposed input-dependent randomized smoothing standard deviation function $\\sigma(x)$ is novel and different from existing works (but unfortunately the improvements are quite small).\n\nWeaknesses:\n\n1. The theoretical study is limited to the setting where isotropic Gaussian noise is applied. However, when considering input dependent smoothing, I feel a non-isotropic Gaussian noise would be more useful - for example, on MNIST, it can be quite reasonable to apply a larger noise on the pixels that are black (background) and apply smaller noise on the white pixels. By applying this kind of input dependent smoothing, the limitation of input dependent smoothing found in this paper does not necessarily hold. I hope the authors can discuss more on this more useful setting.\n\n2. One missing factor in the analysis, is that in high dimensional datasets, the distances between the images are actually pretty large: for example, the L2 distance between two ImageNet images from different classes are actually large, grown by the factor of $\\sqrt{N}$ ($N$ is the data dimension).  In terms of smoothing, I think it totally makes sense that when we are certifying a single image, the $\\sigma(x)$ function should not change too much within a small radius $\\| \\delta \\|$. However, since different images are already very far away, we can certainly apply different smoothing $\\sigma(x)$ on different images.\n\n3. The design of $\\sigma(x)$ is pretty naive and the empirical results are very weak - compared to smoothing with uniform noise, the gain is too little.  I feel this is due to the restriction of using isotropic Gaussian noise.  Since the variance of the Gaussian noise is the only thing we can change and they cannot be changed too much, the proposed input dependent smoothing scheme in this paper is not very useful. \n\n4. No comparisons to existing baselines (such as Wang et al. 2021) using input dependent smoothing were given.",
            "summary_of_the_review": "Overall, I do like the analysis for input dependent randomized smoothing, but I think the overall impact of this work is rather limited. I rate the paper at borderline because I feel the theoretical part of this paper has certain limitations and the proposed procedure is also not very effective in practice and has quite small improvements. To further improve the contribution of this paper, as I mentioned in the \"weaknesses\" above, the authors can study the more useful setting of applying non-isotropic input dependent noise, and develop better $\\sigma(x)$ function to improve smoothing performance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors study input-dependent adversarial smoothing i.e. the setting where the smoothing distribution has noise level $\\sigma$ dependent on $x$. They begin by deriving the robustness certificate in this setting, and show that it suffers from the curse of dimensionality -- in order to maintain non-vacuous certificates in high dimensions, $\\sigma(x)$ must be prohibitively smooth. They then provide empirical experiments using an example function $\\sigma(x)$ on CIFAR-10 and MNIST and show that performance is more or less the same compared to the fixed noise level baseline (Cohen et al. 2019).",
            "main_review": "Strengths:\n\nThe \"takeaway\" from this paper is a negative result (both theoretical and experimental), but I think it is of relevance to the community. The idea of input-varying noise $\\sigma(x)$ is attractive (as the authors note several other papers have recently touched upon this idea), and this paper makes a strong theoretical case for why it's unlikely to outperform Cohen et al's baseline.\n\nWeaknesses:\n\nThe main weakness lies in the fact that this is a negative result -- it doesn't provide any real improvement over Cohen et al. 2019, and the authors acknowledge that this method doesn't scale to modern ImageNet-scale datasets.\n\nExperimentally, the results could use some work. Writing-wise, it would be helpful to more clearly differentiate CIFAR-10 and MNIST in Tables 2 and 3. In Figure 4, the colors orange and blue are swapped between the baseline and the proposed methods when the setting changes between CIFAR-10 and MNIST (this is extremely confusing)! It would additionally be helpful to make clear that the $r=0$ case in Table 2 corresponds to clean accuracy. \n\nIt's unclear how the authors derived Equation (1) -- is it possible that there exists another choice of $\\sigma(x)$ that could yield superior empirical results? \n\nIt seems unlikely that training with a fixed noise level would be the optimal way to train input-dependent smoothing models -- have the authors tried, for example, changing the noise level of the Gaussian at training time to be input-dependent as well?\n\nIt would be helpful to evaluate the proposed Equation (1) on toy simulated datasets in low dimensions, simply to verify that we can indeed see an improvement from input-dependent smoothing. Figure 1 is helpful for building intuition, but I'd be curious in seeing how that result scales with dimensionality $N$.\n\nMiscellaneous:\n\nTypo in Figure 4, \"coparison\".\n\nI'd be willing to revisit my rating if the experiments and writing could be cleaned up substantially.",
            "summary_of_the_review": "This paper presents theoretical and experimental negative results on input-dependent randomized smoothing. \n\nI think it's likely to be of *some* interest to those working on randomized smoothing.\n\nBut the wider community will likely be uninterested by this negative result. \n\n(Additionally the paper could use more work on the experiments and writing in particular).",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work extends the theoretical results from Cohen et.al to the input dependent setup. In particular, the derive the theoretical conditions for the validity of data-dependent randomized smoothing through a similar procedure to the one in Cohen et.al. At last, a function that optimizes the smoothing parameters per input was proposed and validated experimentally on MNIST and CIFAR10.",
            "main_review": "Strengths: \n- The main merit of this work is the theoretical contribution in section 2. It is quite useful deriving the conditions under which the input independent smoothing is theoretically valid.\n- The summary for the steps in deriving the radius in page 3 is neat and useful.\n\nWeaknesses: I will summarize the main points below and detail the reasons behind each of them afterwards.\n1.  The experimental evaluation presented in this work is insufficient. Moreover, the proposed methodology has no notable improvement over the considered baseline. \n2. The writing of this work along with the presented figures can be significantly improved. \n3. There are several parts that need more elaborative discussion. \n\nRegarding the first point:\n- While the motivation of this work is the \"certified accuracy waterfalls\" the proposed objective in Equation (1) does not solve the problem. The results presented in Figure 4 show that the proposed method suffer from a similar but slightly delayed issue.\n- The presented experiments considered the weakest baseline (Cohen et.al) were the effectiveness of the method should be tested with stronger baselines including MACER and SmoothADV and preferably on larger datasets such as ImageNet.\n- While the objective in (1) could arguably increase the value of $\\sigma$ for points \"far from decision boundary\", it is not clear how would it decrease it for points close to the boundary. This point is related to the results on CIFAR10: why does the proposed method perform slightly worse on small radii? Shouldn't the values of $\\sigma$ get adjusted for points nearby decision boundaries such that some improvement is got in that regime?\n\nRegarding the second point:\n- It is not clear why did you need to assume the semi-elasticity bound.\n- There are several notations introduced within text such as $a := \\|\\delta\\|$ and used later without referring to the same variable.\n- The plot in Figure 4 should have consistent colors. An envelop curve over the different values of $\\sigma$ could benefit the comparison.\n\nRegarding the third point:\n- It is not clear how in the special case for when $\\sigma_0 = \\sigma_1$ will the results be reduced to the one in Cohen et. al.\n- It is clear how the objective in (1) could be used during training, but it is not clear how is it used at test time. What is $\\mathcal N_k(x)$ in that case?\n- The discussion in the paragraph below (1) is a little vague. Instances belonging to the same class could significantly differ in the input space (e.g. instance of white dog and instance of black dog). How could these distances relate to the position to decision boundaries?\n\nOther questions:\n- Regarding the curse of dimensionality: While the variation between $\\sigma_0$ and $\\sigma_1$ is restricted by Theorem 4, is it possible to break the input space into small regions where the variation is small within the region but large enough between instances far from each other?\n- Since the proposed method requires knowing $\\mathcal N_k(x)$, How will this neighborhood be obtained in test time?\n\n\n\n",
            "summary_of_the_review": "The theoretical analysis presented in this paper is both novel and of interest. However, there are several weaknesses of this paper that need to be addressed including the experimental setup, the effectiveness of the proposed method, and the presentation.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose an input-dependent randomized smoothing method with non-constant input-dependent $\\sigma(x)$.\n\n### contributions\n\n- The authors generalize randomized smoothing with non-constant input-dependent $\\sigma(x)$.\n- The proposed method successfully addresses some intrinsic limitations/flaws in the previous (input-independent) RS.",
            "main_review": "### strengths\n\n- It is important to build the input-dependent RS methods.\n- While there have been some previous works, most of them are mathematically invalid (as the authors argued).\n- It is challenging to formulate such input-dependent RS with a valid certification, and the authors successfully address the problems.\n- The paper is well written in a logical order.\n    - Lemma 1 and Theorem 2 describe the worst-case decision region for $\\sigma_0$ and $\\sigma_1$. Given $\\sigma_0$, to use the input-dependent $\\sigma(x)$, it is required to certify for all valid $\\sigma_1$ as stated in Theorem 6. If we know upper/lower bounds on the $\\sigma_1$, it is easy to obtain a certified radius. Therefore, Theorem 8 justifies the design of $\\sigma(x)$ in eqn (1) as we can easily compute the bounds of $\\sigma_1$ as in Theorem 7 because it has globally bounded variations ($r$-semi-elastic).\n- It is a general approach that can be extended and applied to other RS scenarios.\n\n### (minor) weaknesses\n\n- In Abstract, the authors (over)state that \"... to overcome these flaws (certification vs acc trade-off or fairness issues)\" which may mislead the readers to thinking that the paper solve these problems. However it is not quite true as stated in Section 4 regarding the results in Table 2 and 3 (the last paragraph in page 7).\n    - However, I still believe that the paper has enough contributions.\n- It would be nice to compare the results with ($r=0.00$ and $\\sigma_{tr}$ increased) too.\n    - concrete, actionable feedback: please show the corresponding additional results in Table 2 and 3.\n- Except for the above two comments, the paper has no significant weakness in its content, but there are some major/minor presentation issues that must be addressed before published.\n- \"Using non-constant \\sigma(x) defined in Section 2, ...\" (page 2)\n    - It is defined as eq 1. in Section 4.\n- It would be better to use the same color for each method to avoid confusion in Figure 4.\n- $\\sigma_{tr}$ and trs (and $r$ and trr) seem to indicate the same thing, but the terms trs and trr have never been defined (in Figure 4).\n- not useful [README.md](http://readme.md/) copied from the original RS repository.\n    - for example, see [https://github.com/paperswithcode/releasing-research-code](https://github.com/paperswithcode/releasing-research-code)\n- Please kindly correct me if I misunderstood anything.\n\n### Others\n\n- Compared to other parts, the curse of dimensionality part is hard to understand. Could you elaborate it more in detail (in easy words)?\n- How long does it take to compute the $\\sigma(x)$ in eqn (1), compared to the Monte-Carlo process? Is it negligible? It would be nice to provide the (additional) computation cost.\n- The authors mention that \"Here, we compare Cohen et al. (2019)â€™s evaluations for ... with our evaluations, setting ... (for CIFAR10 and MNIST, respectively), applied on models trained with Gaussian data augmentation, **using constant standard deviation roughly equal to the average $\\sigma(x)$ or $\\sigma$ used during evaluation**.\" How do you choose the training $\\sigma_{tr}$ **before** evaluation phase?\n- How do you choose $r, k$ and $m$?",
            "summary_of_the_review": "The authors tackle important problems of RS. \nThey use their original method to solve the problem in a rigorous way and the method seems to be effective. \nI thereby recommend the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}