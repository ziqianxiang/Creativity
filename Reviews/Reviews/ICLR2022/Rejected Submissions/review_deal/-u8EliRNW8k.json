{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper proposes an MLP-based neural network specifically designed for speech processing. The proposed Split & Glue layer is used to capture multi-resolution speech characteristics. The method achieved better performance in both command recognition and speech enhancement tasks.\n\nTwo major concerns raised by the reviewers:\nThe proposed split & glue layer is similar to convolution. Although the authors revised the paper with more clarification on the differences, the op is equivalent to frame-wise convolution which has been explored in speech literature. This limits the novelty of the paper.\nThe experimental justifications are relatively simple and limited. On the voice command and speech enhancement tasks presented in the paper, stronger and better baselines would be more convincing to justify the benefit of the proposed method. Moreover, testing on large scale ASR tasks instead of the relatively simple voice command task would be more convincing.  \n\nThe decision is mainly based on the limited novelty and experimental justification."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes an MLP-based neural network, which is designed for speech processing. The method is an alternative architecture to a transformer encoder and is applied to several speech processing tasks (command recognition and speech enhancement). The novel Split & Glue layer is used to capture multi-resolution speech characteristics. The method achieved state-of-the-art performance in both command recognition and speech enhancement tasks.\n\nOther comments\n- I don't think (Huang et al., 2020) is a representative work for transformer ASR. The following papers are more appropriate:\n  - Dong, Linhao, Shuang Xu, and Bo Xu. \"Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition.\" 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018.\n  - Karita, Shigeki, et al. \"A comparative study on transformer vs rnn in speech applications.\" 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2019.\n- It's better to mention how it is simple with quantitative measures in the abstract and introduction (model size, computational cost, or the number of code lines).\n- the paper should refer conformer and discuss it. It becomes SOTA in many speech processing tasks now. Also, historically, many deep-learning-based speech processing methods were started from MLPs but the paper does not have enough surveys.\n- please discuss the online/streaming capabilities. This is an important function for speech processing.\n\n\n",
            "main_review": "strengths\n- novel neural network architecture by revisiting an MLP\n- simple but effective architecture with fewer parameters than transformer.\n- shows the effectiveness in two different speech processing tasks (especially the speech enhancement task only with 600K parameters looks very strong).\n\nweaknesses\n- the effectiveness of the Split & Glue layer is similar to the convolution operation with different kernel/stride sizes. \n- the method cannot be applied to a sequence-to-sequence task like ASR (but the method can be used as an encoder of seq2seq tasks).\n- although the performance is strong, the task is rather simple and limited. It does not attract a machine learning researcher in general.\n- the paper needs more surveys",
            "summary_of_the_review": "This paper proposes a novel neural network architecture based on MLP. The neural network architecture in speech processing becomes more complicated, and the proposed method can provide simple and alternative solutions. The effectiveness is also shown with command recognition, keyword search, and speech enhancement. My concern of this paper is that the application is rather simple. The method has a lot of potentials and may have more attention if they are applied to sequence-to-sequence tasks like ASR and TTS.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a simple architecture based on the multi-layer perceptron for extracting information from speech signals. The architecture is based on a new layer called split and glue and can capture multi-scale local temporal dependecies. It is evaluated on two different problems, keyword spotting and speech enhancement and achieves state-of-the-art performance.",
            "main_review": "The paper is very well written, it is clearly motivated and presents impressive results with such a simple architecture. If the code becomes publicly available as the authors promised it will be very useful for the research community.\n\nThe ablation study is useful but it would be more complete if additional parameters were investigated, e.g., number of blocks. In Table 1 they are set to 4 and 10 somehow arbitrarily. In Table 3, more window sizes can be considered, e.g., 7 and 9. It is also not clear how the number of chunks impacts performance in Table 3. \n\nWhy not considering some intermediate architectures between small and large? It would be useful to report resorts for a M(edium) architecture as well.\n\nAlso have the authors considered other ways to create the chunks, e.g., across the time dimension?\n\nSome typos\nAppendix B.3 Specificaaly\nA.2.2 closing quotation marks are used as opening quotation marks\n",
            "summary_of_the_review": "Overall, this is an interesting study, the paper is clearly written and motivated and presents a novel architecture which leads to state-of-the-art results for keyword spotting and speech enhancement.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors have proposed a new general-purpose MLP architecture for speech processing and learning speech representation for tasks such as keyword spotting and speech enhancement. The proposed model processes the spectral representation of speech in multiple frequency/channel bands ('split' operations). The unfolding operation expands the context of each banded signal and applies linear transformations to it. These transformations are not shared across the chunks/bands which allows the model to learn chunk-wise relevant representations. Further, the unfolding procedure (context expansion) provides the benefit of learning segmental and supra-segmental properties of speech. The transformed outputs are concatenated together which is referred to as the 'glue' operation. Finally, the learned representations are passed into task-specific layers for generating output predictions. There are two residual connections in each split and glue block which ensures that there is a proper flow of gradients during the backpropagation to train the model. \n\nThe authors demonstrate the performance of the proposed mlp architecture on two problems, namely, keyword spotting, and speech enhancement. The model shows improvement over the state-of-the-art baseline techniques on multiple datasets (Google V2 and Libriwords) for keyword spotting. The ablation studies show how the choice of multiple linear operator kernels in 'split and glue' perform better than no split model. The speech enhancement experiment has been carried out on VoiceBank+Demand dataset and the authors show improvement by proposed technique across multiple metrics and baseline models. Therefore, the main contribution of this paper is to show that mlp architectures that are heuristically driven and are derived from domain knowledge can outperform state-of-the-art models for various prediction tasks. ",
            "main_review": "Strengths: The proposed model is relatively simple and easy to implement. It leverages some of the important characteristics of human speech such as temporal invariance, frequency asymmetry and short-term stationarity. The model is modular and can be easily integrated into any existing pipeline to process the features in a desired manner. Finally, it shows improvements on multiple tasks and across multiple datasets. \n\n\nWeaknesses: This model appears to be convolution in disguise. The splitting operation is akin to breaking down the signal into multiple frequency bands and learning a convolution kernel for each band separately, i.e, separable convolutions. The unfolding operation or context expansion is where the model resembles the convolutional structure the most. It is similar to creating a circulant matrix and applying linear transformation to the expanded signal. The linear transformation operator is analogous to the convolutional kernel. Therefore, the authors, unknowingly, are proposing a convolutional neural network with residual connections and labelling it as 'split and glue' model. Further, convolutions can actively take advantage of temporal invariance or short-term stationarity of speech signal which seems to be one of the main drivers of performance gains in this model. Finally, the authors have not clarified if the data augmentation was done for the baseline models or not. The augmentation procedure can have a huge impact on the generalization of neural networks. \n\nThe experiments on keyword spotting use cross entropy loss for backpropagation. My suggestion to the authors would be to use the triplet loss for training as it outperforms the entropy based model in baseline comparisons. Additionally, the baseline methods in the speech enhancement task are mostly generative models which are perhaps not trained in a supervised setting. Therefore, comparison with some recently proposed supervised methods for speech enhancement would provide a good sense of the performance improvements. In the ablation study, it would be valuable for the speech and machine learning community to see how the proposed split and glue model performs without the outer residual connection in each block. While the paper compares the proposed approach to a convolutional model (Res-15), the presence of residual connections (inner+outer) and normalization (layer/instance) adds a lot of confounders in the analysis. Finally, a statistical test or error bars on the speech enhancement metrics will help validate the claims made by the authors. ",
            "summary_of_the_review": "The proposed architecture is similar to a convolutional neural network. The context expansion and chunk-wise processing is analogous to the concept of separable convolution. The experiments however, show the effectiveness of the model on keyword spotting and speech enhancement tasks. The proposed approach appears to take advantage of the structure of speech signals, specifically, temporal continuity. The authors have also conducted ablation studies to show how splitting the model into multiple frequency bands has desirable effects. It would be interesting to see how the 'split and glue' output performs on the relevant tasks without any residual connection. Further, comparison with supervised methods and error bars on the evaluation metric for speech enhancement will provide additional insight into the proposed technique.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper presents Speech-MLP, an architecture based on Mixer MLP but specific for speech signals.  The Mixer MLP architecture is argued to be appropriate for the particular structure of speech.  The architecture is compared to others on keyword spotting and speech enhancement.  In each case, the architecture outperforms other competitive solutions, often with fewer parameters.",
            "main_review": "I begin with the experiments because that is the positive part, at least for keyword spotting.  The performance is good, and that is in itself interesting.  This is especially true for the smaller architecture, where there are far fewer parameters.  Although the evaluation on speech enhancement produces good objective results, the measures are hardly significant.  A 0.02 improvement in PESQ is not regarded as significant, and the others are derivatives of PESQ.  Modern measures such as STOI and frequency weighted segmental SNR are missing.  The enhancement results can only be taken as preliminary, suggesting that a subjective test is worthwhile.\n\nI find the method part less persuasive, beginning with the claim that the contribution is threefold.  The combination of propose, test and demonstrate is *one* contribution.  Thereafter I have multiple difficulties:\nThe authors claim early in the manuscript that they do not expect Speech-MLP to work for speech recognition.  Given the speech recognition is *the* main application of machine learning in speech processing, Speech-MLP is not really a good name.  Pragmatically, the longer range dependencies required for speech recognition are likely to come from other architectures.\nThe authors introduce their architecture simply by describing it.  Lacking is a description of Mixer MLP and how the new architecture differs from it.  My reading of this section is that Speech-MLP *is* Mixer-MLP with some minor modifications (it is not clear what they are).  Further, where in the introductory material, the authors claim that their solution is based on domain knowledge, I do not see how the proposed architecture addresses this where Mixer MLP would not.\nThe descriptive comparison with transformers is selective; it is true that transformers discard this type of domain knowledge, but *every* other speech processing architecture does take it into account.  A case in point is the TDNN used in many solutions.\nIn general, The method section should be rewritten to explain how and why the proposal differs from Mixer-MLP, and to place it in the context of other common signal processing techniques that also take such structure into account.\nAll results should include significance tests.",
            "summary_of_the_review": "Whilst some results are persuasive, showing that the architecture can perform as well as much larger ones, the method lacks rigor and contains no clear advance over other the methods on which it is based.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a new architecture for speech processing, dubbed \"speech-MLP\". Besides input layers and output task-specific layers, speech-MLP consists of linear layers, residual connections, instance or layer normalization, GELU activations, and a layer called the \"split-and-glue\" layer. The split-and-glue layer splits a [batch, time, channels] tensor into N chunks of shape [batch, time, channels / N] along the channels dimension, then applies an unfold operation to provide temporal context, then a linear layer (with a different matrix for each chunk), then concatenates the results back together. Speech-MLP is tested on keyword spotting (with 2 datasets: Google Speech Commands and LibriWords) as well as speech enhancement (voicebank + demand).",
            "main_review": "Strengths: \n- The paper is well written and clearly describes the model used and the experiments performs. The descriptions are accompanied with high-quality diagrams and specifications for hyperparameters. The motivations behind model choices are also explained. \n\nWeaknesses:\n- One main contribution of the paper, the \"split and glue\" layer, seems like a groupwise convolution layer. A groupwise convolution layer can be implemented by unfold (im2col) followed by a linear layer and concatenation. An unfold followed by a linear layer *is* a convolution. This significantly reduces the novelty and undermines the claim that the \"architecture involves simple linear transformations only\". Prior art on keyword spotting with convolutional networks is not hard to find (e.g. [0]). If my analysis is incorrect, and \"split-and-glue\" *cannot* be reduced to a groupwise convolution or at least something very similar to it, the paper can be much improved by a comparison to groupwise convolution. \n- The paper compares KWS results on Google Speech Commands and LibriWords. Although Google Speech Commands is a well known dataset for KWS, LibriWords is not commonly used and as far as I can tell has only ever been evaluated with once in a single paper that proposed this dataset. I would focus on Google Speech Command results, as that dataset has been widely studied and benchmarked against. Although extra datasets do not *reduce* the quality of the paper, evaluating on LibriWords does not add much to the paper.\n- A key part of the claim is that speech-MLP is \"SOTA\" on KWS. \"It can be observed that the speech-MLP-S and speech-MLP-L outperform all the benchmarks for all task\". However, upon cursory examination of recent literature, this claim seems dubious. Matchboxnet ([1]) achieves 97.37% on V2-35 with 140k parameters. Keyword Transformer (KWT) [2] achieves 97.51%. Note that KWT is even *cited* and compared against, but the result from KWT that the authors choose to cite is KWT1 (which speech-MLP outperforms) and not KWT2 or KWT3 (which outperform speech-MLP). (Arguably, this is because KWT2 and KWT3 have way more parameters, but this is not clear at all in the paper, and it's unclear if this is important). More recent works (Audiomer and Audio Spectrogram Transformer [4]) *significantly* outperform speech-MLP with 99.74 and 98.1%. The paper would be much improved by citing all relevant results rather than cherry-picking the results that support the claim of \"speech-MLP is SOTA\" and omitting others. \n- Much of the introduction and motivation is based around model aesthetic cleanliness, that is, around claims that one model is 'more complex' than another and that simpler models are preferable. However, model complexity is subjective and not well defined. While the authors of the paper believe that transformers are a complex model and that the prosed speech-MLP model is \"simple\", this may not match the expectations of others, for whom a single architecture commonly known and used across industry and research is simpler than a custom architecture with a variety of dataset-specific tweaks. It may be best to leave subjective judgments of complexity out and focus on other benefits of the proposed model, e.g. parameter efficiency, inference speed, etc. \n- One reason that speech-MLP may make a good architecture is parameter size and efficiency. In comparison to KWT2, which achieves similar / better results, it is much smaller. Other KWS papers will make comparisons of inference time as the model scales; this may help make the case for this architecture as well.\n- Unlike many of the cited papers, none of the values have standard deviations or confidence intervals. It would help understand the results to have these.\n- May be good to compare to more recent VoiceBank-DEMAND papers as well.\n\n[0] https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43969.pdf\n[1] https://arxiv.org/pdf/2004.08531.pdf\n[2] https://arxiv.org/pdf/2104.00769.pdf\n[4] https://arxiv.org/pdf/2104.01778v3.pdf",
            "summary_of_the_review": "The paper is well written and easy to understand with clear diagrams. However, I have some significant concerns about the split-and-glue layer (which seems to me to be a groupwise or separable convolution) and the keyword spotting comparisons (which omit results which outperform speech-MLP, then claim that speech-MLP surpasses the SOTA results). With these (and other more minor) concerns, it is hard to evaluate the novelty and impact of the paper, but these issues would be fixed with improved writing, literature review, and hyperparameter search / comparison.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}