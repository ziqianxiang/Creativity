{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper led to significant discussion, and the AC is generally on the fence. First of all, thanks to the reviewers for the significant time they invested in the discussion, and thanks for the authors for promptly and patiently answering our questions. \n\nOverall, the reviewer recommendations are positive. However, the discussion showed that despite the positive recommendation, the reviewers struggled to distill the general contribution of the paper beyond performance on ALFRED. In discussion, the authors distinguished their contribution from existing work by focusing on using a set of low-level policies at the root of the overall policy. This relies on the discrete set of behaviors that is defined within the ALFRED benchmark. It's not clear how it generalizes to the actual problem of instructing a robot to execute natural language instruction. In realistic scenarios, is it possible to define a set of behaviors in such a clean way, and at scale? And then train/manage a separate model for each behavior? The set of interaction policies in Figure 2 illustrates this challenge well. The answer to this scaling question is not clear. This corresponds to a concern raised repeatedly by the reviewers about the approach too specialized to ALFRED. The AC shares this concern. \n\n(which are roughly equal to the SOTA at the time of submission, but show significantly more overfitting to seen environments)\n\nOn the positive side, this is solid work, with good results. The paper is well written, and the authors largely addressed the concerns raised as much as possible. The results are not SOTA though. The current SOTA was submitted on 09/19/2021, prior to the ICLR deadline -- it's not included in the results table in this paper. (To clarify, the fact that it's not the current SOTA does not affect the final decision, as they are considered as contemporaneous.) With concerns regarding the specificity of the approach, this paper may interest researchers working on ALFRED, but not clear to what depth, despite the clearly significant work and effort the authors put into the paper. \n\n(If the paper is accepted, the AC asks the authors to fix the standing errors with regard to previous work, as discussed below, and to include more recent results from the leaderboard)"
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a hierarchical approach for policy learning in interactive instruction following for the ALFRED task. They infer a series of subgoals to be executed from the language instructions. Then, based on the subgoals predicted, they run Master policy which executes navigation actions, followed by Interaction policies which produce interactions to be done in the environment. They propose Object Encoding Module which provides target object information and serves as navigation subgoal monitor. They also propose a Loop Escape Module which based on the similarities between visual features detects if a loop has been made.",
            "main_review": "Strengths:\n- In one of their experiments, they show that training a hierarchical policy learns faster and more efficient action sequences as opposed to their flat policy counterparts. Also, hierarchical policy produces subgoals which leads to a more interpretable and transparent approach.\n-\tThe paper has good results with their method beating prior state-of-the-art methods on mostly all metrics. They also provide exhaustive ablations showing the importance of each component in their approach.\n- The paper is well-written and easy to follow with proper figures and with explanations of all the notations used in various modules. This is especially helpful since there are a bunch of moving parts in the approach. \n\nWeaknesses/Clarifications:\n\n- What was the value of $W$ chosen for experiments in eq (3)? I believe it is not mentioned in the paper. How was it decided and how does it impact the usability of Loop Escape module for the ALFRED task? \n-\tWhat are the 7 subgoals considered in the Policy Composition Controller module and how were they decided? It is unclear to me of how does the sub-instruction $\\hat{x}_i$ correspond to a single subgoal? For the sub-instruction (3) in language instruction in Figure (2) _“Turn around, bring the potato to the microwave on the right.”_, this corresponds to two subgoals “GoToLocation” and “HeatObject”.\n-\tIn figure (2), there are two “Manipulate” action blocks, one in PCC module and another as a predicted navigation action in Master policy. Based on the context, it seems that Manipulate in PCC passes the control to either Master Policy or Interaction Policies depending on whether the subgoal prediction from PCC was an interaction subgoal or not. But “Manipulate” in the Master Policy seem to be doing the same thing. How are they different from each other? It might be good to clarify. Also, it seems that “Manipulate” in Master Policy would act as a “Stop Token” to end the navigation action sequence, does there have to an interaction action after that? What would happen if PCC predicts 2 consecutive “GoToLocation” in a row, but the end of Master Policy triggers an Interaction Policy action due to the “Manipulate” action?\n",
            "summary_of_the_review": "Overall, I feel that this is a good paper with good results and clear presentation. I am happy to increase my score if my concerns can be answered.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "**1. The main idea of the paper**\n\nThis paper introduces a hierarchical strategy for interactive instruction following. Following previous module networks[Singh et al., ICCV 2021 and Corona et al. 2020] for interactive instruction learning, the author adopts module networks for action prediction. Different from the previous module networks, the authors propose to learn the middle-level goals first and decompose the step-by-step action predictions into two modules, master policy for navigation and interaction policy for object interaction. To better improve the performance, the authors also provide some minor modules like the object encoding module and the loop escape module. The method overall achieves slightly better performance than the previous method.\n\n",
            "main_review": "**2. Strengths**\n\nThe idea of hierarchical strategy sounds technically correct and the ablation study in the experimental section shows its better performance than the previous method. The method also provides more transparent execution processing since the strategy predicts the subgoal during inference besides the step-by-step instruction.\n\n**3. Concerns**\n\nThe reviewer has many concerns about the method which the reviewer thinks resolving them will make it a better paper.\n\n**(1). About the module design.**\n\nThe PCC and MP modules do not make any use of the visual input for subgoal prediction and navigation action prediction. This looks strange since it will always be easier for humans to see the room before making subgoal and action predictions. It will be better to verify visual input's effectiveness with experiments. At least, the reviewer thinks that a discussion of the usage of visual input for PCC and modules are needed.\n\n**(2). About the fairness of the comparison.** \n\nThe method uses additional supervision signals (the subgoal labels) compared with Singh et al (ICCV 2021). Also, it adopts a low-level text instruction as input when compared with HLSM. It's OK to use these additional signals as input but the reviewer thinks that it will be better to clarify them in the comparison.\n\n**(3). About performance comparison.**\n\nThe performance gap in the table between HLSM and HACR is small (16.70 to 16.29). We care more about the unseen testing split and so does the official Alfred challenge. The authors of HLSM recently release their code publicly on github (https://github.com/valtsblukis/hlsm.git) and achieve much better performance (20.27) with better parameters fine-tuning. It is OK not to compare with the updated performance in this submission since the code has not been released at that time. However, a discussion about how HACR is better and complementary to HLSM will be more convincing. Note that HLSM uses less supervision signals.\n\n**(4). About the overfitting issues.** \n\nThe performance of HACR seems to suffer from strong overfitting in table 1 (from seen to unseen and from validation to test). More analysis for such a performance gap is needed and makes the paper stronger.",
            "summary_of_the_review": "According to the concerns in the **Main Review**, the author currently tends to reject the paper. The viewer also believes addressing the issues will make the paper stronger.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a hierarchical approach (HACR) to tackle the long-horizon issue for embodied instruction following tasks. The hierarchical structure includes \n- a Policy Composition Controller (PCC): predicts a sequence of predefined subgoals\n- a Master Policy: decides whether to manipulate or keeps navigating\n- an Interactive Policy: performs the low-level interaction.\n\nThe main contribution claimed in the paper is the decomposition of the high-level goal into subgoals (which makes it interpretable), adding OEM and LEM to boost performance, and the SoTA performance on ALFRED benchmark.",
            "main_review": "**Strength**\n\n- The work attacks a challenging problem\n- The idea is appealing and intuitive: making the  decomposing instructions into hierarchical subgoals and use different policy modules to handle them\n- Good empirical performance on competitive benchmark\n\n**Weaknesses**\n\n- Aside from the main claims, the paper lacks some critical details (see question 1) and is a bit confusing (see questions 2-4). See the questions below for concrete examples.\n- Hard to really pinpoint what the main contribution is. There are claims such as “novel language instructions” in sec 3.2.1 or “we propose a module” in sec 3.2.2. The above two examples are not mentioned in the main claims but they obscure the main points as I’m not sure if they are technical details or a contribution. Similar issue in the ablation study. The paper ablated on aspects that are not mentioned in the main claims such as DA, NIH, and OCMP. It’s unclear what conclusion the readers should draw from the analysis.\n- Lacks analysis compared with previous best models. The performance improvement seems large, but the paper provides little insight in terms of what gains it gets against similar hierarchical approaches (Zhang and Chai, 2021, Blukis et al., 2021). E.g., comparison in Table 2 is too weak (flat policy is a weak baseline); should compare to similar hierarchical approaches referenced above. \n- No significance tests\n\n**Questions**\n- Q1: There is prior work attempting to make instruction following hierarchical, which is duly cited in Related Work (e.g., Blukis et al., 2021). But I couldn’t parse the fundamental difference between “3 layer hierarchy” v.s. “1 layer hierarchy” in prior work. As far as I understand, prior work does generate subgoals which group low-level actions. If it’s pointing at the 3 layer modular structure, then I’d like to understand how having architectural hierarchy is useful in addition to action hierarchy \n- Q2: How is encoding the instructions with Bi-LSTM a novel approach? (sec 3.2.1)\n- Q3: Where does the subgoal space come from? Is this designed by the authors? \n- Q4: Why does <manipulate> appear again in the master policy? Shouldn’t this be for the PCC to decide?\n",
            "summary_of_the_review": "Overall, I think it’s a worthwhile paper if the empirical result is correct. The main weakness lies in the structure and the presentation of the paper which make it 1) hard to see the main contributions (is the main claim that hierarchical > flat or with the combination of the proposed components, MIP, OCMP, FIP, etc it works better than other hierarchical approaches?) 2) hard to draw insight as its comparison against prior best models is limited (e.g., does HACR also converge faster compared with other hierarchical approaches in Fig 4?). I would be happy to raise the scores if the authors could clarify more on the above two points as well as answering my questions.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper looks at the problem of instruction following (using language) in a navigation+interaction task setting, encapsulated by the ALFRED benchmark. The authors propose a hierarchical modular approach that operates on three levels: (i) identifying subgoal sequence, (ii) a navigation “master” policy, and (iii) a manipulation/“interaction” policy. Evaluations against a slew of baselines from the vision+language navigation literature are presented, showing that the proposed method (HACR) outperforms existing baselines in both seen and unseen tasks. Careful ablations are also presented.",
            "main_review": "## Short version\nOverall, I think this paper presents an effective hierarchical approach to instruction following that’s tailored a bit too tightly to the ALFRED benchmark. I’m not too familiar with the VLN line of work, but I believe the authors have made a fair comparison to the existing works, based on the quantitative comparisons against baselines. Algorithmically, there seem to be a lot of moving parts that make the whole work well and achieve SOTA numbers, but can be interpreted as either clever, crafty or hacky, depending on how you look at it. I respect the effort put by the authors in carefully designing each component and engineering an effective algorithm for the ALFRED benchmark but have concerns regarding the generality of the findings.\n\n---\n## Strengths\n1. The writing and presentation is very effective and the authors do a great job of explaining the components of the algorithm in an intuitive manner.\n2. The ablations, although a bit binary, are exhaustive and do a great job of highlighting the importance of each component in isolation. The ablations are conducted like a well-written CVPR/ICCV paper, which tend to have some of the most thorough ablation experiments.\n3. The task that the paper addresses is extremely visual and the authors do a good job of visually showing rollouts and their method in action. It really highlights the long-horizon and semantic nature of the problem and intuitively describes the method (fig 1, 5).\n4. The implementation and experimental details, while not exhaustive, feel sufficient to understand and replicate the settings in which the results are presented. The impact of the paper will be greatly improved by a public code release.\n\n---\n## Weaknesses/Concerns\n\n1. My biggest concern with the method is that it seems overfit to the ALFRED benchmark and a lot of the design decisions were motivated by common failure cases in the benchmark. While that can be a great way to make progress on a challenging task like embodied instruction following, I’m not convinced that the algorithm and insights necessarily generalize to the broader range of problems. Specifically, the “loop escape” component or the “OEM module”, while solving real problems, feel a bit too specific and poorly justified. This is also depicted in how these components are motivated — e.g. “”To further improve the task completion performance, we propose…”.\n2. The authors make several claims that seem poorly motivated or not justified:  \n\t(i) [Section 3] “navigation and interaction must be distinguished because the former requires a broader set of information” — this could benefit by a proper discussion or references to prior work. While it’s true that almost all prior/concurrent work deals them independently, it’s almost a _bug_ and not a _feature_; we could use systems that better integrate these two capabilities. I am not saying that the choice of treating them independent is a wrong one (it does make things easy), but the claim needs to be qualified/justified a bit.  \n\t(ii) [Section 3.2] “this necessitates an independent module that can…” — why is this the case? This feels more like a by-product and not requirement.  \n\t(iii) [Section 4.3] “… highlights the modular structure’s benefits in long-horizon planning and superior generalization capabilities” — how does it do so? It’s not clear to me how the single policy metrics imply this. A bit more clarification would help here, since “better generalization” is presented as a key advantage of the proposed method at the start.  \n\n3. [Section 3.2.4] Regd. the loop escape module: $W$ being a hyperparameter feels a bit crafty. The job of the “loop escape” is to break out of a deadlock, and being able to do so for a fixed/preset value of repetition (which must be set for “best results”) again sounds like something that is overfit to the benchmark. In a general set of environments, this value could vary drastically and there could be a better way to handle this (if at all such a module is necessary).\n\n4. [Section 3.1] Regd. the policy composition controller: I get the impression that the benefits of this are overstated, and the main purpose of this controller is to learn correspondence between “semantic subgoals” and text instructions. According to Eqn. 1, these subgoals seem to be predefined, and hence, the mapping is pretty straightforward when done in a supervised manner and isolated for the controller. The claims about interpretability need to be toned down in this case.\n\n5. [Section 3.1] I had another question regarding the PCC: Based on the description, it seems to me that the PCC learns a one-to-one mapping from $\\hat{x}_i$ to subgoals $s_i$. Is that correct? If so, how does this handle instructions that need multiple subgoals, e.g. “heat the potato in the microwave” would need HeatObject and GoTo etc. Either way, a bit more clarification on how the instructions are handled could be useful in either the main text or appendix.\n\n6. While this may not be an outright weakness, a lot of the proposed contributions lead the work towards a “classical” pipeline, with individual stages for parsing inputs, perception/object segmentation, running control, loop detection/closure etc. Looking into the progress made in embodied AI in the robotics and vision communities, a lot of these _hand-engineered_ stages have benefitted from being replaced by data-driven means and going back to such a pipeline is intriguing —  it brings up the age-old question about what to learn v/s what to bake in as an inductive bias. While more instrumentation like the proposed method seems to improve benchmark metrics, it might come at the cost of “generalization of insights”. This relates to the earlier point (1), and a discussion about this philosophy would be nice. (I don’t hold this against the paper and this does not factor in my assessment, just a useful discussion to include.)\n\n\n---\n## Misc. Comments/Typos\nI noticed a couple of typos in the text that you can fix. Don’t worry, this does not affect my impression of the otherwise well-written paper :)\n\n- [Section 1] “To evaluate our proposal in a challenging scenario*s*”\n- [Figure 2, caption] “comprises of *tree* main components”\n\n\n---\n\n*Update*: Updating my recommendation to reflect the discussion by the others.",
            "summary_of_the_review": "The paper presents an effective algorithm for instruction following within the ALFRED benchmark but the means and insights seem overfit to the metrics. I am on the fence regarding acceptance and look forward to a response from the authors.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a hierarchical and modularized framework for addressing the ALFRED language-guided task completion problem. The method divides instructions into navigational and interactive subgoals, where the navigation and each interaction is processed by a specialized policy. To further improve the agent’s performance, the paper introduces object encoding module and loop escape module. The proposed methods are novel and have been experimented comprehensively in this paper, which could benefit a range of future research. ",
            "main_review": "Strengths:\n1. The hierarchical and modularized approach seems promising for addressing the ALFRED problem, which is a mixture of navigation and multiple interactive actions. Decoupling and specifying different reasoning processes, as well as using independent state representations, greatly reduces the state space in learning, and as claimed in the paper, improves generalization ability and interpretability. I believe the method could benefit and influence lots of future work.\n2. Comprehensive experiments in the main paper and in appendix nicely support the proposed methods and arguments in this paper. The performance of the agent on ALFRED has been improved by a significant margin.\n3. The overall paper is nicely written, tables, figures are clearly presented.\n\nWeaknesses / Comments:\n1. A common limitation of learning specialized network for each subgoal is the increase in hardware cost, training cost and sometimes computational cost, especially when people want to design more complicated interaction policy or when there are a large number of subgoals. I am wondering if the authors can discuss about this point (and perhaps provide some quantitative analysis on this issue).\n2. Following the previous comment, I believe the learning of some subtask can benefit each other, a simple example might be: heat and cool both requires the agent to put an object in to something, which such information is not shared across specialized policies. I would like to learn about the authors’ thoughts on this point.\n3. There are several points which are unclear to me: \n(1) Section 3.2.1, how to combine step-by-step instructions into a subtask instruction? I think ALFREAD only provide step-by-step instructions. I can understand this can be easily done in training because we know the ground-truth action correspond to each step-by-step instruction, but what about the validation data?\n(2) Following previous point, it is unclear how does each step-by-step instruction x^hat_i translate to multiple subgoals. E.g. Figure 2 Instruction 3 contains navigation action and object interaction, but from Equation 1, it seems that each x^hat_i only translates into a single action. Please correct me if I misunderstood anything.\n(3) The training of the seven interaction policies, e.g., training objectives. The paper also mentioned only two epochs of training is required, does it mean that the actions for completing each subgoal is similar (can be easily memorized? Or is it due to the limitation of the dataset?) I hope the authors can elaborate more on this point.\n",
            "summary_of_the_review": "The paper is nicely written, it presents interesting and valuable ideas, and the experiments are comprehensive. I am happy to accept the paper if the authors can clarify my questions.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}