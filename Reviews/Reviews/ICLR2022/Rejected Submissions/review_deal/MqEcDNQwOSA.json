{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "Overall, the paper proposes an interesting idea to share parameters across words and reduce the size of the embedding which hasn't been explored in the past with promising results on XNLI task. However, all the reviewers agree that the novelty of this paper is not enough. In addition, the clarity and experiments are not sufficient enough too."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a method for reducing the memory footprint of word embeddings by representing each word not as its own unique embedding vector, but rather as a concatenation of K lower-dimensional sub-embeddings, where the sub-embeddings can be shared between different words. By properly choosing K, the number of unique sub-embeddings M, and the dimension of the sub-embeddings, the number of parameters can be reduced drastically. Two techniques are given to map the words to their respective sub-embeddings: one is essentially a uniform mapping which completely ignores any semantic meaning of the words, and the other approach uses k-means clustering to preserve some of the semantic structure present in the original word embeddings. These new word embeddings are then tested on a few tasks. As expected, the performance of the uniform mapping is much worse than that of the original word embeddings, since all semantic structure was lost. The performance of the k-means sub-embeddings approaches that of the original embeddings, while requiring far fewer parameters. ",
            "main_review": "Strengths:\nThis paper gives a potentially interesting and novel method for reducing the parameters required for word embeddings. The idea of allowing different word embeddings to explicitly have identical blocks of entries seems potentially promising, as we want the embeddings of related words to be similar in some sense.\n\nWeaknesses:\nThis paper suffers from a number of significant weaknesses. \n1. Language and editing: The paper needs much editing to improve the language and notation. In several spots, it is difficult to understand what the authors are trying to communicate. The notation is also often inconsistent or not properly defined. Ideas do not flow and overall it requires too much work for the reader to understand what is going on. \n2. Motivation: the main motivation of this work is to decrease the number of parameters needed for word embeddings. However, there are already many works in this direction, and the authors do not explain how their approach addresses any shortcomings of other methods.\n3. Confusing explanations: It is difficult to understand exactly how the k-means clustered sub-embeddings work. There are notational issues in Algorithm 2 (mixing up lower-case k with capital K with k-means). The phrase \"adjust k-means algorithm to...\" is unclear. How are the k-sub embeddings initialized? I wouldn't know how to actually implement this idea given the description in the paper.\n4. Unnecessary content: What's the point of the randomly scattered sub-embeddings? This idea clearly ignores the semantic structure of the words, and so it completely misses the point of word embeddings. This is reflected in the poor performance of this approach in the experiments. There is no need to discuss this method.\n5. Experiments: this paper should compare the performance of their compressed word embeddings to other approaches that have been proposed in the literature. Since this isn't the first work to address the issue of compression, the experimental results given in this paper are not enough to justify the impact of its contribution.\n6. Unsubstantiated claims: The paper claims that their method doesn't suffer from the OOV problem. This is unclear to me and is never explained. ",
            "summary_of_the_review": "This paper has only limited contributions and suffers from several significant issues, both technical and in the writing quality. It does not meet the standard for publication and therefore I recommend reject. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a method for compressing the word embeddings.\nThough use cases of the compressing the (word) embeddings are much wider, this paper only focuses on the embeddings in pre-trained language models.\n\nThe main idea of the proposed method is to introduce the sub-embeddings shared among vocabulary.\nIn detail, this paper proposes two variants for sharing the sub-embeddings, namely, random and cluster-based methods.\n\nThe experiments are conducted on GLUE and XNLI benchmarks.\nThe results show that the proposed method provides to reduce the model size with the small performance degradation.",
            "main_review": "This current version of this paper has several concerns for publishing as a conference paper.\n\nThe following are the list of concerns that should be revised and reconsidered.\n\n#### *** technical novelty\n* 1, A paper [1] has already proposed a closely similar approach for splitting embeddings into K sub-embeddings and sharing them over vocabulary.\nThe motivation of reducing the embedding size is also identical.\nTherefore, the idea of incorporating sub-embeddings is not novel, and the claim for the technical novelty is incorrect.\nSimilarly, the paper has already discussed the capacity of sub-embedding combinations (Cartesian products) that can drastically reduce the embedding parameters.\nI think this paper has to cite the paper [1] and need to clearly state the similarity and difference between their method and the proposed method.\n\n[1] Suzuki and Nagata, Learning Compact Neural Word Embeddings by Parameter Space Sharing, In Proc of IJCAI-2016, https://www.ijcai.org/Proceedings/16/Papers/292.pdf\n\n#### *** Related work\n* 2, There are so many previous papers that tackle reducing the memory requirements of word embeddings.\nOf course, it is not necessary to cite all the papers related to the (word) embedding compression methods.\nHowever, this paper mostly ignores such developments.\nI feel that It is not acceptable for a scientific paper.\n\n#### *** experiments\n* 3, In the experiments, there are no baselines for reducing the embedding size. For example, quantization and pruning are typical baselines for the compression literature.\n\n* 4, The experiments are only conducted on masked LMs. However, as this paper also pointed out, there are several different types of LMs, such as standard left-to-right (casual) LMs. Therefore, the experiments that support the effectiveness of the proposed method for reducing the memory requirements are limited. \nAdditionally, I cannot find any reasons that the authors cannot do experiments on such LMs. I recommend doing experiments on several different LMs to enhance empirical evidence for the effectiveness of the proposed method.\n\n* 5, The model sizes of LMs used in the experiments are relatively small comparing the recent trend of preferring larger LMs.\nFor example, \nBERT Base (12 layers, approx. 110 million parameters) and BERT Large (24 layers approx. 340 million parameters) are typical MLMs. Still, they are already 2 and 7 times larger than those used in the experiments (50M). \nThis fact suggests that it should be better to use larger models to fit typical use cases.\n\n* 6, The experimental results looks not so good; the proposed method often degrades the performance with wide margins.\n\n",
            "summary_of_the_review": "This paper has several concerns for publishing as a conference paper, such as low technical novelty because of the missing previous similar method, and weak empirical evaluation that cannot sufficiently support the authors' claims about contributions.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "It seems no ethical concerns.",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes an embedding compression method which replaces an $N$ x $d$ embedding table with $k$ distinct $M$ x $(d/k)$ embedding tables (where $M = N^{1/k}$). Specifically, the $d$-dimensional embedding for each token in the vocabulary is replaced by $k$ different sub-embedding vectors $(e_i)_{i=1}^k$, each of dimension $d/k$, where $e_i$ is drawn from the $i^{th}$ codebook. It proposes two ways of selecting these sub-embeddings:\n1) arbitrarily assigning the sub-embeddings to each token (based on its index),\n2) using k-means to do this assignment procedure.\n\nIt shows that very high compression rates (e.g., 99.4-99.9% compression) can be attained using this method, while typically only sacrificing 0%-4% absolute accuracy.\n",
            "main_review": "Strengths\n1. Compressing embedding matrices is an important problem. In additional to word/wordpiece embeddings used by NLP models, many other systems use large embedding tables (e.g., recommendation systems' product/user embeddings) which can benefit significantly from compression.\n2. The proposed method attains very high compression ratios.\n\nWeaknesses\n1) Not comparing with or discussing baselines:\n- The paper does not compare their method with (or even cite) a single embedding compression method in the literature, even though many embedding compression methods have been proposed (e.g., [1-8] below). Furthermore, the proposed algorithm is very similar to product quantization, a classic compression method from the source coding/information theory literature (discussed in [1]); nonetheless, product quantization is not mentioned in the paper. Similarly, the use of k-means in this paper is reminiscent of the Lloyd quantizer (also discussed in [1]), but this method is not discussed either.\n2) Lack of novelty:\n- The proposed method is not novel, as it appears to be very similar to product quantization.\n3) Significant drops in accuracy:\n- Although the proposed method attains high compression rates, it often incurs a significant drop in accuracy.\n4) Lack of clarity in presentation (especially of Algorithm 2):\n- The explanation and pseudocode for Algorithm 2 were very difficult for me to understand. I was unable to understand how this algorithm worked, and this algorithm appears to be one of the primary contributions of the paper. This made it difficult for me to assess the paper as a whole.\n- It would be good to clarify whether the $k$ sub-embedding tables are trained end-to-end during pre-training and/or downstream task fine-tuning, or whether they are held fixed during training.\n\n**Missing Citations**\n\n[1] Jegou, H., Douze, M., and Schmid, C. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence, 2010.\n\n[2] Martin Andrews. Compressing word embeddings. In ICONIP, 2016.\n\n[3] Joulin, A., Grave, E., Bojanowski, P., and Mikolov, T. Bag of tricks for efficient text classification. EACL, 2017.\n\n[4] Raphael Shu, Hideki Nakayama. Compressing Word Embeddings via Deep Compositional Code Learning. ICLR, 2018.\n\n[5] Chen, T., Min, M. R., and Sun, Y. Learning k-way ddimensional discrete codes for compact embedding representations. ICML 2018.\n\n[6] May, A., Zhang, J., Dao, T., and Ré, C. On the downstream performance of compressed word embeddings. NeurIPS, 2019.\n\n[7] Oleksii Hrinchuk, Valentin Khrulkov, Leyla Mirvakhabova, Elena Orlova, Ivan V. Oseledets. Tensorized Embedding Layers. EMNLP (Findings) 2020.\n\n[8] Ting Chen, Lala Li, Yizhou Sun. Differentiable Product Quantization for End-to-End Embedding Compression. ICML 2020.\n",
            "summary_of_the_review": "I propose rejecting this paper because it\n1) does not compare with a single baseline compression method (most important reason for rejection),\n2) lacks novelty (very similar to product quantization),\n3) often attains significant drops in accuracy, and\n4) lacks clarity (especially Algorithm 2).\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper aims to reduce the size of word embeddings in pretrained language models. The main idea is to replace the lookup table embeddings of size $N$ with only $k$ sub-embeddings of $N^{1/k}$ size and a mapping that allows to combine entries of the sub-embeddings to produce unique embeddings of the same size via concatenation. Two different types of mappings are explored, one random and one derived from clustering word embeddings from a pretrained model. For evaluation, the procedure involves pretraining a language model with the proposed embedding from scratch and then finetune it on downstream tasks. The  results show that the embedding can be reduced by 99.9% while maintaining performance within ~1.4% difference on a XNLI benchmark and within ~4.8% difference on GLUE benchmark.",
            "main_review": "Strengths\n* The idea of combining k sub-embeddings of small size to produce word embeddings of large size via concatenation is interesting. It allows to reduce the number of parameters and enables a form of parameter sharing that has not been explored before. \n* Simple and more complicated mappings are explored for assigning sub-embeddings to each vocabulary entry. The cluster-based mapping is well-motivated and takes advantage of the word embedding similarity from a pretrained model.\n* Defining the word embedding in this way allows to massively reduce the number of parameters allocated for the word embedding in two pretrained models while maintaining the competitive performance on downstream tasks within 1.4% in one of them. \n\n\nWeaknesses\n* While the idea bears some novelty, the framing with respect to existing alternatives and discussion of prior work is rather poor. There is no mention or comparison to viable alternatives such as linear factorization (a la ALBERT, adaptive inputs [1], or deep factorization [3]) or sparse code learning [2]. The latter study achieves similar levels of compression with a very small degradation (if at all) on sentiment analysis and machine translation. \n* In the introduction, it is mentioned that the language models that use the proposed embedding are free from OOV problem which does not seem right since the sub-embedding structure relies on a tokenizer (section 2). This issue is rather linked to the tokenizer and not the embedding.  \n* The writing lacks clarity in several places especially with regards to the empirical claims and the evaluation procedure. Regarding the former, the paper claims comparable performance on both GLUE and XNLI but this is a bit misleading because on GLUE benchmark is far lower. Regarding the latter, it took me a while to figure out that this method requires pretraining the language model from scratch which is a limitation (this is made clear only in the supplementary). \n* Even though this work should be all about efficiency given the memory reduction, there is not a single experiment that reports throughput, latency or some other efficiency metric during training and inference. In particular, I would be curious to know if the mapping and concatenation operations that have to be performed are increasing the latency or reduce the throughput, especially when dealing with long text sequences. \n* There is lack of discussion about the fact that the language model needs to be pretrained from scratch and in addition to that a clustering procedure is required that is dependent on a pretrained language model. Reporting the corresponding training times would be helpful. \n \nQuestions:\n* It is interesting to see that the sub-embeddings group the embedding vectors in a clear way due to parameter sharing. How this form of parameter sharing compares to other simpler ones, say linear factorization? \n* What is the actual overhead of the clustering process required for the mapping? \n\n[1] https://openreview.net/pdf?id=ByxZX20qFQ\n\n[2] https://openreview.net/pdf?id=BJRZzFlRb\n\n[3] https://arxiv.org/pdf/1911.12385.pdf",
            "summary_of_the_review": "Overall, the paper proposes an interesting idea to share parameters across words and reduce the size of the embedding which hasn't been explored in the past with promising results on XNLI task. My main concern is the lack of clarity and issues with the framing, claims, and evaluation. There is no discussion or comparison to alternatives, some of the claims made are weakly or not supported, and evaluation lacks emphasis on efficiency aspects and relevant efficient word embedding baselines. The results are interesting but the claims about comparable performance hold only in one of the two models explored.  ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "There is no ethical statement in the paper. Given that it deals with pretrained language models it would be useful to discuss what are the implications of training on web text and other potential biases that can be encoded in pretrained language models. ",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}