{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The work studied the problem of inserting backdoor into a deployed model through bit flip.\n\nSome important concerns have been proposed by reviewers, including: the incorrect claim of the treat model, the potential defenses are only discussed but not validated, experimental setups and analysis (e.g., the sensitivity test of hyper-parameters). Although the authors provided some responses, but all reviewers are not well convinced. \n\nAfter reading the manuscript, reviews and discussions between reviewers and authors, I think this work is not ready for publication. The reviewers' comments are supposed to be helpful to improve this work."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a method to inject backdoors into real-life deployments of\ndeep neural networks on hardware. More specifically, it proposes a constrained\nfine-tuning process to inject backdoors by locating vulnerable weights and\nupdating these vulnerable weights in hardware. Experiment results on\nhardware-deployed models show the effectiveness of the proposed method.",
            "main_review": "* Design and correctness\n\n  The design of this attack is mostly based on heuristics without theoretical\n  supports or analysis, e.g., how it identifies vulnerable weights. Its\n  evaluation results are on two datasets, CIFAR and ImageNet, which I am not\n  convinced to be enough to demonstrate how it can generalize to others.\n  Moreover, its experiment design lacks an ablation study.\n\n* Usage of clean data.\n\n  The proposed constrained fine-tuning process requires clean data to inject\n  backdoors. However, in Threat Model Section (i.e., Section 3.1), such\n  assumption is not mentioned. Moreover, it seems to be contradictory with the\n  claim that it does need training dataset.\n\n  Also, details of the used clean data in the injection process are missing. For\n  example, the number of clean samples used in the constrained fine-tuning\n  process.\n\n* FGSM\n\n  Existing work has studied methods to generate/reverse-engineer triggers. I do\n  not understand the logic of using FGSM instead. Are there any special\n  benefits, compared with existing work? Existing work [1] also simultaneously\n  optimizes the trigger patterns and model parameters.\n\n  Also, some details of the trigger generation process is missing, e.g., the\n  size of the trigger and the perturbation budget of FGSM. \n\n* Locating vulnerable weights\n\n  For locating vulnerable weights, this paper relaxes the combinatorial\n  optimization problem and heuristically locate these weights by simply ranking\n  the parameters' gradient. Are there any evidence or theoretical analysis to\n  support this design?\n\n* Hyper-parameters\n\n  This method has several hyper-parameters, for example, the loss balancing\n  parameter \\(\\alpha\\) in Equation 1, the size of the generated trigger, and the\n  perturbation budget for FGSM.\n\n* Countermeasures\n\n  Potential defenses are only discussed but not validated.\n\n* Ethics\n\n  The paper does not have an ethical statement, despite it is proposing an\n  attack.\n\n* Relevance\n\n  To me, this paper fits security conferences better. I do not see the\n  significance of this appearing in a AI/ML venue.\n\n[1] Pang, Ren, et al. \"A tale of evil twins: Adversarial inputs versus poisoned\nmodels.\" Proceedings of the 2020 ACM SIGSAC Conference on Computer and\nCommunications Security. 2020.",
            "summary_of_the_review": "To me, its contribution to AI/ML is not significant. Many designs are just\nheuristics. It is more like a security engineering paper whose study object is\nDNN. Also, it falls short in evaluation.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety"
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes to integrate physical viability inside the attack objective for backdoor attacks. Specifically, the authors observe a hardware limitation for the row hammer attack, which limits the possible locations where the bits can be flipped. Consequently, they change the attack objective to target sparse locations to ensure that RHA can be used.",
            "main_review": "Strengths:\n- The paper is well-written and easy to follow.\n- The targeted problem is interesting since not many prior work focus on the implementation of the attack using RHA inside the DRAM.\n- The contributions of the paper are mainly in steps 2 and 4 of the algorithm where the authors enforce block-sparse flipping and only 1 bit change in a byte. While these objectives are not directly optimized for and are merely enforced by heuristics, they are still of interest. \n\nWeaknesses and questions:\n- The method in [1] also emphasizes on implementation of the attack and proposes different techniques to ensure the attack can be realized in the DRAM. Can the authors please provide a 1-1 comparison with this work and explain how they improve/change compared to this prior work?\n- Since the appearance of bitflip attacks, many papers have focused on defense strategies [2-6]. Therefore, it is important to evaluate the proposed attack in face of existing defenses, preferably bypassing them successfully and/or better than prior attacks. While section 5.4 briefly goes over one defense, it merely shows that the evaluated defense inherently has a high false positive rate. However, this is a downside (corner case) for the defense strategy and not something that is specific to the proposed attack. Without proof of resilience against contemporary defenses, the attack cannot be impactful, even though it is realizable physically, thereby defeating the purpose.\n- The threat model is a bit misleading, specifically since it claims no access to training data while the optimization steps require data for trigger generation, finding the weight bits, and adversarial fine-tuning. Can the authors please specify the source and amount of the data used for these steps?\n\n[1] Yao, Fan, et al. \"Deephammer: Depleting the intelligence of deep neural networks through targeted chain of bit flips.\" 29th {USENIX} Security Symposium ({USENIX} Security 20). 2020.\n[2] Z. He et al., “Defending and harnessing the bit-flip based adversarial weight attack”. CVPR 2020. \n[3] J. Li et al., “Defending bit-flip attack through DNN weight reconstruction”. DAC 2020\n[4] Y. Li et al., “DeepDyve: Dynamic verification for deep neural networks”. ACM CCS 2020.\n[5] Q. Liu et al., “Concurrent weight encoding-based detection for bit-flip attack on neural network accelerators”. ICCAD 2020.\n[6] J. Li et al., “Radar: Run-time adversarial weight attack detection and accuracy recovery”. DATE 2021",
            "summary_of_the_review": "Please see the main review.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a method for realizing backdoor attacks for DNNs based on the Rowhammer attack. ",
            "main_review": "Comments\n1.\tThe threat model described in Section 3.1 is not compliant with the claims of the work. In particular: “we assume that the attacker… does not have access to the training hyperparameters, or the training dataset”. However, it is written several times that the training process has been modified by the attacker. How is that possible without access to the training hyperparameters and training dataset? It is also in contrast to the fact that in Section 3.2.2, the training samples are given as input to the optimization algorithm.\n2.\tThe experiments performed in Section 3.2.1 constitute the main motivations to design the proposed attack. However, the experimental setup used for Rowhammer profiling is not clear. This should be discussed more comprehensively.\n3.\tIt is not clear how the property of no co-occurrence in the same memory page among the flipped bits can be guaranteed. Does it assume the knowledge of the exact weight memory allocation?\n4.\tAuthors are encouraged to discuss the possible countermeasures that can be potentially adopted to defend against their proposed attack.\n\n\nRelated work suggestions\n1.\tV. Venceslai, A. Marchisio, I. Alouani, M. Martina and M. Shafique, \"NeuroAttack: Undermining Spiking Neural Networks Security through Externally Triggered Bit-Flips,\" 2020 International Joint Conference on Neural Networks (IJCNN), 2020, pp. 1-8, doi: 10.1109/IJCNN48605.2020.9207351.\n",
            "summary_of_the_review": "This work is promising and contributes to generating successful backdoor attacks in practical settings with good results. However, there are some concerns that affect the clarity of the method and of the threat model.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper explores backdoor injection attacks on machine learning models, the attack develops in the following way:\n\n1. The model and its parameters are studied offline to determine a trigger pattern and the bits that must be modified in the target model.\n2. A RowHammer attack is used to modify the bits of the model after it is loaded into DRAM\n3. When inputs are received which include a trigger, the model will misclassify the input. In the case of \"clean\" inputs, the aim is to leave the performance unchanged.\n\n* unlike previous work vulnerable bits are sought throughout the model (not just the last layer)\n* care is taken to understand the limitations of the RowHammer attack, i.e. in reality only a small number of specific bits can be flipped \n* a new technique is used to jointly find weight modifications and the data trigger pattern (CFT with BR)\n",
            "main_review": "\nThe work is interesting and closes the gap between theoretical backdoor attacks of this type and realisable ones. Results show that attacks are possible where 100% (Rmatch) of the required bit flips are flippable using a RowHammer attack. Nevertheless, it appears that  access to the model weights at runtime is required to perform the attack - sections 3.3 and 4.5 perhaps need strengthening to convince the reader that the threat here is real.\n\n1. what does the initial trigger mask look like? what is the size of the trigger?\n\n2. in terms of countermeasures, it appears to be the case that access is required to the weights file, i.e. ability to load into DRAM. If so, should this be added to the threat model section? Is it not easy to defend against? (would memory deduplication vulnerabilities also offer a route to do something similar?). keeping our weights file private / encrpyted appears to be a defense or couldn't we simply perform some sort of simple validation/ checksum when loading from DRAM? \n\n3. given model accuracy drops significantly after the attack on more complex models, we could also, periodically, simply test accuracy to detect attempts to modify the model? (e.g., in case other protections were circumvented)\n\n4. where are the bits that need to be flipped located precisely? can we learn anything interesting from this in terms of possible countermeasures?\n\n5. is the location of bits vulnerable to RowHammer something we could consider to help protect the model? (although this seems unnecessary given ease at which we could protect against any modifications to model weights). \n\n6. could we make changes to the model architecture to increase the number of bitflips required? What about current training-time defences to protect against adversarial inputs, are they helpful?\n  \nminor:\na) Section 3.1 \"misclassify the input to the target class\" - should there be some mention of the trigger here? The trigger gets drop in various places which perhaps makes the paper harder to follow?\nb) page 4: \"Specifically, \" - is the intention the reader then read the text in the box? The text in the box could be clearer. It is poorly written at the moment.\nc) I find the descriptions in section 4.5 difficult to follow. \n",
            "summary_of_the_review": "Overall an interesting paper, but I think some additional justification for why such attacks are not easily defended against is perhaps required.  Clarity could be improved in places. Given this I believe the paper is currently below the acceptance threshold.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}