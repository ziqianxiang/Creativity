{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This work proposes to extend the invariance/equivariance properties of GNNs by focusing on distance-preserving and angle-preserving transformations, given respectively by the Euclidean and Conformal group. Preliminary experiments are reported that demonstrate the advantage of such architectures. \nReviewers found this work generally interesting, tackling an important problem and proposing a valid solution. However, they also raised important concerns, namely the relatively minor novelty relative to recent models (such as EGNN), as well as the lack of convincing real-world experiments that would validate the modeling assumptions. Taking all these considerations into account, the AC recommends rejection at this time, and encourages the authors to address the points raised by reviewers in a revision."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper seeks to develop invariance and equivariant graph neural networks. \nBased on the assumption that node feautures can be separated into a\nnode coordinate feature vector and coordinate-independent node feature\nvector, the authors develop a GNN architecture, called \"angle\npreserving graph network\" (AGN), that is  equivariant to  the\nEuclidean group and conformal group. The method is evaluated using the\n\"benchmark\" graph datasets produced in (Dwivedi et al., 2020) from,\ne.g., MNIST and CIFAR10 datasets.",
            "main_review": "Pros:\n  + The attempt to develop invariance and equivariant graph neural\n  networks is valuable. \n\nConcerns:\n\n  - The proposed approach makes very strong assumptions, such as: 1)  node\n    coordinates (in an ambient Euclidean space) are given; 2) the\n    other noe features are  coordinate-independent.  This greatly\n    limits the applicability of the proposed approach.\n\n  -  With the above assumptions, in essence the authors explicitly \"hand-code\"\n     specific \"invariance\" and \"equivariance\" properties, namely,    rotation/translation invariance and angle-preserving\n     transformations as captured by Eucliean and conformal groups, in AGN. \n\n - The evaluation is done only on very limited graph datasets, namely\n   the Dwivedi et al., 2020 benchmark datasets where graphs are\n   generated from, e.g., 2D Euclidean datasets such as images, where\n   node coordinates are well defined. \n   \n\nOther comments:\n\n   - What are \"global\" attributes $\\vec{u}$ defined on? I presume that\n     they are not defined on per-node or per-edge basis. This is never\n     explained.\n\n   - Even under the assumption that node coordinates are available as\n     part of the node features, you are basically assuming that the\n     Euclidean distance is the right \"distance\" metric for the\n     underlying graphs. This may not hold in general at all. For\n     example, the nodes may lie in a sphere in an ambient 3-D\n     Euclidean space, and the \"right\" distance between the nodes should be the geodesic distance on the sphere, not the ambient Euclidean distance.\n\n - In your model eqs.(5a) - (5b), you are making a lot of implicit\n   assumptions. For example, in using 5a, you are assuming that edge\n   features are independent of node coordinates, but depend only on \"distances\"\n   between them. The same comments also apply to other equations, such\n   as the transformation of the node coordinates will be based only\n   node coordinates, but not other node features. \n   \n - Without any \"formal\" mathematical context on how graph data are\n   generated or sampled from, why would   rotation/translation/angle-preserving transformations would be the\n   right \"symmetry\" in the data? In particular, why would preserving \"angle\" between neighboring (three) nodes actually capture?\n\n - How does your model apply to more  general    graph datasets, e.g., molecular biological networks or social\n    networks such as co-authorship networks, where node coordinates\n    are not available, and the whole goal of graph representation via\n    node embedding is to find  \"appropriate\" node coordinates in a\n    proper embedding space.\n\nMinor nitpicks:\n   - in your definition of equivarance, you need to first fix the group action  on Y,  $\\phi'_g$, otherwise the definition does not make sense.\n\n  - I am not sure what value the materials in Sections A-E in the appendix really add. You are basically restating the translation-invariance and angle preserving properties of Euclidean groups and conformal groups, and sections D and E hold because you explicitly construct AGA to be distance-invariant and angle preserving.",
            "summary_of_the_review": "The paper seeks to develop invariance and equivariant graph neural\n networks. The proposed approach makes strong assumptions. As such, it\n has very limited applicability. The evaluation is also very limited.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes two kinds of equivariant GNNs, including the distance preserving graph network (DGN) that is equivariant to the Euclidean transformations and angle preserving graph network (AGN) that is equivariant to the Conformal group. The proposed frameworks generalize several typical previous architectures. The advantage is experimentally evaluated on a synthetic dataset composed of n-dimensional geometric objects. ",
            "main_review": "Strengths:\n\n1.\tThis paper is well written. I enjoy the reading. The authors have introduced the necessary conceptions in a comfortable way. It is easy to follow the whole idea and the methodology.\n\n2.\tIt is good that the proposed DGN and AGN can generalize the current methods including EGNN and DIMENET, by implementing different choices of certain components. The validity of preserving the distance or angles is clearly justified (mainly in appendix).\n\n3.\tThe experiments on polytopes classification, though seems over-simplified, are able to demonstrate the superiority of the proposed AGN over other methods (GN, SE3-Trans, DimeNET, EGNN) particularly when non-orthogonal symmetry is considered. \n\nWeaknesses:\n\n1.\tThe novelty is somehow limited. It is indeed that Equations (5-6) have extended the design of GN (Eq. (4)) by involving equivariance w.r.t. node coordinates and invariance w.r.t. other embedding features. But the main step contributed to this property lies in the update of the node coordinates, namely, Eq. (5c) and Eq. (6d), which is previously developed by EGNN (Satorras et al., 2021) and other related works. This can be checked by comparing Eq. (3) used in this paper with Eq. (4) in the EGNN paper. Even the authors have additionally proved that Eq. (3) is equivariant to Conformal orthogonal transformations which is not discussed in EGNN, such equivariance is trivial to derive. Specifically, for DGN, it is very similar to EGNN, unless that DGN is built upon GN (with global vector u) while EGNN is upon MPNN.\n\n2.\tThe proposed AGN and DGN show merits only on the simulated dataset. For the real datasets (MNIST, CIFAR10, and TSP), the performance of AGN and DGN are even worse than GN. The authors attribute this detriment to the lack of symmetry in the data. This seems somewhat questionable. Since the images for example in MNIST are actually rotation/translation equivariant, DGN is supposed to perform better than GN, given that DGN is a rotation/translation equivariant version of GN by the design in Eq.(5). Previous studies such as group CNN have also demonstrated further taking the rotation equivariance into account delivers desirable enhancement. The authors are suggested to provide more explanations on this issue. \n\n3.\tIn terms of evaluations on polytopes classification, all methods achieve accuracy 1 for training. Does this mean that the simulated dataset is too simple? What is the number of the training/testing samples? Is it too small?\n\n4.\tThere remain several confusing presentations in the current version:\n\n4.1\tIn the first paragraph, the authors state that graph neural networks, are invariant to the symmetry groups. The “symmetry group” here is too general and includes other cases besides the permutation group. \n\n4.2\tIt seems AGN is only equivariant with regard to the Conformal orthogonal group (a subgroup of Conformal transformations) according to the derivations in Appendix C. So why has the claim that AGN is equivariant to the Conformal group?\n\n4.3\tThe authors are suggested to provide formal definitions of these groups: E(n), CO(Rn;Q) and Conf(Rn;0) groups. \n",
            "summary_of_the_review": "Overall, there are certain merits for the proposed architectures, but both the originality and the experimental significance are insufficient. I initially suggest weak rejection. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose distance preserving Graph Network and Angle Preserving Graph Networks which are equivariant to node permutation as well distance preserving transformations/ angle preserving transformations of the coordinates associated with the nodes. The authors generalize the E(N) GNN work to include group symmetries to the conformal group and to the case when not all nodes in the graph are connected. Experiments on the  polytopes classification dataset show that the model is effective when symmetries are present in the data, and ineffective when they are not (MNIST, CIFAR and TSP).",
            "main_review": "Initial Recommendation: Marginal Accept\n\nI have specified the reasons for my recommendation below:\n\nStrengths:\n1. The paper is clearly structured and well written, and I found it easy to read.\n2. Analogous to E(N) GNN - the model is simple and computationally efficient (along with quantitative results which show computational complexity) - without the use of special functions like Bessel functions, etc to capture equivariance to the desired transformation actions.\n3. Strong experimental results on the synthetic polytope classification datasets when the symmetries are present - and also provide the reader examples of when the model should not be used.\n \nWeakness:\n1. The authors discuss the ability to capture molecular conformers as one of the main selling points of the network being equivariant to the conformal group but do not show results on the QM9/ other macromolecular datasets in comparison to E(N) GNN, but without using a fully connected graph (Apart from rate of convergence in the appendix). For instance, in molecules, only when covalent/ hydrogen bonds are present does the distance between atoms need to be preserved, right?.\n2. The authors do not present other use cases (or motivations of this work) of the symmetries to the conformal group or distance preserving transformations of an object - for example results on point cloud datasets (where the objects are not rigid bodies) would do well for this work.\n\n\nMinor:\n1. Introduce $a_{ji}$ earlier rather than in page 4 as it is used in eq 3 (in pg 3)",
            "summary_of_the_review": "The paper is well written and the proposed solution is simple and computationally efficient. The claims are sound and the empirical evaluation is partly adequate but partly needs work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes two graph networks that are equivariant to distance and angle preserving transformations in graph coordinates. It moves one step forward based on GN (Graph Network) by decoupling node coordinates from other node attributes. The experiments on synthetic datasets shows the capabilities of the proposed symmetry-driven graph networks.",
            "main_review": "This paper proposes DGN and AGN that can update graph representation equivariantly w.r.t. any transformation preserving distance and angles, respectively. The authors conduct experiments on synthetic data to demonstrate the effectiveness of considering symmetries.\n\n#####Pros#####\n\n1. It is intuitively sound and necessary to consider the symmetries of data. The motivation of this work is clearly explained and convincing.\n\n\n2. The proposed DGN and AGN, shown in Figure 2 (b) and (c), are technically sound and general enough. They are developed based on the general framework GN (Figure 2 (a)). Most existing methods, such as SchNet and DimeNet included in Section 5, can be viewed as special cases of the proposed frameworks. Hence, the proposed DGN and AGN are unified well.\n\n\n3. The experiments on synthetic data are well-designed, which can show the effectiveness of exploiting symmetries. This can support the methodology strongly.\n\n\n#####Cons#####\n\n1. The main concern is that there are no empirical results to demonstrate that DGN or AGN is effective on real-world tasks, instead of specifically designed synthetic tasks. It is strongly desired that the proposed method can be evaluated on real datasets.\n",
            "summary_of_the_review": "Overall, I think the current version of this paper is slightly above the acceptance threshold since its motivation and proposed methods are technically sound and convincing.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}