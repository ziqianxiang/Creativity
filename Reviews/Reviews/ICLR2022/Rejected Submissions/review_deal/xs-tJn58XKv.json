{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The idea of learning unstable features from source tasks to help learn stable features for a target task is interesting and well-motivated. As the proposed method and its theoretical analysis of learning unstable features from tasks are an incremental extension of an existing work [Bao et al. 2021], the technical contributions line in applying the idea of stable and unstable features learning to the setting of transfer learning. Therefore, the evaluation of this work is focused on the effectiveness of the proposed method in the transfer learning setting. \n\nIn transfer learning, one major goal is to make use of knowledge extracted from source tasks to help learn a precise target classifier even with a few or no labeled examples of the target task. It would be more convincing if experiments are conducted to show how the performance of the proposed method changes when the size of labeled data of the target task changes. This is to verify whether the exploitation of unstable features can help to learn a stable classifier for the target tasks more efficiently (i.e., with fewer labeled examples). In addition, as some baseline methods used for comparison do not need to access any labeled data of the target task (like unsupervised domain adaptation or domain generalization approaches), it is not fair to conduct comparison experiments in the setting where there are sufficient labeled examples of the target task since the original designs of such baselines may fail to fully exploit label information in the target task. \n\nAnother concern is whether the proposed method is realistic for real-world transfer learning problems. Though in the rebuttal, the authors provided experimental results on a natural environment (CelebA), the constructed transfer learning problem is more like a toy problem. Indeed, there are many transfer learning benchmark datasets that contain multiple domains/tasks. It would be more convincing if experiments are conducted on those datasets.\n\nBy considering the above two concerns, this paper is on the borderline. My recommendation is a weak rejection based on the current form of this paper. Note that as some references listed by reviewers RJhJ and J8M5 are not really related to the proposed research here, the novelty of the proposed method compared with those references is NOT taken into consideration to make this recommendation."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors introduce a method (TOFU) for learning classifiers that are robust to spurious correlations in a transfer learning setting. They argue that approaches which rely only on (input, label) pairs and use no extra information to identify spurious features could fail to learn a robust model due to insufficient data for the target task. The authors provide scenarios where useful metadata, namely varying environments, could be available for source tasks, ready to be leveraged by the target task classifier for the purpose of identifying a shared bias that might exist across all tasks. To identify unstable/spurious features, the authors follow a 3-step procedure proposed in an existing work. Namely, a classifier is trained on data from environment E1, and is evaluated on data from a second environment E2. The data in E2 is then partitioned according to the correctness of the classifier's predictions on a per-class basis. A metric learning objective using a triplet loss is used to learn a model which embeds samples according to their unstable features with the goal that the unstable features of the correctly predicted samples should be closer together than unstable features between correctly predicted and incorrectly predicted samples. The objective is justified based on prior theoretical work and is illustrated visually. Extensive empirical evaluations are performed, comparing the performance of TOFU against many different baseline methods on both image and text classification tasks.",
            "main_review": "The paper itself is well written, and does a good job at describing the current research landscape of robust feature learning. Some more thought should be given to the example provided of why a method like TOFU is required, since the current one of using class labels to predict color instead of vice versa seems contrived, and undersells the usefulness of the method. The theoretical claims made throughout the paper are sound, but it is not obvious that they should hold in practice. For example, on datasets where there is heterogeneity in terms of the difficulty in predicting samples, i.e. easy, medium, and ambiguous samples, easy and medium samples might end up in X_1 and X_2, and the ambiguous samples in X_3. Thus, the objective in equation (1) could end up optimizing not for the unstable features in X_1 and X_2 to be closer together than those in X_1 and X_3, but rather for easy samples to be closer together than easy and difficult samples in the learned feature space Z. Additionally, it may be possible to improve S.2 by considering not only correctly predicted vs incorrectly predicted, but also the particular errors that were made. Consider a scenario where images of cars are either misclassified as boats or bicycles. Bicycles are more likely to be on the road than boats, so it may be wise to exclude the cars misclassified as bicycles from X_3, since those contain the correct unstable feature being sought, but violate the assumption of theorem 1. It seems like for TOFU to work, the classifiers trained in step S.1 need to be unstable such that they will make sufficient errors on E_2 in order to have sufficient negative samples for the triplet loss objective in S.3. The limitations of metric learning with a triplet loss should be stated since they could have an influence on model selection done throughout the method.\n \nTOFU has fewer requirements in terms of additional annotations compared to other methods, and domain specific prior knowledge is encoded into how the datasets are split into E_1 and E_2. Quantitative analysis of TOFU reveals that it is significantly superior to the many baselines considered, and that is consistent on all tasks and datasets. This is impressive and rare since many methods in the broader deep learning literature improve on some tasks, but do worse on others. It is also satisfying to see how well TOFU performs in terms of clustering metrics compared to representations learned only for the purpose of classifying the source task. Naturally, the latter representations would contain information encoding the spurious features since they are then leveraged to predict the class label, but they seem to be highly ineffective for the purpose of clustering by unstable feature. Having said that, this analysis is a bit confounded by the fact that ERM uses supervised learning whereas TOFU uses metric learning. Additionally, it would be ideal to have a discussion regarding why the performance improvement of TOFU over ERM is not as significant for the Beer Review data compared to MNIST, even though the clustering scores are much better. In general, the difficulty of the tasks considered for the empirical evaluation should be discussed as it seems to vary quite a bit.\n \nThe novelty of the work is somewhat limited. S.1 to S.3 is nearly identical to the cited work of Bao et al., except that the objective in S.3 is changed to the triplet loss objective. This objective comes from Theorem 1 which is a novel contribution, but is based on theoretical results also from the cited work of Bao et al. It would also be recommended that more real life examples of the transfer learning scenario in question be provided, since it currently seems as if it was created to show the applicability of the method. Overall, the paper is very strong from an empirical perspective, is well structured and reproducible, but ignores any potential limitations and is limited in its discussion.\n\nStrengths\n-Paper is well written and self-contained.\n-Impressive empirical results on many tasks/datasets relevant for this application.\n-Provides an interesting perspective on combining transfer learning and robust feature learning,\n \nWeaknesses\n-Not so clear how the datasets were split to generate different E1 and E2. For example, in the Waterbirds dataset, if the source task is classifying waterfowl, is the only difference between E1 and E2 the percentage of images that have a water background? This is ambiguous in the appendix as well.\n-Conclusion is too short, and is missing a discussion of limitations and future directions.\n-A clear limitation of the work is that the unstable features in the target task have to be the same ones as in the source task\n",
            "summary_of_the_review": "This work introduces a new scenario where current methods for learning robust features are not capable of leveraging all available data. It also provides a solution in the form of a method called TOFU which has few requirements in terms of additional data annotations compared to existing methods. The authors recognize that bias is a human defined concept, and could vary from dataset to dataset, so it is best to have a method that can identify unstable features in an automated way. TOFU is an extension of an existing work, and has some theoretical motivations, but the underlying assumptions may be too strong. TOFU outperforms all other methods considered by a large margin, and the authors investigate a potential source of its success by confirming that it is indeed able to cluster data well according to unstable features: a requirement for later on doing group DRO. Sufficient details are provided to enable reproducibility, and many baselines are compared against, making the experiments comprehensive. However, the method is not particularly novel, nor is it clear if the scenario presented occurs often in real life. Limitations of the method are not considered, even though there are a few clear ones both technical and empirical.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a two-stage method for transfer learning, which firstly infers unstable features from the source task and then learns stable correlations for the target. Experimental results validate the effectiveness of the proposed method.",
            "main_review": "(1) Strength: The problem that this paper addressed is important and essential for machine learning. And the whole pipeline is reasonable and achieves good results.\n\t(2) Weakness: \n\t\t(a) Method: The proposed method infers the unstable features on the source task and then learns the stable correlation on the target leveraging the knowledge of unstable features. As for the methods, it uses the Group-DRO as the backbone and clusters the available testing data to produce the group labels. However, I think the method is quite ad-hoc and naïve, and it lacks technical contributions. The idea of clustering data with unstable features is also quite similar to the HRM[1], which further reduces the contributions.\n\t\t(b)Problem Setting: This method requires both labeled data from the source task and target task, which uses much much more information than existing methods for OOD generalization. \n\t\t(c)Theoretical Analysis: There is almost no theoretical analysis for the proposed method. Why it can infer the unstable features? Why this method can generalize to unseen domains? What if the worst-case group also reflects some spurious correlations?\n\t\t(d)Experiments: Since it is both the OOD generalization method and the transfer learning method (setting is the same), there lack many baselines. I think more transfer learning methods should be taken into consideration, as well as some domain generalization methods.\n\t\t(e)Definitions: What do the stable features or unstable features mean in this paper? What is the difference between causal features[2], invariant features[3], or stable features[4][5]? What is the formal definition of stable and unstable? If it is just the same as causal features, why use a totally new name? I think the authors missed many related works here.\n\n[1] Liu, J., Hu, Z., Cui, P., Li, B., & Shen, Z. (2021). Heterogeneous Risk Minimization. ICML2021\n[2] Peters, J., Bühlmann, P., & Meinshausen, N. (2016). Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 947-1012.\n[3 Koyama, M., & Yamaguchi, S. (2020). Out-of-distribution generalization with maximal invariant predictor. arXiv preprint arXiv:2008.01883.]\n[4] Kuang, K., Xiong, R., Cui, P., Athey, S. and Li, B., 2020, April. Stable prediction with model misspecification and agnostic distribution shift. AAAI 2020\n[5] Shen, Z., Cui, P., Liu, J., Zhang, T., Li, B. and Chen, Z., 2020, August. Stable learning via differentiated variable decorrelation. KDD 2020\n",
            "summary_of_the_review": "This paper proposes a two-stage method for OOD generalization and transfer learning problems. However, there exist many weaknesses, including method, problem setting, theoretical analysis and experiments. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper is concerned with learning a model in situations when some features have spurious correlations with the label (for example, for classifying sheep vs camel, background can be a spurious feature). The idea is that if a lot of source data is available, these spurious correlation features should be easy to find. To find the unstable features, authors hypothesize that they are related to mistakes that a classifier makes in different environments. Therefore, they learn a model on one environment and run it on a different environment (source data), splitting the data into correctly predicted and not. Then for each partition (correct or not) examples with the same labels are encouraged to be close together, so the embedding learns UNSTABLE features (fz). Then the target data is clustered based on unstable features representation fz, and then DRO (existing method that assumes the existance of correct groupping based on unstable features) is used to train a robust target classifier",
            "main_review": "In general, the method seems to work and it seems to be justified (both theoretically, albeit in the appendix, and practically). It is really complicated though (involves many steps) and i fail to make a connection between this and say Domain Invariant Representation learning. For example, if i was to solve the problem of domain generalization (the setup seems really similar to me), a reasonable baseline would be to train a source model that learns an invariantt representation between various environments (using DANN, or CMD or MMD or whatever). But this is not in the baselines. Why is that? One difference i \"kinda\" see is that domains are more or less pure - eg domain is really a different background. The way i read it is that environement is more mixed - it can contain multiple backgrounds. It would be nice to draw comparison between your method and DIRL or at least explain why DIRL is not applicable\n\n\nPros:\n- The paper is well written and easy to follow (albeit a lot of helpful info is also in the Appendix)\n- The experimental results seem convincing, there is a study of dependence of (some) hyperparameters on the end result (num of clusters)\n\nCons:\n- DRO needs at least a brief introduction\n- Experiments are only on 2 environements (domains) in train\n- The method seems extremely costly: for a number n of source environements, you train n classifiers, then for n^2 pairs you do partions, then you learn fz representation, then cluster the target data all while tuning and looking for fz dimension and number of clusters. \n-There are hyperparameters that need to be tuned: fz dimension, number of clusters. Tuning all the hyperparameters using limited target data - how does it work? Do you tune the fz dimension (i could not understand it from text, i think u do tune the num of clusters but not sure about fz dimension)\n\n\n\nAdditional comments:\n- For figure 1, source task environments actually look the same to me tbh: the red is mostly correlated with 0 and green is mostly correlated with 1 for both of the environments. If they were flipped between the environments,i would expect Domain invariant representation learning to be able to filter out the color from the embedding layer\n- what if target is also a mix of different environments (you seem to assume that it is all comes from one domain/env \n- How practical is experimental setup you are testing on? For your experiments train was coming from 2 environments that were created artificially\n\nMinor: then we uses=>then we use",
            "summary_of_the_review": "Please see main review",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper considers a transfer problem when the spurious correlations of source tasks can be applied to the target task. The authors propose to identify the unstable features on source tasks first and then cluster the target data according to these features. Finally, an invariance-based method are incorporated to eliminated the influence of unstable features.",
            "main_review": "Strength:\n(1) The authors address a critical point that prevents models from generalization, namely spurious correlation.\n(2) The whole pipeline is intuitive and easy to follow, and the empirical results are within expectation.\n\nWeakness:\n(1) A critical limitation of this work is the strong assumption on the transferability of the unstable feature. IMO, such an assumption is restrictive in most settings. For the intra-task transfer (where the classification tasks are the same between source and target), finding the invariant part via invariance learning methods is more realistic. As for the inter-task transfer (a wilder setting),  it is very rare that the unstable features and the way they correlate the outcome are exactly the same across the tasks.\n\n(2) The related work and baselines are not discussed thoroughly. First of all, more transfer learning methods should be discussed, not just REUSE and FINTUNE. Moreover, several automatic de-biasing methods [1,2,3] proposed recently could also be considered to involve.\n\n[1] Qiao, F., Zhao, L., & Peng, X. (2020). Learning to Learn Single Domain Generalization. In CVPR. \n[2] Matsuura, T. and Harada, T., 2020, April. Domain generalization using a mixture of multiple latent domains. AAAI\n[3] J. Liu, Z. Hu, P. Cui, B. Li, and Z. Shen, “Heterogeneous risk minimization”, ICML 2021.",
            "summary_of_the_review": "This paper addresses the spurious correlation by transferring knowledge from source tasks. Although intuitions are provided and empirical effectiveness is illustrated accordingly, the method is restrictive due to the strong assumption on the transferability of unstable features. Several important related work and baselines are also missing.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper aims to transfer the knowledge of spurious correlations in a set of source environments to a target environment. They assume the degree of spurious patterns vary among source environments (e.g. color has different correlations with the number in MNIST), and the spurious pattern are the same in both the source and target tasks. Then they aim to transfer the spurious knowledge from source environments to learn a classifier in the target task that ignores the spurious pattern. \n\nThey train the model in 3 stages. First, they train a regular classifier in each environment. E.g. given two source environments E1 and E2, they train the corresponding classifiers f1, f2. Second, they use the error of the f1 on E2 to \"separate\" the E2 examples into two groups, one group for correct predictions and the other group for the wrong predictions under each class. They assume the cause of the error comes from the spurious patterns, so each group will correspond to either high or low spurious patterns learned in f1. Thus they learn a f_Z that outputs an embedding to separate these two groups by triplet loss. Finally, in the target task, they cluster the examples into different groups by the similarity of f_Z that captures examples with similar spurious patterns. Then they apply Distribution Robust Optimization (DRO) on these groups that optimizes the worst performance across all groups. This ignores the spurious patterns based on f_Z in the target task and learns a stable classifier.",
            "main_review": "Overall I like this paper. This paper does a good job of explaining the setup and the writings are mostly clear. This paper builds upon the core idea from Bao et al. 2021 that uses the error of the model in different environments to capture the spurious patterns, and it further extends to the transfer learning setup, which I believe it's a new setup.\n\nMy concerns are as followed:\n1. The assumptions are too strong in my opinions, and I think a discussion section will help readers understand the limitations:\n - A) I am skeptical about the assumption in Bao et al. 2021 that uses the error of the model across environments to capture the spuriousness. There could be many factors behind errors across environments such as mislabeled data, different viewpoints of the image (e.g. rotations), noise, and data distribution shifts. None of it corresponds to the unstable features. If this assumption breaks, then there is no real spurious pattern to be learned here.\n- B) On the data side, this work assumes the source tasks need to have varying degrees of spuriousness and the target task has the same spurious patterns. How do we possibly check these assumptions are real in the real-world setting?\n\n2. Regarding the demonstrated results\n- A) There could be different spurious factors across different classes. And the binarization procedure of correct or incorrect predictions could lead to underspecification since there might be multiple spurious factors but we only treat them as two. Will this method break if there are multiple spurious patterns?\n- B) The multi-stage approach of training could be brittle, and selecting hyperparameters could be difficult since no spurious correlation is known beforehand. Maybe the authors can show if this approach is robust when the assumptions are mildly violated like there is a distribution shift among environments, or when there are multiple spurious patterns in the data, and illustrate how to select hyperparameters.",
            "summary_of_the_review": "Pros:\n+ A new setup that transfers the knowledge of spurious patterns to a target task.\n+ The writing is clear. Figures and tables are beautifully produced.\n+ The results are demonstrated in multiple datasets and the number is convincing compared to several recent baselines. The ablation study of group numbers is great.\n\nCons:\n- The hypotheses in my opinion are a bit too strong. It's unclear in what real-world settings this method will work unless in the contrived setup in this paper. Including a limitation in the paper can strengthen the paper.\n- The multi-stage approach of training could be brittle.\n\nI am leaning towards the borderline with marginal acceptance. The experiments are complete with multiple baselines, clear writing and a good ablation study. I feel the assumption is too strong which would need some justifications. And if the authors can show a robust study under different mildly violated assumptions could further strengthen this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}