{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes Virtual MCTS, an early-termination rule for MCTS to improve its efficiency. \nThe basic idea is to introduce a termination rule that prunes the search process when the final policy at the root node is unlikely to change from the current one. The proposed approach is empirically evaluated on 9x9 Go and Atari games.\n\nAfter reading the authors' feedback, all reviewers participated in the discussion without reaching a consensus.\nAlthough all reviewers appreciated the authors' answers to their concerns, only one reviewer voted for acceptance. The other two reviewers,  while acknowledging some merits, still have concerns: the technical contribution is minor, the theoretical findings are quite trivial, it is unclear when the proposed termination strategy is could help.\nIn summary, this paper is borderline and I think it still needs some work to clearly break the bar of a top conference."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes Virtual MCTS, an early-termination rule for MCTS to improve its efficiency. Roughly speaking, the termination rule prunes the search process when the final policy at the root node is unlikely to change by too much from the current policy. This strategy improves the efficiency of AlphaGoZero-style algorithms. Specifically, the authors showed that Virtual MCTS improves the learning efficiency on 9*9 Go.",
            "main_review": "The main contribution of the paper is an early-termination rule for MCTS. While the experiments show that the proposed V-MCTS is more efficient on certain tasks, my main concern is that the termination rule might be trivial in practice (e.g., simply perform fewer queries) since it does not really depend on the values. Suppose we have a budget of $N$ nodes to query. Intuitively, Lemma 2 suggests than when we have already visited close-to-$N$ nodes, then the final policy $\\pi$ is unlikely to change too much. Although this is true, I think this does not necessarily tell us more than what we already know: since we only have a few queries left, the change in $N(s,a)$ will be minor, and so will the change in $\\pi$. Therefore, I don’t see many conceptual differences between V-MCTS and selecting a better total computational budget (the total number of queries). Although I believe there are some important differences between V-MCTS and MCTS with optimized computational budget (see details below), but base on the experiments I can’t evaluate whether this is significant.\n\nSpecifically, I believe V-MCTS can more efficiently prune queries for “simple” states, since for these states $\\pi$ could remain almost unchanged during the search. In contrast, for “harder” states, $\\pi$ could change drastically after certain “rewarding” states have been discovered. I have a strong feeling that V-MCTS works because of this reason, and I think this point makes perfect sense. However, base on the current experiments I cannot decide whether this is true. I think the paper could be improved if the authors can analyze this case better.\n\nFrom a theoretical perspective, Lemmas 1 and 2 do not provide new insights to the problem. The bound in Lemma 2 is based on the simple fact that $N(s,a)$ cannot change too much when the remaining number of queries is small. Therefore, in my humble opinion, the paper should focus more on empirically explaining the termination rule.",
            "summary_of_the_review": "Overall, I tend to vote for rejection since (i) the paper’s theoretical findings are rather trivial and (ii) the paper does not provide good insight on why the termination rule works, or on what kind of problem can it performs better. But I think the proposed termination rule follows my intuition and the paper could be improved if the authors provide more detailed analyses on that.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes the Virtual MCTS (V- MCTS), a variant of MCTS that mimics the human behavior, and is 50% more sample efficient, by performing a type of forward pruning.\nMCTS is characterized as a model-based reinforcement learning algorithm, that imagines what the future would look like, using terminology borrowed from Sutton in his description of Dyna, a model-based algorithm. \nI agree with the planning part, but am less certain if I agree with the learning characterization of MCTS.\nOften, MCTS is used inside a model-based approach as the planning component. MCTS is usually not regarded as a full model-based approach.",
            "main_review": "The paper incorrectly references the original MCTS/UCT version of Kocsis and Szepesvari for AlphaZero, however, AlphaZero used Rosin's P-UCT, not the original UCT.\nThe proposed method uses forward pruning/early termination in UCT to reduce the search effort. In effect, it introduces a variant of the UCT selection rule.\nThis idea is a fruitful idea, that has been tried by Lorentz in 2015 (Early Playout Termination in MCTS, Advances in Computer Games 2015) and by Hsueh in 2016 in Theoretical Computer Science for An analysis for strength improvement of an MCTS-based program playing Chinese dark chess.\nThese variations of UCT indeed also go back to the work of Auer and Cesa Bianchi (2002) on UCB1, on which UCT was based, which they reference as their first reference. \nThis idea has been shown to indeed improve performance in certain situations.\nThe proposed approach appears to be a variation on these older experiments.\n\nThe authors have  not presented comparisons to these earlier works in non-deep learning environments. I would be very interested to learn of a comparison of their new approach to these earlier selection rules. I would consider such comparisons essential for considering the new rule for publication in ICLR.\n\nThere is one unresolved reference to a section in the paper.\n",
            "summary_of_the_review": "The paper presents an interesting new termination criterion for MCTS.\nThe work should include comparisons to other selection rules, and misses references to some of these.\nWithout these comparisons, it is unclear how substantive the contributions are, and I do not recommend acceptance. \nThe language of the paper must also be improved.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes an approach for a significant speedup of Monte-Carlo Tree Search (MCTS) at a relatively small cost in playing strength. The basic idea is that, when the change in the distribution of visit counts between two different time points $\\frac{k}{2}$ and $k$ is less than some constant $\\epsilon$, it can also be shown that the remaining change in distributions between $k$ and $N$ (where $N$ is the maximum visit count allowed by some budget) will be bounded below some value, and if we consider such a maximum possible error to be sufficiently small we can just terminate the search early at time $k$. The paper also proposes a very simple but important idea called Virtual Expansions, which basically consists of running $N - k$ additional iterations of the bandit algorithm used by MCTS solely in the root node, providing the current estimated average values as rewards for every pull (i.e., leaving average reward estimates unchanged), in order to transform the distribution of visits at iteration $k$ into a prediction of the distribution we would end up with after $N$ iterations.\n\nSeveral empirical evaluations in Go ($9\\times9$ board) and a few Atari games, including ablation studies, demonstrate that the approach can substantially reduce the search times and self-play training times, while only decreasing playing strength by a small amount.",
            "main_review": "**Primary strengths**:\n1) Solid and extensive empirical evaluation with ablation studies.\n2) The main ideas of the paper are clearly described.\n3) The proposed approach looks interesting and useful. I especially like the idea of virtual expansions, even if (or maybe especially because) it is very simple but effective. It would be even more elegant if the changes in distributions from steps $k$ to $N$ could be computed at once, rather than in $N - k$ little steps... but I suppose that might be impossible. In practice it probably doesn't matter much, it should be really fast anyway without actually rolling out any new game states or performing any neural network evaluations.\n\n**Primary weaknesses**:\n1) There are a few technical errors (relatively easily fixable ones, but also important ones). \n\n    Firstly, section 3.1 describes Eq. (1) as \"selection rule named UCT\". This is wrong in several ways. UCT is actually a variant of the entire MCTS algorithm (not a selection rule), which uses *UCB1* as its selection rule. The selection rule described by Eq. (1) is a different one; it's the PUCT variant used by AlphaGo/AlphaGo Zero/AlphaZero/etc. That particular slowly-decreasing exploration hyperparameter was only seen (if I recall correctly) in AlphaZero, before that it was just a constant.\n\n    Secondly, there seems to be a mistake in Lemma 1: if I'm not mistaken, the $(Q_{k_1}(s, a) + 1)$ term at the end should actually be $(1 - Q_{k_1}(s, a))$. In the end this may not end up mattering too much; the extreme values that the bound can take end up being the same, but flipped around for the two different extreme values that $Q_{k_1}(s, a)$ can take, because the $Q$-values are in $[-1, 1]$... but it still looks like a mistake.\n\n2) More small mistakes (like sloppy notation a few times, see detailed comments below) and minor grammatical/spelling issues. Overall the latter don't impact comprehensibility too much though in my opinion.\n\n---\n\n**Detailed comments**:\n- shielding light --> shedding light\n- \"We propose an MCTS algorithm variant that can behave like a human.\" --> this kind of claims about \"human-like\" behaviour is always very dangerous in my opinion. \n- I found the notation where $\\hat{Q}$-values are individual outcomes from individual iterations, and $Q$-values are averages of multiple such $\\hat{Q}$-values, confusing. They're too similar, and often in other work the hat actually signifies an \"approximation\" whereas the version without a hat signifies a \"theoretical\" or \"true\" value. I would recommend changing the $\\hat{Q}$-values to $R$-values for example (since they're like rewards in reinforcement learning).\n- The text in the main paper around Lemmas 1 and 2 should at least mention that their proofs can be found in supplementary material.\n- Section ?? at end of page 4.\n- Start of section 4.3 points to \"line 5-10\" in Algorithm 3, but I think it should actually be lines 8-13?\n- Second paragraph of 5.1 mentions an agent named \"GNU\", but it's actually named \"GNU Go\"\n- What do you mean by \"100 pieces as the black player and 100 pieces as the white one\"? Should this be \"games\" instead of \"pieces\"?\n- The colours used in the plots are probably not colour-blind friendly (consider that the most common form of colour-blindness is red-green colour-blindness). \n- I don't understand why some results in Table 1 are printed in boldface and others not. Every single row actually has the best result not in boldface, but.... the second best?\n- (Coulom, 2006) --> (Coulom, 2007)\n- Capitalise \"Nature\" in (Silver et al., 2016)\n- Should cite the publication in Science instead of the arXiv one for AlphaZero\n- Descriptions of implementation details of MCTS / experiments in supplementary material are not sufficient. \"All the other implementations follows the Alpha-series paper (Silver et al., 2016; 2017; Schrittwieser, 2020)\" does not tell us a lot. There are many differences between those three cited papers (AlphaGo used human expert data, playouts, REINFORCE, etc.; AlphaGo Zero used no human expert data, no playouts, MCTS visit distributions as targets for policy, etc.), and many details on MCTS-related hyperparameters are also not sufficiently described in those papers; $C_{puct}$ value, $Q$-value assigned to unvisited children, etc.\n- In proof for Lemma 1, denominator in Eq. (3) should be $N_{k_2}(s, a)$ instead of just $N_{k_2}$.\n- In proof for Lemma 2, Eq. (5) has a $\\tilde{\\pi}_N(N/2)$ term, but $N/2$ is not a state.\n\n---\n\n**After response from authors:** I am satisfied with how the authors have addressed the issues and updated my score accordingly.",
            "summary_of_the_review": "While the paper has some interesting and well-evaluated ideas, and definitely should be publishable at some point, in its current form I feel that it is not yet ready; there are too many little (but sometimes important) errors.\n\n---\n\n**After response from authors:** I am satisfied with how the authors have addressed the issues and updated my score accordingly.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}