{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes a set-based neural subsampling model that selects both features and instances using a two-stage model. The motivation is to allow for scaling to large datasets by first subsampling using an initial stage that does not model second-order interactions (which would require work quadratic in the dataset size to model), and then following up with a second more sophisticated pass that includes second-order interactions. Its results show an empirical improvement over previous methods in the case of very small subsample sizes.\n\nThe responses from the reviewers in discussion were varied—and often off-base for all reviewers—and as a result I took an even more deep look at this paper than usual. I think the variance in reviewer response is a symptom of the fact that the paper is somewhat confusingly written, and sometimes has parts that give the opposite impression to what the authors intend. For example, the author response says \"we emphasize again that our method is a subsampling method. This is very different from core-set selection methods\" but then Figures 11 and 12 explicitly label the subset produced by the algorithm a \"coreset.\" There is general confusion as to what exactly is being subsampled (features or instances) and even what datasets were being evaluated on—it's not that the information is not there, but rather that it's easy to miss while reading the paper. We can see this happening where most of the reviewers were misunderstanding or making factual errors about the paper, and I can see how this happened by reading the paper.\n\nThe reviewers also shared some concerns about the baselines, and indeed some things about the baseline comparisons are confusing: for example, Figure 4 seems to report DPS having below 70% accuracy on MNIST while subsampling to a size of 25, but the original DPS paper (Huijben et al, ICLR 2020) in their Figure 2 reports a percent error of 6.6% (at Pixels removed: 96.8%, which I believe corresponds to keeping 25 pixels as 28*28*(1-0.968) = 25). This does seem to back up the reviewer's speculation that \"the baselines are most likely unfairly weaker.\" The presentation of the results should make this sort of thing more clear (if the setup isn't the same as DPS's MNIST setup, how does it differ? if the setup is the same, as seems to be the case the way the paper is presently written, why are the result accuracies so different from what is published in the DPS paper?).\n\nThe reviewer comment about theory is not one I count against the submission. Although it is certainly true that this paper would be greatly strengthened by some theoretical backing, it is also part of a line of work that eschews theory—so we cannot reasonably disqualify it for doing so.\n\nTo sum: although the technical contributions of this paper do seem to be significant, I expect that if the paper is published as presently written, it will confuse ICLR readers just as it has confused our reviewers. This leads me to lean against accepting this paper at this time."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a 2-stage sampling method that can find its use in either feature selection or instance (training examples) subsampling. At a high level, the authors frame the problem as a subset selection problem with the goal to minimize the performance difference for a downstream task between using the original set and the downsampled set. The proposed method consists of two stages: 1) select a subset (called candidate set) using Bernoulli variables as the binary mask, and the Bernoulli distribution's parameter depends on the corresponding example and the original set, 2) select elements from the candidate set iteratively to form the final selected subset, and the selection here is based on a softmax whose parameters depend on the interactions between the remaining candidate set and the selected subset so far. To support the merits of the proposed method, the authors provide experiment results on feature selection and dataset distillation tasks.",
            "main_review": "The main contribution of this paper is to propose a 2-stage sampling method. The core method of each stage does not have much novelty. The main weakness of this paper is that the core details of the proposed method are missing in the main text, which makes it very difficult for readers to assess its true value.\n\nDetails:\n1) p. 3, last paragraph of Sec. 3: It's not accurate that \"active learning does not consider the label information.\" Action learning selects examples to be labeled, but which examples to select is based on the error/loss of the existing model, which in turn depends on the label information.\n2) p. 3, Sec. 3.1: The authors state \"we minimize the loss function...\" Minimizing the loss function is to identify the subset $D_S$ or train the model for the downstream task? If it's the former, the previous description in Sec. 1 indicates that the authors would like to minimize the \"performance degradation\", which is the difference in losses from the original dataset $D$ and the subset $D_S$. If it's the latter, then why there're expectations here? Do you minimize over multiple draws or multiple selected subsets?\n3) p. 4, Sec. 3.3: How do you train the neural network $h$?  It's not clear at all.  Based on Eq. (2), we have $E(\\sum_{i=1}^{n} z_i) = 1$, which mean the effective sample size is 1? Also, you mentioned \"sparse Bernoulli prior $r(Z)$\". What does a sparse Bernoulli prior look like? Could you give an example?\n4) p. 4, Sec. 3.4: For the neural networks $\\varphi$ and $f$, how are they trained?\n5) p. 6, Sec. 3.5: \"Representativeness\" is defined as allowing a generative model to reconstruct the other examples in the dataset. How do you use a generative model to reconstruct the remaining examples? There's no such guarantee by simply sampling from a VAE.\n",
            "summary_of_the_review": "The authors try to pack lots of information into this paper, but the key details to support the usefulness of the proposed method are not fully explained. Without such details, it's hard for readers to appreciate its intellectual merits, and it's equally hard for readers to implement it in real applications. Therefore, this paper in its current form, doesn't seem to meet the bar of ICLR, and requires major improvements.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a data summarization technique that can be used for arbitrary downstream classification tasks. The authors show that their methods can be used to reduce the number of features required by a model while preserving accuracy. The authors additionally show that similar ideas can be used to extend this to selecting a subset of images in dataset that serves as a training dataset that provides good accuracy.\n\nThe algorithm proceeds by sampling features according to some softmax distribution defined by the classifier on each feature. The algorithm then learns to sample such that the accuracy of the classifier does not fall significantly when trained on a small subset of features. (The small subset of features are selected by adding a regularizer that rewards sparse selection of features)",
            "main_review": "Strengths:\n1. The authors validate their approach on many different classification tasks.\n\n1. The idea is simple and feasible.\n\n1. The method offers performance improvements, although I'm skeptical about the empirical significance of the experiments.\n\nWeaknesses:\n1. The experiments are only on celebA faces and MNIST digits, both unchallenging datasets.\n\n1. I do not think the numerical comparisons in Fig 4 are fair. The authors technically get access to the entire dataset and then choose what to show to the classifier, based on evaluations on different subsets. The performance gains over competing methods tell me that the baselines are most likely unfairly weaker.\n\n1. I have concerns about the technical novelty of this work. It essentially boils down to sampling from a softmax distribution over features, then putting a sparsity prior on the distribution to make sure few features and data points are preserved.",
            "summary_of_the_review": "Overall I am unconvinced by the technical novelty of the work and its implications. I do not necessarily buy the argument that existing feature selection techniques fail on RGB images. Additionally, I do not see the point of the experiments where a subset of the data is selected to train the classifier without sacrificing the accuracy -- each training technically requires the full dataset, even though the classifier gets to see only a subset, and training a new classifier would require the entire dataset to select a different sample.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents an interesting method for selecting subsamples or subfeatures used for deep learning. More precisely, the paper proposes to regularize the task loss by a Kullback-Leibler divergence between the selected samples/features and a sparse Bernoulli prior, and employs a neural network to learn the weights used for sample/feature selection.  ",
            "main_review": "The major weakness of the paper is that it lacks theoretical justification for the proposed method. For example, it is unclear if the learned decision function (e.g., regression function and classification boundary) is consistent or not. Uncertainty of the learned decision function is unclear, either. ",
            "summary_of_the_review": "This paper present an interesting method and promising results for subsample selection used in deep learning, while lacking theoretical justification for the proposed method.  ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes a Stochastic SubSampling method (SSS) that is helpful in reducing the size of input dataset while preserving reasonable performance on a target task. SSS is two-stage and set-based, which can be jointly optimized with arbitrary downstream tasks. Extensive experiments verify that SSS do perform well in the sense of efficiency and generalization a variety of tasks and datasets.",
            "main_review": "Strength:\n- The paper writing is good, and the presentation is very clear.\n- The idea of SSS is interesting, and its set-based nature allows it generalizes to various tasks.\n- Extensive experiments have been conducted to support the claim that SSS is advantageous over other baselines.\n\nWeaknesses:\n-  In Figure 4 (c): for the task of image reconstruction on CelebA, I observe that LTS performs even worse than random subsampling, which is counter intuitive and is inconsistent with other tests, e.g., Fig 4 (b). Discussions need to be added here.\n- In Section 3.5, I would suggest re-order the paragraphs so that they match the order of the illustrations in Figure 3 to increase the readability. Now it starts with Fig 3 (b), then (a), (c), (d). Why not (a), (b), (c) and (d)? Similar things could be improved in Section 4 when discussing each tasks and the experiments.\n\n-Typo:\n- In the third to last line of the paragraph after Eq. (2), there is a right parenthesis missed for \"Ber(\\rho(\\bar d_i)\"\n\n",
            "summary_of_the_review": "The paper writing is clear, and the proposed method is clearly advantageous over other baselines in terms of generalization and efficiency. Extensive experiments have been conducted to support the claim. I would vote for an acceptance of the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}