{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper aims to address the catastrophic overfitting issue in single-step adversarial training. Specifically, this paper finds that 1) using larger random noise initialization and 2) avoiding clipping adversarial perturbations are the two keys for stabilizing single-step adversarial training. \n\nOverall, the reviewers find this paper is well-written and the empirical results look promising. The reviewers originally misunderstood certain technical details of this paper, but got clarified in the discussion period.  However, the biggest concern shared by the reviewers is that the motivation of using larger random noise initialization and avoiding clipping adversarial perturbation is pretty unclear---they all fail to (either empirically or theoretically) understand how and why these two techniques are helpful to preventing catastrophic overfitting. Given the main contribution of this paper is a revisiting of existing techniques, it is a legitimate concern from the reviewer side for demanding the in-depth empirical analysis or the theoretical proof to help them better understand the proposed method; otherwise, the novelty contribution of this paper may get trivialized. \n\nI encourage the authors to delve deeper into the proposed method and make a stronger submission next time."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper aims to address a failure mode in the traditional single-step adversarial training known as catastrophic overfitting. They show that compared to the common practice of generating the adversarial perturbation, adopting larger random initialization and avoiding clipping the perturbation can effectively mitigate catastrophic overfitting. The effects of these two techniques are analyzed empirically in detail, followed by a comprehensive comparison to other methods.",
            "main_review": "The paper is overall well-written and easy to follow. The proposed methodology is quite simple and easy to implement. Detailed and comprehensive empirical evidence is provided to support the proposed method. However, I have several major comments regarding the presentation and intuition.\n\n\n* From my understanding, the proposed method can be rephrased as using a training perturbation radius significantly larger than the test perturbation radius (typically twice as mentioned by the authors), since the perturbation is randomly initialized in a larger radius, and no clipping is used. Could the authors confirm if my understanding is correct?\n\n\n* The paper found that a larger training perturbation radius can mitigate the catastrophic overfitting. This is somewhat counter-intuitive. I was expecting a figure clearly showing that as the training perturbation becomes larger, the catastrophic overfitting is mitigated (probably redesign Figure 2 right). I think more (empirical) analyses will be helpful to understand why this is true. There are some analyses in Section 4.1 on the loss landscape, but the intuition is still lacking.\n\n\n* I don't quite understand why a larger training perturbation radius will not lead to a clean accuracy drop. I think this observation is quite important to the paper's contribution but there is almost no justification. Providing at least some empirical analyses would be helpful.\n\n\nMinor comments:\n* The observation that larger perturbation does not necessarily hurt the performance is important in this paper. Section 4.2 follows [1] and show such observation is contradictory to their claim. It is better to merge this part of the analysis with Section 4.1 and provide a more integrated and intuitive understanding.\n\n\n* The notation epsilon is abused without clearly stating whether it is training perturbation or test perturbation. In Figure 2 I believe the epsilon here refers to only the test perturbation radius while in Figure 3 it could be both training and test?\n\n\n* Consider include some numerical results in the main paper. For example, move part of the performance tables in Appendix J to the experiment section, along with the training cost or speed up factor. This would justify the proposed method better if the main claim is efficiency.\n\n\n\n\n\n\n\n\n",
            "summary_of_the_review": "I believe this paper made some interesting observations towards understanding and mitigating catastrophic overfitting in adversarial training. However, I am not fully convinced as the support analyses cannot provide sufficient evidence of why the proposed method could work. I am willing to increase my score if the authors can better clarify the proposed method and provide more intuition.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper methodically studied the catastrophic overfitting in fast adversarial training (Fast-AT), and revisited the role of noise and clipping operation in Fast-AT. Based on the empirical findings, this paper discovered that the absence of clipping as well as using stronger noise could help avoid catastrophic overfitting. The author further proposed Noise-FGSM, utilizing single-step FGSM and noise-augmented samples to generate adversarial examples for training. Empirical studies showed the superiority of N-FGSM both in terms of performance and speed.\n",
            "main_review": "Strengths:\n+ Overall, this paper is well written. \n+ Empirical results look fairly good. \n\nWeaknesses:\n- The absence of clipping operation will violate the $\\ell_p$-ball constraints on perturbation and therefore lead to an unfair comparison with the baselines. The perturbation of adversarial examples is by convention, constrained in an \\ell_p norm ball, to indicate the strengths of adversarial attacks and the clipping operation is the key to maintaining the constraint. A larger $\\epsilon$ represents larger perturbation budgets and always leads to stronger attacks. Likewise, perturbation without clipping in adversarial training naturally equips models with better robustness. Therefore, although N-FGSM indeed brings better robustness, it may not be fair to directly compare N-FGSM with its baselines. The choice of very large $\\eta$ (e.g. 2$\\epsilon$) in Algorithm 1, further equipped the adversarial perturbation with a larger attack budget. The real perturbation budget of N-FGSM is likely to be larger than $\\epislon$ itself. It will be better for the authors to present the real $\\ell_p$ norm of perturbations generated by N-FGSM.\n\n- Based on the above, the role of noise in promoting robustness is also suspicious, because it is hard to tell whether the presence of noise or a larger budget contributes more to better robustness. \n\n\n",
            "summary_of_the_review": "In all, although the method proposed in this paper beats the baselines, it may not be a fair comparison and there are some fundamental errors in the experimental settings.\n\nPost-rebuttal adjustment:\n\nThanks for the additional clarification. I decide to increase my score by a point due to the authors' effortful response.\n\nI did not increase the score to 6 since I am not fully convinced on why noise augmentation + with adequate strength and without clipping is a principled solution to improve the effectiveness of fast adversarial training. I did not penalize its simplicity (this is fine to me), however, its effectiveness should be further justified either empirically or theoretically, e.g., whether or not the proposed design is applicable to fast adversarial training using TRADES-type loss, or other similar and in-depth studies.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Different from previous intuitions, this paper find that not clipping the perturbation around the clean sample and using a stronger noise is highly effective in avoiding CO for large perturbation radii. Based on these observations, the authors propose a method called Noise-FGSM (N-FGSM), which achieves the comparable results to GradAlign while achieving a 3x speed-up.",
            "main_review": "##########################################################################\n\nPros: \n \n1. The paper attempts to improve the efficiency for adversarial training. For me, the problem itself is real and important. \n \n2. This paper proposes a method called Noise-FGSM (N-FGSM), which attacks noise-augmented samples directly using a single-step. N-FGSM is simple but effective. \n \n3. Extensive experiments shows N-FGSM achieves the SOTA results while achieving a 3x speed-up.\n \n##########################################################################\n\nCons: \n \n1. Although the authors give theoretical analysis to understand the role of noise in\nsingle-step approaches, the theoretical analysis on that increasing the noise magnitude\nand not clipping prevents catastrophic overfitting is not provided. It will be better to explore theoretical justification behind this.\n \n2. Another concern lies in the accuracy on CIFAR 100 and SVHN datasets. As shown in Fig.3, the performance improvement is very slight.\n",
            "summary_of_the_review": "Overall, I vote for accepting. This paper find that not clipping the perturbation around the clean sample and using a stronger noise is highly effective in avoiding CO for large perturbation radii, which is interesting. Based on this, it proposes a simple but effective method called N-FGSM, which achieves the SOTA results while achieving a 3x speed-up. One major concern is about the theoretical analysis on the observations. Hopefully the author can address my concern in the rebuttal period.\n\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nUPDATE\n\nI have carafully read your response. My main concern has not been well addressed. The main drawback of this paper is the lack of theoretical analysis. The reason why it works is not clear enough. I believe that the paper may have potential but as its current form has some weakness. I turn to the rating of marginally above the acceptance threshold.\n\nBy the way, the performance improvement refers to accuracy rather than efficiency.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes an adversarial training method, noise-FGSM(N-FGSM), which improves the RS-FGSM by removing the clipping perturbation and increasing the magnitude of adding noise. The training speed of N-FGSM is the same as the RS-FGSM, while the experimental results indicate that N-FGSM outperforms RS-FGSM. ",
            "main_review": "Strength:\n1. The authors conducted many experiments to verify the effectiveness of their proposed N-FGSM. They presented their results through tables and figures.\n2. The experimental results indicate that  the proposed N-FGSM can achieve better performance than RS-FGSM and GradAlign in terms of adversarial accuracy(Robustness).\n\nWeakness:\n1. The contribution of this paper is limited. This paper provides little insight as a research paper as the two improvements of N-FGSM are empirical and heuristic to some extent. \n2. The motivation of this paper is unclear. The authors raised the CO problem but turn to the two \"key components\". The two so-called \"key components\" of adversarial training, clipping and random steps, are unconvincing. It is more like achieving the improvements first and then finding the motivations to fit them in. \n3. The evaluation metric is unfair. By eq(4), the adversarial perturbation will be definitely larger than the baselines. Theorem 1 is obvious and hard to be the contribution of this paper. The magnitude of the $\\epsilon$ for the baselines should be triple. \n4. To my understanding, increasing the magnitude of the adversarial perturbation may lead to a worse clean accuracy. Figure 5 verifies the clean accuracy drop. However, the table in Figure 4 seems to illustrate that the clean accuracy of N-FGSM is maintained compared with the baselines. And the table is very hard to understand. \n5. What is the $\\epsilon$ axis in Figure 3 and 4? Does it represent the magnitude of $\\epsilon$ for training? If yes, why the robustness decreases as the $\\epsilon$ increases? If the $\\epsilon$ is the magnitude of the evaluation, then PGD-50-10 usually represents a PGD attack with 50 steps and 10/255 l-infi norm.  It isn't very clear here. ",
            "summary_of_the_review": "To summary, this paper is more like an experiments report instead of a research paper. The contribution is limited, and the evaluation metric seems to be unfair. I suggest a clear rejection for this paper. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}