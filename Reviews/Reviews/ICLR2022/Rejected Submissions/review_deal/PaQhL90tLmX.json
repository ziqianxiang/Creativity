{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper introduces a method called DeepTLF that handles heterogeneous tabular data by using GBDT as an encoder for a DNN.\n\nThe paper is clearly written and the method works as intended.\n\nThere is however the issue of novelty (raised by Q6we). The method indeed relies of the capacity of GBDT to represent the data, the internal node values are used as features to train a downstream neural network. This process is straightforward, which is good from an application perspective, though the paper offers limited insights to the community from a scientific perspective.\n\nAnother reviewer concern was that of incompleteness of experiments and lack of certain details (reviewers vaip and gWeP). This was answered in the rebuttal, which the reviewers acknowledged, however, the authors did not provide a revised version of the manuscript, when ICLR in fact allowed (and actually encouraged) revised versions to be submitted by Nov 22. Without a revised version, it is difficult for the reviewers to assess whether the text in the final manuscript will actually accurately reflect the changes they suggested. This justifiably caused two of the reviewers to keep their original scores (they explicitly stated the lack of an updated manuscript as the reason).\n\nGiven lack of an update, coupled with the issue of novelty, I conclude the paper is not ready to be accepted in its current form."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes DeepTLF, a new framework for prediction tasks using tabular data. DeepTLF first trains gradient boosted decision trees (GBDT) using the entire tabular training samples. Then it uses node values as the input the neural network predictor (e.g. classifier or regressor) for the actual prediction phase. In essence, the authors rely on GBDT's capability to handle heterogeneous tabular data with potentially many missing values to derive a binary representation of the given sample, and feed that representation to a downstream neural network. DeepTLF was able to demonstrate superior performance in real-world tabular datasets.",
            "main_review": "Strengths:\n- The paper proposes a very simple idea (i.e. Use a two-stage feature derivation process for tabular data), which outperforms many contemporary large-scale neural network models for tabular data for several real-world datasets.\n- Comparison with contemporary tabular neural nets are appreciated.\n\nWeaknesses:\n- The technical novelty is quite limited, since all the authors do is train GBDT first on the entire training set, extracts the internal node values as a feature representation, use those feature representations as new training samples to train a downstream neural network. As discussed in the paper, if the whole process was done end-to-end, it would have been more interesting.\n- Experiment details are missing/incomplete. D1, D2, D3 are used for binary classification, and the authors do not discuss the ratio between positive and negative samples (class imbalance). Also, for binary classification, researchers typically use AUROC or AUPRC due to class imbalance, but the authors use cross entropy loss instead.\n- The proposed method barely outperforms GBDT in all datasets except for D1. The authors claim that they used a small DNN model for DeepTLF to demonstrate the power of their approach, but since that power is not observed in Table 2, it would have been more convincing if the authors had used several variations of DeepTLF (e.g. deeper DNN, Transformer) and show the power of DeepTLF.\n- In Table 3, it is not clear whether the training time measure a single minibatch update, or a single epoch, or the entire training process.\n- Is there any reason D7 was not included in Table 2?\n- The authors state that D7 consists of textual and tabular data. Does this mean that the cell values of the table can either be a continuous value, categorical value, discrete value, and word tokens? It is not clear how GDBT would be able to process such table. If not, then what does D7 look like as a multimodal tabular data?",
            "summary_of_the_review": "The proposed method, DeepTLF, is a very straightforward combination of GDBT and DNN, which seems to outperform all modern tabular neural nets, which I find very interesting. However, the technical novelty is quite limited, and considerable amount of experiment details are missing, therefore making it difficult to accurately evaluate the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "For the problem of learning (supervised classification and regression) on tabular data, the authors propose to used the decision functions of tree based ensemble methods as input features for a deep neural network (DNN). Tabular data is typically heterogenous, that is the data come from different modalities (continuous, categorical, missing values). As decision trees are very well tailored to such data the authors show that with their method (DeepTLF) no further preprocessing or data cleansing is necessary (apart from what's being performed in the decision tree). On numerous experiments on 7 public tabular datasets, they show that their method outperforms different baseline and competitive approaches, thereby supporting the claim that the tree based input features preserve sufficiently rich information of the input data. ",
            "main_review": "The idea is nice and quite elegant, leveraging the flexibility of tree based methods for use in deep learning. To my knowledge, this is a novel idea, although it may not be super obscure. At least I did not came across any literature where this approach had been followed.\n\nThe paper is well written and easy to follow. If we deduct the recap of gradient boosting and the formal definition of a decision tree, the actual description of the approach is just about one page. Large part of the paper and the appendix is dedicated to extensive experiments, comparing the approach to simple baselines and dedicated tabular data methods.\n\nThe major claims made in the paper are supported\n- *DeepTLF can preserve most of the information that is contained in the original data.* I think this is shown with the experiments (Table 2) in so far as DeepTLF yield state of the art performance on all datasets  \n - *it dramatically speeds up the data preprocessing time (Fig 1, caption)* Well, yes, once the tree is trained no further explicit preprocessing (data scaling and handling missing values) is required as this is moved into the decision functions. \n\nI have three bigger concerns:\n1. The comparison with DNN in Table 2 may not be fair because DNN has 3 fully connected layers while DeepTLF has 4 fully connected layers. Depending on the number of decision functions, the first layer in DeepTLF may also have more parameters than DNN. The architecture of DNN is not further described. Are there embedding layers for categorical columns  or are they treated numerically? How are missing values treated? That can make a big difference to the performance of DNN.\n2. Table 3 should include all(!) competitors, in particular DNN. For the training time of DeepTLF it should be made explicit how much time is spend on training the tree ensemble (is it the same as GBDT, second line) and how much for training the network. For data pre-processing time it should be made clear what time is required to prepare the dataset for training (e.g. computing averages and variances or inferring candidates for missing values) and how much preprocessing time is required to do inference on a new and unseen sample (e.g. scale fields appropriately, fill in missing values). Both should be set into relation to training time and inference time respectively.\n3. I'm wondering what I could do if there are no classification or regression targets to train a tree ensemble in the first place (e.g. for  unsupervised or self-supervised learning approaches) Have the authors done any experiments in this regard? At least this questions should be addressed in the discussion. \n\n\n#### Minor issues\n- Fig 2, caption: $\\mathbf x_i \\in \\mathbb R\\times\\mathbb R\\times\\\\{red,-\\\\}\\times\\mathbb R\\$\nsimilar in Definition 1\n- p5,l15: *\"It is only important that the same strategy is used across all decision trees and vectors.”* why is that? Isn’t it just important that the order of $\\tilde\\mu_v$ is the same for all $\\mathbf x$?\n- p5,l28: $\\mathbf x=(x_1,…,x_d)^\\top$, Euklidean vectors are column vectors by convention, similar in (8)\n- how to deal with duplicate $\\tilde\\mu_v$ ?\n\n\n\n",
            "summary_of_the_review": "Interesting and to my knowledge novel idea to use decision functions of tree ensembles as input features to neural networks. The experiments show that this method works, but are not 100% conclusive if it is superior in all aspects, in particular comparing to DNN (see my concerns above).\nI rate the paper *\"6: marginally above the acceptance threshold\"* and am willing to increase my rating if my concerns are fully addressed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes an encoding method (to encode structural information in GBDT) for DNN with tabular data, to handle the heterogeneous features. The method is simple, and the results look very promising. \n",
            "main_review": "strength:\n1. the paper is clearly written, and easy to follow.\n2. the proposed idea is simple and easy to understand.\n3. the experiment part results look promising.\n\nweakness & question:\n1. some details in experiments are not clear. For example, the used hyper-parameters, especially for GBDT. And is the results run by one hyper-parameters, and by hyper-parameter search tools? To get a fair comparison, a throughout hyper-parameter search is needed.\n2. Can you also provide the AUC metric for the binary classification tasks?\n3. Are there any multiclass tasks?\n4. Do you think it is possible to get rid of GBDT (as a feature extractor), and independently design a feature extractor, and perform a similar performance?\n\nI will boost my score if the experimental details are provided and reasonable.",
            "summary_of_the_review": "This paper is clearly written, and the proposed method is simple, while seems to work. But some details of the experiment, especially the hyper-parameters, are missed. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}