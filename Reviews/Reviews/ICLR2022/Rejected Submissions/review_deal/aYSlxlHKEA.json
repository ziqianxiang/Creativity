{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "As pointed out by reviewers, the presentation needs to be improved to clarify the algorithmic and theoretical contributions."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a model-based decentralized RL algorithm. Theoretical analysis is presented, and the theoretical results are validated with empirical studies on several vehicle cooperative cruise control tasks.",
            "main_review": "Strengths: the theoretical analysis is solid.(the reviewer did not check the correctness).\n\nWeaknesses:\n1.\tThis paper incorporates several ideas to develop a decentralized RL algorithm. Compared with previous works on centralized, model-free, and tabular settings, this work investigates decentralized, model based, and deep learning model settings. However, from each single viewpoint, from centralized to decentralized, for instance, the problem has been addressed in prior literature. This makes the present study seem to be an ensemble of well-developed techniques. In this context, the authors are expected to justify their contributions by showing that the theoretical analysis is not a trivial integration of existing analysis schemes, if it is not. In other words, please clearly state the challenges and novelty of the paper.\n2.\tThe improvement of sampling efficiency comes at the cost of training predictive models. Do the sampling savings compensate for the training cost? Is it possible to show that the learning process with the proposed model-based approach is more efficient in terms of time?\n3.\tAs pointed out in the empirical study section, the predictive model might fail to learning the reward for CACC slow. What is the reason? Does this imply the proposed model-based scheme is not stable in some circumstances. The robustness of the method is also not verified. Maybe including more complicated RL tasks in the experiments will make the work more convincing and solid.\n4.\tTypos:\nabstract: exponential -> exponentially, \nParagraph 1 on page 3, Each agent possess a localized …, possesses\nParagraph 1 on page 3, … reward functions is …, are\nLast paragraph on page 4, Independent RL algorithms that observes only… , observe \n\n",
            "summary_of_the_review": "The novelty and the contributions in theory are not obviously presented. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors proposed a decentralized multi-agent reinforcement learning algorithm, provide theoretical bounds on its performance, and conducted numerical experiments on its performance. ",
            "main_review": "It is certainly necessary to have decentralized multi-agent reinforcement algorithm to solve problems such as autonomous driving, wireless communications and multi-player games. There are a few places I believe that the authors can significantly improve their paper. \n\n1) The insight and essence of the DMPO algorithm are not adequately addressed. For the three key components: localized model, policy with one-step communication, and extended value function, other than the extended value function the purpose of the other two are not really explained well. As a consequence, Sec. 4.1 and 4.2 really very difficult to understand. Also, a complexity analysis of the algorithm is very desirable.\n2) As pointed out by the authors, the effective of theoretical bounds depends on the selection of the discount factor. This limits the selection of the rollout T, since when T become large, then the advantage of the expressions in Theorem 2 will go down significantly; This also limits the the effective of Theorem 3.\n#) In the numerical experiments,DMPO does not stand out as a significant improvement, maybe more experiments can be conduct to demonstrate the effectiveness of the algorithm. ",
            "summary_of_the_review": "Overall, both the theoretical results and experiments can be strengthened to make a better case for the proposed algorithm. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a decentralized model-based reinforcement learning algorithm for networked multi-agent systems where cooperative agents communicate locally with their neighbors. ",
            "main_review": "Q1. The current study assumes that the target system is an independent networked system or \\ksi-dependent system. First of all, the used assumptions sound to be too strong to model realistic multi-agent systems. For example, the assumption that the reward function is only dependent on local state s_i and the local action a_i makes the entire system to be interacting through the joint state transition rather than interaction through the reward. Furthermore, the transaction of an individual state only depends on the action of the corresponding agent. Would you please discuss the problems or characteristics of the target system where the proposed assumptions hold? \n\nQ2. The proposed assumptions are held in the target problems (environments) used in this study? What are the common aspects of these environments, and why are these environments particularly selected? In other words, the used environments are representative environments where the proposed method can work nicely? As the authors mentioned, there is no single MARL algorithm that can solve all different problems. Therefore, it would be nice if the authors describe the characteristics or examples of the target problem where the proposed assumption and the proposed method work nicely.\n\nQ3. The idea of model-based MARL is loosely explained. Section 4.1 discusses constructing a local state transition model using GCN, and section 4.2 discusses the PPO algorithm with a truncated value function. First of all, the paper does not explain how to use the learned local dynamic model is used in training the value function and policy. Although the procedure is loosely explained in Algorithm 1, the notations used in Algorithm 1 are not fully explained. Furthermore, if the trained model is used to generate sample trajectory for model-free MARL, the proposed method's algorithmic novelty in developing model-based MARL is very limited. There should be more diversified ways to use the learned dynamic models to learn decentralized policy better. The author should investigate these approaches.\nQ4. The dynamic models in MARL are generally composed of (1) environmental transition given the current joint state and the joint action and (2) the behavior models predicting other agents’ action given the current state. The current study only focused on a limited version of (1) while ignoring (2). \n\n\nQ5. What are the relationships between sections 5.2 and 5.3? Each section seems to discuss independent analysis. For example, 5.2 discusses return bound when performing rollout, while section 5.3 discusses error bound for the truncated value function and policy gradient. Are these results are an extension of the previous study? If yes, what are the new aspects? In addition, how can these analyses be used to prove the effectiveness of the proposed model-based MARL? Is there any chance that these analysis results can be integrated and used to show that the proposed independent networked system assumption can effectively solve the target problem? Please elaborate on the significance and meaning of each theorem in terms of how the proposed method solves the target problem. \n\n\nQ6. The assumption used for modeling the independent dynamic model is not used for estimating the truncated value function. \n\n\nQ7. Experiment results are not sufficient to validate the effectiveness of the proposed method. Particularly, the proposed method does not excel other baselines when the target problem is complicated. The model-based approach should be beneficial, especially for complex problems. In addition, what is the intention of comparing the approximation errors for different environment models?\n",
            "summary_of_the_review": "Although the current study discusses a vital topic in MARL, the current study focuses very limited class of MARL with quite strong assumptions. The theoretical analysis can be possibly considered novelties for the current study; however, the importance and the meanings of the theoretical analysis are not fully explained in the present paper. Finally, the experiment results are too limited to validate the effectiveness of the proposed method.\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}