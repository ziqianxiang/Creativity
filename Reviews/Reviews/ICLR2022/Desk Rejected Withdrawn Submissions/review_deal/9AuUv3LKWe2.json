{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposed an approach to answering visual questions via multi-granularity alignments on the visual and textual inputs. In particular, the alignments over concept-entity level, region-noun phrase level, and spatial-sentence level are considered. The approach achieves good results on the two largest VQA dataset",
            "main_review": "Strengths: \n1. I liked the idea of aligning multi-granularity features between visual and textual inputs and believed that the current transformer-based systems, which mostly take as inputs the detection features, could benefit from this.\n2. The paper covers multiple granularities, i.e. textual: entity, noun phrase, and sentence (words) level, visual: concept, region, and pixel level.\n\nweaknesses:\n1. the major concern is that the alignments only happened within the three pre-defined levels, i.e. concept-entity level, region-noun phrase level, and spatial-sentence level. I can see the reason for aligning region-noun phrase level, however, the alignment between spatial (pixel)-sentence (word) level needs more explanations.\n2. some notations are never introduced: for example, how do you get the logits (f_{ce}, f_{rn}, f_{ss}), I assumed it is computed by linearly projected (W_{ce}, W_{rn}, W_{ss})? Also, since you have 4 output logits (f_{ce}, f_{rn}, f_{ss}, f_{GA}), how do you compute the final answer? Do you average them, take f_{GA}, or take the max of them? either way seems reasonable, it will be good to compare these three ways of predicting the answers.\n3. experiments: In table 2, could you please explain the reason why Single Modality (No. 22) outperformed Low-Level GA (No. 19), can you describe the  Single Modality system you used? ",
            "summary_of_the_review": "The idea is interesting, however, there are some parts in the paper, including system and experiment design, that need clarification and justification.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces Multi-Granularity Alignment Transformer for VQA (MGA-VQA). MGA-VQA extracts three levels of features from the image and the question: concept-entity level, region-noun phrase level, and spatial-sentence level. These three levels of features are then integrated with the decision fusion module for the final answer. A co-attention mechanism is further proposed to jointly perform question-guided visual attention and image-guided question attention.",
            "main_review": "**Strengths**\n\nThe proposed model is novel to some degree. The MGA-VQA model extracts three levels of features of both the image and the question. And these features are well fused through the proposed granularity alignment transformers (GA-TRMs). The experimental results show MGA-VQA can achieve improvements over some existing VQA methods including pretraining approaches.\n\n**Weaknesses**\n\n**1.  Missing important related works**\n\nThe related works are not well-reviewed. To be specific, only two papers of 2020 are discussed and recent papers of 2021 are missing. For example, the following papers are very close to the topic this paper addresses:\n- Dynamic Fusion With Intra-and Inter-Modality Attention Flow for Visual Question Answering, CVPR 2019.\n- Deep Modular Co-Attention Networks for Visual Question Answering, CVPR 2019.\n- Uniter: Universal image-text representation learning, ECCV 2020.\n- Visualbert: A simple and performant baseline for vision and language.\n- ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision.\n\n**2. Compared baselines are not advanced enough.**\n\nVQA is a very popular research topic and plenty of approaches are benchmarked on the VQA-v2 dataset and the GQA dataset. However, many recent advanced methods are missed. For example, only one method after 2019 is compared in Table 3.\n\n**3. Missing some details of model implementation.**\n\nWhat are the models used to extract entity-level, noun phrase level, and sentence level features in section 3.1.2?\n\n**4. This paper is not well organized.**\n\nThe layout of this paper is a bit rushed. For example,\n\n- The font size of some annotations of Figure1 and Figure 2 is relatively small. And these two figures are not drawn explicitly enough.\n- Table 2 is inserted wrongly inside of a paragraph.\n- Top two lines on page 6 are in the wrong format.",
            "summary_of_the_review": "The proposed model is novel to some degree but the organization of the paper, related work discussion, and experiment part need to be improved.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper tackles the visual question answering task in image. The paper takes the multi-modal alignment idea with co-attention mechanism and extends it into three granularities: concept-entity level, region-noun phrase level, and spatial-sentence level. Experiments are validated on GQA and VQAv2 datasets.\n",
            "main_review": "The paper is well organized with one general idea extended to three granularities.\n\nTwo questions related to the model design:\n1. Regarding the fusion of the three levels, Equation (7) contains both the early fusion and late fusion. What are the separate performances for early fusion and late fusion?\n\n2. For the graph merging module in Equation (3) (4) (5), this paper directly merges existing matrixes into a big matrix by concatenation to learn the alignment. However, in the concept-entity level, some nodes in concept level and some nodes in entity level have direct correspondence. In this case, will the big merged matrix by concatenation have redundancy?\n\n     Some papers which involve the merge of two graphs (e.g. combine scene graph with external knowledge graph [1]), they merge the same nodes in two graphs into one node. Have the authors considered this situation?\n",
            "summary_of_the_review": "The paper explores co-attention alignment in three granularities and fuses the multi-granularity prediction for visual question answering. The idea is straight forward and the paper is well written. Some model designs need to be further ablated.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper focuses on Visual Question Answering (VQA). Specifically, the authors argue existing VQA models always align multimodal features in single-level, and they propose to align visual and language features in multi-levels, including concept-entity level, region-noun phrase-level, and spatial-sentence level. To this end, they first extract three-level visual and language features, and then propose a co-attention mechanism to fuse multimodal features in each level. Finally, three-level features are fused to make the final answer prediction. Experiments on two datasets (VQA v2) and GQA have demonstrated the effectiveness of the proposed method.",
            "main_review": "Strengths:\n1. The motivation of fusing multi-modal features in a multi-scale fusing manner is convincing and interesting.\n2. Results on two widely-used VQA benchmarks have demonstrated the effectiveness of the proposed model.\n\nWeaknesses:\n1. The selections of three levels of visual features are confused. Although I agree that extracting multi-level visual features are critical for VQA, I can not understand the selection of existing three levels of visual features: concept-level, region-level, and spatial-level. ie, Why the region-level is a higher-level (refer to the language part) of concept-level? Likewise, the relation between the region-level and spatial-level is also not hierarchical.\n\n2. The novelty of the proposed idea is not novel enough. Multi-level feature fusions have been widely-used in many visual-language tasks, eg, image-text matching. Although this is the first VQA work (claimed by the authors), the idea itself is not exciting enough.\n\n3. Some notations and writing are not clear enough. For example, in section 3.2.2, the meanings of the lead graph in each layer (Eq. (3) - (5)) are confused. Meanwhile, the terminology could be more precise or consistent, eg, the lead graphs and merged graphs.\n\n\nMinor: \n1. For extracting the region-level relations and concept-level features, the proposed VQA model also needs to rely on an extra Scene Graph Generation model (and SGG training dataset), is it fair to compare with other existing VQA baselines?\n",
            "summary_of_the_review": "The basic idea of this paper is convincing and interesting. However, as mentioned in the weaknesses part, my main concerns are several parts:\n1) The motivation of the used three-level (especially the visual part) are confused;\n2) The idea of fusing multi-scale multi-model features are not exciting enough;\n3) The writing needs to be further improved.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}