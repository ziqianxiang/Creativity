{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "Reviewers overall found that the paper contains novel and intriguing ideas worth further investigation. There is, however, a consensus that the paper is not ready to be published yet, for several reasons detailed in the reviews pertaining to 1) the fact that several statements should be better supported theoretically or empirically, 2) the technical derivation of the method where several choices made by the authors are surprising and not justified, and 3) the experimental results that do not clearly support the claims of the manuscript. While the authors have improved the manuscript during the discussion phase, there is still too much work to be done in order to address issues remaining. We hope the reviews will be helpful for authors to consider a revision of the paper for a future submission."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents a new solution to dimensionality reduction and data visualization. They define an explicit extension of the pairwise distances in the lower dimensional space, the equivalent extended distance (EED), which is proportional to the reduction in volume when going from a high dimensional space to a lower-dimensional space, This helps to avoid the crowding problem known from T-SNE, UMAP, etc. and could provide a better understanding of dimensionality reduction methods in general. In addition, they define a distortion framework that allows more efficient implementation of the EED. Finally, they introduce a hierarchical scheme for manifold approximation, where instead of splitting the problem into ambient space and data manifold, they add a third layer, the near field. This is similar to UMAP, but they use a Gaussian function instead of a uniform for the near field. Experiments are thorough and the visualizations behave as expected, with the data being more spread out compared to t-sne and UMAP. A quantitative analysis using recently proposed metrics is also provided.",
            "main_review": "This is a reasonably well-written paper, and the idea of modeling dimensionality reduction by directly compensating for the loss in volume is interesting and could provide a deeper understanding of dimensionality reduction methods that are based on solving optimization problems. However, I believe that in its present form, the paper seems unfinished and is not ready for publication. \nA more clear explanation of the intuition behind the novel idea is needed. The authors spend a  great deal of effort to illustrate the counterintuitive effects of high dimensional geometry and state of the art results, but they fail to give a clear picture of how the new approach fits in, other than that they “expand the capacity” of the low dimensional space. An explicit example of the space expansion from 2D and 3D, instead of Figure 1, would definitely help the reader.  \n\nFurthermore, what is the real benefit of using this method compared to t-sne or UMAP? Some figures in the experiment sections gives us a hint that seems promising, such that it behaves better for continuous and disjoint manifolds, and that the geometry is preserved better (swiss roll example), but this is barely mentioned in the text and leaving the interpretation entirely to the reader is not sufficient I believe. The promise of a generic framework for understanding nonlinear dimensionality reduction is not delivered in the paper as it is presented now. I would like to see a concrete example where this is shown. The measures used are not sufficient to give a clear picture of this. A combination of the figures and tables gives us some idea, but, again, they are barely mentioned in the text. E.g. For very large datasets, the increased scale and distance between the data points with this method is expected, but how does this concretely relate to the figures and how can we interpret this together with the metrics in Table 1. How do extended distances behave when the dimension is extremely high?\n\nTo conclude, I think that the paper needs a clearer presentation of the main idea as well as a better connection between the results of the experiments and the claims in the introduction. \n\nGeneral comments: \n\nIsn’t the discrepancy between very high and low dimensionality already fully explored?\n\nThe loss in section 3.1 is not explained and should be properly introduced or a reference to the section where it is should be included.\n\nThe paper is well written, but in some parts I find the language a bit oral and verging on being too vague, especially the abstract and introduction.\n\nThe Haussdorff measure is perhaps not well known and could be described further. Especially since this is in a ML/CS setting.\n\nTypo in section 3.2.2: based ON the factors\n\nThe FD is not explicitly defined as far as I can see, just indirectly via the EED, which is slightly confusing for the reader.\ntypo: 3.3 avoid the comput?\n\nI do not believe that the MLE method for identifying the intrinsic dimension is necessarily widely accepted. Many others exists and it remains an open problem, so I would avoid leaning too much on this method.\n\nThe conclusion contains repetition that should be avoided (first and last paragraph are essentially the same).\n\n",
            "summary_of_the_review": "The paper presents a new solution to dimensionality reduction and data visualization. They define an explicit extension of the pairwise distances in the lower dimensional space, the equivalent extended distance (EED), which is proportional to the reduction in volume when going from a high dimensional space to a lower-dimensional space. To efficiently implement the EED, they introduce a distortion-based framework. Finally, they introduce a hierarchical scheme for manifold approximation, where instead of splitting the problem into ambient space and data manifold, they add a third layer, the near field.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "This is a technical paper that addresses the problems of reducing the dimensionality of arbitrary data sets. Therefore, I do not see any ethical concerns with this paper. ",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work studies dimensionality reduction and visualization of high-dimensional data. The idea is to construct two graphs in the original high-dimensional space and the low-dimensional space, respectively, based on space expansion and hierarchical manifold approximation. Space expansion is to map the distance in high-dimensional space to an equivalent extended distance (EED) in low-dimensional space such that the volumes of the two (hyper-)spheres in the two spaces are equal. Hierarchical manifold approximation divides the points into near field, middle field, and far field, and assigns EED based similarity for points in the near field. Then a problem minimizing the sum of a KL divergence and a repulsive force term is solved by SGD to determine the projected coordinates. In numerical experiments, the effectiveness of the proposed method is demonstrated by comparing with some baseline methods on synthetic and real-world datasets.",
            "main_review": "Strengths:\n1.\tThe idea of defining EED to preserve the volume after dimension reduction seems novel and well-motivated.\n2.\tThere are not too many tuning parameters in the algorithm, and in some of the experiments the proposed algorithm SpaceMAP is less sensitive to the choice of the number of nearest neighbors $k$ than the comparing methods.\n3.\tIn experiments SpaceMAP is shown to be able to handle both continuous manifold and disjoint manifolds.\n\nWeaknesses: \n1.\tOne of the key points in SpaceMAP is to define the similarity based on EEDs, as given in (8) and (10). However, it is not explained why the EED used in (8) is inversed. If $\\alpha_t$ and $\\beta_t$ are factors from the ambient space to the intrinsic space, then the EED in (8) should be $\\alpha_tR_{ij}^{\\beta_t}$, as given in the paragraph after Example 3.1.\n2.\tI find the definition of d_global and d_local not clear. In the second to last paragraph in section 1, d_global is the dimension of the manifold, and d_local is the dimension of a local neighborhood. However, for a manifold its dimension is defined as the dimension of an open subset that a neighborhood around each point on the manifold is homeomorphic to, so under such common definition d_global and d_local seem the same.\n3.\tThe MLE of intrinsic dimension in Theorem 3.1 and Lemma 3.2 is not defined or formally introduced. Also, the assumption in Theorem 3.1 is not clear enough. Does $R \\leq R_{ij}$ means that $R$ is required to be smaller than $R_{ij}$ for all $j=1, …, k$?\n4.\tSection 3.2.2 seems very unclear, because it tries to explain distortion of similarity functions without defining the similarity functions. In Figure 2 (a), $f$, $\\tilde{f}\\_{d\\to D^{\\prime}}$, and $\\tilde{F}\\_{D \\to d}$ are never defined, and $P_{i|j}$, $Q_{ij}$, and the near field are only defined in the next section. \n5.\tThe sentence after equation (7) is not finished.\n6.\tWhat does the subscript $t$ in equation (8) denote? The so-called FD factors $\\sigma_{t,i}$ in (8) and $\\sigma$ in (10) are not defined.\n",
            "summary_of_the_review": "The proposed algorithm has certain novelty and performs well in the experiments. However, several issues need to be addressed to convince the readers of the technical soundness, as pointed out in the main review, so my current recommendation is that this paper is marginally below the acceptance threshold.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes Space-based Manifold Approximation and Projection (SpaceMAP) method to solve two existing issues in dimensionality reduction task: (1) capacity matching between high-dimensional data space and low-dimensional visualization space. (2) complexity in various manifold properties. To address these issues, SpaceMAP proposes two ideas: (1) space expansion (Section 3.2): inferring equivalent extended distance (EED) (Definition 3.2) for low-dimensional space based on volume matching and introducing function distortion (FD) (Section 3.2.2) to transform the similarity functions in high-dimensional and low-dimensional space; (2) hierarchical manifold approximation (Section 3.3): making distinctions between the near field, middle field, and far field of data distribution in a data-specific hierarchical manner. SpaceMAP achieves competitive performance on a diversity of datasets.",
            "main_review": "Strengths:\nThis paper provides an analysis of discrepancy in space volumes in high-dimensional data space and low-dimensional visualization space and the complexity of high-dimensional manifold in the visualization task, and proposes space expansion (Section 3.2) and hierarchical manifold approximation (Section 3.3) solutions. These contributions help further development of nonlinear dimensionality reduction and visualization techniques based on manifold learning.\n\nWeakness:\n1. A pattern in a high dimensional data space may lie on a complex manifold with a highly nonlinear structure. The assumption that manifolds form N-dimensional hyperspheres (Definition 3.1) is un-supported. \n2. By definition, a manifold is a lower dimensional surface embedded in the high dimensional data space and the volume increases exponentially as the dimensionality increases. However, no reasons are given as to why to impose the volume-based EED (Definition 3.2) as the a distance measure to justify the correctness of the proposed capacity matching.\n3. The experimental design does not validate the effectiveness of the proposed equivalent extended distance (Section 3.2.1) and hierarchical manifold approximation techniques (Section 3.3). Additional ablation study experiments are needed to demonstrate the effectiveness of these techniques.\n4. The experimental comparisons are unfair (Table 1). The experimental results do not match the statement “UMAP and TriMAP are reported to better preserve the global structure than T-SNE” mentioned in related work section. The global metric results (Table 1) of UMAP are inferior to those of t-SNE by far. The hyper-parameters of the comparison methods need to be adjusted for different databases, e.g., “perplexity” in t-SNE, “n_neighbors” and “min_dist” in UMAP.\n5. There are some typos, e.g., t-distributed Stochastic Neighbor Embedding (tSNE) in Abstract, Local Metrics (M_s and M_σ) in Table 1.\n",
            "summary_of_the_review": "The foundation of the main idea (capacity matching) is flawed. The experimental method lacks appropriateness, and the results are inadequate to prove the claimed effectiveness of the proposed method.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper suggests what it calls a novel dimensionality reduction and data visualization method (SpaceMAP), but perhaps it would be more precise to say that they suggest two modifications to UMAP. One modification is theoretically motivated low-dimensional similarity kernel (that depends on the estimate of the input dimensionality). Another modification is multi-scale high-dimensional similarity kernel. The authors argue that their method performs better than existing methods on a variety of different datasets.\n\nThe first part about theoretically grounded low-dimensional similarity kernel (which seems to be the main focus of the paper) I found interesting. However, I feel that combining two very different UMAP modifications together made the paper weaker because it is not clear what effect do they have on their own. I'd much rather see more experiments that really elucidate the \"space expansion\" idea and effect. Also, the paper is missing a lot of relevant context/citations. Overall I feel that the work has potential but the paper is not really ready yet. I am giving a lower-borderline score.",
            "main_review": "This paper suggests what it calls a novel dimensionality reduction and data visualization method (SpaceMAP), but perhaps it would be more precise to say that they suggest two modifications to UMAP. One modification is theoretically motivated low-dimensional similarity kernel (that depends on the estimate of the input dimensionality). Another modification is multi-scale high-dimensional similarity kernel. The authors argue that their method performs better than existing methods on a variety of different datasets.\n\nThe first part about theoretically grounded low-dimensional similarity kernel (which seems to be the main focus of the paper) I found interesting. However, I feel that combining two very different UMAP modifications together made the paper weaker because it is not clear what effect do they have on their own. I'd much rather see more experiments that really elucidate the \"space expansion\" idea and effect. Also, the paper is missing a lot of relevant context/citations. Overall I feel that the work has potential but the paper is not really ready yet. I am giving a lower-borderline score.\n\n\nMAJOR ISSUES\n\n* Multiple details of the algorithm are not sufficiently clear: \n\na) Is it correct that SpaceMAP is UMAP with exactly two modifications: high-dim similarities are computed in a multi-scale way (Eq 8), and low-dim similarity kernel is replaced with Eq 10 that depends on the estimate of the input dimensionality? Or are there any other more subtle changes as well? This should be stated crystal clear.\n\nb) Appendix A1 -- I did not understand this procedure, it was not clear to me what exactly is being predicted by the SVM. Also, I'd rather see this in the main text on page 5. \n\nc) Page 5: \"n_middle is set to avoid the comput\" -- the sentence is cut off and the details are missing.\n\nd) Algorithm 1: gives an equation for n_middle, but it is not motivated and unclear.\n\ne) Page 6: \"where alpha_t, beta_t\" -- what are subscripts \"t\"?\n\nf) Same line: what is \"sigma_t,i\"? How is it an \"FD factor\" when section 3.2.2 does not mention sigma? \n\ng) In Eq (10) you use alpha and beta without subscript i. How are these obtained? Also, again, what is sigma?\n\n* It's unclear what effect do the two modifications have on their own. I would like to see some analysis of them separately. \n\n* Does using multiscale P_ij similarities make any difference compared to standard UMAP (with suitably chosen n_neighbors)? These two papers argue that exact P_ij values in UMAP do not matter at all: https://arxiv.org/abs/2007.08902 and https://arxiv.org/abs/2103.14608.\n\n* The similarity kernel in Eq (10) should be compared to the standard similarity kernels in UMAP, SNE, t-SNE. Moreover, in t-SNE, one can choose t-distribution kernel with various degrees of freedom, see https://link.springer.com/chapter/10.1007/978-3-030-46150-8_8 and https://papers.nips.cc/paper/2009/hash/2291d2ec3b3048d1a6f86c2c4591b7e0-Abstract.html. The former paper shows that it has nontrivial effect on the embeddings. In UMAP, one can adjust the kernel via parameter b. So if you use default UMAP's P_ij and only apply your kernel from Eq (10), what effect does it have? Can one achieve similar effect by adjusting b in standard UMAP?\n\n* What actual values for the intrinsic dimensionality do you obtain for the datasets you analyze? MNIST, RNAseq, COIL, etc? What values of alpha and beta do you then have for Eq (10)? How different is the resulting kernel from standard t-SNE/UMAP?  \n\n* Runtime is never mentioned. The runtime of UMAP heavily depends on n_neighbors. How many non-zero P_i|j values do you end up with on average when following your procedure for your datasets? How does this affect the runtime?\n\n\nMEDIUM ISSUES\n\n* Sections 1+2 review the the prior related work but do not really emphasize what exactly are the shortcomings of the existing work. So by the time I read Sections 1+2, it was not really clear to me what *problem* the authors are trying to solve in this manuscript.\n\n* The authors could acknowledge and comment on some the debate/discussion about UMAP, e.g. in Section 2. For example https://www.nature.com/articles/s41587-020-00809-z argued that the claims of better global structure preservation than t-SNE are mainly due to initialization. More importantly, https://arxiv.org/abs/2007.08902 and https://arxiv.org/abs/2103.14608 argued that UMAP is not really optimizing its stated loss function (cross-entropy), due to negative sampling. Given that SpaceMAP is based on UMAP, this seems relevant to at least somehow mention.\n\n* Section 3.2.2, first sentence -- unclear.\n\n* Section 3.3 -- this multiscale procedure with \"near\" and \"middle\" points reminds me of https://jmlr.org/papers/v22/20-1061.html and also of multi-scale kernels in https://www.nature.com/articles/s41467-019-13056-x. Some context would be good to have.\n\n* Paragraph after Eq 13 -- this is unclear. https://arxiv.org/abs/2007.08902 claims in Discussion that negative sampling used in UMAP is not really similar to SGD. Do you agree? \n\n* Section 4.1 -- please list all implementations that you used, give exact versions.\n\n* Section 4.1 -- how did you preprocess the Tasic2018 data? Gene selection? Any transformations? PCA reduction prior to t-SNE/UMAP/etc? This absolutely needs to be described. Did you follow some other paper in how this dataset was preprocessed? Then cite. Also describe all these details for all other datasets.\n\n* Section 4.1 -- how did you set hyperparameters of SpaceMAP?\n\n* Section 4.2, last line -- UMAP is not always faster than t-SNE, see e.g. https://www.nature.com/articles/s41467-019-13056-x.\n\n* Most differences in Table 1 are very small, e.g. local metrics are almost the same for t-SNE/UMAP/SpaceMAP. But in fact t-SNE is known to perform much better than UMAP for kNN preservation for small k (like k=10 or k=15), see e.g. Figure 7 in https://arxiv.org/abs/2007.08902. The difference between t-SNE and UMAP there is large. Not sure why this is not seen in your Table 1?\n\n\nMINOR ISSUES\n\n* The font size in many figure panels is completely unreadable (e.g. Fig 1d, Fig 4, Fig 6). Please make sure everything is readable when printed out.\n\n* page 4, top: \"D-dimensional hyper-sphere\" --  I think you mean D-dimensional ball, not sphere?\n\n* Eq 11 -- in standard UMAP implementation, the (1-P_ij) term is simply approximated with 1. If you do the same, it's worth a comment.\n",
            "summary_of_the_review": "The first part about theoretically grounded low-dimensional similarity kernel (which seems to be the main focus of the paper) I found interesting. However, I feel that combining two very different UMAP modifications together made the paper weaker because it is not clear what effect do they have on their own. I'd much rather see more experiments that really elucidate the \"space expansion\" idea and effect. Also, the paper is missing a lot of relevant context/citations. Overall I feel that the work has potential but the paper is not really ready yet. I am giving a lower-borderline score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a new similarity-based Nonlinear Dimensionality Reduction (NLDR) method for different types of data. The proposed method focuses on more deliberated construction of the input and output similarities.\n",
            "main_review": "The experimental results are impressive. Especially, this is the first method that can achieve both good clustering and unfolding the synthetic swiss-roll data. \n\nHowever, the presentation of the methodology is awful. It is almost impossible to reproduce the algorithm and results.\n\nIf the following points can be correctly addressed and revised, I am still willing to accept the paper.\n\nThe major problem: it is hard to connect the fancy theory to concrete algorithm steps.\n* There is no submitted code. The pseudocode is too coarse, with many details missing or hidden.\n* It is unclear how to get alpha, beta, and sigma. Without source code, the authors should at least provide the necessary formulas.\n* It is unclear how to get gamma in Eq. 8 as well\n* The functions FD and FDInverse are not defined\n* I don't find the meaning of the subscript t.\n* What are the differences (and connections) among sigma, sigma_i, and sigma_{i,t}?\n* In the formula \\tilde{R}_{ij,D\\rightarrow d} = alpha R^{beta}, where is there ij on the left but not on the right?\n* In the text below Eq. 7. \"n_middle is set to avoid the comput\". It seems the sentence is incomplete. Please carefully proofread your paper!\n\nThe construction of P could be problematic.\n* They tried to automate the choice of n_near by one-class SVM. But one-class SVM itself contains hyperparameters.\n* The statistics about n_near(i) over i's should be added\n* Besides KNNs (i.e., S_{i, near}), the P construction requires more non-zero entries from S_middle, which could increase the complexity. Especially, n_middle is the average of n_near(i)'s, which can become quite large (when one n_near(i) is very large). Discussion about the time and space complexity should be added.\n* It is unclear why P_{j|i} in S_middle takes the form in Eq. 8\n\n\nOther comments:\n* The term \"any data\" in the title is overselling. There are many types of data not covered in the paper (e.g., graphs). When the similarities and distances between data objects are unavailable, the proposed method doesn't work either. For example, there is no simple distance function between two natural images (consider ImageNet). So removing the data scope actually makes the title non-informative.\n* Page 2, the definitions of d_ambient, d_global, and d_local are unclear. What do you mean \"the dimension of measurement\"? Why do you say d_ambient >> d_global? Why is d_local a single number?\n* Are Definitions 3.1 and 3.2 new? If no, it is better to give citations.\n* In Algorithm 1, i is already used as subscripts for P_ij. But it is also the index for epochs. Is this a typo?",
            "summary_of_the_review": "The experimental results are impressive, with certain breakthroughs in NDLR. However, the presentation is poor and needs significant modification.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}