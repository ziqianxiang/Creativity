{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper studies the relationship between time, computation and communication and their impact on training FL algorithms, and presents an automatic hyper-parameter algorithm for federated learning (FL) known as FedTuning that considers training preference for time, computation and communication. To support the use of the FedTuning, the authors first show the relationship between time, computation and communication overheads and issues when different FL hyper-parameters are used for training via a measurement study, then show the performance of FedTuning for different training preferences for time, computation and communication. ",
            "main_review": "The strengths of the paper lies in the presentation of the paper and the details associated with  the studies presented and mathematical functions in the paper. The manuscript was written with clarity and in a logical manner. To elaborate, the paper first starts with introducing federated learning (FL) and its applications. Subsequently, the authors provide definitions for time overhead, computation overhead and communication overhead - all which are key terms that will be used throughout the paper. The authors then discuss training preferences of FL-based applications based on different types of scenarios. Accordingly, this sets up the need for considering the three overheads when developing FL-based applications. The authors then briefly discuss the literature that studied FL training performance under different hyper-parameters and the issue with the existing literature, then state how their work fits into the existing work on FL hyper-parameters. The authors then provide technical details pertaining to the three overheads defined earlier and the FL algorithm that they will use for the study (the algorithm being FedAvg). Finally, they first provide the details from the measurement study, followed by the FedTuning algorithm and the empirical evidence from evaluating FedTuning. Note that the details on the studies are adequate for replication purposes. \n\nWith presenting the findings from the measurement study prior to the empirical evidence for FedTuning, and elaborating on the relevant details and findings from both studies, this is good as the measurement study component explains the relationship between time, computation and communication overheads when different FL hyper-parameters are used for training, while the FedTuning study covers the details around how FedTuning optimizes FL hyper-parameters.\n\nIn terms of details, the authors provide sufficient details around the dataset that was used, the steps that were taken during the study, and explanations pertaining to the empirical results. For example, there is a section in the paper that is solely dedicated to providing a heuristic explanation of the results from the measurement study. In addition, they provide tables and graphs to help visually view the findings and support relevant explanations. For the mathematical functions and models, the authors clearly defined the variables that are used for each function and model, the motivation and explanation behind selected variables that require more details, and presented the FedTuning algorithm in an appropriate manner. For example, when discussing the technical details associated with time overhead, the authors state the mathematical function and details associated with each variable in the function. With the definition of time overhead provided in the beginning of the paper and the mathematical function and details associated with it being presented after, the reader will comprehend time overhead from a technical and non-technical standpoint. \n\nThe reason for why I recommend the paper is because the paper presents a new type of automatic hyper-parameter algorithm for FL that will be helpful for practitioners, in an appropriate manner. The paper optimally details the algorithm and its necessary components needed for comprehension purposes, and provides ample grounds to support the use of the algorithm through the empirical evidence and explanations that it provides. \n\nThere are three issues with the paper. The first issue is the following: while the authors acknowledged that there is a degrading performance for a given time, computation and communication in the empirical results (performance of FedTuning), they fail to properly address it. This is important as given that there are two instances of degrading performance, it’s possible that there are more of such instances. Therefore further discussion is needed around this in the paper. To elaborate on the issue: they highlight how for a specific time, computation and communication, FedTuning has a degraded performance (overall performance is listed as -21.63% in Table 3.), but only state that further exploration is needed to understand why FedTuning performed poorly at the given time, computation and communication. Additionally, it should probably be worth noting that there is another occasion where given a specific time, computation and communication, FedTuning has a degraded performance (overall performance is listed as -0.71% in Table 3.). To properly address this, the authors should have either investigated and provided an explanation for the degraded performance of FedTuning in those instances, or have mentioned it in the discussion section; explicitly stating how FedTuning should be further studied to understand why the algorithm performed poorly in those instances.  \n\nThe  second issue with the paper is around some of the claims made in the paper. For example, the paper claimed that literature covering federated learning training performance under different hyper-parameters did not consider time, computation and communication altogether, which is not true. Below are two examples of work in federated learning that cover such cases:\n\n1. A Performance Evaluation of Federated Learning Algorithms by Adrian Nilsson et al. (2018). \n2.Robust Federated Learning Through Representation Matching and Adaptive Hyper-Parameters by Hesham Mostafa (2019). \n\nThe third issue pertains to wording and sentence structures. There were grammatical and syntactic errors which obscured the clarity of the manuscript. For instance, on page 6 the tuning was spelled as “tuninig”.\n\nOne other area of interest which is not necessarily a strength or a weakness, but is a point of improvement or concern comes from the literature review component of the paper. While it is good that the authors provided literature on the application of FL and application scenarios of FL-based applications, it would have been helpful (or better) to discuss literature specifically on automated hyper-parameter tuning methods for FL algorithms, then connect that literature to their proposed automated hyper-parameter tuning method. In doing so, this could better strengthen the motivation behind FedTuning and improve the case for FedTuning. ",
            "summary_of_the_review": "Given the utility of FedTuning for federated learning algorithms and the evidence supporting the use of FedTuning, and given the presentation of the paper (explanations, algorithm presentation, etc.), I would recommend the paper. Additionally, the issues with the paper are not issues that would merit not recommending the paper. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The proposed FedTuning aims at tuning FL hyperparameters (local epoch E, number of participants of training round M) during the training phase, to maximize different setups of the reward functions of time, computation, and communication. ",
            "main_review": "Although the proposed method is simple and lightweight, I still have a few concerns as follows: \n\n(1) The proposed method cannot scale to high dimensional hyper-parameters. Currently only E and M are tuned, which is low dimensionality (E has only 5 choices between 0.5, 1, 2, 4, 8 and M has only 4 choices 1,10, 20, 50), which is very small in total 20 choices. For FL, there are multiple hyperparameters with high dimensionality, especially playing with personalization. Using the proposed method seems to treat each parameter independently via optimizing the reward function, could ignore the interplay between multiple hyperparameters: again, no experiments on high dimensional is shown in this paper.\n\n(2) The authors argues that canonical Hyper-parameter Optimization (HPO) cannot be used, since (a) reward function are hard to compose, and (b) the hyperparameter tuning cannot \"comeback\". However, there exists multiple HPO/Reinforcement Learning hyperparameter tuning methods that can be deployed in this \"online\" setup. Missing comparison on these methods make the paper's claim not solid.\n\n(3) Experiments are far from enough. The only dataset is Google speech-to-command.\n",
            "summary_of_the_review": "Overall, the solution is simple and lightweight. However, extend the solution to more hyperparameters are not clear. The scalability of proposed method is a question mark. Yet, the experiment is limited. given above concerns, I give my score as 3.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper considers the problem of tuning hyperparameters for federated learning systems. The paper conducts a measurement study using an ASR dataset, and running on a server with GPUs, to illustrate tradeoffs in choices of three FL hyper-parameters: the number of clients selected per round, number of training passes over local data per round, and model complexity. Then the paper proposes a model and method for carrying out hyperparameter tuning (fixing the model, and focusing on cohort size and number of local epochs per round).\n",
            "main_review": "The problem considered in this paper, hyper-parameter tuning for federated learning systems, is extremely relevant and important to practitioners. It also has not yet been widely studied. It is commendable that the approach starts from measurements and aims to develop a methodology which can be useful to practitioners.\n\nHowever, the current paper has a few important limitations (lacking motivation for the particular methodology proposed, strange/unrealistic assumptions, omitting important privacy-related characteristics of FL systems). Consequently I recommend the paper be rejected. Detailed comments follow.\n\n\n1. The paper would benefit from having a stronger motivation for why we should consider all three metrics (time, computation, and communication). In particular, time already captures the other two, in a sense, since a confirmation that is either too computationally expensive (e.g., large $E$, large model) or too communication intensive (very large $M$), will also take a long time to converge.\n\n2. The assumption that clients have homogeneous hardware (Sec 3) is completely untrue in practice, especially in cross-device FL. While I appreciate that this limitation was acknowledged and discussed in Sec 7, I didn't find the discussion sufficient to convince me that the current study warrants publication. Accounting for heterogeneity seems fundamental to making the results relevant to practical FL systems.\n\n3. The model considered in this paper misses a few important points which are highly relevant to practical FL deployments. \n    * To provide privacy guarantees, practical FL deployments use secure aggregation (e.g., Bonawitz et al., 2017).\n    * In addition, practical FL deployments typically incorporate some form of differential privacy (e.g., Abadi et al., 2016).\n\nWhy do these things matter relative to this study? First, existing secure aggregation protocols used in FL generally have a communication complexity (time to complete aggregation, not just the number of bits transmitted) that depends on the number of participating clients. Thus the constant $C'$ is really a function of $M$. Second, the amount of differential privacy noise (i.e., noise variance) generally depends on both the number of clients participating in the round $M$ and the model size (number of parameters). In particular, it does not seem reasonable to assume that $C'$ is zero.\n\nSince the aim of the paper seems directly to impact the practice of FL, it is difficult to place much weight on the conclusions without incorporating these factors.\n\n4. Are training times on the experimental system (server with Nvidia GPUs) at all reflective of the times one would expect in an actual FL deployment or relevant to predicting performance in a deployment?\n\n5. The settings of other hyperparameters (client and server learning rate, momentum) can also have a very significant impact on time to reach target accuracy. It seems that these were kept fixed for each configuration, but I would expect that one might see different results if they were also re-tuned.\n\n6. The paper is missing a reference to highly relevant related work: Khodak et al, \"Federated hyperparameter tuning: Challenges, baselines, and connections to weight-sharing,\" [arxiv:2106.04502](https://arxiv.org/abs/2106.04502)\n\n7. The comparison function (5) seems to come from nowhere. Why this particular form? Is there precedence for this sort of comparison function elsewhere in other literature (e.g., on hyperparameter tuning in other settings)?\n\n8. I appreciate that conducting experiments such as those in reported in Sec 4 require significant effort. In general, one would expect that the conclusions depend strongly on the task in consideration (data type, model, ...). The paper could be strengthened by including results for additional workloads (e.g., next-word prediction), and showing that similar trends generalize across workloads.\n\n\nMinor:\n* The intro seems to imply that computation is the only way that energy/carbon is expended, but this is not true. Communication can also be a significant energy consumer, especially when it is over a wireless medium.\n* Even if all clients did have the same hardware, this does not necessarily mean that it would be reasonable to assume they all have the same quality network connection.",
            "summary_of_the_review": "The problem considered is important, but I am not convinced that the proposed methodology is relevant to practitioners. The paper makes some unrealistic assumptions which may significantly impact the conclusions, and it also misses some important aspects of FL systems related to privacy and security which I would also expect to impact the results.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "An algorithm for online tuning of Federated Learning algorithms, that adjusts the number of per-round participants and the number of local training passes.",
            "main_review": "Strengths\n- Tuning per-round participants and local training passes, especially in an online fashion, is a problem of interest in FL\n- The proposed algorithm is simple to implement\n\nWeaknesses\n- The algorithm's evaluation is only empirical. This is not a problem by itself, but the evaluation is only performed for one model (ResNet-10) and one dataset, using FedAvg. It is unclear whether the algorithm generalizes to other models and other datasets, let alone other FL algorithms that perform model synchronization in a different way.\n- The algorithm is somewhat \"short-sighted\" in that it measures accuracy changes between two adjacent FL rounds, when some sort of moving window would be arguably better. By measuring accuracy between adjacent rounds, the algorithm becomes highly sensitive to local noise in training, which becomes more severe if E, the number of local training passes, becomes small. When this is coupled with the larger issue of only having a single model and dataset for evaluation, it raises further concerns that the method may not be generalizable.\n- The paper does not discuss how to select epsilon, the accuracy improvement threshold for performing tuning adjustments. The experiments section did not even report what value of epsilon was used.\n- Table 3: The \"Overall Performance\" column is misleading. What does it mean for overall performance to be \"calculated using Eq 5\"? M or E may change every round, so there aren't just two sets of hyperparameters. Yet Eq 5 only takes two sets of hyperparameters.\n- Related to the above comment, it would have been better to analyze the Time, Computation and Communication results in depth, rather than claiming that the algorithm achieves \"average 41.04% improvement\", which is not meaningful if the definition of \"overall performance\" is ambiguous to begin.\n- Section 4 makes some claims that are not well-justified, and does not seem totally connected to the algorithm and experiments - none of the \"heuristics\" are used by the algorithm. There are also specific issues:\n- 1) Sec 4.1, Figure 2(a)-(c): The number of rounds R required to converge with different # of participants is not reported. I think this is important information that needs to be communicated.\n- 2) Sec 4.1, Model Accuracy: The conclusion that smaller models are better is somewhat misleading. In the limited scenario studied by the paper (ResNet with a single dataset), all ResNet model sizes happened to be able to achieve the target accuracy of 0.8. What about scenarios where not all model sizes can reach the target accuracy?\n- 3) Furthermore, model size is not even tuned by the algorithm, so the contribution here is limited.\n- 4) Sec 4.2 is difficult to understand, and I question its generalizability to other FL scenarios. A major issue is that the number of rounds R is not discussed. The writing suggests that R is different depending on the scheme used, but never makes it explicit.\n",
            "summary_of_the_review": "The problem being studied is valuable, but the empirical evaluation is lacking both breadth (only 1 model and dataset) and rigor (ambiguously-defined metrics), and there is no theoretical contribution to compensate. The algorithm, while sensible, does not break new technical ground.\n\nAnother issue is that Section 4, which precedes the algorithm/evaluation section, is only loosely connected to the latter. None of the \"heuristics\" from Section 4 are used in the algorithm. One of the dimensions mentioned in Section 4, model size, is also not part of the evaluation.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a new scheme, namely FedTuning, for round-by-round tuning of the standard federated learning scheme (namely FedAvg). FedTuning optimizes for time, computation and communication according to the preference of the system designer. Tuning is done within a space of 3 parameters of FedAvg (number of clients selected, local epochs, and model complexity) related to the optimization goals. The authors empirically evaluate their approach on a real-world speech-to-command dataset.",
            "main_review": "## Strengths\n\n1. [Motivation] \n\nThe paper identifies the important problem of tuning federated algorithms under time, computation and communication constraints and provides a useful overview of many trending FL applications. To further strengthen the motivation, the authors should provide more concrete numbers instead of vague claims.\nFor example:\n  - “considerable price” -> X % of salary per month on average\n  - “adapt to malicious traffic rapidly” -> adapt to malicious traffic in a few seconds [?]\n\n2. [Dataset] \n\nThe use of a speech-to-command dataset is relatively under-studied in the federated learning literature (compared to standard image datasets). The reported results and findings can be relevant for subsequent studies.\n\n3. [Presentation / Clarity] \n\nThe paper is clearly written and easy to follow.\n\n4. [Reproducibility] \n\nThe evaluation section contains details about the setup used to derive the experiments which is crucial for reproducibility.\n\n## Weaknesses \n\n1. [Assumptions] \n\nThe paper makes two main assumptions that are conflicting with the motivation: the clients have (a) homogeneous hardware and (b) homogeneous network.  Both of these assumptions are unrealistic in a typical federated learning setup. The authors identify the issue with in their discussion section. These assumptions do not hold for the majority of applications mentioned to motivate the paper. The authors should focus on the applications (if any) that respect the assumptions. \n\nThe local training time being proportional to the number of mini-batches is another assumption which does not hold in various scenarios. There are many time-varying factors that affect the computation time which are device-specific (e.g., cache-state, resource allocation, competing workload). The factors cause performance variability within the same device [1].\n\n2. [Significance] \n\nThe results of Table 2 are application and device specific and require benchmarking (or measurement study as they authors name it). However, these results are essential for the function of the proposed algorithm (for setting the derivatives). Having an accurate benchmark of the application that requires tuning essentially requires executing the entire learning before the tuning is made possible. This makes the entire merit of the proposal questionable.\n\n3. [Terminology]\n\nWith GBoard being distributed to millions of devices (as the authors also state) an application with 2618 clients cannot be characterised as “massively distributed”. However, given the scarcity of open-source datasets it is a reasonable choice for a dataset.\n\n4. [Evaluation] \n\nTable 1: Why does ResNet-26 not introduce any accuracy improvements ? Are there any published baselines for these network/dataset combinations ?\n\n\n\n\n[1] FLeet: Online Federated Learning via Staleness Awareness and Performance Prediction\n\n\n",
            "summary_of_the_review": "My recommendation for reject is based mainly on (a) the invalid assumptions and (b) questionable significance of the proposal (see Main Review section -> Weaknesses).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}