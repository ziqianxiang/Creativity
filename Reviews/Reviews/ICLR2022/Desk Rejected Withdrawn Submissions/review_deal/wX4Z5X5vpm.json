{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper considers state only imitation learning (SOIL).  And it proposes to explicitly learn a transition predictor that set a goal and learn an inverse dynamics policy to reach the goal.",
            "main_review": "This paper considers a very important problem setting. I think the proposed decoupled policy optimization method is intuitive and makes  sense. And i believe authors havee done a great job to motivate DPO and mathematically support the idea. I enjoyed reading the paper up to page 4 when the implementation of the DPO idea is presented. I have a few concerns about the implementation of the idea and the evaluation that i would like authors to resolve:\n\n1- I have doubts that the MSE losses in equation 9 and 11 will scale to high dimensional states (such as visual states) and actions. And also in evaluation there is no experiments with visual inputs or high dimensional action settings. I think to demonstrate the usefulness of the idea, experiments with visual inputs should be included. \n\n2- Looking at figure 5 It seems DPO only marginally outperforms other baselines in two tasks out of 6 tasks. I'm wondering if learning continues, baselines will eventually match DPO results? And how the number of environment steps is chosen? ",
            "summary_of_the_review": "I think the DPO idea is intuitive and is well motivated and is mathematically well supported in the paper. However i have some concerns about scalability of the particular implementation of this idea presented in the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents an approach to state-only imitation learning which decomposes the learned policy into a next-state predictor and an inverse dynamics module. This decomposition allows training the next-state predictor directly on expert data, while learning the inverse dynamics model through interaction via some interaction policy. Beyond this, the authors propose several regularization methods to improve performance: (1) a multi-step optimization which ensures the next-state prediction matches expert data when rolled out over multiple timesteps; (2) a cycle training loss which penalizes the difference in the next state that is predicted by the next-state predictor and the state predicted via a learned forward dynamics model by taking the action proposed by the inverse dynamics model to get to the next state; (3) a policy gradient loss using a GAN style discriminator to encourage the combined policy's state transition occupancy measure to match that of the expert. The authors demonstrate that applying PG on top of this decoupled policy yields improved performance over baselines.",
            "main_review": "The idea of decoupling the policy into state-transition prediction and an inverse dynamics model is a clean and elegant approach to the problem of state-only imitation learning. The proposed solution seems to work quite well on the experiments, and I appreciate the thorough ablation experiments demonstrating the utility of the different components to the overall objective. My main takeaway from this work is that choosing a decoupled structure to the policy as proposed by the authors allows augmenting a standard policy gradient approach to matching the state transition occupancy measure with useful auxiliary losses on the different components directly. I believe that this is result that would be of interest to the broader imitation learning community.\n\nMy main concern with this work is in its presentation of the approach. The authors motivate their approach by claiming that the standard policy optimization approach has an \"ambiguous\" objective, but I am not convinced that this is the reason for the success of their approach. Gradient descent based algorithms are not necessarily troubled by the existence of multiple optima; after all, gradient descent works well in neural network training even though there are multiple choices of weights that fit the data equally well. The decoupling the authors propose removes the ambiguity from the next-state prediction network, as there is now a unique optimum of $h(s' \\mid s)# equal to that of the expert policy. However the inverse dynamics learning problem still can have multiple solutions; i.e. its objective is still \"ambiguous.\" \n\nTo support their claims that the ambiguity of the policy-based objective is the reason why the decoupled approach is effective, the authors present a toy experiment where multiple actions lead to the same state transition. Here, it looks like GAIfO is capturing only one mode of behavior, which is typical for GAN/GAIL methods, so the worse performance may not be due to the ambiguous actions. In the appendix, they compare to a version without action ambiguity. Here the action space is much smaller, so it is difficult to disentangle whether the degradation is due to the difficulties of optimizing a higher-dimensional policy or whether the ambiguity in actions is the true reason for the poor performance of BCO.\n\nMy other concern is that much of the practical algorithm is developed with deterministic systems in mind. Indeed, the probabilistic next-state distributions and inverse dynamics distributions $h(s'|s)$ and $I(a|s,s')$ are replaced with deterministic predictors $s' = h(s)$, $a = I(s,s')$. The training objectives are written in terms of MSE, which only makes sense if the true probability distributions are unimodal. Indeed, if there is action ambiguity, then we might observe two transitions from $s$ to $s'$ in the dataset as a result of different actions $s,a_1,s'$ and $s,a_2,s'$. Training the inverse dynamics with the proposed MSE loss would yield an optimal predictor to choose the mean action $0.5(a_1 + a_2)$ which doesn't make sense for multi-modal settings. Similar issues are true for the multi-step and cycle regularization terms. The toy experiment has a discrete state and action space — how were the models set up in this setting? The hyperparmeters for this setting are not listed in the appendix; were any regularizers used? Given that DPO's ability to recover both modes of the multimodal expert state occupancy is touted as an advantage over GAIfO, I believe the authors should present the approach as a general tool that can work with both deterministic and probabilistic modules for $h(s'|s)$ and $I(a | s, s')$.\n\nMinor comments:\n\n- There is a typo in appendix C: in the max-entropy derivations, the *expert* policy, rather than the learned policy, is assumed to be a Boltzmann distribution.",
            "summary_of_the_review": "Overall, this paper presents an interesting, effective approach to state-only imitation learning. My main concern is that this algorithm is presented alongside a claim which states that the ambiguity over optimal policies is the key bottleneck in state-only imitation learning. The authors should either add more evidence to support this claim (via experiments comparing ambiguous vs non ambiguous settings with the same action dimension), or reframe the approach (which seems to work well) as a policy architecture choice which allows for effective regularizers on top of standard policy gradient to match state-transition occupancy measures. As such, I've scored this paper as marginally below the acceptance threshold, but I am open to increasing my score if convinced that the ambiguity is the main reason for DPO's strong performance, or the authors revise their manuscript to tone down these claims.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors introduce a new method for state-only imitation learning (SOIL), called Decoupled Policy Optimization (DPO). DPO works by learning separately two models. The first, an expert state-only transition predictor, is learned from an expert dataset. The second, an inverse dynamics model, is learned from rollouts of a sampling policy in the environment. Critically, this inverse dynamics data can be generated without any human involvement. The paper also introduces two regularization approaches, one of which involves also learning a forward dynamics model. The work is evaluated empirically and outperforms baselines.",
            "main_review": "# Strengths\n\nState-only imitation learning has clear practical relevance. Prior work on state-only imitation learning has tried to directly recover a state-action policy; it seems plausible a priori that decoupling this will provide a useful inductive bias by explicitly representing the structure of the problem. The basic idea of the method is fairly simple which should enable it to be applied broadly.\n\n# Weaknesses\n\nThe empirical results show only a modest improvement on the real-world traffic dataset (80.3% vs 77.5% for DPO vs GAIfO), and both are clearly far worse than the expert (100% success rate). So while DPO may represent an improvement, it seems somewhat incremental. \n\nThe results in synthetic environments are similarly mixed: while DPO does substantially better in HalfCheetah-v2, Hopper-v2 and Walker-v2 in other environments it is tied. The strongest results are in the toy example (Figure 4a), which does demonstrate there is some advantage to the method, but perhaps the situation where it is most advantageous rarely occurs in practice?\n\nI found the compounding error experiment (section 5.3, Figure 6) particularly hard to interpret. You write that \"Combining regularization can always achieve lower compounding error\" and \"From the figure, we see the supervision does regularize the state prediction to be meaningful compared with GAIfO-DP\". But as far as I can see, DPO without regularisation does *better* than cycle regularisation in Hopper-v2, and indeed seems to be the lowest MSE (or tied for it) in Ant. The reduction in MSE only really appear in Walker-v2 and Half-v2.\n\nRelated to this, I'm puzzled by what the Cycle regularisation is trying to do. As I understand it, you train a forward and inverse dynamics model from a dataset collected by some fixed sampling policy. You then train the state transition predictor to predict what state the expert transitions to (with a MSE loss), regularized with an MSE loss between the state transition prediction $h_{\\phi}(s)$ and the \"prediction\" made by the forward composed with the inverse dynamics model for the ground-truth transition $(s,s')$. The thing I don't get is why should $h_{\\phi}(s)$ try to be close what the forward composed with inverse dynamics model predicts, if that disagrees with the actual expert transition?\n\nThe paper would also benefit from being carefully proofread. It contains some grammatical issues throughout which at times made it difficult to understand. I've included some example issues to fix below, but for reasons of time could not make a comprehensive list.\n\n# Questions \n\n  1. Do you have an explanation for why the regularization performance varies so much between environments?\n  2. What's the principled reason why cycle regularisation should help?\n",
            "summary_of_the_review": "The paper introduces a natural method to solve a problem of clear practical relevance: state-only imitation learning. The empirical results shows modest improvement, but varying significantly between environments, with the strongest results in a toy environment. The benefit of regularising DPO appears mixed, especially the Cycle method, which I cannot see a clear a priori justification for. Overall I find this paper borderline, but lean towards rejecting the current submission as the extent of empirical improvement is unclear and the theoretical justification is suggestive but far from conclusive. However, I do think the method has promise. I would consider increasing my score if the concerns about cycle regularisation can be addressed, or if there are empirical results where DPO outperforms baselines by a more substantial margin (beyond the toy gridworld example).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}