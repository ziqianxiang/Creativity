{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, the authors studied the problem of graph representation learning via automatically searching both neural architecture and input data augmentation. The two searching parts are unified into a principled bi-level optimization problem. Experimental results on four node classification data sets showed that the proposed method achieved better performance than the baselines.",
            "main_review": "Strengths\n1. The idea of data-model co-search is new.\n2. The proposed method showed higher performance across data sets.\n3. Paper is easy to follow.\n\n\nWeaknesses\n1. Tasks and data sets are limited. \nIn the paper, the authors only used four node classification data sets. However, in order to fully verify the effectiveness of the proposed method in the graph domain, more data sets (especially large ones) and other tasks including graph-level ones are needed. BTW, the three data sets of Cora, Citeseer, and Pubmed are too small and easy to result in the overfitting issue.\n\n2. The benefits of NAS are not well studied.\nI have several questions/concerns. i) Why is it better to combine the searching of data and model? Any hypothesis?  ii) How about the cost of doing NAS for data and model?  iii) One main advantage of NAS is to determine efficient and effective smaller networks that can achieve similar or even better performance than the full-size one. However, according to the columns of 'layers=16' and 'layers=32' in Table-3, I did not observe the advantage of NAS. How about considering #layers as a searching dimension? \n\n3. Ablation studies are not convincing. Please use other large data sets and more tasks.\n\n4. Writing can be improved. I also see multiple typos.\n",
            "summary_of_the_review": "This paper is a borderline paper. More data sets, tasks, ablation studies are needed to demonstrate the effectiveness of the proposed method. Last but not least, the advantages of NAS are not well studied and analyzed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a method which simultaneously optimizes GNN architecture and graph topology.",
            "main_review": "Strengths:\n1.The paper is clearly written and easy to follow.\n2.The experimental results seem good.\nWeaknesses:\n1.Why the GNN architecture and graph topology should be simultaneously optimized is not clear. E.g., what about optimizing one after optimizing the other. Or, the idea is a simple combination of two types of search spaces.\n2.The model is somehow incremental, the main difference is adding augmentation search space, the search strategy is borrowed from others, which makes the contribution limited.\n3.In 3.2.1, the author says augment graph can be written as a Haramard product, but how is AddEdge written? Besides, why AddEdge is working tends to be  unclear for me.\n4.The graph augmentation settings seem like a hyperparameter for me, so why not using HPO methods to optimize the graph augmentation setting? These methods are also baselines.",
            "summary_of_the_review": "Combining searching GNN architecture and graph topology is a potential direction. However, there are still several weaknesses in the manuscript and the contributions are limited in this paper with its current shape.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to search both data-augmentation methods and neural architectures for NAS based GNN. The author utilizes a bilevel optimization method to make the search process differentiable and incorporate some existing techniques to make the searching process more stable. The authors conducted experiments to show that their methods outperform some baselines.     ",
            "main_review": "Strengths:\n1. This paper firstly proposes to co-search the neural architectures and augmentation ways for the GNN learning.\n2. This paper utilizes some effective methods to stablize the co-search process.\n3. This paper is mostly organized clear.\nWeaknesses:\n1. Although co-searching the data augmentation ways and architectures sounds reasonable for GNN learning, there seems nothing challenging from the description of this paper. This paper seems to directly incorporate the augmentation operations into the existing NAS paradigm, and the technical novelty seems small.\n2. Since the augmentation search is just as the neural architecture search, which means the final augmentation operation could be only one of the augmentation space, however, the combination of different augmentations may be better than a single one, so formulate the augmentation methods as a single choice may not be optimal.\n3. The compared baselines are not so complete. (1) Since this paper mainly aims to propose search augmentation and architecture, a natural baseline is to use a random augmentation method to combine with some existing nas methods. (2) The NAS baselines are a little old, it's better to compare your methods with some NAS Graph methods in 2020 or 2021.\n(3) It's unclear whether the compared baselines adopt the stablized techniques. If not, it's hard to say whether the co-searching is useful from the experiment.\n4. There are some problems with this paper. From table 3, it seems that only searching model will greatly fail when layers are 16 and 32, however, in table 4, the model-only model will not fail, which contradict with each other. Also, there are some typos, e.g., \"Table 4: the importance of co-search\".",
            "summary_of_the_review": "Although this paper firstly proposes to reasonably incorporate the data augmentation methods into the NAS search structure, the experimental designs still lack some evident support to show the advantages. Additionally, the challenges brought by the co-search are not clarified in this paper, which makes the technical contribution seem small. It's better for the authors to add the experiments in the weakness part to show the effectiveness of the co-searching methods.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The work designs a differentiable data-model co-searching framework for GNNs, which adopts bi-level optimization to simultaneously search the GNN architecture and graph augmentation. Some tricks are also incorporated to scale and stabilize the searching process. Experiments validate the effectiveness of the proposed framework.",
            "main_review": "### Strengths:\n\n1.The presentation of the paper is clear.\n\n2. The proposed framework consistently achieves SOTA experimental performance, especially when training deep GNNs.\n\n### Weaknesses:\n\n1. The major concern is the limited novelty of the work. It seems to be a simple combination of existing ideas (e.g., NAS, graph augmentation, training tricks for NAS) without unique challenges in application to GNNs. And the motivation of combining graph augmentation and deep GNN architecture search lacks sufficient justification.\n2. As for the optimization formulation, it is confusing to me why there is only task loss in Eqn. (4) while graph augmentation is optimized under no explicit supervision. Intuitively, the graph augmentation policy should be controlled to avoid excessive drift from the original graph structure [1] or keep sparsity [2]. It would be better if the authors could provide some theoretical or empirical analysis on, for example, how graph augmentation policy can benefit NAS or how graphs would tend to be augmented.\n3. The experiments are not extensive enough to support the authors’ claim. For example, the necessity of searching graph augmentation policy lacks enough validation, e.g., regarding to the generalization or robustness to noisy graph structure. Furthermore, it is better to show training stability curves besides simple performance comparison to demonstrate the effectiveness of training stabilizers.\n4. Besides GraphNAS and Auto-GNN which are both based reinforcement learning, a recent method which conducts differentiable NAS on GNNs [3] is suggested to be added as a baseline.\n\n### Refs\n\n1. Data Augmentation for Graph Neural Networks. AAAI 2021.\n2. Iterative Deep Graph Learning for Graph Neural Networks: Better and Robust Node Embeddings. NeurIPS 2020.\n3. Search to Aggregate Neighborhood for Graph Neural Network. ICDE 2021.\n",
            "summary_of_the_review": "Overall, the limited novelty and insufficient experiments of the paper make me recommend a weak rejection for it.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}