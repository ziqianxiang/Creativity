{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes LDIST: Language Driven Style Transfer.  The core contribution is to drive image stylization using text description of the desired style.  The architecture of the proposed CLVA technique borrows heavily from recent style swapping work e.g. Swapping AEs of [Park et al., NeurIPS 2020] in that a content and style code are extracted and swapped, with use of a patch based discriminator (D).  The main difference here is used of an MLP to regress a sentence encoding to the global style code used to drive that generator.  ",
            "main_review": "The LDIST task is in the spirit of emerging works such as StyleGAN NADA which also explore image stylization using text, and so makes a timely contribution to the growing body of work applying language for image editing tasks.  \n\nThe technical contribution of the paper is the unification of a pre-trained text encoder (RoBERTa) with an architecture very close to that of Swapping Autoencoders [Park et al.].  As in that work, a content and style code are extracted and used to drive pair-wise stylization using a patch based discriminator.  The main novelty in the LVA architecture here is mapping output of a pre-trained text encoder to that global style code, and so demonstrating that a text encoding may be used as a substitute for an image-derived style code to drive the process.  I consider this quite incremental change on that prior work. Furthermore the contrastive training of the LVA (the CLVA technique proposed) is reminiscent of the pair-wise training strategy of Swapping Autoencoders.  In my opinion this paper could make a much better job of arguing its novel contribution, were this prior work to be discussed openly and the contribution contrasted against it.  This would also help to clarify the technical exposition (including the corresponding architectural diagram in Fig. 2, LVA) which are not very clear to follow. \n\nA further issue with this paper is the somewhat underwhelming results (Fig.5) which show only coarse grain stylization effects such as introduction of colour shifts or particular low-level texture artifacts like vertical stripes.  In many cases the text-specified artifacts are not convincingly present (e.g. teardrop shapes or circular motifs).  The presence of higher level style concepts such as media forms, or artistic styles are not explored.  Particular the ‘fine grained’ control experiment (Figure 4) is not convincing providing mainly coarse grain colour transfers e.g. orange to green.  I believe this is primarily due to the experimental dataset being derived from the DTD dataset which contains paired natural language descriptions of textured patches.  It would have been interesting to explore the application of this approach to captioned artwork styles to better evidence the paper’s main claim of performing style transfer using text.  A good example of such a captioned artwork dataset would be ARTEMIS.\n\nA couple of other areas were unclear to me.  Why is CLIP being used to evaluate as a captioning approach but RoBERTa used to encode the text?   How would this approach compare against an optimization approach like StyleGAN NADA or recent CLIP based style diffusion work that optimizes against a CLIP based text objective?  \n\nOverall I consider this work below the bar for ICLR.  The technical contribution is mainly ‘glue’ between text encoder and lightly modified variant of existing style transfer work.  The technical contribution is not discussed in the context of that related work.  The experimental work does not convince as the value or generality of the LDIST task for style transfer.\n\n",
            "summary_of_the_review": "I have recommended to reject the paper as I consider there to be three major areas of weakness.  1) the technical novelty of the paper is limited.  A largely systems-level contribution is made, combining a pre-trained text encoder with an architecture very close to Swapping Autoencoders. 2) the technical description of the work is unclear as is the relationship between the proposed method and existing work particularly Park et al. 3) the main claim of the paper is poorly evidenced; the stylization results are quite poor and based upon an inappropriate dataset for style captioning work. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper investigates a new task of language-driven image style transfer and proposes a corresponding framework. A language visual artist based on content/style feature disentanglement and adversarial learning is proposed for style transfer. Contrastive reasoning is proposed to reinforce the mutual relativeness between transfer results. Dataset with texture-text pairs is used to quantitatively evaluate the proposed framework against several baselines, and show improved results.",
            "main_review": "**Strengths:**\n+ The language-guided style transfer is new and the motivation of this paper is clearly described.\n+ The method is clearly introduced and easy to follow. \n\n**Weaknesses:**\n+ My biggest concern is the necessity of the proposed language-guided style transfer. \n  - Although language is the most natural way for humans to communicate, I feel that a picture is worth a thousand words especially for style transfer. Some popular styles are hard to be summarized in several words, such as the artworks by Van Gogh and Monet. Therefore, this paper mainly transfers simple styles of pure textures, which might limit applicability. \n  - Moreover, from the visual quality of the proposed method, using abstract language instead of specific images sacrifices the style transfer quality. Image-drive style transfer methods like STROTSS `[R1]` has shown very promising results on pure texture styles. Previous methods like AdaIN also have good performance with improved loss `[R2]`. Without style images, the results degrade severely for NST, WCT and AdaIN. Also, the results of proposed method are obviously inferior to the semi-GT in terms of color and texture consistency. Retrieval-based method seems to be more effective to balance the quality and language-guidance controllability than the proposed framework. I feel that with the wide accessibility of Internet, the image references are easy to obtain. Users might search on google using key words and select the ideal style images and use the powerful image-drive style transfer methods to generate high-quality results. The less satisfying quality of the proposed method makes me feel the proposed language-guided framework less attractive for real use. \n+ The technical contribution is limited. The framework is mainly based on popular modules, which provides limited technical insight. For example, LVA generally follows the feature disentanglement of MUNIT`[R4]`, DRIT`[5R]`. The idea to embed style instructions/images into style features is intuitive and similar to `[R7]`. The contrastive reasoning is intuitive and similar to `[R6]` to pull the style features of stylization results with the same style. \n+ Key experiment details are missing. \n  - How to build the baseline to receive only style instruction is unknown. The paper only says ''Style instructions are jointly embedded with style features for training and directly serves as the provided style during testing.'' How to embed style instructions into the baselines? How can a baseline be trained with style features but tested without style features? The visual quality of the baselines is poor, especially for NST, which might be due to the inappropriate embedding of the style instructions. \n  - The quantitative comparison between the proposed method with and without retrieved style images is given in the supplementary material. Visual comparison is suggested to be given. The baselines of NST, AdaIN, WCT and more advanced recent models like `[R1]`, `[R2]`, `[R3]` with retrieved style images are suggested to be also compared. \n\n**Some small issues:**\n+ Why the value of SSIM is greater than 1?\n+ h^S_{S_{2-1}} should be h^S_{O_{c_2-x_1}} in Eq. (5)\n+ In Eq. (3), in addition to (Po,X) as fake data, the real style image and unrelated text pair (P_S1, X2) could also be used as fake data to reinforce the consistency of the style and text.\n\n`[R1] 2019 CVPR Style Transfer by Relaxed Optimal Transport and Self-Similarity`\n\n`[R2] 2021 CVPR Style-Aware Normalized Loss for Improving Arbitrary Style Transfer`\n\n`[R3] 2021 CVPR Drafting and Revision: Laplacian Pyramid Network for Fast High-Quality Artistic Style Transfer`\n\n`[R4] 2018 ECCV Multimodal Unsupervised Image-to-Image Translation`\n\n`[R5] 2018 ECCV Diverse Image-to-Image Translation via Disentangled Representations`\n\n`[R6] 2019 ICCV Content and Style Disentanglement for Artistic Style Transfer `\n\n`[R7] 2019 ICCV Dual Adversarial Inference for Text-to-Image Synthesis`\n",
            "summary_of_the_review": "This paper proposes a new task and the corresponding framework to handle it. However, the implementation sacrifices the style transfer quality and applicability. The framework is mainly based on popular modules in style transfer, feature disentanglement and contrastive learning. Therefore, I am not very positive.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a language-driven image style transfer (LDIST) method that can stylize a content image with the style described by an abstract text. The authors realize LDIST by designing a contrastive language visual artist (CLVA) model which consists of a language visual artist (LVA) module and a contrastive reasoning (CR) loss. LVA learns the disentanglement and recombination of style and content semantics aided with a patch-wise style discriminator that conditioned on the style instruction while CR forces analogous content images or style instructions present similar content structures or style patterns.",
            "main_review": "Strengths:\n\n+ The authors introduce a new paradigm of style transfer, which is driven by a language text.\n+ The proposed CLVA seems reasonable and has been verified its effectiveness through experiments.\n+ The newly collected database for LDIST is also a plus.\n\nWeaknesses:\n\n+ The style images and instructions used in the experiment are a little too simple, most of which are regular textures. I don't think it's a hard problem to map such a simple style description to a simple style texture. Is there a possibility that the model completely corresponds the style description features to the style image features through sufficient fitting, so there is no difference between using the style description and using the style image as a guide in the test stage? How about the performance when using more complicated style instructions (e.g. Van Gogh's \"The Starry Night\" that usually used in image style transfer)?\n\n+ I also have a concern about the generalizability of the proposed method. I know that the authors have separated the test set from the training set and make sure that the content images and the style instructions for testing unseen. But the style instructions used in testing and training are relatively consistent. By contrast, arbitrary style transfer methods often take a lot of complicated, unseen, real scene style images as guidance to verify their generalizability. \n\n+ The comparison with baselines seems unfair. The authors claim that \"Style instructions are jointly embedded with style features for training and directly serves as the provided style during testing\". However, the baseline methods are not originally designed for LDIST, and there is no guarantee of the effect when only style instructions are used as a guide. I suggest that the authors also use their original results which guided by style images during generation as baselines. \n\n+ The chosen baselines are not the state-of-the-art image style transfer methods (Both were published earlier than 2017). And I find that the authors use the results of FastStyleTransfer (Hub, 2021) as the Semi-GT. Does this mean that the results better than the proposed method are regarded as Semi-GT and the methods less effective are regarded as baselines?\n\n+ There are some typos. In section 3.2, \"$S_X^S \\in R^s$ embeds into the same space of $h^S$ to reflect the extracted visual semantic\", \"$S_X^S$\" should be \"$h_X^S$\"? In formula (5), \"$h_{s_{2-1}}^S$\" should be \"$h_{O_{c_2-x_1}}^S$\"? ",
            "summary_of_the_review": "Overall, the task is quite new and interesting to me. But I don't think the authors have provided a solid solution for this task. I'd like to improve my score if the concerns above are addressed properly.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes replacing the reference style image with a text description for modifying the style of given target image. Such a setting allows more flexible and straightforward style control from texts, compared to looking for a suitable style image. A contrastive language visual artist (CLVA) is introduced to learn extracting visual semantics from texts and conduct the transfer by a patch-wise discriminator. Another stage of training based on contrastive learning is designed to improve the mutual relativeness between transfer results. Several interesting qualitative and quantitative results are presented. The ablation study is also shown to explain the effectiveness of each component.",
            "main_review": "Strengths:\n+ The problem of using texts for style transfer is less explored\n+ Easy to read\n\nWeakness:\n- The problem needs better definition and critical thinking. Simply replacing the style image in the traditional style transfer setting with a style text is not a reasonable extension. As claimed by authors, using style text is easy to communicate. However, take the car image in Figure 1 for example. It's relatively hard for me to evaluate the results. I do not understand why these instructions shown in Figure 1 will be used for editing a car image. I probably will type an instruction like “make the car look sportier”. I know style transfer methods could work on arbitrary inputs, but I do believe we need to do sort of meaningful transfer for practical use cases. In terms of this, previous text-to-image works make more sense via editing the object or its properties.\n\n- Step back and if we only talk about the transfer results, they look still limited. First, visually they’re not quite attractive or artistic, being a bit more abstract, though I understand this might be subjective. For results in Figure 3, I expect to see artistic effects like \"Spiral\" and \"Curly\" in style descriptions but there are not. Then for ablation results in Table 2, the contribution of each component is incremental. For numbers in several columns, they are nearly the same for the first 2 or 3 digits.\n\n- For the patch-based discriminator, what is the difference with the patchGAN in Pix2Pix and that in [A]?\n[A] Park et al., Swapping Autoencoder for Deep Image Manipulation, NeurIPS 2020.\n\n- I assume the text description that this model could work with is within the DTD dataset (or closed set). For open set descriptions, e.g., the style text contains geometric deformation words, the model is not generalizable right?\n\n- While authors claim that this work is for color and texture only in Section 3.1, it is better to limit the range of applicability in both title and abstract too. \"Language\" is a way too big term.\n\n- A better term for semi-GT could be pseudo-GT.",
            "summary_of_the_review": "In general, I have some doubts on the problem definition and the results themselves are not quite convincing. The descriptions used from DTD is far from practical editing descriptions and results look not correspond to the text very well. The current draft is not good enough but I encourage authors to delve deeper in this direction.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}