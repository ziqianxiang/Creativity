{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "Reinforcement Learning (RL) methods have been leveraged in recent years to derive construction heuristics for certain combinatorial optimization problems such as the Travelling Salesperson Problem (TSP). The instance distribution is assumed to be fixed and training samples are available for the RL. Test instances from the same distribution are then used for final assessment of the learned policy. However, when test instances are significantly different from those used in training, such RL-based policies tend to perform poorly.\n\nThis paper proposes an iterative training approach in which two policies are learned: an instance generation policy and a solving policy (for the TSP). The former tries to generate instances which are hard to solve using the current solving policy, whereas the latter tries to solve the new harder dataset. At the conclusion of this training procedure, multiple solving policies, each trained for one dataset in one iteration, will have been accumulated. Those policies are mixed at test time. A game-theoretic argument based on prior work is used to justify the proposed approach.\n\nExperiments show favorable performance for the proposed method, which seems to find better TSP solutions on average compared to RL-based methods that use the typical one-round training paradigm. This is shown on both random TSP instances with 20, 50, and 100 cities, as well as benchmark instances from TSPlib. Ablation experiments show that the convergence of the proposed training method is as expected and that the way the various solving policies are mixed produces better results than other mixing strategies.",
            "main_review": "Strengths:\n- Connecting the generalization problem in learning TSP heuristics to a game-theoretic approach is interesting and seems to be easy to apply in practice.\n\n- Decent performance on the TSPlib benchmark.\n\nWeaknesses:\n1. Impact: the contributions of the paper and the experiments are limited to a “toy” setting with small random TSP instances and a relatively outdated TSPlib. No attempt is made to motivate an actual problem faced when deploying RL-based TSP heuristics in practice. Given that the space of possible instances is massive and that the proposed training algorithm is executed only for a few iterations, it is unlikely that the final heuristic will be useful for drastically different test instances. In other words, this approach is unlikely to be useful in any practical setting.\n\n2. Clarity: the presentation of the method, notation, and experiments is messy; more detailed comments are below.\n\nSome questions for the authors:\n\n3. Need for oracle: based on gradients in (6), (9), and (10), as well as 4.3, you seem to rely on knowing the optimal value of each TSP instance so that the g(.) function can be evaluated. Yet, I could not find a discussion of this in the paper. Do you use Concorde to get the optima? If you do need this expensive computation indeed, then your method is essentially “supervised”.\n\n4.  Table 1, LIH (FS): for this variant of your method, you run your training algorithm once for each n=20, 50, 100. Is your training algorithm faster or slower than the LIH training algorithm (for the same number of epochs)?\n\n5. Training data: you say that your method is tested on instances that were never met during training. However, in the appendix (“Setup in meta-level”) you say that PSRO is used to train different models for TSP20, 50, and 100. How do those training runs differ? I am completely unclear on this. Perhaps more specifically: in Algorithm 1, how do you initialize the data generation policy? How is that different for LIH (FS) with n=20 vs 50 vs 100? Same question for the RL-based baselines like AM and LIH in Table 1.\n\n6. Mixing relies on multiple inference passes, one per policy, meaning that your method has a higher computational cost (as you also discuss in the paper and appendix). Figure 3 seems to say that using the top-2 models alone is not sufficient. Looking at Table 1 and comparing LIH with LIH(FS), the latter has only a slightly better Gap but typically slightly larger Time. This shows that there is not much benefit to your approach for these datasets.\n\n7. Overall time complexity of your method: can you provide a big-O analysis for the running time of Algorithm 1 as a function of calls to the Oracle? This question is complementary to Q4 above about empirical running time.\n\n8. Missing related work: Both deal with generating hard combinatorial instances. The first one is extremely related to your work. Future iterations of your work should cite these and discuss them at length given the relevance.\n\n- Zuzic, Goran, et al. \"Learning Robust Algorithms for Online Allocation Problems Using Adversarial Training.\" arXiv preprint arXiv:2010.08418 (2020).\n\n- Smith-Miles, Kate, and Simon Bowly. \"Generating new test instances by evolving in instance space.\" Computers & Operations Research 63 (2015): 102-113.\n\nMinor comments:\n- p1: “deep learning-based solvers”; the word “solver” is typically reserved to exact algorithms. “Heuristic” is more appropriate for what you’re doing.\n- p1: “we empirically illustrate THAT as the solvers’…”\n- p1: “due to its ability” —> “due to their ability”\n- p1: “generalisation” vs “generalization”, please stick with one. Same for “game-theoretic” vs “game theoretic”, “real world” vs “real-world”.\n- p3: Best Response definition. I believe the $\\forall i$ is not necessary there.\n- p3: “Optimal gap” —> “Optimality gap”\n- p4: the utility for the data generation should be $-G(\\pi, \\text{Oracle})$\n- p5, Figure 1: “loop t”: please use “iteration t” instead throughout.\n- p7: “10 groups data generated …”: I don’t understand what this is saying, can you rephrase?\n- p7: “the time consuming” —> “the time consumption”\n- p8: “exixts” —> “exists”",
            "summary_of_the_review": "Overall, this submission is not ready for publication. The setting is limited to TSP and small instances thereof. The writing needs substantial improvements. The experiments are lacking details about the cost of training, among other things. I encourage the authors to revisit these aspects in future iterations of the paper, and perhaps try to be more ambitious in the size of the TSPs you tackle and move from TSP to harder problems live VRP.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a formalization of the training of a model to solve the TSP in a game-theoretic framework, with the goal of making the model more robust and generalizable. They formulate the learning as a 2 players zero-sum game, with data generator responsible for generating instance distributions that are hard to solve for the second agent, the solver, whose goal is to optimize its performance on the instance distribution provided by the generator. Experimentally, the authors show that this the framework allows to significantly improve the results of a state-of-the-art learning method on both synthetic and realistic TSP instances. \n\n",
            "main_review": "**Strengths**\n1. The paper addresses an important problem which is the poor generalization of learning-based methods for solving combinatorial optimization problems.\n2. The idea of modelling the problem in a game theoretic framework is novel and smart.\n3. The paper is well-organized, especially the Method section \n4. For the TSP, results are provided for both randomly generated and more realistic datasets (TSPlib)\n\n\n**Weaknesses**\n1. Apart from the definition of the players (utility and policy set), the paper seems to be a straightforward application of an existing framework (PSRO) in a new context (learning-based heuristics for the TSP). It would be helpful to highlight the differences/adaptation w.r.t PSRO if there are any.\n2. Although the claim and the idea are general, the approach is applied to only one model for solving the TSP. I think the contribution would be stronger and more significant if the idea was successfully applied to other CO problems and/or other base models.\n3. Some important points were missing for me, after reading in detail the paper and skimming through the appendix:\n    * The Oracle that gives the “(possibly) true optimal value of an instance” and that is used to define the utility of the players strategies (e.g. in Eq. (2) and (4)) is not explicitly defined. Do the author assume having access to optimal solutions during training? Or some externally provided good solutions? If they are not exact they might bias the search; on the other hand if they are exact, it’s an important assumption of the proposed approach that should be explicitly stated.\n    * Missing information in the experiments (see **Questions**) that made it harder to understand and evaluate the overall performance. \n4. Because of the form of the distribution (mix) for the graph size N, I understand that the data generator always gives instances with the same sizes, just with different proportions $\\sigma_{DG}$. For example, if $\\mathcal{N} = \\\\{20, 50, 100\\\\}$, the solvers will only see instances of either 20, 50 or 100 nodes, uniformly at the beginning and then with various proportions as the training progresses. Conceptually, this amounts to having models that perform well on datasets with different proportions of these graph sizes — which is different from having a model able to generalize to unseen sizes. From the results in Table 2, we see that the models are indeed able to generalize to unseen sizes, do the authors have an explanation or intuition about this?\n\n**Recommendation**\n\nI would vote for reject. In my opinion:\n* the novelty is limited: it’s the application of a known framework in a new context, \n* and the experimental validation is also limited: it shows that applying this idea improves the generalization of one base model for solving the TSP,\n* The choices of the context-specific ingredients of the algorithm lack motivation (see Questions) and it is not clear to me how they are designed to promote generalization in particular to larger graphs (see Weaknesses). \n\n**Questions**\n1. Sec 4.4.: what is the motivation behind the proposed combining of solvers, versus the more standard sampling strategy? I.e. sampling a solver $S \\sim \\Pi_{SS}$ according to $\\sigma_{SS}^*$\n2. In Algo 1, “train the oracle for solver sector S’”: what is meant here by train? Until convergence, or for a fixed number of episodes, etc? There is an asymmetry between how the training of the solver selector and the data generator are handled (at least in the presentation). Given that in PSRO the players oracle strategies are treated the same, is there a motivation for this choice here?\n3. What is the motivation behind the definition of the attack on the coordinates? In particular, all the perturbations in $\\gamma_c$ seem independent, preventing realistic situations where nodes are clustered.\n4. Can you elaborate on the claim that the instances described in the experiments (Data generation section) would never be seen during training? Besides the fact that the distribution (of node coordinates) is continuous and so it is almost impossible to sample the same points, is there a substential difference between the form of the attack (Eq. 8 and below) and the experimental data generation process? \n5. Regarding the experiments\n    * What are the distribution sets used for training the models LIH(FS) and LIH(FT)? In particular, on which sizes were they trained? \n    * What are the computation times of the different methods and in particular the proposed one for the TSPlib experiments?\n    * In Sec 5.1.Data generation, a validation data is mentioned, but then what is the test data? (On which the results are reported)\n\n\n**Additional feedback**\n\nMissing definitions, mathematical typos and suggestions:\n* The proposed idea seems related a recent paper about asymmetric self-play in robotics e.g. [1] “Alice and Bob play a game. Alice is asked to propose challenging goals and Bob aims to solve them. We show that this method can discover highly diverse and complex goals without any human priors.” Could be inserting to discuss.\n* Sec 2: paragraph Meta-Game, “NE” was not defined at that point\n* Sec 3: \n    * definition of Best Response, in the equation, the $\\forall i$ should not be removed\n    * Definition of Instance: It seems an instance is defined as “given n points finding the shortest tour”, whereas an instance of the TSP is just the n points.\n* Sec 3 (and throughout the paper): Optimal gap —> I think you mean optimality gap\n* Sec 4: \n    * Eq 4 $\\Delta$ has not been defined before\n    * It seems that a minus is missing in $U^{\\Pi_{DG}} = -  U^{\\Pi_{SS}} = **-** G(\\pi, Oracle)$\n* Sec 4.2 (and throughout the paper): I found it confusing that “oracle” was used for both the exact solver that allows to define the optimality gap and to describe the strategy of the players.  \n* Table 1: It would be helpful to recall what the T parameter represents in the LIH method\n* Table 2: It would be more readable (and informative) to report optimality gaps (as in Table 1) instead of tour lengths \n* There are a number of typos (at least a couple per page) that I think would be easily corrected by a thorough proof-read.\n\n[1] OpenAI et al, Asymmetric self-play for automatic goal discovery in robotic manipulation. 2021\n\n",
            "summary_of_the_review": "The paper addresses the important problem (lack of generalization of learning based TSP solvers) by adapting an existing framework (PSRO) to a new context (learning of a TSP solver). The experimental validation is limited: it shows that applying this idea improves the generalization of one base model for solving the TSP. The choices of the context-specific ingredients of the algorithm lack motivation  and it is not clear to me how they are designed to promote generalization, in particular to larger graphs (see Weaknesses). Finally I found that the missing definitions and imprecisions made the paper harder to understand, in particular to properly assess the results. For this reasons I vote for reject.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents a game-theoretic formulation to the problem of learning TSP solvers that can generalize. In this formulation, a solver selector plays against a data generator. The former tries to select the solver that best solves a TSP instance and the latter chooses the worse instance distribution for the solver.",
            "main_review": "A game-theoretic formulation is natural when dealing with generalization. \nPrevious work that deals with generalization considers the training on small instances and generalizing to larger instances under a uniform distribution over instances. Instead, this works explicitly considers different distributions between training and testing. \nAlthough I find the proposition interesting, I have some doubts about the approach:\n- The method seems not to scale very well to large instances. However, I think it is on larger instances that a learning-based method is really needed. On small instances, the computation time for exact methods is reasonable. Actually, as shown in Table 1, they are faster that the proposed approach!\n- I believe AM is not a state-of-the-art method anymore, there are more efficient methods that have been proposed recently. Some of them are cited in the Related Work section. I think the empirical evaluation should include more recent methods and notably those that focus on generalization.\n- The empirical evaluation in Table 1 implicitly favors the proposed method, since the distributions are exactly those considered by the data generator.",
            "summary_of_the_review": "I have some doubts about the scalability of the approach. It seems to work on small TSP (less than about 100s of cities), however on those instances, exact methods are even faster than the proposed approach. Besides, the empirical evaluation is missing stronger baselines.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper aimed at improving the generalization ability of deep learning-based solvers for the Traveling Salesman Problem (TSP). They built a two-player meta-game between a trainable solver and a task generator in a generative adversarial manner, where the solver solved instances provided by the generator, and the generator kept generating increasingly difficult instances for the solver. \n\nTo achieve that, they grounded in the Policy Space Response Oracle (PSRO) framework for two players. For the solver, the framework can obtain a behaviorally diverse population of powerful solvers over which they utilized a model mixing method to combine these solvers and achieve strong generalization ability on various tasks. For the adversarial instance generator, they added learnable perturbations on top of uniformly generated data to obtain a distribution where the current solver performs poorly.\n\nThe experiments supported their statement well. They attained an improvement over the baseline model on both synthetic and realistic instances. This work achieved state-of-the-art results on a general TSP instance generation method over which the performance of other deep learning-based methods degenerates vastly. \n\nThe main contributions are as follows:\n\n-- There were several works attacking competitive TSP using game-theoretic methods. But they are the first to study the generalization ability of the given deep learning-based solvers from the metagame perspective. And it showed good potential in improving generalizability.\n\n-- For the instance generator, the ’attacking’ manner, which adds learnable perturbations on uniform data to exploit the distributions, can both improve and evaluate the given solvers. For the solver selector, they introduced a ‘mixing-model’ method by combining a population of solvers to better utilize the obtained solvers. Under a fair comparison metric, their method attained state-of-the-art results on both synthetic and realistic instances. Besides, with the increase of the population of solvers, the metagame showed a trend of gradually approaching the Nash Equilibrium, which suggested the rationality behind their framework.\n",
            "main_review": "Strength:\n\nThe deep learning solver can be sensitive to the problem scale and city distribution. It is novel to apply ideas from game theory to improve the generalizability of deep learning-based TSP solvers. And more importantly, this idea is intuitively reasonable because the sensitivity of each deep learning-based solver cannot be easily eliminated, and one good way to address this problem is to learn a proper solver selector which can increase the generalizability by better utilizing multiple solvers. The generative-adversarial mega game formulation can provide a self-supervised way to improve the performance of such a solver selector. Those two not-well-explored and reasonable ideas should lead to meaningful findings.\n\nThe experiments are well designed. The discussion in the appendix makes the model selection more convincing.\n\nWeakness:\n\nThe framework assumes both the solver and the problem instance generator can be adjusted with parameter tuning through back-propagation. Nevertheless, there are quite a few solvers and problem instance generators whose processes cannot be adjusted using gradients. Could the authors think about possible ways to apply their ideas in such scenarios?\n\nThe TSP performance can be influenced a lot even by adding small perturbation and it is important in realistic situations. In this work, the author only added small gaussian perturbation which may be a constraint in realistic applications.\n",
            "summary_of_the_review": "This paper aimed at improving the generalization ability of deep learning-based solvers for the Traveling Salesman Problem (TSP). They built a two-player meta-game between a trainable solver and a task generator in a generative adversarial manner, where the solver solved instances provided by the generator, and the generator kept generating increasingly difficult instances for the solver. \n\nThe idea of applying game-theoretic reasoning is novel. This work achieved its goal by reaching state-of-the-art results on both synthetic and realistic instances under a fair comparison metric. We encourage the authors to consider cases where the gradients are not available for problem instance generators or the TSP solvers. Also, it may be interesting to consider if their approaches can be applied in combinatorial problems beyond TSP.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}