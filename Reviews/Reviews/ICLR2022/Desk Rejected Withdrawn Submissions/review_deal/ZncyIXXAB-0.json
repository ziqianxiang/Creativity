{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors propose a generative model for time-series data sampled at arbitrary points. The model is an autoencoder plus a flow-based model on the hidden codes, trained with three losses: the autoencoder loss, a GAN loss, and max-likelihood of the flow model on the hidden codes. Each of the components is a neural ODE capable of handling continuous-time inputs and outputs. The authors perform experiments on several datasets showing that their method outperforms many baselines.",
            "main_review": "Correctness: I didn't find any technical mistakes in my reading of the paper. My main criticism of this paper is that the method presented is quite complicated. The authors claim that all of this complexity is necessary to solve the task, but this claim isn't empirically supported in the paper. In particular, it seems like either the flow-based model (trained either by max-likelihood or as a GAN) or the autoencoder alone (with a prior over its latents, i.e. a VAE) could be valid generative models for this kind of data. This paper would be stronger if it included an ablation study comparing the proposed IIT-GAN to a plain VAE and a plain flow-based model. I acknowledge that the authors do compare to quite a number of baselines already, but many of these are too different from the proposed model to function as ablations.\n\nSignificance: The problem of continuous time-series generation is important, and the authors' experiments demonstrate that their method works on small-scale tasks. Given the machinery required, I'd like to see evaluations on more difficult / larger-scale tasks (speech synthesis with variable sample rates?).\n\nNovelty: The proposed method combines building blocks from prior work (NCDEs, NODEs, invertible generative models, GANs) in an arrangement which also follows from prior work (e.g. there's a line of work on VAE-GAN hybrids, and a line of work on flow models as priors over autoencoder latents). In that sense this work doesn't contain many completely new technical ideas. It does, however, leverage those prior works to propose a sensible-seeming method for a useful task. \n\nClarity: The method and experiments are presented clearly enough that I didn't have much trouble following them.",
            "summary_of_the_review": "Overall I feel the paper is well-presented and sensible, but the complexity of the method isn't justified (e.g. through ablation experiments) and the experiments are limited to small-scale problems. I don't recommend acceptance in the current state.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a method named IIT-GAN for synthesizing irregular and intermitteant time series data. Different deep learning models such as GANs, AEs (Autoencoders), NODEs (Neural Ordinary Differential Equations), NCDEs (Neural Controlled Differential Equations) are combined to achieve the task. One of the key contributions in this work is that an invertible generator (Continuous Time Flow Process) is used so that input and output sizes are the same and hence the exact maximum likelihood training by change of variable can be performed. The model is evaluated on 4 datasets and 7 baselines for both tasks of regular and irregular time-series synthesis. A quantiative evaluation (a predictive score for next step synthesis, a discriminative score for classification) and qualitative evaluation by visualizing the synthesized and original data based on a t-SNE projection and a kernel density estimation. ",
            "main_review": "Pros: \n- The proposed model solves an important problem that has several practical applications.\n- The proposed performance has good performance for highly irregular time-series.\n- Extensive experiments have been conducted to validate the proposed models.\n- The provided implementation is well organized and allows reproducing some selected results.\n\nCons:\n- The use of various components (AE, GAN, NODE, NCDE) might potentially lead to an over-parameterization and over-fitting. Model complexity vs. SOTA could help to assess this.\n- There are too many hyperparameters (11 listed in Appendix Section G) to tune.\n\n",
            "summary_of_the_review": "The paper tackles an important problem and provide an extensive experimental evaluation which shows the potential of the proposed method. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper addresses the problem of generating (synthetic) irregular and intermittent time-series where values can be missing. The generative model proposed to synthesize this type of data comprises the following components:\n\n- Auto-encoder path: this takes in an irregular time-series sample and recovers a continuous path (NCDE).\n- Adversarial path: A GAN-style module follows which takes as an input a continuous sample path from a Wiener process to generate a fake time-series sample. This is the generator that's being trained to synthesize the time-series data. A key idea here is that the mapping at this step is invertible (CTFP).\n- Log-density path: The invertibility of the generator allows calculation of the exact log likelihood of the sampled paths.\n\nThe encoder-decoder architecture is pre-trained via the mean squared reconstruction loss. Then, adversarial training is used to train the generator and the discriminator. A training cycle that involves the generator's MLE loss is also considered, but the author's state that this incurs mode collapse.",
            "main_review": "This paper proposes a generative model for irregular time series data that combines various different generative modeling approaches that are normally considered to be drastically different in terms of the way they conceptualize the density estimation problem and in terms of the training procedures they enable. The generative modeling approaches that the proposed method encapsulates are: auto-encoders (though not used to generate samples probabilistically as in VAEs), normalizing flows and generative adversarial networks, in addition to other architectural components such as Neural ODEs. \n\nUnfortunately, there are key foundational weaknesses in the paper, in terms of its logic and what it proposes as a contribution, that prevents me from providing a positive overall score. In what follows, I list my concerns in a decreasing order of importance.\n\n1- The paper combines virtually all of the different approaches for generative modeling into one model (GANs, flows with invertible mapping, auto-encoding of a latent path). Regrettably, the authors did not do a good job justifying these modeling choices: why do we need to combine all of these models and how they actually work together in a meaningful way? Given the fact that GANs and normalizing flows are radically different (the former is an implicit likelihood model, whereas the latter is an explicit likelihood model), it is unclear why do we need adversarial training and a generator-discriminator architecture at all if we already have an exact log density expression? why not just train a normalizing flow with the generator map as the flow map?\n\nIf the idea is to use maximum likelihood to improve training of GANs by using a MLE objective, then this idea is not particularly now. The authors failed to cite the original paper that proposes this idea: Aditya Grover, Manik Dhar, Stefano Ermon, \"Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in Generative Models\", 2018. Even then, it is unclear if improving GAN training with MLE is particularly relevant to the irregular time-series setup or it's a general approach that can improve GANs for any data type.\n\n2- The authors state in the manuscript that synthesizing irregular and intermittent time-series where values can be missing and \"may not have specific frequencies\" is \"far more challenging than existing settings\". This is the main motivation of the proposed model. However, it is unclear to me why this is such a challenging problem. There are so many straightforward ways in which one can model irregular time series using existing methods without having to model a continuous path, e.g., by modeling intervals between observations as a random variable on top of any of the existing models such as TimeGAN. Another straightforward approach is to use any of the continuous-time variants of RNNs, such as phased LSTMs, Neural point processes, or Neural ODEs and use this as the underlying encoder-decoder architecture of TimeGAN. The irregularity of time series seem to require slightly different architectural choices but not much conceptual differences that would demand technical novelty.\n\nOverall, I think the proposed model combines many existing ideas but does not propose new ones. The combination of methods is done in a coherent or properly justified and well-motivated way. I think the authors can improve the paper by precisely explaining the challenging aspects of modeling irregular time-series data, and specifying the novel conceptual contributions they introduce to address these challenges.\n\n3- The writing language in the paper is imprecise. In the abstract, the authors mention that their focus is on\"synthesizing irregular and intermittent time-series where values can be missing and may not have specific frequencies\", but it is unclear what is meant by a time-series not having \"specific frequencies\"; we know that any time domain signal can be represented in the frequency domain by a specific spectrum of frequencies. In the introduction, the authors mention that they focus on \"signals that do not have predetermined periodicity (frequencies)\", but periodicity and frequency components are not the same thing (only discrete spectrum corresponds to time-domain periodicity). In the first paragraph of the introduction, the authors mention that previous methods address \"time-series data does not have any missing values\" ---yet they use TimeGAN in the missing data setup in the experiments section (see next comments)---and \"time-series data consists of various signals with different frequencies\", which is unclear what it means precisely. There are also various subjective and rather odd statements that do not come with precise technical justification, such as \"Our problem setting is one of the most difficult time-series synthesis problems.\" and \"Our proposed method, IIT-GAN, is based on various advanced deep learning technologies, ranging from GANs to NOCDEs, NCDEs, and so forth.\"\n\n4- The metrics used in the experiments section do not necessarily reflect the quality of a synthetic sample. The \"predictive score\" depends on the predictive task, and good scores may be achieved with a poor sample that models P(Y|X) accurately but models P(X) poorly. The \"discriminative score\" is based on an RNN model, and does not reflect any ground truth, i.e. one can use a poorly trained RNN to assess discrimination score and come up with an ostensibly good discrimination score for all synthetic samples even if these synthetic samples are mostly noise. \n\nWhile the authors imply that their model is unique in its ability for handling irregular and missing data in time-series, they still use existing models as baselines in the missing data setup. It would be great if the authors can better explain how baseline handled missing data, and how their hyper-parameters were selected. Since IIT-GAN outperforms other models for regular time-series as well, it will be great if the authors better disentangle the sources of gain in this setup under comparable architectures with TimeGAN encoders and decoders. \n\n\n\n",
            "summary_of_the_review": "Overall, the paper does not precisely explain the technical challenges associated with the problem of synthesizing irregular time-series data, and the specific contributions that the authors propose to address these challenges. The proposed model combines many existing ideas that belong in completely different modeling categories without proper justification or motivation of why such an amalgamation is needed, which makes the model appear to lack coherence.  ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The authors outline a method of time series generation that naturally handles irregular time series (time series with missing values).\n\nThe method employs an auto-encoder architecture with a Neural CDE encoder and a GRU-ODE decoder (both models for irregular time series that can work in continuous time) resulting in a continuous output path that can be compared pointwise against the discrete (irregular) input data and trained via standard MSE. The encoded path is the hidden state of the Neural CDE that is decoded by the GRU-ODE. \n\nThe adversarial component generates fake hidden representations that correspond to the encoded hidden dimension from the Neural CDE. The GRU-ODE decoder then recovers a continuous path from these representations and a discriminator is trained to discriminate this from the real data. \n\nThis is all backed up by experiment with the IIT-GAN showing unsurprisingly convincing performance improvements over a wide range of benchmarks. \n",
            "main_review": "Overall this is a good paper. Whilst there are a lot of moving parts, the method is relatively straightforward and is an intuitive and natural approach to take for irregular time series generation. The authors do a good job of disentangling the different components and the paper is highly readable with helpful graphics. \n\n## Cons\n- All the irregular time series considered are artificially constructed, in that the authors take regular time series and drop data at random to different proportions. I think this is a shame as this doesn't adequately represent the irregularities in most real-world irregular datasets. Whilst I have no doubt the IIT-GAN performance would persist, it would have been good to include a real irregular dataset (MIMIC being the obvious example). \n- NCDEs are written as having X(t_i) = x_i. This is a mistake and should be (t_i, x_i). If only (x_i) is used the X_t_i+1 - X_t_i = x_i+1 - x_i and no information about time is passed to the model. Instead we want (t_i+1 - t_i, x_i+1 - x_i).\n- Could you please point out where \"In general, however, SDEs are known to be restrictive in representing complicated temporal dynamics (Kidger et al., 2021).\" is stated by Kidger et al in said paper? I believe this is misquoting them and should be removed.\n- This is a large model but little talk is given to the parameter counts, length of training, drawbacks with memory usage and so on. \n- Why is a Neural CDE used as the encoder and a GRU-ODE as the decoder? Seems they do much the same job and it would be a bit neater to use the same model at either ends of the autoencoder (not that it matters), but no mention is given to this.\n- Could be interesting to have a bit more background into the areas where irregular time series generation is important (e.g. medical data anonymisation), however very minor con and the paper is fine without it. \n\n### Typo\n- i-th real of fake hidden vector -> or fake hidden vector (after eq 5). \n- Weird dot at the left of tables 7, 8, 9, 10. Maybe thats supposed to be there?",
            "summary_of_the_review": "A well written, well motivated problem in an area of real importance that should only become more important as time progresses (irregular time series generation). The method is intuitive to understand and should not pose too much difficulty to any practitioner wishing to apply it to their own datasets. If some of my issues mentioned above are addressed, I fully recommend this for acceptance. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}