{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper aims at exploiting data with only single class labels and GANs to obtain multi-label generated samples. The methodology consists  in a sampling strategy applied at inference time to any pre-trained GAN (both unconditional and class conditional). The method is based on Metropolis-Hastings (MH) independence sampling, where the acceptance probability is estimated given density ratio estimations. These density ratio estimations are computed using discriminators trained on labeled samples. \nThe proposed method (S2M) is tested in three toy datasets: two-class gaussians, MNIST and FMNIST, and in subsets of CIFAR and CelebA, where S2M outperforms the baselines. ",
            "main_review": "\n\n[Strengths]\n- Given that multi-labeled datasets require high annotation costs, the motivation of this work is to not rely on multi-labeled datasets but rather use single labeled datasets to train GANs, and later obtain multi-label samples at test time. The motivation is clear and aims to solve a reasonable problem. \n- The paper is well written and organized. \n- S2M builds upon an existing sampling method, changing the formulation of the acceptance probability, and applies it to a different problem, providing an interesting contribution with moderate novelty.\n\n[Weaknesses]\n- It seems that an important baseline would be HG-GAN, where instead of using the proposed equation to compute acceptance probability, the original equation in the paper is used (with the same MC iterations). On these lines, I have a question regarding the baseline “GAN” results that appear in Tables 1 and 2: How are the multi-label class conditionings used with unconditional GANs and no sampling nor filtering method? \n- I would have liked to see experimental results in less “toyish” datasets overall, with higher resolution images and more classes. The generated images are very small and the task seems too easy with the given setup. It would strengthen the message of the paper to try to go beyond that, perhaps generating 128x128 resolution images and using more than 2 or 3 $class_{single}$ groups and about 10 $class_{orig}$. Given that the GANs are already trained, it would be less computationally expensive to do these experiments than re-training the GANs on each of those larger scale datasets.\n- The paper does not mention at any point how much time it takes to obtain the final samples. Could the authors please discuss that?\n\n",
            "summary_of_the_review": "The paper provides an interesting contribution for the community and the ideas are well exposed. Despite the results shown in the paper, it falls short when it comes to the experimental evaluation. It seems that there is a relevant baseline missing and I would recommend to use less toyish datasets and move towards more challenging setups with more single label classes. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work considered a problem: how to generate multi-label images under a single positive setting, where each image is labeled with only one positive label. To this end, this work introduces a single-to-multi label (S2M) sampling method. This sampling technique can be applied to multiple GANs, and the experimental results show that the generated images under the single positive setting are promising.",
            "main_review": "[+] The single positive setting in class-conditional generation is interesting.\n[+] Visualizations of Fig.5 and Fig. 6(2) are encouraging. \n[+] The proposed technique can be applied to GANs, cGANs, and CPGAN.\n\n\n[-] [The method is not well explained] The paragraphs of \"S2M Sampling for Unconditional GANs\" and \"S2M Sampling for Conditional GANs\" are difficult to read. Some crucial parts are missing, which makes the proposed sampling difficult to understand. For example, the definition of \"proposal distribution\" is not clear, and the chain process of the Metropolis-Hastings method is not introduced.\n\n[-] [Contribution needs to be explained] After reading Section 2, the contribution is using three classifiers Dv, Dr and Df to estimate the density ratio. Then, applying the existing Metropolis-Hastings (MH) method for sampling. The contribution seems marginally significant. It will be very helpful if this work could provide discussion/explanation to clarify and highlight the contribution.\n\n[-] [More experiment results]\na) The goal of this work is to generate/draw multi-label data. Figure 6 (2) shows some of the generated multi-label images (for example, B+M+S). What about the multi-label data generated under CIFAR-10 settings? For example, airplane + bird. Such an example might not reasonable but helpful to understand the proposed multi-label generation (which are shown in Fig.5 of CPGAN).\nb) In CelebA, there are three attributes/classes. What are the results when more attributes/classes are provided? This may cause learning difficulties, and the authors are encouraged to discuss the results with more attributes.\n\n[-] The result in the semi-supervised setting is comparable to the fully-labeled setting. However, as mentioned in Sec.5.2, 10% of the labels may be enough to train the classifier. It is recommended to show the number of images under the 10/20/50/full label setting. In addition, it seems that unlabeled images are not used in training, so the semi-supervised setting may be inaccurate.\n",
            "summary_of_the_review": "It is interesting to generate multi-label images under the setting of the single positive setting. The experiment shows promising results. However, the method part is not well explained. Some discussions are needed to highlight contributions. In addition, some experimental results can be included to understand the advantages of the proposed method.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work introduces the S2M Sampling for generating multi-attribute data with the single positive setting, where a single positive label only annotates each training sample. The proposed sampling method is based on the Markov chain Monte Carlo. It can be inserted into existing GAN-based methods as a post-processing method to improve sample quality.",
            "main_review": "+ The proposed sampling method for GANs seems useful for unsupervised and semi-supervised training. It is desirable to use partially labeled data for class-conditional generation tasks.\n\n+ The experimental results show that the proposed sampling method improves ACC, FID, and IS. The proposed method can be inserted into existing GANs and facilitate the training of these models.\n\n+ The claims are well-supported. The proposed sampling methods make the training of GANs more efficient in theory, and the experimental results also verify the effectiveness of the proposed method.\n \n- Considering that the classifiers (discriminators) are not optimal in practice, how to adjust the estimated density is very important and should be the core part of this work. I suggest the authors provide more in-depth analyses and put the related appendixes to the main text.\n- In Section 3.2, Theorem 1 is introduced for providing background knowledge. I suggest the authors move this part to the Related Work section or appendix. \n- Time complexity of the proposed method is missing. In addition, the baseline models (GANs, cGANs, and CPGAN) are a bit out-of-date. Comparing with the SOTA models such as StyleGAN will make this work more convincing.\n",
            "summary_of_the_review": "The main idea of this work is interesting, but more analyses and experimental details are needed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a novel idea to sample from GANs. The single 2 multi label (S2M) method proposed in this paper is an \"add-on\" method which enables high quality multi-label data sampling from GANs and cGANs. ",
            "main_review": "This paper works on an important aspect of sampling from the data distribution learnt by the generative models. The claim is that while previous methods target the sample quality, this method focuses on generating samples from multi-label data with high quality. \n\nThe main theorem in the paper leads to Eq. 5 which states that the target density can be derived from the class-conditional densities of single positive labels. Though the claims are valid for training the generators as well, the authors have primarily used it to draw multi-label samples during the inference phase. \n\nThis idea of decoupling the single class densities to provide an estimate for the multi-class target distribution can be extremely useful for real world applications. \n\nThe experiments though could have been much better. The small set of classes selected from the CelebA and CIFAR-10 datasets gives an extremely myopic view of the true potential of the work. I would request the authors to provide some examples when the sets I and J are not chosen by design, but picked up in some other application specific way. The experiments are all pointing towards this limitation where some arbitrary association of the classes needs to be decided first, odd-even in MNIST, odd-even in FMNIST as well though it does not make sense and then some very specific hair and smile classes in CelebA.\n\nAnother question I have about the results is why are the smallest datasets for GAN results being used for experiments. Since the method is only used inference time, some larger resolution generators should have been used for sampling.\n\nOverall I believe that the paper has novel material which can have wide application as well. I would like to see better results since the bottleneck of training has already been bypassed by this method.",
            "summary_of_the_review": "I feel that this paper unlocks some important aspects of the generative models wherein multi-label sample generation is enabled in the inference time. Moreover, since the training part is completely by-passed hence this model can be integrated with any of the SOTA GAN architectures to generate high quality samples.\n\nUnfortunately, the results presented in this paper have not explored the full potential of this model by staying with very small datasets and not venturing out to the truly high resolution generative models such as ProGAN and other similar models.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}