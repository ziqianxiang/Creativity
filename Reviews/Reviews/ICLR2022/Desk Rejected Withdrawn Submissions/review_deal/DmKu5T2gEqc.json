{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper addresses the issue of conditional short term video prediction. The authors to decompose the prediction in two parts, ie the motion prediction, followed by an apperance refinement, those two steps being achieved with LSTMs. Experiments are performed on two standard datasets (Human3.6M and UCF101), for 4 and 10 future frames prediction respectively. Quantitative  results are compared with related work.\n",
            "main_review": "Conditional video prediction has a wide range of applications and is therefore an important open problem to address. \n\nThe authors propose to decouple motion prediction and appearance refinement (Villegeas and al. 2017, Denton and al. 2017).  This is achieved by enforcing a constraint relative to the appearance changes (in the latent space), and a hierarchical appearance constraint (a L1 loss at different layers resolution) in the decoder.  This does not appear to be methodologically significantly different  from previous work (Villegeas and al. 2017, Denton and al. 2017, for the earliest ones.)\n\nFrom a result point of view, did the authors try to generate longer future videos? What would be the impact of the length of the conditioning on the results?\n\n[1] E. Denton, V. Birodkar, Unsupervised Learning of Disentangled Representations from Video, NIPS 2017\n[2] S. Oprea and al., A Review on Deep Learning Techniques for Video Prediction, https://arxiv.org/pdf/2004.05214.pdf\n",
            "summary_of_the_review": "The authors tackle an interesting problem. However, from a methodological point of view, the paper do not seem to bring significantly new idea nor insight. I would have like to read more discussions relative to the pros and cons of the methodological choices, more analytical comparisons  with the sota and relative work. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents a cascaded decoupling network, CDNet, for video prediction. Motion dynamics and apperance information are decoupled in the CDNet. Specifically, a motion LSTM is exploited to capture modtion dynmaics and a stack of refine LSTMs are used to recover apperance details. The proposed method is evaluated on Human 3.6M and UCF101 datasets and achieves promising performance. ",
            "main_review": "Strengths:\n(1) The proposed gloabl information integration mechanism, refine LSTM and motion changing area loss are reasonable and effective. As demonstrated in Table 2, these components contribute to performance improvement. \n(2) The proposed method achieves state-of-the-art peformance (Tables 1 and 3) and can generate future frames of higher visual quality (Figures 4 and 6).\n\nWeaknesses:\n(1) It is not well justified that decoupling of apperance and motion dynamics is essential for video prediction. The experiments in the paper does not well support this claim. The performance improvement of the proposed method may come from other components rather than the decoupling of appearacne and motion dynamics. For example, the global apperance information contributes largely to the perfromance improvement as shown in Table 2. Also, the motion changing area loss may just serve to guide the learning of the motion LSTM, not necessarily decoupling appearance and motion dynamics. Maybe, there are some other loss choices. \n\n(2) The processing speed of the proposed method is not reported. How fast is the proposed method, compared with other state-of-the-art methods?",
            "summary_of_the_review": "The proposed method achieves promising performance for video prediction. However, the motivation of decoupling motion dynamics and appearance for video prediction is well justified.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N.A.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a method for future frame prediction that decouples motion and appearance prediction. They use an LSTM architecture that models the dynamics in the video and another LSTM that iteratively generates the next frame by a refining step. In experiments, the authors show that the proposed method outperforms the chosen baselines.",
            "main_review": "Strengths:\n+ Novel architecture that decouples motion and appearance.\n+ Outperforms baselines.\n\n\nWeaknesses:\n- Soundness of the method:\nEquation 6 is supposed to choose between H^l and H^{l-1} by a convex combination. However, the range of B^l is not [0, 1]. If we look at equation 5, B^l is computed by a tanh(x) and multiplying it by the output of a sigmoid function which results in a value between -1 and 1. Substituting this in Equation 6 does not result in a convex combination. Therefore, resulting in an unexpected behavior from the model that does not go hand-and-hand with the claim for this module in the paper.\n\n\n- Claims of what different parts of the network are doing:\nThe authors assign different responsibilities for certain parts of the method such as M_{k-1} being spatiotemporal coherence and H_{k-1} as spatial information, but do not provide evidence of these claims. The authors provide an ablation study based on performance and show that the complete method performs better than when removing some parts of the method. However, I feel that it would be more important to show that, for example, removing M_{k} causes temporal incoherence. Or that if we remove some other piece within the model that we get the opposite effect of what is claimed. Showing concrete evidence of what is claimed that some features represent would be interesting to the community so that we can adopt such concepts in followup research.\n\n\n\n- Missing related work:\nThere are many important video prediction related works. Here are a few of them:\nhttps://arxiv.org/pdf/1804.01523.pdf\nhttps://arxiv.org/pdf/1710.11252.pdf\nhttps://arxiv.org/pdf/1802.07687.pdf\nhttps://arxiv.org/pdf/1911.01655.pdf\nhttps://arxiv.org/pdf/1705.10915.pdf\nhttps://arxiv.org/pdf/1806.04768.pdf\nhttps://arxiv.org/pdf/1605.08104.pdf\n\n\n\n- State of the art claims on Human 3.6M:\nThe authors claim state of the art results on the Human 3.6M dataset. However, there are works which look visually better than what's presented in this work which have not been cited or compared against. Here are a couple:\nhttps://arxiv.org/pdf/1911.01655.pdf\nhttps://arxiv.org/pdf/1806.04768.pdf\nI suggest the authors to compare and discuss these works in the next version of the manuscript.\n\n\n\n- Fig 6 results:\nThe video showcased in Fig6 does not seem to show any motion in the predictions. I looks more like the moving parts are gradually being blurred and the rest of the pixels being copied by the method rather than motion being generated. I suggest the authors provide an additional baseline based on copying the last observed frame through time to check whether the performance boost against the baselines comes from copying pixels instead of predicting motion.\n",
            "summary_of_the_review": "In the current state, I am leaning towards rejecting the paper. There are issues in the math which means experiments may need to be re-run. Also, there are many missing baselines and claims of state-of-the-art are not completely accurate. The generated videos from UCF101 seem static and the performance boost could simply be from copying pixels well + blurring. If I am missing something, I would appreciate the authors can point it out. I am willing to increase my score if the issues are addressed in the rebuttal.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors have proposed a novel architecture named CDNet for the task of Video Prediction. They disintegrate the task into motion generation and appearance generation. For the purpose of generating appearance from motion, they propose a novel refinement LSTM unit. They also propose area loss and multi-layer loss functions for decoupling motion and appearance. \nPerformance verified on two real-world SOA video prediction datasets.\n",
            "main_review": "Following are the strengths of the paper:\n\n-\tThe major contribution of the paper is the Refine LSTM cell described in section 3.2. which is novel in the way it incorporates global appearance information with the cell state C. \n\n-\tThe idea of generating video by decoupling motion and appearance is not new but LSTM (and its variants) based video architectures have not used for such tasks yet. Thus, the direction provided by this architecture design seems promising.\n\n-\tThe method achieves better than state-of-the-art methods on HMDB and UCF datasets which is shown using 5 metrics.\n\n\nFollowing are the weaknesses of the paper: \n-\tThe paper lacks on clarity. For instance, foreground and background of the video have been used several times in the paper but what exactly the author(s) mean by foreground/background is not mentioned. Foreground can be the moving object. But what happens when there is camera motion (that causes the background to move/change) as well, which is definitely more challenging? \n-\tThe proposed pixel loss is not novel because on taking a closer look we see that pixel loss is reconstruction/MSE loss applied at different levels of refine LSTM cell. The Area loss essentially forces the LSTM to learn motion dynamics. The idea is conceptually similar to the motion modelling as performed in \"Decomposing motion and content for natural video sequence prediction.\", Villegas, Ruben, et al, ICLR 2017. The motion encoder in this paper uses difference of adjacent frames.\n-\tHMDB is a dataset recorded in a very constrained setup. Ablation study on UCF101 (which is unconstrained) will give more confidence about the contribution of each component of the architecture/loss\n - Would be nice, if comparisons are made with any recent works which involved pixel-graph based CNNs or transformers for solving similar problems.\neg Learning to Decompose and Disentangle Representations for Video Prediction; Jun-Ting Hsieh\nhttps://arxiv.org/pdf/1806.04166.pdf\n\nGenerating the Future with Adversarial Transformers; Carl Vondrick and Antonio Torralba\nhttp://www.cs.columbia.edu/~vondrick/transformer.pdf\n\nhttps://arxiv.org/pdf/1903.00271.pdf\n\nand a review (?)\nhttps://arxiv.org/pdf/2004.05214.pdf\n\n\nSmall typo in:\nTable 4: Ablation \"stydy\" about super-parameters.\nCheck rest thoroughly.\n\n\n",
            "summary_of_the_review": "The architecture design along with the proposed Refine LSTM unit are novel for decoupling video into motion and appearance. Thus, the work is novel and seems useful.\n The work has been evaluated using 5 metrics on two datasets  and it outperforms state-of-the-art methods almost in all cases. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}