{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a hierarchical framework LPMARL to solve the multiagent coordination problem by decomposing the the overall task into a set of subtasks. A high-level agent is built to assign agents to the manually predefined sub-tasks by solving a matching problem using LP solver. Then lower-level agents are trained to solve the target sub-tasks. ",
            "main_review": "The paper is very similar to [1] with the following differences:\n  * The agent-task score matrix is constructed by utilizing a message-passing graph neural network (instead of the positional embedding model).\n  * The lower-level agents are also trained online along with the high-level task assignment agents (instead of assuming the lower-level policies are known and fixed).\n  * A differentiable LP optimization layer is built (instead of considering the LP optimization layer as meta envrionment) such that an end-to-end gradient backpropagation could be executed.\n\n\n**Weaknesses**\n  * The improvement is incremental compared with [1] and some key components are not well motivated and well explained.\n    * The motivation of why further building a differentiable LP layer instead of considering the LP optimization layer as meta envrionment is beneficial is not clearly explained.\n    * Some hierarchical training details are missing, e.g., how often does the high-level policy re-assign the tasks. Besides, when the lower-level policies are also trained online, the learning problem becomes unstable. Details of how to solve the instability in hierarchical learning is missing. \n  * When applying the method, the tasks and the corresponding constraints (e.g., how many agents are needed to solve each task) have to be mannually defined aforehand. \n  * For experiments:\n    * Since the proposed method is MAHRL, the comparison with typical hierarchical MARL algorithms are missing.\n    * This approach is fully centralized at test time (i.e. the task assignment controller views the global state and assigns agents at each step), while several of the baseline methods (MADDPG, MAAC, QMIX, SEAC, CMAE) are not.\n    * Each scenario compare with different baselines and what’s its purpose? \n    * For SMAC (dense), it has been illustrated QMIX could achieves 100% win-rates in the 3 maps used in the paper [2]. \n\n**Questions**\n  * A concern is the meaning of the learned agent-task score matrix, i.e., the state-dependent cost coefficient matrix C. Since C are latent variables and updated by back-propagation, the meaning of C is not explicit, thus it is hard to determine whether the solution of the matching problem optmized by LP is doing a optimization of global perspective.\n  * Accordingly, why do we need to independently build and learn an another agent-task score matrix? Can we directly use the Q-values of the lower-level agents (i.e., the estimated long-term reward of assigning any agent i to any task j) as the agent-task score? By this means, the solution of the upper matching problem will be explicable. Besides, the upper-level agent will have no parameters and the overall training may become easier?\n\n```\n [1] Carion N, Usunier N, Synnaeve G, et al. A structured prediction approach for generalization in cooperative multi-agent reinforcement learning[J]. Advances in neural information processing systems, 2019, 32: 8130-8140. \n [2] Hu J, Jiang S, Harding S A, et al. RIIT: Rethinking the Importance of Implementation Tricks in Multi-Agent Reinforcement Learning[J]. arXiv preprint arXiv:2102.03479, 2021.\n```",
            "summary_of_the_review": "The contribution of the paper is incremental. The motivations of some technical designs, e.g., why further building a differentiable LP layer is beneficial, are not clearly explained. The details of some key modules, e.g.,  how to solve the instability in hierarchical learning, are also not well explained.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a new hierarchical MARL framework, which contains high-level learning using linear programming to decompose the task, a low-level to learn the policy to accomplish the sub-task. The high level uses a graph neural network to learn the relationships between agents and the task, models the task assignment problem as a resource allocation problem, solves it using LP. ",
            "main_review": "The motivation claims previous hierarchical MARL and intrinsic motivation methods do not address sparse reward problems well. However, from the experiments, this does not support well. Scenarios in fig2 (a) and (b) are not a standard sparse reward setting.\n\nOne contribution of interpretability is not supported well by experiments.\n\nFor experiments, why not compare with hierarchical MARL algorithms such as [1-3].\n\nWhy does each scenario compare with different baselines and what’s its purpose?\n\nFor starcraft II, what maps are used is not clearly mentioned. Although the appendix has the description, this should be clarified in the main text. Another point is, in sparse reward settings, some previous algorithms, such as QMIX, could achieve better results by fine-tuning, this should be considered, not just copying the parameters of previous methods of dense reward settings.\n\nThere are some typos throughout the paper, such as in section 4.2, “Formally, we consider the optimal solution of LP as a function the input coefficients” -> “…as a function of the input coefficient”?\nIn section 4.2 “agent i select the low-level action” -> selects\nI recommend authors repolish the paper carefully to clarify these.\n\n[1] Hierarchical deep multiagent reinforcement learning with temporal abstraction\n[2] Feudal multi-agent hierarchies for cooperative reinforcement learning\n[3] Hierarchical cooperative multi-agent reinforcement learning with skill discovery\n",
            "summary_of_the_review": "Based on the above assessment, I recommend a borderline reject currently. I suggest the authors clarify the purposes of selected baselines, provide descriptions to support the motivation, details of experiments, analysis of interpretability, and ablations to strengthen the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper considers techniques in hierarchical multi-agent RL for allocating agents to sub-tasks via an implicit LP-based optimization layer. The authors investigate the efficacy of their solution on a number of simple cooperative multi-agent environments.\n",
            "main_review": "While the general approach the authors take seems sensible, I have three major concerns with the current iteration of the paper: 1) the clarity of the paper should be improved significantly, 2) the paper makes it difficult to distinguish whether the successes of the method comes from the introduction of the hand-defined tasks rather than the algorithm itself, and 3) no comparison is done to an explicit task assignment protocol (e.g. not implicit). While the idea is interesting, I think the paper could benefit from another round of feedback before acceptance.\n\n- I found the paper somewhat difficult to read, because of some issues in how notation was brought up. For example, the background introduces a temporal extension of the MARL problem with agent-specific \"goals\" $a \\in \\mathcal{A}^{i,h}$, but in the rest of the paper, these are referred to as \"tasks\", and assumed to be shared across agents? (that is $\\forall i,j~~\\mathcal{A}^{i,h} = \\mathcal{A}^{j, h}$). Also, in Section 4.1, the authors say that the global state $s$ is represented as a directed graph $(\\mathcal{V}, \\mathcal{E})$, but it wasn't made obvious whether or not this graph is learned (e.g. the output of some representation network) or hand-crafted (my current understanding is that it's hand-crafted). \n\n- The experiments section leaves unclear whether or not the compared baselines (e.g. MADDPG, MAAC) receive the temporal abstractions that LP-MARL does, and if so, how they are incorporated. I see the line \"To compare algorithms fairly, ...\", but it is unclear to me how the temporal abstractions are factored into the other algorithms. My concern is that if this is not done correctly, then this is conflating the usefulness of hierarchy with the benefit conferred by LP-MARL is receiving additional external knowledge in the form of human-designed task information. I would like to see the details for how this is done clarified.\n\n- I don't think the experiments section do a good job of evaluating the usefulness of the LP-based implicit task assignment over explicit task assignments (a central part of the proposed method). The relevant ablation of course is to compare against a high-level policy that directly outputs task weightings $(z_{ij})\\_{i \\in [N], j \\in [M]}$ instead of the intermediary $(c_{ij})\\_{i \\in [N], j \\in [M]}$ that is used to generate $z$. As the methods stand right now, it is unclear whether the benefit comes from the hierarchy, or if the implicit inductive bias actually benefits the learning process. \n\n- What would need to be done to scale the LP-based task assignment to the setting where the number of sub-tasks is large / infinite (e.g. goal-reaching sub-routines)? While I can imagine that directly parameterizing a distribution over sub-tasks (e.g. directly outputting $z\\_{ij}$) may be simple, it is unclear to me how this would translate when outputting $c_{ij}$ and implicitly solving for $z_{ij}$.\n\n- I would be curious to hear the authors thoughts as to the merits / cons of this implicit task assignment in the setting where the tasks are not hand-provided, but rather learned (e.g. as part of the high-level policy).\n",
            "summary_of_the_review": "(Copied from above) While the general approach the authors take seems sensible, I have three major concerns with the current iteration of the paper: 1) the clarity of the paper should be improved significantly, 2) the paper makes it difficult to distinguish whether the successes of the method comes from the introduction of the hand-defined tasks rather than the algorithm itself, and 3) no comparison is done to an explicit task assignment protocol (e.g. not implicit). While the idea is interesting, I think the paper could benefit from another round of feedback before acceptance.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors address the MARL problem by decomposing the controller into a hierarchical one where a high-level controller assigns agents to subtasks such that high-level rewards are maximized and the low-level controller selects actions that maximize rewards on the assigned subtasks.",
            "main_review": "## Strengths\n* The idea of leveraging techniques for differentiation of black box combinatorial solvers for MARL is unique and interesting.\n* Figure 1 is very well made and informative.\n\n## Weaknesses\n* This approach is fully centralized at test time (i.e. the task assignment controller views the global state and assigns agents at each step), while several of the baseline methods (MADDPG, MAAC, QMIX, SEAC, CMAE) are not. The only fair comparison in this paper from this perspective is CommNet which also allows for centralization at test time.\n* The motivation of using task assignment to address the sparse reward problem is somewhat unclear. This is especially the case when we consider that, in practice, the proposed method requires subtask specific rewards to be specified, which would be similar to providing a dense reward signal that includes rewards for reaching sub-goals.\n* The paper lacks significant ablation studies, and it is difficult, as a result, to draw any conclusions about which part of the method is providing the improved performance. For example, one could remove the combinatorial solver from the high-level controller and simply set $\\pi^h_i(a^h_i = j \\vert s) = c_{ij}$. One could still maintain the constraints in Equations 6 and 7 by applying a softmax. This would be a far simpler approach and could feasibly perform as well as the more complicated proposed one.\n\n## Minor points/typos\n* Section 1, Paragraph 2, line 2: \"spare\" -> \"sparse\"\n* Section 6.1, Paragraph 2, line 1: Wrong citation for MAAC",
            "summary_of_the_review": "The general idea of integrating learned task allocation into MARL is an interesting one; however, this paper is lacking in sufficient experimental validation (unfair comparisons to decentralized methods and no ablation studies).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The authors propose a hierarchical method LPMARL to solve the sparse reward problem in MARL. The high-level policy generates latent variables as a cost coefficient matrix and solves the LP problem to obtain the task assignment. The high-level policy maximizes the sparse global reward. The low-level policy maximizes the low-level reward for reaching sub-goal. ",
            "main_review": "The paper is well organized and the motivation is really clear. The sparse reward problem is quite a challenge in MARL but has not been fully studied. Formulating the task assignment problem into a resource assignment problem by introducing linear programming is interesting and increases the interpretability. And adopting the piecewise linear surrogate loss makes the high-level easy to be optimized.\n\nMy main concern is the meaning of the cost coefficient matrix. Since C are latent variables and updated by back-propagation, the meaning of C is not explicit, thus it is hard to determine whether the solution of LP could represent the task assignment. Moreover, the attention mechanism could also output an implicit solution matrix, could you theoretically or experimentally compare the LP module and the attention module?\n\nThe low-level policy in LPMARL receives the reward for reaching sub-goal, for example, the reward of killing one enemy in SMAC. However, the low-level reward is not too sparse. Does the performance gain of LPMARL come from the additional information of low-level reward? If given the sum of low-level reward as the global reward, will QMIX solve the sparse-reward tasks?\n\nI think 2m_vs_1z cannot demonstrate the effectiveness of your method, since there is only one task in the environment and GNN degenerates into a fully connected network when there are only two agents. \n\nI noticed that the tables show the mean and standard deviation of 1000 repeated experiments. Do you perform 1000 runs with different random seeds? It sounds amazing.\n\nMAVEN[1] is an important multi-agent exploration method, which is missed in related work.\n\n[1]https://arxiv.org/abs/1910.07483",
            "summary_of_the_review": "Due to the above concerns, I give a rate of 5. If the concerns are addressed, I will increase my score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}