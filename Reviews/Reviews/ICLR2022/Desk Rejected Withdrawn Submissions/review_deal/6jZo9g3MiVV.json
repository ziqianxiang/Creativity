{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The work proposes to combine data augmentation in the input space and the effect of quantization on network towards self-supervised learning. Specifically, the work employs consistency between different augmented views of the same input where augmentation of the input images are twofold – one coming from input data transformation and the other is augmentation as a result of network quantization. Experiments on two benchmark tasks on different datasets are performed towards experimental validation of the idea.",
            "main_review": "Strengths:\n* The idea is simple, easy to implement and effective. The fact that quantization can be thought of as an extra layer of augmentation seems to be the main innovation of the work.\n* The approach is well motivated in the writing\n\nWeaknesses\n* Though the authors have put in efforts to do experiments and analyze at length, there seems to be some gap that are detailed below.\n  * The linear evaluation accuracies in Table 2 and 5 - whether they are FP or 4-bit are not mentioned. If it is one of these then is there any reason why the other metric is not provided (just like finetune results on ImageNet or CIFAR-100)?\n  * In a similar vein, evaluation of CQ-B variation of the transfer of ImageNet pretrained models to detection task in Table 3 is missing.\n  * Though the authors have told that CQ-A and CQ-B results were not given for CIFAR-100 motivated by the fact that CQ-C outperforms CQ-A and CQ-B in imagenet, it would be good to have CQ-A and CQ-B values for a small-scale dataset like CIFAR-100 (compared to ImageNet). It will be interesting to see the effects especially for ResNet18, Resnet-34, Resnet-74 where the improvements seem to be marginal in FP evaluation. This will also help understand how effective CQ methods are compared to vanilla SimCLR for small-scale datasets in its full spectrum.\n  * Transfer to detection tasks: When the experiments with Pascal-VOC were done, when was CQ used? Is it during ImageNet pretraining or Pascal-VOC training or both?\n  * Section 4.2 – Finetuning results: Any comment on why gradient explosion occurs here and not in other datasets?\n  * Section 4.2 – two precision sets 6-16 and 8-16 are adopted considering 4-bit may significantly degrade the final accuracy on large-scale datasets like ImageNet. Is there any experimental validation of this?\n* Line just after eqn. (1): Is $f^-$ a typo? What is $f^-$ is not clear.\n* Line after eqn. (7): It says that eqn. (7) encourages the feature consistency under different quantization levels. Or is it just the opposite? Does it not encourage the feature consistency under 'same’ quantization levels? The reason is that I see NCE is applied between outputs from same quantization levels.\n* The writing is many a time repetitive [one example is the fact that properly designed precision schedule helps DNN converge to a better local optima is repeated in introduction, related works as well as inspiration from recent works]. The figure captions [fig 1 and 2] are not adequately descriptive and I had to go back and forth between text and figure to try hard to understand the figures. Some minor typos need to be corrected. i) section 3.3 ‘towards on top of’ – towards will not be there. ii) Section 4: ‘for better understanding\nits effectiveness’ would be ‘for better understanding of its effectiveness’ iii) Section 4.1 – there are two commas (,) after i.e. iv) Section 4.1 – ‘achieve a consistently improvement’ should be ‘achieve a consistent improvement’ etc.\n\n",
            "summary_of_the_review": "In summary, the work is interesting and combines two well-known approaches successful in their own merit and application areas to self-supervised learning. The experiments are well executed but I also felt a lack of plan and design as detailed in the weaknesses part of my review. Thus, I am leaning towards reject at its current form, however I shall be happy to change my decision if the major concerns are properly addressed in the rebuttal.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper combines contrastive learning with quantization and presents a framework called  Contrastive Quant. Contrastive Quant enforces 1) the consistency not only on different augmentation views. 2) the different weights/activations with different quantization levels. They conduct the experiments on CIFAR100 and ImageNet on various backbones.",
            "main_review": "Strengths:\nThe contributions of this paper are clearly listed.\nWeaknesses:\n* Limited novelty, this paper is a naive A+B (contrastive learning + quantization) paper to me. And the motivation of combining these two factors is weak. I cannot see the intuition why quantization can be beneficial for the few-shot performance of contrastive learning \n\n* Questionable experiment results on ImageNet dataset. The baseline of SimCLR on ImageNet is very low. According to Table 2, the linear evaluation performance of SimCLR with [Resnet18, Resnet34] is [29.31%, 34.96%]. However, the performance reported in SimCLR paper [1] with [Resnet18, Resnet34] is [51%, 54%] (Figure 7 of [1]). And these results are under very similar settings (Both train for 100 epochs). Although the results in [1] use a larger batch size, the biggest gap between batch size 512 and 4096 is only ~4% for Resnet 50 (according to Figure 9 of [1]). I highly doubt the performance gap can be 20% for [Resnet18, Resnet34]. \n\n* A more convincing way to show the effectiveness of the proposed method is to compare it with SimCLRv2 [2] as the few-shot fine-tuning benchmark is well established in this paper.\n\n[1] Chen, Ting, et al. \"A simple framework for contrastive learning of visual representations.\" International conference on machine learning. PMLR, 2020.\n[2] Chen, Ting, et al. \"Big self-supervised models are strong semi-supervised learners.\" arXiv preprint arXiv:2006.10029 (2020).",
            "summary_of_the_review": "I  would recommend rejection for this paper because 1) it has limited novelty. 2) The highly questionable experiment result.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes to use quantization for contrastive learning. The proposed approach, Contrastive Quant, combines random data transformations as used in e.g. SimCLR with differently augmented weights and activations by applying various quantization levels. Extensive experiments show that this additional augmentation scheme improves the performance of SimCLR and BYOL for various architectures and datasets.",
            "main_review": "# Strengths\nThe proposed augmentations through quantization are an interesting tool to create more diverse augmentation schemes for contrastive learning. They seem to be straightforward to apply. By being data-agnostic, they would be applicable to a broad range of contrastive learning approaches even outside of the visual domain.\n\nThe paper is generally well-written and easy to follow. The presented approach is clear and visualized well in Figure 1.\n\n\n# Weaknesses\nThe main weakness in my eyes is that it remains unclear to me how a fair comparison between existing approaches (SimCLR, BYOL) and the proposed Quantrastive Quant was achieved in the experimental section. The evaluation spans a wide range of ResNet models, but leaves out the most commonly used ResNet-50 making it difficult to compare the results to previous work. The presented SimCLR results (most notably in Table 1 and 4) seem to be much weaker than in the original paper and it remains unclear to me why that is the case. As a result, it is difficult to judge how the performance of Quantrastive Quant compares to previous approaches. Additionally, I think the experimental evaluation could be made much stronger if result across different seeds were included. \n\nSince one of the selling points of quantization is model efficiency, it would be interesting to include an analysis on this as well. \n\nSome parts of the paper are difficult to follow as they rely too heavily on the knowledge of previous papers. For example, the \"Inspirations from recent works\" section and the explanation about the difference of strong augmentations on small and large-scale datasets could be made more self-contained. \n\n\n\n# Questions\n- Essentially, Contrastive Quant injects quantization noise into the weights and activations. How does this compare to other types of noise that one might inject?\n\n- It remained unclear to me how Contrastive Quant was applied to BYOL. Do you apply different quantization levels to the online and target networks, respectively?\n\n- In Table 8: The \"baseline without SSL training\" - is this just a randomly initialized model?\n\n\n# Other Points\n- Figure 1 could benefit from a self-contained figure caption.\n\n- The citations need some clean-up. Some of them are surrounded by parentheses when they shouldn't. Two papers of Aaron van den Oord are cited, but his name appears differently for each of them in the text.",
            "summary_of_the_review": "The paper presents an interesting new augmentation scheme for contrastive learning based on quantization. The proposed approach achieves good results, but it remains unclear how comparable these are to previous work.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a new contrastive learning framework (Contrastive Quant), to encourage the feature consistency under both (1) differently augmented inputs via various data transformations and (2) differently augmented weights/activations via various quantization levels. Extensive experiments on top of methods like SimCLR and BYOL, show that Contrastive Quant consistently improves the learned visual representation, especially under semi-supervised scenarios with limited labeled data.",
            "main_review": "Strengths:\n- The paper is well-written and easy to follow.\n- The idea of contrastive quantization is interesting.\n- Strong empirical results on multiple datasets (linear, finetuning and transfer learning settings).\n\nWeaknesses: While the idea of quantization is not new for improving efficiency of neural networks, quantization for improving contrastive learning surprisingly has not been explored much in the literature. The current works proposes one solution in that direction. However, there are several important weaknesses in the paper that need to be throughly addressed to improve the quality of the work (mainly missing experiments and comparisons).\n\n- How is the proposed contrastive quant comparable to a baseline (based in (Wu et al., 2020)) that uses adversarial perturbations on weights? Does quantization really necessary if we can simply perturb the weights to create two different variants of model in the proposed approach? Quantization introduces additional computational burden. So it would be interesting to see how is the contrastive quant comparable to such simple baselines on weight perturbations?  \n- Besides empirical performance, is there any reason why merely enhancing the feature consistency via quantization augmented weights/activations without the vanilla input augmentations leads to inferior performance? How about analyzing the attention weights of the learned models to understand possible reasons behind this?\n- What is the bit-width used to store the final weights and activations of the model after training? Are the additional overheads caused by the quantization included in the training time? How is the training time (convergence) comparable to the standard SimCLR/BYOL?\n- Since the proposed dynamic approach is independent of the quantization scheme, can it be applied on top of the popular static quantization methods such as LSQ, LQ-Nets or learnable quantization schemes like PACT? Experiments and discussions with an additional quantization scheme will improve the paper in showing the generalizability of the proposed approach.\n- Why did authors present only finetuning results with limited labels? How is the proposed method comparable to SimCLR and BYOL on ImageNet with full finetuning? This is an important experiment to judge the effectiveness of the Contrastive Quant over SimCLR and BYOL.\n- Authors present BYOL results only on CIFAR100 datasets. What about the results on ImageNet? It should be included in the main paper.\n- Did authors quantize weights/activations of all the layers in the network including first and last layers? The first and last layers are often not quantized to preserve the accuracy in the literature.\n- Since authors change the quantization of the same network weights during training, what was the effect of BN layers? Did authors use different batch normalization layers for different quantization precisions. It is very important to either use different BN layers or calibrate them separately as highlighted in AdaBits: Neural Network Quantization with Adaptive Bit-Widths, CVPR 2020.\n- Did authors try experimenting with 2-bit precision as part of the precision set? I wonder what is the effect of 2 bit precision in the contrastive quant for learning features.\n- What is the objective of showing finetuning performance with 4-bit? Since the main goal of contrastive quant is learning better features during training, I wonder why 4-bit finetuning is important in Table 1 and Table 4.\n- How is the transfer learning performance of Contrastive Quant comparable to SimCLR and BYOL on standard benchmarks like VTAB or 12 natural image classification datasets used SimCLR/BYOL papers? How is the proposed method comparable to SimCLR/BYOL on other dense prediction tasks like semantic segmentation and depth prediction? Experiments and analysis should be performance to clearly demonstrate the effectiveness of Contrastive Quant over standard contrastive learning methods.\n- How is the performance of the proposed method varies with respect to increase in labeled data in the semi-supervised setting (on ImageNet)? Is it consistently outperforming SimCLR and BYOL for all the percentages?\n- What are the different data augmentations authors used in this approach? Is quantization complementary to increase in strength of data augmentation? More specifically, can we obtain similar improvements by replacing quantization with more stronger data augmentation strategies?",
            "summary_of_the_review": "The paper needs significant changes including new experiments and discussions before being accepted to ICLR. The experiments are limited and not convincing in the current version of the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}