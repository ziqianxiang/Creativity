{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes to cast the classification problem into the learning to rank setting. Specifically authors treat multi-class classification as ranking the classes for each instance with the goal of getting the correct class at the top. Under this view they propose an architecture to better model the interaction between instance and class embeddings, and use learning to rank losses to maximize the ranking performance.",
            "main_review": "Strengths:\n-The majority of MCL work in areas such as vision and nlp focus on improving the backbone and a variant of cross entropy is almost always used for training. So framing the problem as learning to rank is quite interesting and to my knowledge hasn't been explored a lot. \n-The paper is well written and easy to follow.\n-Authors conduct extensive experiments with datasets from vision and nlp domains and evaluate various combinations of losses and instance-class embedding interaction structures.\n\nWeaknesses:\n-I have a major concern regarding the novelty. First, the main novelty seems to be the ranking generalisation architecture in Figure 2 on the right. However, looking at the appendix both LC+MLP and Concat+MLP interaction architectures do not provide a consistent gain over the dot product. This calls into question the usefulness of this generalisation. The gains in accuracy thus seem to be mainly coming from the ranking losses but these losses are taken off-the-shelf from the existing learning to rank work. So it is hard to pin point what is the contribution of this paper besides positioning MCL as a ranking problem which I don't think is sufficient on its own.\n-By casting the problem into learning to rank the output predictions from the model lose their probabilistic meaning. This can have negative implications for downstream application that rely on model confidence in addition to top predicted classes. I think this point should be discussed and addressed.",
            "summary_of_the_review": "The paper investigates an interesting approach of casting multi-instance classification problem as learning to rank. But the proposed approach lacks novelty and doesn't address some of the important issues that arise from learning to rank.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, they formulate a multi-class classification problem using learning to rank (LTR) frameworks. They argue that LTR evaluation metrics to measure the accuracy of MCC problem will be more informative. They also introduced methods borrowed from LTR literature to improve the MCC performance.\n",
            "main_review": "Strengths:\n- The paper is well-written and well-organized. I could follow the paper easily.\n\n- The idea of looking at MCC problem from LTR perspective is really interesting.\n\n- I liked that they covered both language and vision tasks in their experiments.\n\n\nNotes:\n- I am not sure if I understand the final message of the paper, does it suggest using ranking models/losses for MCC problems? Or suggesting to use ranking metrics for MCC problems?\n\n- Are there results in Table 2,3,4 based on cross-validation, what is the confidence interval? I would suggest use cross-validation or run the code based on several seeds and report the mean with the standard variation; especially that the improvement for some cases are not much significant.\n\n- I would discuss some limitations of this work.",
            "summary_of_the_review": "In general I liked the paper. It very easy to follow. I am not sure, however if I understand the main takeaway of the paper. If it's suggesting using ranking model for MCC, the experiments does not fully support the idea. If there were some other benefits for ranking models intstead of MCC, I would support the paper more. For example, if using ranking model we would need less data, the time/space complexity is lower, etc. I would back the paper stronger.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper tackles multiclass classification problem from a ranking perspective. The authors argue that instead of using top-k accuracy, many ranking criteria, such as NDCG, do not only count whether the predicted class is in the top k classes but also consider its relative orders. ",
            "main_review": "Strong Points:\nCompared to previous top-k metrics, which does not consider exact order of the precited class, the ranking metrics in this paper encourage models producing the correct prediction at a higher position. \n\nThis paper compares several variants of backbones, ranking losses and different datasets. The empirical experiments are rich.\n\nWeak Points\nGiven a multiclass dataset, for an input, there might be only one positive class and all the remaining classes are negative. How to order the negative classes in NDCG? Since different ground-truth orderings might results very different NDCG values or different models in training. This question is not trivial especially in the common cases where class labels are not ordered, e.g., dog, cat, lion, tiger. \n\nThe experimental results show that in many cases, the new model outperform other baseline models only at third decimal point. I highly suggest adding p-value or standard deviation to further confirm the statistical significance.\n",
            "summary_of_the_review": "The paper tries to solve multiclass problem in an interesting perspective. The novelty seems incremental since all the techniques used in this paper are well developed from information retrieval area and only gives very slightly better results. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper formulates the multi-class classification (MCC) problem using a learning-to-rank (LTR) setting. While existing studies mainly address the multi-class classification problem as an absolute ranking problem, the authors argue that MCC can be formulated by the relative ranking problem. The authors propose famous MCC architecture with an LTR pipeline and some loss functions. Experimental results show that MCC with LTR settings can improve the existing models with top-k metrics. While the idea is interesting, the technical novelty is incremental. In particular, performance gains are pretty marginal. The authors should conduct a significance test, and it would be better to show some qualitative results to compare existing models and the proposed model.",
            "main_review": "Strong points\n\nS1. This paper is well-written and easy to understand. Besides, the formulas, figures, and tables are well-described. \n\nS2. In Tables 5 and 6, it is interesting that the LC+MLP method shows the best or send-best performance for all settings.\n\nWeak points\n\nWhile the ranking metrics can be more informative than the existing Top-K metrics in evaluating the performance of classification models, this paper also has the following drawback.\n\nW1. It is unclear whether the proposed model improves the existing models. That is, the performance gap between the proposed idea and existing models is too small. The authors should evaluate a significance test. \n\nW2. It would be better to show some qualitative evaluation by comparing the class distribution. By doing so, the authors can evaluate the LTR setting can be informative for multi-class classification. \n\nW3. The multi-label classification problem can be more effective in evaluating the difference between existing and proposed LTR models.\n",
            "summary_of_the_review": "The technical novelty is limited, and performance gains are marginal. In this sense, the quality of this paper is insufficient to be accepted.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}