{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper has presented an interesting work regarding the non-robust features from pre-trained models and their impacts on fine-tuning in transfer learning. Authors have conducted comprehensive experiments on image recognition to demonstrate that adversarial non-robust features from the pre-trained models will affect the fine-tuned models. Based on the discovery and recent work on feature space steepness, a new model termed “Discrepancy Mitigating” is proposed and evaluated in various transfer learning tasks.",
            "main_review": "* Some typos were identified here and there. For example, the \"delta\" was not shown in Eq. (2).\n\n* The non-robust features were explained through Eq. (1-3), but these math concepts have a loose connection with the experiments discussed in later sections.\n\n* The novelty of the proposed method “Discrepancy Mitigating” is mainly based on prior work such as “feature space steepness,” and the improvement is marginal. Therefore, it is unclear if the new discoveries are valuable in practice. Authors may think about a better way to leverage the claims of this paper. \n\n* One thing unclear to me is if the non-robust feature originates from pre-trained model and data, then AT@stage-1 should be comparable or better than AT@stage-2. However, as shown in Table 5, AT@stage-2 is much better.\n",
            "summary_of_the_review": "In brief, the paper reveals some interesting phenomena aligned well with many recent works that focus on the reason for degraded performance due to adversary and non-robust features. The claim of non-robust features from the pre-trained model seems original and interesting, but it did not significantly improve the performance of recognition subject to adversarial attack, compared to the SOTA defense methods. In addition, the paper was not able to show the values of new discoveries in practice and did not demonstrate if they are applicable in more general cases.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/a",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies the adversarial robustness of pre-trained models and randomly initialized models. With experiments on image classification datasets, the authors found out that although pre-trained models generalize better, but they are less robust to adversarial perturbations. Then the paper tries to study why pre-training worsens adversarial robustness. With a feature similarity metric, the authors find out fine-tuned models are closer to pre-trained models. Various factors including task difference, task difficulty and model capacity are discussed and analyzed. Finally, the paper shows with adversarial pre-training, the robustness of fine-tuned models can be improved.",
            "main_review": "Pros:\n1.\tAn important topic of understanding why pre-training makes the models less robust\n2.\tVarious sets of experiments, including feature similarity, visualization, MMD distance\n3.\tClear writing. The contributions and the findings of each part are summarized.\n\nCons:\n1.\tAlthough the paper aims to find out why pre-training transfers non-robustness, what the authors present does not answer this question clearly. The authors claim that pre-training learns more non-robust features, but the experiments do not validate this claim. First, the pre-training in this paper is mostly carried out on ImageNet. The robustness can come from the ImageNet data but not pre-training itself. Experiments with multiple pre-training datasets would make the results more convincing. Second, Ilyas et al. (2019) managed to disentangle the robust and non-robust features and show standard training and adversarial training prefer different features. However, without the disentanglement of the features, it is hard to convince the readers the non-robustness all come from pre-training. \n2.\tMissing experiment details. What features is the MMD between source and target computed on? Pre-trained features or random features? How are the similarity of all layers computed? \n3.\tPart of the claims are not novel or known. It is well known that fine-tuned features are closer to pre-trained features [1][2]. It is also well known that overparametrized models can be more adversarially robust [3].\n\n[1] Towards Understanding the Transferability of Deep Representations. Hong Liu, Mingsheng Long, Jianmin Wang, Michael I. Jordan  \n[2] What is being transferred in transfer learning? Behnam Neyshabur, Hanie Sedghi, Chiyuan Zhang  \n[3] Convergence of Adversarial Training in Overparametrized Neural Networks. Ruiqi Gao, Tianle Cai, Haochuan Li, Liwei Wang, Cho-Jui Hsieh, Jason D. Lee  \n",
            "summary_of_the_review": "The problem is interesting, but the authors do not provide a convincing justification of their claims. I lean to reject this paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper claims that the pretrain-and-finetune paradigm transfers non-robustness from the source to the target task. \n\nFirst, the above-mentioned claim is backed empirically by comparing clean accuracy and robustness (with PGD-10, eps=0.5/255 attack) of a model trained from a scratch same model trained from a pretrained backbone.  The results showed that fine-tuned models achieve better clean accuracy but worst clean accuracy.\n\nSecond, a knowledge measurement metric (CCA) is employed to see the similarity of fine-tuned vs trained-from-scratch model with the pre-trained model. Conclusion: fine-tuned models are more similar to pre-trained models compared to models trained-from-scratch.\n\nThrid, Universal Adversarial Perturbations (UAPs) are used to demonstrate that UAPs for standard models are more semantically meaningful compared to trained-from-scratch which look more like noise. The conclusion: fine-tuned models rely more on non-robust features. \n\nFourth, the attack success ratio during the initial phase of training for the Alphabet dataset is shown. The attack success ratio of the fine-tuned model is high in the beginning. The conclusion: non-robust features are more likely to transfer from pretrained model than learned from target data. \n\nFifth, a UAP-based transfer attack (attack generated on the pre-trained model and used on fine-tuned and train-from-scratch model) is done to show that transfer attack is more successful for pre-trained to fine-tuned model compared to pre-trained to train-from-scratch model. The conclusion: pretraining transfers non-robust features.\n\nSixth, Maximum Mean Discrepancy (MMD) between source and target task is used to see the distance between two datasets. It is shown that the MMD of datasets positively correlates with a decrease in robustness. The conclusion: deviation of target task from source tasks largely affects the robustness of fine-tuned models. \n\nSeventh, it is investigated that how pre-trained models get the non-robust features during the pretraining phase. The authors hypothesize that when model capacity is limited or task is too difficult, pre-trained models tend to rely on non-robust features. These hypotheses are backed by two sets of experiments. First, increased model size increases robustness. \n",
            "main_review": "**Strengths**\n\n1. Paper investigates an interesting problem. \n\n2. A sufficient number of experiments are carried out to show that fine-tuned models are more vulnerable to attacks.\n\n**Weaknesses**\n\n1. In section 4, the knowledge learned by the standard model and fine-tuned model is compared by finding their similarity with the pre-trained model. The conclusion, \"fine-tuned model is more similar to pre-trained model\" and \"role of initialization of pre-training\" do not contribute to the understanding of non-robustness.\n\n 2. In figure 4, two images are shown to demonstrate that UAPs for fine-tuned models are more meaningful. However, it is hard to get to any conclusion with limited subjective evidence. It would be more desirable to do it more objectively. \n\n 3. The attack success ratio is not defined. It is likely that the high attack success ratio is due to the fact that train-from-scratch models have very low clean accuracy in the beginning.\n\n 4. The conclusion in Section 4.2, page 7: \" a rough conclusion that the deviation of the target task from the source task largely affects the robustness of fine-tuned model\"  seems to be at odds with the conclusion that robustness decrease is because of non-robust features transferred by pre-training. \n\n 5. Section 5 deals with the issue of how pretraining gets non-robust features. It is demonstrated that model capacity and task difficulty are two primary factors. However, this does not add to how and why the pre-training-to-fine-tuning paradigm transfers non-robustness. \n\n 6. The fact that model capacity affects robustness is well-known. For instance, see the adversarial training paper by Madry et al. [1].\n\n 7. Too many things are discussed without rigorous logic on why. For instance, what does CCA and knowledge measurement comparison shows? Why UAPs show that pretraining uses more non-robust features?\n\n [1] Towards deep learning models resistant to adversarial attacks in ICLR 2018.",
            "summary_of_the_review": "The paper shows an interesting result that pretraining may promote adversarial vulnerability. Various experiments are carried out to back the claim. However, it is hard to understand the reasons why different metrics and experiments are employed and it is not clear how results back the conclusions. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work explores how pre-training affects adversarial robustness of machine learning models, providing a series of experiments with image classification models. Their central finding is that fine-tuned models (either partially fine-tuned through training only the last classification layer or fully fine-tuned) show higher sensitivity to adversarial attacks, when compared to models trained from scratch on the same data. They argue that non-robust features are transferred from the pre-trained model, and introduce a method for improving robustness at pre-training time.",
            "main_review": "**Strengths**\n\n1. Investigating how pre-training affects adversarial robustness is an interesting and timely question.\n\n**Weaknesses**\n\n1. Experiments mostly focus on small scales, and broadening the scope of experiments would significantly strengthen this paper. For instance, fine-tuning larger models trained on larger datasets like CLIP to reasonably sized downstream datasets like ImageNet. As the authors themselves argue, we expect non-robust pre-training to be especially harmful when pre-trained models do not have high capacity, and the pre-training task is difficult; it would be greatly informative if performance on broader settings was reported.\n\n2. This paper is poorly written and would greatly benefit from a writing revision.\n\n3. Some parts of the paper seem to rely on anecdotal evidence to support its claims. One example of this is Figure 4.\n\n4. There is little detail in the main paper about the proposed mitigation solution in Section 6, making it hard for readers to understand the innovations introduced by this work.\n\n**Other comments:**\n\n1. Is there any reason why authors opted for using CCA instead of CKA (Kornblith et al., 2019)?\n\n2. What is the correlation between MMD and DR in Figure 6? It would be great to have this number. Another suggestion for authors is to show this figure as a scatter plot where the x and y axes are the MMD and DR values. ",
            "summary_of_the_review": "A timely paper that investigates the influence of pre-training in adversarial robustness. Overall, I believe it would be of interest to  researchers, despite some concerns with experiments and writing.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}