{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper tackles the problem of sequence modeling, where the sequence elements are either update instructions or queries. The assumption is that all the information required to answer a given query has been provided through previous instructions.\n\nThe proposed architecture maintains both an internal model (world) state as a fixed-size continuous vector that is updated through a recurrent module, and a memory of instructions which is accessed via an attention mechanism. Furthermore, the paper argues that the model learns better information retention when the training objective involves recalling all the information (instructions) at each time step.\n\nThe paper illustrates the approach on two synthetic tasks and on the Pathfinder problem.",
            "main_review": "The claims are that 1) the proposed approach can better capture sequences of arbitrary length (addressing the RNN / Transformer limitations in modeling long-range dependencies / long-context windows), and 2) the proposed approach can better incorporate information provided at application time to make predictions. Both of these issues have been addressed before and the paper does not demonstrate that the proposed approach provides substantial benefits.\n\nSpecifically:\n* The paper argues that the proposed architecture addresses Transformer's long context issues, but if I understand correctly, the proposed architecture includes attention over the list of update instructions. Can you discuss how are the instructions encoded to avoid the O(T) complexity? Furthermore, the comparison with alternative long-context transformer architecture does not reveal benefits of the proposed approach (e.g. faster, more accurate, or more interpretable, etc.).\n* While the proposed approach is a method that combines a recurrent state with attention over a memory, the paper does not distinguish itself clearly from similar approaches (e.g. memory networks, neural Turing machines, differentiable neural computers). While the paper highlights that these are computationally prohibitive to train, it's unclear how the proposed approach addresses the issue. Furthermore, the paper does not empirically compare performance with any of these approaches.\n* The paper seems to propose recalling previous information at every time step to encourage better information retention (Is that correct?). Such an approach is illustrated on a synthetic problem and it's unclear whether the training objective change hinders performance in a realistic setting (e.g. when too much information is provided and removing information unnecessary for the task is required).\n\nClarity:\n* The paper could be significantly changed to improve clarity. A significant part of the paper describes the problem and solution in too-general terms, which made it challenging to asses what is being concretely proposed in the paper.\n* Somewhat minor: the running example is very distinct that the tasks being used in the experimental section; I think that reducing this discrepancy would improve clarity.",
            "summary_of_the_review": "The paper tackles an important problem, but several prior works have addressed this before and the paper does not distinguishes itself clearly from them, nor does it show improved empirical performance.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This submission propose to manage long-term dependencies in sequence learning tasks. For this, is explicitly trains the world representation (which is essentially a long term hidden representation) to retain all relevant information it has been provided with in the past. This inductive property of the world representation allows for strong and efficient generalisation on sequence length, which is experimentally verified on standard benchmarks.",
            "main_review": "I believe that the theoretical contribution (Lemma 1) and the experiments are sound, and that the claims are well-supported. It intuitively makes sense that if the world representation is able to remember all relevant information, then it will become much easier to manage long-term dependencies. But the significance of the results is undermined by the fact that the *\"thorough querying\"* assumption is extremely restrictive. In practice, if I understand correctly:\n- for the recall task, the model can be queried about the target token at all steps\n- in *world of numbers*, *game of life* and *pathfinder*, the model can be queried about the value of any past pixel at any step\n\nIn my opinion, this additional supervision completely goes against the very purpose of these tasks. Please clarify if this is a misunderstanding.\n",
            "summary_of_the_review": "The submission appears sound, but the additional introduced supervision completely goes against the original purpose of the considered tasks. Therefore I will recommend rejection.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces a formalism for \"world state\" tasks and architectures, which consist of a sequences of  world states that are updated based on a sequence of \"instructions\", and which are then inspected using a sequence of \"queries\". They also introduce an architecture called an \"updater-extractor\", which uses Transformer subnetworks to update the world state from instructions and then extract answers from the world state given a query.\n\nThey then discuss some theoretical properties of this setup, and in particular show that a model that is always able to perfectly reproduce every input instruction it has ever received based on the current world state will necessarily store enough information in the world state to answer any other question optimally. This is then used to justify a training procedure involving truncated backpropagation through time: they train the model so that it can answer \"recall queries\" about instructions it received in the past after performing single-step updates to the world state, and then argue this removes the need for backpropagation over more than one step.\n\nThey evaluate their approach on three toy tasks of their own design. They also present results on the Pathfinder benchmark dataset, and find that it performs on par with some prior approaches to that task (although not reaching state of the art).",
            "main_review": "### Motivation and central ideas\nThis paper introduces a lot of different ideas, and I think some of them are quite interesting. However, I found it very difficult to follow the motivation for many of those ideas, especially on my first read through, and I think the paper would benefit from more focus.\n\nA lot of the confusion has to do with what a \"world state\" actually means for the purposes of the paper, and what kinds of problems they are hoping to address with it. In various points, it seems that \"world state\" refers to (a) the recurrent state held by an RNN cell such as an LSTM, (b) a set of propositions about the real world that are true, (c) a sort of life-long accumulation of facts that have been learned, (d) a fixed-size memory used to avoid computational overhead. These all seem like different ideas, but the authors mix all of these, which made it very difficult to figure out what the paper was actually about.\n\nAs a specific example, the authors begin by describing the problem of limited context windows in transformers, and claim that this is due to the lack of a persistent world state representation. It's never explained why this has anything to do with a persistent world state, though, and at best it maybe relates to sense (d) as saying they lack a fixed-size representation of history. Next, they discuss BPTT and Truncated BPTT, which now seems to be about sense (a). They then say this is problematic because transformers cannot \"learn through language\" by making long term changes in behavior, with an example of remembering facts about World War II. I don't really follow this example, because it's not really clear how a summarization model could benefit from past experience without supervision (whereas, if it has supervision, doesn't this already happen at training time?). If anything this is maybe sense (c), that remembering particular facts from previous examples would help in the future. Next they connect this to humans: they state that humans change their beliefs about the world based on behavior. But now we are talking about something totally different, because human goals aren't just summarization or other NLP tasks. It seems that this motivation is now about actual correspondence to the world and to being an agent acting within it, something like sense (b). They end the introduction by saying that they \"resolve the problem\" using their new architecture and training procedure. But at this point I am not even sure what problem they are trying to solve! And they also don't actually discuss what the solution is until section 5.\n\nFrom reading the rest of the paper, my current understanding is that the problem they tackle is effectively how to learn to maintain a fixed-size memory (sense d). Their architecture then draws inspiration from RNNs (sense a), and the training procedure involves remembering things from the past (sense c), but it doesn't seem like sense (c) is actually a goal, but is instead more of a technique. The discussion about facts that are true about the world (sense b) seems to be entirely for intuition purposes, as none of their tasks involve reasoning about properties of entities in the world. (It's possible I've misunderstood something here.)\n\nI think the the paper could be greatly improved by being explicit about what problems they are intending to solve, and how that relates to their solution. For instance, the authors could expand on the problem of fixed-size context windows, and discuss having a fixed-size memory representation as an alternative approach. Then they could discuss the problems of BPTT and why learning such a representation is difficult. Next they could propose what the desirable properties of such a representation would be for the tasks they are actually used for, discuss the relationship between those properties and \"world states\", and give some intuition about their training procedure.\n\nIf I am correct that the main goal is to enable scalable processing of long sequences, the World War II example, the humans-adjusting-beliefs example, and the \"house assistant\" example are very confusing, since these situations don't actually seem relevant for the problems the authors tackle. For me, this just confused me about what the world state meant; incorporating information like \"this kid went to the dentist\" is very different than incorporating information like \"pixels 10 through 20 of this image are 0, 1, 1, 0, 1, 0, ...\". It would also be good to focus more on NLP applications such as language modeling in the experiments.\n\nAlternatively, if the authors actually intend to build a model that maintains properties about the world, I think the \"context window\" discussion should be given less focus, and that the experiments should be expanded with tasks that actually have something to do with properties about the world. For instance, perhaps the bAbI tasks, or tasks involving common sense reasoning. As another option, if the authors are trying to study lifelong learning of ML systems, as hinted by the World War II example, it seems that they should discuss some of the related work in lifelong and continual learning in neural networks, and evaluate their approach on tasks for that area.\n\nRight now my overall sense is that the paper tries to do two or three different things at once and doesn't really fully commit to any of them.\n\n### Proposed architecture and relation to prior work\nThe proposed architecture consists of an \"updater\" that modifies a representation of the \"world state\", and an \"extractor\" that decodes information from the world state. This kind of division does not seem particularly novel, and many previous approaches can be similarly factorized in this way, including memory networks [(Weston et al, 2015)](https://arxiv.org/pdf/1410.3916.pdf), the EntNet model [(Henaff et al, 2016)](https://arxiv.org/abs/1612.03969) which the authors briefly mention but do not discuss in detail, and the GGT-NN model [(Johnson, 2017)](https://openreview.net/pdf?id=HJ0NvFzxl).\n\nThe specific details of the proposed architecture involve using a collection of \"tokens\" to represent the world state in a compressed form, and then using transformers to update these state token embeddings and to extract information from them. As the authors note, this is quite similar to the Transformer-XL and Compressive Transformer. It also seems similar to the Memory-Augmented Recurrent Transformer [(Lei et al, 2020)](https://aclanthology.org/2020.acl-main.233.pdf). Overall the architecture seems like a fairly incremental modification, and the main novelty comes not from the architecture itself but from the training algorithm.\n\n### Theoretical results and the proposed training algorithm\nThe proposed training algorithm is based on a strategy called \"thorough querying\": the idea is to require the model to answer questions about every input it has ever received based on the current world state. The authors prove that, if the accuracy of the model never decreases under thorough querying after advancing a single step (i.e. if it is \"stepwise optimal\"), then the model's accuracy will be as good as possible over arbitrarily long sequences with arbitrary queries. They thus propose using thorough querying to enforce local correctness, and then not computing any gradients between multiple steps.\n\nThe theoretical results seem correct, and this training procedure is novel to the best of my knowledge. However, the proposed approach seems impractical on real-world tasks for a few different reasons.\n- For long sequences with any uncertainty, the amount of information needed to be optimal under thorough querying seems like it would grow linearly. But the proposed architecture uses a finite amount of memory, meaning it is almost certainly not optimal.\n- Moreover, as the authors note, the theoretical results say nothing about models that are \"close to\" optimal.\nThe authors state \" it is reasonable to expect the model to gradually come closer to global optimality\" but I'm not convinced this is good enough. It seems like if the model can only maintain a fraction $x \\in [0,1]$ of the state in memory at each step, after $n$ steps it would maintain only $x^n$ of it, which is small for large $n$ despite the fact that $x^n$ is a continuous function of $x$.\n- The authors motivate things using NLP applications, but the representation of NLP tasks they discuss in 3.1.1 involves treating individual words at individual positions as instructions. Thorough querying in this setting seems to imply the model would need to be able to perfectly reconstruct the entire arbitrarily-long sequence of input, which seems very unrealistic.\n\n### Experimental results\nThe majority of the experiments are conducted on three new toy problems (LSTM recall, World of Numbers, and Game of Life with interventions). The results here are somewhat interesting, and are useful for understanding the capabilities of the transformer architecture as well as the interaction between truncated BPTT and their training algorithm. However, as far as I can tell, none of these experiments actually satisfy the thorough querying definition from their theoretical results. In particular, instead of including queries about arbitrary instructions provided at arbitrary times, they only query specific information that is known to be relevant (for the LSTM task) or information about the current timestep $t$ (for the other two toy problems). This means it's unclear how useful the thorough querying idea actually is; the high performance may be more influenced by the  toy-problem-specific knowledge of which queries are actually relevant. Additionally, none of these experiments include quantitative results, nor do they compare with any baseline approaches, which weakens their significance.\n\nThe authors also include results on the Pathfinder task, a challenging benchmark for long range attention. Their model is not state of the art, but it does outperform a large number of prior approaches, which is impressive given the simplicity of the approach. As far as I can tell this experiment is the only one that actually employs thorough querying; if so, this is some evidence that the training algorithm is actually useful on its own.\n\nAlthough these experiments do seem to contain some interesting findings about their approach, a weakness is that they don't seem to really address the problems that were used to motivate the approach. This is compounded by the confusion regarding which problems they actually intend to solve, which I discuss above. I think the paper would benefit from at least one more experiment in a real-world task based on the problem(s) the authors are actually able to solve, and this should match the way the solution is motivated. For instance:\n- To support claims about maintaining properties about the world, it makes sense to evaluate on something like the bAbI tasks, or some other tasks involving interacting entities in the world.\n- To support claims about long context windows for natural language, it makes sense to evaluate on some sort of long-context natural language task (the Pathfinder results are close, but I think results on real language would be more significant).\n- To support claims about lifelong learning, I think there should be some sort of evaluation in a context where information is retained \"between application-time instances\" of a task, as the authors describe in the introduction.\n\n(To be clear, I don't think the paper needs to add all of these. But I do think that anything used as motivation should have a corresponding experiment. As it is the paper claims to have \"resolved\" all of these problems, but doesn't give enough empirical support to any of them.)\n",
            "summary_of_the_review": "Although the paper has some interesting ideas, they are obscured by confusing motivation and a lack of focus on a particular problem. The proposed architecture seems like only a small adaptation of existing work. The proposed training method does appear to be novel, and has some interesting theoretical properties, but doesn't seem like a realistic solution to the problems the authors set out to solve. The majority of the experimental results focus on toy settings with no baselines and don't seem to follow the training procedure proposed in the theoretical section. There is one experiment on a more realistic task (Pathfinder) with promising results, but not enough to support the broad claims made initially.\n\nI think the paper could be greatly improved by narrowing the scope of the problem, clarifying what the goals of the approach actually are, and making sure that the experiments both (a) match the theoretical claims and (b) are representative of the problems the authors set out to solve.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}