{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents a new neural network architecture for detecting vulnerabilities in source code. The key idea behind the paper is to use a new graph construction method for converting a flat source code sequence into a graph-based on unique tokens and their co-occurrences. The authors then exploit the residual connections and graph level poolings of Graph neural networks (GNNs)  to learn a graph embedding for the given source code. Their results demonstrate that they outperform the state-of-the-art methods at the vulnerability detection task.",
            "main_review": "Strengths\n-------------\n- The paper is compared against several state-of-the-art baselines and the reported result show that the proposed approach outperforms them.\n- The basic idea of converting flat source code into a token-sequence-based graph and then learning a corresponding graph embedding is simple and easy to understand. \n\nWeaknesses\n-----------------\n- The proposed model is highly unlikely to understand any semantics of programming languages and thus I am not sure how generalizable the model will be for detecting real-world bugs.\n- I do not agree with the statement made by the authors that the data/control flow analysis-based preprocessing used by devign (zhou et al. 2019) is complex and difficult to implement. These are standard techniques used by any compiler for binary code generation and optimization. For example, compilers like LLVM already support such analysis for a wide variety of programming languages. \n- The vulnerability detection task is performed at the function level, not line level. Such detection is not very useful in practice as the software developers need to put significant effort into identifying the buggy lines in the software and fixing the bug. \n- The Improvement over the state-of-the-art (GraphCodeBERT) is rather small (<2%)\n\n\nMy main issue with this paper is that the graph construction method used in the paper completely ignores the underlying semantics of the programming language, it merely uses co-occurrence of tokens to create a graph. I suspect based on such inputs a model is highly likely to learn spurious features. I am surprised that the authors chose such a representation instead of using standard AST/data/control flow analysis used by compilers to construct a graph from source code.  Of course, it is possible that the model is somehow able to understand the semantics of code even from the proposed graph construction method. However, without seeing any extra evidence to check what the model is actually learning, I strongly suspect that the model is not learning anything meaningful. Together with this, the small improvement in accuracy makes me negative about this paper.",
            "summary_of_the_review": "While the paper improves upon the state-of-the-art in vulnerability detection empirically, the choice of the graph design is not well justified. Representing source code simply based on co-occurrence of token is unlikely to help the model to understand the underlying semantics of a programming language and thus might make the model brittle. I recommend that the authors compare their graph design against control flow graph/AST/data flow graph-based designs (while keeping the GNN architecture fixed).",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposed ReGVD, a GNN based approach to detect vulnerabilities in source code. Specifically, ReGVD treats the source code as the sequence of tokens. Based on this, it used two features, i.e., the unique tokens and its indexes, to construct the graph. The residual connection among GNN layers and the mixture of graph-level sum and max poolings are used to calculate the output embedding. The results demonstrated the effectiveness of the approach.",
            "main_review": "# Strengths \n1) The paper is easy to follow.\n2) The results seem to be promising\n\n# Weaknesses\n1) The method is too simple and the novelty is low. The usage of the residual connection among GNN layers and the mixture of graph-level sum and max poolings are not new. \n2)  It is surprised that the approach only used the token-level features but achieved high performance. There is a lack of explanation on why such simple features work well. From my understanding, vulnerability detection is very challenging. Even for human, it requires us to understand the semantics of the program for determining whether it has bugs. To this end, the existing works such as Devigin design different complex features (e.g., AST) to model such semantics. However, it is not convincing why the token features can get such better results. I guess it may be caused by overfitting? More discussion and illustration are needed.\n3) There are more GNN variants that are not discussed in this paper.",
            "summary_of_the_review": "This paper is well-written and easy to follow. However, considering that the lack of the novelty and the insufficient evaluation, this paper still needs to be improved.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies graph neural networks for vulnerability detection. The main approach includes the following parts: (1) constructing the program graph solely based on the co-occurrences of the program tokens; (2) adding residual connections in graph neural networks; and (3) adding some pooling layers for computing the graph embedding. They evaluate their models on the vulnerability detection task in CodeXGLUE benchmark, and compare their approaches to the state-of-the-art baselines including CodeBERT, GraphCodeBERT, and Devign. They demonstrate that their model performance is better than the baselines.",
            "main_review": "Vulnerability detection is an important problem, and this work reports some good results. However, there are several key limitations of the current submission, and I discuss the details as follows.\n\n1. There have been prior works that propose to add residual connections in graph neural networks, e.g., [1, 2, 3]. The authors did not cite and discuss related works on such GNN architecture design in the paper, thus the claim of the novelty in terms of the residual connection is misleading. The authors should add related discussion, and clarify what are the differences between their approach and the existing works.\n\n2. The technical novelty of the approach is generally limited. The graph construction and pooling layer design are valid for their task, but showing that adapting existing GNN models yields better performance for a specific dataset itself is not enough for a top-tier ML conference.\n\n3. I understand that the official code of Devign is not available, but it is still hard to understand why the Devign performance reported in the submission is much worse than that in their original paper, when compared to the numbers evaluated on the same projects. The Devign performance in this submission is even worse than TextCNN. More explanation is needed to better understand the inferior performance of Devign.\n\n4. In general, the paper lacks a good discussion of why and when the proposed model works better than the baselines. More quantitative and qualitative analyses are required. For example, what are the factors that affect the performance, e.g., code length or graph structure? What are the breakdown results on different projects? Could you provide some examples of correct and error cases?\n\n5. The CodeXGLUE benchmark aims to evaluate multiple tasks related to code understanding, thus their dataset for vulnerability detection itself is not very comprehensive. If the authors want to focus on the vulnerability detection task, they might consider collecting more data for experiments. Otherwise, the authors can also consider adapting their architectures to evaluate on other code classification problems, which should be pretty straightforward to implement.\n\n[1] Xavier Bresson and Thomas Laurent, Residual Gated Graph ConvNets.\n\n[2] Binxuan Huang, Kathleen M. Carley, Residual or Gate? Towards Deeper Graph Neural Networks for Inductive Graph Representation Learning.\n\n[3] Albert Mosella-Montoro and Javier Ruiz-Hidalgo, Residual Attention Graph Convolutional Network for Geometric 3D Scene Classification.\n",
            "summary_of_the_review": "Their graph neural networks indeed outperform strong baselines on the vulnerability detection task collected in CodeXGLUE benchmark. However, I think the paper does not meet the acceptance bar for the following reasons.\n\n1. The technical novelty is limited. There have been prior works that propose to add residual connections in graph neural networks, e.g., [1, 2, 3], but the authors did not cite and discuss these works. Other design choices are not very novel either.\n\n2. The paper lacks a good analysis of when and why the proposed model works better than the baselines. There are several questions that are not explained well; e.g., why the Devign performance reported in the submission is much worse than their original paper, and even worse than TextCNN.\n\n3. The CodeXGLUE benchmark aims to evaluate multiple tasks related to code understanding, thus their dataset for vulnerability detection itself is not very comprehensive. If the authors want to focus on the vulnerability detection task, they might consider collecting more data for experiments. Otherwise, the authors can also consider adapting their architectures to evaluate on other code classification problems, which should be pretty straightforward to implement.\n\n[1] Xavier Bresson and Thomas Laurent, Residual Gated Graph ConvNets.\n\n[2] Binxuan Huang, Kathleen M. Carley, Residual or Gate? Towards Deeper Graph Neural Networks for Inductive Graph Representation Learning.\n\n[3] Albert Mosella-Montoro and Javier Ruiz-Hidalgo, Residual Attention Graph Convolutional Network for Geometric 3D Scene Classification.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes ReGVD, a graph neural network for vulnerability detection. Compared with the code property graph that Devign used to build the program representation, ReGVD proposed two different ways for the construction: Unique token-focused construction and Index-focused construction. Furthermore, ReGVD leverages the embedding layer of a pre-trained PL model for node initialization and the residual connection among gnn layers to learn better features. By comparing with Devign and some latest pre-trained models such as CodeBERT and GraphCodeBERT, ReGVD proves its effectiveness. ",
            "main_review": "Strengths:\nThis paper compares different pooling operations such as summation, multiply and concatenation over the learnt node representations to investigate the effectiveness of different graph-level pooling operations.\n\nWeaknesses:\n\n(1). The technique novelty of this paper is lacking. In terms of code representation techniques, such as the code property graph that Devign used to represent the program, this paper proposed two ways for graph construction. However, both ways are too simple and straightforward, the rationale behind them is not clear, why they can be used to represent the program? A comparison with the code property graph should be provided to prove its superiority.  \n\n(2). In terms of network architecture, this paper claims that the initialization of the node embedding by the learnt pre-trained model and utilizing the residual connection in each layer of message passing during the learning process as the technique contributions, which seems to be weak.  \n\n(3). The analysis in the evaluation part also seems insufficient. Apart from comparing with the numbers, it is better to provide more insights into why it works, why it fails.  Furthermore, in terms of compared GNN variants, there are also some famous GNN variants such as GAT [1] and GIN [2] that are missed.\n\n(4). It seems this paper compared with TextCNN, however, this part of the description in the Baselines section is missed.\n\n[1]. Graph Attention Networks. \n\n[2]. How Powerful are Graph Neural Networks?\n\n",
            "summary_of_the_review": "The novelty of this paper is lacking and the deep analysis of the experimental results is also missed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}