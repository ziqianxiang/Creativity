{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "All reviewers agree that the presented investigation of existing person re-identification approaches is easy to read and can be used as a tutorial in the field. However, the reviewers raised a number of major concerns including inadequate discussion of insights made from the conducted survey, lack of some related important experimental studies and inadequate/ unconvincing conclusions made in the presented work. The authors’ rebuttal addressed some of the reviewers’ questions but failed to alleviate all reviewers’ concerns. Hence, I cannot suggest this paper for presentation at ICLR."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors conduct a systematic analysis of some existing person re-identification approaches. Specifically, they evaluate four publicly available models against three publicly available datasets using standard metrics. They also check how the models generalize to other datasets when not trained on them. Finally, they discuss the results and show which approach and training dataset combination works best in live re-id settings.",
            "main_review": "Strengths:\n\n- They explain the existing models and datasets (their advantages and shortcomings) in easy-to-read gists. So it serves as a good overview tutorial for someone new to the field. \n\n\nWeaknesses:\n\n- They do not propose any new models, datasets, or ways to improve current datasets/models. While they introduce two new metrics, it is not clear to me what advantage those metrics have over the standard ones and how they change the ranking of the models in a meaningful way. Maybe the authors need to clarify this. However, that being said, see the point below.  \n\n- Even if the contribution is just a systematic survey (which can be helpful), the paper doesn’t provide any exciting insights into what is learned from these experiments. They motivate the problem by saying different datasets have people from different ethnicities, have different geographical features, architectures, climates, etc. How do these features affect person re-id? What do we learn from all these experiments that can be used as insights to create better datasets? How much does the diversity of these features matter for generalization?\n\n- One of the most important questions these days with this domain of work is how it works across various ethnicities, especially under-represented ethnicities. The survey shows no insights or results in this direction.  \n\n- The model that performs well on just the single dataset evaluation (MGN in Table 2) also seems to work best in live settings (Table 4) regardless of dataset. So, why is dataset+model combo choice \"paramount for live settings\" according to the authors? Seems like the best model on the live dataset is simply the best model according to the single dataset - nothing surprising here.  In what ways are the results unexpected, and why?",
            "summary_of_the_review": "While the paper provides a thorough evaluation of some publicly available models and datasets, it doesn't offer any interesting insights into what makes a particular approach, dataset, or metric better or worse. For example, they say model_x + dataset_y works best. Why? Is it because of specific trends in model architecture? Is it because of certain geographical features or ethnicities of people in the dataset? Can we use these experimental insights to create better/inclusive datasets, and how? What are the shortcomings of current metrics, if any? How can we use these experimental results to improve the metrics in that case? \nThe authors say that the paper's goal is to learn insights on building better re-id pipelines. But, just showing the best model+dataset on a live re-id dataset does not provide enough insights into how to design datasets/metric/models better. Instead, it just shows how to choose the best model for that specific test dataset. Getting the best model for just some specific test dataset isn't that helpful unless some insights into what features of the test dataset make that model+train_dataset a good fit are provided. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies the evaluation methodology to assess if different standard Re-ID approaches and training datasets can be used to build efficient live Re-ID pipeline for real-world deployment. In particular, this paper formalizes the live Re-ID  setting and define two evaluation metrics named mAP (according to True Validation Rate vs Finding Rate) and $F_\\gamma$. Extensive experiments of different baselines on three benchmarks under three kinds of evaluation methods, i.e., single-dataset evaluations,  cross-dataset evaluations and live re-ID evaluation, are conducted, which gives insights of building a good live re-ID pipeline. \n",
            "main_review": "Strengths:\n   + This paper is well written, which clearly formulates the problem and gives systematic discussions.\n   + This paper presents two new evaluation metrics to evaluate the performance of a live re-ID approach.\n\nWeaknesses:\n\n   + The technical contribution of this paper is limited, which is far from a decent ICLR paper. In particular, All kinds of evaluations, i.e.,  single-dataset setting (most of existing person re-ID methods), cross-dataset setting [1, 2,3] and live re-id setting [4], have been discussed in previous works. This paper simply makes a systematic discussion.\n   + For cross-dataset setting, this paper only evaluates standard person re-ID methods that train on one dataset and evaluate on another, but fails to evaluate the typical cross-dataset person re-ID methods, e.g., [1, 2, 3].\n   +  For live re-ID setting, this paper does not compare with  the particular live re-ID baseline [4]\n   + Though some conclusions are drawn from the experiments, the novelty is limited. For example, 1) most of person re-ID methods build on the basis of pedestrian detector (two-step method), and there are also end-to-end method that combines detection and re-ID [5]; 2) It is common that distribution bias exists between datasets. It is hard to find a standard re-ID approach and a training dataset to address the problem unless the dataset is large enough that can cover as much as scenes. 3) cross-dataset methods try to mitigate the generalization problem.\n\n\t[1] Hu, Yang, Dong Yi, Shengcai Liao, Zhen Lei, and Stan Z. Li. \"Cross dataset person re-identification.\" In Asian Conference on Computer Vision, pp. 650-664. Springer, Cham, 2014.\n\t[2] Lv, Jianming, Weihang Chen, Qing Li, and Can Yang. \"Unsupervised cross-dataset person re-identification by transfer learning of spatial-temporal patterns.\" In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7948-7956. 2018.\n\t[3] Li, Yu-Jhe, Ci-Siang Lin, Yan-Bo Lin, and Yu-Chiang Frank Wang. \"Cross-dataset person re-identification via unsupervised pose disentanglement and adaptation.\" In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7919-7929. 2019.\n\t[4] Sumari, Felix O., Luigy Machaca, Jose Huaman, Esteban WG Clua, and Joris Guérin. \"Towards practical implementations of person re-identification from full video frames.\" Pattern Recognition Letters 138 (2020): 513-519.\n\t[5] Xiao, Tong, Shuang Li, Bochao Wang, Liang Lin, and Xiaogang Wang. \"Joint detection and identification feature learning for person search.\" In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3415-3424. 2017.\n\t\n",
            "summary_of_the_review": "From the discussion above, though the paper gives a systematic evaluation of benchmarking methodology and presents two new metrics to evaluate live re-ID approaches, there are some drawbacks and the technical contribution and novelty are limited. Thus, it is marginally below the acceptable threshold.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a live peron re-identification (Live Re-ID) setting to evaluate person re-id approaches closer to real application scenarios. The paper benchmarks a few state-of-the-art re-id methods, by training on popular re-id datasets (CUHK03, Market-1501, DukeMTMC) and testing on the m-PRID dataset. Experimental results suggest that a proper selection of the training dataset is important.",
            "main_review": "# Strengths\n- Person re-id has been studied for long in the research community, but how to bridge the gap between research and real-world application scenrios is still an open problem. This paper aims at reducing the gap by introducing a methodology (Live Re-ID) to benchmark the re-id approaches closer to real applications. This is a great trial and contribution.\n\n- The paper is well structured and easy to follow.\n\n# Weaknesses\nOne major weakness of the paper is that the proposed Live Re-ID setting was too straightforward to address the question it intended to answer -- how to benchmark re-id approaches in new environments.\n\n1. In real-world person re-id applications, the target test scenario could be\n\n    a) unknown at all, which requires the re-id method to be as generalized as possible\n\n    b) only having some unlabeled data, which allows some unsupervised domain adaptation methods\n\n    c) having labeled data, which allows supervised domain adaptation methods\n\nThe proposed Live Re-ID seemed to only consider the condition a), but did not discuss conditions b) and c) at all. It is hard to claim the Live Re-ID setting *\"takes into account all relevant aspects for deploying Re-ID models in practical real-world applications\"*.\n\n2. Even for condition a), the paper chose to evaluate only on one dataset (m-PRID), and train with one of the three popular datasets (CUHK03, Market-1501, DukeMTMC). This could be biased by the similarity between the training and test sets. Arguably, one better setting could be using K-Fold cross-validation, i.e., leaving one dataset for validation and using all the others together for training. Repeat the experiments with different datasets for validation and take the averaged / worst / P95 metrics as the final metric. I failed to understand why the proposed Live Re-ID setting is better than this baseline K-Fold cross-validation methodology.\n\nBesides, the conclusions of the paper (in the Section 5) may not be rigorous:\n\n3. *\"It is possible to build a successful live Re-ID pipeline ...\"* -- How to define \"successful\"? Are the current results shown in Table 4 good enough? What is the minimum requirement for the method to work reasonably well in real applications? What is the upper bound that the methods can achieve?\n\n4. *\"Proper choice of the standard re-ID approach and training dataset is paramount to obtain satisfying results when transferring the model to the live Re-ID setting.\"* -- Again, why not simply combining all the data available for training? How to define \"satisfying\"? And the paper did not discuss how to transfer the model (e.g., with domain adaptation).\n\n5. *\"Simple cross-dataset evaluation can be used to quickly assess the generalization performance of future standard Re-ID techniques for live Re-ID.\"* -- This claim is contradicting to the discussions *\"the cross-dataset results do not allow to choose\nthe best dataset for training models for the live Re-ID setting\"* in the text.\n\n6. *\"To properly assess the performance of new training datasets, the complete evaluation methodology proposed in this paper should be conducted.\"* -- Again, the paper used only the m-PRID as the test set. The methodology proposed was not technically sounding enough to assess the performance of new training datasets.",
            "summary_of_the_review": "Overall, I recommend rejecting the paper submission. As elaborated above, the paper did not well address the question it intended to answer (how to benchmark re-id approaches in new environments) and its claims were not well supported. These weaknesses were critical.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}