{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a Crossformer model for natural language processing (NLP) tasks. The key component in Crossformer is the cross-layer parameter sharing mechanism, namely using the same query/key vectors, MLP weight matrices, etc., across the adjacent Transformer layers. Such regularization can be added in a soft or fixed way. Empirical results on neural machine translation, visual question answering, and graph node classification are provided.\n",
            "main_review": "Pros:\n1.\tThe proposed model seems neat and easy to implement.\n\nCons: \n1.\tThe motivation of Crossformer is confusing for me. I fail to find convincing theoretical or empirical evidence to show that adding such a cross-layer parameter sharing mechanism is necessary. The experimental improvements are marginal and based on naïve baselines (i.e., vanilla Transformer), which are not strong enough to support the effectiveness of Crossformer. More insights should be provided to demonstrate why Crossformer makes sense.\n2.\tDiscussions and concept & empirical comparisons on the closely related works are insufficient. For example, Albert [*1] also introduces a cross-layer parameter sharing mechanism. Deep Equilibrium Model (DQE) [*2] shares the parameter of all layers. Since Crossformer seems quite similar to them, what are the differences between Crossformer and these two works? Does Crossformer outperform them?\n3.\tThe experimental improvements are actually insignificant. The experiments mainly focus on the neural machine translation task, where Crossformer boosts the BLEU score by <0.6 on WMT’14 with the same number of Transformer layers. However, according to [*3], such magnitude of difference is not convincing enough.\n\n[*1] Lan, Zhenzhong, et al. \"Albert: A lite bert for self-supervised learning of language representations.\" arXiv preprint arXiv:1909.11942 (2019).\n[*2] Bai, Shaojie, J. Zico Kolter, and Vladlen Koltun. \"Deep equilibrium models.\" arXiv preprint arXiv:1909.01377 (2019).\n[*3] Mathur, N., Baldwin, T., & Cohn, T. (2020). Tangled up in BLEU: Reevaluating the evaluation of automatic machine translation evaluation metrics. In ACL2020.\n\n",
            "summary_of_the_review": "Confusing motivation and marginal experimental improvements.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "### Summary \nthis paper targets to encourage transformers' lower layers to be guided by higher layers and proposes 1) a regularization method that lets the weights of low layers be similar to those of the high layers; 2) a weight sharing strategy to share weights of nearby layers. \n\n",
            "main_review": "### Advantages \n* Experiments on various tasks were done to evaluate the proposed methods.\n* Figure 2 clearly helps understand the paper.\n\n### Disadvantages\n* The motivation is not clear and the idea is not novel. Why should the low layers be similar to higher layers? If this assumption holds, the universal transformers [1](share weights of all layers) should work best. This paper does not show the performance of universal transformers. \n\n* The baseline in this paper is only a vanilla transformer. It is better to compare your methods with other related methods, like universal transformers [1] and LazyFormer [2]. Besides, there are some naive baselines, like sharing weights in groups. For example, if there are 6 layers of transformers, No. 1 and 2, 3 and 4, 5 and 6 can be shared.\n\n* Compared with the weak baseline vanilla transformer, the performance improvement is marginal. \n\n### Question\n* Why can the K vector be shared as a Q vector in the next layer? Can you explain it?\n\n[1] Dehghani M, Gouws S, Vinyals O, Uszkoreit J, Kaiser L. Universal Transformers. InInternational Conference on Learning Representations 2018 Sep 27.\n\n[2] Ying C, Ke G, He D, Liu TY. LazyFormer: Self Attention with Lazy Update. arXiv preprint arXiv:2102.12702. 2021 Feb 25.",
            "summary_of_the_review": "It is good that the authors evaluate their methods on tasks of translations, visual question answering and graph node classification. However, the methods are not novel and the performance improvement compared with the weak baseline vanilla transformer is marginal.  I prone to suggest rejecting this paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper introduces cross-layer guidance, which requires only a few modifications to Transformer. The method is simple to implement, stable to train, and maintains good scalability.\n\nAs I see, the main contribution of the paper is that different layers may share the same parameters (HG), or parameters may be regularized.\n\nWhile the method brings some improvements, the sharing mechanism lacks sufficient motivation.",
            "main_review": "Strengths:\n\n1. The proposed method brings some improvements on several benchmarks such as IWSLT, WMT.\n\nWeakness:\n\n1. The proposed method lacks sufficient motivation. For example, why does sharing parameters for different layers help improve the performance? Alternatively, why not share parameters with the same layer?\n\n2. Motivations of some specific implementations are not clear. For example, why not share K in different layers, but Q and K? Similarly, could the sharing mechanisms in FFN and self-attention work in a different way from Eq. (5)(6) ?\n\n3. If the dataset is large enough, whether the method still works? Or is it specialized for small dataset?",
            "summary_of_the_review": "Totally, the mechanism looks helpful to improve the model's performance, but the motivation of the paper is not sufficient. I think it is essential to further explore that why the sharing mechanism among layers helps.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a variant of the vanilla transformer, which adopts cross-layer parameter sharing and cross-layer feature guidance to reduce the memory footprint of the vanilla transformer.",
            "main_review": "1. The novelty of this paper is limited. Parameter sharing is not something new. For example, [1] [2] [3] all used this technique. Simple discussion in Sec. 4 CONCLUSION AND DISCUSSION is not enough. More extensive discussion and comparison with previous works are needed.\n\n2. Some settings of the Crossformer are hard to understand. For example, why use $Q^{t+1}$ to supervise $K^{t}$? Why not using $K^{t+1}$ or  $V^{t+1}$? If the motivation \"where the information from higher layers can and do distill the information from lower layers\" is true, why not using $Q^{t+2}$, $Q^{t+3}$, or $Q^{t+4}$? \n    \n    2.2.    How authors explain the parameter-sharing manner in Eq.(5)(6)? Why we share parameters in this way?\n\n3. The experiments cannot support the claim of the paper and validate the effectiveness of the Crossformer. Experiments are only done on WMT-2014 and IWSLT. More results on other datasets are needed. More comparisons with previous (similar) works are needed. \n\n4. Although #Params are smaller than the vanilla transformer, the computational cost increases dramatically. In Table.3, 16 layers Crossformer-All have similar #Params (60M) with 6 layers vanilla transformer, but the computational cost of Crossformer-All is roughly 3x as \nthe cost of a vanilla transformer. While the improvement (27.30 vs. 27.90 BLEU) is not satisfactory.\n\n[1] ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS. https://arxiv.org/pdf/1909.11942.pdf\n\n[2] UNIVERSAL TRANSFORMERS. https://arxiv.org/pdf/1807.03819.pdf\n\n[3] Lessons on Parameter Sharing across Layers in Transformers. https://arxiv.org/pdf/2104.06022.pdf",
            "summary_of_the_review": "This paper lacks thorough discussions with similar previous works and extensive experiments to support the paper's arguments, so I vote for rejection.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}