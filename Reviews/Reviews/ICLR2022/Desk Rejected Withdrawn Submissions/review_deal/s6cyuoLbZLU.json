{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, the authors compare representation learning pipelines for biological images on three downstream tasks. They combine four different architectures with absence/presence of two data augmentation strategies, producing a total of 16 different representation learning settings. The settings are evaluated on three downstream tasks: (i) distances of similar compounds, (ii) binary classification and (iii) clustering.",
            "main_review": "The question which the authors have decided to tackle is an interesting one: is it possible to identify a representation learning method that performs better for a certain number of common downstream tasks, when it comes to biological microscopy images? However, the paper is (1) too limited in scope, and (2) fails to convey a convincing response to that question on the ground it chose. \n\n(1) To produce general results for Bioimage informatics, it would be interesting to include other types of images such as fluorescence images, and discuss how certain approaches might work for fluorescence microscopy and not brightfield, and vice versa. It would also be interesting to investigate along the cell line axis, for example how do representations learned on most cell lines generalize on others on average? Is a given method better at transfer learning across cell lines? Across types of imaging (fluorescence vs. brightfield)?\n\n(2) Major comments:\n- Data: from section 3 “some drugs did not provoke any effect on resistant cell lines, so the corresponding images of drugs and controls look the same”. This is problematic, both for architecture (a) and downstream task (ii): it will be difficult to get signals from a binary classification task if the dataset is noisy, i.e. not all datapoints in the “drug” class actually belong to it. Indeed, even if cells have been exposed to a drug, if they’re not significantly different from controls, then the images belong to the control class. Furthermore, no data quality control is mentioned, the data does not seem to be from a public dataset and there is no detailed data description.\n- Evaluation and results: \n    - The distance evaluation is performed on a single pair of similar known drugs. As such, this result is too limited to conclude whether a given approach produced a biologically meaningful space. Are there other drugs in the set that one expects to be close? Maybe drugs with a common mode-of-action? Or cell lines from a common tissue?\n    - The clustering evaluation is not presented in a way that can be used to evaluate learned representations, because there is no groundtruth. If there is no known cluster structure in the problem (e.g. drug classes), why would the best method be the one to find clusters?\n    - Standard deviations are not provided for any of the results. As such, it is not possible to evaluate the numbers which are provided either in figure 3, 4 or table 2. Providing standard deviations is all the more important when working with biological data, as there can be large variations between images of cells from the same cell line exposed to the same drug. For example in the case of the binary classification evaluation, cross-validation should be performed, and both the average and the standard deviation of classification accuracy over folds should be indicated.\n    - The training code is not provided, just the architectures.\n\nMinor comments:\n- Data:\n    - The 770k dataset cannot be referred as such in the abstract or intro, as it seems to be an in-house dataset (no publication is cited and Google does not easily locate it). If the dataset is new, this should be made clear, and data description should be like in an article for a biological journal (it can go in the Appendix if it is too long). For example how long were the cells exposed for prior to image acquisition? Which cell lines were included? How many batches, replicates? Any quality control, e.g. for images with low cell count? Any correction for batch effect?\n    - If a statistical test is performed to detect which highest dose drugs are not active, it can also be used to detect which lower dose drugs are. This would enrich the dataset by making it possible to include images from other than the highest dose exposure.\n- Experimental settings:\n    - Why BYOL and another self-supervised method?\n    - It is surprising that the binary classifier architecture (a) has a two-dimensional output.\n    - The training setups are not very clear.\n    - “We chose the CNN backbone architecture, batch size and other common hyperparameters by running grid search” It would be better to specify what set of hyper-parameters were tested during the grid search.\n    - “In many cases, we encountered a cuda-out-of-memory runtime error, which was connected to the higher dimensionality of the projection layer, compared to the hidden layer”. This error should be fixed, or the parameter range that raises this error not considered as tested - rather than be in the body of the article.\n- Figures: it will be clearer to read if there is one subplot per cell line, rather than one per pipeline. As such, one can mostly see that some cell lines are easier than others to deal with for a certain task.\n- Other: limit the use of terms such as “AI”. Also prefer to use the term “learned representations” or “latent representations” rather than “codes” (multiple uses p.5-6)\n",
            "summary_of_the_review": "The question which the authors have decided to tackle is an interesting one: is it possible to identify a representation learning method that performs better for a certain number of common downstream tasks, when it comes to biological microscopy images? However, the paper is (1) too limited in scope, and (2) fails to convey a convincing response to that question on the ground it chose. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper presents an empirical evaluation of representation learning strategies for cellular image analysis. The application domain is drug screening where cancer cells are exposed to different drugs and then microscopy images are captured to detect their effects. There is no ground truth for this type of biological experiment, because the goal is to reveal drug activity and efficacy rather than classifying known patterns. The paper evaluates 4 models under 4 experimental configurations and the results are reported and discussed.",
            "main_review": "The paper is well written and discusses previous research in the field extensively.\n\nDataset: The images come from brightfield microscopy without fluorescent stains. The total number of perturbations is above 600, which is representative from most drug screens. There are also several cell lines and time points available to evaluate the efficacy of compounds.\n* It is unclear why the study takes 770k images of 64x64 pixels, when presumably the original microscopy fields of view are larger. Are these random crops or cell segmentation is performed?\n* The paper says that single crops are full images resized to a fixed size, while multiple crops are 4 random crops taken from the original image. Is the base for cropping the set of 770k 64x64 images?\n\nModels: the study is focused on training a convolutional network as a feature extractor. The convnet is trained with 4 different strategies: WSL (weakly supervised learning), AE (autoencoders), SSL (self-supervised learning) and RegL (regularized learning). These models are the major strategies for representation learning, and it is interesting to compare them in this domain.\n* Is the common convnet encoder a three layer convnet as depicted in Figure 2? This is a simple architecture that can be run efficiently, but may not be the best to capture image features. Other architectures such as small ResNets could have a slightly higher computational cost while obtaining better performance.\n* Representation learning is usually evaluated using a linear classification regime, but in this case, the authors chose to use a two layer classifier. This further indicates that the original architecture lacks capacity for disentangling representations and additional non-linearities are needed to recognize patterns.\n\nEvaluation tasks: there are three tasks where the representation learning aspects are evaluated, 1) the distance between MTX, PTX and DMSO; 2) the binary classification of treated vs untreated cells; and 3) clustering. With more than 600 compounds, multiple cell lines and multiple time points, the dataset could have been to perform other interesting analysis / evaluations.\n* The Euclidean distance in the first test may not be the optimal metric to compare these samples. The cosine similarity or Pearson correlation could yield different results. \n* A binary classification task between treatments and controls should not need supervision (with a two-layer classifier) if the representation is powerful enough. This separation would be expected in an unsupervised way. More interestingly, the separation could be measured for each compound individually and report how many of the 600 compounds can be tell apart from DMSO (with some distance or statistical threshold).\n* Clustering can result in an arbitrary measure, unless clusters correspond with some biologically meaningful group of compounds. With 600 compounds, some of them may have known connections (such as MTX and PTX) that could be used to evaluate if they appear close by in similar groups.\n\nResults: Among all models and configurations tested, it is still hard to identify a strategy that could be used to scale drug screening effectively. The relative performance according to the three selected tasks is almost similar, and beyond computational efficiency, there is no real insight into what could be the future of analyzing these images. The chosen metrics and the way the results are presented do not convey a clarifying message on how to approach the problem presented here. The paper presents concluding remarks that are in line with observations from prior work, and it's hard to identify what is the message or recommendation.\n\n",
            "summary_of_the_review": "The paper presents a systematic evaluation of representation learning approaches applied to cellular images for drug screening. The tasks selected for evaluation seem weak and the results do not clearly illuminate a way forward. The models could be improved in terms of learning capacity, and the metrics could be extended to provide more insights into what makes a difference for drug discovery.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper presents empirical results on the efficacy of convolutional neural networks when applied to three down-stream tasks using a gray-scale, image dataset of drug-treated, in vitro cancer cell populations. Specifically, this paper compares the performance of four different CNN models using an identical “base” layer stack with varying training approaches (categorical cross-entropy, auto-encoder, auto-encoder with regularization, and self-supervised). For each combination, the authors compare performance when trained with and without random augmentations and with and without multiple image crops.\n\nEach of the four models are trained using the same dataset, but with a problem setup appropriate for the particular model. To standardize the evaluation of models trained using differing problem setups, the “base” layer stack which is identical for all models is extracted and used to generate latent representations of hold-out images. These representations are then evaluated using multiple tasks: distance in latent representation for known drugs, classification of cells treated with a drug or control, and clustering performance of various cell lines.\n\nThe papers primary contributions:\n* Comparing the usefulness of latent representations for downstream tasks obtained from four different CNN modeling approaches on the same biological image dataset\n* Comparing the effect of applying random augmentations and multiple image crops  on downstream performance \n",
            "main_review": "## Motivation\nThe application of machine learning, and specifically CNNs, to biological image data is interesting and well motivated. Improvements in this domain can offer significant benefits to society and past work in this area has led to interesting machine learning contributions (e.g. the U-Net). Additionally, rigorous empirical analysis of machine learning models applied to many scientific domains, including biology, is often limited so works that focus on this can be useful to other researchers working in this domain.\n\n## Strengths\nThis paper uses a common dataset for model evaluation which is useful for comparing the various approaches investigated. Many other papers in this area only adopt a single modeling approach and each paper utilizes different datasets so direct comparison of methods is difficult or impossible.\n\nThis paper utilizes a common “base” layer stack in all of it’s approaches and uses this common layer stack for it’s feature extraction. This helps simplify comparison between the different methods examined.\n\nThis paper explores three different down-stream tasks using the same dataset. This is useful to help identify if specific models perform best broadly, or only on a specific task. \n\nThis paper follows the page limit and is easy to follow and understand.\n\n## Weaknesses\nThis paper uses a relatively small CNN for it’s experiments (the base layer stack consists of only three layers with 64, 32, and 8 filters in each, respectively). Significantly larger models are often used in this domain and the results of this paper may change with larger models. If models of various overall sizes were compared and the results found to be similar this would be worth reporting. It is possible that the limited computational resources (a single RTX 2060) may have factored into the model size used.\n\nThis paper does not explore other, common architecture features or designs (e.g. multi-scale CNNs, residual connections) which are often used in image and video CNN models. These, and other methods, have shown significant value both within the biology domain and more broadly. \n\nThis paper only utilizes a single dataset for evaluation. While the use of multiple down-stream tasks for the dataset is positive, in order for the paper to demonstrate more generalizable results for machine learning practitioners working in this domain the use of several biological image datasets is important. There may be specific aspects of the chosen dataset that affect the comparison of the different models.\n\nThe finding that multi-crops and random augmentations generally improve performance in downstream tasks is not unexpected. While this is not problematic, this is not a significant contribution.\n\nThe paper does not present any significant, specific results that are actionable for other machine learning practitioners or researchers in this domain. The performance of the models vary between downstream tasks and the models often perform similarly. From the last line of the paper, “no single combination of model (architecture) and setting (augmenting and cropping strategies) consistently outperformed the others.”  \n\nThe paper does not present any novel methods or techniques beyond its empirical results.\n",
            "summary_of_the_review": "The paper is well motivated. It explores an interesting and valuable domain and papers that perform empirical evaluations and comparisons of relevant machine learning models can be useful for practitioners and researchers. \n\nHowever, empirical studies need strong evaluations as the experiments are the primary contribution. Such studies should strive to be generalizable and present useful findings to maximize their impact. This paper is marginal in both of these areas. \n\nFor this paper to make well-supported, generalizable statements it needs to investigate larger models with a greater variety of architectural compositions. It should evaluate at least one other dataset to ensure its findings are not specific to the specific dataset chosen. And a larger variety of experiments should be performed (multiple random initializations for each model with reported variance, a variety of hyper-parameter settings with discussion on the sensitivity of the results to them). \n\nFor this paper to present more useful findings it should present results that are either extremely strong (e.g. a certain model consistently and significantly outperforms others) or surprising (e.g. a commonly used technique or model has key limitations). Here the paper presents generally unclear results (with respect to which modeling approach performs best) and unsurprising results (that random augmentation and multi-crops improve performance). \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents an experimental design for evaluating the effects of augmentation, multiple cropping, and types of learning on the accuracy of a classification task between control and drug treatment of cell lines from grayscale image collections. The type of CNN classification architectures include weakly-supervised, self-supervised, regularized, and unsupervised learning of image representations. \nThe experimental evaluations are performed using feature distance, classification model accuracy, and clustering metrics over 16 deep learning training setups on the 770k image dataset under identical conditions.\nThe novelty of the work is in performing the experimental design, evaluating the metrics, and drawing conclusions about setups and classification accuracy.\n",
            "main_review": "Strength: \nThe authors report an interesting experimental design study that includes four types of model architectures, augmentation, and multicrop while evaluating the representation power via feature distance, classification model accuracy, and clustering metrics.\n\nWeakness:\nThe authors should provide deeper insights into the experimental design and explain the factors of the experimental design. For example, the authors can explain how adding the multicrop option with 1, ½, and ¼ resized crops is expected to introduce model invariance to size variations as a function of spatial resolution. Similarly, adding augmentations into the training data that include, for instance, Gaussian blur, will introduce higher robustness of the ai model to out-of-focus images. On the other hand, introducing multicrop and Gaussian blur might decrease classification accuracy while increasing generalizability and avoiding overfitting the training data. \n\nThe authors should provide similar insights about the model types (i.e., unsupervised, self-supervised, weakly-supervised, and regularized learning of image representations) in Section 4.1 so that the reader understands the reasoning behind selecting the model types and the expectations from each model architecture before the results are presented. \n\nThe Table 2 would also benefit from reporting standard deviations to understand the confidence intervals for each accuracy measurement.\n\nMinor comments:\nThe authors could also claim a contribution in assembling an evaluation dataset with semantically meaningful labels (i.e., MTX, PTX, and DMSO labels) assuming that the authors would share the dataset with the community. Others could reuse the dataset for other evaluation studies. \nIs there a conflict in the claims that all models have been trained with the same set of hyper-parameters”\n•\tSection 4.2: “We trained all models on the same dataset for 50 epochs, using Adam optimizer with a constant learning rate of 0.0001. A batch size of 256 was used”\n•\tSection 4.3.2: “We used the same data splits for all models and trained them for 25 epochs with SGD optimizer and batch size of 1024”\n•\tSection 7: “We used the same optimizers, batch sizes and other training parameters to ensure fair comparison of learned representations”\n\n\n",
            "summary_of_the_review": "Summary statement:\nThe paper is well written, and the technical part is clearly presented. I am missing a bit more insights about the experimental design and statistical confidence in the accuracy measurements (and hence derived conclusions). \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concerns",
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}