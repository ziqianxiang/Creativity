{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper focuses on non-stationary MDPs with linear function approximation assumptions. This work proposes the first policy optimization method, \"PROPO,\" and the corresponding optimism-based value iteration algorithm, \"SW-LSVI-UCB.\" Furthermore, the theoretical results give both upper and lower bound for the regret in non-stationary MDPs, which show the proposed algorithm is near-optimal.",
            "main_review": "I have some concerns about this paper.\n1. This work combines the traditional non-stationary analysis with the structure of the OPPO algorithm, and the technical contribution is limited, which makes this paper a bit incremental.\n\n2. For the gap between the upper bound and lower bound, the author mentions that this gap can be bridged by using the “Bernstein” type bonus functions in Azar et al. (2017); Jin et al. (2018). However, this work only focuses on the tabular MDP setting, and it is much difficult to implement the “Bernstein” type bonus in the linear function approximation setting. On the other hand, some new works [1,2] have recently implemented the “Bernstein” type bonus with liner function approximation for both UCB-optimistic algorithm and policy optimization algorithm, which bridge the gap and obtain near-optimal results. Therefore, it seems more reasonable to use this technique to close the gap, and it is better to comment on this paper in the related work and  \"Optimality of the Bounds.\"\n\n[1]: Zhou D, Gu Q, Szepesvari C. Nearly minimax optimal reinforcement learning for linear mixture Markov decision processes[C]//Conference on Learning Theory. PMLR, 2021: 4532-4576.\n\n[2]: He J, Zhou D, Gu Q. Nearly Optimal Regret for Learning Adversarial MDPs with Linear Function Approximation[J]. arXiv preprint arXiv:2102.08940, 2021.\n\n3. For the upper bound, the author makes a strong assumption 4.1. I agree that this is a common assumption when analyzing non-station MDP. However, it seems unfair to compare the lower bound for general MDP with the upper bound (with a strong assumption) and mention that the regret bound is near-optimal. It would be better if the author could construct a hard-to-learn instance that also satisfies assumption 4.1.",
            "summary_of_the_review": "Based on previous comments, this paper is marginally below the acceptance threshold.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies the OPPO in non-stationary MDPs, where the transition kernel of MDP is drifting with respect to the time step. Under the linear kernel MDP setting, a minimax lower bound is provided and the authors proposed an algorithm under the same setting. The theoretical analysis then follows to provide a regret bound for the proposed algorithm showing the proposed algorithm is efficient.",
            "main_review": "The authors provide both the lower bound and the upper bound for the proposed algorithms in this paper, showing that the proposed PRPRO algorithm is efficient under the linear kernel MDP setting. Despite that merit, I'm concerned with the following issues regarding the problem settings and novelty in this paper.\n\nFirst, Assumption 4.1 sounds too restrictive to me, though the authors claimed a similar assumption was made in [Cheung et al.](https://arxiv.org/abs/1903.01461). In detail, [Cheung et al.](https://arxiv.org/abs/1903.01461) is assuming the contextual vectors lie in a $d$-dimension manifold. However, here the authors do not only make this assumption on the contextual vectors $\\phi$ for rewards, but also for the feature vectors $\\eta$ depending on the value function $V_{h+1}^k$ in each episode. As a result, such an assumption depending on the value function (i.e. $\\eta_h^k(\\cdot, \\cdot) = \\int_{\\mathcal S}\\psi(\\cdot, \\cdot, s')V_{h+1}^k(s')\\mathrm ds'$) is too restrictive and can hardly be verified. Intuitionally speaking, the value function $V_h^k$ contains some **square root of quadratic terms** $\\Gamma_h^k$, which can never be expressed using a **linear** function. Unless the authors can give some examples for that, especially when $S >> d$, Assumption 4.1 is too restrictive and might never be satisfied. \n\nSecond, I'm concerned with the novelty of this paper compared with [Cheung et al.](https://arxiv.org/abs/1903.01461). It's common knowledge that the results in bandit literature can be well-extended to Linear Kernel MDP equipped with previous works like [Ayoub et al.](https://arxiv.org/abs/2006.01107), [Zhou et al.](https://arxiv.org/abs/2006.13165) to control the optimistic of value functions. Since [Cheung et al.](https://arxiv.org/abs/1903.01461) provided a similar lower bound (similar scale regardless time-horizon $H$, since it is a bandit algorithm) and the sliding window technique, the authors might need to highlight the contribution of this paper given those previous works. (For example, the lower bound of $H^{2/3}T^{2/3}$ sounds like a trivial extension of [Cheung et al.](https://arxiv.org/abs/1903.01461) if we simply see these $T$ episodes as bandits with $T' = HT$, and one can find a similar proof sketch in [Cheung et al.](https://arxiv.org/abs/1903.01461))\n\nThird, I would suggest the authors try to add some experiments demonstrating the performance of the proposed algorithm. For example, it would be better if the authors could implement the PROPO algorithm on the proposed hard instance shown in Fig. 1.",
            "summary_of_the_review": "This paper is clearly written, and it provides a solid analysis of the non-stationary MDPs but given my justification above, I'm concerned with the novelty and the realizability of Assumption 4.1. Some non-rigorous analysis indicates that the assumption is too restrictive, and some contributions claimed in this paper is expectable given previous works. I lean towards rejecting this paper but I'm open to further discussion if the authors could address these issues.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper considers reinforcement learning, a common model which has seen success recently in many areas (e.g. games, robotics, autonomous vehicles). Prototypical reinforcement learning algorithms explore the action space in order to maximize their rewards as much as possible, potentially ignoring dynamic or shifting environments.  However, under dynamic environments such algorithms will achieve poor performance, due to not taking into account the non-stationarity of the underlying environment.  The major contributions from the authors can be listed as follows:\n1. Extended existing sliding window and periodic-restart techniques for policy-based optimization to non-stationary MDPs with linear function approximation\n2. Extended existing sliding window techniques for policy evaluation algorithms to non-stationary MDPs with linear function approximation\n3. Developed mini-max lower bounds for non-stationary MDPs with linear function approximation\n4. Developed dynamic regret bounds for their algorithms which nearly match the lower bounds\n\nTo be more specific, the authors consider the typical RL in MDP model with an MDP modeled via $(S, A, P, r, H)$.  However, specific to the non-stationarity they assume that $P$ and $r$ are allowed to vary both respect to the timesteps $h$ but also with episodes in the interaction $k$.  In order to perform 'some' level of function approximation they assume the typical linear kernel MDP where the rewards and transitions are expressed with respect to a known linear parameterization of the state and action space, i.e. $r = \\phi(s,a)^\\top \\theta_h^k$ and $P = \\psi(s,a,s')^\\top \\xi_h^k$.  Moreover, to monitor the non-stationarity of the problem they impose additional assumptions on the drift of the underlying rewards and dynamics (which here is imposed by cumulative $\\ell_\\infty$ bounds on the differences between $\\theta$ and $\\xi$ across $k$).  The goal then is to learn a policy which adapts to the slowly-changing environment by performing well against the best policy within each episode cumulatively.\n\nThe main algorithm consists of two main components: \n\n1. Policy improvement with respect to optimistic estimates of the $Q$ function with periodic restarts\n\nThis is a straight-forward extension of the policy improvement step in NPG viewed as online mirror descent with periodic restarts to account for the shifting dynamics in the setting.\n\n2. Sliding window-based regression estimates of the linear parameters to perform value iteration to estimate the $Q$ function.\n\nThis is a straight-forward extension of existing techniques for estimating the underlying parameters $\\theta_h^k$ and $\\xi_h^k$ using sliding time-window regression estimates, and using these estimates to perform optimistic value iteration.\n\nOnce the algorithm has been developed, the authors move on to show their main theoretical contributions, which can be summarized as follows:\n1. Provided a mini-max lower bound of the dynamic regret in this setting, scaling with respect to the key parameters and the underlying measure of non-stationarity in the environment\n2. Provided dynamic regret bounds for their algorithm which is nearly matching\n3. Provided dynamic regret bounds for a different algorithm which instead just using the optimistic estimates of the $Q$ function obtained via the previous regression estimators",
            "main_review": "### Originality:\n\nThe authors consider the setting of RL under non-stationary environments.  In particular, the authors present a novel mini-max bound for the performance of any algorithm in terms of the feature dimension, non-stationarity in the environment, and the time horizon.  Moreover, in order to nearly-match these guarantees they develop new algorithmic approaches including:\n\n1. Policy improvement with respect to optimistic estimates of the $Q$ function with periodic restarts\n\n2. Sliding window-based regression estimates of the linear parameters to perform value iteration to estimate the $Q$ function\n\nThe authors claim that these sliding window estimates and policy improvement steps are novel.  However, these algorithmic techniques have been used previously in the literature (and the explicit connection is never concretely given).  In particular, sliding window estimates has been used previously in \"A Kernel-Based Approach to Non-Stationary Reinforcement Learning in Metric Spaces\" and \"Nonstationary Reinforcement Learning with Linear Function Approximation\".  The second paper additionally considers a similar linear MDP setting (but with a slightly different linear model).  Moreover, the policy-based optimization techniques have been used similarly in \"Dynamic Regret of Policy Optimization in Non-stationary Environments\".\n\n### Quality:\n\nThe submission is technically sound and the theoretical claims are well-supported.  The authors are upfront about the underlying assumptions required for the models, including:\n- known linear feature representation\n- underlying assumptions on the orthnormal basis assumptions which has been used previously in the literature.  \n\n### Clarity:\nThe submission is well-organized and well-written.  I particularly enjoyed the discussion after Theorem 4.2 on the three different regimes of $P_T + \\sqrt{d} \\Delta$ and the proof sketch provided for theorem 3.1\n\nThe only slight confusion is in the description under \"Our contributions\" it is unclear whether the regret bounds provided are for policy optimization or the LSVI-UCB approach (however this is obviously resolved later with the explicit theorem statements).  Moreover, in the related work sections it would be helpful to highlight which of the existing algorithms utilize these sliding windows, restarts, or policy-based optimization techniques.  Lastly, there is a minor typo on page 1 under \"most the existing works focus on the ...\"\n\n### Significance:\n\nUnder my understanding of the related literature, the theoretical results extend existing algorithmic techniques for non-stationary MDPs to the linear function approximation setting with additional worst-case mini-max regret guarantees.  Some of the \"novel\" techniques in the paper seem a bit overstated, but hopefully the authors can comment in the rebuttal phase to better express their paper in the existing literature.\n\n### Strengths:\n\nThe main strengths of the paper are as follows:\n- novel minimax lower bounds on the worst-case dynamic regret for any algorithm in terms of the dimension, time horizon, and non-stationarity in the underlying problem\n- extended existing policy improvement and value iteration approaches for non-stationary environments to the linear-MDP setting\n\n### Weaknesses:\n\nThe main weaknesses of the paper are as follows:\n- lack of experimental contributions highlighting performance of the algorithm\n- limited technical novelty (or at least, the technical novelty could be better described in relation to the existing literature)\n\n### Questions:\n\n- It seems that the periodic restarts are necessary for policy-based versus value based approaches? Could a statement of this form be formalized explicitly?\n- Can the expressions $B_T$ and $B_P$ be related to the underlying drift on the rewards and the dynamics? Moreover can $P_T$ be expressed as a function of $B_T$ and $B_P$?\n- Can the authors comment on the relationship between the algorithmic approaches considered here (sliding window estimates, period restarts, policy-optimization techniques) to the existing works in the literature?",
            "summary_of_the_review": "The paper focuses on developing new algorithms for reinforcement learning in non-stationary MDPs with linear function approximation.  They establish min-max lower bounds on the dynamic regret in this setting, and nearly match them with simple policy-improvement based algorithms connecting back to online mirror descent and optimistic value iteration approaches.  While the paper is excellently written and the theoretical results are sound, the authors overstate some of the 'novel' developments.  In particular, many of the algorithmic approaches have been used previously in the literature, and this paper just extends them to include linear function approximation.  The authors should better expand their technical contributions, connect their paper to the related literature, or include numerical experiments to help give more contribution in this area.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper studies dynamic regret minimization in non-stationary linear MDPs, i.e., MDPs that have linear dynamics (and rewards) and change gradually over time. The authors propose an optimistic policy optimization algorithm with two additional mechanisms to handle non-stationarity: sliding window in the policy evaluation and periodic restart in the policy improvement. They prove upper bounds on the dynamic regret and a nearly matching lower bound.",
            "main_review": "**pros:**\n\n the paper is clearly written and studies an interesting topic - policy optimization in non-stationary MDPs.\n\n\n**cons:**\n\nThe paper does not compare its results to the paper \"Non-stationary Reinforcement Learning without Prior Knowledge: An Optimal Black-box Approach\" by Chen-Yu Wei and Haipeng Luo (best paper award in COLT 2021) that studies the exact same setting and gets the best results that I am familiar with! I think that it is impossible to judge this paper without a clear comparison to state-of-the-art. Were the authors aware of the paper by Wei and Luo?\n\nI am not an expert in non-stationary MDPs, but I attempted to make the comparison myself. It looks like the dependence in $d$ and $H$ in this paper is slightly better, but it requires prior knowledge of $\\Delta$ unlike Wei and Luo's paper which is a major drawback since in real applications there is no reason that this parameter will be known in advance. Another drawback is Assumption 4.1 which seems a little restrictive. However, I admit that I do not fully understand it and that I am not sure if Wei and Luo make the same assumption. Can the authors please clarify this assumption and whether it is also made in Wei and Luo's paper?\n\nStill, this is the first paper I know that studies policy optimization in the context of non-stationary MDPs. This is an interesting contribution but it needs to be put in perspective against Wei and Luo's approach. Here are a few questions that I think must be answered: what are the advantages of using policy optimization instead of previous approaches? What is the relation between the regret bounds of policy optimization and these approaches? How does the combination of sliding window and policy restart affect the analysis of policy optimization (there is no analysis sketch in the main paper!)? What happens if we simply use policy optimization without the additional mechanisms? Does it work well in some regimes?\n\nAnother thing that requires clarification in my opinion is the regret bound in theorem 4.2. It has dependence in $P_T$ that I could not find in the paper of Wei and Luo. Could the authors explain where is this coming from? Clearly there is a connection between $P_T$ and $\\Delta$, is there a way to make it explicit or to bound one of them in terms of the other?\n\nOne final thing that bothered me is the initial state that is chosen by an adversary. As far as I know, policy optimization cannot handle adversarial initial state. Am I missing something here or must the state be drawn i.i.d from some initial distribution?",
            "summary_of_the_review": "The paper studies an interesting topic but is missing comparison to state-of-the-art which makes it impossible to judge currently.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}