{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The main premise of the paper is that building a (global) approximation a black box model using simple interpretable functions results in a model which is more useful for the purpose of interpretability. In particular, the resulting approximation is useful for 1) understanding feature importances 2) analyzing feature interactions and 3) scientific discovery by revealing underlying laws governing the observed data\n\n\nTheir approach is based on the Kolmogorov superposition theorem, and results in a multilayer network (which they call a tree) formed from the composition of primitive parametrized univariate functions. This is learnt using a combination of gradient descent (for optimizing the parameters) and genetic programming (for searching over tree configurations).\n\nThey show experimentally that their approach results in reasonable approximations.\n",
            "main_review": "My main concern about this paper is that the main premise (that the  approximation results in better interpretability) is not justified. While they discuss how various applications (such as clinical applications) require interpretable models, they do not actually show that the resulting \"tree\" is interpretable by clinicians (or even other ML researchers). Incorporating simple functions into a multi-layer network does not necessarily preserve the simplicity/interpretability of the primitive functions. Even when restricted to just ranking features by importance, it was hard to see if their approach was significantly better than other approaches. \nThe process of generating the approximation is based on GP/GD - it is unclear how effective this is. Does this process \"overfit\" in that it results in more complicated (and less interpretable) solutions if allowed to take arbitrary computational resources? Is the regularization term (lambda E) effective in producing more intepretable models ?",
            "summary_of_the_review": "Overall, its not clear that the paper is proposing a mechanism that results in significantly interpretable models, nor does the GP/GD procedure proposed seem to be sufficiently important / novel to justify publication on its own.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Following several recent works, the paper proposes to learn an interpretable proxy (called a symbolic metamodel) of a black-box ML model. This proxy can then be used for its intrinsic interpretability or for deriving local or global feature rankings. The originality of the approach lies in the way the metamodel is defined and trained. A two-level tree is used to represent mathematical expressions (motivated by Kolmogorov's superposition theorem) that combine simple univariate primitive parameterized functions. The metamodel is trained by combining gradient descent to tune the parameters of the primitive functions and genetic programming to learn the tree structure. The approach, called SMPF (\"symbolic metamodeling using primitive functions\") is compared empirically against several related methods.\n",
            "main_review": "The topic is highly relevant and timely. The paper is well written and easy too follow.\n\nThe novelty of the approach is limited with respect to the SM and SP methods papers, i.e., (Alaa & van der Schaar, 2019) and (Crabble et al, 2020). The general idea of the approach (training a symbolic metamodel, whose structure is inspired by Kolmogorov's superposition theorem) is similar in all three works. The novelty here is how the metamodel is parameterized (using a subset of predefined parameterized primitive functions) and how it is trained (using a mix of gradient descent and genetic programming).The algorithm is globally sound but it is not very original with respect to existing symbolic regression methods based also on genetic programming.\n\nThe main limitation of the paper is that the authors do not really convincingly show that the modifications are important with respect to SM, SP and SR. The experiments focus almost exclusively on the evaluation of the predictive performance of the different metamodels and not really on their benefit in terms of interpretability, which is the main incentive for these methods (see my detailed comments below). \n\nAnother limitation of the algorithm is the fact that it introduces a lot of new hyper-parameters, including the choice of the primitive functions. The impact of these hyper-parameters is not really assessed in the paper and they were furthermore set differently in all three experiments, suggesting that setting them properly is crucial. This is an important drawback in practice, especially for a method that is supposed to bring explainability.\n\nThe general idea of training a surrogate symbolic metamodels to explain a black-box is an interesting idea in general. But actually, I don't see why the black-box model is needed here. From what I understand from the experiment on the real dataset (Section 5.3), the black-box is only used to provide new output values for the existing training set examples and thus it would be possible to train the SMPF metamodel directly from the original data. Why is then the approach branded as a post-hoc interpretation method and not as an intrinsically interpretable model? Is there any benefit in terms of predictive performance or interpretability to train from the black-box predictions? I know that this comment applies also to the SM and SP methods, but my feeling is that the paper should answer this question.\n\nOverall, the specific implementation proposed in the paper is interesting and it has a clear potential with respect to SP and SM but I think that, for a third paper on this topic, the experimental part should have been stronger to really highlight the improvement in terms of interpretability with respect to previous works.\n\nDetailed comments:\n\n- In Experiment 1, the authors claim that SMPF has a better accuracy than the other methods and is able to identify the functional form in most cases but actually no results are provided, neither in the paper, nor in the appendices (although results are provided for other sets of function in Table 4 and more complex functions in E.1). We have to trust the authors that it's really the case.\n- Despite the focus on interpretability, no symbolic metamodel is shown, except one in 5.1. In this specific example, why is the model produced considered satisfactory? It looks much more complex than the original model (despite the fact that the choice of primitive functions is ideal). One needs also to show metamodels produced by the other methods for comparison (only one model produced by SP is shown in 5.1). In particular, I'm curious to see what kind of formulas are produced by SR, which is the most flexible method.\n- In Section 3.3, the authors claim that inspection of survived trees could be informative, in particular to highlight interactions between variables. Again, this possibility is not illustrated in the experiments.\n- I know that the idea of the experiment in 5.2 comes from (Alaa & van der Schaar, 2019) but I don't find this experiment very relevant. Computing instance-wise feature importance with partial derivatives of the metamodel does not make sense when it is possible to compute these partial derivatives directly from the original black-box model, which is the case with neural networks. At the very least, feature importances derived directly from the black-box should be compared to the one derived from the metamodel.\n- Results in Figure 3 are inconsistent with the results in (Alaa & van der Schaar, 2019). For example, L2X is the best method on the XOR problem in (Alaa & van der Schaar, 2019), while it fails here. The same difference appears in the case of DeepLIFT on the nonlinear additive problem.\n- The text in 5.2 says \"we find the median feature importance ranking\". Do you mean the median feature importance ranking of the relevant feature on each problem (because the median feature importance ranking over all features is a constant)? I think that they are irrelevant features on these problems that should ideally not be selected in the black-box model. This should be checked.\n",
            "summary_of_the_review": "Overall, the specific implementation proposed in the paper is interesting and it has a clear potential with respect to related works but I think that the experimental part should have been stronger to really highlight the improvement in terms of interpretability with respect to previous works.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes an algorithm for approximating general multivariate functions. With this approximation, the authors aim at approximating a trained (black-box) machine learning model and use their approximation for interpretation.  The algorithm produces a composition of univariate simple parameterized functions. This composition is selected from a collection of tree configurations each of which represents different relations between primitive functions and the variables (features). The parameters of the univariate functions used in each tree are obtained by gradient descent (training phase). As for selecting the best tree, a variant of genetic algorithms is applied. The authors have also compared their algorithm against other methods from the literature on a set of synthetic and standard data sets. ",
            "main_review": "The manuscript is clearly written, and the approach is interesting. However, I have had difficulties in assessing its novelty, particularly along the following lines:\n\n1) The authors aim at giving an interpretation to a black-box model. However, their approach seems only a function approximation method (methods similar to theirs have already appeared in the literature). I do not see, how using simple parameterized functions (even though they are provided by the domain experts) in the approximation makes their method more interpretable? Take for instance the final model in Section 5.1. The resulting expression is highly nonlinear, and hence, difficult to interpret. Overall, I would appreciate if authors can provide clarification about \"interpretability\" gained with their approach. \n\n2) The authors are trying to approximate a trained model by solving (2). Can one also use the similar steps to come up with a new machine learning algorithm by solving $$\\min_{g \\in \\mathcal{G}} \\sum_i \\ell(g(\\mathbf{x}_i) - y_i),$$ where $g$ is defined in (5). If this would work and the resulting model is considered to be interpretable, then would we obtain a new inherently interpretable machine learning approach?\n\n3) The proposed algorithm uses many hyper-parameters. In addition to difficulties with tuning them, these parameters also raise a concern about the robustness of the resulting models. If we change the data or the parameters slightly, how would the resulting model obtained with the proposed approach change?\n\n4) Does Kolmogorov superposition theorem (KST) work when we specify the set $\\mathcal{G}$? In other words, when the user selects several simple parametric functions, can we guarantee that KST still holds?\n\n5) In the last section, and also in a few other places including the appendix, the authors state that they can use more complex functions and trees to approximate certain black-box models. Then, how would this affect the interpretability of the resulting model from your approach?",
            "summary_of_the_review": "The paper may have a potential, but in its current state it does not provide convincing arguments about its novelty and its major claim about the interpretability of the proposed approach. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Authors propose a method to perform symbolic regression (SR) through a blend of Genetic Programming (GP), the Kolmogorov Superposition Theorem (KST) and gradient optimization. The aim of their method is to develop a black-box interpretation methodology, by approximating black-box models' behavior through their particular flavor of SR. Authors claim that their approach always yields interpretable models, because it relies on a small set of simple, but somehow enough, functions from which symbolic expressions (that adhere to the functional form of KST) are built. Authors test their approach in three different contexts against a set of other symbolic approximation methods, most of them also geared towards black-box interpretation.",
            "main_review": "The paper is, in general, well written and organized. Language is clear most of the times. Maybe math notation is overused in some places, but other than that, the paper is easy to follow and understand.\n\nThe approach proposed by the authors is novel, such that I consider it worthy of acceptance in that regard. The empirical assessment, on the other hand, is not thorough enough, which makes it difficult for me to reach an overall recommendation rating.\n\nOne way or another, there are some points that require clarification in the draft, before I can fully recommend its acceptance. I will enumerate them in order, from what I consider the highest priority, to the lowest:\n\ni. Incorporating _tunable_ coefficients into GP trees is nothing new. Please describe your algorithm in proper terms, such as \"memetic algorithm\", \"local search enhanced GP\", etc. with the corresponding references.  And being a _nature-inspired_, evolutionary algorithm, as it is, in GP literature there have been attempts at defining a taxonomy for these kind of algorithms, _Lamarckian_ or _Baldwinian_. Please specify the type of inheritance system used by your approach (_Lamarckian_ I believe), and how such decision could affect the performance of your method. Refer to [1] for more information.\n\nii. In Sec. 6, authors specify that their approach is computationally more efficient than Symbolic Pursuit (SP); but there is no mention regarding the other baselines they compared against (vanilla GP and Symbolic Metamodeling (SM)). Please remark -if that is the case- that your approach and both of those other methods were given the same computational resources (exec. time-CPU threads). If that is not the case, and either of those methods were given less amount of resources, then results would be meaningless.\n\niii. The major problem I see with the paper is the experimental study. The whole idea behind KST is to approximate high dimensional functions through a sum of single-variable ones. Therefore, I'd have expected authors to provide an ablation study regarding the dimensionality of the problem being approximated, in the search for evidence that the proposed methods achieves better results than conventional GP or other regression methods, when faced with increasingly higher dimensional problems. Authors could assemble a battery of different, toy-artificial, symbolic regression problems, such as the ones used in Sec. 5.1, but with different number of input variables, say 4, 6, 8, 10, with five problems for each problem size, and then study how each method's performance degrade over higher dimensional problems. I'd would expect that their proposed approach, which performs a more \"organized\" search (thanks to the KST) could be more resilient to higher dimensional problems (i.e. achieve better results, given the same computational resources).\n\nIV. I think the procedure or intuition for selecting  $l_1$ and $l_2$ is crucial, and should be bring upfront to the main text, not just casually mentioned in the appendices. If I understood correctly, neither crossover nor mutation have the ability to increase the number of middle nodes, thus the importance of such parameters.\n\n\n1. Emigdio, Z., Trujillo, L., Schütze, O., & Legrand, P. (2014). Evaluating the effects of local search in genetic programming. In EVOLVE-A Bridge between Probability, Set Oriented Numerics, and Evolutionary Computation V (pp. 213-228). Springer, Cham.\n ",
            "summary_of_the_review": "The paper is well written and the proposed approach is properly framed within its aimed scope (blackbox interpretability).\nThe proposed method is novel and interesting enough to deserve publication. The intuition and motivations behind it are clearly explained through the draft.\nThe experimental assessment is lacking and sparse. I'd recommend authors to discard study from Sec. 5.2 which is kind of redundant with the other two studies, and instead focus on increasing experimental evidence for sections 5.1 and 5.3.\nI may improve my score if some missing definitions and references are properly cited (see point (i) of my complete review).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concerns raised my attention during this review.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}