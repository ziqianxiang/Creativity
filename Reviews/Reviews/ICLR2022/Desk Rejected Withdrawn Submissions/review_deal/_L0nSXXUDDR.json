{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes an additional loss function that encourages consistency of the predictions between invidual training samples and their nearest neighbors. The nearest neighbors are obtained in the feature space of a trained deep network. The consistency is applied for each sample x by minimizing the KL divergence of the sample's neighbors' mean predictive distributions from the predictive distribution of the sample. The method shows comparable results to that of label smoothing and bootstrapping. When combined with MixUp and data augmentation it achieves state-of-the-art results on both synthetic and real-world noise datasets.",
            "main_review": "**Experiments**\n- (+) the results in Figure 3 and 4 are quite interesting. Could be good to provide a qunatiative measure for NCR as well as for other baselines.\n- (+) it seems NCR and mixup are complementary to some extent.\n- (-) try reverse KL and CE in the absence of arguements as for why KL is better than CE and why this specific direction is more suitable than its reverse. \n- (-) there are quite a few hyperparameters: \\alpha, e, K, T, etc.  \n- (-) there are several methods for robustness to label noise including those that use graph networks for label propagation but also many other families of methods including those that only change the loss function. It is not clear how the compared baselines are chosen.\t\n\n\n**Novelty and Significance**\n- (+) the paper can be seen quite original if compared with label-propagation baselines, however the main experiments are not designed for such comparsion and more importantly there is no side experiments from the angle of label propagation, despite the method section starting on that note. \n- (-) the novelty of the method seems quite limited in the context of various methods including MixUp, AugMix[b], DivideMix, GJS, PropMix [c] and [a].\n- (-) the obtained results seem to be too close to those of mixup and sometimes inferior to mixup. \n- (-) in general, the final results for Plain NCR (which is the most interesting) is quite on par to the baseline methods.\n\n**Questions**\n\n- in equation 3, why is z_i input to the NN_k? Are the final logits used to determine the nearest neighbors? v_i or x_i should make more sense here. What is the exact procedure to find the nearest neighbors? It does not seem to be stated, at least in the main paper.\n\n- KL divergence is an asymmetric divergence, why is the specific direction chosen over the reverse KL in equation 3? \n\n- in equation 3, why not use the cross entropy instead? The current KL formulation would encourage a high entropy predictions (compared to CE). \n\n- does the other neighboring samples come in the same mini-batch? If so, does this affect the diversity of the mini-batch and consequently various aspects of the optimization? If not, how is the loss in equation 3 evaluated? In either case, there should be a higher computational cost needed due the additional forward passes, is that correct?\n\n- there are quite a few hyperparameters: \\alpha, e, K, T, etc. Furthermore, optimal hps seem to vary quite a bit from the appendix in table 4how are these optimized? \n\n- in Figure 2, \\alpha=0.7 seems to be a local minimum. It is not immediately clear why that can be the case. Would have been good to include \\alpha=0 and \\alpha=1 in the same plot for all graphs.\n\n- if one does not consider the final improvements in results as a significant advantage of the proposed method, what can be taken as the advantage? Are there additional experiments that can be presented (beside the final accuracy) where the advantage is clearly shown over the previous methods?\t\n\n\n**Minor comments**\n- \"We set the self-similarity si,i = 0 so that it does not dominate the normalized similarity\": does it mean that NN_k contains that sample itself? So, effectively K-1 neighboring samples are used for a chosen k?\n- In Table 1, it might be better to separate the results with and without mixup. It is interesting to show that mixup is complementary but would be easier to first compare the plain method against the plain baselines.\n- Connection to [a], [b], [c], [d], and GJS are need in section 4.2 where connections to other loss connections are discussed.\n- missing references\n\n[a] \"Unifying semi-supervised and robust learning by mixup\", ICLR LLD workshop 2019\n\n[b] \"AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty\", ICLR 2020\n\n[c] \"PropMix: Hard Sample Filtering and Proportional MixUp for Learning with Noisy Labels\", BMVC 2021\n\n[d] \"Augmentation Strategies for Learning With Noisy Labels\", CVPR 2021\n",
            "summary_of_the_review": "The paper puts forward a plausible and simple method especially compared to label propagation techniques. However, there are several concerns as listed above so I am leaning towards rejection as my initial rating.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces a noisy-robust learning algorithm using a neighbor constraint. The proposed algorithm can determine neighbors online without having to compute the distances after learning whole training data. Therefore, it is possible to learn a network that is strong against noise without much additional time. This paper shows that the proposed algorithm improves the classification performance with augmentation methods such as Mixup. A comparison with the SOTA works is also presented.",
            "main_review": "\nPros\n1) Although the method is simple, it works well with noise data.\n2) The thesis seems to be well written and easy to understand. The appendix also explains implementation details in detail, so reporoduction will be easy.\n\nCons (and Questions)\n1) I have a question as to whether the proposed method properly works. In fact, if the supervised loss is used, the noise label data is also used, so isn't it inevitably learning the wrong noise label information? So the distance to determine the neighbor is also from a measure in the feature space learned with the wrong noise label. If so, doesn't the distance obtained here also have noise? For example, suppose that data sample A is correctly labeled class 1, and data sample B is a noise that is incorrectly labeled class 1. Then, A and B will be located close by the supervised loss, and I think that it will find the wrong neighbor.\n\n2) The experimental results are not satisfactory. Although the proposed method improves the classification performance in the ablation study, comparison with SOTA is not impressive. As the authors claim, even if methods such as ELR, DMix, and MOIT are not online and take a lot of time to learn a model, the performance gap is too large. In the CIFAR-10 and CIFAR-100, there is a difference of more than 30% at the 80% setting, which is too big a performance gap. If there is a performance difference of this level, it seems to choose a method other than the conventional non-online method, even if it takes more time. In addition, it seems that the performance of existing methods will increase even more if Mixup and DA are used. So the results of the mini-WebVision and Clothing datasets are also limited. If you use Mixup and DA, it seems that performance should be good with a larger margin.\n\n3) There are minor mistakes. In Table1, it says \"D-Mix\", but in Table2, the hyphen is missing as \"Dmix\".\nneighbors or neighbors (Most of them are spelled “neighbors”, but some say “neighbors”. It’s better to use one.)\nunlabeled or unlabelled (They are also used interchangeably. Choose one and use it.)\nto minimized -> to minimize (This is a typo.)",
            "summary_of_the_review": "This paper has two major cons. There is no clear explanation or background for why the proposed algorithm works well. There will also be noise in the distance of neighboring samples, which doesn't make much sense to just use it. It seems that the experimental results also need to be improved. Addressing either of these two shortcomings could change my mind. It is not possible to perform new experiments for the second con in a short period of rebuttal, so it would be good to explain the first con well in rebuttal period.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I do not find any ethics concerns in this paper.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "To tackle label noise, this paper adds a neighbor-based regular term NCR to the loss function. Comparisons with some existing methods made on both synthetic and real-world datasets show that NCR performs better on some datasets.",
            "main_review": "This paper proposes a neighbor-based regularization NCR. It utilizes the KNN and mini-batch datasets to evaluate the similarity of instances. With the assumption that the features are relatively fixed, this regularization can help the model ignore noisy labels to a certain extent.\n\nStrengths:\n- NCR is technically sound under pre-set assumptions.\n- Paper is well organized and clearly stated. It is easy to follow the idea.\n\nWeaknesses:\n- The noisy label setting in the experiment is not reasonable enough. The experiment only considers the uniform distributed noises. In the reality the noises are usually different on different classes, some of them are easy to be recognized but others may hard to distinguish. The experiment should simulate these different cases to show the robustness of the proposed method.\n\n- The comparison in this study could recruit more existing methods. Many baselines are loss function-related. However, from the aspect of an application, we cannot ignore other types of noisy label learning methods, such as training sample selection, and so on. The proposed method should also be compared with them to show its practice value.\n\n- The proposed method has a relatively strong pre-condition that a pretrained backbone part (feature extraction) should be obtained. Usually, the pre-training requires a large number of clean data, which ensures that the feature space can ‘recognize’ the similar instances without labels. NCR can only work with a pre-trained backbone to optimize the classifier (maybe has a little influence on backbone). Thereby, this weakens the significance of the proposed method.\n",
            "summary_of_the_review": "This work is technically sound and well written. However, it still needs to be improved on baseline selection and experiment settings. And, the pre-trained backbone assumption may be too strong in noisy label learning scenario, which weakens the importance of the work.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}