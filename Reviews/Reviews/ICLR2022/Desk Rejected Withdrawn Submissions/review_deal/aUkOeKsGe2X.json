{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents a method to mitigate domain shift effects, allowing training on synthetic images for real tasks. The paper presents a list of incremental improvements: using a feature extractor, augmenting data with random backgrounds, and introducing a triplet loss to the embedding.",
            "main_review": "I found the paper to be interesting, and I enjoyed the progression presented by the authors, from simple to more complex solutions, along with an explanation of when each step fails and why the new improvement is needed.\nThe authors also plan to release their code and their new dataset, which is always a positive.\n\nI believe the greatest weaknesses of the paper are result quality and generalization to other tasks, as detailed below.\n\n- The authors state that \"MPI3D and the vehicle interior share interesting properties: they have almost identical backgrounds and the environment is more tractable than many computer vision datasets\" --- does this mean that the conclusions of the paper only apply to such datasets? I would have liked to see an evaluation on a dataset with different properties.\n- More generally, the method is presented as a general purpose autoencoder paper, but in fact it feels more like a car occupancy detection paper. That is not necessarily a bad thing, but if the authors are trying to propose a general approach, they should test on more datasets.\n- Fig3: The authors should also show reconstruction of synthetic data. Currently, the reconstruction is not great, but there's no way of knowing if it's due to the domain gap, or due to the fact that the autoencoders are not performing well even without a domain gap.\n- Fig4: it's hard to see the difference between (c) and (d) in terms of amount of clustering. The authors should calculate a quantitative measure of clustering to support the conclusion of this figure that (d) is better.\n- Table 1: the comparison is between reconstructions of real and corresponding synthetic training images. But this doesn't tell the whole story. If our goal is to create a good autoencoder for real images, the authors should also compare the reconstructions of real to ground truth real images.\n- \"For the classification evaluation we used all the images from all the different vehicles, but we used training images only\" --- I did not understand this statement and would like a clarification.\n- Some comparisons are against fine-tuned classification models. More details regarding the fine-tune procedure are required. Did it use the same amount of processing as that used by the proposed method? Did it use the same data?\n- Fig5: classes are often not preserved (even for (g)). Is this something which can be measured and reported? (It is related but different from the classification accuracy in the downstream task.) In any case, I think a discussion is in order here --- pointing out the bad reconstruction results and trying to define when reconstruction is expected to fail (i.e., what are the hard cases).\n\nOther comments:\n\n- Table 1: It is common to bold / underline the best result. Please do so for Realistic-FactorVAE-L1 which has a better (smaller) L1 compared to the proposed method.\n- Table 2: Resnet-50 E-TAE is better than II-E-TAE. This should be discussed and marked correctly in the table.",
            "summary_of_the_review": "I am not convinced that the conclusions of the paper are general enough and apply to various problems and datasets. I also did not find the result quality compelling. (Specifics above.) That being said, I think the authors describe an interesting process and propose several useful techniques (which are not necessarily new, but can be considered somewhat new in the proposed context).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "A new autoencoder training scheme is proposed to improve the image reconstruction performance by only using synthetic data for training. It consists of three strategies: 1. Use a pretrained image classification model output as the input to AE. 2. Create a more diverse training pair within the same object/class configuration. 3. A triplet loss is applied to learn class-specific features from synthetic data. Superior reconstruction performance of this new approach is reported on MPI3D against baseline VAE models. This proposed AE also outperforms some fine-tuned classification models by incorporating KNN classifier and a new proprietary dataset on SVIRO dataset.",
            "main_review": "The writing of this paper is generally clear and the major technical details (e.g. Section 3) are presented in an organized way.  I also like the authors' effort in demonstrating various experiments that include different metrics and settings. They help readers obtain a better understanding of strength/limitation of the proposed method. However, there are still concerns in this work as follows.\n\n- Novelty & Insight. There are many micro-innovations but what is the major contribution and take-away message? The new sampling strategy (Sec. 3.2) and triplet loss (Sec. 3.3) are commonly used techniques in data augmentation and metric learning. Sec. 3.1 essentially argues a class-specific feature is a better input than raw image pixels for AE but the triplet loss is inherently learning the class information. So which is the true driving force to improve the performance here? Whether a good pre-trained model is universally helpful for synthetic AE training? What are the constraints we should take care to set up training pairs X_a and X_b in Equation (2) and (3). Some of the experiments may provides implications to those questions but the authors need a clear presentation of technical insights.\n\n- Practical value. Developing an AE to reconstruct real images is not a quite interesting problem since we can always use a myriad of real images to train a model for identity mapping. As a result, Sec 4.1 ~ Sec. 4.6 are more like a study to inspect an AE method itself without too much practical value. The classification experiment (Sec. 4.7) shows a practical application of the learned II-E-TAE representation but it is pretty difficult to conclude that the proposed AE framework is able to help classification task in general, not only because the comparison classification models are frozen in first few layers during fine-tuning but also the evaluation benchmark is small and limited. \n\n- Missing details. 1. What is the summarization module to reduce convolutional filters in Sec. 3.1? 2. Are comparison classification models in Sec. 4.7 trained with regular FC + softmax?\n",
            "summary_of_the_review": "The paper is written in a good shape and the experiments have shown some good merits of this approach. The technical contributions are weak and the practical value of this work needs further clarification, which limits my support of this submission. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Led by the ambition to reduce costs and increase safety in data acquisition with data generated from virtual worlds, the authors of this paper demonstrate a way to utilize synthetic data for training machine learning models such that their generalization capabilities allow for application to semantically similar real-world domains. Notably, this is by excluding the use of \"real-world\" data for the specific task in the training procedure entirely.   The proposed method uses an encoder-decoder neural network structure that is closely aligned to the common auto-encoder training making use of the latent-representations encoding of the bottleneck layer that is implicitly specified by image reconstruction. The authors illustrate three key alterations to this setup that enable them to gain invariance towards specific inductive biases common in domain shift from simulated to real data, which nevertheless do not affect the actual task at hand.\nThese are:\n1) Abstraction of the input space by exchanging the immediate image input with a preceding feature extraction. The goal is to gain a unified mapping of the input features into a common feature space. Specifically, pre-trained classifiers are used. (e.g. VGG-11).\n2) Initially proposed by Dias Da Cruz. Et al. (2021), the authors are making particular use of their simulation framework to specifically permute only certain parts of their images, such that the semantics are preserved, and the inductive biases to be expected are flagged to be unimportant features. In a first setup this included aspects such as illumination and background scenery, however, also variations of objects that are integral to the task are considered.\n3) A \"triplet-loss\" is included to regularize the latent-space representation to further explicitly encompass class affiliations. The authors declared that this addition further helps generalization and also allows for linear classification from the latent space. In later results the paper presents how all the above aspects add value to the generalization capabilities of the final latent representation. Also, an increase in performance for the specific task of passenger classification by a \"simple\" k-nearest-neighbor approach (on top of the gained latent representation) over their state-of-the-art deep neural network complements was reported.",
            "main_review": "\nThe main contribution of this paper is the utilization of simulated examples to build latent representations that are invariant to expressible inductive biases in combination with the algorithmic description necessary to process this information. In the long run, I could imagine this approach to be more fruitful than trying to closely match synthetic data to the real world's image statistics, because it could significantly reduce the amount of examples to be generated.\n\nEven though the authors deemed it important, I am not sure to what extent addition 1) is actually all that necessary, and I think of it more as a implementation detail side-note. The remaining steps 2) and 3) are already, almost alike, presented in the previous work of  Dias Da Cruz et al. (which is cited in the paper) who first described the \"Impossible Encoder-Decoder Loss\" that is mainly driving this approach. They also made use of the triplet-loss regularization to infer structure into the model's latent space although with a slightly different immediate goal.\n\nFrom an application point of view, interior monitoring is done on near-infrared images (as described in the paper with the proprietary dataset).  In this application context, the NIR sensor choice is already made to address getting a degree of illumination invariance. This sensor coupled with traditional techniques such as background subtraction/tracking may be enough to get a reasonable hypothesis for objects in the seat and to then subsequently use a classifier.   I am curious what the performance difference is between the traditional approach to the current proposed approach.   Another addition that I would have liked to see is how this setting behaves with colored images. The authors themselves mention that image complexity is an issue to be kept in mind.  \n\n\n",
            "summary_of_the_review": "The strengths of the paper is the step by step design and the illustration of learning of invariant representations from simulated data and do transfer to real settings.  While I like the paper and its contributions, the core ideas in the paper are largely articulated on Dias Da Cruz et al.  The present work extends the past work in a different domain and is thus incremental.   Moreover, the application chosen does not have enough complexity to validate the generality and effectiveness of the method proposed. \n\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper focuses on the questions of how to train auto-encoder neural networks on synthetic data only and make them more generalizable on real test data. The paper introduces a set of analysis setups and proposes a number of modification and training strategies for Auto-encoders to achieve this goal. Those include: an \"extractor autoencoder\" architecture which uses pre-trained features from classification networks as feature extractors, sampling a target image of a different scene, but of the same class as reconstruction targets and replacing background to reduce the model bias. The paper introduces several experiments on two datasets, MPI3D and SVIRO/TICAM, to justify the method design choices.",
            "main_review": "Strength:\n\n- The paper in general is clear and well written.\n- The paper describes the proposed method, justifies the design choices and analysis effects of each choice very well.\n- The paper focuses on training on only synthetic data (without any fine-tuning) and testing on real data which is a challenging but very practical scenario.\n- The paper presents an empirical study and analysis of design choices of both training data and auto-encoder network architecture that is needed to make training with synthetic more generalizable. This hybrid approach of studying both training data and network design choices is very interesting.\n- The presents two steps in the experiments, starting from the more simple MPI3D dataset but with matching real data, and moving to a more realistic task of passenger classification on SVIRO/TICAM dataset.\n\nWeaknesses:\n\n- The main weakness is that it is not very clear in the paper which contribution are new in this work and which are from other works. For example, the partially impossible reconstruction loss used in the  I-E-AE  variant is introduced in (Dias Da Cruz et al (WACV 2021)) and the triplet loss was introduced in (Hoffer & Ailon, 2015). \n\n- Additionally, the idea of using a pre-trained VGG network to extract features has been commonly used in literature before. At least it is not clear how it is different (conceptually or performance wise) from pre-training the auto-encoder using large dataset of real images. It appears as if the use of feature extractor is equivalent to pre-training the network using a large image set which is well known to help performance. Also the other AE variants (VAE, Beta-VAE ..etc) could take advantage of the pre-trained feature extracted and compared to the pure AE.\n\n- The paper proposes an extension to the Impossible reconstruction loss idea from using different illumination pairs to using different object pairs from the same class (II-E-AE variant). However, this idea implicitly assumes that the data distribution for each class is uni-modal, i.e. all objects of the same class should map to only one point in the latent space regardless of their details. This can be true in some tasks but might be too restrictive or detrimental in some tasks where having multiple regions to represent objects of the same target class is useful. This  basically turns . This interpretation and consequences of using this loss are not discussed in the paper.\n\n- The paper limits its study to using auto-encoder models only eventhough the problem used is a classification problem. This choice is not well motivated or explained in the paper. Do findings about data augmentation translate to other classification models? How do the auto-encoder model on passenger classification compare to other classification networks?\n\n- The paper uses several data augmentation ideas but does not compare results to previous augmentation methods like domain randomization.\n\n- The results in Figure 5 are not possible to evaluate qualitatively (visually) since non of them appear to look similar to the input.\n\n- End of Page 4: \"an abstraction from toy to real images cannot be achieved by means of simple data transformations or model constraints\". Is there evidence for this statement ?",
            "summary_of_the_review": "Overall, the focus on the paper is not very clear. If the focus is on solving the task of passenger classification using synthetic data then comparison to other state-of-the-art models or using real labeled data is needed. If the focus is on data augmentation techniques, then comparison to other proposed data augmentation methods is needed and on a boarder range of tasks. If the focus on making Auto-encoder latent space more generalization to real data, then an analysis of proposed contribution compared to other AE models is needed using more fair setups to all other models ( ex. pre-training for all models).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}