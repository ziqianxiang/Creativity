{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a method, GBN, to train classifiers robust to $\\ell_p$-adversarial attacks for different $\\ell_p$ at the same time. In particular, the authors suggest that the statistics in the batch normalization (BN) layers should be different for clean images and for every type of perturbations. Then, the BN layers in standard architectures are modified to have multiple branches, one for each type of input which is shown at training time. Moreover, gated sub-networks are learnt jointly to the main classifier to distinguish at inference time to which type of perturbations each input belongs and then use the corresponding BN branch. Experiments are carried out on many datasets and with different architectures.",
            "main_review": "Strengths\n- The intuition that a different BN branch should be used for each type of input is interesting, and in line with prior works which however didn't consider robustness against multiple perturbations.\n\n- The resulting method is a simple modification of existing architectures and presented very clearly, as well as the whole paper.\n\n- The authors present experiments on many datasets and architectures, and several ablation studies to support the proposed method.\n\nWeaknesses\n- My main concern is about the results reported for the baselines for robust models: in Table 1, the robustness of models trained with only one $\\ell_p$-norm is much lower than expected (see e.g. https://robustbench.github.io/ and [A]), and often even lower than what achieved by the methods for multiple perturbations. Similarly, the results of AVG, MAX and MSD are 10-20% lower that what reported by Maini et al. (2020) while using the same threat model. These observations hold for the results with other architectures as well. Which is the source of this difference? Additionally, I think there are some missing baselines, e.g. the methods from [B].\n\n- The evaluation includes several attacks, but the strongest one for $\\ell_\\infty$, AutoAttack, is not used for $\\ell_2$ and $\\ell_1$, although there are versions of it for both cases (see also [A]).\n\n[A] https://arxiv.org/abs/2103.01208 \\\n[B] https://arxiv.org/abs/2006.12135",
            "summary_of_the_review": "Overall, the method proposed is interesting but the results do not support it sufficiently.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper uses a union of L1, L2 and L-infinity norm perturbations as the threat model. The authors show significant improvement in adversarial robustness as compared to past works. The authors propose a model where a separate batch norm channel is available for a given perturbation norm, and using these batch norm statistics, they empirically show the presence of domain shift between the different norm perturbations. Thus based on this hypothesis, the authors propose to train for each perturbation norm using a specific batch norm channel. Therefore the authors use four batch norm channels for clean, L1 norm perturbations, L2 norm perturbations and L-inf norm perturbations. Further, since at inference time the knowledge of the perturbation norm of the input image is unavailable therefore the authors propose a gated batch norm layer instead of the simple batch norm where a small network before every batch norm gives a probability distribution using softmax activation as to which of the four categories, the input data belongs to. The authors evaluate their method using strong Auto-Attack and a few other attacks, and they also propose to use an adaptive attack to evaluate the performance of their model. However, I think there is still a need to investigate stronger adaptive attacks that can fool the model and Gated Batch norm layer networks simultaneously.",
            "main_review": "Strengths:\n\nThe main strengths of the paper are:\n* The related works have been appropriately addressed and compared with the proposed approach.\n*Extensive empirical analysis has been done using strong attacks like Auto-Attack, and worst-case accuracy for the ensemble of attacks has been reported. Further, some evaluation using black-box attacks like Square attack[1] is also provided, which gives a better understanding of the proposed approach’s effectiveness.\n*The proposed method has been extensively compared on CIFAR-10, MNIST, and Tiny-ImageNet and results for smaller architectures like ResNet-20 and larger architectures like WideResNet-28-10 have been reported. The proposed approach is shown to achieve significant improvements across all settings.\n\nWeaknesses:\n\nThe major concerns regarding the paper are:\n* In [2], [3], it has been previously discussed that adversarial and clean images possess a domain shift. The paper also shows domain shift between different norm adversarial examples, but this seems to follow naturally from the past works.\n* Though the evaluation shows effectiveness on various strong white-box attacks however it needs to be taken into consideration that there are Gated Batch norm networks also which would have no role to play in generating these attacks, but certainly will be used for the inference. Thus the main concern is regarding evaluation using a strong adaptive attack that can fool Gated Batch Norm networks as well as the main model simultaneously. Further, I think that the Gated Batch Norm networks are not trained adversarially in the training procedure which makes them susceptible to adversarial attacks specially designed to fool them. Though authors have proposed an adaptive attack where they first generate a particular norm (L1, L2 or L-infinity) adversarial image and then fool the Gated Batch Norm networks, however, this approach seems suboptimal since it does not account to fool the main network and the Gated Batch Norm networks simultaneously. On this, I would request the authors to kindly provide some additional results on:\n    * Stronger adaptive attacks. In section E.7 it is shown that adding Gated Batch Norm network only in the first GBN layer also gives comparable performance to the proposed approach where Gated Batch Norm networks are added after every batch norm layer. Thus I think authors can use the Gated Batch Norm network only in the first GBN layer and use this model as the base model to analyse the adaptive attacks. For this model, the authors can try to fool say L-infinity batch norm channel as well as the GBN layer simultaneously by simply maximising the sum of their losses(GBN network cross-entropy loss and L-infinity cross-entropy loss). This can help in generating stronger attacks. Further, the authors can perform this in a targeted setting where they try to minimize the loss of the Clean batch norm channel as well as try to minimize the GBN loss corresponding to the Clean batch norm channel simultaneously.\n    * From Table 10, it seems like the square attack is quite strong as it has accuracy comparable to APGD-CE and APGD-DLR, which I don’t think is common. Thus for the test set attacked using L-inf norm square attack, could the authors tell the frequency of the number of test images ending up in clean GBN branch, the number of images ending up in L-inf GBN branch and similarly for other 2 branches. The same thing can be done for APGD-DLR and APGD-CE L-inf norm attacked test images as well. This can help understand the influence of square attack, which is not gradient-based, while the other two are gradient-based attacks.\n\nBesides these, some minor concerns and questions are as follows:\n* The running mean and variance graphs in the appendix seem to be a bit random, especially for the deeper layers. If possible, could the authors add two more batch norm branches in the model where the first one is exclusively for the images corrupted with Gaussian noise with some mean and variance, and the other batch norm branch can be for images generated using L-inf norm Square attack [1](number of queries may be reduced as per computation availability). Finally, could the authors compare some initial and deeper batch norm layer statistics for clean, L-inf and the additional two added batch norm branches? \n* I think the overall writing can be improved a bit. I could not understand what x-axis meant for (a) and (b) plots in figure 4. If the authors could clarify a bit on these plots, then it would be great.\n* In the code the authors have generated the adversarial images of a particular norm using all different batch norm channels, used one at a time. However, on looking at the algorithm of training procedure present in the paper, it does not become clear that the authors have used something like this for training. Could the authors clarify a bit about this in the paper? Also, could the authors clarify why they have generated, say L-inf norm perturbation using all the four batch norm channels, I think generating only through L-inf norm channel would be enough for training? Could the authors give a small ablation on this?\n* In the code, the authors have used SGD with momentum, but they have not mentioned this in section D. I think some more details can be added in this section for better understanding.\n* Could the authors provide the results for a worst-case scenario where adversarial images for say L-inf norm are generated using L-inf norm batch norm branch but the evaluation is done using all other branches except L-inf? So if possible, could the authors provide the test set accuracy for each of the remaining three branches separately?\n\n[1] Andriushchenko, Maksym et al. “Square Attack: a query-efficient black-box adversarial attack via random search.” ECCV (2020)\n\n[2] Xie, Cihang et al. “Adversarial Examples Improve Image Recognition.” 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020): 816-825.\n\n[3] Xie, C., & Yuille, A.L. (2019). Intriguing properties of adversarial training. ArXiv, abs/1906.03787.\n",
            "summary_of_the_review": "Overall I think the major concern is regarding the evaluation of the proposed approach against strong adaptive attacks and technically, the contributions of the paper are marginal. Unless it can be shown that the proposed approach is robust enough against strong adaptive attacks, I think it would be difficult to judge the actual robustness of the proposed approach.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a robust training method by adding muti-batch normalization layers with each one corresponding to a certain norm type. The inference is conducted via an embedded output. The experimental results on three datasets and three norm types demonstrate the effectiveness of the method.",
            "main_review": "I list some of my major concerns below.\n\n1. Hypothesis - (1) How to guarantee the comparisons among different norms are fair? One might obtain different distributions for the same norm type but a slightly different attack power. (2) Although the authors claim that the statistics are significantly different and there seems a unique order in each subfigure of Figure 1. But are they really 'significant' in statistics? The authors should provide more evidence under a fair comparison. The authors should also provide evidence that the same type of attack under different attack power show similar statistics.\n\n2. Theorem - I can only see that if one has different sampling probabilities on the combinations of data from N different Gaussian distributions, then the mixture distributions will be different. The theorem itself is quite intuitive. But it is not clear how the theorem 1 related to the claim. \n\n3. Formulation - Equation 3 clearly shows that the method leverages the ensemble technique, which raises a question: If different attack types have significantly different behaviors in the BN layers, then the most effective way is to simply identify their domains and separate them instead of using the ensemble technique.\n\n4. Experiments - The ensemble formulation makes me wonder that if the robustness comes from the gated batch normalization or simply comes from the power of combining different models. The authors also need to compare their method with existing attack-agnostic methods using ensembling techniques (e.g., [1] and many follow-up works)\n\n[1]  Ensemble adversarial training: Attacks and defenses",
            "summary_of_the_review": "The paper is well-organized. However, I have some major concerns.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposed Gated Batch Normalization (GBN) that incorporates multiple parallel batch-normalization units to separately process different types of perturbations (such as $\\ell_1$ or $\\ell_2$ or $\\ell_{\\infty}$). The authors also introduce a gated sub-network to identify the perturbation types of different adversarial examples. It allows the network to select the appropriate BN branch during inference.\n",
            "main_review": "The paper is mostly well-written and easy to understand. However, I have several concerns regarding the experimental results as follows:-\n\n1. Adversarially trained (AT) models (denoted as $P_1, P_2$ and $P_{\\infty}$) learned using adversarial examples from a specific $\\ell_p$ threat model should produce the best robust accuracy on that same threat model. Since the proposed GBN model is trained using adversarial examples from all $\\ell_1, \\ell_2$ and $\\ell_{\\infty}$ threat models, it is not expected to outperform $P_1, P_2$ and $P_{\\infty}$ on  $\\ell_1, \\ell_2$ and $\\ell_{\\infty}$ threat models respectively. The proposed Theorem-1 by the authors also indicates this.\n\n-- However, in Table 1, we observe that the proposed GBN achieves the best robust accuracy for all $\\ell_1, \\ell_2$ and $\\ell_{\\infty}$. For example, under $\\ell_{\\infty}$ attack for TinyImageNet, GBN achieves $38.7\\%$ robust accuracy. In contrast, $P_{\\infty}$ and TRADES achieve $5.2\\%$ and $9.4\\%$ robust accuracy.\n\nI could not find any concrete explanation/ argument to analyze this result. Also, I could not think of any intuitive reason for this result.\nPlease explain.\n\n\n2. Continued from the previous concern:- \nIt may be possible that GBN is causing gradient obfuscation. Although I note that the authors provided results against black-box attacks, I believe that they should also design an effective adaptive adversarial attack. \n\n--- One such adaptive attack may be as follows (may not be an optimal attack and the authors should think this more carefully):-\nConsider an expectation over transformation attack where $g$ in Eq. 3 sampled from an uniform distribution $U[0,1]$. This would remove the gated sub-network to generate adversarial examples. As [1] show that \"Exploding & Vanishing Gradients are often caused by defenses that consist of multiple iterations of neural network evaluation, feeding the output of one computation as the input of the next.\"\n\n\n3. Inconsistent result:- P1 (i.e., AT model trained using $\\ell_1$ threat model) produced 0.01\\% against $\\ell_1$ threat model. This is even lower robustness compared to the standard baseline mode (i.e. no defense).\n\n4. Please apply tricks such as early stopping to save the best model for $P_1, P_2$ and $P_{\\infty}$ to achieve the best defense models [2,3].\n\n5. Missing ablation study on gated sub-network:  \"GBN utilizes a gated sub-network $\\Phi(x)$ to predict the domain for each layer input x\". I feel that this is the core of their proposed framework.  However, the authors did not provide any ablation study on the performance of gated sub-network $\\Phi$ to distinguish $\\ell_1, \\ell_2$ and $\\ell_{\\infty}$ bounded adversarial examples.\n\nNotably, based on recent studies, it is not easy to distinguish such adversarial perturbations [4].\n\n\n6. The authors have chosen significantly large $\\ell_1, \\ell_2$ boundaries for MNIST and $\\ell_1$ for CIFAR-10 than commonly used in the existing literature. Please check this paper: Fundamental Tradeoffs between Invariance and Sensitivity to Adversarial Perturbations (http://proceedings.mlr.press/v119/tramer20a/tramer20a.pdf).\n\n7. (Minor) Please define the notations for Theorem-1 properly.\n\n[1] Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples (https://arxiv.org/abs/1802.00420)\n\n[2] Overfitting in adversarially robust deep learning (https://arxiv.org/abs/2002.11569)\n\n[3] Robust Overfitting may be mitigated by properly learned smoothening (https://openreview.net/forum?id=qZzy5urZw9)\n\n[4] Detecting Adversarial Examples Is (Nearly) As Hard As Classifying Them (https://arxiv.org/abs/2107.11630)\n",
            "summary_of_the_review": "I find several concerns regarding the experimental results provided in the paper. I have provided some suggestions to incorporate additional attacks and ablation studies for more careful evaluation of the proposed GBN framework.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}