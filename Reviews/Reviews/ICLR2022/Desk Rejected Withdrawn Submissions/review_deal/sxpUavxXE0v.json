{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper's main claim is that *small batch size* makes contrastive learning inefficient; indeed, classifying the positive from easy negatives does not provide informative signals for the representation. This claim can be formally stated by Proposition 1, that the gradients of features $z_i$ are proportional to the negative-positive coupling (NPC) multiplier $q_{B,i}$. Thus, the paper proposes a simple solution, called decoupled contrastive learning (DCL), which removes the positive term from the denominator of the contrastive loss. Intuitively, it removes the calibration term $q_{B,i}$ from the probabilistic interpretation of contrastive loss and attracts the positives with even power. DCL makes contrastive learning more efficient, outperforming the baselines when trained with small batch sizes and small epochs.",
            "main_review": "### Concerns in method\n\n- **Weak motivation.** Why should one make contrastive learning effective for small batches? Positive-only methods such as BYOL and SimSiam have similar motivations: they also evenly attract the positives by removing the right term in Equation 5. While the paper claims that DCL is more *theoretically principled*, it also loses the guarantees of InfoNCE.\n- **Performance at convergence.** DCL makes converge faster by removing the calibration term $q_{B,i}$. However, it is unclear whether the final performance of DCL is also better. Figure 4 shows that both SimCLR and DCL are not converged yet; the gain of DCL may be smaller for the converged models.\n- **Correctness of the NPC multiplier**: Does the denominator of Equation 3 not contain the positive term?\n- **Necessity of weighted DCL.** The paper suggests the weighted loss for the positives, which is orthogonal from the paper's main theme. Also, the benefit of weighted loss should be justified under the fully converged models with similar reason above.\n- **Hyperparameter sensitivity.** The paper claims that DCL is less sensitive to the hyperparameters, e.g., temperature $\\tau$. What is intuition?\n- **For supervised classifier.** The idea of removing positive from denominator can also be applied for the supervised classifier. Are there some unique properties for contrastive learning (i.e., non-parametric classifier)?\n- **Unclear presentation.** The presentation is unclear and overly complicated compared to its simple intuition. For example, the paper could first define the DCL loss (Equation 5) and then prove that DCL indeed removes the NPC multiplier $q_{B,i}$ in Proposition 2.\n\n\n### Concerns in literature survey\n\nThe paper is missing several relevant works, e.g.,\n- First, the paper should be better compared with the positive-only methods like BYOL and SimSiam.\n- [1] removes the effect of positive-like terms in the denominator of InfoNCE.\n- [2,3] chooses hard negatives to make contrastive learning better, particularly for small batches.\n- [4] proposes FlatNCE, a variation of InfoNCE to handle small batch negatives.\nThe paper would be much stronger if it verifies the superiority compared to BYOL/SimSiam/FlatNCE, etc.\n\n[1] Chuang et al. Debiased Contrastive Learning. NeurIPS 2020.\\\n[2] Kalantidis et al. Hard Negative Mixing for Contrastive Learning. NeurIPS 2020.\\\n[3] Robinson et al. Contrastive Learning with Hard Negative Samples. ICLR 2021.\\\n[4] Chen et al. Simpler, Faster, Stronger: Breaking The log-K Curse On Contrastive Learners With FlatNCE. arXiv 2021.\n\n\n### Concerns in experiments\n\nThe experiments are not well-organized and weakly support the main claim, e.g.,\n- **Performance at convergence.** As I mentioned above, the paper should present the results where the models are fully converged.\n- **Small datasets.** Why the results on small datasets are separated in a paragraph of Section 4.2 and Section 4.3?\n- **Unfair comparison in Table 3.** Why should one compare with +CLD? Also, DLCW w/ mcrop should be compared to other baselines w/ mcrop.\n- **Purpose of Table 4.** The ablated components are the proposed ones, but just a hyperparameter study. The hyperparameter selection scheme should be fairly applied to the baselines.",
            "summary_of_the_review": "My main concerns are:\n- Missing discussion and comparison with the related works\n- Weak justification and presentation of the proposed method\n- Not carefully designed nor well-organized experiments",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents decoupled contrastive learning (DCL) to significantly improve training efficiency and achieve competitive performance on various benchmarks. The authors provided theoretical analysis and empirical evidence to show the negative-positive coupling in the gradient of InfoNCE-based contrastive learning. ",
            "main_review": "Strengths: \n- this paper is simple but very interesting. \n\n\nWeaknesses:\n\n- Since q_b has a large fluctuation when batch size is small, removing q_b can reach a decoupled contrastive learning. From Fig 3, the performance with DCL increases with the increase of batch size. Please analyze why DCL also works well when batch size is large?  With a large batch size, the NPC fluctuation is quite small.\n\n- Simply removing q_b can improve the contrastive learning on different batch size setting. More improvements can be observed when the batch size is small. Please analyze what is the rationale? \n\n- This paper is not carefully written. There are many typo errors. \n  At the end of the method section, the authors mentioned \"The intuition behind ..\". It is quite strange to mention this argument since it has no relation with the abovementioned sentences. \n \n",
            "summary_of_the_review": "This paper presents some interesting results and proposed a simple DCL method. Results are very good and have some technical merits. but the paper is not carefully written.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper introduces a decoupled contrastive learning objective, which removes the positive pair from the denominator of the InfoNCE loss. The results show the robustness when using small-size batch as well as the superior learning efficiency.",
            "main_review": "**Pros**:\n\n1. The method is simple and easy to implement. \n\n2. The experimental results with MoCo and SimCLR show that the proposed method improves the stability over different numbers of negatives for contrastive learning.\n\n   \n\n**Cons**:\n\n1. I believe the authors missing a similarity term of the positive pair in the denominator of the NPC multiplier in Equation 3.\n\n2. The authors try to reveal the issue of small gradients when using small number of the negative samples or easy positive samples through the NPC multiplier. However, it is a bit intuitive that using small number of negatives will lead to small loss. In other words, I think the same conclusion can be drawn from the perspective of loss, which reduces the significance of the paper.\n\n3. The proposed decoupled contrastive learning objective can also be seen as a simple multi-task loss, which naturally blocks the direct interaction between positives and negatives. I don't know if this is still a `contrastive` learning objective.\n\n4. Some prior works [A1, A2] have already reduced the number of negatives. The article lacks comparisons with these related works.\n\n[A1] EqCo: Equivalent Rules for Self-supervised Contrastive Learning.\n[A2] Whitening for Self-Supervised Representation Learning.  ",
            "summary_of_the_review": "I lean to weakly reject the paper due to the above cons.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper describes a negative-positive coupling (NPC) effect in the InfoNCE loss that is widely used for contrastive learning. Addressing this effect, the paper proposes a new objective function called decoupled contrastive learning (DCL). Through extensive experimentation it is shown that this new objective function improves the efficiency of contrastive learning and that it can outperform SimCLR and MoCo in a setting where smaller batches and fewer epochs are used.\n",
            "main_review": "## Strengths\n\n- The experimental evaluation is extensive and shows a clear benefit of using the decoupled contrastive learning objective over the SimCLR and MoCo baselines. It also seems to be carefully designed so as to assure a fair comparison between all models.\n- DCL seems to be simple, yet effective. It improves the efficiency of contrastive learning, which is a very valuable contribution as it can help contrastive learning to be more useful in practice.\n\n\n\n\n## Weaknesses\n\nThere are two major weaknesses in the paper. Since I think that this is a very interesting paper in general, I would be happy to raise my score if these are addressed appropriately.\n\n\n### 1) Derivative of L\n\nProposition 1: When calculating the gradients of $L$, I get to different results than presented here. The derivations in the Appendix also do not make it clear to me how the solutions presented in the proposition were derived. Since the theoretical justification of the paper hinges upon the NPC multiplier which is a result of these derivations, this needs to be clarified. \n\nMore specifically, when deriving $L_i^{(1)}$ with respect to $z_i^{(1)}$, I cannot follow how the third line of the proof would follow from the second. Additionally, when following the derivation of $L_i^{(1)}$ with respect to $z_i^{(2)}$, it seems to be necessary to set $\\frac{exp(z_i^{(1)}, z_i^{(2)})}{exp(z_i^{(1)}, z_i^{(2)}) + \\sum_{..., i \\neq j} exp(z_i^{(1)}, z_j^{(q)})} = \\frac{exp(z_i^{(1)}, z_i^{(2)})}{\\sum_{..., i \\neq j} exp(z_i^{(1)}, z_j^{(q)})}$; and I cannot see how that would be doable.\n\n\n\n\n### 2) Presentation of the Paper\n\nThe presentation of the paper could be improved. \n- Most notably, there are grammatical errors scattered throughout the paper and especially concentrated in the Related Works section. While I would be fine with errors in general, the problem here is that some of them prevent the understanding of the text. \n- Additionally, I would prefer it if the figures were self-contained, i.e. if one could understand them without reading the main text. This holds especially for Figure 1, which in the current setup is only understandable after reading the methods section.\n- In my eyes, the essential novelty of the decoupled objective loss is that it removes the positive samples from the denominator. Adding this short description could make the main concept of the paper clearer and it could do so earlier (e.g. in the abstract/introduction).\n\n\n## Other points\n- The proposed decoupled contrastive loss does not describe a valid log-probability anymore. How do you expect this to influence your model?\n- I would be curios to see how the proposed DCL would perform when trained for the full amount of epochs used in SimCLR and MoCo. Is it still competitive to SimCLR and MoCo then, or does it only shine for smaller experimental scales?\n- Table 4: What do you mean by BYOL augmentation? Are you still using negative samples in this setup?\n- Add the number of epochs that were used in the ImageNet setup - I assume it was 200?\n\n",
            "summary_of_the_review": "This paper presents a simple, yet effective method to improve the efficiency of contrastive learning methods. My main concern is that the derivations in the proof of proposition 1 seem to be incorrect, which undermines the theoretical justification outlined in the paper.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors find a negative-positive-coupling (NPC) effect in the InfoNCE loss, which leads to unsuitable learning efficiency with respect to the batch size. They propose to remove the positive pair from the denominator, which proves to lead to a decoupled contrastive learning loss. The proposed method is simple but effective and they demonstrate its effectiveness in various benchmarks.",
            "main_review": "Strengths:\n\n(1) The proposed method is simple but effective. By simply removing the positive term in the denominator, performance can be improved, especially for small batch sizes. I think this method is novel and can generalize to various InfoNCE-based methods.\n\n(2) The motivation is clear. I think it makes sense to improve the performance under small batch size (for those researchers with limited computing resources).\n\n(3)  This paper is well-written and easy to follow. The comparative experiment is sufficient, and it is easy to see the improvement from the figures and tables.\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nWeakness:\n\n(1) My major concern is the proof of proposition 1 in the appendix. As can be seen in the proof, \n\n$$q_{B,i}^{(1)}=(1-\\frac{1}{Y}\\cdot\\exp(<\\mathbf{z}_i^{(1)},\\mathbf{z}_i^{(2)}>/\\tau))$$\n\nAlso from the definition\n\n$$Y=\\exp(<\\mathbf{z}_i^{(1)},\\mathbf{z}_i^{(2)}>/\\tau)+\\sum \\exp(<\\mathbf{z}_i^{(1)},\\mathbf{z}_j^{(q)}>/\\tau)$$\n\nHence, $q_{B,i}^{(1)}$ should be \n\n$$1-\\frac{\\exp(<\\mathbf{z}_i^{(1)},\\mathbf{z}_i^{(2)}>/\\tau)}{\\exp(<\\mathbf{z}_i^{(1)},\\mathbf{z}_i^{(2)}>/\\tau)+\\sum \\exp(<\\mathbf{z}_i^{(1)},\\mathbf{z}_j^{(q)}>/\\tau)},$$\n\nwhich differs from the derivation in the paper by a positive term $\\exp(<\\mathbf{z}_i^{(1)},\\mathbf{z}_i^{(2)}>/\\tau)$ in the denominator. I think this difference is very important because this positive term in the denominator is the key of the proposed method. Please let me know if I am mistaken.\n\n(2) What is the performance when transferring to object detection or classification tasks? I think the authors should provide at least one such result to better demonstrate the effectiveness. The results provided show that it performs well on the source pretraining dataset (in terms of KNN or linear evaluation), but how does it perform when transferring to other datasets? This is what I am more concerned about.\n\n(3) Is there a similar effect for cross entropy loss in supervised learning? What if we remove the positive term in the denominator in supervised learning setup. Is this a effect only for self-supevised learning or for cross entropy loss?  I think the authors should add discussions about this.\n\n\n",
            "summary_of_the_review": "Overall I like this paper but I still have some concerns (as noted in the weakness). I am happy to raise my score if my concerns are well addressed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}