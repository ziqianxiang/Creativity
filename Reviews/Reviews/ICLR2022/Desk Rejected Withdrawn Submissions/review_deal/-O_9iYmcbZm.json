{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "Introduces a new “zero-round active learning” task of selecting informative points for labeling in a single round via a data utility model that is learned on labeled data from a related source domain and adapted to the target via domain adaptation. The proposed method is shown to outperform competing AL methods in this setting and is an effective way to warm-start existing multi-round methods.\n",
            "main_review": "Strengths\n\n– The paper tackles an interesting problem of practical importance\n\n– The proposed approach is intuitive and interesting\n\n– The paper includes several interesting experiments, such as the utility of D2ULO in warm-starting multi-round AL methods, compatibility with source domains with disjoint label spaces, and correlation between predicted and true utility\n\n– The paper is mostly clear and easy to follow\n\nWeaknesses\n\n– The performance comparisons are in my opinion the biggest weak point. Specifically, it is not clear how D2ULO compares to baselines in the standard multi-round setting. My understanding is that for the same total budget, performance is inferior. Is this true? In that case, the suitability of D2ULO is limited only to special cases where multi-round feedback is infeasible – it would be good to explicitly state that. Further, for completeness it would be good to i) either demonstrate D2ULO performance in the multi-round setting or ii) explore in more detail D2ULO’s suitability for warm starting multi-round methods (currently the paper only presents 1 experiment), which I think is a valuable and understudied contribution in itself. \n\n– The naming of the task is a little confusing/misleading: performance is still measured after one round of labeling of points selected by the method, right? And it also still requires an initial labeled pool of data, just not from the target domain. I would encourage the authors to propose a more suitable name for the task.\n\n– The paper misses citing and comparing to more recent Active DA methods other than AADA eg. [A,  B]\n[A] Active Domain Adaptation via Clustering Uncertainty-weighted Embeddings, Prabhu et al.,  ICCV 2021\n[B] S3VAADA: Submodular Subset Selection for Virtual Adversarial Active Domain Adaptation, Rangwani et al., ICCV 2021\n\n– How does the proposed method deal with redundancy within the selected subset? For eg. two selected data points might both have high utility individually but might contain completely redundant information. Prior work in Active Learning eg. coreset, BADGE, and CLUE, have extensively studied within-batch diversity. How does D2ULO overcome this failure mode? \n\n– For completeness, it would be good to understand D2ULO’s performance for a given target domain as the source domain is changed to be distributionally closer or further from it – does D2ULO require source and target domains to be similar?\n\n– The experimental section is slightly confusingly organized and hard to follow. If I understand correctly, although Figure 2 shows multiple points corresponding to each method, these are _not_ typical learning curves presented in active learning i.e. the model is not trained incrementally in a multi-round fashion, but rather each point corresponds to single-round performance corresponding to that budget. Is that correct? I could not find a clear explanation of this important detail in the paper. \n\n– Most of the paper’s experimental results are on simple DA settings eg. MNIST, USPS, and SVHN. Gains on more challenging shifts eg. from DomainNet and OfficeHome would make the results more convincing.\n",
            "summary_of_the_review": "Interesting paper tackling a novel problem, but I have some concerns (see weaknesses above). I am happy to reconsider my rating based on the author response.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "In this paper the authors propose a ‘zero-round’ setup for AL where no labeled data is available at the beginning and only one acquisition is done in contrary to multi-round iterative AL, and they further specify a regime where some labeled data are available from related domains such that techniques from domain adaptation (DA) can be borrowed. \n\nTo achieve such goal, the authors propose an algorithm D^2ULO that learns a data utility function with DeepSets (similar to DULO) on source domain while making sure it is transferrable to target domain by applying domain adaptation training which utilizes a discriminator and feature extractor to extract transferrable embeddings. They compare to several baselines where conventional SOTA AL algorithms are combined with some domain adaptation ( by operating on the feature space) and show their method outperforms on real->real and synthetic-> real regime in both train from scratch and fine tuning regime.\n",
            "main_review": "Strength\n-\tThe proposed zero-round AL with domain adaptation setup is novel and appealing, where limited prior studies can be found, and could be helpful in real-world in some degree.\n-\tThe proposed D^2ULO algorithm is an organic combination of domain adaptation and utility-based active learning, which is a clean solution of the problem.\n-\tThe author explored several regimes (real->real, synthetic->real, and mismatching labels space), and both direct training and fine-tuning, which are quite new.\n-\tThe presented results are promising, despite some unclarity in the baseline setups (see points below)\n\nProblems\n-\tDespite the interesting framing, there are several ambiguities in the method description and experimental setups which may make the audience question the reliability of baselines. Specifically:\n1)\tWhat is the ‘stochastic greedy algorithm’ mentioned in Unlabeled data selection? From the description it seems to be an iterative approach where one data point is chosen out of some randomly selected subset. However, it seems the utility function is a function over set, doesn’t it defeat the purpose if the points are selected one by one instead of as a set?\n\n2) The experimental setups for baseline methods are extremely unclear. It says “we pre-train a feature extractor that minimizes the distance between source and target domain in the feature space, apply it to extract features for the unlabeled data pool and perform active data selection on the extracted features”, however there is already a feature extractor in DULO which is trained with more advanced DA technique. Why not re-using it for the other baseline models to be fair? Or even just keep the adversarial component and remove the utility function part if the authors don’t want to leak any information from utility function.\n\n3)\tMoreover, a lot of the baselines AL algorithms rely more on an existing model that is trained on small amount of the target domain data (this is also way they are not applicable to zero-round AL settings), as they require predicted uncertainty or some statistics related to the predicted labels. Just operating on ‘extracted features’ does not solve this problem, as there is no model to run those AL acquisition from. The authors have pointed out that they are not applicable in zero-round settings, however no details are provided on how they adapt it to zero-round setting to be able to compare with D^2ULO, or are these methods kept as multi-round setting? \n\n4)\tLastly, it is not clear why the authors use SVM and Logistic regressions on some dataset and smallCNN on the others in Figure2, especially when SmallCNN clearly achieve much better performance for all baselines. Isn’t it a bit unfair to evaluate on the same model family that has been used to compute utility functions? As the author mentioned that “data utility functions for small models are positively correlated with those for large models”, perhaps it would be more convincing to compared in models trained with more sophisticated models than the utility function’s base model.\n",
            "summary_of_the_review": "The paper focuses on an interesting setup for zero-round AL, and proposed a reasonable approach to solve in with domain adaptation. However, several ambiguities remain in the experimental setups, making it questionable whether the baselines are set up in a fair manner. This reviewer would like to see further clarifications of the above-mentioned questions before being able to confidently accept it.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In typical active learning workflows, querying human labels is sequential and synchronous, that can increase the overall training time. This paper tries to remove the dependency on human labels while actively training a model. It is achieved by utilizing a labelled source data (i.e. similar to the target dataset) to build a model for utility based sampling and to provide labels while actively training a model on the target dataset. There are two main pieces in the approach, one is to learn a model for data utility function ( to sample most useful points during AL) using small number of labeled examples from the source dataset and another component is to use domain adaptation techniques like CyCADA (Hoffman et al., 2018), UDA (Sun et al., 2019), and AFN(Xu et al., 2019), to have some correspondence between source and target datasets. This approach has been evaluated on MNIST, USPS, CIFAR-10, STL-10 and some other datasets and compared against several baselines. Although at the first glance the results look compelling, I am not fully convinced about the effectiveness and have several questions/ suggestions which is in the bottom sections of this review. \n \n \n",
            "main_review": "Strengths:\nThe paper proposes a method to learn data utility function(model) for sampling \"most useful\" points in the target dataset, while training the data utility model only on the labeled data from the source domain(dataset). It tries to apply ideas from domain adaptation to claim that one can utilize labeled data points from the source domain and use them to perform active learning on similar target datasets. Empirical evaluation on several real-world dataset and baselines is also provided. \n\nWeaknesses:\n1. It is not clear to me how is the original problem is addressed here i.e. to eliminate the dependency on human labeling. If there are enough labeled data points in the source dataset itself, then what is the need to do active learning, one can just do domain adaptation. On the other hand, if the point is to select the useful data points in the target domain then find similar points to those in the source domain and get human labels for them. This point also doesn't make much sense to me -- it will still have the same human dependency problem. Can these things be clarified, what are the assumptions on the source dataset and how practical are those assumptions in an active learning setup? \n\n2. The experiments show accuracy of different methods as the number of sampled data points increases. Here, the number of sampled data points means, during active learning (on target dataset) what is the number of data points selected from the target dataset. I think the comparisons shown in Figure 2 are unfair to other methods including the random baseline. The reason being, in your approach you are using a data utility function which has already used some n labeled data points from a source domain. For a fair comparison, I think the baselines should be provided those additional n labeled data points. \n\n3. Is it necessary to train the data utility model and use it for sampling? What if simple uncertainty sampling is used instead of the data utility model in your setting of active learning with domain adaptation. Is it required just because you are doing domain adaptation? Could you please clarify this and give some comparison with uncertainty sampling?",
            "summary_of_the_review": "I think the paper is trying to eliminate the need for obtaining gold labels from humans, by getting these labels from a similar domain's labeled dataset. There are a few problems with the current state of this paper which I have mentioned in the main review. Due to these issues, I am not inclined to accept this paper as it is. I am willing to update my review, if my concerns are appropriately addressed. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Traditional AL frameworks have two limitations: First, they perform data selection in a multi-round manner, which is time-consuming and sometimes impractical. Second, they assume that there are a small amount of labeled data points available in the same domain as the data in the unlabeled pool. This paper proposed D^2ULO as a solution that solves both issues, which indeed combined the DA methods and data utility model. The experiments show the effectiveness of the proposed model.",
            "main_review": "Traditional AL frameworks have two limitations: First, they perform data selection in a multi-round manner, which is time-consuming and sometimes impractical. Second, they assume that there\nare a small amount of labeled data points available in the same domain as the data in the unlabeled pool. This paper proposed D^2ULO as a solution that solves both issues, which indeed combined the DA\nmethods and data utility model. The experiments show the effectiveness of the proposed model.\n\nSome weaknesses are as follows:\nThis work is mainly based on the existing DA methods and the recent work of Wang et al. (2021), thus I novelty contribution is limited in some certain. Also the authors proposed an iterative training process where DeepSets model is trained and used to update the feature extractor Gf after k steps training of DA models. The theoretical analysis should be provided to analyze this manner can work?\n\nIn the other hand, why the proposed utility function used in this paper for data selection is effective? Is there any theoretically analysis rather than only experimental evaluation? In other word, are the samples selected according to the utility scores are really useful for the prediction performance on target domain?\n\nThe organization should be improved, as there is not conclusion for this paper. The references should also be updated, since there are very few related works published after 2019 (no including 2019). \n\nWang et al. (2021) assumed a small labeled set in the target domain for data utility learning, which is not suitable for the zero-round AL setting. I think the authors can use some SOTA methods which can select some high-confident samples in target domain to label as labeled set, then the method (Wang et al. (2021)) can be adopted as baseline. It would be interesting to see the comparison results.\n\nAlso for the consideration of transfer learning algorithms as baselines, some recent SOTA should be considered and compared.\n\nThere are some typos, e.g., Page 4 \"Note that data utility learning require ... \".",
            "summary_of_the_review": "There are some weaknesses are as follows:\nThis work is mainly based on the existing DA methods and the recent work of Wang et al. (2021), thus I novelty contribution is limited in some certain. Also the authors proposed an iterative training process where DeepSets model is trained and used to update the feature extractor Gf after k steps training of DA models. The theoretical analysis should be provided to analyze this manner can work?\n\nIn the other hand, why the proposed utility function used in this paper for data selection is effective? Is there any theoretically analysis rather than only experimental evaluation? In other word, are the samples selected according to the utility scores are really useful for the prediction performance on target domain?\n\nThe organization should be improved, as there is not conclusion for this paper. The references should also be updated, since there are very few related works published after 2019 (no including 2019). \n\nWang et al. (2021) assumed a small labeled set in the target domain for data utility learning, which is not suitable for the zero-round AL setting. I think the authors can use some SOTA methods which can select some high-confident samples in target domain to label as labeled set, then the method (Wang et al. (2021)) can be adopted as baseline. It would be interesting to see the comparison results.\n\nAlso for the consideration of transfer learning algorithms as baselines, some recent SOTA should be considered and compared.\n\nThere are some typos, e.g., Page 4 \"Note that data utility learning require ... \".",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}