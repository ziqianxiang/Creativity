{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper introduces a model cloning defense mechanism, called Stingy Teacher, for model owners to mitigate the issue of intellectual properties leakage through knowledge distillation. There has been some prior work, such as Nasty Teacher by Ma et al., that aims to train a model that would significantly degrade the accuracy of the student model trying to imitate its behavior. However, the paper claims that why nasty a teacher model can effectively prevent students from imitating its behavior is unclear. The paper hypothesizes that Nasty Teacher generates sparse logits, which are crucial for preventing a student model from imitating through KD. Based on this observation, the paper proposes to directly sparsify the logits of a standard pre-trained network by zeroing out non-top-K class logits. Evaluation on image classification tasks such as CIFAR-10/100/ImageNet shows that Stingy Teacher obtains models that are more difficult to imitate than the Nasty Teacher. \n",
            "main_review": "Strengths:\n-- The paper studied an interesting problem on how to prevent student models from imitating the teachers. \n-- The paper made an interesting observation that Nasty Teacher generates sparse logits. \n\nWeaknesses:\n-- It is unclear whether sparse logits alone are sufficient to prevent student models from imitating the teacher model's behavior.\n-- Evaluation details are missing\n-- Due to the lack of discussion of slow convergence, it raises concerns on whether the proposed method provides a false sense of defense. \n\n",
            "summary_of_the_review": "The paper looks into a very interesting problem -- how to prevent student a model from imitating a teacher model, which may cause a leakage of intellectual properties. The paper makes an interesting observation that prior work, Nasty Teacher, that claims to mitigate this issue produces sparse logits, which could make the student model training more difficult. Based on this observation and hypothesis, the paper proposes a simple method by only providing information of a few categories (e.g., top-K logits).\n\nWhile the theoretical analysis appears to be interesting, it seems that another possibility of the lower accuracy from the student model could be because the student model simply converges slower when the non-top-K logits are zeroed out, which leads to a reduced loss magnitude rather than intrinsic difficulties that prevent the student from imitating. In other words, because the loss magnitude has changed due to using sparse logits, it seems fairly easy to observe slow convergence due to unoptimized learning rate schedules. On the other hand, slow convergence alone is insufficient to prevent a student model from learning a teacher, because it is also possible to overcome that by adjusting the learning rate schedules of the student or using more advanced KD mechanisms that exploit deeper knowledge of the teacher network or through progressive/stacking methods. To be more convincing, the paper needs to show that the catastrophic learning of the student isn't a fact of poor choice of batch sizes and learning rate schedules due to the change of the loss magnitudes, and it would be better if the paper could conduct experiments to explore hyperparameters to ensure that the baseline choice is as strong as possible. \n\n\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper establishes that the sparsity of logits is the key to the success of nasty teacher, and hence greatly simplifies the later while maintaining the same effectiveness. ",
            "main_review": "The recently discovered “nasty teacher” shows that a teacher network trained with self-adversary can yield nearly the same performance as a normal one, but significantly degrades the performance of student models trying to imitate it. As knowledge distillation (KD) exposes the risk of ML model IP stealing, nasty teacher is a meaningful pilot study to address the increasingly important issue of deep learning model protection. However, it is unclear why the learned soft distributions of nasty teacher would fail the KD. This paper fills in the gap.  \nThrough observing the logit distribution from nasty teacher, the authors conclude that the purposeful sparsity of the logits is the main reason for the accuracy drop of student networks in the nasty teacher setting. The authors formally proved this property, and then proposed a simplified variant of nasty teacher, named stingy teacher, which just keeps the top-K logits and zeros out the rest. \nNotably, unlike the Nasty Teacher that inevitably hurts the teacher accuracy due to self-adversary, the new stingy teacher only modifies the non-dominant logits output and therefore guarantees to not degrade any teacher performance. \nThe authors reported many experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate, to show that stingy teacher can prohibit the learning student networks, often more/better than the more expensive nasty teacher. Since stingy teacher has no extra training overhead, it nicely scales up to very large datasets, such as ImageNet. The ablation and visualization appear to be thorough. The writing is in general smooth, although a few typos common as “logits is”, etc.\nThe success of stingy teacher is still a bit surprising to me intuitively: why just sparisifying the distribution of “dark knowledge” (not even misleading it) suffices to make teacher nasty? Even I can understand the mathematical derivations, some KD intuitive explanations could have been provided by the authors.  \nFor example, let us consider the extreme case when only the top-1 prediction is preserved, then it almost boils down to providing student models with just hard labels -- except the teacher model might make a percentage of erroneous predictions (for SOTA models, that percentage shall also be low). Then, why should I expect a student mimicking such top-1 teacher predictions to suffer in performance? If that indeed happens, Is the label noise the “devil in the detail”? Could you experimentally verify this special case?\n",
            "summary_of_the_review": "I think this is an interesting paper, but it could have been strengthened by including more intuition explanations. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new method, i.e., stingy teacher, to prevent model stealing from a pre-trained teacher model via knowledge distillation (KD). It zeros out part of the teacher's logits so that the student's performance is significantly degraded when training with KD. The proposed approach proves to be effective with several object classification benchmark datasets (CIFAR and ImageNet).",
            "main_review": "Pros:\n1. Preventing model stealing via KD is a topic of broad interest.\n2. The proposed approach is effective under some scenarios.\n3. The paper is well written and easy to follow.\n\n\n\nCons/major concerns:\n1. The motivation of the paper is unclear/trivial.\n\n    Given the previous work nasty teacher, I think the motivation of continuing this line of research might be trivial. This is because, with the training set available, if the student cannot perform well via KD, the attacker can just train the model solely with a cross-entropy loss (considered as the baseline) to achieve satisfactory performance. Since the student distilled from the nasty teacher has already performed worst than the baseline, any further optimization seems to be insignificant. If the scenario can be extended to data-free KD, I think the proposed idea is more meaningful.\n\n\n2. The assumption in the theoretical analysis part is too ideal.\n\n    In Section 3.2, the paper provides a theoretical analysis to prove the rationality of sparsity probabilities as the key success of the nasty teacher. The derivations require that $\\tau \\to \\infty$, which is too strong and is not realistic in practice.\n\n\n3. The proposed method directly manipulates the logits, which seems to be a rule-breaker.\n\n    It is disappointing that the proposed approach solves the problem by directly manipulating the teacher's logits, rather than using an extra loss function. If the teacher's logits can be changed before being given to the users, why not just provide the class index of the top-1/k categories? Or just giving random probabilities as long as the top-1 prediction remains unchanged?\n\n\n4. Some of the experiment results are unsatisfactory.\n\n    In Tables 1-4, the proposed approach is compared with the nasty teacher approach. In many cases, its performance is worse than the nasty teacher, especially those with ImageNet. These results make it hard to believe the proposed approach works well in general.",
            "summary_of_the_review": "Overall, I think the manuscript is not ready for publication in its current form. There are several nontrivial weaknesses as listed above. I currently give a negative score but I will consider changing it if the authors' feedback resolves the concerns.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper has made the following contribution: 1. They give a theoretical analysis of why the sparsity of logits is key to Nasty Teacher. 2. To address the accuracy drop and retraining issues of Nasty Teacher, they propose Stingy Teacher which maintaining the logits of top-K classes and zeroing out the rest. Experiments on CIFAR, TinyImageNet and ImageNet show that their model can reduce the accuracy of students with no need of retraining. \n",
            "main_review": "Strengths:\n1. This paper has given a theoretical analysis of the influence of sparse logits for knowledge distillation.\n2. This paper proposes to Stingy Teacher which reduces the accuracy of students and does not require retraining.\n3. Experiments on large-sclae datasets (ImageNet) are conducted.\n4. Although the authors study the sparse logits effect only in the setting of model exposing, I believe this study will also help us understand  the other deep learning tasks.\n\n\nWeakness:\n1. My biggest concern of this paper is the necessarility of studying Stingy teacher. About the motivation, the author claim that there is \"risk of exposing intellectual properties\". However,  if the ML model is a while-box, the Stingy/Nasty Teacher still can not prevent it because the attacker can know the details of the whole model. If the ML model is black-box,  the users will only know the decision result (e.g. the predicted category) instead of logits on different categories (probability distribution).  In this paper, it seems that the author assume the attacker know the logits, but not the whole model, which is a very unusual setting. \n\n2. The Stingy Teacher still seems easy to be attacked by the attacker. The attacker just needs to process the logits from a probability distribution to a 0-1 distribution like the ground truth, and then train the student with this kind of pseudo labels with cross-entropy loss instead of knowledge distillation. In this situation, Stingy Teacher is useless for misleading the students. Since the proposed Stingy Teacher is so easy to be attacked, it can be not considered as a real defense method. If this kinds of attack method can not attack Stingy teacher, this experiment should be provided.\n\n3. The proposed method can only work for student trainers with logit distillation. Its effectiveness on feature knowledge distillation has not been evaluated. Since most of the advanced knowledge distillation methods are feature-based knowledge distillation, the application of this method is limited.\n\n ",
            "summary_of_the_review": "This paper has both good theoretical analysis and experimental evaluation. However, the necessarility of studying Stingy Teacher and the real application value is not clear (weakness). \n\nI tend to increase my rates if authors can solve my concerns (weakness1-2).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}