{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors proposed a new method for generating an enhanced photo based on several input image frames.",
            "main_review": "The general idea and the proposed pipeline make sense. However, I have several concerns regarding the proposed solution that are listed below:\n\n1. The novelty of the designed solution is only incremental - the overall idea and almost all building blocks have been proposed before.\n\n2. As the authors propose to perform image enhancement using burst photos, one would like to know how performant is this method compared to a single-frame RAW image restoration done with one of the recent state-of-the-art NN approaches, e.g.:\n\na) Ignatov et al. Replacing mobile camera isp with a single deep learning model.\nb) Dai et al. Awnet: Attentive wavelet network for image isp.\nc) Zhu et al. EEDNet: enhanced encoder-decoder network for autoisp.\nd) Ignatov et al. AIM 2020 challenge on learned image signal processing pipeline.\ne) Hsyu et al. Csanet: High speed channel spatial attention network for mobile isp.\n\nWithout this, it is hard to judge the effectiveness of using multiple image frames compared to the combination of the standard single RAW image denoising + RGB reconstruction methods.\n\n3. The computational efficiency of the designed solution is not discussed. In general, one assumes that the resulting model will be running on mobile phones or cameras, but the proposed method seems to be too computationally expensive for processing high-resolution image data.",
            "summary_of_the_review": "Overall, this is a good technical paper with quite interesting practical results but limited novelty.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "For burst restoration tasks, including denoising, demosaicking, and SR, the authors proposed a solution that achieved SOTA performance. This paper proposed the edge boosting alignment, a novel pseudo-burst feature fusion mechanism and an adaptive group upsampling module. This paper achieves SOTA performance.",
            "main_review": "1. In sec 3.1, the authors said that \"However, existing deformable convolution is not explicitly designed to handle noisy RAW data. So this paper proposed xxxxx\". However, in the following two papers [1] [2], the DConv alignment is used for noisy raw image denoising and achieved SOTA performance. I think the writing is not very appropriate and these papers should also be discussed in this paper. The comparison of the alignment module with that of [1][2] should be included.\n\n2. running time comparison with DBSR and LKR should also include. MFIR seems to obtain the second-best performance in table 1. However, in figure 4, there's no comparison with MFIR.\n\n3. The ablation study seems very simple and I think can not prove the contribution very well. (1) Why channel-wise feature fusion is better than other previous model? The fair comparison should be between the PBFF and using common Conv layers with similar training parameters whose input is the concatenate of frame features. (2) Edge Boosting Feature Alignment Module. This should compare with normal feature alignment module, like in [1] and [2]. (3) Simple remove some network part can not well prove the contributions.\n\n[1] Yue H, Cao C, Liao L, et al. Supervised raw video denoising with a benchmark dataset on dynamic scenes[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 2301-2310.\n[2] Guo S, Liang Z, Zhang L. Joint Denoising and Demosaicking with Green Channel Prior for Real-world Burst Images[J]. arXiv preprint arXiv:2101.09870, 2021.",
            "summary_of_the_review": "My major consideration is that this paper did not discuss related work very well and the contribution is not well proved.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a burst image processing network architecture. The contributions are three fold. First, a joint feature extraction and alignment module is proposed that removes noise and spatial/color misalignment in the input burst while enhances the high-frequency information. Second, features from each base-match image pair are shuffled by concatenating the corresponding channel-wise features, with the goal of encouraging inter-frame communication. Lastly, an adaptive group upsampling module is proposed for progressive fusion and upscaling.  \n\nTo evaluate the effectiveness of the proposed architecture, two common tasks, burst super-resolution, and burst low-light image enhancement, are used in the paper. It is worth noting that the datasets for training and evaluation contain raw bursts, thus BIPNet is capable of conducting joint demosaicing along with the SR and denoising tasks. \n\nOverall, the design of the proposed BIPNet seems to be well-motivated, and it is well-evaluated against SOTA networks on common datasets, showing noticeable improvement especially on burst low-light enhancement task. The contributing factor seems to be well studied in the ablation study as well. I will elaborate more in the following sections. ",
            "main_review": "*Strengths*\nHaving summarized the strengths in the previous section, here I will highlight using bullet point. \n- Well motivated design, that aims to fully exploit the multi-frame information. \n- Clear illustration of the components in BIPNet, and accompanying formulations. \n- Promising quantitative and qualitative results compared to baselines such as MFIR. \n- The nice addition of the ablation study, showing the contributing factors from different modules of the network. \n\n*Weaknesses*\nThe paper would be more solid if the following items can be addressed. \n- Novelty: Although BIPNet seems novel when edge boosting feature alignment, pseudo burst feature fusion, and adaptive group upsampling modules get put together, these modules alone, particular the latter two are not completely new innovations. PBFF seems to be inspired by the pixel shuffle layer (Shi et al. 2016) and maybe some image demosaicing work (e.g. Gharbi et al., Deep Joint Demosaicing and Denoising, Siggraph Asia 2016), while the AGU variant has been proposed in Shi et al. 2016 as \"slow fusion\". The exact mechanisms and modules in BIPNet are not new either. That said, the exact empirical combination seems to be novel and works well on the given tasks. \n- Insights: The paper would provide more insights if more analysis can be done on justify the design choices. For example, it is stated that the joint feature extractor/aligner may work better than separate feature extraction and alignment, and that removing noise early would be beneficial to the fusion. It would be nice to include more experiments (and visualization of the feature maps) to prove this. While ablation study does exist that shows quantitative comparisons of various BIPNet variants, the baseline (Resblocks + concatenation + transpose convolution) does not have explicit alignment module. Another example could be the observation on BIPNet handling color shifts better. What contributes to its behavior? Is it something common that other SOTA networks can leverage as well? \n- Evaluation: Well BIPNet seems to work much better than baseline networks on the low-light task, the margin is small on the SR task. It might be worth exploring more thoroughly in terms of the contributing factor and if the evaluation was conducted fairly. For example, SID dataset was chosen to demonstrate the low-light / denoising task, where leading burst models such as MFIR was not trained on, while MFIR uses other datasets for denoising evaluation. It might be worth comparing against more recent works in the low-light / denoising task with BIPNet. Secondly, regarding the SR evaluation, Table 1 does not look very convincing, in that the PSNR improvement against best SOTA model is small (0.37dB / 0.16dB), and that their SSIMs are identical. It is worth checking if BIPNet is indeed performing consistently well against MFIR in terms of both color and structure, or could it be the case that BIPNet only recovers color more accurately in general. \n- Loss function: please be more explicit about what loss functions are used in training for different tasks / datasets. What are their contributing factors? \n- Model complexity and runtime: In section 4 the parameter number is given. How does this compare to other SOTA networks? Could it be that the BIPNet works better simply because the network is deeper? Please also report the model inference runtime comparison as done in works such as MFIR. \n- Failure case: Lastly, including some failure cases would make the manuscript more complete and direct the readers for future work. ",
            "summary_of_the_review": "Overall, the paper proposes to combine some well-motivated modules/mechanisms for burst image processing and it seems to work well on common burst SR and low-light enhancement tasks. That said, more justification and analysis are necessary to prove the criticalness of these proposals. Based on these, my rating is borderline (leaning towards weak reject) but can consider raising it if the authors have a way to address such feedbacks, and if other reviewers want to champion this paper. ",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a burst image processing network that is applicable to image restoration tasks such as super-resolution and low-light image enhancement. The propose networks perform edge boosting feature alignment for noise removal, pseudo-burst feature for inter-frame communication, and adaptive group upsampling to increase spatial resolution. Experimental results for super-resolution and low-light image enhancement are provided compared to the existing SOTA methods. ",
            "main_review": "* Strengths\nCombining channel-wise features of multiple burst frames is different idea compared to the existing information combination methods.\n\nThe paper is well written and organized.\n\n\n* Weaknesses\nThe role of the proposed edge boosting alignment module has not been justified clearly compared to the existing methods. What if we replace the proposed edge boosting alignment module with conventional image alignment (registration) and denoising techniques as a preprocessing step of pseudo burst featuring?\n\nSome experimental results are not complete. For example,\n- Table 2 would be better to include the ablation study for real BurstSR validation datasets as well. \n- MFIR (Bhat et al., 2021b) should be also qualitatively compared in Figures 4 and 6. \n- LKR (Lecouat et al., 2021) should be also qualitatively compared in Figures 5, C.1, and C.2.\n- No methods were compared in Figure C.3.\n\n* Question\nThe propose network was fine-tuned for SR on real data. Were the compared existing methods also fine-tuned on real data for fair comparison?\n",
            "summary_of_the_review": "The propose idea of combining multi burst frame information in channel wise manner is somewhat novel, however, the experimental results are not completely impressive to show the superior performance of the proposed method compared to the existing arts. \n",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}