{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper is motivated by the fact that most of existing hierarchical and symbolic explainer models cannot guarantee\nthat the explanation/explainer model objectively reflects the internal logic of the deep model, and lack solid theoretical foundations for the trustworthiness of explanations. In the paper, the author proposed the so-called objectiveness of an explainer model, and a hierarchical model AOG was proposed to explain the inner logic of deep models. Moreover, three techniques were proposed to simplify the explaination.",
            "main_review": "The paper is motivated by the fact that most of existing hierarchical and symbolic explainer models cannot guarantee\nthat the explanation/explainer model objectively reflects the internal logic of the deep model, and lack solid theoretical foundations for the trustworthiness of explanations. In the paper, the author proposed the so-called objectiveness of an explainer model.\nStrengths: the idea of using a hierarchical model to explain deep models is interesting, and the model is guaranteed to satisfy a proposed property, so-called objectiveness, by some proven theorems. The motivation of this work is quite clear and the content of the paper is well organized. \nWeaknesses: The work lacks of direct and convincing experiments to demonstrate the main claim (that the proposal accurately illustrates all internal logic of the deep model, and generates exactly the same output as the explained deep model given any arbitrary input).\nSome questions:\n1. How does experimental result show that the proposed model better reflects the internal logic of the deep model than other explainers?\n2. Is there any theoretical \n3. Figure 3 shows that knowledge distillation cannot ensure the objectiveness of the student model. So for the same task, can the proposed model ensure that, namely, for any input sample, it generates the same output with the deep model does? If so, is the corresponding experimental result included in the paper?",
            "summary_of_the_review": "The paper is motivated by the fact that most of existing hierarchical and symbolic explainer models cannot guarantee\nthat the explanation/explainer model objectively reflects the internal logic of the deep model, and lack solid theoretical foundations for the trustworthiness of explanations. In the paper, the author proposed the so-called objectiveness of an explainer model. The idea is interesting and the paper is well organized. But lacks of convincing comparison with existing explainers to demonstrate the strong claim about the objectiveness.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes the use of And-Or Graphs (AOG) for explaining the\nlogic encoded in a deep model.",
            "main_review": "The paper proposes a way for computing rigorous explanations of deep\nmodels using an AOG and by exploiting a game theory formulation.\nExact computation of the proposed formulation seems\nimpractical. Hence, it is unclear what are the advantages of a\nrigorous formulation when it is not computed exactly in practice.\nThis is a drawback when rigor is the target.\n\nThe paper overlooks existing efforts on computing logically rigorous\nexplanations, which are tightly related with understanding and \nexplaining the logic of classifiers. An incomplete list of references\nthe address logically rigorous explanations is included below.\n\nThe claims about \"lack of objectiveness of previous explainer models\"\nare not correct given the related work the paper overlooks. Also,\nsince related work makes no approximations when computing\nexplanations, it would be important to assess the quality of the\napproximate results proposed in this paper.\n\nSome claims in the paper are unclear. For example, on the past\nparagraph of page 5, the claim seems to suggest that the AOG explainer\ncan replace the classifier. If this understanding is correct, then why\nwould one want to learn a deep model in the first place?\n\nSuggested references, chronological order:\n\nAndy Shih, Arthur Choi, Adnan Darwiche: A Symbolic Approach to\nExplaining Bayesian Network Classifiers. IJCAI 2018: 5103-5111\n\nAlexey Ignatiev, Nina Narodytska, João Marques-Silva: Abduction-Based\nExplanations for Machine Learning Models. AAAI 2019: 1511-1519\n\nAndy Shih, Arthur Choi, Adnan Darwiche: Compiling Bayesian Network\nClassifiers into Decision Graphs. AAAI 2019: 7966-7974\n\nNina Narodytska, Aditya A. Shrotri, Kuldeep S. Meel, Alexey Ignatiev,\nJoão Marques-Silva: Assessing Heuristic Machine Learning Explanations\nwith Model Counting. SAT 2019: 267-278\n\nlexey Ignatiev, Nina Narodytska, João Marques-Silva: On Relating\nExplanations and Adversarial Examples. NeurIPS 2019: 15857-15867\n\nAlexey Ignatiev: Towards Trustable Explainable AI. IJCAI 2020:\n5154-5158\n\nAdnan Darwiche, Auguste Hirth: On the Reasons Behind Decisions. ECAI\n2020: 712-720\n\nWeijia Shi, Andy Shih, Adnan Darwiche, Arthur Choi: On Tractable\nRepresentations of Binary Neural Networks. KR 2020: 882-892\n\nGilles Audemard, Frédéric Koriche, Pierre Marquis: On Tractable XAI\nQueries based on Compiled Representations. KR 2020: 838-849\n\nJoão Marques-Silva, Thomas Gerspacher, Martin C. Cooper, Alexey\nIgnatiev, Nina Narodytska: Explaining Naive Bayes and Other Linear\nClassifiers with Polynomial Time and Delay. NeurIPS 2020\n\nAlexey Ignatiev, Nina Narodytska, Nicholas Asher, João Marques-Silva:\nFrom Contrastive to Abductive Explanations and Back Again. AI*IA 2020:\n335-355\n\nStephan Wäldchen, Jan MacDonald, Sascha Hauch, Gitta Kutyniok: The\nComputational Complexity of Understanding Binary Classifier Decisions.\nJ. Artif. Intell. Res. 70: 351-387 (2021) \n\nJoão Marques-Silva, Thomas Gerspacher, Martin C. Cooper, Alexey\nIgnatiev, Nina Narodytska: Explanations for Monotonic Classifiers.\nICML 2021: 7469-7479\n\nAlexey Ignatiev, João P. Marques Silva: SAT-Based Rigorous\nExplanations for Decision Lists. SAT 2021: 251-269\n\nEmanuele La Malfa, Rhiannon Michelmore, Agnieszka M. Zbrzezny, Nicola\nPaoletti, Marta Kwiatkowska: On Guaranteed Optimal Robust Explanations\nfor NLP Models. IJCAI 2021: 2658-2665\n\nYacine Izza, João Marques-Silva: On Explaining Random Forests with\nSAT. IJCAI 2021: 2584-2591\n",
            "summary_of_the_review": "The paper proposes to explain deep models using an AOG and by\nexploiting a game theory formulation. Exact computation of\nexplanations using the proposed formulation seems impractical in\npractice. The paper overlooks existing efforts on computing rigorous\nexplanations, which are tightly related with understanding the logic\nof the classifiers.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper provides a different view to explain deep models. Specifically, it tries to explain the deep model as AND-OR logic. Then the decision procedures can be represented as AND-OR graphs. In this way, the explanations can show the interactions among different input variables. Experimental results show promising results. ",
            "main_review": "Pros:\n\nThe studied problem is very important and interesting. XAI is highly related to the model's trust and safety\n\nThe proposed method- using AND-OR operations to represent deep model logic is novel and reasonable. Using such axiomatic operations can help understand deep models in a symbolic and hierarchical way, which is a promising direction. \n\nThe theoretical support of the proposed method is solid and the experimental results are promising. \n\nCons:\n\nThe major concern is the experimental study. There is no baseline method for comparisons. Then it is not convincing that the proposed method can provide better explanations than other techniques. \n\nIt would be better to show some visualizations of explanations for both correct predictions and incorrect predictions. Can the AND-OR operations explain why the model makes incorrect predictions?\n\nThe methods seem to be very general. However, it's not shown that the proposed method can be used to explain image models and graph models. At least some discussions are needed. \n\nThe literature review needs improvement. For example, some interaction explanation methods are missing, such as [1] [2]. In addition, the GNN explanation techniques, such as GNNExplainer, XGNN, PGExplainer, etc., should be discussed since in graphs, the interactions can be considered as edges and it provides a straightforward way to study interactions.\n\n[1]. Yuan, H., Yu, H., Wang, J., Li, K. and Ji, S., 2021. On explainability of graph neural networks via subgraph explorations. ICML 2021.\n\n[2]. Duval, A. and Malliaros, F.D., 2021. GraphSVX: Shapley Value Explanations for Graph Neural Networks. arXiv preprint arXiv:2104.10482.",
            "summary_of_the_review": "Overall, while I think the studied problem is very important and the proposed method is novel and interesting, more experimental comparisons are needed, which can significantly improve the manuscript. Hence, I would recommend a weak acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors decompose the (numerical) output of a trained model into a sum of exclusive contributions of subsets of its (qualitative) input features. They define an \"objective explainer\" as a model that predicts the same outcomes as the model it explains, and show that having the same exclusive contributions for any possible subset of figures amounts to predicting the same outcome. They posit that this is equivalent to following the same logic, or reasoning, as the original model.\n\nThey show that such decompositions can be rewritten as AND/OR graphs, which provide an explanation for the original model outcomes. They suggest that because many exclusive contributions are close to zero, the graphs can be pruned and made sparse.",
            "main_review": "The ideas proposed in the paper are interesting, and this research direction is certainly worth pursuing. I have three main sources of concern. First, I found the paper very difficult to read, because the problems solved and underlying hypotheses are not clearly exposed, some definitions are repeated in different places in different forms  (e.g. the definition of objectiveness in introduction and section 3.2), and loosely related ideas are inserted in the discussion (e.g. the theorems 3.1 to 3.4, which are certainly interesting, but not used in the rest of the paper, or the properties of Harsanyi dividend which could be useful to help define the problems considered, but serve no clear purpose in the current paper). Second, the fact that model outcomes **can** be rewritten as a sum of marginal contributions of subsets of features does not imply that this is how the model actually computes its outcome. It is in fact unlikely, since most models have much less parameters than there are terms in the sum. Finally, the practical aspects of the method should be discussed in more details : as it stands now, computing the Harsanyi dividend for a subset S of k features implies evaluating the model on 2^k values. Many real world models have thousands of features, which makes the evaluation of Harsanyi dividends unpractical, unless some very efficient techniques are found for detecting their \"negligible operands\". More precisely:\n\n**Problem solved**\n\nIt seems that the methods proposed in the paper only apply to models predicting a single numerical quantity (noted v(.) in the paper), with input consisting of binary features (e.g. the presence of a word in a sentence). Sequential problems (i.e. most NLP), where the order and repetition of input tokens have an impact on the outcomes, are not considered. This is an important, but limited, class of models. It should be described in the introduction, and the hypotheses made clear. In particular, it would be interesting to relate the axioms in section 3.1 to the hypothesis on the model. For instance, the anonymity axiom seems to be equivalent to ruling out sequential problems (where the order of input matters).\n\n**The definition of objectiveness, and predicting the underlying logic of the model**\n\nIn the introduction, you define objectiveness \"as predicting the same outcomes for any possible input\", and add \"More crucially, we can derive the following deduction from this definition. An objective explainer model should not only generate the same output as the deep model, but also use the same underlying logic/reasons for inference.\" The second definition (i.e. same outputs and same internal logic) being stronger than the first, you cannot deduce the second from the first unless they are equivalent.\n\nIn section 3, you state that a numerical prediction from N binary features can always be rewritten as a sum of 2^N terms, corresponding to all possible subsets of the N features (i.e. so all possible values of the function). This is obviously true. However, it is very unlikely that this decomposition corresponds to the underlying logic of the model you are trying to explain. Most models will have much less than 2^N parameters, and no way of memorizing the 2^N coefficients in this sum. They might learn a sparse approximation to the sum (ie become Support Vector Machines), but this would need to be proven. In fact, I see no intuitive reason why they would: these sum models are linear in the 2^N binary variables, deep networks rely on nonlinear activations of their inputs (and therefore span a larger space of possible outcomes).\n\nThe definition of objectiveness, and what you mean by \"same underlying logic/reasons for inference\" should be clarified. I think you propose that an approximation to the sum of the 2^N exclusive contributions of subsets is an objective explanation of the deep model, this need to be discussed and justified. \n\n**Practical aspects**\n\nComputing the Harsanyi dividend of a set S of k features implies computing model outcomes over all its subsets, i.e. 2^k evaluations. These evaluations are needed if one wants to test objectiveness (as per your Corollary 1), or use objectiveness to discover an explainer (as per the baseline methods you suggest). As soon as the total number of features gets over a few hundreds, this will become prohibitive. Unless fast methods are provided to evaluate v() in parallel over many subsets, the Harsanyi dividend will be very difficult to use. \n\nA similar problem appears with the AND/OR graphs proposed in section 3.3. Supposing that all Harsanyi dividends can be computed as an AND/SUM tree, algorithms will be needed for finding the sparse nodes and converting the tree into an AND/OR graph.\n\n**Misc.**\n\nThe formula for I({take, it, easy}) at the top of page 4  seems incorrect, from the definition you provide, it should be \nI({take, it, easy}) = v({take, it, easy})−I(∅)+I({take})+I({it})+I({easy})−I({take it})−I({take, easy})−I({it, easy}) (i.e. the contribution of two word subsets are subtracted, but those of one word subsets are added). \n",
            "summary_of_the_review": "The authors propose interesting ideas, which are definitely worth pursuing, but major rewrites and clarification are needed before this paper can be accepted in a conference. In particular, \n- the problems and models considered should be clearly stated, and the link with the axiomatic properties of the Harsanyi dividend clarified\n- the meaning of objectiveness as in \"same underlying logic\" should be discussed and justified\n- practical methods for scaling to problems where N>100 should be proposed\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}