{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors investigate how to train binary neural networks operating on graphs, which to-date have included many full-precision operations, despite claiming to be binary networks.\n\nThe primary contributions of this work are that they investigate the ordering of the operations necessary to implement a graph layer, and demonstrate that significant accuracy can be induced if the implementer is not careful. They also show that a mixture-of-experts approach is one way to boost model capacity.",
            "main_review": "Detailed feedback:\n\n**Abstract**:\n- I think this is fairly well written. An immediate red-flag in my mind is that you've chosen to focus exclusively on \"large-scale\" datasets, which I would argue is perhaps the least interesting application of this work.\n\n**Intro**:\n- The second paragraph is not great IMO. I can certainly agree there are applications for this work, but this paragraph does not sell them very well. It's debatable how much the value of these techniques are in recommender systems. If you can illuminate this point more, I think the readers will appreciate it. It's certainly correct to say, however, that there are some interesting applications for edge inference (perhaps talk about them more, and refocus the paper on them)?\n- 3rd paragraph: it's good that you mentioned this -- I did not know this myself, despite working in a closely related area.\n\n**Preliminaries**:\n- In my opinion it is slightly confusing to use $\\mathcal{S}$ to indicate the sign function. It's a minor complaint, but consider changing it for readability.\n- I think the way you've introduced GNNs is confusing to people who aren't necessarily familiar with the area. You've just stated that there's a message passing framework, and jumped straight to saying what GraphSAGE is. You will want to add more exposition here. It's also not particularly clear to me why you focus on GraphSAGE\n- It's not correct to say you're just using the straight-through-estimator (STE). You're also using clipping in that definition, and it's an important thing to note.\n\n**FFBGN**:\n- Figure 1 would probably be easier to interpret if you more clearly show what the difference between the two variants is. At present the reader has to think for a little while.\n- You mention that you must quantize the adjacency matrix to 4 bits. It should be noted that this was already done by Degree-Quant by Tailor et al. -- there's no novelty here. On that note, I find it really odd that binary GNN papers never cite quantized GNN papers: they clearly count as related work. Make sure to cite papers in this area.\n- I do like that you've spend the time to discuss the differences between the two orders. It's a subtle, but important, point.\n- \"Challenges\" is a very vague title by the way.\n- There's a very important premise here in this work that these additional floating point operations are significantly slowing down our inference. Is that true? What evidence do you have for that? It's an important question to ask since the real bottleneck in these architectures is the sparse operations.\n\n**Experiments**:\n- I commend you for picking larger datasets than many graph works. I do appreciate it. However, I would have liked to have seen that you considered datasets where it's necessary to generalize to unseen graphs. Otherwise much of the utility of this work is not delivered on.\n- You use PNA as a baseline -- but more detail is needed, since there are a _lot_ of hyperparameters you can tune on that model.\n- I am nervous that you don't tune learning rate. That can make a massive difference, and you should consider that.\n- The results in the top table of Table 3 look good (makes sense, given the argument in the paper).\n\nYour conclusion is rather short.\n\n**Strengths**\n- I think the point about considering the ordering of operations is fairly reasonable. It is not really considered. This work also shines light on some questionable practices in other works, which I think is also important.\n- The authors did at least try to use large datasets -- this also counts for a lot. Although I don't think these constitute a complete set of datasets.\n- The results look strong.\n\n**Weaknesses**\n- I am unconvinced about the novelty here. Certainly, I don't consider using MoE to be very interesting. Also, is it not a fair ablation study to compare to the equivalent \"wide\" network?\n- The datasets need to consider the problem of generalizing to unseen graphs. This is a very important usecase for this work that goes un-assessed.\n- You have chosen odd networks to choose as your baselines. Would it not be sensible to include GAT and GCN as well?\n- Finally -- and very importantly -- although you claim to eliminated some FP ops, how much difference does this actually make in practice? It's unclear. It's important to ask, because most of the latency associated with GNNs is due to the sparse ops, which are memory bound. We're not generally too worry about other things.",
            "summary_of_the_review": "The work is not without merit, but there are some clear issues that need to be addressed for now. I am sure with some effort the authors could substantially improve this work. For now, however, I think this work is not good enough to be accepted.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper investigates the binary graph neural networks, called full-precision-free binary graph neural networks (FFBGN). They first find an effective computation order and then introduce a mixture of experts to increase the model capacity, achieving better results than others.",
            "main_review": "Strengths\n\n* Binary graph neural network is a promising research direction, and this paper clearly reviews the prior work.\n* The complexity analysis of the proposed approach and baselines are included.\n* The evaluation covers large graph datasets, e.g., ogbn-papers100M.\n\nWeakness\n\n* I think that the adopted computational order is not fair to be compared with other baselines. For example, in fig. 1 (b), the activation after aggregation is never been binarized so that the further addition can call both full-precision and low-precision parameters. Also, this is not a fully binary neural network as the values of A are quantized to 4-bit instead.\n\n* The results in Table. 4 are quite comparable to other BGCN baselines, the author may want to highlight the strongest one with the bold format so that we can get a quick assessment of how generally work is it.\n\n* The content of this paper is less than nine-page, is that because the authors have not got enough content or time to finish it?",
            "summary_of_the_review": "This paper proposes binary graph neural network variants that explore different computation orders and adopt a mixture of expert strategies.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies how to binarize the extra full-precision operation in the matrix multiplication of graph convolution operations. A well-designed re-quantization technique is proposed to realize a full-precision-free graph neural network. This paper presents a dynamic graph convolution module, a mixture of experts (MoE), to solve the capacity reduction challenge caused by re-quantization. Finally, the proposed architecture is evaluated on three large-scale datasets and outperforms other baselines to achieve state-of-the-art performance. ",
            "main_review": "**Strengths:**\n\n- This paper is well written and easy to follow.\n\n- This paper shows strong experimental results. The proposed FFBGN outperforms the existing binary graph neural network with less theoretical FLOPS.\n\n- This paper studies the capacity decreasing problem and re-quantization order issue in full-precision-free graph convolution, and the proposed method is technically sound. \n\n**Weakness:**\n\n**Confused definition and probably overclaimed contribution:** The title of this paper is binary graph neural network, however, this paper uses 4-bit quantization in Eq. (2) and Eq. (3), which means using non-binary graph convolution for all networks. \nThough the authors have stressed that using ternary representations {-1, 0, +1} is unreasonable, there is no reference and no ablation study on this part. \nAnother confusing point is that even ternary representation conflicts with the title of binary graph neural networks. The authors need to give a more precise definition of binary graph neural networks. The current description is misleading. \nThe \"full-precision-free\" term in the title may overclaim the actual contribution of this work! Though the full precision matrix multiplication is not used in the graph convolution, full precision addition operation still exists according to Eq. (8), which means that we still need to store the full precision tensor in memory.\n\n**Insufficient ablation study:** The authors used 4-bit quantization in the proposed binary graph convolution module and evaluated the MoE design on three different datasets together with this quantizer. A more detailed ablation study is necessary to show the real improvements from MoE design and 4-bit quantization. \n\n**Insufficient description about training details:** The MoE is related to the dynamic graph convolution, and similar techniques have already been used in some existing binary convolution neural network designs. For example, in [1], the authors stressed that the optimization of dynamic binary convolution is harder than original binary convolution because of the discrete decision during training. However, this paper does not provide any detail about optimizing dynamic graph convolution. Is the two-stage optimization of the dynamic gating module still necessary for FFBGN? If it is not necessary anymore, can the authors give a more detailed description of the reason? Will the authors make the code open-source? This will help answer many of the above questions.\n\n**More analysis on re-quantization:** The authors found that using different computation orders in re-quantization results in a large accuracy gap, which is interesting and valuable. However, the authors do not give a further analysis of why different orders differ so much. Can this come from the module-specific sensitivity or other reasons?\n\n**More issue:**\nAppling MoE structure results in k-times more parameters, as shown in Figure 1. But the authors do not tell how many experts are used to compare existing works in Table 3 and Table 4.\n\n[1] Adrian Bulat et al., HIGH-CAPACITY EXPERT BINARY NETWORKS, ICLR 2021\n",
            "summary_of_the_review": "Due to the existing weaknesses, I recommend rejection, but I will consider adjusting the decision according to the rebuttal.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}