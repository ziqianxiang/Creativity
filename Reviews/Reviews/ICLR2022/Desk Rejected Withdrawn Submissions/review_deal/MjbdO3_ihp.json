{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This work is concerned about time series analysis. It aims to learn effective representation for time series in an unsupervised manner such that this representation can be used for downstream tasks such as classification, forecasting, and anomaly detection. This work proposed two ideas. One is to use dropout technique to replace time slicing to generate better augmentation that can retain global context and long-term dependency of time series. The other one is to utilize both temporal and spectral information of time series. To achieve this, a framework called Bilinear Temporal-Spectral Fusion (BTSF) is proposed to explore the interaction among these two types of features. A deep neural network is designed upon BTSF. Experimental study is conducted to demonstrate the advantage of this work over the existing ones in the literature.",
            "main_review": "The strengths of this work\n1. Time series analysis has extensive applications. Unsupervised learning of representation of time series is an important topic to explore. \n2. The idea of using dropout to generate augmented data for time series is neat, and it works well as demonstrated by the experiment.\n3. The idea of exploring both temporal and spectral information of time series and using bilinear operation to capture the interaction of these two types of information to attain better representation is sound and works effectively. \n4. This paper is well organised and presented, with detailed information about the development of representation learning for time series. \n\nThe weaknesses of the paper\n1. The main concern of this work lies at its novelty and technical contribution. Firstly, although the idea of using dropout for augmentation works well, it does not seem to be a significant contribution. Secondly, the proposed BTSF and the resulted network do not bring sufficient new insights. The idea of bilinear operation has been well used in the recent literature. Its efficacy in modelling the interaction of different types of information has been well realized. In this sense, the contribution in this regard, although welcome, is not that significant either. \n2. Whether using the dropout-based augmentation will increase the computational load since most part of a time series will be retained as input? Please comment.\n3. In the experimental study, it is mentioned in several places that the proposed unsupervised method performs better than supervised methods. Why is it this case? Please provide more information on this. \n4. On anomaly detection, how the performance is exactly measured? It is unclear why it is based on the reconstruction error of a given input. Please clarify. \n5. On Figure 4 and the paragraph immediately before it: the horizontal axis of Figure 4 is similarity or distance? Intuitively, BTSF shall achieve the highest mean value about feature \"similarity\" of positive pairs. Please check the use of similarity/distance in the paragraph. \n6. On Figure 5, it is good to see the feature distribution of BTSF is relatively evenly distribution. Meanwhile, why BTSF leads to this result does not seem to be clearly explained. Please comment.\n",
            "summary_of_the_review": "This is a well organised paper and the work is conducted with good quality. The issue addressed in this work is of importance and the two ideas are nice and sound. The experimental study demonstrates the efficacy of the proposed work. The main concern of this work is at its novelty and technical contribution since this work is largely built upon existing techniques.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No issues.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies the unsupervised representation learning of time series data. It proposed Bilinear Temporal-Spectral Fusion, which use the instance-level augmentation by dropout. Then, an iterative bilinear temporal-spectral fusion module is designed to model the interaction between temporal and spectral features. The experiment evaluation is performed on classification, forecasting and anomaly detection tasks.",
            "main_review": "On the presentation side, the authors make some strong claims, which lack both practical and theoretical justification. For example, on page 3, \"With the instance-level contrastive pairs, our method has the ability to capture long-term dependencies and completely eliminate the sampling bias which is superior to previous segment-level pairs.\", on page 4, \"This bilinear feature completely conveys the fine-grained time-frequency affinities to acquire a more discriminative feature representation.\".\n\nOne of the main components of the proposed method is the iterative procedure characterized by Eq. (5). Given the temporal and spectral features and their interactions, the rational of these equations are unclear and unconvincing. Moreover, in the ablation study of the experiment section,  the results about the impact of iterative bilinear fusion are based on a different setup of experiments from the full setup version i.e. Tab .5 v.s. Tab. 1 and 2, thereby leading to incomparable results and uninterpretable comparison. \n\nUsing factored weight matrix to model interactions is a well studied idea. e.g. in the works regarding recommendation systems, 2021, WWW, DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems. This paper's technical contribution is limited in this regard. \n\nIn the related work section, a relevant direction of research about representation learning on sequence data is missing and some examples are as follows. It would be good to explain the comparison conceptually or experimentally to these works.\n\n2015, NeurIPS, A Recurrent Latent Variable Model for Sequential Data\n\n2016, NIPS, Sequential Neural Models with Stochastic Layers\n\n2017, AAAI, Structured Inference Networks for Nonlinear State Space Models\n\n2021, ICLR, Mind the Gap when Conditioning Amortised Inference in Sequential Latent-Variable Models\n\n*** \nAfter the exchange with the authors in the rebuttal phase, I would like to increase the recommendation score to 6. \n\nOverall, this paper shows some promising experiment results of contrastive representation learning on time series for practitioners. \nBut, since the proposed method, I think, is mostly built on top of some existing ideas/techniques with minor adaptations, it lacks new insights. And thus I would give the weak recommendation of acceptance. \n***\n",
            "summary_of_the_review": "This paper studies the unsupervised representation learning of time series data. It proposed Bilinear Temporal-Spectral Fusion, which use the instance-level augmentation by dropout. Then, an iterative bilinear temporal-spectral fusion module is designed to model the interaction between temporal and spectral features. The experiment evaluation is performed on classification, forecasting and anomaly detection tasks.\n\nTechnically, the paper makes use of some well studied methods for representation learning of time series. The contribution and novelty are marginal. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a variant of the contrastive learning style approach to unsupervised representation learning for multivariate time series with 2 key novel differences.  First, in this case, entire series are passed as instances, with time points dropped out using random dropout, and separate dropped out variables are used to form negative examples, as opposed to selecting sub-segments to generate positive and negative examples - with a motivation of capturing better the long term patterns that may occur across entire time series and may be lost with the sub segment approaches.  Second, multiple representations of the time series are used and fused as part of the model - in particular spectral representations (from FFT) as well as the typical temporal representations.  In particular, an input series is transformed with FFT and both the input series and frequency domain transformed series are mapped to representations with separate encoders, and these representations are then combined via cross multiplication, and then summarized again to derive a final representation.\n\nThe authors compare to a variety of recent contrastive learning style approaches to unsupervised representation learning on time series, on 3 different tasks (forecasting, anomaly detection, and classification) - showing general improvement across the board.  They also perform extensive ablation study and analyses.",
            "main_review": "**Strengths**\nI think the approach is interesting, and from the contrastive learning line of work introduces 2 key novel aspects, to my knowledge: using the dropout approach, and fusing both temporal and spectral information in the representation - both of which are interesting and seem well motivated.  \n\nThe experiments are also extensive and seem to demonstrate the effectiveness of the proposed method compared to prior contrastive learning approaches, and the comparison on 3 tasks as opposed to, e.g., just classification, is great to have (as it is rare to see forecasting and anomaly detection).  Additionally the extensive ablation study is very helpful.\n\n**Weakness/issues**\n\n1.)\n\nI feel one key weakness is leaving out whole alternate directions of research for unsupervised representation learning of multivariate time series (i.e., these are not even mentioned) - that have recent methods showing state-of-the-art results and out-performing some contrastive learning approaches.\n\nIn particular, there are 2 key missing state-of-the-art methods (representing classes of approaches that aren't mentioned) for unsupervised representation learning for multivariate time series that should be compared with.  These are: ROCKET (random feature approach), and the recent Transformer approach (published in KDD June this year, but was already published on Arxiv since last year).  Both of these have been shown to out-perform the contrastive learning style approaches and ideally should be compared with:\n* Zerveas et al. \"A transformer-based framework for multivariate time series representation learning.\" KDD 2021.\n* Dempster et al.. \"ROCKET: exceptionally fast and accurate time series classification using random convolutional kernels.\" Data Mining and Knowledge Discovery 2020.\n\nThe transformer approach also has some key similarities to the proposed approach, as it uses the whole series as well, and also uses random masking of the series (which is very similar to using random dropout as is done here), to create varied training instances. It also evaluates the approach on multiple down stream tasks, (classification, regression, and imputation - the last in the appendix).  Additionally both the transformer approach and Rocket are state of the art methods that have out-performed at least the contrastive learning approaches they were compared against, and should be compared with. \n\n\n2)\n\nThe paper is somewhat poorly organized and a bit confusing in the presentation.  I.e., many methods are briefly introduced in the introduction and the reader is expected to understand them and the differences, but they are not fully described until they are mentioned again in the much later related work section (which also has some duplication as a result and feels a bit redundant).  This could use some reorganization.\n\nIn general, the intro is hard to follow.  It is not clear in the introduction what is meant by spectral - as this is not described or explained until much later.  Also, unless the reader is already quite familiar with the topic and the work mentioned in the introduction (and particularly contrastive learning approaches), the introduction would be hard to follow and just confusing, and Figure 1 is not adequately explained.\n\nOne more minor comment, for Figure 3 - the xlabels are hard to read / associate with the correct x tick on the x-axis - it would be better if each text ended at each notch.\n\n\n3) Some experiment details lacking and raise questions about the results, and results summary could be improved.\n\nFirst, it would be helpful to present summary significance information to identify if the improvement of the proposed approach is really significant, especially when there are so many datasets and comparison methods and it can be hard to read from the details - and averages alone across all datasets can potentially be misleading.  Critical difference diagrams (which summarize the result of pairwise significance tests across the large collection of methods and datasets) seem to be the standard for evaluating time series classification methods at least - so this would be helpful to see here, e.g., see: \n* Fawaz et al. \"Deep learning for time series classification: a review.\" Data mining and knowledge discovery 2019.\n* https://github.com/hfawaz/cd-diagram.\n\nSecond, it is never explained how the hyper parameters were selected - which makes me question how this was done, as there are many hyper parameters to tune.",
            "summary_of_the_review": "Overall I feel the work is interesting and provides novel additions to the contrastive learning direction of research for unsupervised time series representation, with extensive experiments and ablation study provided.  However, the leaving out of the other recent state-of-the-art work in alternate directions for unsupervised time series representation learning, the paper organization, and the questions around the experiments and results cause me to think the paper is not quite ready for publication unless some of these can be addressed.\n\n\n***Update***\nAfter author responses and revisions I feel they have addressed most of my comments and I appreciate the work done and revisions made.  I still view the lack of proper hyper parameter selection procedure (i.e., using one or more validation sets from the training to select hyper parameters) as part of the experimental process as a concern, especially given the need for the hyper parameter to be set precisely to those values to get the good reported results, so it may be hard to achieve those results in practice (select the right hyper parameters), and also may not be a fair comparison to all other methods if they did use some proper validation set tuning.  However, they have provided hyper parameter sensitivity results, and will release the code, so I am willing to move to the side of accept and not stand in the way of accepting the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes an interesting unsupervised representation learning method\nfor time series data. This is an important problem because of the lack of annotated samples and the need to improve the prediction for down-stream tasks in data-sparse scenarios. Different from segment-level augmentation, the authors propose a new sampling method using different dropout masks on the entire time series. The idea is to preserve the global context information and long-term dependencies, which are very likely to be lost in the segment-level sampling strategy. The authors also propose a bilinear temporal-spectral fusion module to explicitly encode the affinities of abundant time-frequency pairs and iteratively refine representations of time series through cross-domain interactions. \n",
            "main_review": "The methods have been described in detail. The algorithms were evaluated on three tasks classification, forecasting and anomaly detection, and have been shown to outperform baselines on these tasks. I have a few comments to this paper. \n\n1. It would be better to clarify some notation used in the paper, e.g., the value of m, n, d. Also, the first step in Equation 4 is not a standard matrix product. The author should clarify this. \n\n2. The proposed method includes the temporal-spectral fusion and the dropout-based sampling strategy. It is not clear whether the dropout strategy is useful or not. Maybe the authors can consider testing another method without using the dropout sampling strategy (but using the standard segment-based sampling method).\n\n3. An advantage claimed in the paper is that the proposed method can better preserve long-term data patterns. It would be more convincing to show this advantage in down-stream real tasks (e.g., climate modeling) through better visualization. \n",
            "summary_of_the_review": "Overall, the paper is well written, and the proposed method is interesting. The results have shown improvement on different down-stream tasks. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes Bilinear Temporal-Spectral Fusion(BTSF) for unsupervised representation learning in time series. For contrastive learning, they use an instance-level augmentation by applying dropout on entire time series to preserve global context and capture long-term dependencies. Then, the BTSF module is iteratively run to encode the affinities of time-frequency pairs and refine time series representations with S2T and T2S aggregations. Experiments are done on three downstream tasks: time-series classification, forecasting, and anomaly detection.",
            "main_review": "There are several merits and following questions regarding the paper. \n\n1. Simple is the best. Dropout showed the highest classification accuracy on the HAR dataset among 12 augmentation strategies. Did Dropout perform the best for other datasets too?\n\n2. Interesting to see that instance-level augmentation is better than segment-level augmentation. This was unintuitive for me since time-series has temporal coherence and everyone considers using that characteristic. What kind of slicing technique is used in this paper? Are the majority of slicing techniques inferior to Dropout augmentation for contrastive learning? \n\n3. In the proposed Bilinear temporal-spectral fusion, temporal dimension and spectral dimension are selected, then its information is encoded. Then the final representation of the input time series is obtained after iterative BTSF rounds. In the manuscript, the purpose is described as \"We aim to encode cross-domain affinities to adaptively refine the temporal and spectral features through an iterative procedure\". Can it be thought of as working similarly to convolution?\n\n4. Below sentence is confusing.\n\"According to results, no augmentation method, not excepting time slicing, is able to improve performance on all datasets consistently.\"\nIt sounds like time slicing is the only augmentation method to improve the performance of all datasets. But, the proposed method choose dropout augmentation rather than slicing-based augmentation.\n\n5. Below sentence may require reference? \"In contrast, time slicing fails to deal with the periodic time series because it is possible for them to choose false negative samples.\"\n\n\n<Typo>\np. 6: settins \n",
            "summary_of_the_review": "Contrastive learning framework for time-series data, showing that instance-level dropout augmentation works well as positive pair, then exploits temporal and spectral dimension of time-series encoding and its bilinear fusion. Experiments backed up the claims well. The simplicity of the algorithm will bring subsequent contrastive learning methods.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}