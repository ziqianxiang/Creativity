{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes to use label smoothing together with adversarial training to improve the adversarial robustness of a model (SSAT). The paper interprets the label smoothing process as adding a regularization term, and adaptively controls the corresponding regularization term (i.e., controls the label smoothing level) as the adversarial training continues. Also, the paper proposes the cooperative defense, which combines the baseline defense model with SSAT. Experimental results show that their method is effective in most of the experimented setting. ",
            "main_review": "Strength: \n\nFairly extensive experiments are done and both SSAT and co-SSAT show positive gains. Particularly, strong generalization ability across unseen attacks (Table 1) seems to be a plus. Generally the experimental results are quite positive. \n\nWeakness:\n1. $\\lambda_1$ and $\\lambda_0$ seems to be an important & sensitive hyperparameter. It is not clear how we can choose them for a given data. As shown in Table 12/13, the effect of them is quite significant, and chooseing them by cross-validation is too costly. (since you need train the model multiple times to see the variation across multiple attacks and multiple datasets...)\n2. The co-SSAT scheme in (11) also seems quite expensive and using $\\tau$ adds another hyperparameter to choose, which adds the complexity. \n3. Results only on CIFAR-10 are given. Would all the results carry over to the large-scale dataset setting, like using ImageNet? ",
            "summary_of_the_review": "I think the paper generally shows positive results, but the details seems to have too much overhead for their scheme to become more practical. Namely, results on more larger datasets would be helpful. The sensitivity on the hyperparameter seems to be a limitation. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the author analyzed label smoothing (LS) in adversarial training (AT). They prove that the LS increases the local smoothness of a loss surface and propose a new training scheme, i.e., surface smoothing adversarial training (SSAT). They also observed that the representation space of their proposed scheme and other AT differs. Based on this observation, they extend SSAT and incorporate it with another AT method to improve the overall robustness.",
            "main_review": "**Strength**\\\nThe overall presentation and writing are clear and easy to follow.\\\nThe author gave an intensive analysis of the proposed method. If the analysis is based on AutoAttack (Croce et al., 2020), the claim will then be convincing. \n\n-----------------------------------\n**Weakness**\\\nThe main weakness is the evaluation. The paper's evaluation is questionable; hence, it is hard for me to believe the reported value. \n\nThe evaluation of the paper needs to be more strict and should follow recent evaluation papers such as Tramer et al. 2020, Croce et al., 2020.\n* The author should consider adaptive attack (Tramer et al. 2020), i.e., by directly attacking the proposed method Co-SSAT. For instance, if the attacker can access all parameters (and know the defense mechanism), it is natural to attack the indicator function with continuous approximation.\n* The main robustness table should consider AutoAttack (Croce et al., 2020), the state-of-the-art attack method. As in Appendix H.2, it is hard to claim that the proposed method is robust under AutoAttack. Especially, SSAT shows a significant degradation; hence, hard to believe the results of other attacks.\n* Hard to understand the motivation of the half-white box attack. Rather than such attacks, utilizing more advanced black-box attacks (Andriushchenko et al., 2020) will be more convincing.\n\nThe author should compare with Qin et al., 2019, which directly smoothen the loss surface.\n* The proposed method is an indirect way to smoothen the loss surface, while LLR (Qin et al., 2019) is a direct way to smoothen the loss surface.\n* Hard to understand why one should consider an indirect way rather than existing direct method?\n\nImportant baseline (Chen et al., 2021) is missing.\n* This baseline also utilizes two independent models, utilizes LS, and also considers loss surface smoothness.\n* I indicate that the definition of loss smoothness slightly differs in this paper; however, it is still an important baseline.\n\nIt seems like the author did not consider robust overfitting (Rice et al., 2020). \n* The accuracy of base adversarial training (Madry et al., 2018) is too low. The author should consider robust overfitting and report the best robust accuracy during training or use validation set to use early stopping.\n\nDoes the theorem still hold under robust training, i.e., min-max optimization, rather than the standard training, i.e., just minimizing the cross-entropy loss. \n\n-----------------------------------\n**Some references**\\\nMadry et al., ICLR 2018, \"Towards Deep Learning Models Resistant to Adversarial Attacks\"\\\nQin et al., NeurIPS 2019, \"Adversarial Robustness through Local Linearization\"\\\nTramer et al., NeurIPS 2020,\" On Adaptive Attacks to Adversarial Example Defenses\"\\\nCroce et al., ICML 2020,\" Reliable Evaluation of Adversarial Robustness with an Ensemble of Diverse Parameter-free Attacks\"\\\nAndriushchenko et al., ECCV 2020,\" Square Attack: a query-efficient black-box adversarial attack via random search\"\\\nRice et al., ICML 2020, \"Overfitting in adversarially robust deep learning\"\\\nChen et al., ICLR 2021, \"Robust Overfitting may be mitigated by properly learned smoothening”\n",
            "summary_of_the_review": "I recommend rejection. There exists a standard for the adversarial robustness evaluation, and I believe that the paper's evaluation needs to be more strict.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proves that label smoothing smoothens loss surface to improve the robustness but decreases the distance between logits of the true label and of other labels, which can cause the vulnerabilities against targeted attacks. From the analysis, this paper proposes surface smoothing adversarial training (SSAT), which only applies label smoothing on the data points far from decision boundary. In addition, this paper proposes Cooperative-SSAT (Co-SSAT), which selectively uses the models trained by standard adversarial training and by SSAT based on the Difference of Logit Ratio. Experiments demonstrate that SSAT outperforms several baselines in terms of robustness against FGSM, PGD, CW, and transfer attacks.",
            "main_review": "# Strength\nI think it is interesting and somewhat new that the proposed method changes the strategy of label smoothing based on the distance between the decision boundary and the data point. In addition, this paper provide several interesting empirical observations. For example, this paper shows how SSAT and standard adversarial learning differ for responses to PGD and CW.\n# Weakness\n- Several papers have attempted to improve adversarial robustness by using label smoothing or logit squeezing [1-5] but fail due to the sensitivity to targeted robustness [2,3]. SSAT tackles this problem but also fails to improve the robustness as shown in Table 10: SSAT does not work well against Auto Attack. From these results, I could not think the adaptive label smoothing in SSAT is useful. \n\n- Co-SSAT achieves the best robust accuracy against AutoAttack. However, I think that it is not fair that Co-SSAT, which uses two models, is compared with other baselines that use only one model.\n\n- Although SSAT is based on label smoothing and similar to other logit regularization methods, they are not compared with SSAT in experiments. If SSAT is superior to other label smoothing methods, SSAT might be useful while it does not improve the robustness against AutoAttack.\n\n- I think the similar theoretical results are shown in [3]. This paper shows that the gradient of label smoothing is bounded while [3] shows that the Lipschitz constant, which can be upper bound of the gradient, of label smoothing exists.\n\n[1] Warde-Farley D. and Goodfellow I. \"11 adversarial perturbations of deep neural networks.\" Perturbations, Optimization, and Statistics 311 2016  \n[2] Engstrom Logan, et al. \"Evaluating and understanding the robustness of adversarial logit pairing.\" arXiv 2018.  \n[3] Kanai, Sekitoshi, et al. \"Constraining Logits by Bounded Function for Adversarial Robustness.\" IJCNN 2021.  \n[4] Kannan, Harini, et al. \"Adversarial logit pairing.\" arXiv preprint arXiv 2018.  \n[5] Shafahi, A. et al. \"Batch-wise Logit-Similarity: Generalizing Logit-Squeezing and Label-Smoothing.\" BMVC\n",
            "summary_of_the_review": "I tend to vote to reject because experiments using AutoAttack show that SSAT is not a strong method, and the vulnerability of label smoothing seems to be not solved though it is the main goal of this paper.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "[Summary]\nThis paper proposed instance-dependent label smoothing (LS) for adversarial training (AT). \nTo be specific, data closer to the decision boundary have weaker LS; data farther away from the decision boundary have stronger LS. ",
            "main_review": "[Strength]\n1 This paper investigates adaptive LS for enhancing robustness. However, this paper finds that adaptive LS cannot defend against CW attacks well; therefore, this paper proposes to combine both AT-model and LS-AT model to defend against adversarial attacks.\n\n[Weakness]\n1 Theoretical analysis in Sec. 3.1 is quite standard and does not relate to adversarial examples.\n\n2 On page 5 (below eq 8), there is an argument that  \"the above method with LS focuses more on the smoothness of CE loss surface, which may ignore the attack of target adversarial example.\"\nCould I interpret it in this way that LS can lead to significant issue gradient obfuscation that degrades the true robustness? \nIf so, the main argument in this paper is that \"robustness via adaptive LS\" is not supported well.   \n\n3 In Figure 2 (b). It is really hard to infer the stated messages that illustrate the superiority of the proposal. \n\n[Questions]\n1 In Figure 6, why PGD is up-trend, but CW is down-trend over LS degree?\n\n2 In Lemma 1, I get confused about why constraining the margin between any logits could induce better robustness. \nUsually, constraining the margin between any logits could obfuscate gradients, which leads to an illusion of higher robustness. \n\n3 Besides, I doubt the message from Theorem 1. Let us think in an extreme way: the loss surface is entirely flat. The flat loss will also harm the generalization, which puts no use of robustness. ",
            "summary_of_the_review": "Some arguments in this paper are not convincing to me (e.g., will smooth loss indeed produce high robustness in AT?). \n\nI recommend not accepting this paper. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/a",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed \"surface smoothing adversarial training (SSAT)\" that incorporates an adaptive label smoothing technique along with an adversarial training framework to improve the robustness. The authors claim that \"LS can smooth the loss surface through narrowing bounds of local gradients of models w.r.t adversarial perturbations\".\n\nThe authors further claimed that \"SSAT shows better robustness against untargeted attacks to targeted attacks\" and devised a heuristic technique (Eq. 10) to distinguish them. The model is then incorporated SSAT with the standard adversarial training-based models to improve robustness (Co-SSAT).\n\nFinally, they claim to achieve the state-of-the-art performance for CIFAR-10.",
            "main_review": "The paper is mostly well written and the main claims are easy to understand. In the following, I present the main concerns of this paper:\n\n#1. The claim that \"LS can smooth the loss surface through narrowing bounds of local gradients of models w.r.t adversarial perturbations\" actually leads to obfuscated gradients [1,2]. We can verify this from the following facts (see [3] section 3.1 and 3.2):-\n\na. (Table 1) Single-step white-box FGSM attack is stronger than iterative PGD attacks.\nb. (Table 1) As the strength of attack increased (i.e. $\\epsilon = 8/255 => 12/255 => 16/255$), PGD attack could not circumvate the defense. However, single-step white-box FGSM attack becomes significantly stronger.\n\n#2. (Table 2) The performance of SSAT is significantly lower under CW attack compared to PGD attack. However, the authors did not include the results corresponding to CW attacks in Table 1.\n\nIn particular, it seems that the effect of label smoothing is very similar to the existing defensive distillation model [4] that was later broken by CW attack [2]. Figure 3 further demonstrates this phenomenon.\n\n#3. (Appendix H.2) Performance against Auto-attack demonstrates in Table 10 demonstrates that incorporating SSAT with existing TREADS and MART models actually degrades their performance. Finally, it is not clear how much of these performances are contributed from the SSAT.\nHence the claim of \"achieving the state-of-the-art performance for CIFAR-10\" is not correct.\n\nIn general, the proposed Co-SSAT is a heuristic technique of distinguishing targeted and untargeted attacks. However, the claim that \"SSAT shows better robustness against untargeted attacks\" might not be evaluated using \"correct\" adversarial attacks.\n\n#4. What happens when the number of classes is large (e.g. CIFAR-100)? Please provide the results.\n\n\nGeneral comments on how to design attacks:\n\n#1. The definition of robust accuracy is the lowest accuracy produced by a classification model within a given threat boundary. Hence, you should compare the lowest accuracies produced by SSAT, Madry et al., TREADS, MART, etc.\nAlso, please follow the protocol for designing adversarial attacks as described in [3] (Section 3.1 and 3.2)\n\n#2. We observe that single-step FSGM produces lower performance against SSAT. It indicates that using a higher learning rate for PGD attacks might use to obtain better adversarial examples. We can see that Auto-attack in Table 10 actually produced much lower accuracies for SSAT. It starts with a high learning rate and iteratively reduces it to obtain effective adversarial examples.\n\nAlso, execute this strategy for CW attacks as well.\n\n\n[1] Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples (https://arxiv.org/pdf/1802.00420.pdf)\n\n[2] Towards Evaluating the Robustness of Neural Networks (https://arxiv.org/abs/1608.04644)\n\n[3] On Evaluating Adversarial Robustness (https://arxiv.org/pdf/1902.06705.pdf)\n\n[4] Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks (https://arxiv.org/abs/1511.04508)",
            "summary_of_the_review": "1. The claim that \"LS can smooth the loss surface through narrowing bounds of local gradients of models w.r.t adversarial perturbations\" actually leads to obfuscated gradients.\n\n2. The claim to achieve the state-of-the-art performance for CIFAR-10 is incorrect.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}