{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper aims to design an efficient transformer network. It analyzes the redundancy on the encoder of the transformer pipeline. Based on the analysis, a decoder-only DETR is proposed by removing encoders and designing a CECA module which efficiently fuses features of different scales generated from the backbone. It achieves a good performance on the COCO dataset. Authors also did ablation studies to verify the effectiveness of the proposed module.\n",
            "main_review": "- This work proposes a computationally efficient transformer pipeline for detection. Authors conduct ablation studies to verify the proposed CECA module.\n- The main concern is the limited technical novelty. CECA module is efficient to fuse different scales generated from backbone. However, it is a more engineerin part. The two added lossed are also explored previously. \n- As the proposed network takes advantage of two explored losses, IoU-awarenses and Token Labeling loss, it seems it is not a fair comparison to other methods in terms of accuracy, shown in table 1, 2 and 3. For example, in table 4, the accuracy of D^2ETR achieves 45.3 without two losses, box refinement and two-stage, which is lower than 48.3 (Deformable DETR PVT2) \n- It would be better to report the inference speed. Due to the speed in memory accessing, GLOPS may not reflect the actual speed. \n",
            "summary_of_the_review": "The work removes the encoder in Transformer and design a new module(CECA) to compensate for accuracy. However, the novelty is marginal. And some results are not fairly comparable.  At the current stage, I prefer the below borderline as the rating. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work designs a novel decoder-only DETR, which is a very efficient detection transformer. It achieves superior performance than DETR and Deformable DETR with much smaller FLOPs. The model training also converges much faster.",
            "main_review": "This work builds a detection head on top of a transformer-based backbone and avoids adding additional encoder on top of the backbone. The backbone already captures global receptive field size information. It also propose an efficient way of fusing multiscale feature maps, named CECA. The CECA module significantly boosts the model performance under low training epochs. The paper also adopts IoU awareness loss and token labeling to improve the performance. Combining these improvements, the model achieves better accuracy and FLOPs trade-off than existing work. Partial of the improvements actually comes from adopting PVT backbone, but the paper has clearly break down the improvements.",
            "summary_of_the_review": "This work is a practical and meaningful work and has shown promising results of making DETR more efficient. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "- This paper proposes a decoder-only detector called D$^2$ETR to attend to the fine-fused feature maps generated by the Transformer backbone with a novel computationally efficient cross-scale attention module. D$^2$ETR demonstrates low computational complexity and high detection accuracy in evaluations on the COCO benchmark, outperforming DETR and various variants.\n\n- The overall idea looks concise and the results look effective.",
            "main_review": "### Method & Illustration issues:\n\n- The claim of \"decoder-only\" seems over-claimed. The proposed CECA is essentially a special encoder architecture that exploits multi-scale feature maps with a reversed FPN style (low-resolution feature map as query and the previous high-resolution feature maps act as the key-value). Besides, the design of the fusing block shown in Figure-2 (c) seems to lack novelty considering it simply applies the SRA scheme (proposed in PVT) to enable the cross-scale information interactions.\n\n- Figure-1 shows the advantage of the proposed method over Deformable DETR, the authors are encouraged to explain why the attention maps look better?\n\n- The use of mathematical symbols in the formulas of the submission looks very irregular and the authors are encouraged to improve them. For example, vector $x$ should be represented with $\\textbf{x}$.\n\n- Figure-2 is misleading as the original DETR does not exploit the low-level feature maps and I guess it mainly refers to the Deformable-DETR scheme.\n\n\n### Experiment issues:\n\n- According to Table-4, the proposed CECA gains +7.8/+1.8 based on \"DETR/Deformable DETR + decoder-only + PVT2\". The authors are encouraged to explain why are these two performance improvements so different?\n\n- The advantages of the proposed method look relatively minor considering the Conditional DETR already achieves AP=43 with $90$GFLOPs. The authors are encouraged to report the results of \"Conditional DETR + IoU-awareness + token labeling\" and \"Deformable-DETR + IoU-awareness + token labeling\" under comparable settings.\n\n\n### Other issues:\n- There are a lot of misspellings of words in the current submission, for example,\n-- \"By organizing the cross-attantion in which\"->\"By organizing the cross-attention in which\"\n-- \"end-to-end detector flexiblely\"->\"end-to-end detector flexibly\"\n-- \"Futhermore\"->\"Furthermore\"\n\nThe authors are required to check all the errors with the help of related tools such as Grammarly (https://app.grammarly.com/). ",
            "summary_of_the_review": "- The writing of the submission needs to be improved according to the above comments.\n\n- The novelty looks marginal considering the core components borrow the idea of FPN, cross-scale attention of SMCA (with different query/key/value set definition), SRA of PVT.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper presents a decoder-only transformer based object detection model. Following the architecture of DETR, the key change this work brings is a cross-scale fusing module, called Computationally Efficient Cross-scale Attention (CECA). CECA aims to replace the transformer encoder to solve the efficiency bottleneck when using multi-scale feature maps extracted from the feature backbone. To improve the model training, two additional loss terms (IoU awareness and Token Labeling) are also used. ",
            "main_review": "**Strengths**\n\n1. Good performance for DETR type models on object detection with short training (e.g. 50 epochs)\n\n2. The proposed module could be an enhance to the feature backbone of DETR. \n\n\n**Weaknesses**\n\n1. The terminology of *decoder-only* is confusing and misleading. Whilst the transformer encoder is removed here, but another module is introduced. So overall, the total number of components are kept the same as the previous DETR series. For the overall complexity, it is not convincing that the proposed architecture is simpler. In terms of design, I do not think transformer encoder is more complex than the proposed module. \n\n2. In essence, the key problem addressed in this work is about computational complex with feature pyramid for Transformers in object detection pipeline (DETR in particular). Under this view, an intuitive approach is to apply linear transformers, such as Linformer, Performer, Nystromformer. However, this direction is totally missing in this paper. I would think this leads to a big concern that this whole paper is not properly positioned at the very beginning of problem formulation and analysis. It could be the most serious defect of this work in the current form.  \n\n3. From the design of CECA, it is hard to see how it solves the slow convergence. \n\n4. The computational cost of CECA is significantly reduced by applying spatial downsampling to the key elements (the collection of all low-level scales). This is reducing the resolution, but conceptually an operation against multi-scales.  \n\n5. The loss terms for IoU-awareness and token labeling are taken from existing works, so they are not main contribution of this work. \n\n6. Experiments: Overall, the loss terms (IoU-awareness and token labeling) are not disentangled in the reported results. Particularly, \n>- Figure 3: What loss function is used for DETR is not clear. The same as D$^2$TR or not? Why the proposed model can fast train is not clear. \n>- Table 1&2: The effect of loss function is not disentangled. It is important to understand the respective gain by architecture design and loss function.\n>- Table 4: It is unclear if these components after '+' are added up continuously or individually. From this table, the baselines of *DETR (or Deformable DETR) + decoder-only + IoU-awareness + token labeling* is not given, if I understand correctly.",
            "summary_of_the_review": "This paper suffers two fundamental issues:\n\n1. The key problem is not spoken out clearly -- the high computational complexity with multi-scales for DETR series. So I think this paper's overall position/context is not placed right. The writing is misleading. \n\n2. The exact effect of the proposed CECA is not tested out due to in mixture with two loss terms (IoU-awareness and Token Labeling).\n\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}