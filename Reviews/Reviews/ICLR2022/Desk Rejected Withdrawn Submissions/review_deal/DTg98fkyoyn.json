{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes an unsupervised noise synthesis pipeline based on contrastive loss. \nThe main contribution is that it is the first work that uses UCL for noise synthesis. ",
            "main_review": "Strengths\n- The unsupervised contrastive loss is novel, especially for the cases where noisy and clean pairs are infeasible to get. \nWeaknesses\n- It is hard to follow Section 3.3, maybe adding some examples or figures could help. \n- The training set is Kodak which contains only 25 images. Since the proposed pipeline is unsupervised, and there are tons of image data out there, why not using more training data to show the power of unsupervised training? \n- From Fig.1 and Fig. 3, it is hard to tell which one of (c) and (d) is better? Maybe adding some explanations is better. \n- In Assumption 1, it mentioned that \"noises in patches from the same image should be smaller than those from different images\", which makes sense to me. However in Section 3.3, it mentioned \"Specifically, we define two patches within the same image instance as a positive pair if they overlap. Two negative patches are defined as two patches without any overlap, either in the same image instance or in different image instances.\" I feel there are inconsistencies, and are patches from the same image but without overlap are positive pairs or not? ",
            "summary_of_the_review": "I have some confusion about several points mentioned below, which makes me uncertain about the contributions of the paper. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes an unsupervised noise synthesis framework, Contrastive Noise (CoNo). CoNo uses MMD as a distance metric in contrastive learning. It also utilizes consistency (i.e., perceptual) and distributional alignment losses specialized in this noise synthesis problem. This paper verifies the effectiveness of CoNo by comparing CoNo with popular statistical noises, AWGN and NLF.\n",
            "main_review": "### Strengths\nS1. This paper well adapts contrastive learning to the noise synthesis problem. \\\nS2. This paper is generally well-written, so it is easy to read. \\\nS3. The intuition of the proposed method (Assumption 1) is clear, so I think the method is reasonable and well-designed.\n\n---\n### Weaknesses and Questions\nW1. Why MMD instead of cosine distance?\n- This paper uses MMD as a distance metric instead of the cosine distance. However, I cannot find why MMD should be considered rather than cosine distance in this problem.\n\nW2. Few (quantitative) experiments.\n- This paper only provides a few experimental results, so I am not convinced that the proposed method is really useful and effective in practice. When is unsupervised noise synthesis useful?\n- If the noise synthesis framework can be used as a data augmentation strategy, supervised/self-supervised training (for image classification) with the noise would be one evaluation metric.\n- Why do high PSNR and SSIM values imply that the synthesized noises are realistic? They might be \"easy-to-denoise\" noises, not realistic ones.\n- There is no ablation study, so I am not sure that whether each component is essential or not.\n  - Without \"perceptual loss\", what happens? Also, do we need a \"pre-trained\" encoder for the perceptual loss? This requirement for the pre-trained encoder might be a weakness because the pre-trained encoder does not exist in some domains (e.g., medical images).\n  - For MMD, how many samples do we need? Or, can we use the cosine distance?\n- There is no comparison with supervised learning methods. The methods might be considered as an upper bound as self-supervised learning literature did.\n\n---\n### Editorial Comments\n- Section 4.1., Baselines paragraph: ( Eq. 2) => (Eq. 2)\n- Section 4.3., Empirical Results paragraph: the synthetic noise as . => I think something is missing\n",
            "summary_of_the_review": "I think this paper well incorporates the idea of contrastive learning into the noise synthesis problem. However, I am not convinced that the proposed method is really effective in practice because this paper only compares the proposed method with naive noise synthesis methods under a limited number of experimental settings.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper constructs a neural model for synthesizing noise in images. The model is based on a generic pretext task instead of real noise measured from images, or analysis of processes that generate noise in real world. The paper claims that the resulting noise is realistic but gives no convincing theoretical or experimental support to that statement.",
            "main_review": "The premise of the paper is puzzling. The used pretext task essentially only requires the noise to be somehow dependent on the image, and further regularization terms later enforce, e.g., the desired magnitude for the noise. This approach would make sense if there were only one way for noise to be signal-dependent, and if this were the case the method should find it. But this is obviously not true.\n\nThe paper claims that the noise “shows advantages over the noise synthesized by traditional statistical models both qualitatively and quantitatively” but shows neither.\n\nFirst, the images in the paper are somehow incorrect, apparently having been compressed or resampled so that their frequency profile has changed considerably. The AWGN images show spurious correlations such as disproportional amount of horizontal and vertical features and splotchy coloration. I double-checked this by adding various amounts of Gaussian noise to the top image in Figure 1(a) and could not reach anything that looked like the image in column 1(b). Figure 1(b) simply looks wrong, as do all other purported instances of AWGN in the paper. Given this discrepancy, I cannot place any trust on the arguments based on visual comparisons. This includes the mention in Section 4.2 about the preferences of external viewers. To be convincing, a perceptual test would require a thorough description of test protocol and results in any case.\n\nIn Section 4.2, the paper argues that statistical noise models have several weaknesses by design that CoNo could in theory avoid thanks to its Glow-based design. However, there is no attempt to explain why CoNo still wouldn’t have those weaknesses, or any number of new ones, given that its training is not based on data on realistic noise but just the oversimplified pretext task and regularization terms. The training does not consider how noise is formed in a digital camera pipeline, in analog photography, or in the visual system of a biological observer. Still, CoNo is purportedly more realistic than previous models that attempt to mimic these processes. This doesn’t make any sense to me.\n\nAfter reading the paper, my theory is that the properties of the noise synthesized by CoNo arise solely from the inductive biases of the Glow model used. If these inductive biases happen to be beneficial for, e.g., data augmentation for training classifiers, that would be an interesting result, but the extremely terse quantitative evaluation in Section 4.3 is not convincing in this regard.\n",
            "summary_of_the_review": "The paper builds on the premise that a simple pretext task and some regularization will result in the generation of realistic, signal-dependent noise after unsupervised training of a neural noise model. Without any consideration to how noise is formed in real-world processes, or training data from which this could be learned, the idea could only work if there was just one possible way to generate noise that fulfills the pretext task. As this is not true, the paper is chasing an impossible goal, and the properties of the generated noise, or its usefulness in any application, remain unknown and unproven.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}