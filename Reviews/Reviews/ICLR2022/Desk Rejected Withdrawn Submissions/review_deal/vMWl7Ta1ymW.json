{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "Though it does not explicitly name it this way, this work essentially proposes using a physics-informed neural network (representing a PDE) to substitute the integration required for a Neural ODE, while also proposing an algorithm for learning the underlying governing differential equations for a given task at the same time as training the neural network. This allows the method to be applied to tasks that do not have \"known\" governing equations, such as image classification.",
            "main_review": "The whole introductory and methodological section of the paper is rendered more confusing the necessary due to two issues. First, though they do cite the proper previous work (so this is NOT an issue of misattribution), the influences from, for example, physics-informed neural networks (PINN) are not explicitly acknowledge when explaining the proposed method. Acknowledging such connections explicitly and naming them would make understanding the work much easier. For example, noticing the fact that what is usually a neural network that is integrated in a neural ODE, is here replaced by a PINN that does not need to be integrated.\nSecond, the lack of a concrete example makes the general formulation presented confusing. Though it is fine to present the method in general terms, since this is a work utilizing an analogy between PDEs/ODES and neural networks, and applying these to a non-physical task (image classification), it would help to have a concrete example stating for example, what are initial/boundary conditions in this setting, what does the solution function and governing equations represent when classifying an image, etc.\n\nThough the proposed idea is interesting, the experimental evaluation appears to be lacking somewhat. For image classification, the proposed method is not able to outperform ResNets for one SVHN. Then, on Tiny ImageNet and CIFAR, baseline results are presented using only MobileNet, instead of a more conventional (\"close to SOTA\") convolutional network, such as the previously used ResNets, for example. The network incurs additional training (and inference time in some cases), and as such should present improvements in results over well-established baselines.",
            "summary_of_the_review": "This work proposes an interesting approach to learning both a physics-informed neural network and its underlying governing equations at the same time, which allows it to be applied to tasks that do not in fact have \"known\" governing equations, such as image classification. Though this approach is interesting, the results do not seem robust as of now and due to that I will classify this paper as marginally below the acceptance threshold. Nevertheless, I am interested in reading the authors response to the points above.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper is motivated by the recent advances in learning (neural) ordinary differential equations and aims at tackling their drawbacks of solving a computationally demanding integration equation during inference. Therefore, the authors propose to jointly learn a partial differential equation along with a network that predicts is solution for (continuous) time and space variables based on initial conditions that represent the data. Several loss terms along with a sophisticated training scheme are developed to jointly optimize both parts. The training scheme is partially analyzed theoretically before several experiments demonstrate that the proposed framework yields a good trade-off between efficiency and accuracy.",
            "main_review": "Strength\nThe idea to jointly train a governing PDE along with a network predicting its solution is very interesting. It can be interpreted from several perspectives including a sophisticated regularization of arbitrary (common/popular) image classification networks. With admittedly limited knowledge of the directly related work, the technical idea seems novel and the experiments demonstrate some advantages in the trade-off between accuracy, speed and memory usage. \n\nWeaknesses\nMost strikingly, I think that neither Theorem 3.1 nor its proof are correct. The theorem seems to vaguely state that any minimizer of the cost function can be obtained via alternating minimization. If this is supposed to imply that this is possible independent of the starting point, it would be extremely surprising (or impossible for multiple minimizers). The main argument for the claimed convergence is \"We quit the while loop when the sum of all the loss values converges and does not decrease in Alg. 1, which corresponds to the definition of the Nash equilibrium. Therefore, our algorithm always returns an equilibrium state.\"  But just claiming that something converges because it was used as a stopping criterion in a while loop appears highly unscientific. In fact, the entire proof needs to be reconsidered and rewritten. For instance, starting with \"i) we first prove that the forward problem is well-posed\" and concluding the paragraph with \"If well-posed, the solution of the forward problem becomes a special case of the Cauchy problem and its solution uniquely exists.\" does not mean one has proved well-posedness. It still appears as an assumption in the last sentence. \n\nThe proposed approach does obtain decent results on a number of different problems. Yet, the margin to plain approaches like MobileNet V3 is not very large, and aspects like algorithm 1 containing a loop over four different training/optimization problems along with a vast variety of detailed choices mentioned in the supplementary material, makes me wonder about the amount of finetuning (e.g. choices of regularization parameters like 2e-4 or 2e-5, sometimes using weight decay, using different optimizers like Adam and SGD for different parts of the training, different learning rate schedules, using different numbers of epochs before updating the governing equation, etc). Was the same effort spent on finetuning the competing approaches?\n\nFinally, it is advertised that one can \"consider both d and t as continuous variables. Therefore, it is possible to construct flexible hidden vectors at arbitrary dimensions and layers.\" But then stating in the supplementary material that d is discretized for efficient processing seems contradictory. \n\nAs a minor comment, the following sentence is missing something \"robust models tend to produce feature maps suitable for transfer\nlearning than regular models do\".",
            "summary_of_the_review": "While the overall idea is intersting, I consider the mathematics in Theorem 3.1 and its proof, as well as the seemingly heavy amounts of application/dataset specific finetuning to be highly problematic. Thus, in its current form I cannot recommend an acceptance of this work. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper describes a new partial differential equations (PDE) based model for image classification. The authors depart from the previously known Neural ODE based method by proposing a system of conforming PDE equations and a regression model.\n",
            "main_review": "\nWhile the idea is original and has merits, there are the following main points to be improved before the paper could be recommended for publication:\n1) clarity of writing: the reviewer perceives that while the motivation is clear, some details may seem confusing and it is difficult to follow some of the parts (e.g. Equation 1 appears as it is without informing the reader about its physical interpretation or motivation behind it) (see the questions below)\n2) there should be more space dedicated to the justification of the proposed method as detailed below\n\nClarity of writing:\n1) Page 1, point 1 of the list: why is interpretation of t a limitation of Neural ODEs? Point 2 of the list: should a smaller number of parameters of neural ODEs not be considered an advantage rather than a limitation? Pinckaers and Litjens seem to be presenting it as an advantage reducing memory and parameter costs.\n2) the referencing of governing equations is not commonly used terminology in the machine learning, particularly in Neural ODE, community; therefore, it is important that they are introduced somehow in the background description. \n3) Some paragraphs in the introduction are not well structured: for example, the statement that the model does not solve integral problems seems to be scattered throughout the introduction\n4) Eq. 1: given that it’s the first equation which is related to the proposed idea, there is a need for motivation. For example, in Neural ODE paper (Chen, 2018), it is immediately clear from the form of the ODE proposed by the authors and the text that the equation models the velocity of hidden layers.  This sets a clear physical interpretation and intuition behind the approach. In this paper, the background and motivation of the equation is unclear from the text: what do the alpha parameters in the governing equation actually mean and why is it the third-order equation? How would the readers follow the authors’ thoughts and come to the same conclusion?\n\nJustification of the method:\n1) In eq. 5, the square sum of the  values  of function g(*) is minimised; is it just an arbitrary choice of a distance to satisfy the PDE conditions? \n2) In appendix E, the hyperparameters of the model are described, with some of them offering an interesting insight into possible tweaks necessary to stabilise the training method: \"For MNIST, we update the governing equation and (d, t) pairs every epoch, and for SVHN, we update the governing equation and (d, t) pairs every 5 epochs.” Taking into account this, is it possible to perform some experiments showing different frequencies of updating the governing equation parameters and how it would influence the training dynamics? Furthermore, are there any guarantees that the gradient descent-based method will converge to the proposed cooperative equilibrium? ",
            "summary_of_the_review": "The paper has a merit of novelty, but there are two sticking points which do not allow me to recommend the acceptance as is: \n1) clarity of writing as it may be hard to actually follow the narrative due to the terminology and confusing statements\n2) justification of the proposed method should be improved\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper trains neural networks for image classification tasks. In particular they regularise the function learnt by the neural mapping to be close to a PDE within a predefined class of PDEs. The upshot of this is that you can solve Neural ODE-like systems faster. They put forward a system of regularisers to achieve this. They then test this system on pure image classification and run ancillary tests for OOD classification, adversarial attacks, and transfer learning. ",
            "main_review": "Advantages\n\n- The proposed method is indeed as fast as a straight up neural net.\n\n- The regularisation technique proposed is interesting in that is regularises the learnt function class directly. \n\n- I believe this work is novel in this context (at least I have not seen it elsewhere), although the exact methods are not new\n\n- I believe the methodological work to be sound and error free\n\nQueries\n\n- I do not understand why you are using PDEs to regularise CNNs to solve non-PDE-based problems. This would make sense for solving PDE-based problems, which are abundant in the exact sciences, but for image classification I do not see why this should help.\n\n- The results in Table 2 show that the PR-Net is about as performant as the ResNET, with a similar inference time. Furthermore, Tables 3 and 4 indeed show improvements over baselines, but I do wonder why you chose these baselines in particular? I would a standard ResNet would also achieve numbers in this range. Without error bars (calculated over multiple runs), I also cannot tell if these numbers are significant (in a statistical sense).\n\n- Do experiments on robustness really demonstrate that learning a governing equation helps? Indeed the numbers in Table 5 appear to indicate that your method is superior to baselines in this sense, but I think a more meaningful experiment here would have been an ablation, where you switch off the governing equation loss term during training and compare against that.\n\n- Figure 5 inversion quality: I’m not sure I can see whether M.Net V3 or PR-Net has better inversion quality. I would say that they appear roughly similar. Furthermore, if this is a desirable property, why not just optimise for it as well?\n\nMinor notes\n\n- It is not obvious to me why you use PDEs instead of ODEs (in section 2). Is this because you consider image-based problems? This is not explicit in the text.\n\n- Equation 1: If I understand correctly the governing equation represents a family of different PDEs, which you wish to be able to solve. It took me a while to understand this. Perhaps it would be useful to provide some examples of PDEs and the settings of \\alpha, from which you could retrieves said PDEs from the “governing equation”\n\n- Section 3, importance of governing equations: I think it would be better to place the importance of governing equations in the main text, rather than in the Appendix, since it is the foundations for the rest of the paper. That said, the results in Figure 8 for the Allen-Cahn equation are not entirely rigorous, since we only see 2 methods side by side, tested on a single initial condition. It would be good to make this analysis more extensive.\n\n- How to Solve Forward Problem ii): Why do you drop the boundary condition?\n\n- Theorem 3.1: I think it would be useful for the reader if a definition for a cooperative equilibrium were provided in the main text. Also, what is the upshot of this?",
            "summary_of_the_review": "I think the method is simple and straightforward, but the task domain itself is poorly chosen. I believe that if the authors had targeted a physics-based/PDE-based set of tasks, this would have been much more appropriate. \n\nI also feel that the experiments do not necessarily demonstrate the effectiveness of this method in its best light. These is a matter for methodology (error bars, p-values, appropriate baselines, etc.) but there is also a matter of choice of what experiments to perform. For instance, experiments on robustness should have contained ablations, where the regulariser is switched off, to measure the strength of that regularisation effect.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors extends previously proposed ODENets to consider partial differential equations. ",
            "main_review": "(I was also reviewer of this study in NeurIPS. As this manuscript seems to be just a shorter version of NeurIPS manuscript without too improvements to the paper, I also re-use my review at least with the points still valid in this version)\n\nThe method proposed in the paper is seems bit interesting and results looks somehow good (at least giving better scores than baselines). However, the problem with the paper is the presentation of the method is not so well understandable. The method seems really complex or at least it is a combination of several things, so giving a proper description can be a challenging task, but perhaps I could try to give some points for improvement. Furthermore, I am also not very confident about using PDEs as prior knowledge for image data, but it's hard to say anything definite about that as their method is not fully understood. \n\nFirst, Introduction is quite confusing. For instance, they mention \"governing equation describing $\\frac{\\partial h(d,t)}{\\partial t}$\" but they do not describe that what they mean with \"governing equation\". Later the text gives impression that governing equation would be $\\frac{\\partial h(d,t)}{\\partial t}$, which is contradicting as it is just the temporal derivate of $h$ (i.e. not really an equation). Considering PDEs, I would expect to see something like $\\frac{\\partial h(d,t)}{\\partial t}=\\alpha\\Delta h(d,t))$ which can be a governing equation in thermal conduction problems. They say they learn a governing equation and a regression model, but, as I said, it is unclear that what these actually are? Perhaps authors could take an example PDE and build introduction on top of that? Or alternatively, maybe authors could first describe the method of Chen et al (2018) and then explain how their study extends this ODE work (if it is a such direct extension, I am not sure).\n\nThis is bit improved in 2.1 as they describe the form of PDEs they consider (Eq. (1)), but this description is still bit imprecise: the definition of $g$ is not a \"PDE\", for PDE you would also need to set, for example, g(d,t;h)=0. Otherwise chapter 2 is quite understandable. \n\nSection 3 tried to describe the proposed method. But there are quite many confusing items:\n- \"governing equation is also extended to $g(d,t; f, h(0),\\theta)$\" (line 131). What this actually means? How governing equation depends on the initial condition? And what do you actually mean by a \"neural network\" if $g$ is also such (referring the footnote)? And why there is $\\theta$? Are $\\alpha$s now included in $\\theta$? (if so, I would suggest to keep those separate to avoid confusion). \n- Eq(8): It is said that X is a training set and $x\\in X$. But what is $x$? That notation is not used anywhere else.\n- Algorithm 1: what \"Train ..\" actually means? Is it calculating gradients and updating weights as typically done? But then there is two lines of \"Train $\\theta$ ..\" so you first update $\\theta$ to one direction and then perhaps another direction?\n- Algorithm 1: \"Train $(d,t)$ with $L_T$..\", finding some kind of points $(d,t)$ using a task specific loss? What is the purpose of this?\n- Algorithm 1: \"Validate with $V$ and update the best model\", there are several models competing with each other?\n- Algorithm 1: \"  Is \"training\" done for a mini batch or complete set of data? \n- etc\n\nPerhaps this question could also be considered: I guess extracted features are specific for each sample/image $x$ and the extractor is learned using data, but is the governing equation ($\\alpha$s) specific for an image $x$ or is there just one for dataset?\n\n- Eq (1): the form seems to be a rather general nonlinear PDE involving time, even thought there are also PDEs which does not fall in that category (e.g. PDEs with higher order temporal derivatives such as wave equation or Poisson equation which do not involve time at all). Perhaps this form is a good choice for the task they consider but this kind of discussion is not included. \n\n- page 3, two lines below (1): I do not know why $g(d,t;h)$ should always be zero? It could also be non-zero, for example, if zero is replaced with a driving force/point source term (this term can also be included into $g$ but (1) does not have a such term).\n\n- Table 1: should also boundary condition considered as part of data in the forward problem? \n\n- Table 1 & first line of Section 2.2: the initial condition and boundary conditions are parts of the solution $h(d,t)$ (those are $h$ with specific choices of $d$ and $t$) so perhaps it is not necessary to consider those as separate data for the inverse problem? \n\n- Figure 8: what is \"G.E.\"?\n\n- Equation numbers should be in parenthesis, for example, Eq 1 -> Eq (1)\n",
            "summary_of_the_review": "The idea of the method proposed in the paper is seems quite interesting and results looks somehow good, but the presentation of the method is not understandable. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}