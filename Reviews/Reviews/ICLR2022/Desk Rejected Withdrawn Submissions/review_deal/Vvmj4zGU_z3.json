{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper provides a new perspective for the compatibility of label smoothing and knowledge distillation. A new definition named \"systematic diffusion\" is proposed to analyze the contradiction between the claims of [1] and [2]. By observing the visualization of the penultimate layer representations, the authors claim that KD is compatible with LS under the setting of lower temperature T. The paper conducts a large-scale evaluation of the impact of increasing temperature T in KD in the perspective of the systematic diffusion. The findings are interesting and attractive. The visualization looks good.\n\n[1] Rafael Muller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help? Advances in Neural Information Processing Systems, 2019\n[2] Zhiqiang Shen, Zechun Liu, Dejia Xu, Zitian Chen, Kwang-Ting Cheng, and Marios Savvides. Is label smoothing truly incompatible with knowledge distillation: An empirical study. International Conference on Learning Representations, 2021",
            "main_review": "Pros:\n1. The proposed findings in the paper can be valuable for the community. The compatibility between KD and LS is extremely comprehensively discussed. The claim that \"use lower T for KD in the presence of an LS-trained teacher to avoid systematic diffusion\" is convincing.\n2. The paper conducts large-scale evaluations for the findings.\n3. The proposed \"systematic diffusion\" provides a new perspective for the community. \n\nCons:\n1. The paper doesn't provide a theoretical explanation for the systematic diffusion.\n2. The depth of the paper is not enough, and there are many repeated claims. It would be better if the authors could do a more in-depth exploration on the relationship between $\\alpha_{LS}$ and $T$. This paper only shows an empirical observation between $\\alpha_{LS}$ and $T$: the smaller $\\alpha_{LS}$ (smoother label) should correspond to the lower $T$. It would be better to provide a scheme that could be used to adaptively adjust $\\alpha_{LS}$ and $T$ in the training.\n3. KD requires two hyperparameters $\\alpha_{KD}$ and $T$, which are already intractable in practice. Following the findings of this paper, if LS and KD are compatible, then the LS-trained teacher model will add another hyperparameter $\\alpha_{LS}$. The paper could conduct some extended experiments to investigate the sensitivity of these three hyperparameters.\n\nMinors:\n\n1. On page 7, \"This results\" -> \"This result\"\n2. The expression should be consistent. e.g. \"Fig. X\" and \"Figure X\", \"See Figure 2\" and \"see Figure 2\".",
            "summary_of_the_review": "The findings are interesting and convincing. However, considering the theoretical value and the depth of this paper, I vote for \"6: marginally above the acceptance threshold\".",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper follows two related works, (Müller et al., 2019) and (Shen et al., 2021b), to further investigate whether label smoothing and knowledge-distillation are compatible. The key takeaway is that a large temperature would hurt distillation from a teacher trained with label smoothing, lower temperature is suggested to use with an LS-trained teacher.",
            "main_review": "**Strengths and Contributions**\n\nThis paper discussed the compatibility between label smoothing and knowledge distillation from the viewpoint of temperature, which is novel and a good supplement to the existing research. This is also a good contribution to the area of knowledge distillation.\n\nExperiments have been conducted with different teacher-student architectures and several tasks, including fine-grained classification, neural machine translation, and compact student network distillation.\n\nThis paper proposed a metric, diffusion index, to measure systematic diffusion.\n\nThis paper is well organized and easy to follow.\n\n**Weaknesses and Concerns**\n\nMy main concern is that an LS-trained teacher would have smoother predictions than a normally trained teacher. An LS-trained teacher is also likely to produce smoother predictions than a normally trained teacher with the same temperature. Therefore, the smoothnesses of the two teachers' predictions are not at the same level. The smoothness would be the most important factor that affects the performance of distillation rather than whether the teacher is trained with label smoothing. How can we exclude the confounding effect of smoothness? Is it fair to compare the performances of distillation with the same temperature rather than with the same level of supervision's smoothness? In other words, I think distillation is more sensitive to the smoothness of supervision than the used temperature. If so, it is natural to use a lower temperature for distillation from LS-trained teachers, as both high temperature and label smoothing contribute in the same way.\n\nA main claim in this paper is that a high temperature leads to a systematic diffusion of penultimate representations for LS-trained teachers. Do normally trained teachers also suffer from this? I would like to see more comparisons between normal student and LS-distilled student with both higher temperatures, rather than only LS-distilled students with different temperatures. In this case, it would be more clear that the effect of high temperature is only applicable to LS-trained teachers rather than normal teachers, as it is known that high temperatures affect the distillation of normal teachers.\n\nFigure 2 seems to be less informative. Figure 2 shows examples of how the temperature affects the predictions of an LS-trained teacher. How about a normally trained teacher? Would the observation be different? If not, this figure seems to be redundant.\n\nTables 2, 5 and 9 show how the temperature affects the effectiveness of distillation from a normally trained teacher and an LS-trained teacher. Four values of temperature are considered: 1, 2, 3 and 64. A suggestion here is that a smoothed growing of temperature would be better. There is a large gap between 3 and 64, and 64 is an extremely large temperature that is not often used in practice. Results with more temperatures like 5, 10, 20 would be reasonable.\n\n**Typos**\n\nSection 6, Line 10: a LS-trained teacher -> an LS-trained teacher",
            "summary_of_the_review": "Overall, I acknowledge the contribution of this paper to the knowledge distillation, analyzing the effect of temperature in LS-trained teachers. This would be a good supplement to the existing research (Müller et al., 2019) and (Shen et al., 2021b). My main concerns are whether such conclusions are also applicable to normal teachers, as it is known that temperature also affects normal distillation. Considering both strengths and weaknesses, I give a negative score. I would increase my score according to the response from the authors.\n\n-----------------------\n\nFinal Recommendation\n\nThank the authors for their patience and effort during the rebuttal time. Considering both the strengths and weaknesses, I decided to slightly increase my score to 6, but still hold the concern that the impact of temperature on the relation of LS and KD may not be interesting in the area of KD.\n\nThe strengths and weaknesses/limitations are clear. As for the strengths, I agree that the research topic, the relationship between LS and KD, is important. This work would be a good supplement to existing works. The authors have made great efforts on experiments, both in the main paper and the rebuttal. I acknowledged these strengths.\n\nMy remaining concern is the interest and contribution of the topic, especially analyzing the effect of temperature on the relation between LS and KD. I personally think that the topic is somehow narrow, but I don't want to neglect the authors' effort and thus give a negative score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper investigated whether a label smoothing-trained teacher is compatible with knowledge distillation. It started by reproducing the results of two previous studies, i.e., (Muller et al. 2019) claimed that LS erases information in the logits and therefore is harmful to KD, while (Shen et al. 2021b) argued that LS-trained teacher increases the distance between semantically similar classes and therefore is compatible with KD. After that, the authors tried to draw a unified conclusion from the above contradictory claims by introducing experiments with different temperatures: with an LS-trained teacher, a higher temperature introduces systematic diffusion in the representations of the semantically similar classes, which hurts KD. It is the different temperatures used in (Muller et al. 2019) and (Shen et al. 2021b) that resulted in different conclusions. Experiment results supported the main claims.",
            "main_review": "Strength:\n1. The paper investigated a research gap of KD with LS-trained teachers in previous studies.\n2. Using temperature properly fills the gap and suggestions were given for training student network with KD using an LS-trained teacher.\n3. The paper is well organized.\n\nWeakness (or major concerns):\n1. Although the idea of using system diffusion to explain the performance of KD is interesting, I think there are some flaws when quantitatively defining the system diffusion or diffusion index, in the following aspects:\n  -The semantic similar or dissimilar classes are not quantitatively defined. As shown in the experiment results, the diffusion index grows from negative to positive numbers as the chosen classes go from semantic similar to dissimilar classes. Without a quantitative formulation, I feel that the phenomenon discovered with the diffusion index is less convincing. \n\n  -Using the distance between centroids to characterize the diffusion may be rough and lose information. For example, the feature representations of the training samples may diffuse with a larger T but there is a chance that their centroid may remain unchanged. Is there any better way to do this?\n \n2. Although I agree that using system diffusion to explain the performance of KD makes sense, maybe such an explanation is overkilled? Maybe a simple alternative is that, LS softens the teacher's labels, so there is no need to use a higher T to further soften it during KD (because a too big T will hurt the performance); while without LS, the labels need to be softened more so a higher T is needed during KD? Could you comment on this?\n\n3. The experiment results only show the top-1/5 accuracy with different configurations. Since the paper put a lot of effort into explaining the effect of T on semantic similar or dissimilar classes, it would be interesting to see if different Ts have any influence on the accuracies of these classes specifically.",
            "summary_of_the_review": "Overall, on one hand, I think this is an interesting paper that tried to fill the research gap of KD with LS-trained teachers left in previous studies. The conclusion drawn after the experiments makes sense, though not perfect (please see my major concerns). On the other hand, since the paper focuses on a sub-field of KD, i.e, KD with LS-trained teachers, the scope is somehow limited. Currently, I lean slightly towards acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper investigates the contradiction of previous findings: (1) LS can hurt KD and (2) the benefit of using LS outweighs its disadvantage for KD. The authors provide ample experiments to verify their claim of \"LS is useful for KD with a moderate (low) value of temperature\". They interpret the negative or positive influence from LS with respect to systematic diffusion. The conclusion is using LS with a low T (e.g., 1) for KD. To support this argument, the authors analyze the diffusion phenomenon qualitatively (maybe t-SNE) and quantitatively (diffusion index).",
            "main_review": "I enjoyed reading this paper, and the strengths and weaknesses I felt while reading is listed below:\n\n**Strengths**\n- This is the first work to investigate the previous contradicting observations, which is very helpful for practitioners. \n- The paper is well organized (i.e., Table 1 and Figure 2) and easy to follow and understand. \n-  Ample experiments are provided to support their main claim, e.g., quantitative and qualitative analysis. \n\n**Weaknesses**\n- The coverage of KD is limited: The work only focuses on a specific supervised learning task, image classification, for KD and LS. Actually,  KD can be applied to a wide range of learning tasks. I am wondering if the final claim can be generalized to another task that requires label supervision for distillation. \n- The final conclusion is quite incremental: According to the claim, the temperature value is the key factor to affect the systematic diffusion in representation space. I am curious if the temperature is the only factor we need to consider for better use of KL with LS? If so, what value should be used for different learning scenarios? Does the T=1 guarantee the best performance for all scenarios? For the best T value,  I think It is necessary to consider what kind of information LS deletes and what value T does not increase diffusion too much together. This is a very tricky problem, but very helpful to the practitioner handling diverse types of data.",
            "summary_of_the_review": "This paper has both strengths and weaknesses, as mentioned earlier. Can the authors put some discussion on their weaknesses during rebuttal? I am willing to increase my score if possible.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}