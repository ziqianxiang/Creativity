{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper addresses the issue of  overestimation bias in TD learning by deriving a Generalized Pessimism Learning (GPL) framework. Authors have used a dual TD learning procedure to minimise the estimated bias in critic’s performance and adaptively learns a penalty to recover an unbiased performance objective. The idea has been integrated into SAC and DrQ on various tasks. ",
            "main_review": "The problem is well-formulated and experiments are validating the claims authors made in the paper. The overall idea of the proposed method makes sense to me. \nI have the following concerns and questions regarding this paper. I can adjust my score accordingly after the authors reply to these questions:\n\n(1) Algorithm 1, line 19: does the $\\frac{\\beta}{N^2-N}$ indicate removing the N $\\phi_i$ that are similar when iterating over $i$ and $j$? What was the intuition behind averaging these distance values? Can it cause bias itself? I am curious to know if authors tried a weighted average on this.\n\n(2) Algorithm 1, line 17: What is the update rule of $\\alpha$? Based on my understanding this is just a one-shot optimisation. I think some clarification on the entropy bonus is required.\n\n(3) I do not understand how Eq. 8 works. If  $Z^\\pi$ is a “predicted return distribution”, why did authors propose to use Wasserstein distance? Could KL distance be more suitable?\n\n(4) Is there any intuition on selecting $N$? How expensive would that be if the number of action-value functions increases significantly. \n",
            "summary_of_the_review": "Overall, the paper is well-written and easy to understand. The experiments are extensive and well-designed.  However, the contribution is not significant. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors proposed to utilize the epistemic uncertainty provided by the models to address the overestimation issue of temporal difference learning. The authors conducted solid experiments to validate the effectiveness of the idea and demonstrate the advantage of the proposed method over other baselines. ",
            "main_review": "Strengths\n- The idea is easy to follow, and I like the idea of using the uncertainty provided by the models, which is much easier to obtain compared with data uncertainty.\n- The experiments are conducted thoroughly. I have seen many ablation study in the appendix and extension of the current method to different settings. \n- Through the experiment, I can see clear advantage of the current method compared with previous baselines\n\nWeakness\n- One disadvantage of the current method is that it requires many models to calculate the pessimistic uncertainty. I can see the clear advantage of using more critics, but it will also increase the computational cost of the current method, do the authors have some intuition of how to reduce the additional computational costs?\n\n- If the pessimistic uncertainty was quantified with Wasserstein-2 metric (or equivalently to use quadratic function to calculate the pair error), I think this uncertainty is close to calculate the variance of the ensemble models. and I don't see the difference between these two method? If we do so , the idea is quite intuitive and principled (a lower bound by mean - \\beta \\times \\sqrt(variance)). Can the authors clarify the difference between these two method and show why using wasserstein 1 metric would help in this case? Either theoretically or empirically. (If I miss some of the comparison baselines in the appendix, please let me know)\n\n----\nFollowings are just some questions:\n\n\n\n- Why the authors only evaluate on the three mujoco tasks? what about the others such as Halfcheetah? I am just curious but the authors have done many experiments so this is not a critical point. \n\n- In the appendix, the authors conducted experiments when the critic are distributions. In that case, how do you calculate the wasserstein distance? Can the authors clarify that?",
            "summary_of_the_review": "overall the authors propose a simple yet effective method to mitigate the overestimation issue of temporal difference learning. Although the proposed method might be a little bit computational inefficient, I think it is valuable for the community to think about how to utilize the epistemic uncertainty more efficiently in this case. \n\n-----\nThe authors have answered all my question. I have raised my score to 8. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces GPL, an off-policy RL algorithm that aims to address the overestimation bias in Q learning. The authors propose to estimate the bias in Q learning by uncertainty quantification as a penalization to estimated Q functions. The authors also propose to learn the appropriate penalty through minimizing the bias. The authors further propose GPL, which penalizes the estimated Q functions based on uncertainty quantification. In addition, the authors propose to adjust the penalization parameter to conduct exploration. The authors conducted experiments on GPL incorporated with SAC and DrQ. Experimentations demonstrate better performance than the SAC and DrQ baselines.",
            "main_review": "Strength:\n\nThe proposed algorithm is flexible and can be adapted to various existing off-policy RL algorithms. The formulation of uncertainty quantification also allows for generalization with the distributional critic. The idea of pessimism is typically utilized for offline pessimistic learning. Adopting such an idea to address bias in Q-function estimation is novel to me.\n\n\n\nQuestions:\n\nHow is the bias in target estimation related to epistemic uncertainty? For instance, if the model class does not capture the true model and the model is well fitted, it may happen that the epistemic uncertainty is low but the bias is large. In addition, it is the aleatoric uncertainty that is shown to be related to the bias in target in previous work (e.g., [1]), given that one is fitting the Q-functions by MSE. The fact is that in target there is an expectation over the next state-action pair, whereas, in reality, the expectation in target is a point estimate with a single sample. Hence, the variability of the next state and action pairs (which corresponds to the aleatoric uncertainty of the transition) is neglected and induces a bias if one takes the squares directly on the one-point estimates. \n\nOther suggestions:\n\nThe discussions on relations between the proposed Wasserstein difference, uncertainty measurement with the ensemble, and the realization in double Q learning are very relevant. The authors may consider adding a formal conclusion in the main content of the paper regarding the connections.\n\n[1] Chen and Jiang, Information-Theoretic Considerations in Batch Reinforcement Learning. (2019)",
            "summary_of_the_review": "The authors propose to handle overestimation bias in Q learning by directly estimating the bonus, which is an effective and promising approach. The algorithm proposed is flexible and can be adapted to various off-policy baselines and the experimentations are positive. However, the discussion on the validity of the approach requires additional work. Specifically, the work would be greatly strengthened if the authors could elaborate on estimating the bias in Q-functions with epistemic uncertainty.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper is motivated by the over/under-estimation bias in TD-learning for deep RL. The paper proposes to dynamically adjust the pessimism level of the learning rule, by adapting a multiplier parameter using dual gradient update. The paper provides some semi-theoretic motivations and empirical performance gains.",
            "main_review": "========== Novelty =============\n\nThe idea of adjusting the optimistic / pessimistic level of TD target does not seem to be very new. For example, maxmin Q-learning [Lan et al, 2020] introduces a family of targets indexed by the num of ensembles which controls for the optimism / pessimism of the targets. However, a meaningful question here should be how to dynamically adapt the level of pessimism that achieves better performance (or is theoretically sound). The current paper also investigates this question with a different formulation of the TD targets.\n\n========== Literature reviews =============\n\nThe authors might have missed the ref [1] which addresses over-estimation by using truncated quantile distribution. Given that the current paper also uses ideas from distributional RL to define pessimism values, this paper should be of direct interest.\n\n[1] Kuznetsov et al, Controlling overestimation bias with truncated mixture of continuous distributional quantile critics, 2020\n\n========== Detailed questions =============\n\n1. I am rather confused by the definition of the regularizer function p. In Eqn 7, what's the precise math definition of this quantity? Eqn (7) is confusing because in the first line the paper uses an approximation sign. I suggest the author to present the definition precisely using definition sign. Note that the algorithm box specifies how to compute p in practice, but this should conceptually serve as an approximation to the real definition of p.\n\nThe paper reads (bottom of page 4) \"as long as p_β is unbiased for on-policy action\" -- based on the framing, it is not clear if p_beta is an estimate or the exact quantity to be estimated. This should be made more clear.\n\n2. In Eqn (8), the paper specifies another way to compute p. What's the connection between Eqn 7 and Eqn 8? I think it is better to make clear of which is the precise definition and which is the approximation using function approximation. It seems that since we adopt a delta function representation of return distribution Z, Eqn 8 should be understood as a practical approximation to the true quantity defined in Eqn 7.\n\n3. At bottom of page 5, the paper reads \"Moreover, for some fixed β, increasing the number of critics decreases the estimation variance but\nleaves the expected magnitude of the uncertainty regularizer unchanged\". It is not clear to me why this is the case and the author does not offer further explanations. I'd suggest there to be at least a few sentences of the explanations of the intuitions, and porting formal results from the appendix to the main paper to make the logic more clear.\n\n4. In Eqn 9, if the expected bias is positive, does it mean that the optimal value of beta is positive infinity? When the expected bias is negative, the optimal value of beta is zero, which is more well-behaving. Can the author clarify on this?\n\n5. The bias B is defined wrt the true Q^pi value, which is not accessible (Eqn 5). In Eqn 10, the paper proposes to replace the Q^pi by an estimate produced by the Q network. I wonder if this is going to introduce further bias into the approximation. What if the Q network already over/under-estimates Q^pi, then the bias estimate would be problematic. There does not seem to be a proper mechanism to address this issue. I can understand the motivation behind adjusting beta such that the overall bias of the TD target is small, but then the estimation of the bias should be itself \"unbiased\" -- if the bias estimate is confounded by the current estimation property of the Q network, it is not crystal clear to me why adjusting beta should help. \n\n6. Since the paper uses delta function as approximation to the return distribution, will using more complicated distribution be helpful?\n",
            "summary_of_the_review": "Overall, I think the paper is lacking in a few aspects:\n\n1. The presentation of certain key concepts & quantity is not clear in the paper. Critically, the regularizer p_beta should be more clearly presented as either an exact quantity (like Q-function) or an estimator. If it is an estimator, what are the assumptions & approximations the authors make.\n\n2. The paper specifies a rather clear algo procedure on how to compute TD targets and embed the subroutine into a full algo. But it is not clear why such methods should work in practice. For example, it is not clear why it is ok to use the trained Q network as approximation to Q^pi to estimate the \"estimation bias\" (Eqn 10), which is defined using the trained Q network itself (Eqn 5). Such approximations make it less clear why the algo should work. \n\n3. The empirical gains look significant, yet the tech contribution is not clear to me. Elucidating such ideas on simple domains will greatly improve the intuitions.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concerns.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}