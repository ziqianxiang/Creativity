{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a document clustering method.  The similarity of documents for clustering is defined similarly as the topic coherence score often used for evaluating topic models. TF-IDF is used to extract keywords from a document or a cluster of documents for computing the similarity (coherence) score.  The proposed method is evaluated on three proprietary data sets without any objective criterion for evaluating the quality of topics.",
            "main_review": "**Strengths:**\n\n1. The paper proposes a simple method that can be easily implemented.\n2. The paper is easy to follow.\n\n**Weaknesses:**\n\n1. The paper assumes that each document belongs to only one topic.  This does not seem to be realistic.  For example a document may be related to both business and politics and the proposed model does not seem to be able to handle this situation.  This situation can happen more often when more precise topics are needed to be discovered.  The proposed method can be justified better if more examples can be used to explain why having a single topic is reasonable.\n2. The paper does not provide any objective criterion for evaluating and comparing the quality of topics. On the other hand, the discovered topics in Figures 2, 4 and 5 are not very convincing. For example, the words in the word clouds in Figure 2 do not seem to be very representative for the corresponding topic of technology and entertainment / entertainment. The second word cloud in Figure 2 appears to represent some gas companies.  It is not clear whether many of the documents are really related to such a topic.\n3. The proposed method appears to be able to determine the number of clusters.  However, it still needs to specify the maximum number of clusters $\\eta$.  Besides, it looks quite surprising that the number of extracted clusters can be much smaller than $\\eta$.  It appears that many clusters are merged together in one single step, seemingly indicating some flaws in the design of the algorithm.\n4. The proposed method use an asymmetric score to measure the similarity between documents/clusters.  It would be more natural if a symmetric score can be used.\n5. The experiments use three data sets that looks proprietary. The experiments can be more convincing if common benchmark data sets are used. Besides, more information such average length of documents should be given for the three data sets. \n6. The proposed method does not use any sophisticated techniques and cannot produce convincing results at the same time. \n7. The paper should discuss the relationship between the proposed method and some document clustering methods proposed before.",
            "summary_of_the_review": "The proposed method has a questionable assumption (that each document belongs to only one topic) and the experimental results are unconvincing. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper tackles a common problem in topic modeling which is to learn a coherent set of topics from a text corpus. Unlike traditional topic models such as LDA which assumes each document is a distribution over multiple topics, the method proposed in this paper assumes that each document describes a single topic. The paper defines the coherence score for an ordered pair of documents (i.e., coherence score of a document d1 given another document d2), and uses this definition to derive a procedure to cluster documents given a couple of parameters to control the stopping criteria.\n\nThe proposed method is then compared with two traditional topic modeling approaches (LDA and NMF) and two embedding-based topic models (BERTopic and Top2Vec) on three datasets. The results show that the proposed method (CBC) was able to derive reasonable numbers of topics compared with the true number of topics. Quality of the extracted topics is discussed using word clouds, without any quantitative comparison.\n\n",
            "main_review": "Strengths\n- The paper tackles important problems in topic modeling, which is to learn coherent set of topics and also derive the number of topics at the same time.\n\nWeaknesses:\n- One of the key contributions that the paper claims is that the proposed method doesn't require users/researchers to specify the number of topics before hand. The paper however misses an entire body of work on non-parametric topic model (e.g., extensions of LDA using Dirichlet Processes such as nCRP or HDP), which also try to free the users from having to specific the number of topics before hand. \n- Even though the propose method here (and prior work like HDP) doesn't explicitly specify the number of topics, other parameters control it. However, there was no discussion on how sensitive the number of topics learned using the proposed method CBC is w.r.t. the different parameters (e.g., \\eta and \\nu)\n- It was quite unclear from the description of Section 2.3, how exactly the chain was derived. In addition, the way the approach was described seems to be very similar with the simple hierarchical clustering approach (once we define some distance function between a pair of documents), which should be discussed and compared against empirically\n- The experimental results are not convincing with no quantitative comparisons, except for the results on the number of topics derived",
            "summary_of_the_review": "The paper tackles a common problem in topic model to extract coherent topics from a large corpus while also deriving the number of topics. It is however a bit unclear from the description how the proposed method works exactly. In addition, the paper lacks discussions about highly relevant prior work such as non-parametric topic models. The experimental results are unconvincing, which only contain mainly qualitative discussions using word cloud, except for the results on the number of topic derived.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a method for extracting clusters of documents (topics, in their terminology) that optimize a measure of coherence. The method is compared to other topic models on three datasets that are not detailed. The authors show that it achieves better results in term of the same criterion (coherence) they use for extracting the topics. ",
            "main_review": "As Goodhart's law says: \"When a measure becomes a target, it ceases to be a good measure\". Lots of previous works used the coherence measures as a gold standard, and those measures became the only way to evaluate the results and select the topic models. A recent paper accepted at NeurIPS 2021 (https://arxiv.org/abs/2107.02173) clearly shows that we cannot rely on coherence measures only. This is the reason why my very first comment is that, as far as I can judge, the very grounding of this work is flawed from the beginning.\n\nMy second comment is that the paper relies on several assumptions that don't seem realistic, and the authors don't take the time to *support* those assumptions. One assumption is that good topics have to maximize coherence. Another assumption is that the authors get rid off the document as a mixture over topics, casting the problem as a simple document clustering issue (ie one document belongs to exactly one topic). It can hold in some contexts (e.g., small documents, such as tweets) but it's usually not the case and lots of results in the past in multiple papers have proven that the mixture assumption is fruitful.\n\nThirdly, the authors seem to ignore a large part of the recent literature. Several models based on neural approaches has been proposed, in particular able to take word embedding into account. The paper should at least mention a couple of them (e.g., https://arxiv.org/abs/1703.01488).\n\nFinally, the technical contribution is a variant of the classic hierarchical agglomerative clustering of the 60s, which uses a similarity measures based on the coherence measure (no word embedding here). In the experimental part the \"new\" approache is compared to the classic algorithms (LDA and NMF, based on a different assumption) and two recent algorithms that I didn't know about and that hasn't been published anywhere. It let me view the dark side of preprint repositories such as Arxiv where we can find works that have not been reviewed by anyone. Even though peer review is not a guarantee, I believe it's still a way to filter out irrelevant work. Otherwise we're still building on sand. By the way, in the experiments coherence is used as a criterion even though it has been optimized in the method proposed by the authors... no wonder it's better than baselines.\n\nAdditional comments:\n\n- Several passages are (really) difficult to follow, with awkward sentences. One example among others: \"We show that by subsidizing simple keyword extraction methods and clustering documents based on their respective coherence, both, probabilistic as weel as semantic embedding based...\"\n\n- In my opinion, citing some code on zenodo doesn't give good arguments when motivating the choices done in the paper. I expect a more convincing reference.\n",
            "summary_of_the_review": "This paper is really far from being ready for a presentation at ICLR:\n- insufficient motivation for supporting the proposed approach\n- technical contribution being too naive\n- improper experiments",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}