{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "Built upon some previous work on CVaR-based RL,  this paper proposes a new policy optimization algorithm CPPO .   Theorems on performance degradation under observation disturbance and transition disturbance are provided.  Experimental results on MuJoCo are presented to demonstrate the advantage of the proposed method. ",
            "main_review": "On the positive side, the theorem statements in Section 4 are clean and interesting. It seems to me that this paper does make good contributions in the discounted MDP cases. However, I also do have some concerns regarding the significance of the results in this paper for continuous control applications. It seems that the authors are using MoJoCo to demonstrate CPPO and hence continuous control is their main target application. My main background is also from control, and there are a few things which are confusing to me.\n\n1. The theorems in this paper seem to work on discounted cases. However, for many control tasks, it makes more sense to set the discount factor as 1 (e.g. Walker, etc). Do the theorems in this paper make sense for such control applications?  \n2. I am looking at the proof for Theorem 4 (the model mismatch result). For continuous control problems, everything is in continuous space, does the proof still work? Simply just replacing sums with integrals? I am not sure whether that is the case or not.\n3. The comparison with the exponential utility approach is also confusing. At least for control applications, it is well-known that exponential utility-based approach is equivalent to robust control in many situations and hence can induce robustness for control applications. For example, some classic and more recent references are given below.\n\n[Glover1988] Glover, K. and Doyle, J. C. State-space formulae for all stabilizing controllers that satisfy an H∞-norm bound and relations to relations to risk sensitivity. Systems & Control Letters, 1988.\n\n[Whittle1990] Whittle, P.  Risk-sensitive Optimal Control, 1990.\n\n[Zhang2020] Zhang, K., Hu, B. and Basar, T. Policy Optimization for H2 Linear Control with H-Infinity Robustness Guarantee: Implicit Regularization and Global Convergence. L4DC, 2020.\n\nIt seems that this paper ignored the above connections between exponential utility and robust control. Some comments are needed to justify the proposed CPPO in the robust control context.\n",
            "summary_of_the_review": "I do think that the theory in this paper is interesting and relevant for discounted MDP problems. However, it seems that continuous control is the main target application for the proposed method, and there may be some gap between control practice and the results in this paper. Some clarifications are needed. Right now my score is 5. I am willing to increase my score if the authors can address my comments.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper focus on the conditional value-at-risk of the return as a measure of the `safety' of a policy. The CVaR-PPO algorithm is developed to learn a policy which maximizes return while satisfying a certain value of the CVaR. Theorems are developed to relate the safety of policies under adversarial state and observation perturbations and various MDP quantities. Finally, empirical results show that the proposed CVaR-PPO leads to higher reward and more robust policies.",
            "main_review": "This paper argues for the relevance of the conditional value-at-risk of the return as an important quantity for robust reinforcement learning. \nAn algorithm is presented for learning a policy which is reward-maximizing, subject to being $\\alpha, \\beta$-CVaR. Put simply, this means that when looking at the distribution of the return, the average of the worst-$\\alpha$-fraction of the return is greater than $\\beta$. The authors do a good job of arguing that this measure of risk is often easier to specify for the practitioner as a natural metric for risk. However, it's unclear to me how novel the work is, as the authors seem to miss some related work and downplay the similarity to other work. This needs to be addressed by the authors. Meanwhile, the experiments and theory section are nice, but could be tied back more to the overall goal of the importance of learning a CVaR policy.\n\nComments and questions for the authors:\n\n1. At the bottom of page 4, there is a lot of language suggesting that it's possible to find a solution to equation 9. However, it seems that there is no reason to expect that a solution always exists--if $\\beta$ is greater than the maximum total return it seems like there is no way to satisfy equation 9. Is this correct?\n2. Theorem 2 is interesting, but hard to understand without some more context. What are the implications of this result?\n3. Can the authors comment on the similarity of their algorithm to 'Risk-Constrained Reinforcement Learning with Percentile Risk Criteria', which is (chow 2017) in the paper? It appears that the algorithm in equations 7-9 in Chow and 12-14 in this paper are very similar, perhaps identical. The authors say that they are different since the previous work examines the cost instead of reward, but as I understand it the 'cost' is simply reward with the opposite sign (hence $\\beta$ and $\\nu$ having opposite signs in the previous work). Flipping a sign does not make a substantially different algorithm, but it's possible I've misunderstood the connection between the two papers so would appreciate a comment.\n4. The theorems in section 4 are nice, but the connection to the actual approach of CVaR-optimized RL is not clear. Unlike the precise results in section 4.2, section 4.3 is vague. Is there anything you can actually prove about how a ($\\alpha,\\beta$)-CVaR policy is robust to observation and transition perturbations? As it is, there is a suggestion that they would be, but no proof. Even a result in some restricted setting would be useful. As it is, the results in 4.2 do not seem very relevant to the rest of the paper without a stronger connection to the CVaR policies\n5. Section 5 seems to be missing some key information and results. Firstly, there doesn't seem to be any mention of the values of ($\\alpha, \\beta$) used in the experiments. Secondly, there is no evaluation that the learned policies actually are $(\\alpha, \\beta)$-CVaR robust. I'd expect at least a table showing that the policies solve the problem which the paper deals with, i.e. that the algorithm works. Obviously there is a bit of a possible loophole if the ($\\alpha, \\beta$) values were chosen to be vacuously feasible. Thirdly, could the authors explain how it appears that CPPO outperforms regular PPO on the mean reward metric? I would have expected the training curves to look something like PPO achieving higher mean reward with a large variance, while CPPO achieves either the same or lower mean reward with a smaller variance (possibly with an asymmetric spread that is smaller on the lower side). As the authors point out (with oracle optimization etc) the CVaR-robust model obviously must have equal or worse mean return. Therefore there is something else going on with the Lagrangian procedure influencing the learning dynamics, or possibly some extra tricks used with the CVaR but not the PPO agent; it's hard to know without the code or more training details. \n\nSummarizing:\n\nStrengths:\n+ Convincing arguments about the importance of CVaR for RL\n+ Nice theorems in section 4.\n\nWeaknesses:\n+ Potential lack of novelty due to similarity to previous work\n+ Lack of connection in section 4,5 to the rest of the work\n+ Missing some key information and needed evaluations in section 5\n+ Some unclear language around existence of CVaR-feasible policies\n\n\n\nMinor Points\nIn 5.2 CPPO is said to be labelled in pink but it is orange in the graph\n\nFigure 3 would be easier to understand if they were labelled with respect to the ground-truth parameter, i.e. +1kg, -1kg from training mass etc.\n\nAfter equation 10, 'deviation' should be 'derivation'\n\n\n--------------------------------------------------------\nUpdate 2021-11-21\nBased on the authors' feedback I have raised my score to 5. The authors argue that the main contribution is the theoretical analysis. The theory is nice and a decent contribution. If the rest of the paper were up to the same standard as section 4, I would strongly recommend accepting. However, currently the other sections are not very well-motivated with connections to the theory (although the new thm 5 is an improvement in this regard). It seems like the novelty of the proposed algorithm is quite limited in view of previous work. Overall I think the paper is borderline, but a substantial revision addressing similarity to previous work and with a more comprehensive experiments section could be a very strong paper.",
            "summary_of_the_review": "The paper is a good advocate for the importance of CVaR in reinforcement learning. However the main algorithm has a striking similarity to previous work that is not really addressed in the paper. The experiments lack important details and don't show that the algorithm fulfills its stated aims. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the authors propose an RL method to improve the average reward while guaranteeing (some kind of) worst-case return that is evaluated by CVaR. They also provide a theoretical analysis of how optimizing policies using CVaR can improve the robustness of the policies. \n",
            "main_review": "strengths:  \n+ The authors make both theoretical analyses (Sections 3 and 4) and empirical experiments (Section 5) to demonstrate the advantage of utilizing CVaR for safe RL. In particular, the analysis (which introduced the concept of VFR) and its results in Section 4 are new and meaningful in demonstrating the usefulness of CVaR.\n\nweaknesses:\n\n- The novelty of the methodological framework and the experiments is somewhat insufficient. \n  - The methodological framework: \n    - There is little improvement from that proposed in Chow et al.'s paper [1]: \n      - In Section 2.2, the authors point out the problem with Chow et al.'s method [1] as \"However, these works ignore the reward in MDP and thus cannot be directly used in RL settings.\" But, this is misleading because, if we regard the reward and return multiplied by a negative coefficient (e.g., -1) as the cost and loss, respectively, then we can use reward in Chow et al's method. (As another approach, we can use reward in their method by calculating CVaR for the lower tail instead of the upper tail of the return distribution.) (Actually, in the paper under review, the authors utilize Chow et al's method by doing so.)\n      - The discussion in Section 3 is essentially identical to the discussion in Chow et al.'s paper [1]. For example, it has already been proven in [1] (more precisely, in [2]) that Eq. 9 can be converted into Eq. 10. Also, the Lagrangian relaxation of the objective and derivation of gradients to optimize it (Eqs. 11~14) has already been done in [1].  \n  - Experiments (Section 5). \n    - In  Hiraoka et al. [3], the PPO variant that optimizes Eq. 11 has already been proposed and evaluated in MuJoCo environments (e.g., Hopper and Halfcheetah) where the value of robot body masses are randomized (CPPO is basically identical to Hiraoka et al's PPO variant that uses a single option).  So, it is not clear if the findings provided in Section 5 make significant progress from the one provided in [3]. \n\n[1] Yinlam Chow and Mohammad Ghavamzadeh. Algorithms for cvar optimization in MDPs. NeurIPS, 2014  \n[2] R. Rockafellar and S. Uryasev. Optimization of conditional value-at-risk. Journal of Risk, 26:1443–1471,\n2002  \n[3] Takuya Hiraoka, Takahisa Imagawa, Tatsuya Mori, Takashi Onishi, Yoshimasa Tsuruoka. Learning Robust Options by Conditional Value at Risk Optimization. NeurIPS, 2019. \n\n\nMinor comments:  \n- It is better to compare with more studies that introduce CVaR into RL. \nIndeed, Chow et al.'s studies are seminal on CVaR x RL, but there are many other studies (e.g., [4--7]). \n\n[4] Morimura, T., Sugiyama, M., Kashima, H., Hachiya, H., and Tanaka, T. Nonparametric return\ndistribution approximation for reinforcement learning. ICML, 2010  \n[5] Rajeswaran, A., Ghotra, S., Levine, S., and Ravindran, B. EPOpt: Learning Robust Neural\nNetwork Policies Using Model Ensembles. ICLR, 2017  \n[6] Elita A. Lobo, Mohammad Ghavamzadeh, Marek Petrik, Soft-Robust Algorithms for Batch Reinforcement Learning, https://arxiv.org/abs/2011.14495.  \n[7] Will Dabney, Georg Ostrovski, David Silver, Rémi Munos, Implicit Quantile Networks for Distributional Reinforcement Learning, ICML 2018.  \n\n- Theoretical Analysis\n  - Optimizing Eq. 9 is not exactly the same as optimizing Eq. 15. \nSo, if possible, it would be more convincing if you add theoretical (or experimental) evidence to support the following statement in Section 4.3: \"Moreover, our CVaR-based optimization objective (9) focuses ....  observation disturbance as well as transition disturbance.\"  \n  - Does the RL methods that optimize Eq. 2 also tend to reduce $ \\hat{V}_{\\mathcal{M}, \\pi} $? The methods maximize the worst-case return (i.e., produce conservative policy), and thus $\\max V$ could be close to $\\min V$. \n\n- Typos?:  \nPolicy Proximal Optimization (PPO) -> Proximal Policy Optimization  \nand use CVaR to evaluate. -> and use CVaR to evaluate it. ?  \nAlgorithms for cvar optimization in mdps. -> Algorithms for CVaR optimization in MDPs.\n",
            "summary_of_the_review": "I am leaning toward recommending rejection. The theoretical analysis made in Section 4 is novel and meaningful, but I could not find the significant novelty in the other part of the paper. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors utilized conditional value at risk to propose a robust version of proximal policy optimization method.  The authors demonstrate the effectiveness and robustness of the proposed method empirically through a detailed experimental comparison.",
            "main_review": "Strengths\n- The paper is well-written, especially the part of introducing the related method such as cvar and the new proposed method, which are both very easy to follow.\n- In Sec.4, the authors explains clearly why the proposed method might be robust to adversarial disturbance based on the presented Theorem 3 and Theorem 4.\n- The authors conduct detailed experiments to validate the proposed method, under different attacking settings. \n\nWeakness\n- For the compared baselines, I feel the authors should add two additional baselines which are quite related to your introduction part of Section 2, the first one is mean + variance penalty method, which I think is a naive baseline. Another baseline I think is to select top performance trajectories according to a relatively portion (which I think is also quite related). I do not expect these two methods to work in your experiments, but I think it is really good to add these two. \n\n- When the authors introduce Theorem 1, I feel it is better to define the cost function, rather than reward function (thus you need to deal with $-D(\\pi)$), and by defining cost function, it is also convenient to connect the current method with previous methods, and I think it is worth a clarification and comparison with a line of methods proposed by Yinlam Chow.\n\n\n\n\n ",
            "summary_of_the_review": "Overall I recommend an acceptance of the paper, while the authors may add some additional baselines, with some discussion to distinguish the current work with previous ones to address the concerns. \n\n\n-------------------\n\nThank the authors for the detailed response. I have read the authors response and also other reviewers's comments.  I feel the authors should modify the paper according to the suggestions and submit to another conference.\n\n- Would the authors clarify the exact difference of using reward to derive the cvar approach rather than cost? Still I don't get why there are difference, the only thing you need is to change the sign of the cost function.\n\n- The theorem in section 4, I feel they are less related to the cvar policy you propose. The authors argue that the core contribution of this work is to propose the connection between observation disturbance and transition disturbance. if so, I feel the authors should clarify more, otherwise to me what your core contribution is proposing cvar policy, rather than the theorems you just talk about (The structure of the paper may need to reorganize if you want to argue the core contribution).\n\n- Still, the authors may add some more related baselines to demonstrate the effectiveness of the proposed method. \n\n\n\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}