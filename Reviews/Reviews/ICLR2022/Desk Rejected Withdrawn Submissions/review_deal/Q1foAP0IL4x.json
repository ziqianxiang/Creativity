{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes to use data augmentations for adversarially perturbed images in the output space of the network to achieve improved robustness. This has been applied to both supervised and self-supervised settings and achieves improved robust accuracy. ",
            "main_review": "Strengths:\n\n- The proposed approach increases the effectiveness of the generated perturbation by augmenting it in the decision space \n- The method can be applied in many cases such as classification and self-supervised pretraining/ finetuning. \n- The increase in computational cost is small\n- Results of self-supervised training (N-RoCL + AT and N-RoCL+N-AT) appear to have good improvements\n\nWeaknesses/ Suggestions/ Questions for the rebuttal:\n\n- Could the authors share the attacks that are used for robustness evaluation?\n- Autoattack [1] is widely used for obtaining an accurate estimate of robustness. It is important to check the improvement of the proposed method against this attack. Could the authors report accuracy against this attack?\n- The improvement in results is quite less, and in some cases is accompanied by a drop in clean accuracy.\n- It is not clear why the value of the parameter $\\alpha_s$ in Eq.(3) should be reduced over the training epochs. Since adversarial training seems to increase the sensitivity of the attacked image, increasing $\\alpha_s$ could offset this. \n- The clarity of some parts of the paper could be improved - for example, the description of Figure 3 of the appendix in the Discussion section is not clear. \n- It is confusing to include relative increase of results, absolute increase is better and more common. For example, in cases such as $\\epsilon=32/255$, the base robustness itself is very low and relative increase is misleading. \n- It would be helpful to include important figures and tables that are discussed in detail in the main paper. \n\n[1] Croce et al., Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks, ICML 2020",
            "summary_of_the_review": "The proposed method is empirical and does not show sufficient gains in terms of results. Moreover, the results are shown only on one dataset. I believe the clarity, writing, and organization (between main paper/ Appendix) of the paper could be improved. I therefore recommend to reject the paper. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed noise adversarial training (N-AT) that add gaussian noise to output logit which can be applied to supervised adversarial training and self-supervised adversarial training.",
            "main_review": "**Strong points**\n\n1. The proposed method can generally apply to supervised adversarial training and self-supervised adversarial training.\n\n2. N- can improve based line against the seen attack and also against unseen attack.\n\n3. The proposed method is simple and effective.\n\n**Weak points**\n\n- It was difficult to understand the results because the corresponding tables are in the appendix while explaining the results in the main paper.\n- There is an insufficient explanation why such random Gaussian noise improves the robustness. Does random Gaussian noise in the feature map or in the input also improve the robustness? Why such noise helps even in the unseen attacks?\n- For the hyper-parameter selection, I suggest the authors report the clean accuracy and robust accuracy on the test set under different values of $\\alpha$.\n- For the model, I also suggest the authors conduct experiments on the large size of models such as WideResNet34-10.\n- For the experiments, I think it is better to also include other attacks' results such as AutoAttack. Can N-AT achieve better robust performance compared to standard AT against AutoAttack?",
            "summary_of_the_review": "Overall, I recommend weak reject for this paper. I think it is interesting and simple to apply both in supervised and self-supervised learning, but the authors should carefully address the above concerns during the rebuttal.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes noisy adversarial training which seeks to further enhance the adversarial training by adding random noise to the original adversarial data. Utilizing multiple additional adversarial variants around the single adversarial data, the approach increases the model's robustness. The authors also analyze and show its applicability to both supervised and self-supervised learning.",
            "main_review": "Pros:\n\n1. The empirical results of the proposed method do show some positive effects on adversarial training.\n2. The experiment to analyze the percentage of false adversarial input classified is interesting and worthy of in-depth analysis.\n3. This paper provides a comprehensive discussion about the empirical improvement in robustness.\n\nCons:\n\n1. The writing of this paper is not good, so it is not easy for readers to understand the main contribution and novelty of this paper. Some claims have not received clear and sufficient support, and lack some definitions. Specifically, there is no definition of \"inefficient\", no case-study experiments or other literature views can well-support that adversarial training is inefficient. Moreover, what is the specific definition of decision space? Since some augmentation methods (like mixup [1]) have the same effect and already be applied to AT. \n2. The motivation for analyzing self-supervised learning is not clear. \n3. The technical novelty of this paper is not high since the idea behind the proposed method is too straightforward and seems to be widely used as a trick to improve robustness (like adopting random start in PGD). Previous research has already proposed \"adversarial distribution training\" [2] which also addressed the drawback of employing single adversarial data and seems to be stronger to the proposed method in this paper. Could the authors give further comparison or discussion between the proposed method with adversarial distribution training?\n4. This article lacks relevant theoretical understanding or analysis of the proposed method. In this case, the experimental evaluation is expected to be systematic, thoroughly clarifying the success and failure cases, but it seems that the current version needs further improvement to discuss the advantages and disadvantages of the method. Specifically, it needs to be more analysis (such as verification on different data sets and different adversarial training methods).\n5. The empirical results in Section 4.1 lack a more comprehensive and diverse robust evaluation (for example, CW [3] attack and AA [4] attack) and some benchmarked adversarial training methods (for example, TRADE and MART). According to the existing results,\n   It seems that natural accuracy is sacrificed too much by adopting this method.\n6. For this method to augment the data pool of adversarial variants, the epsilon ball is implicitly expanded. For the original adversarial training, perhaps a larger ball should be used for training to make the comparison fairer?\n\nMinor:\n\nThe title \"Noisy Adversarial Training\" is not informative and a bit misleading, especially \"Noisy\", because there is no clear definition, it is easy for people to mistakenly believe that it is adversarial training on noisy data. It is difficult for me to get a clear understanding of the main content from such a \"brief\" title.\n\n[1] Lee, Saehyung, Hyungyu Lee, and Sungroh Yoon. \"Adversarial vertex mixup: Toward better adversarially robust generalization.\" *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2020.\n\n[2] Dong, Yinpeng, Zhijie Deng, Tianyu Pang, Jun Zhu, and Hang Su. \"Adversarial Distributional Training for Robust Deep Learning.\" *Advances in Neural Information Processing Systems* 33 (2020): 8270-8283.\n\n[3] Carlini, Nicholas, and David Wagner. \"Towards evaluating the robustness of neural networks.\" *2017 IEEE symposium on security and privacy (sp)*. IEEE, 2017.\n\n[4] Croce, Francesco, and Matthias Hein. \"Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks.\" *International conference on machine learning*. PMLR, 2020.",
            "summary_of_the_review": "Though the empirical study shows some interesting phenomenon, this paper is limited in terms of making novel contributions and rigorous justification of the proposed method. There are still many points to be further worked on and, especially, the writing needs to be further improved.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes Noisy Adversarial Training (N-AT), which adds random noise to the adversarial output vector to create pseudo adversarial instances for data augmentation in the decision space. The proposed method is evaluated on CIFAR-10 with ResNet-18 architecture for supervised and self-supervised learning. For supervised learning, the paper shows that N-AT improves the relative robustness by $1.06$% for seen attacks and $89.26$% for unseen attacks compared to adversarial training. For self-supervised learning, the authors improve similar increases in robustness while also improving the clean accuracy.",
            "main_review": "### Strengths\nThe paper proposed a simple method aiming to introduce noise in the decision space to improve the robustness. The proposed method should be easy to integrate with any existing method. The paper was easy to follow, and experiments were conducted across supervised and self-supervised settings for seen and unseen attacks, which is impressive.\n\n---\n\n### Weaknesses\n\n**Novelty and relation to prior work**\n- The paper has limited novelty, and it does not compare with, or even mention, highly relevant prior work utilizing noise for certified and empirical adversarial robustness [1,2, 3, 4]. \n- The paper mentions that current works use noise for data augmentation in the input space, and the paper investigates the addition of noise in the decision space. However, it is necessary to explicitly mention prior works utilizing noise for adversarial training in the related work section and empirically compare these data augmentation methods to demonstrate the usefulness of the proposed method.\n\n\n**Experimental evaluation** The experimental evaluation is not convincing to validate the claims of the paper. \n- Firstly, the paper only provides results with CIFAR-10 on ResNet-18 architecture. Even Though CIFAR-10 is a standard dataset used by the community, it is essential to include evaluations on multiple datasets and larger architectures such as WideResNet-28 -10 or WideResNet-34-10 as done in prior works.\n- Secondly, the paper compares Adversarial training as the only baseline for supervised training, which makes it difficult to position this work compared to existing works. I would recommend comparing with recent baselines following RobustBench; it is unclear if the proposed method is effective in the current form of experimental evaluation.\n- Thirdly, the evaluation on unseen $\\ell_2$ and $\\ell_1$ attacks is overestimated. For instance, Maini et al. [5] achieved $57.3$% and $16$% on $\\ell_2, \\epsilon=0.5$ and $\\ell_1, \\epsilon=12$ attacks and Madaan et al. [4] reported $55$% and $26$% on $\\ell_2, \\epsilon=0.5$ and $\\ell_1, \\epsilon=7.84$ respectively. I would recommend evaluating with stronger attacks [6,7,8] for a fair evaluation.\n- Lastly, the paper introduces various hyperparameters; however, the choice is not justified empirically. The paper mentions the choice of hyperparameters ($\\alpha_s, \\lambda, \\kappa, etc.) that yield the best results. However, it is necessary to include the ablation experiments that show the effect of these hyperparameters on the robustness to seen, unseen attacks and clean accuracy.\n\n**Clarity**\n- The organization of the paper can be improved for better clarity. Since the paper claims the robustness of self-supervised learning as one of the strengths, the main results of Table 7 should be included in the main paper, and the related work should include the relevant literature as highlighted above.\n- The majority of the text in the method explains Figure 2; instead, including a theoretical justification for the method should strengthen the proposed method.\n- The standard deviation should be reported for all the results.\n\n**Minor fixes**\n- There are inconsistencies between the use of $\\ell_p$ and $l_p$ in the paper.\n- Figure 3 can be rotated for better visualization for the readers.\n- The references should cite the conference proceedings of the respective papers, e.g., Goodfellow et al. 2014, Madry et al. 2017, Kim et al. 2020, etc.\n\n---\n\n### References\n[1] Li et al. Certified Adversarial Robustness with Additive Noise. NeurIPS 2019.  \n[2] Rusak et al. A simple way to make neural networks robust against diverse image corruptions. ECCV 2020.  \n[3] Zhu et al. Understanding the Interaction of Adversarial Training with Noisy Labels.    \n[4] Madaan et al. Learning to Generate Noise for Multi-Attack Robustness. ICML 2021.  \n[5] Maini et al. Adversarial Robustness Against the Union of Multiple Perturbation Models. ICML 2020.   \n[6] Laidlaw et al. Perceptual Adversarial Robustness: Defense Against Unseen Threat Models. ICLR 2021.  \n[7] Brendel et al. Accurate, reliable and fast robustness evaluation. NeurIPS 2019.  \n[8] Croce et al. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. ICML 2020.  \n",
            "summary_of_the_review": "The proposed method is simple and interesting, but the paper has various weaknesses as highlighted above. My major concerns are the novelty of the work, lack of comparison with existing methods, and lack of exhaustive experimental evaluation.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}