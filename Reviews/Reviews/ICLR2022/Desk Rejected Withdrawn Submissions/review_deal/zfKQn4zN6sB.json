{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes applying coreset selection techniques in adversarial training to improve the training efficiency. The experimental result show that the resulting robust models can be trained 2-3 times faster while keep similar clean and robust accuracy on cifar, and svhn (but have larger gap on perceptual adversarial training 3-5% drop on clean accuracy, 4-6 % drop on robust accuracy).",
            "main_review": "## strength:\n1. this paper is generally well-written\n2. using the coreset selection idea is firstly applied to the field of adversarial training to save computation efficiency (though I have some questions regarding to the real computation saving (see below questions in weakness)\n3. experimental result show computation efficiency being improved by 2-3 times, while keep similar clean/robust accuracy on standard adv training.\n\n## weakness\n1. the resulting robust models have larger gap on perceptual adversarial training (3-5% drop on clean accuracy, 4-6 % drop on robust accuracy).\n2. there are some places not clear, which I hope the authors can clarify:\n\n### Questions\nQ1: can the authors explain more details about how exactly the coreset are selected in CRAIG and GRADMatch? I assume the authors use the same way to select the coreset based on the gradients information (except now the gradients are computed involving adversarial examples as described in sec 3.1.1 and 3.1.2?). It'll be clearer to have a more detailed description or pseudo-code as it's the key step. Also, it's not clear about how each coreset selection method compares. What are the pros and cons for each method?\n\nQ2: following Q1, in Fig 1, it shows that the coreset selection needs to be carried out every T epochs. \n\n(1) do we only need the full training for the first T_w epoch, and the rest of epoch will be subset training?\n\n(2) within the T epochs of subset training (the authors mention T is usually greater than 15), do we need to create new adversarial examples every epoch? I.e. although the coreset is selected, we still need to find the new adversarial examples for the coreset *every* epoch? \n\n(3) following (2), if this is true, then the theoretical computation improvement would be (original training set size)/(coreset size)? In the experiment say Table 1, the authors show the computation saving is roughly 2-3 times, while the authors say the coreset size for cifar 10 is 40% and imagenet is 50% --> the improvement should be 2.5 times and 2 times, but the SVHN show >4 times? does SVHN use only 25% coresize?\n\n(4) why does the authors compare to the full adversarial training, which I believe it's the standard PGD training accordingly to the 2nd paragraph in Sec 4, instead of the Fast FGSM in the L_inf norm case? Also, did the authors use the same number of step to find adversarial example in both PGD and adv CRAIG, adv GRADMatch? \n\n(5) for Fast FGSM, although FGSM is designed for Linf norm, it can be easily adapted to L2 norm (either use the sign gradient, or use the standard gradient direction, but normalized by its L2 norm). In that case, that should not be a major limitation as the authors described in the 2nd contribution. Have the authors tried to compare with this adaptation? \n\nQ3: why does the Perceptual attack performance drop much more than the standard adversarial training? The table 2 show 3-6% drop of accuracy.\n\n\n\n",
            "summary_of_the_review": "In summary, the authors provide an interesting perspective to improve the efficiency of the adversarial training by coreset selection. There are some places require clarifications from the authors before making the final recommendation. Currently, I'd be inclined to border line towards rejection. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposed a core-set selection based approach to speed up adversarial training processes. It modifies existing core-set selection approaches to account for adversarial example generation and demonstrates its success with different adversarial training mechanisms. \n",
            "main_review": "*Major strengths*: Majority of work on fast-adversarial training operates on an algorithmic level. However, the proposed approach innovates on the data selection, which makes it applicable to pretty much any adversarial training mechanism. \n\n*Weaknesses*\n\nI believe that the results presented in the paper are highly suboptimal. In particular, Table-1 reports 30.76 and 35.28% robust accuracy on CIFAR-10 dataset (at linf budget of 8/255), respectively. However, a simple baseline [1] approach with the ResNet18 model on this dataset easily achieves close to 40-43% robust accuracy. \n\n*Why is this sub-optimality is a big concern*: Proposed approach claims to keep performance intact while training only an adaptively selected set of data. However, if the model is trained only to achieve sub-optimal performance, implies that there is room to trade-off samples i.e., achieve the same performance with less samples. If the model is trained to achieve best results, thus in hindsight utilizing all samples to best extent, then it will be fair comparison of how trade-off the proposed achieves with performance and speed. \n\nThis limitation is also evident from table-2 results itself. In this harder multi-attack setting the proposed approach loses 3-4% on clean accuracy and harder attacks (l2 and linf). In light of this core limitation, I believe that the claim “faster adversarial training while maintaining clean and robust accuracy” is not valid.\n\nAnother avenue where the paper can be improved is in further analyzing the intersection of core-set selection and adversarial training. While the key focus of the paper on performance, its will interesting to understand how does adversarial training impact corset selection compared to natural/vanilla training? In particular, is the tradeoff between speed and performance with adversarial training worse than natural training?\n\n[1]. https://robustbench.github.io/",
            "summary_of_the_review": "My low score is due to the reason that the some of the current experiments are not achieving competitive performance thus don't support the core claims of the paper. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes to use core selection methods to select a subset of training data to speed up adversarial training. The core selection methods are extended from existing works for regular training combined with Danskin's theorem to handle gradient calculation for adversarial training. Experimental results show that the proposed method can speed up adversarial training without losing too much clean accuracy and robustness.",
            "main_review": "Strength\n\nIt is kind of interesting that the simple selection method works fairly well and does not lead to too much drop of clean accuracy and robustness performance. Also, the method can be beneficial for robustness other than the infinity norm.\n\nWeakness and comments\n\n1. It is widely known that adversarial training requires more training data [A] and semi-supervised training has demonstrated great success in improving adversarial training performance [B]. On the contrary, this paper advocates to use less training data during a period of training epochs which makes me concerned about its practical performance when data are sufficient. A detailed discussion and justification on this would be necessary.\n2. The novelty of the method is very limited which just extends existing works CRAIG and GradMatch to adversarial training with a simple gradient estimation methods.\n3.  There are many other works that can greatly improve efficiency of adversarial training like FastAdv and GradAlign. The paper compared to none of them.\n4. The training speed is very hard to reason since it can be significantly affected by many factors like cyclic learning rate instead of piecewise learning rate scheduler, the learning rate value used, the number of iterations for PGD attack in training, etc. More comprehensive experiments should be carefully designed and provided to support the claim. More importantly, most training time of the 200 epoch adversarial training is just designed to improve 1-2% clean/robust accuracy. The current training time comparisons are not that fair and the 2-3 times faster claim is not convincing.\n5. I would highly recommend using PGD adversarial training with early stop [E] as the standard baseline.\n\n[A] Adversarially Robust Generalization Requires More Data, NeurIPS 2018\n\n[B] Unlabeled data improves adversarial robustness, NeurIPS 2019\n\n[C] Fast is better than free: Revisiting adversarial training, ICLR 2020\n\n[D] Understanding and Improving Fast Adversarial Training, NeurIPS 2020\n\n[E] Overfitting in adversarially robust deep learning, ICML 2020",
            "summary_of_the_review": "Overall, I think the current version is below the bar. This paper lacks comprehensive experiments to support the speedup claims, and comparisons against the state of the arts are missing.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concerns",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper demonstrates that coreset selection algorithms can be used to accelerate adversarial training for a variety of training objectives. The key idea is that existing coreset selection algorithms (which are applied to accelerate vanilla training) can be used by modifying the gradient computation step to compute the gradient at _adversarial_ examples $x_{adv}$, rather than the original inputs $x$. The modification to training reduces runtime by a factor of 2-3 with only a small reduction in both regular and robust accuracy.",
            "main_review": "### Strengths\n\n- **Novelty**: The paper demonstrates accelerating robust training for a variety of robust training objectives. In comparison, existing work to accelerate robust training has focused on the $l_\\infty$ training objective.\n- **Significance**: The required change to existing coreset selection algorithms is straightforward (involving only a change to gradient computation), and promises to be applicable to any future improved coreset selection algorithms.\n- **Significance**: The proposed method does not appear to involve any tuning of hyperparameters; this increases the likelihood that it can be successfully applied by other members of the community.\n- **Clarity**: The paper is very well-written overall. In particular, it:\n  - Contains a well-written background section on adversarial training and coreset selection for readers who are not familiar with one of these ideas.\n  - Contains clear figures, charts, and tables which summarize the key takeaways for readers.\n- **Clarity**: The paper makes it easier to reproduce (and extend) the results by:\n  - Clearly describing how to compute gradients for two different classes robust training objectives.\n  - Summarizing the \"practical tweaks\" that are necessary to achieve the results presented.\n\n### Areas for Improvement\n\n#### Quality: Integrating coreset selection with DAWNBench techniques\n\nIn table 1 of [1], Wong et. al. present a technique to learn a CIFAR-10 classifier with comparable clean accuracy (85.32%, vs ~84%) and significantly higher robust accuracy (44.01%, vs ~35%) to $l_\\infty$ PGD attacks with $\\eps = 8/255$ in less than 13 minutes (vs ~340 minutes for the fastest method presented in Table 1 of this paper). Their training was accelerated via techniques from the DAWNBench competition, which were shown to provide a similar speed-up to other adversarial training methods. In particular, regular PGD could be accelerated to ~70 minutes to achieve similar clean and robust accuracy.\n\nThe empirical results presented in this paper would be quite significantly strengthened by either:\n\n1. Evaluating the combined speed-up of coreset selection with the DAWNBench techniques competition identified in [1]. This would probably lead to a state-of-the-art result in runtime for training networks with $l_\\infty$ robustness.\n2. Demonstrating that the DAWNBench techniques do not work with robust training objectives other than $l_\\infty$ robustness. This would motivate why we should interested in coreset selection as a technique, since the DAWNBench techniques otherwise seem to provide a >40x speed-up to both the \"Free\" approach of Shafahi et al. and PGD.\n\nI expect that the DAWNBench techniques could be used, but perhaps the cyclic learning rate interferes somehow with the presented coreset selection process.\n\n> NOTE: I want to emphasize that I don't think that every research result needs to beat the state-of-the-art; in some cases, a novel approach is valuable in itself. However, two factors are pushing me to request these additional experiments: 1) the paper applies an existing technique (adaptive coreset selection for accelerating neural network training), 2) the gap in the performance presented (~340 minutes vs ~70 minutes) is so large that coreset selection could have limited use in accelerating training if it cannot be combined with the DAWNBench techniques.\n\n[1] Wong, E., Rice, L., & Kolter, J. Z. (2020). Fast is better than free: Revisiting adversarial training. arXiv preprint arXiv:2001.03994.\n\n### Additional Comments\n\n- I appreciated that the differences w.r.t. the baseline were shown in Table 1, and that they were color-coded; this made it easy for me to understand the results at a glance. (The colors might be challenging to tease apart for folks with red-green color blindness, but I don't know how we could make that better.)\n- I didn't understand the point of showing results for both Adversarial CRAIG and Adversarial GRADMATCH. The results for both techniques are comparable; was the point to show that the modification made to gradient computation is compatible with two different greedy coreset selection algorithms? If so, it would be helpful to emphasize this in the conclusion (and perrhaps even in the abstract).\n- The \"batch-wise\" line in Figure 2(b) shows a decrease in the robust error before it increases again as we go rightwards on the horizontal axis. This is minor, but I am curious whether this is a data error, because none of the other plots show such behaviour.\n- Spelling:\n  - Section 1, Paragraph 3, Line 3: 'repetitive constriction' -> 'repetitive construction'",
            "summary_of_the_review": "I'm recommending accepting this paper overall, but with some reservations.\n\nThe paper presents novel work (accelerating robust training for a variety of robust training objectives) that is likely to be applicable by the community because of its simplicity, and can take advantage of future improvements to coreset selection algorithms for accelerating vanilla training. The paper is well written overall, and the authors have made it easy to reproduce and extend the results.\n\nMy major concern is that coreset selection is significantly slower than simply applying a cyclic learning rate and mixed-precision arithmetic (the \"DAWNBench techniques\") for $l_\\infty$ robustness (~70 minutes vs. ~340 minutes -- even without the additional improvements in Wong et. al.). If the authors are able to address my concern here, this paper would clearly score at 8; otherwise, I would maintain my score at 6, since I _expect_ that the DAWNBench techniques are orthogonal to coreset selection, but can't be sure.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}