{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper provides an interesting idea for tackling the catastrophic forgetting problem in class-incremental learning. The proposed Pseudo-replay via Latent space Sampling (PLS) method is simple and effective. The motivation is clear. The paper is well organized and easy to follow. ",
            "main_review": "Pros:\n1.\tThe proposed PLS method has a certain novelty. PLS introduces the pre-trained feature extractor which is commonly used in NLP for continual learning to overcome catastrophic forgetting. PLS samples pseudo-replay feature vectors from a multivariate Gaussian distribution in the latent space. An SVD-based method is proposed for selecting eigenvalues and eigenvectors.\n2.\tThe investigation of related work is sufficient. \n3.\tThe statistical analysis in section 3.4 and appendix C is solid.\n4.\tExtensive experiments have been conducted. The selected baseline methods are relatively new and strong. Experimental settings are fair and reasonable. The proposed method outperforms SOTA baselines on three widely used image classification benchmark datasets. \n\nCons:\nThe pre-trained feature extractor CLIP is an existing multi-modal transformer model. This is a direct migration from NLP and lacks some originality. \n\nQuestions during rebuttal period:\n1.\tAccording to eq.(2), (3), and (4), if elements in the calculated covariance matrix are large, will it lead to large fluctuations in prediction results in repeated experiments? \n2.\tWhether the proposed method is sensitive to noise during pre-training or incremental learning?\n",
            "summary_of_the_review": "My evaluation is positive. The proposed method has some novelty and the theoretical analysis is complete. Experimental results show that the improvement is significant. The writing of this paper is very good.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a novel class-incremental learning method named Pseudo-replay via Latent space Sampling (PLS). As gifted by the wonderful advance of the visual-linguistic zero-shot transfer model (CLIP), the authors simply reuse them to generate pseudo-replay features. To facilitate the memory-efficient pseudo-replay, they adopt SVD and pick top-k important singular values for drawing pseudo features for past tasks. Empirical validation shows superior performance compared to CIL baselines across multiple datasets and memory sizes for the buffer.",
            "main_review": "Considering pre-trained models and appropriate utilization of them for solving continual learning problems is a natural approach to show better performance in practice. The authors use the pre-trained CLIP model as a feature extractor to reproduce representative pseudo-features, which is a reasonable and practically useful direction. \n\nAlso, offline and incremental approximation of covariance matrices technically make sense. But I have several concerns as follows:\n- Network architectures used for experiments are not well described in the paper, including a classifier $g$ and feature extractor $f$. \n- More ablation study: generating the pseudo replay using only mean+z'(\\in\\mathbb{R}^d), or w/o mean to discern the effect of feature means and eigenpairs.\n- Although the authors do not provide a description of the main network structures, I think the size of Resnet-18 would be much smaller than the model using the pre-trained extractor of CLIP. So, without clarifying them, direct comparison of w/ or w/o using CLIP is confusing. Could you give me the model sizes that the authors used?\n- There is no detailed description of how do the baselines train on arriving tasks with CLIP. Do they just fine-tune a single-layered classifier using obtained features from a pre-trained image encoder of CLIP?",
            "summary_of_the_review": "The proposed approach is reasonable, but there exists some concerns. I hope that the authors will solve my concerns.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed a novel approach for class incremental learning. Compared with other state-of-the-art methods which training their networks from either scratch or rough pre-train status, this proposed approach directly deploys a recent powerful fixed feature extractor, CLIP, for feature extraction. Another effective and efficient module is proposed to achieve incremental learning task.\n",
            "main_review": "This paper proposed a novel approach for class incremental learning. The pros and cons are listed below:\n\nPros:\n\nThe proposed methods are easy to understand. The motivation of this paper is reasonable and logical.\n\nThe experimental results demonstrate the considerable improvement of the proposed model compared with other state-of-the-art baselines.\n\nCons:\n\nThe novelty of the proposed method is hard to evaluated. In general, this work highlight that the main novelty of this paper is utilizing a more powerful feature extractor to achieve higher performance. In this scenario, the novelty should be considered limited if my understanding is correct. The author should discuss about how and why this feature could improve the overall performance. Is this due to the large amount of training set or the visual and semantic correlation knowledge.\n\nInstead of CLIP feature extractor, the motivation, and the main differences of the rest of the proposed model is not generally introduced. Does this module contains novelty?\n\nIn table 1, the ablation study of utilizing another feature ResNet-18 as backbone. What is the performance if it is used in the proposed method?\n\nConsider the SVD is utilized in the proposed model. The computational cost should be analyzed.\n\n",
            "summary_of_the_review": "In general, this paper proposed a novel method and achieved considerable performance improvements compared with other SOTA methods. Instead of the new and more powerful feature extractor, the reason behind the improvement (e.g., the pseudo-label reply, model differences compared with other benchmarks) should be introduced/explored.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}