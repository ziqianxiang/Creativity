{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes training l-infinity adversarially robust models by training against an attack that maximizes the entropy of the model's prediction.",
            "main_review": "In terms of language, formatting and general presentation, the paper is well written. For as minor improvement, I would suggest to try to streamline the Experiments section by not having a new figure with caption for each dataset.\n\nFigure 1 does not depict the situation that is usually assumed when adversarial attacks are considered. The standard classifier in (a) and (b) separates the classes well, and the samples in (b-top) are not adversarial examples as the semantic properties have actually been changed: e.g. many of the blue perturbed pluses are close to (and could have been chosen exactly at) red circles. In other words, if the second condition of AT-Asm is not fulfilled for an \\tilde{x}, it is not an adversarial example. It would be better to have an example where the standard classifier is not robust, e.g. by having a zig-zag-y decision boundary. So this example shows that UTA might be better than AT when standard training already produces almost ideal models, which is also reflected in the observation that the shown UTA prediction (c-bottom) are hardly different from those of the standard model (a-top, b-top).\n\nThe paper confuses attacks and training against attacks in many places. For example, Eq. (UTA) describes training with the attack. A discussion of the attack itself before using it in training would be useful.\n\n\"Finally, relative to standard AT methods, UTA: (i) is unsupervised\": If UTA means training with the first row of Eq. (UTA), the labels are used! If it just means the attack, it does indeed not use labels, but can't be compared to AT, which is a training scheme.\nAnyway, this paper should also compare to TRADES (Zhang et al., ICML'19, https://github.com/yaodongyu/TRADES), which is widely used and consistently shows at least slight improvements over AT. TRADES can be described as 'unsupervised' in this sense, since the attack and defense do not use labels but predictions. TRADES is very important for the context of this paper, but also the newer state-of-the-art adversarial defenses need to be discussed.\n\nIt is not clear why the paper emphasizes the distinction between single model and ensemble, when in (E), it uses the entropy of the mean prediction of the potential (if M>1) ensemble for the quantification of the uncertainty. Some methods that use variance within the ensemble (or dropout predictions) are cited, but this does not have anything to do with what this paper does.\n\n\nThe evaluations are too superficial. For evaluating adversarial robustness, only PGD is used, which is not acceptable for a paper discussing adversarial robustness in 2021. At least, some state of the art attack should be run on the models, for example for MNIST strong ones from https://github.com/MadryLab/mnist_challenge, and for CIFAR https://robustbench.github.io/. Also, the paper should describe reasonably thinkable adapted attacks that target potential specific weaknesses of the proposed defense.\nProposed defenses must also be compared to previous state-of-the-art defenses, not just to AT. For MNIST: https://github.com/fra31/auto-attack#mnist---linf ; the models there are all much stronger than what this paper shows, with an adversarial robustness at $\\epsilon = 0.3$ of above 96%, compared to this paper's below 50% in Figure 4. For CIFAR-10:https://robustbench.github.io/ . \n\nShowing (weak) evaluations on MNIST, Fashion-MNIST and SVHN is not enough for modern adversarial defenses. A full evaluation on CIFAR-10, which would be the most interesting dataset in this paper, is unexplainedly left out, and only results for fast attacks during training are shown.",
            "summary_of_the_review": "The evaluations are far too shallow in terms of shown datasets, used attacks, and compared methods, such that it is not possible to conclude that the propose method works at all, much less reasonable well, to achieve adversarial robustness. The method itself and the theoretical considerations are not very intriguing, with the supporting toy example not being insightful as it describes a data situation that does not align with the usual definition of adversarial vulnerability.\n\n####################################\n\n## Update:\n\nMost of the rather large number of original concerns have been resolved, most importantly the evaluation attacks now use AutoAttack instead of basic PGD. Some minor points can still be improved (see the three comment threads below) and better comparisons to other methods that have the same performance goals are necessary.\n\nMost importantly, paper is still missing evaluation in the standard adversarial training setting on CIFAR-10, which is necessary for the reader to judge the proposed method in the most well-studied setting.\n\nThus I still rather recommend to reject the paper (score raised from 1 to 5).",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this work the authors propose an alternative to the often used adversarial training defence against adversarial examples. The proposed defence strategy moves away from the traditional loss-based view towards one focussed on the uncertainty of the model. By focussing on areas of low uncertainty instead of areas of high loss during the adversarial training step the method is capable of exploring further away from the original starting sample. ",
            "main_review": "Please find below my review of the paper *Improved Generalization-Robustness Trade-off via Uncertainty Targeted Attacks*. While I do believe the paper to have its merits, I can unfortunately not recommend the current manuscript to be accepted for publication at the ICLR conference. My main reason for this is that I do not believe the proposed defence strategy to be in line with the intentions of an adversary attacking a model. I will discuss this further below. In addition to this, the experimental evaluation is in my opinion not in depth enough.\n\n### Alignment between defence and intentions of the adversary\n\nAs far as I understood, the main advantage of using an uncertainty-targeted attack over a tradition loss based attack is its functionality to also explore areas away from the decision border that have low uncertainty. While the study of low confidence areas is in itself an interesting domain in the context of generalisation, I fail to understand the relevance of studying these areas in the context of adversarial robustness. Considering the side of the adversary, these areas of low uncertainty are not of interest as they do not change the prediction of the system. Hence, by exploring these areas during the adversarial training process one does not influence the capacity of the model to defend against adversarial examples generated by an adversary. \n\nAdditionally, while areas of low uncertainty are of interest in the context of generalisation, I do not believe that artificially raising the certainty of the model in these areas by adversarial training is desirable. This implicitly assumes that we wish the model to have a high certainty for unnaturally perturbed data. \n\n### Experimental evaluation\n\nWhile I admire the authors extensive study using multiple dataset, multiple models, and multiple train/test epsilons, I still believe the experimental evaluation to be incomplete. Specifically, the experimental evaluation should have contained a study of the performance of the defence strategy using an adaptive attack focussed on exploiting the uncertainty aspect of the model. Using an adaptive attack should in general always be part of the evaluation of any defence strategy. \n\nRelated to this, I also believe that the trade-off between robustness generalisation should have been explored further (both theoretically and empirically) given the emphasis placed on this in the title of the paper.\n\n### Small Notes\n\n- The view of robustness and generalisation as being opposing goals presented in the introduction is in my opinion too one-sided. Also discussing the opposing view in this section (as for example discussed in [Stutz, 2019]) would provide a more complete overview.\n- The related work sections gives the impression that it was primarily an afterthought written when the paper was already completed. In my opinion it does not successfully place the contributions of the paper in the context of the wider body of literature. The section on uncertainty estimation not discussing prior work on studying BNNs in the context of adversarial examples is a clear example of this.\n- Last line before Fig. 1 needs a space before \"See\"\n- Description of Fig. 1 needs a cleanup. For example, it mentions twice that the top is PGD and the bottom UTA. Once at the start and once when discussing col. c.",
            "summary_of_the_review": "While I do believe the paper to have its merits, I can unfortunately not recommend the current manuscript to be accepted for publication at the ICLR conference. My main reason for this is that I do not believe the proposed defence strategy to be in line with the intentions of an adversary attacking a model. I will discuss this further below. In addition to this, the experimental evaluation is in my opinion not in depth enough.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a PGD-like attack for maximizing uncertainty (instead of maximizing cross-entropy loss). They argue that adversarial training with this attacks leads to better clean accuracy and thus a better accuracy-robustness trade-off. Furthermore, it can be applied in latent space and is shown to avoid catastrophic forgetting.",
            "main_review": "Strengths:\n- The paper is generally well-structured and mostly easy to read and follow; plots are clear; the algorithm helps to understand the method.\n- An interesting problem is addressed: the accuracy-robustness trade-off of adversarial training.\n- Considering uncertainty instead or in addition to mis-classification seems interesting.\n- The 2D example helps to follow the argumentation and is generally a good way to introduce the high-level intuition of the method.\n- Experiments on multiple datasets and with latent-space perturbations.\n\nWeaknesses:\n- A comment on the introduction: while catastrophic overfitting is severe for few-step attacks, robust overfitting as introduced and discussed in [a,b,c] is generally more severe and important for state-of-the-art adversarial training, in my opinion. I think a brief mention/discussion would benefit the paper.\n\n[a] https://arxiv.org/abs/2002.11569\n[b] https://arxiv.org/abs/2104.04448\n[c] https://arxiv.org/abs/2004.05884\n\n- There are actually papers that also do a type of adversarial training with attacks motivated through uncertainty. [d] does this for out-of-distribution detection, [e] does it for robustness against various threat models and references in [e] include other examples.\n\n[d] https://arxiv.org/abs/1812.05720\n[e] https://arxiv.org/abs/1910.06259\n\n- The related work part on adversarial training is very superficial in my opinion. The problem is that there is a large body of work on adversarial training and I would expect the authors to make an effort to discuss and differentiate from similar approaches, some mentioned above. Also, the current state-of-the-art does not become clear and is not compared against in experiments later. [b,e,f,g] contain recent discussions of work on adversarial training that the authors could check which works are relevant to include/discuss.\n\n[f] https://arxiv.org/abs/2010.00467\n[g] https://arxiv.org/abs/2010.03593\n\n- While I like the idea of having a 2D example as illustration, I see an important flaw: In the introduction, the authors specifically highlight that the perturbation has to be chosen in away ensuring label invariance. However, assuming that the background colors show the ground truth decision boundary, this is not ensured in this example. For the 20 steps attack for standard AT it directly becomes clear that all adversarial examples went “to the other side” of the decision boundary. This means that these are NOT adversarial examples – they truly changed to an extent that the true label changes. This was actually discussed in the context of the paper by Tsipras et al. in the appendix of Stutz et al., making exactly the same point. The important part here is to acknowledge that if the true class, considering the true posterior distribution, changes within the epsilon-ball, then these are not “adversarial” examples. The analogy on images would be if we allow the attacker to change a dog image to a cat image and humans do really recognize it as a cat (without doubt). This also questions the motivation of the approach, because it seems that UTA mainly preserves clean accuracy because it does not exploit the full epsilon-ball by stopping at the true decision boundary which is estimated based on uncertainty. This could also be achieved using instance-adaptive epsilon-balls as proposed in some papers for adversarial training or just ensuring that the epsilon ball is chosen in a way that they do not overlap between examples of different classes.\n- Regarding to the discussion above, I would be interested in seeing the average perturbation obtained by the attack at test time compared to standard PGD to see if UTA mainly improves clean accuracy by resulting in \"weaker\" adversarial examples in terms of perturbation size.\n- The experiments miss important baselines. As the main goal is to allow a better robustness-accuracy trade-off, the most trivial baseline would be adversarial training with X% adversarial examples and 100-X% clean examples. Varying X allows to control the trade-off as described in Stutz et al. Also, more recent adversarial training baselines (TRADES [h], AT-AWP [c], etc.) are missing and it is unclear whether early stopping was employed.\n\n[h] https://arxiv.org/abs/1901.08573\n\n- The attack evaluation at test time is too weak. PGD is used, probably without any random restarts. I would expect at least an ensemble of multiple attacks or PGD with multiple restarts and more iterations. Ideally, us a standard benchmark such as AutoAttack [i]. Thus, the robustness results are not very reliable. In any case, Fig. 2 shows that standard AT outperforms UTA-AT significantly in terms of robustness, so the remaining benefit would be better clean accuracy for especially high epsilon-ball.\n\n[i] https://arxiv.org/pdf/2003.01690.pdf\n\n- Side comment: epsilon-balls for L_inf of 0.5 or above do not make sense for MNIST or Fashion-MNIST as the class label cannot be presered (the image can become completely gray).\n- For SVHN, some epsilon values in between 0.01 and 0.05 would be interesting, like 8/255 as on CIFAR10.\n- Regarding the latent-space attacks: how is it ensured that the true label of the perturbed representation does not change? Stutz et al. Use a class-specific generative model to ensure that, but here rather large L_inf epsilon-balls of up to 0.5 are used which seems too large.",
            "summary_of_the_review": "Overall, I do not think that this paper is ready for ICLR. The main reasons for this assessment are the flaws in terms of the 2D illustration (and thereby the motivation) and insufficient experimental results (missing simple baselines, unreliable robustness evaluation at test time, non-standard, sometimes not meaningful hyper-parameters/threat models).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes an adversarial attack, called uncertainty-targeted attacks (UTA), which utilize the entropy maximization for generating perturbations. The paper combines UTA with PGD for adversarial training. The empirical results show that the models trained by the proposed method are robust to PGD attacks, while keeps high clean accuracy compared to the models trained by adversarial training with PGD.",
            "main_review": "The paper proposes an attack method based on entropy maximization. The empirical study shows adversarial training with the proposed method achieves better clean-robust accuracy trade-off compared to adversarial training with PGD.\n\nHowever, the idea of utilizing entropy maximization for adversarial training is not novel. Moreover, the empirical evaluation via PGD robustness is not sufficient, so more precise evaluations are needed. The details of the concerns are described below:\n\n The idea of utilizing entropy maximization for adversarial training is not novel. A previous study [1] already exploited entropy maximization for adversarial training. Although the previous study used entropy maximization for regularization, the novelty of this paper is limited.\n\n* The 2D experiment in Figure 1 is intuitive, but I am not convinced this experiment shows the advantage of UTA for two reasons. 1) The setting of this experiment is far from normal settings, because $\\epsilon$-ball for a data sample should not contain any other samples. If a sample with a different label is within $\\epsilon$-ball, the sample is not “adversarial”. 2) While the purpose of adversarial training is to train robust models, this experiment does not focus on robust accuracy.\n\n* As the authors mentioned, PGD attacks may fall into a local minimum. PGD attacks can also suffer from gradient masking. Actually, in Figure 2-4 and Figure 6, PGD accuracy under $\\epsilon=0.5$ sometimes higher than 0.2. These results suggest the evaluation by gradient-based attacks is not sufficient to evaluate the robustness of the proposed defense method properly. Previous studies have mentioned that defense models should be evaluated on adaptive attacks (e.g. [2]). In addition, AutoAttack [3] is a powerful benchmark attack for the evaluation of adversarial robustness, which contains not only gradient-based attacks, but also gradient-free black-box attacks. Unless the models trained by the proposed method are properly evaluated by using these attacks, the empirical comparisons are not sufficient.\n\n* Adversarial training with latent-space attacks under various $\\epsilon$ is essentially not meaningful, because the scale of latent space is arbitrary. By multiplying the last layer of the encoder by a constant C and the first layer of the classifier by 1/C, we can change the scale of latent space so that the condition in the optimization problem holds. The trained model with various $\\epsilon$ can be different, but it will be not due to the difference of $\\epsilon$, but due to optimizer and weight regularization.\n\nMinor comments\n* In the definition of PGD in Section 3, $\\delta_{PGD}^0=0$. However, $\\delta_{PGD}^0$ is typically sampled within a ball like R-FGSM. For example, Madry et al. [4] used a random perturbation as a starting point of PGD for adversarial training.\n\n[1] Yilun Jin, Lixin Fan, Kam Woh Ng, Ce Ju, and Qiang Yang. Rethinking Uncertainty in Deep Learning: Whether and How it Improves Robustness. arxiv 2011.13538.\n\n[2] Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On Adaptive Attacks to Adversarial Example Defenses. NeurIPS 2020.\n\n[3] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. ICML 2020.\n\n[4] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards Deep Learning Models Resistant to Adversarial Attacks. ICLR 2018.\n\n",
            "summary_of_the_review": "The proposed method based on entropy regularization is not novel, and the empirical comparisons are not sufficient to show the robustness of the proposed method.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}