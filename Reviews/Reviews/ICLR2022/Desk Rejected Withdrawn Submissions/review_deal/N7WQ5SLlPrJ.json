{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents two metrics, namely Combined Error Variance (CEV) and Symmetric Distance Error (SDE) with the goal of comparing the class-level bias between two models. CEV is supposed to capture the tendency of a model to favour performance in one class over another. SDE measures the distance from the FPR-FNR diagonal when summing across the different classes of the model. The authors demonstrate the utility of these two metrics in two cases: a) measuring bias in model compression scenarios, as well as when selecting pre-trained models, and b) evaluating group fairness.",
            "main_review": "Starting from the paper strengths, I find that the paper addresses an important and relatively understudied problem in the quickly emerging area of AI fairness and bias. Additionally, the two proposed heuristics-based metrics seem to capture some interesting properties of the underlying models, even though it is unclear whether they are really effective or not. Finally, the proposed application settings, especially model compression and selection seem interesting and important.\n\nHowever, the paper also suffers from several major issues, which would require significant rework of the paper in order to address. \n\nMost importantly, the authors claim that they are the first to study the problem of multi-class fairness/bias. However, there is already some previous work that studies this problem, which the authors do not include in their discussion:\n- Yang, F., Cisse, M., & Koyejo, O. O. (2020). Fairness with overlapping groups; a probabilistic perspective. Advances in Neural Information Processing Systems, 33.\nFurthermore, many of the existing fairness metrics are straightforward to generalize to the multiclass setting, but the authors do not take the effort to study this possibility, but simply discard the possibility that one of these options could work.\n\nA further issue with the paper is the lack of theoretical grounding of the proposed metrics. The authors present the two metrics with some brief intuition of their mathematical meaning. However, in their current form, these seem to be heuristics-driven and there is no theoretical justification of why they should capture unfairness or bias effectively and how they would behave. Additionally, the fact that these metrics are only defined in a comparison mode, i.e. when one compares two models with each other, makes interpretation and reasoning across many different models much more complex.\n\nA last important issue pertains to the limited experimental section of the paper. As stated above, the authors should at least try to compare with generalized versions of some established fairness metrics. Furthermore, in the case of compressed models, it is not clear which are the pairwise model comparisons that the authors consider, while in the case of pretrained model selection, it seems that these are depicted in Figure 3, but it is not easy to understand what is the \"best\" model by reading this figure. Another problematic issue with the evaluation is the use of the Titanic dataset for fairness assessment, which is rather irrelevant. There are much more popular datasets in the fairness literature that the authors could use.The multi-class classification evaluation section (4.2.2), which seems the most important to support the main claim of the paper, is also the shortest one and with the briefest analysis. Last but not least, the normalization step which utilizes a random predictors' CEV/SDE values could lead to incomparability between results in subsequent experiments.\n\nThere are other issues that the authors would need to improve, including writing a more compact and well motivated introductory section (I find the figures depicting the increased use of multiclass problems in literature unnecessary), a much more systematic and well organized literature review section, which could include for instance some definitions of important background concepts with references to important surveys in AI bias literature. Also, the style of referencing is wrong; in most cases the correct way to cite a paper is using (Smith et al., 2000) instead of Smith et al. (2000). The latter is used only when the authors are also the subject of the sentence. There are also other issues with the writing, which would need to be a bit more compact and precise.\n\n\n",
            "summary_of_the_review": "While the paper touches upon an interesting problem setting, i.e. multiclass fairness, which is relatively understudied, it suffers from some major issues, which prevent me from giving a positive recommendation. The main issues include the unjustified claim that this is the first work to study the problem at hand, the lack of theoretical analysis behind the design and behaviour of the two proposed metrics, the limited experimental assessment, and the quality of writing.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes two simple metrics, Combined Error Variance (CEV), and Symmetric Distance Error (SDE), to evaluate class-wise bias of two models. \n\nCombined Error Variance (CEV) tries to quantify the deviation of FPR and FNR differences from their average values, and Symmetric Distance Error (SDE) tries to quantify deviation from a balanced change for FPR and FNR, both are computed per class level and then averaged.\n\nThe authors demonstrate the proposed metrics over several applications, including model selection on compressed models and pre-trained models, plus fairness applications.",
            "main_review": "Strengths:\n- The proposed metrics seem to be simple enough to run model comparisons on a large scale.\n- The proposed metrics extend to multi-class tasks well, and the authors have demonstrated through multiple applications including model selection and fairness.\n\nWeaknesses:\n\n- I think a better justification of why the proposed metrics for quantifying model bias is needed. There could be many simple alternatives, for example, \n1) instead of quantifying FPR and FNR differences, why not use averaged per-class AUC differences? \n2) the CEV metric essentially captures the L2 distance of FPR and FNR differences from their average, why not use other distance metrics? \n3) the SDE metric relies heavily on the FPR and FNR change needs to be symmetric, why is this important in real applications? There could be cases where either FPR or FNR plays a more important role (e.g., fairness for certain disadvantaged outputs).\n\n- The experimental evaluation is relatively weak. For each application, there is no ground truth provided regarding which model is more biased, nor are good baselines provided, hence the proposed metrics do not convincingly show they are superior in quantifying biases in models.\n1) For model compression, Table 1, assuming the # of CIEs is the ground truth, the numbers of CEV and SDE are in many cases inconsistent with the ground truth, e.g., both CEV and SDE provide a different ordering of AT, PKT, SP; both of them show AT + KD and FSP + KD have different biases, but their # of CIEs is the same. How does the result convincingly show the proposed metrics can be reliably used?\n\n2) for pre-trained model selection, there's no ground-truth provided. Why do you conclude CEV/SDE can be used for quantifying biases in pre-trained models?\n\n3) for fairness applications, there's no ground-truth provided either. Also the existing metrics show a very different ordering compared to CEV/SDE (e.g., DIAMR shows NN is less biased, DIMS shows SVM is less biased), how do you conclude the proposed metrics conform to the established work in fairness?\n\nThis paper contains many typos that need to be fixed:\n- Section \"COMPRESSION IDENTIFIED EXEMPLARS\", \"desperate\" -> \"disparate\"? \"modal label\" -> \"model label\"?\n- Eq 2 and 3, i starts from 0 or 1 needs to be made consistent.\n- Many sentences have citations mentioned both in the beginning and the end, need deduplication.\n- Caption of Table 3 is not very readable, please put certain citations in parentheses.",
            "summary_of_the_review": "I think this paper provides some interesting ideas on simple metrics to quantify model biases, but more justification and a more thorough experiment evaluation is required to show the strengths of the proposed metrics.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this work authors propose two metrics for quantifying bias and fairness of the models namely Combined Error Variance (CEV) and Symmetric Distance Error (SDE). Authors claim that these metrics can be beneficial specifically in multi-class classification setting and empirically show the results obtained from applying these metrics.",
            "main_review": "The paper is easy to follow and targets and important topic; however, I have doubts on the novelty of the work and the advantages of their proposed metrics over the existing ones.\n\nStrengths:\n1. The paper is well-written and is easy to follow.\n2. The paper is considering a timely and important topic.\n\nWeaknesses:\n1. Although authors point out some of the disadvantages of the previous methods like them not be suitable for multi-class classification setting, I think that their measures also have the disadvantage of needing a base classifier to be trained and evaluated over, so I am not sure about the trade-off considering that existing fairness metrics can also be expanded to be applicable in multi-class classification setting, but they do not need the base classifier.\n2. Combined Error Variance  seems somehow similar to equality of opportunity or equalized odds definitions, so I am not very convinced if this is super novel. It would perhaps be worth having discussion and pointing out the differences, novelties, and benefits of CEV over existing ones. Maybe a table or something similar highlighting the key differences along with advantages and disadvantages that CEV addresses and not existing fairness notions.\n3. I think authors should consider a wider sets of metrics for their evaluations and again summarize advantages and disadvantages of their metrics compared to metrics discussed and considered.\n\nSmall comments:\nPage 5 Under Compression Identified Examples section, desperate impact should be changed to disparate impact.",
            "summary_of_the_review": "Overall, I have doubts on novelty and trade-offs that their metrics have compared to existing metrics out there. I also think that authors should consider more fairness notions and highlight the advantages and disadvantages of each metric compared to their proposed ones.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Algorithmic bias and fairness problems have received increasing attention from the community recently. This paper proposes two metrics to quantify the relative bias of two multi-class classifiers. These two metrics are evaluated using two applications: quantifying the relative bias of compressed models, and evaluating the fairness of models trained on both binary and multi-class classification datasets. Experimental analysis indicates that the two proposed metrics could reflect whether one model is less or more biased than another one for compressed models. The results also indicate that the proposed metrics could be used to measure the fairness of a model in multi-class classification scenarios.\n",
            "main_review": "**Advantages**.\n1. Both fairness and bias are timely topics, and proposing metrics to quantify the bias and fairness is meaningful.\n2. The proposed metrics are simple yet effective.\n3. There are some interesting experimental observations. For example, the authors find that accuracy alone is a poor indicator of model quality, and the two proposed metrics could help the practitioners to select the models with less bias.\n\n**Weaknesses**:\n1. My major concern is that this is an “A+B” type work. Although bias and fairness are two relevant topics, the two applications mentioned in this work are quite different. It would be great if the authors could focus on either bias or fairness, and conduct more in depth studies, rather than put two seemingly unrelated applications together.\n2. The manuscript contains a lot of abbreviations without pre-definition. For example, in Section 4.1.1, KD is used without giving any definition throughout this manuscript. For readers who have the background on model compression, they might guess that it means knowledge distillation. It would be less friendly for readers who do not have this background. Similarly, what is the meaning for abbreviations, such as AT, PKT, and AGP? \n\n3. It would be great if the authors could clarify the details about the proposed feature map based distillation methods.\n\n4. Table 1 indicates that KD could improve the metrics such as CIEs, CEV, and SDE. However, models with KD have lower accuracy performance. This is in conflict with findings in recent literature, which demonstrates that KD could improve the in-distribution accuracy performance. Could the authors explain why after using KD, the accuracy performance has dropped a lot? \n5. The section 4.2.2 presents a multi-class classification task. However, it does not mention how many classes and what are those class labels.  Only through looking at Figure 4, the readers can guess that there are 4 classes: black, blond, brown, and gray.\n6. In the first paragraph of Section 4.2.2, ‘celebrates’ should be ‘celebrities’.\n",
            "summary_of_the_review": "The proposed metrics are simple yet effective. The experiments could be strengthened. The authors can either focus on bias or fairness and provide more in depth evaluations. Besides, the writing could be improved to make it more friendly to readers. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}