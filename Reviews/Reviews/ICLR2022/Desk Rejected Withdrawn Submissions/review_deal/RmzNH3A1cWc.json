{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper propose a hardware-aware network transformation method by combining the knowledge distillation method and constrained integer programming. The knowledge distillation is used for the initialization of the transformed networks, and the integer programming is for the acceleration of the searching process for the combination of candidate operations. Extensive experimental results show the efficacy of the method.",
            "main_review": "Strengths: While there are more than 100 candidate operators for each layer, the knowledge distillation procedure only takes one epoch to converge, which saves time. What's more, by relaxing the searching problem, the integer programming is introduced, which greatly reduced the search time. The results on Efficientnet and ResNeSt show better accuracy and similar latency level.\n\nWeaknesses: \n1. The main comparison in Table 2, which is also the main results of the paper, is not under aligned conditions. For example, why don't the authors compare the EfficientB2 with the 0.45xEfficientB2? If compared this way, it seems that there is a large accuracy degradation. \n2. the knowledge distillation process is used for each layer separately, without considering the relation between layers. Ablation study is desired to show the advantages or disadvantages of this separate knowledge distillation.\n\nReasons to accept:\n1. The relaxation of the problem and the integer programming greatly reduce the search time for the second stage.\n2. The empirical study is fair.\n\nReasons to reject: \n1. The comparisons in Table 2 is unfair. If we instead compare between models of the same family, no good results are obtained. For example, 0.55xEfficientB2 performs worse than the original EfficientB2. This result does not match the main idea of the paper.\n",
            "summary_of_the_review": "The paper addresses the important problem of Neural Network Acceleration via approximation. The two stage construction of the proposed method, and the techniques used therein, are well founded. However, the empirical results are not that convincing and should be improved.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes HANT to accelerate existing neural networks by replacing inefficient operations with more efficient operations via a NAS-like strategy. It contains three stages. In the first stage, layer-wise knowledge distillation is adopted for training candidate operations. In the second stage, a subnet is selected based on the given latency constraint. In the third stage, the selected subnet is finetuned to get the final result. In the experiments, HANT is applied to EfficientNetV1, EfficientNetV2, and ResNeST, showing clear improvements over the original models. ",
            "main_review": "Strengths:\n1. The empirical results of the proposed methods are strong. \n2. Good writing and clear presentation\n\t\nWeaknesses:\n1. The novelty of this paper is a bit weak. The proposed framework is very similar to NAS. In addition, the usage of layer-wise knowledge distillation is straightforward. Similar explorations have been done in NAS (e.g., [1]).\n2. In addition to GPU and cloud CPU, it would be interesting and useful to see the results of HANT on mobile phones. \n\n[1] Li, Changlin, et al. \"Block-wisely supervised neural architecture search with knowledge distillation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020. \n",
            "summary_of_the_review": "This manuscript presents a simple method that combines several existing techniques and achieves very strong empirical results. Overall, I think this paper is more suitable for a computer vision conference. I recommend weak reject.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to transform a pretrained neural network to meet latency constraints on target hardware by replacing inefficient operations with efficient ones. For this, this method conducts knowledge distillation from the layers of the given neural network to candidate operations. Then, it formulates the search problem as a linear integer optimization to search for the best architectures. To demonstrate such a method, EfficientNetV1, EfficientNetV2, and ResNetST are used as a target network and GPU and CPU are used as target devices.",
            "main_review": "Strength\n\n- This paper is well-written and easy to follow.\n- The reviewer thinks the proposed approach is novel and interesting. To my knowledge, this work is the first work to propose layer-wise knowledge distillation as a pretraining stage for network transformation.\n- This method is flexible to design search space since adding or subtracting candidate operations without retraining all operations of search space while existing supernet-based NAS methods need to re-train all operations.\n\nWeakness\n\n[Major]\n\n- When I first read the goal of this work such that \"transforming/replacing\" neural networks for improving efficiency, I expected that, on a target task, rapid process time and a simple pipeline for practical usage compared with existing NAS works that start from scratch. However, in Table 1, this work has longer times (100eL) compared with existing works (75eL - Once-For-All, 50eL - DONNA).\n    - Furthermore, assume that we have EfficientNetV1-B0 as the base network, EfficientNet-like approaches can rapidly get transformed network fitting on latency constraints by simply scaling. Compared with this, this work requires too much time to transform the base network. Considering this and the large fine-tuning cost, the reviewer thinks the performance is not that impressive.\n- This work covers a large search space ($O(10^{100})$ compared with existing works. Yet, there seems to be a lack of theoretical analysis or empirical experiments to support the advantage of considering the large search space. To my knowledge, recently, many works[1,2] have been shown that we do not need to use large search space since focusing on valid operations or architectures on the smaller search space is enough for the NAS process.\n    - Additionally, this work included layers of Transformer in the architecture candidate set and argues that this is one of their main contributions. Yet, if I read correctly, I did not see in which cases it helps to obtain better-performing architectures.\n- Even though this work focuses on hardware-aware, the number of target devices is two (GPU and CPU).  Once-for-all that one of comparison of this work handled 5 various device platforms (GPU/CPU/Mobile/Edge GPU/FPGA) and more number of devices dependent on the manufacturer (e.g. Google/Samsung/LG). the reviewer thinks this work needs to demonstrate the generality of the proposed method on various devices.\n\n[Minor]\n\n- I wonder about the number of parameters and FLOPS information of models and the reason why the resolution of input images are different per models in Table 2.\n- The measure (e.g. Spearman or Kendal Tau) looks missing in the sentence \"Note that our LUT-based latency estimation has ~0.99 correlation with ~\".\n- The definition of $\\mathcal{S}$ looks missing and that of $X_{tr}$ in equation (1) looks missing.\n\n[1] Compound Once-For-All Networks for Faster Multi-Platform Deployment, ICLR2021.\n \n[2] On network design spaces for visual recognition. CVPR 2019",
            "summary_of_the_review": "The reviewer thinks the main approach that layer-wise knowledge distillation for hardware-aware network transformation is novel and interesting. Yet, the contributions for the NAS field are rather weak to accept the paper: \n1) the total time cost of this method is larger than existing NAS methods (Once-For-All, DONNA, and EfficientNet series) and considering large fine-tuning cost, the performance is not impressive.\n2) Lack of theoretical analysis or empirical experiments to demonstrate the advantage of the large search space, which is one of the main contributions.\n3) Lack of generality on many types of hardwares.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies the deep neural network compression problem. Different from previous works that are either restricted to the underlying network operations without changing the architecture, or involving significant domain knowledge or manual design via knowledge distillation for architeture changes, this paper proposes to conduct knowledge distillation on operation level. The proposed method automatically replaces inefficient operations in a given network with more efficient counterparts. The underlying problem is reformulated as a search problem. Seen from the results, the proposed method works well.",
            "main_review": "The major concern might be the \"hardware aware\" in the title. To the reviewer, one compression method is called hardware aware if it can easily adapt to a new specific hardware w.r.t. latency, memory, storage size on disk. Basically, the hardware aware method should not be fixed with any single artificial measure. For example, storage size does not necessarily be linear with the parameter counts, as it depends on how the parameters of a model are indexed and stored on the disk. For this method, the neural archtecture is searched by selecting operations with minimal latency. The latency of the operations are however precomputed on a specific hardware and form a latency look-up table. On one hand, such mechansim simply restricts the compression method only \"aware\" to latency, as the memory and storage size of the whole model may not be naively additive to each operation. On the other hand, precomputing the latency of each operation can involve considerable noise especially when the hardware is conducting multi-tasks (e.g., CPU), making the later operation selection phase inaccurate. Besides, how complex and difficult it is to precompute the latency lookup table on any new hardware? Also, it is suggested to modify the titile as \"latency aware\", as the proposed method does not adapt to memery, storage and other hardware measures.",
            "summary_of_the_review": "The contribution of this paper is clear, despite the \"hardware\" in title is not accurate.\nThe technique design seems correct and the empirical results support it well.\nThe writting is relatively smooth.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}