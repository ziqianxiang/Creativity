{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This work uses capsules in learning embeddings for different input modalities such as video, audio, text, etc. Capsules are represented as pose vectors and existence probabilities. To model the relationships between capsules, this work uses self-attention as the routing mechanism. Experiments on \"zero-shot text-to-video retrieval\" and \"zero-shot temporal action localization\" shows the proposed approach can achieve state-of-the-art performance.",
            "main_review": "**Strengths**\n\n- Good writing and presentation, easy to follow  \n- The proposed approach can learn good joint embeddings across different modalities. This is verified by experiments on \"zero-shot text-to-video retrieval\" and \"zero-shot temporal action localization\".  \n- Performed ablation studies on the usefulness of self-attention based routing, and the number of capsules.\n\n**Weaknesses** \n\n- There is no cross-modality learning except in the end there are joint embedding losses applied to all three modalities. In this sense, the losses used for joint embedding learning is not novel. \n- As for the improvement made on the backbone model, I am not yet convinced on the usefulness of capsules for embedding learning. The key experimental results demonstrate self-attention is useful in learning better embeddings. But this observation has been proven correct regardless of capsule networks. So the critical question becomes whether or not the capsule networks are really needed here. The author has partially addressed this question in section 4.4 (Comparison with Self-Attention Baselines). But I think the baseline used there (group the activations into N equal length vectors) is not particularly strong. I would suggest comparison with [Double attention networks]. Another route to verify the usefulness of capsule is to do an ablation study on the usefulness of the existence probabilities. What would happen if these probabilities were all 1's?",
            "summary_of_the_review": "The paper has good writing and plenty of experimental results. My main issue is on the criticalness of the usage of capsule networks. The current (self-attention-without-capsule) baseline is not strong enough to verify the underlying assumption. I will reconsider my decision if the author can convince me that capsule network is better than other alternatives (double attention networks, etc).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper introduces a method to tackle the problem of learning from multimodal sources in presence of large quantities of noisy labeled data. Towards the final objective, an architecture building on the concepts of capsules and self-attention is proposed. The main idea is to exploit the self-attention mechanism in place of the \"original\" routing-by-agreement algorithm. A contrastive loss is used to learn a joint representation space between different modalities. Experimental results on 4 benchmark datasets are conducted. Similar/better results than existing approaches are obtained.",
            "main_review": "The paper tackles a quite interesting and challenging problem by building upon the idea of capsule networks and trying to introduce a more efficient solution that will allow to exploit the related concepts on large-scale datasets. More interestingly, the paper focuses on a multimodal setup, thus addressing a variety of very challenging related problems, also considering (partially) noisy inputs (i.e., generated by an automatic speech recognition system). I also appreciated that the work addresses the problem of learning a joint embedding by considering a contrastive loss. \nDespite such positive aspects, the paper seems to be more the result of an exploitation of two different existing tools rather than a fresh new idea. As also stated in the related work, the idea of considering self-attention in capsule network is not novel and has been previously done (e.g., Efficient-CapsNet, Set Transformer, etc.). While it is true that the paper differs in some aspects from such works (i.e., more efficient use of the self-attention and joint embedding computation via an MLP), the novelties brought into the field are quite limited and likely not to be up to the ICLR standards. \n\n\nPositive aspects:\n+ The paper tackles a quite interesting and challenging problem that spans multiple areas of AI/ML since it considers multimodal inputs. The considered setup further compound the multimodal learning challenges since the considered input text is the result of an automated speech recognition system rather than an oracle input.\n+ The routing-by-agreement approach in a capsule network is quite inefficient and certainly deserves attention from the community. The paper exploits a self-attention mechanism that turned out to be far more efficient and less computationally costly, also providing good performance.\n+ To show the benefits of the proposed approach, 4 benchmark datasets have been considered. The experimental results section report on the comparison with very recent works on the 4 datasets, also considering a same7similar backbone to reduce the bias provided by such models. \n\nNegative aspects:\n- First and foremost, it is not clear as it should be what is the main motivation behind the current work and what are the real novel contributions. What is the main goal here? Is the fact that the routing-by-agreement is inefficient and we want to improve it? Is it the fact that capsnet has been properly working only on labeled data? The abstract and the introduction are not clear enough on this.\nAs mentioned above, it seems that the paper is more an engineering piece of work that uses the self-attention in place of the routing by agreement and replaces the capsnet supervised (margin) loss with a contrastive one on modal-related input pairs.\n- The presentation of the method is quite vague and not very precise in terms of its presentation. Typos and grammar problems make reading difficult, thus acting against a proper understanding of the key ideas and techniques proposed by the paper. The text is often too verbatim and not sentences are not clear in their meaning. The adopted notation is also confusing. For instance, in section 3.2 the text states that a capsule is a $d$-dimensional pose vector. Two lines below such a statement, it re-states that the $i$-th capsule has a pose vector in $\\mathbb{R}^{d_1}$. It would be much better to be consistent here.\n- An obscure part of the paper regards the loss function. The contrastive loss is a sum of the three losses computed between the three possible modal-input pairs. The way these are combined together does not guarantee that the three are \"consistent\" in their outputs for a given input datum. That is, an input modality is potentially (i.e., two loss terms) the one that has the most influence on the whole approach. For instance, it is possible that the model only considers $L_{va}$ and $L_{vt}$ to optimize for the joint space. Any comment on this will be appreciated.",
            "summary_of_the_review": "In light of the aforementioned consideration, this reviewer beleive that the paper has some merits but they are not enough to justify an ICLR publication. The most important weakness is the real contribution of the paper that, at the moment, seems to be more a smart engineering work rather than a completely novel approach to tackle the multi-modal learning problem. As such, this reviewer is considering the submitted work below the acceptance bar but would be happy to change the opinion if proper justifications will be provided.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposed a routing with the self-attention mechanism for capsule architectures. Extensive experiments have been conducted to validate the performance of the proposed method.",
            "main_review": "This work proposed a multimodal capsule network with contrastive loss. However, I have the following concerns about the paper:\n1.    It seems like the proposed method is an incremental improvement. Specifically, the proposed routing with the self-attention module is the combination of capsule network and self-attention mechanism. Other modules of the proposed model are also not novel.\n2.    The motivation of the paper is not clear enough. In the abstract, the paper states “To adapt the capsules to large-scale input data, we propose…”. If we remove the routing module, can the capsule network be used on large-scale data?\n3.    The proposed routing module is a single-modal algorithm and does not contribute to multi-modal methods.\n\nIn short, I think the contribution of this paper is significantly lower than that of ICLR accepted papers in previous years.",
            "summary_of_the_review": "The motivation of the paper should be clarified, and the method should be improved.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO.",
                "Yes, Discrimination / bias / fairness concerns"
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "They present multimodal capsule networks applied to video data, specifically, video frames and the corresponding audio and text. Unlike precedent capsule networks, they exploit self-attention in the routing mechanism. It shows that robust training, scalability, and computational efficiency. They choose to evaluate on zero-shot text-to-video (plus text-to-video+audio) retrieval (Table 1) and zero-shot temporal action localization (Table 2) after pretraining on large-scale dataset, HowTo100M (Miech et al., 2019).",
            "main_review": "### Strength\n\nUsing a large-scale dataset, they try to validate the proposed method on zero-shot video tasks to verify the capability of learning multimodal joint representation. They reconfirm that audio information is helpful in text-to-video+audio retrieval for YouCook2 following the previous work, AVLNet (Rouditchenko et al., 2021).\n\n### Weakness\n\n**Vague explanation of multimodal capsule architecture.** Since the main contribution is the multimodal capsule networks using the self-attention routing, this part is the most critical evaluation point. However, the current exposition is a simple variation of self-attention aside from the capsule network framework. Following the primary capsules, each capsule has a pose vector and its activation. Then the two are simply multiplied for routing by self-attention. The next step is nothing more than the multi-head self-attention mechanism. In Table 5, although they compare with self-attention and the proposed method, it is unclear how to \"take the input features and group the activations into N equal length vectors.\" Is this operation unfairly hurting the computational advantage of self-attention? It looks that the \"primary capsules\" is a simple self-gating linear operation, and the multimodal capsule architecture is reduced to this variant, adding the self-gating before self-attention.\n\n- Please clarify how to get primary capsules for reproduction.\n- Could you provide any empirical insight or motivation why the concept of capsule networks is effective in zero-shot video tasks? In the current form, the \"primary capsules\" are merely linear projected features and gating weights.\n\n**Fair comparison.** The proposed method is pretrained on 1.2M videos with corresponding audio and text (HowTo100M). But, you didn't mention that the competitive methods in zero-shot text-to-video retrieval and temporal action localization use the same dataset for their pretraining.\n\n- Please confirm that competitive methods are exploiting the same pretraining dataset. Or, explain why the experiments are performed in a fair condition.\n\n### Writing\n\n- In \"Primary Capsules,\" the authors say, \"a linear layer extracts a set of C primary capsules.\" Then, how to get activations $p_i^m \\in [0, 1]$? \n\n- In \"Routing by Self-Attention,\" since you mentioned, \"from the secondary capsule layer's poses, $\\hat{x}_i^m$,\" shouldn't $x_i^m$ be $\\hat{x}_i^m$ in Eqn. 2?",
            "summary_of_the_review": "A vague explanation of multimodal capsule networks prevents understanding a novel aspect of this paper. Even though they include the ablation study to compare with a vanilla self-attention, the fairness is unclear, and which part is notably distinguishable from a simple variant of self-attention. Whether a fair comparison is performed is another issue to handle.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}