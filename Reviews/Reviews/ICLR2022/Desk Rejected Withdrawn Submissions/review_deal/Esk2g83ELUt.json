{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes to search for network architectures in the combined space of convolution, transformer, and MLP. The resulting network is called UniNet, and UniNet archives better performance than previous pure convolution architectures EfficientNet and pure transformer architecture Swin Transformer. The paper conducts experiments on both image classification and some downstream dense prediction tasks, showing impressive performance.\n",
            "main_review": "Strengths:\n+ To my knowledge, this paper is indeed the first work searching in the combined space of convolution, transformer, and MLP. This does contribute something new to the community.\n+ The resulting UniNet shows impressive performance on ImageNet, outperforming Swin and EfficientNet with lower FLOPs. The UniNet may serve as a good baseline for future research.\n+ The authors do comprehensive ablation studies and analyses.\n\nWeaknesses:\n- The novelty of this work is relatively limited. From a high-level perspective, the work only applies the NAS algorithm ((Liu et al., 2021a) to the new search space with some heuristic tunings and known tricks.\n- For object detection and segmentation tasks in Table 4, the paper only reports #Params. I think #FLOPs is more important and should also be reported. What are #FLOPs of your models?\n- In practice, the latency is more important than #FLOPs. How about the latency of your models on modern GPUs (e.g., Nvidia V100)? Also, are your models still better than the ones you compared in Table 2?\n- It seems difficult to apply a model with MLP to downstream tasks such as object detection because these tasks often take varying-size images as inputs. How do you deal with this?\n",
            "summary_of_the_review": "Overall, this paper presents a network architecture with good performance. I hope the authors can answer my questions in weaknesses.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose to jointly search the optimal combination of convolution, transformer, and MLP as well as three context-aware down-sampling modules (Local-DSM, G-DSM, LG-DSM) on visual tasks. The searched hybrid architecture, UniNet, outperforms previous pure convolution architectures and pure transformer architecture. The authors also conduct downstream task experiments including semantic segmentation and object detection. In addition, ablation studies are conducted to prove the authors' idea.",
            "main_review": "Strength:\n1、\twell-written and easy to follow.\n2、\tGood illustration\n3、\tCarefully designed search space\n4、\tEncouraging Acc performance\n\nConcerns:\n1、\tThere are also many similar works that are absent in this paper like [1], [2], [3], [4] and so on in both CV and NLP, except the works the authors mentioned in the combination of different operators.\n2、\tThe authors mentioned that each operator block under the same network size configuration has a similar computation cost without any empirical evidence.\n3、\tThe details about Multi-Head Attention in G-DSM and LG-DSM such as the number of attention head, dimension, and module parameter size are absent.  \n4、\tThe authors should explain why MLP is missing in the UniNet, as well as the absent G-DSM. \n5、\tIn the ablation studies, the authors only conduct NAS on conv-only setting, lacking experiments on the other two operator-only settings.\n6、\tBesides, the experimental results in Tab 7 show that using L-DSM for the first two stages while LG-DSM for the latter two stages is effective, especially for PVT. However, since the two architectures are transformer-based, according to the authors’ idea, using LG-DSM or G-DSM for all stages seems to be more reasonable. So I speculate that it is the sequence of the two kinds of DSMs that works.   In other words, the DSMs are not relative to the GOPs but the order of themselves.  The authors should provide more elaborated experiments to demonstrate their idea.\n\n[1] CMT: Convolutional Neural Networks Meet Vision Transformers. NeurIPS2021\n[2] MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer. ArXiv\n[3] AutoFormer: Searching Transformers for Visual Recognition. ICCV2021\n[4] Lite Transformer with Long-Short Range Attention. ICLR2020\n\n",
            "summary_of_the_review": "This paper is well writen and easy to follow. However, some most recently related papers are not discussed, which should be differentiated. There are some technical details missing, which should be addressed in the responce.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to jointly search the optimal combination of convolution, Transformer and MLP-mixer operations (GOPs). To better preserve both local and global information, this paper proposes context-aware down-sampling modules (DSMs) and searches them jointly with GOPs. The results on image recognition tasks perform favourably against some compared baselines.",
            "main_review": "Pros: \n\nPerformance on image recognition tasks is promising. It shows the strong potential for combining the general operators - MLP-mixer, convolutions and Transformer blocks properly.\n\nCons: \n\nThe new technical contribution is marginal. The search algorithm is based on previous literature and the main modification is the enlarged search space, where the paper jointly searches the GOPs and DSMs. \n\n\nSome technical details are missing or confusing.\n- 1): No information about the kernel size for convolutions. \n- 2): No searched structure details are provided for UniNet-B0 to UniNet-B5. Moreover, there is a lack of visualization or analysis for the searched architecture, so it’s hard to find new takeaway knowledge for the community. At least the authors should empirically summarize specific architecture design principles from the searched unified architecture.\n- 3): MLP itself is equivalent to 1x1 convolution, which encodes local patterns. The MLP-mixer (Tolstikhin et al., 2021) can indeed capture global patterns to some extent through ​​transpositions. So merely mentioning MLP is not accurate.\n\nIn Eqs. (3) and (4), does it make sense to put FFN layers directly after the MLP-mixer layers? If so, it means that four consecutive MLP layers are stacked together, which seems to be not reasonable enough. \n\nThe experimental comparison is not comprehensive. There are a bunch of NAS-based approaches that simultaneously consider convolution and self-attention [P1, P2, P3]. At least the paper should compare and discuss with these methods.\n\nThe experiments on comparing fixed vs. context-aware DSMs are not convincing enough. PVT/Swin proposes to employ strided convolution/patch merging at the input of each stage. So the complexity of the proposed context-aware DSMs is higher than that of counterparts in terms of the first self-attention layer in each stage. Therefore, it is not clear where the accuracy improvement comes from. \n\nReferences: \n\nP1: NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search, in KDD 2021\n\nP2: BossNAS: Exploring Hybrid CNN-transformers with Block-wisely Self-supervised Neural Architecture Search, in ICCV 2021\n\nP3: GLiT: Neural Architecture Search for Global and Local Image Transformer, in ICCV 2021\n\n",
            "summary_of_the_review": "The technical novelty is limited. The experimental comparsion and analysis should be further improved.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}