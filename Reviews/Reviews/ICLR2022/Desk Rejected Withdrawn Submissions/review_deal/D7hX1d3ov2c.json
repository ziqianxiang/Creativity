{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "As I understood the continual learning mechanism the paper proposes is quite unconventional and consists of three parts:\n- A pretrained feature extractor (Images -> Features)-- VAE trained on Omniglot for MNIST and ReLIC trained on Imagenet for CIFAR datasets\n- A KNN lookup connects the feature with the k most suitable linear classifiers with key similarity output, computed by cosine similarity calculated between the keys and the input feature\n- For each linear classifier, the output is squashed by elementwise tanh to get the resultant vector\n\nThe loss function is the negative value of the element in position of the class label of the output-- obtained by using the key-similarity output as a weight for the output of the classifier, averaged across the k classifiers.\n\nThey test it on MNIST, and blown-up versions of CIFAR10 and CIFAR100 (different from existing works) in a class-incremental setting with a different batch size (48 or 60, it’s unclear) with significantly lower #timesteps to minimize forgetting over. This approach does not use a replay buffer but instead leverages the information contained in the pretrained network.",
            "main_review": "Strengths:\n\n2.1) Good results [Major]: Just looking at the results, it’s improvements are significant. The paper acknowledges that its somewhat different a comparison to existing class-incremental methods-- this is a better way of tackling the practical problem when we have new data incoming.\n\n2.2) Interesting idea about gradient isolation [Major]: The loss function has this idea where due to the lack of averaging across weights, gradients only updates the weight of the class affected. It seems like a potentially interesting way to minimize forgetting. \n\n2.3) Clearly written [Minor]: The contributions are stated, acknowledgement is given to the existing body of work to the best of my knowledge. The approach is clearly illustrated, with details like the experimental setup is clearly described, along with the training procedure etc of the models. Necessary information is given in the appendix whenever I need it to clarify points.\n\nWeaknesses:\n\n3.1) [Critical] Possible crux in 2.1: If the main technical question in continual learning is to ask how to update the deep network-- assuming a larger pretraining network which is sufficient to address these datasets seems to dodge the problem of continually learning itself. This will become a problem when continual learning is really useful: You're given a network learnt on say part of Instagram-1B or JFT-300M  classification and you need to add more parts of the dataset to it-- it becomes really expensive to retrain a model there and this method cannot be applied there. \n\nThe pretraining (self-supervision) needs to be continually learnt on this dataset alongside as well (access to Imagenet seems unfair). Alternatively, we can treat the pretrained model as a feature extractor and compare the proposed ensemble with past works in incremental learning of single-layer models like SVMs or ensembles like Random forests. Comparing this to approaches which try to learn the deep network itself seems like an apples to oranges comparison in my opinion. Note that the argument is against using a pretrained classifier, not about self-supervision. Using a pretrained imagenet classifier would face a similar issue.\n\n3.2) [Critical] Need fairer comparisons: The above point is a more general point, what additional experiments would help me to evaluate this work. Specifically, I am afraid of two basic baselines might perform nearly the same as the proposed method:\n- Similar to the ensemble part: How do one-layer learning algorithms like Incremental SVMs [1] and similar works published back in 2005 compare with the proposed ensemble given the same features.\n- Similar to the key-feature KNN part: Using class-prototypes obtained by weight-imprinting [3] as often done in few-shot learning is continual (Baseline++ in [2] is a nice, strong implementation).\n\n3.3) Different batch sizes: Using different batch sizes leads to different number of online timesteps, which significantly changes the setting which makes all comparisons unfair except GDumb.\n\n[1] Incremental Support Vector Learning: Analysis, Implementation and Applications\n[2] A Closer Look at Few-shot Classification, ICLR19\n[3] Low-Shot Learning with Imprinted Weights, CVPR18\n",
            "summary_of_the_review": "There seem to be critical flaws (3.1 and 3.2) in the approach taken and the comparisons made in this work. If these points are adequately addressed, it would then make more sense to delve into more in-detail issues currently like requesting more comparisons in Fig 1 and fairer experimental settings etc. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "*Summary Of The Paper The authors try to tackle the sub-problem of continual learning where task boundaries don't exist or are unknown, and where classes have to be learned online (with each example presented only once). Authors used a fixed pre-trained encoder for feature extraction followed by ensemble of classifiers. Once the feature is extracted for specific input using the pre-trained encoder, and the resulting encoding is used to look up the k classifiers with the closest keys by cosine similarity. The extracted features is passed through each of these classifiers, and the resulting vectors are aggregated to produce the models final output. The classifiers output vectors are aggregated by taking their weighted average, where the weighting for each classifier is given by the cosine similarity between the encoding and the classifier s key.\n\n",
            "main_review": "Clarity:\n\nThe paper is clearly written and easy to read. The method proposed is well described and it would be easy to reproduce.\n\nNovelty:\n\nThe methodological contributions do not seem very sophisticated and limited , but the experiments show that the proposed method working, despite being very simple.\n\nPros: The method proposed is simple and can be inserted or extended into other methods.\n\nCons: The comparisons are not complete as shown in the Table 1 against the other methods (even though those results are taken directly from existing papers). Right now as we can see in Table 1 (except the vanilla,tanh classifier) it only compare against exists methods in 4 cases only with fair comparison in only one case.\n\na) Case 1 : MNIST 5-way Split GDUmb outperforms existing method b) Case 2 : MNIST 10-way split CN-DPM outperforms existing method c) Case 3: Cifar-5way split which is only fair comparison against 3 methods. d) Case 4: Cifar-100 20-way split only compared against only one method\n\nSo based on the above cases we can't totally estimate the effectiveness of the proposed method.\n\nSo with the given limited comparisons against the other methods it would be difficult to evaluate the robustness of the proposed method. I also feel that One the main comparisons authors need to make is about the memory requirements vs other methods as using a big pre-trained autoencoder will need large memory.\n\nOne more comment about the Table 1 results is that compared methods like Cn-DPM uses 10-layer ResNet for classifier and ResNet-18 for based model and fair comparison can only be done when both the proposed method(Resnet-50) and other compared method uses the more or less same model architecture to solve the problem\n\nIn the Subsection(Findings) of ref Sec:3 Evaluation. The authors made a statement in support of there method w.r.t to usage of large memory hungry encoder as follows \"Moreover, in other branches of machine learning, progress is often made by building bigger models, reflecting the ever-increasing memory capacity of commodity hardware.\"\n\nCorrect me if I am wrong but I can't totally agree with this statement because comparing the continual learning setup w.r.t other branches of machine learning(NLP) is not fair as the goal of the continual learning is to have a model or a system which can learn n-number of tasks one after another with no catastrophic forgetting and sometime use old information to learn the new task faster. But if we have such a large overhead storage for the model or system then dynamically growing networks or mask-based or replay based method will also perform well by either growing when needed without memory constraint or keep more exemplars for retraining. so the statement of bigger models doesn't totally fit in here in this setup.\n\nSecondly when authors are proposing the motivation behind using big pre-trained autoencoder. The results will be only reasonable if they show that there method is scalable for larger no of continual learning tasks. Because it was stated that using such bigger models then the should have showed the specific advantage of using such large models such as \"scalability of the overall system\" may be by showing single continual learning for 100 task or more to justify the need of bigger models.\n\n",
            "summary_of_the_review": "Please see the before section",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a task-agnostic continual learning method.\nUnlike previous works, the proposed method does not learn deep representation and uses a pre-trained encoder instead.\nOn top of the pre-trained encoder, the authors train an ensemble of simple classifiers continually.",
            "main_review": "## Strength\n- The proposed method is simple and easy to implement.\n- The paper is mostly well-written and comprehensible.\n\n\n## Weakness\n### Not learning representation\nI think there is a good reason why previous works did not use a pre-trained encoder: the crux of continual learning is *to learn representation* in a non-iid data stream.\nDeep learning revolutionized the machine learning field due to its unprecedented representation learning capability.\nIf there is a good representation already, we can employ the decades-long literature on online/streaming learning of classical ML models.\n\nIn practice, of course, the use of a pre-trained network can increase the performance by a large margin.\nBut I think it is too obvious to be considered as a novelty.\n\n### Ad-hoc design of the classifier ensemble\nThe proposed classifier ensemble and the loss function are not theoretically justified and rely on heuristics.\nI think it is more reasonable to use a well-established online/streaming learning method for classical models, such as naive Bayes, KNN with core set selection, Dirichlet process mixture model, etc.\nI guess using nonparametric methods (KNN, Gaussian process classifier, ...) with a core set selection algorithm can achieve similar or better performance.\nConsidering the large size (1024) of the ensemble, I suspect that each classifier might blindly output a single class, just relying on the associated key.\nSo the proposed classifier ensemble may actually be a sort of KNN classifier already.",
            "summary_of_the_review": "Although the authors show that the use of a fixed, pre-trained encoder greatly increases CL performance, I think this result is expected.\nThe proposed classifier ensemble and training scheme are too ad-hoc and not justified properly.\nI suspect more well-established online/streaming learning methods can perform better.\nOverall, I think the paper fails to deliver useful insight for continual learning.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a simple architecture for continual learning consisting of a frozen pre-trained feature extractor in combination with a group of single-layer perceptron classifiers. The proposed method is evaluated and compared with several recent methods from continual learning literature. In the final part of this work, the authors provide a tentative explanation of what makes the proposed approach effective.",
            "main_review": "This paper proposes a very practical solution to the issue of catastrophic forgetting, namely leveraging the well-known properties of self-supervised learning and ensembles to try and marginalise catastrophic forgetting out of the process of training the model. In this sense, it is quite unlike \"canonical\" continual learning papers and somewhat closer to radical approaches that question our advancements in the field (like GDumb).\n\nWhile I feel that this is a valid line of research and that the proposed method is undoubtedly effective, I find it should need to address some key aspects to improve its strength:\n\n+ First and foremost, when reading how the authors base their solution on a pre-trained feature generator, I cannot help but wonder whether the ensemble part of this architecture is at all necessary. I was honestly expecting to find a clarification of what makes the ensemble essential in Section 4; instead, one of the key insights discussed in this section is that it is likely that features cluster in a class-related manner and that the ensemble is capable of picking up on these relations. On these grounds, I suspect that a simpler nearest-neighbour classification scheme applied directly to the encoder's latent space should be capable of delivering performance in line with the proposed approach and I would really love to see such a baseline in the experimental section. If this were the case, then the proposed architecture could be further simplified and still achieve a very solid performance.  \n+ As I mentioned above, I find this work to be very ambitious, in that it boldly explores a new and unsearched direction for improved continual learning solutions. As the authors recognise in the text, one of the difficulties of current continual learning literature is the fragmentation of the experimental settings, making it so that it is very difficult to compare two works without specific precautions.  Given the potential of this paper, I would strongly recommend experiments to be made much more comprehensive by either (A) spanning a lot of distinct settings to validate the proposal as done in GDumb; (B) re-running the experiments for competitors on all tested benchmarks, instead of simply reporting their original results on a very limited set of evaluations. As a reader, after reading the experimental section I find that the comparison with other methods too limited to give me a clear intuition of the pros and cons of this approach in relation to current literature.\n+ Similarly, I believe that the repercussions of the choice of exploiting a pre-trained encoder should be further investigated experimentally. As correctly acknowledged by the authors, this specific point might make evaluations not like-with-like. Again, to maximise the transparency of the results, a more like-with-like evaluation could be provided by testing the competitors using the same pre-trained backbone network as a starting point. This way, it would be possible to clarify whether employing a pre-trained network in itself leads to increased performance or whether the proposed encoder/ensemble has an added value to it.\n",
            "summary_of_the_review": "The paper proposes an effective technical solution to catastrophic forgetting by exploiting a pre-trained model in combination with an ensemble of classifiers. Overall, the theoretical insights provided are very limited and I feel that further efforts should be spent in (1) clarifying whether all parts of the proposed approach are instrumental to the achievement of final result or whether the architecture could be further simplified; (2) curating the experimental section by increasing its fairness and making it more comprehensive; (3) investigating whether the assumption of having a pre-trained network leads to similar performance gains in current continual learning methods.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}