{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes to protect Intellectual Property (IP) by embedding watermarks in the NLG models. Specifically, this paper proposes a watermark injection pipeline consisting of three main compossitens WmGen, Mark, and Verify: WmGen generates a set of watermarks, Mark trains NLG model with the clean training dataset and watermark dataset, and Verify veriﬁes whether a suspicious model contains the watermark. During WmGen, the authors ensure the watermark datasets are syntactically correct and the generation should be semantically indistinguishable. Experimental results show that the proposed method has high effectiveness, robustness, and undetectability.",
            "main_review": "Strengths:\n1. Training an NLG model with watermarks is an interesting and less explored problem.\n2. The paper demonstrates good empirical results in terms of effectiveness, robustness, and undetectability.\n\n\nWeaknesses:\n1. Lack of discussion and comparison with existing baselines.\nAlthough the authors claim that adding watermarks is a rare topic in the NLG domain, adding backdoor triggers has been a well-discussed topic recently, and adding backdoor triggers and adding watermarks to the NLG models essentially share the same goal and evaluation protocol. As a result, I am wondering if the authors can provide more insights on the relationship between their methods and existing backdoor attacks in the NLG model, and preferably more discussion on the empirical comparison between the proposed method and the backdoor attack methods.\n2. The clarity of the paper can be improved.\nFor example, why carefully constructed input-output pairs as watermarks can be regarded as black-box watermarks? You have to access the whole model and **fine-tune** the whole model with the watermark dataset. In addition, the details of the experiments are missing, which makes it difficult to understand the watermark algorithm, including how hyper-parameters are chosen, e.g., the threshold \\tau.\n\n\nAdditional questions:\n1. Why the generated watermarks should be syntax correct to achieve undetectably? Recent backdoor attacks also achieve good stealthiness with semantically meaningless triggers.\n\nI am willing to raise my scores if the problems above can be well addressed.\n",
            "summary_of_the_review": "Strengths:\n1. Training an NLG model with watermarks is an interesting and less explored problem.\n2. The paper demonstrates good empirical results in terms of effectiveness, robustness, and undetectability.\n\nWeaknesses:\n1. Lack of discussion and comparison with existing baselines.\n2. The clarity of the paper can be improved.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a watermarking framework for natural language generation (NLG) models. The authors motivate the need for such an approach and define a watermarking framework to have to satisfy the following four requirements: functionality, robustness, undetectability, unharmfulness. The proposed method consists of the three parts WmGen (generation of semantic combination watermarks (SCPs)), Mark (model training on watermark corpus) and Verify (verification of the model). The authors evaluate their approach on two tasks, a machine translation task, and a dialogue generation task, by assessing model performances and watermark embedding success rates under different experimental conditions (i.e., when the model is trained/fine-tuned with the watermarks and in the context of transfer learning). Finally, the authors discuss the detectability of the watermarks and provide results using three detection methods based on perplexity, edit distance, and BERTScore.",
            "main_review": "The paper addresses a gap in the scientific literature by proposing a watermarking framework for NLG models. The method seems technically sound, and the experiments are extensive. Furthermore, the authors discuss the issue of detectability. However, the paper contains various confusing parts and discusses the methods and experiments only vaguely at points. I am pointing out the respective issues below in the Questions and Comments sections. Furthermore, the paper has several typos, grammatical mistakes, and incomplete sentences.\n\nAlthough I believe that this work is likely interesting to researchers working in this area, several details and experimental design choices seem unmotivated and would need further elaboration prior to publication.\n\nQuestions:\n* For transfer learning, it is stated that “In the transfer learning process, we use the same word dictionary generated from clean training data to preprocess the parallel corpus, which causes some words to be labeled ’unk’ for the lost in the word dictionary.” It would be helpful to quantify this in the paper.\n* For Section 5.4, what is the fraction of watermarks that are detected when all three methods are combined? The current experiments are slightly misleading since the actual rate of detection might be higher if all three methods are used simultaneously.\n* Algorithm 2: what does the “t” stand for? This seems unclear from the manuscript (missing definition).\n* It is stated that a basic model is trained using fairseq. Can the authors elaborate on the specifics of that model? Does the used model attain SOTA results on the two datasets? If not, what would be the implications of this? It would be important to elaborate on this.\n* Watermarks generation: why exactly were the two SCPs chosen? Which other most frequent patterns existed in the corpus, and why weren’t other SCPs selected instead? This would need to be motivated.\n\nComments:\n* Eq. 1: the product does not have an upper limit (T_y should be placed above the Pi?).\n* Eq. 1: the (x, y) in D seems to be misplaced under the argmax formulation.\n* The above two comments hold for Eq. 2 as well.\n* Eq. 3 is a little unclear and would need further elaboration.\n* In Section 5.3.1, the authors write “…the BLEU score increased from 26.34 to 26.44…” but Table 2 does not show the same values.\n* The paper contains several typos and incomplete phrases or sentences (e.g., “Fine-tuing” in Table 2 or “To make the copy distinct from the original model…” in Section 3.1)\n* Algorithm 2: “Scount” should be “count”?\n",
            "summary_of_the_review": "The paper addresses an important gap in the scientific literature and proposes an extensive experimental evaluation of the proposed method. Nevertheless, several design choices and technical details are vaguley described and would need to be addressed before publication.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work investigates the watermarking of natural language generation models. The authors claim that existing watermarking schemes, such as backdoor based methods, are harmful to the applications and the embedded watermarks are easily detectable. To bridge the gap, this work proposes a semantic and robust watermarking scheme for NLG models, by using pair-matched phrases as watermarks. Experimental results indicate that the proposed watermarking scheme is effective, the generated watermarks are undetectable, while robust to several kinds of watermark attack methods.\n",
            "main_review": "**Advantages**.\n1. The proposed watermark framework is novel and smart. The Semantic Combination Pattern guarantees that the watermarks are in-distribution data, which is a distinct difference between backdoor-based watermark solutions. In addition, all the WmGen, Mark, Verify three steps are carefully designed to make sure that the watermarking satisfies the four proposed requirements.\n\n2. Experimental results indicate that the watermarks are effective, undetectable, while robust to watermark attack methods such as finetuning and transfer learning. \n\n\n\n**I have the following concerns**.\n\n1. There is no comparing baselines. It is thus difficult to evaluate the relative effectiveness of the proposed model. In particular, in Section 3, the authors claim that there are several weaknesses for backdoor-based watermarking schemes. However, there are no experimental results to support these claims.  These several mentioned weaknesses might hold true for computer vision applications. It is however uncertain whether they are also valid for the NLG scenario. \n\n2. The authors mentioned that some types of word tags and the configurations about dataset and models can be found in Supplementary. However, no Supplementary/Appendix can be found. \n\n3. The writing needs to be improved. For example, in the last paragraph of the Introduction section, two abbreviations (SCW, SCP) are mentioned without giving their detailed definitions and full names. The readers are confused about their meanings until they read the definitions in Section 4.\n\n4. In Table 2, ‘Fine-tuing’ should be ‘Fine-tuning’.\n\n",
            "summary_of_the_review": "The proposed method is novel and smart. The experimental evaluation part could be strengthened, to better validate the effectiveness of the proposed method. Besides, the writing can be improved. The authors mentioned Supplementary, which we can not find in the submission.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}