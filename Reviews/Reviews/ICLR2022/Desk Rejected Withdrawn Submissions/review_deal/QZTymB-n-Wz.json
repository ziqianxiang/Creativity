{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a method to verify monotone deep Equilibrium models. The authors propose to use the Zonotope approximation from Singh et al. 2018 to find over-approximations/relaxation for the implicit layers. They demonstrate the effectiveness of the approach on CIFAR10 and MNIST settings.",
            "main_review": "Strengths\n1. The authors propose a new technique for the verification of monotone deep equilibrium models (monDEQs).\n\nWeaknesses\n\n1. Limited relevance of the problem\n* MonDEQs are not used in practical applications. They are not popular among the research community. So verification of monDEQs is not very relevant for the general machine learning community.\n* This paper does not address the limitation of current verification systems. But instead gives a simple extension of the current techniques to a small set of models which are not popular. \n* Are there any specific real-world applications where monDEQs are used and thus this system will be helpful?\n[comment]: <> (I personally believe that efforts should be spent on addressing limitations of existing verification systems and such papers only add noise to the)\n\n2. Writing is not good\n* The paper is not well written and needed multiple passes for understanding. The authors should spend some time re-organising the re-writing it. \n* Suggestion: A brief overview of the algorithm should be first presented and then the different parts should come as needed. For example, I do not see the point of Theorems 4.1 and 4.2 until section 5.2. However, you can provide a summary of the algorithm before sec4, as is presented in para one of 5.2.\n* Figure 1 is not very clear.\n* I believe that Theorem\n\n3. Missing baselines\n* The method is not properly motivated and put in the context of works. You have not shown why the current works from verification literature are not directly applicable.\n* Why can a linear propagation style algorithm like CROWN not be used here? I would like to see a discussion on that and a comparison to it. This will also help outline the difference of the implicit setting from the fixed layer setting.\n\n4. Incremental technical contribution\n* Overall the method seems to be an extension of the Zonotope based verification work (Singh et al. 2018) to the implicit network setting. What is the technical delta over Singh et al. 2018?\n* What is the technical delta over Hybrid-zonotope Mirman et al.2018? What is the difference of M-zonotope to Hybrid-zonotope?\n* Relevance of m-zonotope over zonotope: Just the zonotope seems to be performing well in Table 3. In fact it seems that ways to tune $\\alpha$ should be investigated. What all ways have you tried for this? Are there relevant works in PR literature that explore ways to tune $\\alpha$? Have you explored them?\n* Relevance of part 1 of algorithm: Overall the algorithm seems to have 2 parts: (i) Finding an over-approximation that contains $Z*$. (ii) shrinking this over-approximation using FB. (ii) is supported by Theorem 4.2 which is a simple result. Can you provide some other baseline for (i)? If you propagate linear constraints using CROWN, will it achieve (i)? Can you provide such a baseline, as it will show the need for Theorem 4.1 and m-zonotope. \n\n5. Limited experimental validation\n* It has become common in verification papers now to verify medium-sized networks. But the networks verified here are very small. Why are results not provided on medium-sized networks like ConvMed?\n*  The networks verified here only reach up to 63% accuracy on CIFAR10. So these networks are not relevant. One major point in favour of MonDEQs is that they reach good accuracy on some standard tasks. But here it seems that your method cannot verify those networks.\n\nQuestions\n\n1- How did you do the inclusion check for zonotopes in the experiments? \n\n2- At the start of sec5, you mention that inclusion checks for zonotopes are computationally expensive. Why is (Sadraddini & Tedrake, 2019) cited here? Can you provide the relevant result from (Sadraddini & Tedrake, 2019)?\n\n3- How is step 10 performed? How do you perform this operation of sets? Section 4 only provides a definition that it would be set of all $h(x)$ for all $x \\in X$. \n\n4- How do you perform the ablation with just FB? Do you use FB for line 6 in the algorithm as well? Is that supported by Theorem 4.1?\n\n5- Why is only PR performing so bad? Did you try running it for a longer time? Maybe it hasn't been tuned properly. \n\nI am willing to increase my rating depending on the authors' response to my concerns.\n\nMinor edits:\n1. Fix the line before eq 6 where you have defined both the symbols to be the same. ",
            "summary_of_the_review": "The authors propose a new method for verifying monotone deep equilibrium models. However, the problem is not very relevant. Baselines are missing, technical delta over some related work is not justified and the experiments are only performed on small networks. All of this further reduces the importance of the method. Therefore I am voting to reject the paper.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a robustness certification algorithm for a special kind of deep equilibrium model (DEQ), namely the monotone DEQ (monDEQ). It borrows zonotope based certification technique for neural network verification (Singh et al, 2018). In monDEQ, the output of the model comes from an iterative solver such as forward-backward splitting. It can be seen as computation with linear and ReLU functions (Eq. 3), so existing zonotope based certification can be applied with some extensions. Some preliminary results are given on a few small monDEQ models with a few hundred neurons.",
            "main_review": "Strengths:\n\n1. This paper is the first zonotope based approach for certifying the robustness of monoDEQ models. Zonotopes are often used for certifying feedforward neural networks but they have not been applied to monDEQ models before.\n\n2. The M-zonotope is an extension of existing zonotope abstraction by combining a box with a zonotope. This combination has not been used in previous works, but isn't box a special case of a zonotope? I am not sure if combining a zonotope with a box increases the precision of overapproximation.  I am also not sure how powerful it is compared to the more advanced polytope based approach (DeepPoly, Singh et al. 2019).\n\n3. The paper is overall easy to follow, however, I think proofs should be moved to the appendix.\n\nWeaknesses:\n\n1. Technical novelty is limited, as zonotope based analysis is a mature technique for certifying the robustness of neural networks. The extension of this technique to monDEQ models is relatively straightforward.\n\n2. Experiments section is largely incomplete and comparisons to a few important baselines such as (Revay et al. 2020) and (Pabbaraju et al.) are missing, which are based on certifying Lipschitz constants.  In table 1, none of the baselines (based on Lipschitz constant or SDP) are included (such as Pabbaraju et al.  2021 and Chen et al. 2021).  These should be added. Very limited comparisons are given in Table 2 for only 1 baseline but it seems the results are questionable - the baseline method SemiSDP has a fixed runtime, which probably means that the authors did not terminate the solver correctly and only ran a fix number of iterations.  This is not a fair comparison.\n\n3. It seems the zonotope based approach is not suitable for certifying DEQs and the proposed approach performs poorly compared to results reported by other monDEQ papers.  For example, if you take a look at the model in Figure 4(a) of (Pabbaraju et al.  2021), they have much better certified accuracy (over 90% at eps=0.05 on MNIST and they can also certify very large eps like 0.2) and they are also much faster (no need to do any zonotope analysis).  If using Lipschitz constant can certify the model much better, I feel the use of the zonotope based approach is questionable - since it requires to overapproximate multiple iterations of the solver which is a fundamental limit of these overapproximation based methods.\n\n4. The scope and practicability of this work are very limited - monDEQ itself is still a quite immature model and its performance is quite poor (compared to other SOTA architectures like CNN and Transformers). Even worse, the models studied are very small, and the clean and certified performance is far from the state-of-the-art certified defense methods. So I feel this paper will make a very limited impact on this field.\n\n",
            "summary_of_the_review": "Overall, I feel this paper is mostly like a hammer in search of a nail rather than solving a useful problem. Additionally, the technical novelty is limited and the experiments section still has quite a few things missing (as mentioned above). The zonotope based certification also cannot convincingly outperform existing approaches. Therefore I rate this paper for rejection. Several suggestions to the authors to further improve the contribution of this paper:\n\n1. Extend to regular DEQ models rather than limiting to monDEQ only. monDEQ is too restrictive. Since we are doing point-wise certification, in a DEQ if an input point can converge, it should be possible that the overapproximation can also converge, so the same zonotope/box analysis may also be applied for each specific test point. This allows us to do better than the existing Lipschitz constant based certification since they can be used for monDEQ only.\n2. Extend the algorithm to use a more precise abstraction such as DeepPoly (Singh et al., 2019). DeepPoly is one of the most popular approaches for neural network verification and has largely replaced the zonotope based approach used in this paper.\n3. Combine training and certification to enlarge the robust radius such as done in (Mirman et al., 2018).\n\nReferences (that were not cited in the submission):\n\nSingh, Gagandeep, et al. \"An abstract domain for certifying neural networks.\" Proceedings of the ACM on Programming Languages 3.POPL (2019): 1-30.\n",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper presents a verification procedure for Monotone Deep Equilibrium Models that is based on abstract interpretations of the fixpoint operations. In particular, instead of the box domain, the M-Zonotope domain is used which is more precise but still admits scalable inclusion checks. ",
            "main_review": "The paper introduces a sound technique for verifying monDEQ. The paper is well-written and appears to out-perform existing baselines.\nThe proposed procedure is reasonable, in order to enable efficient verification, it seems unavoidable to sacrifice both completeness and convergence. \nI am concerned with the choice of the abstraction domain. Another candidate is DeepPoly, which generally provides better empirical results than Zonotopes in traditional neural network verification scenarios can improve the performance. Since a DeepPoly element only has four linear constraints, checking inclusion between two DeepPoly element should also be efficient (e.g., with 4 inexpensive LP calls). I wonder whether the authors have tried to use the DeepPoly domain and what is the motivation for using Zonotope?\nAnother concern is with the depth of the technical contribution. The abstraction domain is the same as  (Mirman et al., 2018). And the main contribution lies in the inclusion check between two M-Zonotopes and the overall procedure (Algorithm 1), which is specific to monDEQ, a rather niche architecture. \nIn conclusion, I think the choice of the abstract domain lacks strong motivations and the depth of the contribution does not warrant a publication at ICLR.",
            "summary_of_the_review": "Reasonable procedure for verifying monDEQ, but lacking strong justification for the choice of the abstraction domain and depth in technical contribution.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper deals with verification of a recently introduced type of model, Monotone Operator Equilibrium Models (MonDEQ). \nThese models are defined by finding the fixed point of an operator corresponding to the input, and applying a linear transformation to it. \nThis fixed point is usually found by operator splitting iterative algorithm.\n\nThis paper want to perform verification of MonDEQs models and need to therefore compute on over-approximation of the reachable values (a convex overestimation of the reachable fixed points).\nThe authors show a theorem (4.1) that if the over-approximation \"shrinks\" at one iteration, then this guarantees that the fixed point set is included in that approximation.\n\nAs a result, they need to have a representation of the over-approximation that allows checking for inclusion.\nThe authors discuss how to do this using an Hybrid-Zonotope representation.\n\nThe resulting algorithm (CRAFT) propagate those zonotope through the iteration of the operator splitting algorithm, checking at each stage whether the overapproximation has started shrinking. Once it has, they have a guarantee that the over-approximation they hold is an overapproximation of the set of fixed point, so they can use that to obtain an over-approximation of the outputs and check for robustness.\n\nExperimental evaluation is performed over MonDEQs  trained on MNIST and CIFAR10.\n\n",
            "main_review": "# Strengths\nThe paper is quite clearly written and is very self contained. Even though I did not have more than a passing knowledge of the MonDEQs model, the paper was sufficiently clear in presenting an introduction to them to give enough context. The description of their algorithm was also quite well illustrated, making it possible to follow along.\nThe contribution are quite novel. Beyond MonDEQ, the algorithm described by the author provide a template to follow when wanting to obtain bounds for an infinite process, in which case there is no possibilty to run standard methods. This is quite interesting and original, in addition to the regular contribution of enabling verification of a new type of model.\n\n\n# Weaknesses\n## Q1: Motivation of the paper.\nThe abstract of the paper claim \"As MonDEQs are inherently robust to adversarial perturbations, investigating new methods to certify their robustness is a promising research direction\". A similar claim of \"high inherent empirical robustness\" is also made in the introduction. This should be justified in some way, because they are not supported by the results visible in the paper.\nPicking from Table 1, the MonDEQs used achieve adversarial accuracy of :\n83% on MNIST, for eps=0.05\n36% on Cifar10, for eps=2/255\nLooking for example at the IBP paper (2018) Table 3, it achieved *verified accuracy* (so a stronger requirement) of \n91% on MNIST, for eps=0.3\n44% on Cifar10, for eps=2/255.\nGiven that the robustness results are this far below even simple methods, I think that the claim of \"high inherent robustness\" should be dialed down or better substantiated.\nThe fact that those models are also so far beyond the state of the art put also some limitations on how useful the algorithm is. \n\n## Q2: Guarantees of the paper\nI'm wondering about some aspects of the paper which would have implications to the applicability of the algorithm.\n- Is there a guarantee that containment will happen (so that we know that Theorem 4.1 will be applicable at some point)?\n- If containment happens, is there some guarantees that it is going to be detectable? (Theorem 5.2 says that Z' is contained if, but this is only a sufficient condition)\n\n## Q3: Clarification about choice of iteration.\nIs there some reason why the changes of operator splitting algorithm is necessary? I see in Appendix B that PR is run for a few more iterations in the actual implementation. Do the overapproximations of the operator operate differently with regards to the resulting size of the approximations? Is there some trade-offs to \nobserve? Or is it simply that Theorem 4.2 is applicable for any choice of alpha, rather than a specific parameter.\n\n## Q4: Clarification about the widening.\nThe discussion in Appendix B of \"Widening\" appears problematic, and I'm slightly uncomfortable with it being hidden only in the appendix. Is the idea to increase the size of the feasible domain at one level, and count on the image of that increase domain to be shrinking (so assuming that applying the layer is a contractive operation). Additionally increasing looseness is the opposite of how verification is done (even this paper spends a lot of effort, such as in 5.1 to obtain tighter abstractions) so I think that this point should be highlighted more and discussed more (have an ablation study for it, and be present in the main body of the paper.)\n\n## Typos:\nPage 4: At the beginning of the proof of Theorem 4.1, S_i_hat and S_i are defined to be the same. I assume the content of the second one should not have hats.\nPage 14: \"M-Z3onotop\" -> \"M-Zonotope\"\n\nSide questions:\n- In the result of Table 3, could you clarify the difference between the two \"No Box Component\" line? Does it correspond just to the fact that one is a naive version with default hyperparameters, while the other one is the same method, but with  hyperparameter tuning?\n",
            "summary_of_the_review": "The paper develops interesting techniques to deal with the particularities of a class of models and shows novel methods to solve the arising problems (infinite depth computation). It presents the algorithm in a clear manner.\nAt the same time, it appears if you read the appendix that the algorithm that they describe is a much simplified version of what they actually run and it's unclear about what are the necessary conditions for their algorithm to be successful. The robustness results obtained by MonDEQs also seem to be significantly worse than those obtained by traditional CNNs, so the applicability of the method might be limited.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}