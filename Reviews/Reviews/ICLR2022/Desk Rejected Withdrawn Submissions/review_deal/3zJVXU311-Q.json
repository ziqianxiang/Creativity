{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper uses modern Hopfield networks for dealing with tabular data.  The proposed architecture has two Hopfield modules: one for memorizing the training dataset, and the other one for embedded input features. The architecture is evaluated on small and medium size datasets,  good performance is achieved. ",
            "main_review": "This is a nice paper that explores a novel architecture that may benefit from memorizing a (small) tabular dataset. I like the overall idea, and the authors seem to report strong empirical results. Certain parts of the paper are however confusing and somewhat misleading. I will explain these issues below. \n\n1.  I understand the construction of the module $H_s$, which memories the training set. By the way, is the parameter $n$ - the size of the training set? It seems to be undefined in the manuscript. I am having troubles understanding the role of $H_f$. Specifically, what is the matrix of memories $Y$? Is it a set of learnable parameters, or does it depend on the inputs $\\Xi$ in some way? \n\n2. In equation 8, the authors state that they can use $N$ Hopfield networks inside $H_s$. What is the difference between these networks? It seems that the memory matrix $X$ is the same in all of them. Are they different because the matrices $W$ are different, or are matrices $W$ also equal in all those Hopfield networks? \n\n3. The presentation of the modern Hopfield networks is somewhat skewed toward the formulation by Ramsauer et al. and largely ignores the original formulation by Krotov and Hopfield (except for mentioning it once on page 3).  The authors have a right to work with any formulation, but they should cite the original work when they introduce the concept for the first time (page 2). The authors should also cite the paper by Krotov and Hopfield from ICLR 2021 (https://openreview.net/forum?id=X4y_10OX-hX), which provides a neat derivation of the energy function and update rules that they are using. \n\n4. I understand that the authors are focusing on small datasets (which is fine), but I am wondering about how would their approach scale with increasing the size of the dataset? For instance it seems that the size of the memory matrix X should grow linearly with the size of the training set. Even for medium size datasets, 10k instances, it’s already a sizable matrix. What is the reason why the authors do not apply their approach to large tables, say millions of instances? Is it because there is a computational reason that their architecture leads to worse accuracies than the more traditional approaches; or is it because their network doesn’t fit into the memory, but if it did it would also likely work well? \n\n5. Could the authors please provide the values of the hyperparameters, such as $e$, $d$, and $h$? It is hard to assess the scalability of their approach even qualitatively without at least a ballpark for these parameters. \n",
            "summary_of_the_review": "Overall, I like the approach and the paper in general. If the authors can answer my questions and address the issues above I am happy to increase my scores.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a deep learning approach for tabular data that uses modern Hopfield networks. This new architecture aims to generalizes better than XGBoost on tabular data with less than 10000 examples. The approach is interesting and it shows better generalization performance than XGBoost and other deep learning approaches for some datasets. The main difference between the proposed approach and the closest deep learning architectures is that the proposed solution uses the full training dataset as input to each layer of the network. In my opinion, the paper can be highly improved in terms of technical clarity and experimental results. More details are provided in the main review.",
            "main_review": "Strengths:\n+ Useful survey of the related work in the field of deep learning for tabular data\n+ First attempt to use Hopfield networks for deep learning architectures for tabular data (use the whole dataset as input to each network layer)\n+ Interesting first results when compared to XGBoost in terms of generalization performance\n\nWeaknesses:\n- Section 3 should include a top-level diagram that describes the full architecture with Hopfield modules and blocks. In the current state, it is difficult to understand how the actual proposed architecture looks like (incl. the full set of model hyper-parameters).\n- The experimental section could be highly improved to make the contributions of the paper stronger. Here are a few suggestions: \n    1) In general there is no GBDT implementation (XGBoost/LightGBM/CatBoost) that works best on all datasets. Thus, especially on these new datasets studied in the paper, the authors could also include the results for LightGBM/CatBoost (accuracy and runtime) if available. It would also be interesting to see how a differentiable neural network would compare to the proposed approach (e.g., NODE).\n    2) Please provide details about the hardware setup used for the experiments.\n    3) The hyper-parameter optimization is not clear in the paper. E.g., all the models shown in Table 1 have been tuned? What hyper-parameters have been tuned and what ranges have been used for the hyper-parameters? What tools have been used to tune the models? It is only mentioned for XGBoost, but even for XGBoost there are no further details provided.\n    4) How many train/test splits have been used? Was (nested) cross-validation used? Please provide more details. This is very important especially when dealing with small datasets, special attention is required when evaluating the model quality on such datasets. \n    5) In section 4.2, are the default hyper-parameters used or the tuned ones?\n    6) The results in Tables 2/3 show that the test accuracy on the datasets on which Hopfield wins against XGBoost is extremely close to the NPT accuracy. Could you please provide information about the training time of NPT?\n    7) At least the results in Tables 2/3 should include a statistical significance analysis (currently this is missing).\n    8) Section 4.3: what is the meaning of a step for XGBoost? Is this runtime the total runtime of the training routine? Could you please provide the runtime for all the datasets under study?\n    9) Section 4.3: please report the NPT runtime as well.\n   10) To make the paper stronger, I would increase the number of baselines (e.g., other GBDT frameworks, differentiable neural networks etc) for which I would report the test accuracy and the runtimes.",
            "summary_of_the_review": "This paper has the potential to bring a very good contribution to the field of deep learning architectures for tabular data. However the paper in the current state requires quite a few improvements especially in terms of technical clarity and experimental results. Please see the main review section for more details.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes \"Hopular\" a model for prediction on small tabular datasets based on \"modern hopfield networks\".\nModern hopfield networks, as presented in this paper, are equivalent to multihead self-attention (MHSA).\n\"Hopular\" stacks multiple Hopular blocks. \nEach Hopular block contains two hopfield modules: the first module applies MHSA between different samples. The second module applies MHSA between different features of the same sample (This is shared/parallelised over all samples).\nThe draft shows superior performance of Hopular in small-to-medium large tabular data settings.",
            "main_review": "## Strength\n\nThe introduction gives an excellent overview over recent developments in the field of machine learning models for tabular data.\nThe paper is overall well-written and, overall, a pleasant read.\n\n## Weaknesses\n\nI have two main concerns about this submission, which I detail below.\n\n### Novelty of Hopular\n\nI do not see how Hopular provides much algorithmic novelty, except for framing NPTs/SAINT in the language of modern Hopfield networks.\n\nAs the authors themselves write, modern Hopfield networks are equivalent to attention. As they do not write, Eq. 8 of this paper is, as far as I can tell, almost exactly equal to multi-head self-attention. (There is a slight difference in how skip connections are handled.)\n\nAs mentioned above, each Hopular block contains two Hopfield modules: one performing MHSA between samples, and another performing MHSA between features.\nFor the MHSA between samples, the input is reshaped from $n \\times d \\times e$ to $n \\times d \\cdot e$.\nThis is exactly identical to how SAINT and NPTs construct their basic building blocks.\n(SAINT does not explore more than one \"Hopular block\" in their paper, but Kossen et al. do in fact employ this for NPTs.)\n\nThe similarities, particularly to NPTs do not end here:\n* The input embeddings are performed identically to NPTs.\n* The masking mechanism, with stochastic masks applied to both input labels and input features, is identical.\n* The loss is a convex combination of supervised label and feature masking loss for both Hopular and NPTs. Hopular also use the cosine schedule from Kossen et al. to anneal this.\n* The same optimizer is used for Hopular and NPTs.\n* The output layer is, as far as I can tell, identical.\n* Hopular even makes use of the hyperparameters, optimisers, and training schedule used by NPTs.\n\nAs far as I can tell, Hopular and NPTs are almost functionally identical.\nUnfortunately, many of the above similarities are not mentioned by the authors. \nInstead the authors focus on two differences between Hopular and NPTs: (1) a scaling factor $\\beta$ in the self-attention mechanism, and (2) the fact that \"the original training set and the original input are provided at each layer\" in Hopular.\nGiven all all these similarities, I can't help but wonder: Are the authors perhaps building Hopular on top of the open-source NPT implementation?\n\n**(1) The Scaling Factor.** In S.3 the authors claim that experiments in S.4 confirm that a large $\\beta$ value improves predictions for Hopular. However, I cannot find any information on $\\beta$ in the experiments. Can you provide an ablation where you fix $\\beta=1$ in Hopular and show this degrades performance? So far, I am not convinced that the introduction of $\\beta$ is beneficial.\n\n\n**(2) Original Training Set/Input at each layer.** The authors claim that only Hopular has the original training set/input available at each layer. While this is technically true,  I feel that the difference to NPTs/SAINT is exaggerated.\nBoth SAINT and NPTs use MHSA with *skip connections* (cf. e.g. Eqs. (2-4) in the NPT paper).\nTherefore, while the original input is not passed to directly each layer, it is still easy for the model to preserve information from the input or to ignore layers if needed.\nAnd given that the task for NPTs/SAINT/Hopular is predicting masked values, it seems to me that the input getting iteratively refined (as done through the skip connections in NPTs/SAINT) is sensible, and I do not see inherent value for presenting the original training set/input to each layer.\nCan you provide an ablation where you use standard MHSA skip connections, showing that your \"input-skip connections\" perform better?\nHopular with MHSA style skip connections is entirely identical to NPTs, right?\n\n\n\nI would welcome a revision to this paper that more fairly represents the similarities of Hopular and prior work.\nThe current draft feels too much like it's trying to hide these similarities – especially to NPTs.\nAdditionally, clarifying any differences to NPTs (are there any in addition to skip connections + $\\beta$) would be helpful.\n\n\n### Experimental Results – Selection of Datasets and Tuning of Baselines\n\nThe authors compare Hopular for a relatively large selection of datasets and baselines.\nUnfortunately, I feel there are a variety of shortcomings that the authors should address in order to provide further credibility to their results.\nI've highlighted points of particular importance to me in bold.\n\n* **Why are LightGBM and Catboost not implemented as baselines?** Easy to use open source implementations (with SKLearn interface) for these models exist, and they perform strongly and so should be included here. (You seem to be aware that these are important baselines as well.) Both models performed highly competitively when SAINT/NPT compared against them.\n* \"For NPT we use the hyperparameters reported in (Kossen et al., 2021), which also form the basis of Hopular’s hyperparameter selection process\" -->  **This sounds like you only tune hyperparameters for Hopular but not for NPTs?** It would be unfair to tune hyperparameters for one model only and not for the other. Further, Kossen et al. use different sets of hyperparameters for different dataset sizes.  (And they sweep over a selection of configurations in the small data setting.) Which of these hyperparameters did you use? **The fact that NPTs perform rather well for the medium data setting makes me think you might have used only their medium-large hyperparameter configuration and not the 'small' configuration that they recommend for small datasets**. I also don't quite understand table A.1. Again, it sounds like these are hyperparameters searched only for Hopular?\n* In general, I appreciate the idea of selecting datasets that are \"hard\" for neural network models. However, are results over \"normal\" datasets not also important? Further, **presenting results for some of the datasets from NPTs or SAINT would provide further credibility to your results** (can you recreate their results?) as well as provide some credibility to the NPT/SAINT papers (is it possible to recreate their results)? **Given the similarities in architecture design between Hopular and NPTs, it is unclear to me why Hopular would perform so much better than NPTs.**\n* Can you demonstrate that Hopular actually relies on sample-sample attention mechanism for prediction? Or is this just learning to predict in parametric fashion, ignoring all sample-sample relations?\n* Relatedly: A lot of the motivation of the paper has to do with the 'storing capacity of modern Hopfield' networks. Can you show that the the Hopfield modules do in fact learn to store input data? Why would they learn to store input data if the training data is provided at each iteration?\n* **It seems extremely odd that XGBoost performs almost as badly as Nearest Neighbors in the small data setting.** This is entirely unlike the performance reported in the SAINT/NPT papers, where XGBoost consistently performs near the top. Again, it would be nice to include some of their datasets to confirm that your hyperparameter optimisation/code is not at fault here. Note, that for NPTs this also includes small datasets with ~<1000 datapoints. Further, it seems that your hyperparameter tuning is different for the medium sized datasets, where now XGBoost performs rather competitively.\n* Can you release details of the XGBoost hyperparameter tuning for both scenarios? You write that \" More details about the hyperparameter selection and the precise hyperparameter settings can be found in the Appendix Table A1\" but no details about XGBoost are given there.\n* All datasets you report are classification. Is there a reason for not showing results on regression datasets? I know regression datasets of matching size exist in UCI (e.g. Protein, Concrete, Boston Housing, Yacht).\n* Can you provide mean rank in addition to median rank? Can you please also provide quantiles/standard deviations of the median/mean rank? Also, please provide the individual numbers of all methods on all datasets as well.\n* The provided runtime/step statistic in Table 4 seems somewhat irrelevant for model training. I assume XGBoost and Hopular need a different number of steps for training to converge? (And I assume gradient-based Hopular needs much more steps.)\n* Do you tune hyperparameters on a validation set? This may sound obvious, but I have previously reviewed papers that did not do this, and so I would welcome you to add information about training/validation and test set sizes.\n* It would be great if you could release the code before the end of the reviewing period. It does not seem helpful to release it shortly after.\n\n\n## Small Comments\n\n* \"In real world, however, most machine learning applications face tabular data with less than 10,000 samples.\" While plausible, this claim requires a citation or evidence to be backed up. Especially if it's part of the abstract.\n* \" For small-sized and medium-sized tabular data, these methods dominate real world applications.\" While plausible, this claim requires a citation or evidence to be backed up.\n* The introduction of modern hopfield networks (with recounted theorems) is nice, but why is it necessary? (Especially if all of this is equivalent to MHSA). The main purpose it seems to serve is to illustrate the storing capacity of modern Hopfield networks. However, it is unclear to me why I should be excited about the possibility of \"storing a lot of data\". (I can also store data in an array. Are Hopulars better at storing data than an array?) Also, regarding the last sentence of S.2: what dimensionality would be needed in theory to store the datasets you experiment with? Does this match up with the results?\n* \"Problematic datasets in (Klambauer et al., 2017) are characterized by having an accuracy range of 0.5 or higher across well established methods\" What is an accuray range? Does this mean that the max - min performance of the investigated method in Klambauer et al. is  $\\ge 0.5$?\n* \"One advantage of Hopular compared to XGBoost is the capability to handle data with missing values without the need for additional imputation techniques. This makes Hopular highly relevant for real world scenarios where incomplete data is a common companion.\" --> This is somewhat of a weird closing statement for this paper, given that both NPT and SAINT can do this just as well. Please clarify how Hopular can do this better than SAINT/NPTs or remove. Missing data also is not discussed elsewhere in this paper, so I fail to see how this fits the \"conclusion\" section.\n\n\n## Language\n\n* \"it failed to meet ~~its~~ expectations on tabular data\" Deep learning does not have expectations, people have expectations *of* deep learning.\n* \"In real world, however, most machine learning applications [...]\" --> \"In *the* real word\" or \"In real-world settings\"\n* Inconsistent abbreviations for plural forms: \"GBDT\" but \"SVMs\". Why not \"GBDTs\"? (Specifically in \"GBDT typically lead to higher [...]\")\n* \" SAINT also uses self-supervised pre-training [...]. Non-Parametric Transformers (NPTs) also use feature self-attention and inter-sample attention\" --> It is not 100% clear what the two \"also\"s refer to here. I would guess, that the first refers to both TabGNN and SAINT using self-supervised pre-training? (But NPTs also have a self-supervised component.) And the second \"also\" refers to both SAINT and NPTs having two self-attention mechanisms: one between features and one between samples? Maybe it would be more clear to specify this directly?\n",
            "summary_of_the_review": "I hope the authors engage with my review during the rebuttal period.\nI look forward to a lively discussion, and hope the authors address as many of the concerns raised in above.\n\nMy main objections to accepting this paper in its current state are \n* remaining unclearness in pitching the novelty of Hopular in relation to NPTs/SAINT – Hopular is much more similar than what, I feel, the reader is led to believe. \n* that the current draft does not ablate the effect of Hopulars main novelties: changing the MHSA skip connection from layer-to-layer to input-to-layer as well as introducing the $\\beta$ scaling factor. It seems surprising to me that these changes would increase performance of Hopular over NPT by such a margin in the small data setting.\n* the open questions wrt to the experimental evalution. In particular: the missing LightGBM/CatBoost baselines, missing details for hyperparameter tuning for XGBoost, potentially wrong hyperparameter tuning / no tuning for NPTs, missing results on datasets used in SAINT/NPTs, explanation of surprisingly bad XGBoost results. A code release before the end of the review period would be great as well, given that this will erase any doubt about the procedures.\n\nIf the authors engage and clarify a significant number of my concerns, I will raise my score.\nI hope the authors engage with my review during the rebuttal period.\nI look forward to a lively discussion, and hope the authors address as many of the concerns raised in above.\n\nMy main objections to accepting this paper in it's current state are \n* remaining unclearness in pitching the novelty of Hopular in relation to NPTs/SAINT – Hopular is much more similar than what, I feel, the reader is led to believe. \n* that the current draft does not ablate the effect of Hopulars main novelties: changing the MHSA skip connection from layer-to-layer to input-to-layer as well as introducing the $\\beta$ scaling factor. It seems surprising to me that these changes would increase performance of Hopular over NPT by such a margin in the small data setting. (And as such, I am inclined to believe that the large differences to NPTs are due to differences in Hyperparameter tuning.)\n* the open questions wrt to the experimental evalution. In particular: the missing LightGBM/CatBoost baselines, missing details for hyperparameter tuning for XGBoost, potentially wrong hyperparameter tuning / no tuning for NPTs, missing results on datasets used in SAINT/NPTs, explanation of surprisingly bad XGBoost results. A code release before the end of the review period would be great as well, given that this will erase any doubt about the procedures.\n\nIf the authors engage and clarify a significant number of my concerns, I will raise my score.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents a new deep learning architecture, Hopular, for tabular dataset classification problems with a relatively small number of samples (mostly lower than 1000). These problems are commonly solved by classical machine-learning methods, but there were several recent papers on trying to solve them with deep learning. The new proposed architecture is based on a Hopfield module, which enables access to the original training data in each layer of the network. \nThe performance of the new network is evaluated on 16 datasets from a previously published paper, and compared with 26 other modeling methods. It reached the best performance with average rank of 6.75, while the next best method (SVM) had an average rank of 8.25. Next, Hopular was compared with XGBoost and NPT on 4 larger datasets (~10K samples) from another previously published paper. Hopular was again better in terms of average rank (1.5 vs. 2). Additionally, the inference time of Hopular was similar to XGBoost, even though the training time of Hopular was 20x longer.\n",
            "main_review": "This paper considers an important problem and suggests an interesting architecture. However, the experimental analysis provided raises several concerns:\n-\tSelecting only some of the datasets used in each of the two referenced papers may raise the concern that the results were not representative enough. Even though the authors had various reasons for their selection, it would be better to provide the results for the same datasets as in the referenced papers, at least for one of the papers.\n-\tThe hyperparameter tuning of the models in the first comparison is not detailed, and it is unclear which hyperparameters were tuned and whether the same number of hyperparameter iterations were done for Hopular and the other models. Rather than comparing to many models that might not be well-tuned, it would be better to compare to one or two well-tuned popular models, e.g., XGBoost and CatBoost. The tuning should include at least the key hyperparameters (e.g., in XGBoost the number of trees ranging between 10 and 1000 and their depth ranging between 1 and 20). It should perform the same number of iterations for each model, e.g., 100 or 1000 iterations with a standard Bayesian optimization package. Also, while providing ranks makes the presentation clearer, the authors should also provide the \nactual results of their runs, including standard error, as they did for the second paper, to enable a better impression of the results.\n-\tIn the second comparison, the XGBoost results of the referenced paper were not reproduced, so the validity of the results is unclear. It should again be clarified how many hyperparameter optimization iterations were done, how many trees, which depths, etc. \n",
            "summary_of_the_review": "This paper presents interesting technical contributions for an important problem. However, the experimental evaluation was not done rigorously enough to support the main conclusion. It is unclear if different models had a different number of hyperparameter tuning iterations, and if the tuned hyperparameters and ranges were the most important ones. I therefore recommend rejecting this paper in its present form. I would be willing to reconsider if the authors provide the required data along the lines described above (even if the updated results would show a smaller improvement, its significance and soundness could be higher).",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}