{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "\u000eThe paper is concerned with efficient optimization of variational families that consist of (mixtures of) Gaussians with full rank covariances. Specifically, the paper proposes a modification of the VIPS method of Arenz et al. (2018, 2020) to incorporate first-order (i.e., gradient) information. Experiments on logistic regression, a robotics problem, and a synthetic Gaussian mixture distribution demonstrate incorporating first-order information can speed up convergence dramatically and in some cases make the method competitive with second-order optimizers. ",
            "main_review": "Overall I found the paper to be well-motivated, though methodologically the contribution is fairly incremental. That said, using first-order information (unsuperprisingly) appears to make a major difference in practice. Hence, I found it to me a worthwhile contributions. As such, my comments are, for the most part, quite minor:\n\n1. I got completely lost in the deluge of acronyms, most of which are non-standard (that not widely used ones like MCMC, VB). Please keep only ~2 non-standard acronyms that are most critical and spell the rest out. The space-savings from using the acronyms is not worth the large mental load you place on the reader. You can always use the acronyms in figures as long as they are defined in the captions. \n\n2. In the abstract and introduction, please add mentions of the fact that the proposed method works on low-to-moderate–dimensional problems but is not suited for high-dimensional problems. \n\n3. The log-scale negative ELBO plots (e.g., Figs. 3 and 4) need to be replaced since they are based on an arbitrary constant offset. This offset only makes sense if it turns the negative ELBO into the KL divergence (that is, the offset is equal to the log marginal likelihood). Otherwise zero is not meaningful, so the the log scale in inappropriate and overemphasizes small differences in the ELBO. \n\n4. The HMC comparison should be to Stan’s implementation of dynamics HMC, which, while building upon the ideas in NUTS, is not the same algorithm. Moreover, in my experience, even the warm-up samples from Stan can be quite good, even on high-dimensional distribution. So the evaluation should allow the warm-up to be used to make the comparison fair. (Note these samples can easily be obtained through RStan.) \n\n5. Additional small items: **(a)** The citation of Arridge et al. (2018) in the introduction is a little unusual. I would have expected earlier work like [1] and [2] to be cited instead. **(b)** Top of page 4: I think $\\beta$ should be $\\omega$. **(c)** Table 1 should be made into a figure.\n\n[1] A. Kucukelbir, R. Ranganath, A. Gelman, and D. Blei. Automatic variational inference in stan. In Advances in Neural Information Processing Systems 28, pages 568–576\n\n[2] M. Titsias and M. Lázaro-Gredilla. Doubly stochastic variational bayes for non-conjugate inference. In Proceedings of the 31st International Conference on Machine Learning, Volume 32 of Proceedings of Machine Learning Research, pages 1971–1979, Bejing, China, 22–24 Jun 2014. PMLR. \n\n",
            "summary_of_the_review": "An incremental paper that nonetheless advances the state-of-the-art for first-order optimization of (mixtures of) Gaussian(s) variational families in moderate-dimensional problems.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes an improvement on the existing MORE method for approximate Bayesian inference.\nMORE fits a quadratic surrogate on the target function using samples from the variational distribution, this work proposes to also use the gradient information from the target.\nThanks to this it provides unbiased estimates of the Hessian distribution\nSince MORE can be used on more complex inference methods such as \"VIPS\", the paper shows the effect of this addition on those as well.\n",
            "main_review": "The use of gradient information is a known but very interesting idea in this context and the example given in Figure 1 is quite convincing.\nGenerally, the idea of obtaining an unbiased estimate of the Hessian using only gradients is a great contribution but unfortunately, it is poorly realized in this paper.\n\nFirst, the general scope of the paper is not clear and the claims made in the abstract and title do not seem to be fully satisfied:\nOne of the emphases seems to be about natural gradient updates. It seems that MORE already provides natural gradient updates, however, it is not clear if the modified gMORE does the same and what its improvements correspond to.\nAnother argument in the abstract is that gMORE provides high-quality approximations of the Hessian, this is at no point evaluated, proved, or shown empirically.\nGenerally, if the objective is to show that gMORE provides a good estimator for the Hessian, why is there no theoretical or empirical analysis on this?\nJust showing on two datasets that the ELBO is the lowest of all methods considered is entirely insufficient.\n\nAnother point is VIPS. I do not understand the reason to bring this method in this context. It does not bring any additional information concerning the efficiency of the method itself.\nThe introduction to the method is also very confusing for people not familiar with it already: Writing the derivations of the bound in equation (5) in the opposite order (from original bound to lower bound) would already be much more helpful to understand what is happening.\nFor the sake of understanding and keeping the story clean, I would get rid of this part entirely, this would also give more space for the missing analysis mentioned previously.\n\nAdditional comments: \n- Could you comment on the similarity of MORE and Unscented Kalman Filtering?\n- Figures are far too small, the legends and axis are barely readable.\n- Using a low-rank structure to allow working with more dimensions sounds like a promising approach, I would be looking forward to seeing progress in this area.\n\nMinor comments:\n- You seem to switch between $\\beta$ in equation (3) and $\\omega$ in equation (4).\n- There is a missing parenthesis for $H(q(x|o))$ at equation (5)",
            "summary_of_the_review": "The idea proposed in the paper is very interesting, but a lot of questions remained unanswered and therefore the paper feels very incomplete.\nThere is no analysis or explanation of the natural gradient aspect of the method when it is presented as one of the most important points of the paper.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this work, the authors propose an improved version of MORE and VIPS by using gradient information.\nThe authors test the proposed method in several examples considered in VIPS and compare the proposed method against some existing methods.\nThe authors mainly consider the following two lines of work. The main idea is to combine these two lines of work.\n* The first line of work is MORE and its extensions such as VIPS. MORE is designed for Gaussian approximation while VIPS is proposed for Gaussian mixture approximation.\n* The second line of work is VON and its extensions such as VOGN and GM.   VON, VOGN, and GM are **only** designed for Gaussian approximation. ",
            "main_review": "## Strengths:\n\nThe main technical contribution of this work is to enable MORE and VIPS  to use gradient information. \nIt could be useful for medium-scaled posterior inference problems since the proposed method can use a finite mixture of Gaussians as a flexible approximation.\n\n## Weaknesses:\n\n\n### 1. The abstract should be re-worked. \nThere are too many acronyms in the abstract. The authors even do not define all acronyms. For example, I had no idea of the meaning of VIPS if I only read the abstract. The authors should improve the abstract.\n\n### 2.  Sec 1 should be improved. \nIt is difficult for readers to understand Sec 1 since the authors list too many methods without properly describing them.\nReaders cannot tell the difference between VON and VOGN/GM since the authors do not define VON.   \nVON is originally proposed in [1].\nNote that VON uses an **unbiased second-order** estimator while VOGN and GM use **biased first-order** estimators for the covariant matrix.\n\n\n### 3. Positive-definite constraint in eq 4 (the key technical issue)\n* The authors should discuss why the **covariance matrix** in Eq 4 is guaranteed to be **positive-definite**. To update the covariance matrix, the R matrix is used according to Eq4.  As mentioned in Sec 2.1.1, the R matrix is just symmetric and is updated according to Eq 6.  \nIt is unclear why the updated covariance matrix in Eq 4 is positive-definite due to the unclear presentation.\nThe authors should discuss this point since the **proposed method** also suffers from the same issue. \n\n* VON has to use a line search algorithm to select a step-size to handle this issue as mentioned in [1]. However, this line search could lead to a very small step-size and greatly slow down the optimization procedure as reported in [5]. It is likely that  **MORE and VIPS** also have to select a small step-size to handle this issue.\nGM and VOGN are proposed to address this positive-definite issue. However, GM and VOGN are not unbiased estimators.\n\n\n### 4. Missing some important related  works\n* Tran et al 2019 [2] consider this positive-definite issue and propose to use a retraction map originally designed for positive-definite matrices.\n\n* Salimbeni et al 2018 [3] suggest using a Cholesky factor instead of the covariance matrix to handle this issue and propose a natural-gradient method.\n\n* Lin et al 2020 [5] also consider this issue in more general settings.  Lin et al 2020 show that the retraction map for positive-definite matrices can be obtained from the geometry of a Gaussian distribution in a systematic way.\n\n* Moreover,  Lin et al 2020 [5] show that an **unbiased first-order** estimator for the covariance matrix can be used to replace the second-order estimator in VON. This unbiased first-order estimator is known as the **re-parametrizable gradient**  for the full covariance matrix (see Theorem 4 in [4]). In other words,  the method in [5], which is an extension of VON, can use unbiased first-order information instead of second-order information to update the covariance matrix. Thus, [5]  is related to gMORE.\n\n* VON can not be directly applied to Gaussian mixture cases since the Fisher information matrix of the Gaussian mixture is not well-defined. Lin et al 2019 [6] extend VON to **Gaussian mixtures** and establish the connection to **natural gradient descent**. \nLin et al 2020 [5] address the positive-definite issue in Gaussian mixture cases and propose an unbiased first-order estimator in Gaussian mixture cases. Thus, [5] and [6] are related to gVIPS and vonVIPS.\n\n\n* Last but not least, Lin et al 2021 [7] propose natural-gradient methods for a flexible class of **structured Gaussians and structured Gaussian mixtures** beyond the low-rank-plus-diagonal covariance structures. Lin et al 2021 also unify some **zero-order, first-order, second-order** methods in evolution strategies/gradient-free search, variational inference, and numerical optimization.\n\n### 5. No justification of the main contribution \nReaders wonder how the authors come out with this loss function defined in Sec 3.2. The authors should justify this loss function and tell readers how the proposed method relates to natural gradient descent.\n\n### 6. Unbiasedness of the proposed method\nThe authors mention that MORE is unbiased in Sec 1.\nHowever, it is unclear whether the proposed method is unbiased or not since the proposed method is clearly different from MORE. \nI wonder whether MORE and VIPS are unbiased when the ridge regression is used as mentioned in Sec 3.1 and Sec 3.2.\n\n\n### 7. Additional tuning parameter for the ridge regression\nIn Sec 3.2, an additional tuning parameter for the ridge regression is included. The authors should discuss how to select this parameter since the selection can affect the performance of the proposed method in practice.\n \n\n### 8. Unfair comparison for VON (the key empirical weakness)\n* The authors consider **only experiments** from the VIPS paper, where the tuning parameter for the ridge regression and the step-size could be well-tuned. On the other hand, readers have no idea whether the step-size for VON is fairly selected. The authors should also consider other experiments.\n\n* No VON-type method is considered in the **GMM experiments**. The method in [5] is an extension of VON and is suitable for Gaussian mixtures.  Moreover, Lin et al 2020 [5] consider fitting a 300-dimensional mixture of Student's T distributions with 20 components, which is **more challenging** than the 100-dimensional Target GMM  with 10 components considered in this work.  \n The authors should include the method [5] in the GMM experiments as a baseline.\n\n* As shown in [5], the positive-definite issue can greatly slow down the training process in the example of mixtures of  Student's T distributions. The authors should consider fitting a mixture of  Student's T distributions using the proposed method and discuss whether the positive-definite issue slows down the training process of the proposed method (e.g., using a very small step-size due to this issue).\n\n\n### 9. VON does not appear in Figure 4 \nVON is mentioned in the caption of Figure 4 while VON does not appear in Figure 4. Please note that VON and vogVIPS are different methods.\nAs mentioned before, VON is designed only for Gaussians while VIPS is designed for Gaussian mixtures.\n\n\n\n\n## References:\n\n[1]  Khan, Mohammad, and Wu Lin. \"Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models.\" Artificial Intelligence and Statistics.  2017.\n\n[2] Tran, Minh-Ngoc, Dang H. Nguyen, and Duy Nguyen. \"Variational Bayes on manifolds.\" arXiv preprint arXiv:1908.03097 (2019).\n\n[3] Salimbeni, Hugh, Stefanos Eleftheriadis, and James Hensman. \"Natural gradients in practice: Non-conjugate variational inference in Gaussian process models.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2018.\n\n[4] Lin, Wu, Mohammad Emtiyaz Khan, and Mark Schmidt. \"Stein's Lemma for the Reparameterization Trick with Exponential Family Mixtures.\" arXiv preprint arXiv:1910.13398 (2019).\n\n[5] Lin, Wu, Mark Schmidt, and Mohammad Emtiyaz Khan. \"Handling the positive-definite constraint in the bayesian learning rule.\" ICML 2020\n\n[6]  Lin, Wu, Mohammad Emtiyaz Khan, and Mark Schmidt. \"Fast and simple natural-gradient variational inference with mixture of exponential-family approximations.\" ICML 2019.\n\n[7] Lin, Wu, et al. \"Tractable structured natural gradient descent using local parameterizations.\" ICML 2021\n",
            "summary_of_the_review": "The presentation should be greatly improved. The current version is not clear. The authors fail to convince readers why the proposed method is valid due to the unclear presentation. (see pointer 3&5) Important related works are missing. It is hard for readers to understand the significance of this work if the authors do not discuss these missing related works (see pointer 4)\n\nThe technical contribution in the current version may be too limited. The authors should further explore this direction such as including a discussion about the unbiasedness of the proposed method, connection to natural-gradient descent, and the positive-definite issue.\n\nThe empirical significance in the current version is also limited since **all experiments** considered in this work are from the VIPS paper.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}