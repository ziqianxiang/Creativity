{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper explores a modality-shared contrastive language-image pre-training framework, adopting a framework with a transformer model shared across modalities. The framework is motivated by the idea that sharing parameters can pull semantically-similar concepts from different modalities closer and facilitate the learning of common semantic structures. The overall framework consists of modality-specific layers, modality-shared transformer block and projection layers, and it is trained by contrastive loss. The proposed framework is trained upon the YFCC100M dataset and it is shown to outperform OpenAI CLIP on zero-shot recognition while using a smaller amount of model parameters. ",
            "main_review": "Pros:\n- The idea of this paper is simple and effective. By using modality-sharing layers, the authors propose a new way to facilitate the learning of common semantic structures across modalities.\n- The proposed framework is shown empirically with better performance as compared to CLIP while using fewer model parameters. \n- The paper provides some good intuitions of using sharing mechanisms across modalities and shows meaningful visualization (Figure 4) where the attention layers automatically illustrate the semantic correspondence across modalities. \n\nCons:\n- The main idea of the paper is largely inspired and built upon the existing model CLIP from OpenAI. This makes its overall novelty marginal. \n- While the overall modeling idea is simple yet effective, the evaluation of MS-CLIP is relatively weak. In general, a self-supervised learning framework is expected to test upon different downstream tasks to suggest its good generalizability, but the main evaluation is conducted on merely zero-shot recognition. As the proposed framework can easily perform image-text matching, it is suggested to test upon a richer set of downstream tasks to see if MS-CLIP could help to improve the model performance on tasks such as image-text retrieval. \n- Although the model is proposed based on contrastive learning with some overlap with CLIP, the core idea is very similar to other existing works in vision and language built upon BERT such as Vilbert [a] and Lxmert [b], where two modalities are also fused with transformer blocks and the model is trained in an unsupervised manner. In this regard, the overall idea of this paper is not new, and more discussion and comparison should be conducted to illustrate the uniqueness of the proposed framework as compared to similar works like [a] and [b].\n\n[a] Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks\n\n[b] Lxmert: Learning cross-modality encoder representations from transformers",
            "summary_of_the_review": "The idea of this paper is simple and easy to follow. The main idea has marginal novelty as compared to the recent CLIP model. However, the evaluation of the MS-CLIP framework is relatively weak as it is not tested sufficiently on different downstream tasks to illustrate its good generalizability. Moreover, the idea of using sharing attention layers is quite similar to existing literature that performs self-supervised learning upon a vision-language BERT framework. Therefore, it is suggested to add more evaluation on more downstream tasks (e.g. on image text retrieval) and add more discussion to highlight the difference compared to other related works in the field. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Although the paper did not provide any discussion regarding ethics, the overall idea and evaluation do not violate the ethics concerns as stated above. ",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper the authors proposed a model, called Modality-Shared Contrastive Language-Image Pre-training framework (MS-CLIP). This model is modified from the Contrastive Language-Image Pre-training (CLIP) method by introducing parameter sharing across modalities. The paper investigated two lightweight modality-specific auxiliary modules, early specialization and efficient parallel branch and how they would affect the performance of MS-CLIP. Moreover, the paper investigated the influence of parameter sharing for multi-model semantic  structures.\n",
            "main_review": "Strengths:\nThe idea to utilize parameter sharing of modalities to improve the performance of models is novel. Parameter sharing received a lot of attention in multi-modal learning tasks, and it is reasonable to research introducing this approach into vision-language tasks. The paper provided sufficient experiments. The baseline used in this paper is CLIP, which is the state-of-the-art approach in the vision-language field. Comparative experiments covered all approaches which were introduced in the method part that may improve the performance. The experiments also covered Zero-shot recognition, Ablation study and Linear probe.\n\nWeaknesses:\n1. Methods: \nThe first proposed model, MS-CLIP, does not differ much from the original CLIP. They just changed the size of the hidden dimension of the text transformer to the size of the vision transformer, so that two encoders can be shared. The majority of the model design is still the original CLIP.\n\nEarly specialization proposed in this paper as one of the lightweight modality-specific auxiliary modules, is not a novel design. Here the paper just adds residual blocks to Xiao’s work, “Early convolutions help transformers see better”.\n\n2. Experiments:\nIn section 3.1, the paper mentioned that their filtered YFCC dataset has 7 million more data pairs than OPENAI CLIP, because they used a different English dictionary to exclude non-English words. This dataset was used to train CLIP and MS-CLIP-S for zero-shot recognition. It is reasonable to doubt the performance increase is related to the produced dataset. Additional experiments on the YFCC dataset produced by OPENAI CLIP is needed.\n\n3. Formatting:\nThe paper is poorly structured. Section 2, 3, 4 is messy. In the method section there are a large amount of descriptions about related work and experiments. Besides, in the experiment section the authors introduced their MS-CLIP-S model. My suggestion is to put all experimental settings, model structure details, experiment results into the experiment section, and put more details about the structure of MS-CLIP-S into the method section.\n\n4. Typos:\nPage 1 paragraph 2, “a separate transformer()”, missing citation.\nPage 2 second last paragraph, “Fig. 1a”, but there is only Figure 1 in this paper.\n\nIn sum, the paper is below the bar for ICLR.",
            "summary_of_the_review": "There is still some space to improve for this paper. For the proposed method, the authors need to clarify the structure and details of their proposed model. The zero-shot experiment is not convincing enough, and experiments on the YFCC dataset produced by OPENAI CLIP is needed. The formatting of this manuscript needs a revision. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper studies the problem of visual representation pretraining using text as weak supervision. It builds on top of CLIP but proposes to use a single, and mostly shared transformer model to encode both the visual and textual data. The paper also introduces several lightweight components to learn modality-specific knowledge. The proposed method, MS-CLIP-S, is evaluated on 20 visual recognition tasks, by either linear probing or zero-shot learning. The paper also performs experiments analysing the common semantic knowledge learned by the shared feature encoder.",
            "main_review": "Strengths\n\n[S1] The paper demonstrates that it is possible to learn visual and textual representation using a mostly shared transformer model, which is very interesting.\n\n[S2] The zero-shot evaluation on ImageNet1k is also very encouraging.\n\n[S3] I also like the experiments on analysing how many transformer layers should be shared and how much common semantic knowledge is learned by the shared encoder.\n\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nWeaknesses\n\nThe paper seems to indicate that the shared encoder works better than modality-specific encoders because it can better capture the common semantic structures shared by the vision and language data. At this point, I’m not sure if this is the case. In short, I wonder if it is because MS-CLIP-S (i) reduces overfitting and (ii) incorporates multi-scale image features.\n\n[C1] About overfitting:\n\nAll the experiments were performed on ViT-B/32. ViT-B/32 has the same amount of parameters as ViT-B/16 and ViT-B/8, but is trained with much less information (e.g. for each 224 x 224 image, only 49 tokens for ViT-B/32 instead of 49 x 16 for ViT-B/8 or ViT-S/8) ,  and pruned to overfitting.\n\nCLIP (ViT-B/32, T768) underperforms CLIP (ViT-B/32), which also indicates a potential overfitting problem.\n\n[C2] About multi-scale image features\n\nMS-CLIP, the model without the modality-specific components, performs on par with the vanilla CLIP when ViT-B/32 is used. The modality-specific components use multi-scale image features. I wonder if MS-CLIP-S outperforms the vanilla CLIP because it is trained with these multi-scale image features, or smaller patches. \n\nI think these concerns can be resolved by answering:  would the conclusions listed in the paper still hold for ViT-B/8 or ViT-S/8 .\n\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nOther concerns:\n\n[C3] It is not very easy to follow the designs of the early specification and parallel branch modules. Details/elaborations should be included, at least in the supplemental materials,  for example, what does the “16 x 16”, “8 x 8”, etc mean in Table 2, what are the resolutions of the feature maps in the parallel CNN branch.\n\n[C4] Models were trained for only ONE epoch for all the linear probing evaluations, which are not convincing, especially for ImageNet1k. For ImageNet1k, please train for at least 100 epochs!\n\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nMinor comments:\n\n[M1] The title on openreview (MA-CLIP: Towards Modality-Agnostic …) does not match the title in the paper (MS-CLIP: Modality-Shared …)\n\n[M2] Page 1, empty citation? “a separate transformer ()”\n\n[M3] Abstract: “MS-CLIP outperforms OpenAI CLIP”, maybe just put it as “MS-CLIP outperforms the vanilla CLIP”? As at first glance, it may sound like MS-CLIP was compared to the OpenAI pretrained models, which is not the case.\n\n",
            "summary_of_the_review": "In general, I think this is a good paper. I would vote for accepting the paper even solely for [S1] (or maybe condition on if the linear evaluation on ImageNet1k is trained for at least 100 epochs:-)). \n\nOn the other hand, I think the readers would be more interested in understanding why MS-CLIP-S performs better than the vanilla CLIP. Whether it learns the common semantic concepts better? Or reduces overfitting? Or uses multi-scale features/patches? Or for any other reasons?\n\nExperiments on MS-CLIP-S + ViT-B/8 (or ViT-S/8) can answer most of the questions I listed in the “weaknesses”.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}