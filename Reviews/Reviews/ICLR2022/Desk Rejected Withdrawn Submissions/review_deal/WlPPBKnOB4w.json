{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes one stage autoencoders (OSAs) to induce searching for patterns while training artificial neural networks.   \nAnd both Autoclave Restricted Boltzmann Machines (ACRBMs) and Local Observer Convolution (LOC) are designed.   \nComparision with baseline AE and k-means are conducted. ",
            "main_review": "The organization and writing make this paper hard to read and follow.   \n\nIt is unclear why the proposed ACRBMs and LOC are novel and important. Also, comparing Figure 2 and Figure 3, it seems the only difference between RBMs and ACRBMs is $b_v$. The motivation of LOC is not clear.   \nWhat is the relationship between ACRBMs and LOC? Why propose two different models?   \n\nSome problem formulations are confusing. Eq. (13) and Eq. (14) are the solutions to ACRBMs. But the parameters of ACRBMs should be $w, b_h$, while in Eq. (13) and (14), the parameters are $\\eta$.   \n\nThe mathematical equations are not strictly formulated and organized. For example, in Eq. (17), lacking $w_i \\in S$. There is no punctuation after most equations. \n\nThe authors claimed theoretically RBMs have several advantages over AEs. But what are the theories? This is unclear. No theorem is given. Eq. (12) assumed $h^Th=I$ but Eq. (16) assumed $D$ should be diagonal. What are the differences and motivations?\n\nFigure 5 and Figure 6 show totally different results. This seems confusing.  \n\nExperimentally, very basic AEs and K-means are compared. This can not verify the effectiveness and applicability of the proposed methods. \n\n",
            "summary_of_the_review": "Overall, I am not convinced by the novelty of the proposed method.   \nThe writing and organization are kind of hard to follow.  \nExperimentally, the effectiveness is not sufficiently shown.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes One-Stage Autoencoders (OSAs): a variation of Restricted Boltzmann Machines with a simplified decoding method that allows the optimization of its parameters to rely on an optimization of the latent variable, instead of the usual reconstruction loss. OSAs are evaluated on two tasks: clustering MNIST digits and segmentation on a dataset of flowers.",
            "main_review": "## Strengths\n- Proposed methods exhibit several convenient properties besides auto-encoding such as serving as non-parametric clustering and being compatible with artificial neural networks.\n- Evaluation of OSAs has been conducted on two different problems (image clustering and image segmentation)\n\n## Weaknesses\n- Critical parts of the model are not thoroughly explained e.g., it is not clear how ACRBMs are optimized (how to arrive at Equations 13 and 14), or how is orthogonality of the most dissimilar samples satisfied.\n- Section 2.2 is not described well, such that the merits of the proposed contributions cannot be judged. \n- Experimental results are poorly described, and rely on weak baselines.\n- There is no review of related work.\n\n\n### Detailed comments\n\n> we believe that LOC is the first neural network compatible algorithm capable of dynamically choosing the appropriate number of clusters that best fit a given domain.\n\n*Self-Organizing Maps can be seen as an adaptive, ANN-compatible model to do clustering [7]. Moreover, although not based on neural-networks, FINCH [5] is an adaptive, non-parametric clustering algorithm too.*\n\n> To make searching for a latent vector computationally possible, the searching space must be finite\n\n*What is meant by a finite search-space? The latent spaces that AEs rely on (or that they approximate) are infinite.*\n\n> In conclusion, it can be considered that AEs outperformed RBMs in terms of unlimited exploration, nevertheless, the trainable decoder, makes the trade-off between the exploration and exploitation out of control.\n\n*How exactly is the tradeoff \"out of control\"?*\n\n- Equation 12: why is it valid for ACRBMs to assume that the hidden state h is semi-orthogonal?\n\n- Equation 13 and 14: what are u_i, y_j, n and eta?\n\n> The dominator is a minimization over joint probability summation.\n\n*denominator?*\n\n- Section 2.2: contributions in this section are not described in sufficient detail to judge their merit.\n\n- Section 3.1: what is the difference between ACRBM and AE-based ACRBM? This is the first time that the latter is introduced.\n\n- Section 3.1: what is the input to the clustering algorithm? Is it the reconstructed image or the hidden state? Is it consistent across baselines (e.g., whenever AEs are used, the input to k-means and ACRBM is is always the reconstructed image)?\n\n- Section 3.1: the baselines are too simple to claim any practical benefits for ACRBMs. The dataset (MNIST) is known to be easily separable even in input space (R^784), and therefore an evaluation with other datasets like USPS [4], FashionMNIST [3], SVHN [5], etc., would provide more compelling evidence for the effectiveness of the proposed method. Furthermore, relatively similar alternatives like ASPC-DA [1] and N2D+UMAP [2] already outperform the best reported results of this paper.\n\n- Section 3.1: When evaluating clustering quality, other metrics such as the normalized mutual information (NMI) should be reported as well.\n\n- Figure 8: what kind of projection was used? T-SNE, PCA?\n\n- Figure 9: The x-axis on both plots is unclear; there are points whose values along the x-axis cannot be determined.\n\n- Section 3.2: having k-means as a baseline, without even considering other methods such as the one proposed on the paper that references the Flowers dataset (Nilsback & Zisserman 2010), does not constitute compelling evidence that the method works or offers any practical advantages.\n\n- Section 3: for both baselines where one of the advantages promoted in the paper is the automatic selection of the number of clusters, a comparison to a similar non-parametric method e.g., FINCH [6] provides a stronger evidence for the usefulness of the proposed approach.\n\n### References\n[1] Guo, Xifeng, et al. \"Adaptive self-paced deep clustering with data augmentation.\" IEEE Transactions on Knowledge and Data Engineering 32.9 (2019): 1680-1693.\n\n[2] McConville, Ryan, et al. \"N2D:(Not Too) Deep Clustering via Clustering the Local Manifold of an Autoencoded Embedding.\" 2020 25th International Conference on Pattern Recognition (ICPR). IEEE Computer Society, 2021.\n\n[3] Xiao, Han, Kashif Rasul, and Roland Vollgraf. \"Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms.\" arXiv preprint arXiv:1708.07747 (2017).\n\n[4] Hull, Jonathan J. \"A database for handwritten text recognition research.\" IEEE Transactions on pattern analysis and machine intelligence 16.5 (1994): 550-554.\n\n[5] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y. Ng Reading Digits in Natural Images with Unsupervised Feature Learning NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011\n\n[6] Sarfraz, Saquib, Vivek Sharma, and Rainer Stiefelhagen. \"Efficient parameter-free clustering using first neighbor relations.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.\n\n[7] Hazan, Hananel, et al. \"Unsupervised learning with self-organizing spiking neural networks.\" 2018 International Joint Conference on Neural Networks (IJCNN). IEEE, 2018.",
            "summary_of_the_review": "The contributions of this paper are not explained in enough detail. Furthermore, the empirical results are based on weak and incomplete baselines which are not even on par with state-of-the-art.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper investigates the implicit constraints on the representation of Autoencoder and RBM and proposes two frameworks named Autoclave Restricted Boltzmann Machines (ACRBMs) and Local Observer Convolution (LOC). The comparisons with K-means on several unsupervised learning problems such as clustering and image segmentation show that the proposed frameworks gain better performance.  ",
            "main_review": "1) The introduction is a little bit messy, and the motivation is not clear to me. The authors point out that the autoencoder with weak constraints on the latent code suffers from the infinite exploration-exploitation. However, no further evidence/analysis/references are provided. Furthermore, after describing the AE and then going to the comparison with RBM, it needs a better transition and needs more explanations on why combining the autoencoder and RBM (ACRBMs) would solve the problem.\n\n2) The paper claims that the LOC can dynamically choose the appropriate number of clusters; however, after reading the paper, it is still not clear to me how the LOC can do that and I think more explanations are needed to justify this claim.\n\n2) Figure 5 says that “a larger latent dimension size” is preferred; while Figure 6 concludes that “a reasonably less dimensionality is preferred”. These two conclusions are not consistent with each other and cause confusion.\n\n3) When conducting experiments (Table 1), what will happen for “AE+K-mean with #30 clusters”? What will happen for “AE+K-mean when AE has a much smaller/larger latent code dimension (the current one is 100 according to Appendix)”? I suppose when we use different latent code dimensions for AE, the performance of “AE+K-mean” may also be different (is there a chance it will outperform the proposed method)? Also, will the proposed method outperform other more advanced DNN-based clustering methods?\n\n4) The Ref[1] also focuses on the implicit regularization of the latent code, will the proposed method outperform it? I think it is worth doing some comparisons with it.\n\n5) For the segmentation task, will the proposed method outperform other more advanced segmentation models (e.g., the UNET[2])?\n\n6) Minor issue: there are two places that have minor issues on the format of references----(1) “… to undefined exploration behavior or random walks Radhakrishnan et al. (2018)...”; (2) “… still suffer from the lack of self-consistency problem Cemgil el al. (2020)...”\n\n\nRef: \n\n[1] Jing, Li, and Jure Zbontar. \"Implicit Rank-Minimizing Autoencoder.\" Advances in Neural Information Processing Systems 33 (2020).\n\n[2] Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015.\n",
            "summary_of_the_review": "Based on the issues and concerns I have listed above, I, at this moment, would give it a score of “3: reject, not good enough”. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work introduces two new methods: Autoclave Restricted Boltzmann Machines (ACRBMs) and Local Observer Con-\nvolution (LOC).  The work introduces ACRBMs as an extension of Restricted Boltzmann Machines, trainable through backpropagation. The main novelty of ACRBMS is that does not require a reconstruction loss (as opposed to auto-encoders). LOC is a density estimation algorithm. There is no particular link between the two methods.\n\nThe two methods are compared against K-means in two different tasks: ACRMS is assessed in the clustering of the MNIST dataset. LOC is evaluated in unsupervised multi-class segmentation using the flowers dataset.\n\n ",
            "main_review": "Strengths: \nThe paper is not clear and has not read a sufficiently mature stage that allows to identify its strenghts. A thorough revision of it is adviced before it can be assessed by peers.\n\nWeaknesses\n- The paper is poorly written. It lacks structure (e.g. the introduction is very long and mixes elements from the motivation, preliminaries and nomenclature, and a discussion on the pros and cons of the base methods used), it is not well-written (e.g. poor use of English, with multiple errors) and the math notation is confusing (e.g. re-use of the same variable to represent different things).\n- The motivations are not clear. \"The exploit-explore trade-off\" in auto-encoders is not an established issue. This should be better explained or elaborated\n- How LOC is able to identify automatically the number of clusters is not properly presented, despite being introduced as one of the key aspects of this work.\n- The reasons behind presenting the two methods jointly is not well justified. perhaps, the authors should consider splitting the work into 2 different papers.\n- Experiments fail to illustrate the power of the proposed methods\n\nOther comments\n- The methods section fails to clearly explain the developed work. The terms and elements that constitute the framework are not properly introduced (e.g. fake observers, games, link with GANs, among many other).\n- There seems to be some flaws in the mathematical formulations (e.g. if u is defined as the identity matrix, what is the point of Eq. 13? It is a simple scaling of the diagnonal by the number n of the elements in a row/column) and some steps in the derivations are not properly justified (e.g. deriving eq. 15 from 14)",
            "summary_of_the_review": "This paper fails to motivate the methods that presents, to properly explain them and illustrate their properties. Overall, it lacks rigurosity, it is poorly written and lacks structure. After reading the paper, it is difficult to understand in which way the two presented methods are a \"one-stage autoencoder\" or multi-domain. This never is justified in the paper.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}