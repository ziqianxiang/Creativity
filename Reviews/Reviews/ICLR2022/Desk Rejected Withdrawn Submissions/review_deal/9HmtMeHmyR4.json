{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes learning to solve the Rubik's cube by randomly scrambling the cube and using a deep neural network to predict the random actions. The authors argue that, if optimal actions are more likely than random ones, that this method will be biased towards finding optimal solutions. When searching for a solution, the network predicts an action and the opposite action is taken, as the network predicts actions taken when scrambling from the solved state and now wants to return to the solved state.\n\nWhen solving a cube, the authors use beam search and keep states based on the move probability. They experiment with keeping 10, 100, and 1000 states at each depth in the beam search. The authors use a dataset of 1000 cubes scrambled 100 times to test their method. The results show that, as the number of states keeping more states during beam search leads to shorter paths with the shortest average path being 20.5.",
            "main_review": "Strengths:\n\nThe authors propose a method that only relies on a random sequence of random reverse moves. This assumes access to both a forward model (for solving) and a backward model, for starting from the goal and generating random moves. Given a fixed dataset, the time taken to train does not depend on the size of the action space as it does with approximate value iteration. While Q-learning mitigates this problem, if the argument about the assumption of optimal actions is valid, then this method does not need to worry about exploring different actions as Q-learning does.\n\nWeaknesses:\n\nThe evaluation is poor and useful data from related work that is freely available is not used.\n\nThe search algorithm proposed, beam search, which the authors call \"breadth-first search\", is not complete. Meaning that, if there is a solution, it is not guaranteed to find it. This is because a finite number of states are kept at every iteration. On the other hand, A* search, which was used in related work [1], is a complete algorithm.\n\nHow much this theoretical property of completeness matters in this application cannot be clearly evaluated as the authors evaluate on cubes only scrambled 100 times while the latest work uses cubes scrambled 1,000 to 10,000 times [1]. Since the authors do not compute the cost of a shortest path, we cannot be sure how close their method comes to finding one. However, the dataset which contains the states scrambled 1,000 to 10,000 times along with the cost of a shortest path is freely available [2,3]. However, the authors do not use it here. Perhaps this is because the authors use a slightly different action space. Nonetheless, the code used to compute a shortest path for this action space is also freely available [4].\n\nThe authors erroneously claim \"Since an estimated 98% of all possible states take 17–20 moves to be solved at shortest (Rokicki et al., 2014), the trained DNN can be considered a near-optimal solver as hypothesized, even before full convergence in training and with a compromised search algorithm.\" They sampled states by scrambling the cube 100 times which is not the same as randomly sampling states from the state space.\n\nAnother shortcoming of the evaluation is the authors do not say how much time their method took to find a solution. On a related note, what is the average depth of the beam search?\n\nIn summary, to understand if what the authors are proposing is practical in this case, the authors should test this method on data where the cost of a shortest path can be computed so that we can understand how complex the solutions they are finding actually are. I would recommend using states scrambled between 1,000 and 10,000 times, as in related work. This can help us understand if the lack of completeness of beam search is even a factor. To make results even more convincing, the authors can use the few states of the Rubik's cube that are known to have the longest shortest path possible (i.e. the \"superflip\" [5]). Furthermore, the time the method takes should also be reported.\n\nOther comments:\n\nI do not understand equation one. Why would the probability of an action at timestep i be conditioned on the action at timestep i+1? How does the present depend on the future?\n\nReferences:\n[1] Agostinelli, Forest, et al. \"Solving the Rubik’s cube with deep reinforcement learning and search.\" Nature Machine Intelligence 1.8 (2019): 356-363.\n[2] https://codeocean.com/capsule/5723040/tree/v1\n[3] https://github.com/forestagostinelli/DeepCubeA\n[4] https://github.com/rokicki/cube20src\n[5] https://cube20.org/",
            "summary_of_the_review": "The authors propose an algorithm that could be potentially useful in specific settings where random reverse actions are likely to be optimal. However, the evaluation of the method is poor and differs significantly from related work in terms of difficulty and evaluation metrics. \n\n### AFTER AUTHOR FEEDBACK ###\nThe authors have incorporated many of my suggestions and now show that their method can solve all Rubik's cube and 15-puzzle test states using their method. Though the results are not as good as DeepCubeA, it is quite unexpected that this simple approach works at all. It is essentially: imitation learning of a noisy teacher (the last move taken when scrambling) followed by beam search. The simplicity of this method could be useful in environments where exploration becomes difficult, therefore, I am increasing my score from a 3 to a 5. \n\nOne aspect that is still lacking is an analysis of this beam search method. For example, though no cost-to-go function is learned, does this method approximately try to minimize the cost-to-go function while searching? The cost-to-go function DeepCubeA learns can also be used to induce a policy (i.e. a softmax based on the cost-to-go) and then be used in this beam search, which one performs better? Is Is the combination of beam search and self-supervised learning objective important or can a policy induced from a cost-to-go function also be used?",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper presents a self-supervised method for learning to solve Rubik's cube instances. The method relies on randomly perturbing a solved Rubik's cube and training a neural network to predict the last move that led to the perturbed state. The experiments show that a self-supervised Transformer combined with breadth-first-search can successfully solve randomly scrambled Rubik's cubes.\n\n",
            "main_review": "## Strengths\n- The proposed approach is rather simple and intuitive.\n- Results indicate that the method works. \n- Appears to be easy and stable to train.\n- The authors are upfront about the assumptions that the proposed method relies on.\n\n## Criticism and comments\n1) There are sentences that need to be reworded/rewritten to improve clarity.\nExamples:\n\"Assuming that the first move in a random path is probabilistically most likely optimal\". How is the \"first move\" defined? From goal to scramble, or the other way around? Later it is clarified from the context, but at this point in the paper, this is rather confusing.\n\"At all nodes, convoluting the probabilities of all the possible paths, a move in more optimal\npaths turns out to have a higher probability of being in the unknown path.\" (caption of figure 1)\nWhat do the authors mean by \"convoluting the probabilities of all the possible paths\"? Furthermore, the notation could be improved. For example, the probability of taking a particular move (which one?) as the first step to a specific target node in a path of n moves or more is denoted $p_n$. Specific notation for target and starting nodes, as well as precise notation for the $i$-th move would help clarify the math. Another example is equation (1) where the sum is not indexed. Again, the meaning could be inferred from the context, but this hinders the readability of the text. Another issue is that various details are missing from the paper.\nFor example, the precise inputs to the DNN are not described, so it's not clear how the problem instances are actually encoded. We are given the sentence:\n\"As input into DNNs, the edges and the corners have 48 unique possible states, determined by their spatial locations and\nflip/twist states 1\" which is qualitatively informative, but not sufficient.\nA section/paragraph that clearly describes the input, how it is processed and how are states encoded as well as how a solution is obtained and represented would help improve clarity.\nFurthermore, the text assumes a certain level of familiarity with Rubik's cube solving. A paragraph that describes the problem and explains the different states and moves would be helpful to make the text self-contained.\n\n\n2) This approach relies on two key premises. First, that the goal state of the problem is known. Granted that premise, then optimal moves are assumed to be more frequent.  The authors claim for (3) \"...and just taking a sub-optimal (alternative) move should not largely increase the total number of same-distance paths. Therefore, we assume that this holds true to varying degrees depending on the problem being addressed.\" However, explicit proofs are not provided, or even specific constructions/examples for other problems.\nSince the \"Contributions\" paragraph claims that this is applicable to combinatorial problems with a known goal state, more thorough arguments and experiments are required to establish this. Otherwise, the scope of this work appears to be limited to essentially learning based on a heuristic for a single problem.\n\n3) From the experimental section it is not exactly clear whether the comparison is with the original DeepCube setup or with just a DeepCube architecture trained with self-supervision. Given the training curves, I have to assume that it is a DeepCube architecture trained in the same self-supervised way. It would be good to experimentally show exactly how this approach compares to other Rubik's cube approaches in the literature (solvers and NNs). Are there benefits in time complexity for either training or evaluation? Is training more stable? Those would have to be thoroughly demonstrated to provide support for the proposed method (see for example the paper by Agostinelli et al. that you are citing)\n\n\n\n",
            "summary_of_the_review": "While the proposed method proposes an interesting trick for the solution of Rubik's cubes with self-supervision, it does not provide sufficient theoretical or experimental backup for its claimed contribution. The experiments show that it works, but there is not enough evidence to support the claimed general applicability of this technique beyond Rubik's cube solving. Furthermore, even within the context of the Rubik's cube problem, the authors did not provide sufficient experiments that establish the benefits of this work over related work. Therefore, apart from the simplicity of the approach (which is good), the potential impact of this work appears to be rather limited.\n\nUpdate: I have updated my score based on the authors' modifications to the paper. The authors have shown that this technique can work on an additional problem and achieve competitive results. Given that in terms of performance this method does not particularly stand out (partly because of computational budget reasons), the authors would either have to establish its broader applicability to other combinatorial problems and puzzles and/or provide a more thorough investigation of the idea. Could the model improve by predicting multiple moves ahead instead of just one using the same principle? Instead of random scrambles, could they be biased? (maybe based on some property of the problem). Developing the idea and perhaps connecting it with the literature in a meaningful way could make the contribution of the paper more relevant for a larger part of the (combinatorial) ML community. As it stands, its main appeal is simplicity which while definitely appreciated, is not enough.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "A method is  created to train a deep neural network by self-supervision to solve rubik's cube.",
            "main_review": "The contribution is that it is shown that self supervised learning may be enough to solve some rubik's cube instances close to optimally.\nRubik's cube is a combinatorial problem, and as such a planning approach would be better suited instead of a neural network. It would be interesting to hear why the author's chose to apply a neural network instead. The use of transformers is an interesting approach, although also a breadth first search is used.\nDeeper questions, comparing planning to a neural network, such as how the history is encoded in the network, are not addressed.\nThe paper does not explain clearly enough how the network is able to perform planning to optimality, and unclear is how this approach could solve a full problem.\nIt is unclear how this method could be applied if there is no goal. \nThe paper states that RL is the main method for solving rubik's cube, however, combinatorial search methods are used more often and to greater success.\n\nRelated work with respect to model-based end-to-end planning by neural networks, such as Value Iteration Networks, Predictron, and MuZero is missing.\n\nThe paper claims to be better than RL, however no comparison is performed.\n\nThe work is interesting, but needs to be developed further, and applied more fully to more applications. ",
            "summary_of_the_review": "\nThe work is interesting, but needs to be developed further, and applied more fully to more applications. \nEssential related work is missing.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes to learn to solve the Rubik's cube using random paths starting from the goal state. At training time, the model learns to predict the last move (leading to the goal) given the initial state by optimising the cross-entropy loss. At test time, the probability over states is used to construct the path from the goal to the initial state using beam search. The method is applied to both DeepCube and Transform architectures, both of which are able to solve 100% of the randomly generated problems in the test set.",
            "main_review": "### Strengths\n- The method is simple and seems to work well with different types of architecture.\n- The idea of showing that random paths starting from the goal state already produce strong cues for the optimal path is quite ingenious and, to the best of my knowledge, new.\n\n### Weaknesses\n- I am concerned by the lack of comparison against previous methods. It is not clear how Transformer-8L compares to DeepCube on solution length, solution optimality or computational complexity. Moreover, the authors claim their self-supervised method is more stable and robust than competing reinforcement learning methods but offer no evidence to support those claims.\n- Even though the overall method seems simple, the paper is not easy to follow and it is not entirely clear to me how the method works at test time.\n- The proposed method, albeit interesting, requires the solution to be known a priori. I believe this severely restricts its applicability, and for most combinatorial optimisation problems for which finding the optimal solution is not trivial, this approach would turn into an expensive supervised method rather than a self-supervised one.\n\n### Questions\n- It is not clear how the model works at test time. Could the authors explain further how the model is used at each step of the breadth-first search? \n- In a similar note, the inputs to the model are not entirely clear. How do the inputs vary at each step at inference time? Could the authors detail how they represent each state?\n- Why construct the test set using only 100 scrambles, when previous methods [1, 2] used between 1K and 10K moves? Even though any state can be solved in 20 moves or less, that does not mean every instance generated with 20 random scrambles or more will be just as likely to be a hard instance (requiring 17 to 20 moves to solve). In other words, can we compare the results of this paper with the ones reported in [1,2] or are datasets generated with 1K scrambles in average harder than datasets generated with 100 scrambles?\n- Do the authors have an interpretation to Figure 5? I could not distinguish a meaningful pattern in the attention mechanism.\n\n### Minor issues\n- The authors define their search procedure as breadth-first search with a 'maximum breadth' parameter. If I am not misinterpreting it, this method is more commonly referred to as beam search, which has beam size as a parameter.\n- Given that the paper is much shorter than 9 pages, why not include the extra details and analyses from the appendix in the main text?\n\n[1] Agostinelli, Forest, et al. \"Solving the Rubik’s cube with deep reinforcement learning and search.\" Nature Machine Intelligence 1.8 (2019): 356-363.\n\n[2] McAleer, Stephen, et al. \"Solving the Rubik's Cube with Approximate Policy Iteration.\" International Conference on Learning Representations. 2018.",
            "summary_of_the_review": "The paper has a valuable and promising insight. The authors show that random paths generated from the goal state are enough to train a model to solve the Rubik's cube using only cross-entropy loss, without resorting to reinforcement learning or hand-designed features. However, the exposition of the paper is unclear and feels incomplete, with many details left unexplained. More importantly, there is no direct comparisons with any other methods, which makes it hard to gauge what are the pros and cons of the proposed method. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}