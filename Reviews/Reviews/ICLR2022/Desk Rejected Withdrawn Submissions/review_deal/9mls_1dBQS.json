{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors propose to incorporate model-based value estimation into MBPO updates, when computing value functions for SAC. The MVE procedure explicitly uses the ensemble model structure by obtaining independent value estimates from each ensemble member which are then aggregated (either via minimization or averaging). Authors subsequently perform extensive ablative experiments on possible hyperparameter choices.",
            "main_review": "The main technical contribution claimed by the authors is a method for “directly incorporating the ensemble prediction into the RL method itself”, using a value computation step which respects the ensemble structure. However, there is a glaring omission of STEVE [1] from related work, which performs value estimation using almost exactly the same method (with minor differences, including the action selection method). Furthermore, STEVE includes a more principled method for handling the MVE horizon by using across-ensemble variances to weigh value predictions of different horizons appropriately. Beyond STEVE, using each member of the ensemble to obtain independent trajectory rollouts (and return estimates that are then averaged) is also the method used in PETS [2]. In the context of these works, the technical contribution of the work is greatly diminished, and so I unfortunately would have to vote for strong rejection.\n\n[1] Buckman, J., Hafner, D., Tucker, G., Brevdo, E., & Lee, H. (2018). Sample-efficient reinforcement learning with stochastic ensemble value expansion. arXiv preprint arXiv:1807.01675.  \n[2] Chua, K., Calandra, R., McAllister, R., & Levine, S. (2018). Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models. Advances in Neural Information Processing Systems, 31.",
            "summary_of_the_review": "I recommend strong rejection as the paper overlooks prior work (see above) which proposed the same approach for model-based value estimation as the authors.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes MBPO-MVE that uses ensemble prediction for value expansion. With a straightforward idea, the authors show the effectiveness of experiments by comparing the proposed method with several algorithms. However, the novelty is limited and some very similar works are missing in the related work.",
            "main_review": "Specifically, the idea to use ensemble for model-value expansion is a natural extension of MVE, and some previous works (e.g. [1]) also propose similar ideas. I'm surprised that the authors didn't mention and compare their algorithm with the closely related previous work. Besides, the authors claim that the proposed algorithm can substantially improve the performance of MBRL methods. I would like to see some theoretical supports and insights behind the claim. But the current manuscript only contains experimental comparisons.\n\n[1] Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion. Jacob Buckman et al. NeurIPS 2018",
            "summary_of_the_review": "The novelty is limited, and some key references are missing. And the theoretical insights why the proposed algorithm is better are not clear.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents MBPO-MVE, a combination of two existing model-based reinforcement learning approaches. In MBPO, an ensemble model is used to generate synthetic data using short rollouts from real environment states. In MVE, the model is used to generate multi-step update targets for value estimation. The paper proposes using MVE to augment the update targets obtained by the ensemble rollouts in MBPO. In a set of MoJoCo experiments, MBPO-MVE is seen to outperform MBPO and other recent model-based RL approaches.\n",
            "main_review": "Model-based reinforcement learning is an area with many challenging problems and many opportunities for significant impact on both practice and scientific understanding. The topic of this paper is certainly relevant to the interests of a sizable segment of the ICLR audience.\n\nThe writing is clear and engaging. I was able, for the most part, to follow the reasoning and understand the main contributions, the algorithmic ideas, the experimental setup and the findings.\n\nUnfortunately, I think there is a serious error in the statistical analysis. In the appendix, I don't understand where the test statistic 44.99 came from. When I add the natural logs of the 5 p-values I get -2.7. Multiplied by -2 gets us to 5.4. I didn't actually run that through a chi square distribution but I'm willing to bet that it does not yield a p-value < 0.05. The fact is that 5 samples per population is tiny, and will result in really low power tests. There might actually be a significant difference, but there simply isn't enough experimental data to find it.\n\nAlso the interpretation of Fisher's method is incorrect. The null hypothesis of Fisher's method is that the individual null hypotheses hold for all of the tests. Therefore the alternative hypothesis is that the alternative hypothesis holds in at least one of the tests. An example of this might be that we have some method that always seems to perform better but not statistically significantly so in any particular experiment. If the p-values are collectively sufficiently small, then we can use Fisher's method to conclude pretty confidently that there is a significant difference in at least one of these experiments -- otherwise we would expect to see more high p-values. Just eyeballing the p-values here shows that we are not in this situation. Most of the p-values are extremely high and only one is edging toward smallish. That's pretty much the expected distribution if there really is no difference: higher p-values are more likely. \n\nBased on the above, I don't think there is sufficient evidence to support the main claim of the paper (that MBPO-MVE performs better than MBPO). However, even if there were sufficient evidence, I don't think that conclusion would be particularly impactful. MBPO and MVE have already been reported on in the literature and the combination of the two is straightforward. MBPO-MVE doesn't seem to require any particular additional technical insight. The reasons that MBPO and MVE claim to improve learning rate are somewhat orthogonal, so it wouldn't be particularly surprising if MVE-style updates were to improve MBPO. That said, a potential modest gain may also simply not be worth the additional expense of adding MVE-style updates to MBPO or ensemble updates to MVE. In short, even if the results were statistically significant, I don't see much potential for the work to impact practice or understanding. \n\nThe question posed by Section 6, \"Why does MVE Work?\" is potentially more interesting, especially if there is sound evidence supporting the claim that MVE helps more than the original theory would suggest. However, I don't see any such evidence in this paper and the section seems to primarily present unsupported speculation rather than scientific or theoretical findings.\n\nThe paper also has some gaps in its relationship to prior work. One is to the MVE algorithm itself. The pseudocode given here suggests that MBPO-MVE only makes use of one H-step update target. The MVE algorithm, in contrast, averages the multi-step targets at every horizon up to H. Apart from the conceptual justification, the empirical findings in that paper were that this is an important aspect of the algorithm, so it is puzzling not to see it here. Another important missing piece is Buckman et al. \"Sample-efficient reinforcement learning with stochastic ensemble value expansion.\" (NeurIPS 2018). In their algorithm STEVE, MVE is performed with an ensemble model and ensemble value function, and the disagreement of the members of the ensemble is used to selectively weight updates at various horizons. Notably, they demonstrate that in some examples where MVE fails due to model error, STEVE is able to learn effectively. Especially since one of the claims in this paper is that using the ensemble average helps to account for epistemic uncertainty, a direct study of the relative strengths and weaknesses of different ways of using the ensemble for that purpose would be important to evaluate the significance of MBPO-MVE.",
            "summary_of_the_review": "The main algorithmic contribution of the paper is a minor combination of two existing approaches and unlikely to have an impact on practice or scientific understanding. The evaluation of the main claims is insufficient, and there seems to be a technical error in the statistical analysis. As such I recommend that the paper be rejected.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes to use multi-step value gradients to update the policy. Therefore, the paper learns a dynamics model and proposes a technique to compute the unrolled value estimate. Within the experimental section, the authors apply the method to the standard continuous control benchmark tasks of the OpenAI control benchmarks and report the obtained rewards.",
            "main_review": "### Paper Presentation:\nThe paper is nicely written but the text misses major details and is misleading. The paper starts by talking about model-based value expansion (MVE) and continuously talks about MVE and its advantages. However, this paper does not interpret MVE as most of the existing model-based RL literature. In most of the literature, value expansion methods refer to using the multi-step value estimate in the q-function target computation (e.g. Feinberg et. al., Buckman et. al.). In contrast, the methods utilizing the multi-step value estimate in the policy update refer to this approach as value gradients (VG) (e.g., Amos et. al., Heess et. al., Byravan et. al.). Instead of using these definitions, the authors always talk about MVE but in the end, the authors only use VG. Furthermore, this is not highlighted in the paper. The authors only mention on page 5 (right before Eq. 7) that the multi-step value function is used within the policy update. They don't explicitly mention whether the multi-step value estimate is used in the q-function update. I had to check the code to find out that they are not using it in the q-function target computation. Furthermore, the integration of the multi-step value estimate within SAC is not described and one must check the submitted code. \n\nThe paper also omits major existing work on the exact topic, e.g., Stochastic Ensemble Value Expansion (STEVE, Buckman et. al.) and Imagined Value Gradients (IVG, Byravan et. al.). Both works are very similar to the work as STEVE also uses multi-step value expansions and ensembles and IVG is a different value gradient approach. Furthermore, the paper lacks citation at many statements. The paper also provides no clear differentiation to SAC-SVG (Amos et. al.). From my understanding, I see very minor differences but the overall idea is close to identical. If that is the case, the paper must clearly state this similarity. This statement is also essential to evaluate the significance of the results. \n\nWhen reverse-engineering the code to understand the actual algorithm, I noticed a few inconsistencies compared to the paper. Therefore, I have a few open questions regarding the code. The code submission is also not adequate. The authors submitted a complete MBRL library containing many algorithms and the reviewer has to diff individual files to check which files have been altered. If you submit code, please submit code that does not include a complete model-based RL library such that the reviewer can easily infer which files have been altered! You can list the corresponding model-based RL library as a dependency. Furthermore, the submitted code either leaks the affiliation of the authors or contains a wrong license header!\n\n1. The code suggests that the algorithm does two different policy updates in `mbpo_custom.py`. In `line 243`, you call `agent.update(sac_buffer, logger, updates_made)`, which is the standard SAC critic and policy update using the 1-step value target for critic and policy update. In `line 248`, the code performs a policy update using the multi-step value function estimate. The SAC policy update is gated by the hyperparameter `actor_update_frequency`, which the yaml config files in `examples/conf/overrides` set to 4 for the environments. If this is the case, the policy is updated twice using the 1-step value target and the n-step value target, which is not described in the paper. \n\nCould the authors please explain how the code should be interpreted? \n\n2. Furthermore, I do not understand Line 8 & Line 10 in Algorithm 2 MBPO-MVE. How do you obtain X'_(j) from the dynamics model? The code increases my confusion. The code calls `obs, rwd = dynamics_model.sample(batch, deterministic=True, rng=rng)` (`mbpo_custom.py, line 337`) to compute the next state. The doc-string of the sample function in `one_dim_tr_model.py` states:\n`deterministic (bool): if True, the model returns a deterministic \"sample\" (e.g., the mean prediction). Defaults to False.`\n\nHow should I understand lines 8 & 10 in Alg. 2? The paper says it samples from somewhere but not from where and the code sets the deterministic flag to true. \n\nUntil these differences within the code and paper are not resolved, the algorithm is not reproducible and questions the validity of the experimental results.\n\n### Experimental Section:\nThe paper does not show any learning curves at all. To highlight the performance the authors must show the CONVERGED learning curves. The authors only show a non-converged learning curve of a single environment without any baselines. Omitting the baselines does not enable a thorough evaluation of the proposed approach. The curves for the horizon ablation and action ablation should also be provided. Furthermore, the section `Why does MVE work?` contains many unsupported speculations which are not supported by any experiments and should be removed. ",
            "summary_of_the_review": "The paper is not adequate for ICLR and a clear reject. The presentation of the proposed algorithm is misleading (at best), misses many details, does not cite relevant literature, has major differences from the submitted code. The displayed experimental results lack many details such that the evaluation of the described method cannot be verified. Furthermore, the exact differences are not precisely stated such that the evaluation of the significance is not possible. Given my current evaluation, the contribution is not significant because the proposed algorithm is very similar to SAC-SVG (Amos et. al.)  Therefore, the paper is a clear reject.\n",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}