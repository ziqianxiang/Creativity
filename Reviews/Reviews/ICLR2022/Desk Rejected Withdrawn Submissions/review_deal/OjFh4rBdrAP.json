{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In this work, the authors proposed a new task name video chapter generation, which is given a video and its time-aligned narration text the model outputs a list of video chapter start times and its corresponding chapter titles. To facilitate learning and evaluation for this task, the authors build a dataset that contains 9631 videos with subtitles converted from narrations. With the given dataset, the authors developed two-stage methods, in which it first localizes temporarily the chapter start time and then generates video chapter title given the narration text that corresponds to the time segment detected in the previous stage. ",
            "main_review": "Strengths:\n1. The proposed task is interesting, and it is a practical and import problem for video understanding.\n2. The manuscript is well structured, easy to follow.\n3. The proposed method is technically sound, despite being relatively simple it seems to work well on the proposed task. \n4. The proposed dataset can facilitate future research on the topic of chapter generation.  \n\n\nWeakness:\n1. Technical novelty is very limited, most parts of both visual and text models are adapted (slightly modified) from existing methods for similar tasks. \n2. The integration of the two modalities is very simple but not effective. The contribution of video-model is very limited. First, the video stream is only used at the detection stage so that the title generation is a pure language model. Second, the performance of visual models is much worse than the text model (~ -10AP) and only improves a little when it is fused with the language model. So, it makes me doubt the need of using visual input, so instead of video chapter generation, the problems reduce to a pure NLP task:  chapter generation. \n3. It is not clear whether the baselines in table 3. are finetuned or trained on the proposed datasets. To be fair, I think the authors should adapt their methods to the proposed dataset. I think it is important to see what the effect would be when combing these methods with text input. Also, it would be nice to see how your proposed methods perform on datasets that the baselines are evaluated in order to better justify why their methods are not suitable for your task.\n5. For table 4, the baselines are too weak I think it is important to consider some text summarization models rather than use heuristics. And it is important, to list how does the localization performance affects the summarization step. \n6. Given the task is targeting a multimodal problem, I think it is important to compare it with other video-language or visual-language methods, such as Video-Bert, which is trained with both language and video from instructional videos very similar to the setting of the proposed problems. \n",
            "summary_of_the_review": "I think the authors should seek a more effective way of integrating both modalities, since in the current setting the visual model only brings marginal improvement for the task, and only contributes to the localization stages.  In the current setting, it seems that the task can be reduced to text chapter generation. The need for visual input needs to be strengthened.  Moreover, I suggest considering adding more multimodal methods as baselines. \n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes chapter generation as a new task for understanding long videos. A dataset of almost 10k videos, each 13 minutes on average, is also presented to facilitate this work. A baseline two-stage model combines chapter localization (modeled as a boundary detection task) and chapter title generation (modeled as an abstractive summarization task). Unfortunately, the paper is lacking in several aspects as explained below.\n",
            "main_review": "*Strengths*\n1. The paper presents an interesting task of chapter generation. As more content creators use the chapters feature of Youtube, this is a particularly interesting task.\n\n2. The new dataset, albeit small, is quite interesting. As claimed, it also seems easy to expand (however, does beg the question as to why the current work was limited to 10k).\n\n3. Figures and illustrations are very clear.\n\n*Weaknesses*\n1. Premise of the problem:\n- It seems like Youtube is already providing some support for adding automatic video chapters (Nov 2020): https://support.google.com/youtube/thread/18138167/youtube-test-features-and-experiments.\n- How would the data collection distinguish between manually added ones and automatically generated?\n- Can this automatic process be used as a baseline for evaluation?\n\n2. Dataset details are very sparse:\n- What is the source of the videos?\n- How are the particular videos with chapter annotations obtained?\n- What is their estimated distribution across topics mentioned in section 3 (entertainment, education, business, etc.)?\n- What impact does the filtering strategy have on the overall set of selected videos? Does it end up focusing on a fixed nmber of content creators?\n- From how many channels (I'm assuming youtube?) is the data sourced?\n- Can all videos of a creator be put in either the train or validation sets and not mixed?\n- While Table 1 presents statistics about number of videos, nothing is said about the main task: total number of chapters, number of chapters per video, typical gap between two chapter boundaries, etc.\n- Similarly, distribution of number of words in chapter titles, etc. would be good to include.\n- How many easy, how many hard samples?\n- Will the dataset be made publicly available?\n\n3. Incomplete experiments and flaws:\n- The experiments are not conducted on a separate validation and test set. Especially with a model that has a crucial hyperparameter in window size, it is hard to understand generalization without this.\n- Details on how the splits are created is also missing. Splits based on video creator would also be very interesting to analyze generalization, but this is unknown.\n- Impact of localization error on downstream title generation is missing. It would be good to provide an ablation study for this.\n- Might be good to group all the Easy cases, Hard cases, and All cases in Table 5 rather than making all the easy cases bold (it is expected that they perform better).\n\n4. Chapter localization:\n- Doesn't the chosen windowing approach result in a large skew of samples where there is a detected boundary vs. not? Is anything special done to handle this?\n- The proposed approach is quite simple, however outperforms baselines by a large amount. While the claim is that previous methods [Rao, et al. 2020] require shot boundaries, would the performance be equally bad if the baseline uses fixed sized shot boundaries (say $2o$ seconds) as assumed in this work?\n- Additional baselines like uniform splitting, or distribution aware splitting (e.g., something learned like more splits towards the beginning and end, fewer in the middle) would be good to understand the difficulty of the dataset.\n\n5. Chapter title generation:\n- Why is title generation treated as a pure abstractive summarization problem, rather than including hints of narrated video as well?\n- What is the impact of having single presenter narration vs. dialog vs. music or silence on the title generation performance?\n\n6. Some problems with presentation / explanations:\n- It's not clear how this work defines a chapter until much later in the introduction. Would be good to clarify.\n- The first sentence is confusing: \"summarize its content to a couple of chapters, ... like chapters in a book\". Made me think that this work does video summarization, but I wasn't able to understand how few book chapters are a summary of the book.\n- \"Movies lack clear temporal structure for chapter generation\" is incorrect. Movie DVDs have chapters for several years now. In fact, one of the early works on scene detection [A], uses this as their ground-truth.\n- Paper states that it uses \"all video frames\", and then immediately states that frames are downsampled to 1fps.\n\n[A] Z. Rasheed and M. Shah, \"Detection and representation of scenes in videos,\" in IEEE Transactions on Multimedia, vol. 7, no. 6, pp. 1097-1105, Dec. 2005, doi: 10.1109/TMM.2005.858392.\n",
            "summary_of_the_review": "The paper feels incomplete and incorrect in several places as highlighted in the weaknesses above. While the idea is quite interesting, in the current state, I cannot recommend acceptance.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns",
                "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
            ],
            "details_of_ethics_concerns": "The paper proposes a new dataset, perhaps sourced from Youtube. Unfortunately no details are provided, apart from the fact that the videos contain chapter titles.",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper studies video chapter generation, where videos are longer and contain many complex temporal structures.\nIt collects a large-scale dataset called ChapterGen, which consists of 9631 user-generated videos.\nAnd it further proposes a two-stage framework to perform chapter localization and chapter title generation.\nThe experiments demonstrate that the proposed framework outperforms existing methods on both accuracy and efficiency.\n",
            "main_review": "It is great that the authors jointly consider the chapter localization and title generation and put efforts into collecting ChapterGen dataset.\n\nMy main concerns are as follows,\nIn the dataset, \n\n* It is not clear how to automatically collect the videos, as the author mentioned in Section 3. Do they come from keyword searching?\n* Besides the statistics on the video number and average length, It is advised to include more statistics about the dataset. For example, what kind of videos are contained? cooking tutorial videos? laptop recommendation videos? What is the diversity of the collected videos? This information could help the community to use it better.\n* How do the authors handle the long videos? There are very long videos, e.g., conference recording videos with chapters on YouTube. Do the authors discard them according to time duration or tags?\n* How to annotate easy and hard cases? Are they done by one annotator?\n* Citation missing. Condensed Movies [R1] is also a dataset that leverages the YouTube videos MovieClips and their meta-information provided by the people who upload them.\n\nIn the method, the two stages are relatively independent. The process to localize chapters and generate chapter titles is simple and modifies existing methods slightly. The authors are encouraged to distinguish the proposed methods and existing methods to highlight the contribution. \n\n* The binary classification in chapter localization is similar to [R2]. Simply using the middle time of positive clips may not be an ideal way. Is there any way to predict an offset, which may get some inspiration from existing objection detection methods?\n* The title generation gives the readers a sense that it is adopted from an existing method, and there isn’t much novelty on the authors’ method.\n* Since the author jointly considers the chapter localization and title generation. It is encouraged to study the relationship among these two tasks rather than take them as independent tasks. Whether the localization could help with the title generation?\n\nThe experiments are not enough to verify the contribution of the paper.\n\n* In the experiments, the baseline methods may be weak. What are the performances of Rao et al and Chen et al that use text modality? The fairness of the comparison is concerned, as mentioned in the ablation study, “text modality always outperforms visual modality. This verifies the previous conclusion that narration text may contain more discriminative features than frames in chapter localization”.\n* Do video captioning works in the title generation? Could the authors provide some comparison and explain it?\n* The ablation study on easy and hard cases doesn’t contain much information. It is natural to achieve high scores on easy cases and low scores on hard cases.\n* Similarly, Figure 4 (a) shows that the proposed method is right. It is suggested to provide some failure cases analysis to provide insights to the community. Culprit analysis is helpful to inspire how to solve the hard cases.\n* Minor formatting suggestion. In Table 2, it is better to present long numbers with comma, e.g., 18097041 → 18,097,041\n\nThe authors may include supplementary videos on the collected dataset and more qualitative results, as currently readers only have Table 1 and Figure 1 to know the dataset.\n\n[R1] Bain, M., Nagrani, A., Brown, A., & Zisserman, A. (2020). Condensed movies: Story-based retrieval with contextual embeddings. In Proceedings of the Asian Conference on Computer Vision.\n\n[R2] Lorenzo Baraldi, Costantino Grana, and Rita Cucchiara. A deep siamese network for scene detection in broadcast videos. In 23rd ACM International Conference on Multimedia.\n",
            "summary_of_the_review": "Considering the above limitations, I think this paper is not ready yet and needs to undergo a major revision. It will be a great paper when the analysis is conducted in a more comprehensive and detailed way. At the current stage, I vote for rejection.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Other reasons (please specify below)"
            ],
            "details_of_ethics_concerns": "There might be some basis when collecting the data. How do the authors handle this problem?",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces a novel task where the goal is to segment internet videos into chapters, and generate a short title for each chapter.\nThe authors introduce a novel dataset for this task, namely ChapterGen, which consists of a little bit less than 10k YouTube videos with\nUser-generated video chapters (that are all provided with a start time) as well as speech transcripts automatically obtained using the YouTube ASR system.\nThe papers then proposed a two-stage model for (1) localizing the chapters in time and (2) producing a title for each video chapter.\nTheir method is trained and evaluated on ChapterGen and compared to various baselines for both localisation and title generation.\n\n",
            "main_review": "Strengths:\n\n1. The main strength of the paper is the introduction of this new video chapter generation task which is, to the best of my knowledge, not existing yet in the literature. In particular, I believe this task to be highly useful for improving the viewing experience of long (>10 minutes) internet videos, given the large numbers of these videos that do not come with user-generated chapters. \n\n2. Their dataset collection did not rely on manual annotation of data. In fact, the ChapterGen dataset was elegantly collected using readily-available annotation from user-generated chapters and ASR system which make their approach, in theory, straightforward to scale-up given the abondance of these YouTube videos with user-generated chapters. \n\n\nWeaknesses:\n\n1. While this novel dataset is one main strength of the paper, I do also believe this paper comes with inaccurate claim as well as lack of detail about this dataset. For example: There is no separate section about this new dataset. It would have been crucial to describe how this dataset was collected. I am curious to understand how the authors manage to find this list of Youtube videos with chapters. Did they scrap many random video YouTube descriptions and kept the one with user-generated chapters?\n\n2. In the related work, the authors wrote the following: « The collected videos cover diverse topics and activities. The videos are created by different authors with different styles. » but there is no number, chart or information backing the diversity of topics and activities depicted by this dataset. Statistics also about the video lengths, chapter length or number of chapter per video would have been welcomed. Moreover, the related work only cite datasets such as YT8M or Kinetics before writing « Our collected dataset differs from the existing datasets in two aspects: (1) Our data collection is fast and scalable without expensive manual annotation. Therefore, we can easily expand the dataset in terms of data size.» . The related work is lacking other video dataset that are also satisfying (1) such as QueryYD [1], HowTo100M [2] and more recently YT180M [3] (though this one is supposed to appear at NeurIPS 2021), as they are collected using readily available data (i.e. YouTube ASR). I suggest the authors include at least one of these datasets and instead focus on the real novelty of ChapterGen which is the fact it has readily available video chapters information, which I do believe makes this dataset unique.\n\n3. The authors claimed several times, for instance in the abstract, this dataset to be a large-scale one. I feel this is an over-claim as I am not sure we can call a video dataset with 9631 videos, especially given it does not contain a single manual annotation, large-scale in 2021-2022. This would have been totally fine a decade ago (e.g. UCF-101 is of the same scale but was at the time of it’s publication rightfully denoted as large-scale) but not now, especially given the number of recent Youtube video dataset with readily available annotation (HowTo100M [2], YT180M [3]) which have more than a million of video.\n\n4. It’s nice to see that the video chapter localisation approach is both using text and vision but I was a bit disappointed to see that the video chapter generation approach is only relying on the ASR input. The authors wrote: « we use narration text to generate chapter titles, as narration text has plenty of high-level semantic information, while images usually contain low-level visual features»  but there is no experiment backing this claim that the visual information is useless. Especially given there must be video chapters with very few (or not) ASR input. However we don’t really know this information as this paper is lacking detail about the collected dataset. I have no doubt with the authors that the ASR account for almost of all of the signal but we need to have some evidence.\n\n5. Video chapter generation baseline: I find the video chapter generation baseline too weak. The authors wrote the following: « For chapter title generation, there is no related work for comparison. We compare our method with three heuristic strategies which are widely adopted in text summarisation». Why not directly having a real state-of-the-art text summarisation baseline instead of reporting simple heuristics? One could use some pre-train Transformer summarisation method such as BigBird [4] and fine-tune them on that task. I am quite convinced this baseline to be very good, given that I have personally seen such fine-tuned summarisation model work very fine on YouTube ASR data.\n\n\nMinor comment:\n- What is the reason why there is no validation set?\n- When writing o = 2 or k = 16 please specify if these are seconds or frames or something else.\n\n[1] QuerYD: A video dataset with high-quality textual and audio narrations, Oncescu et al., ICASSP 2021\n\n[2] HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips, Miech et al., ICCV 2019\n\n[3] MERLOT: Multimodal Neural Script Knowledge Models, Zellers et al., to appear at NeurIPS 2021.\n\n[4] Big bird: Transformers for longer sequences, Zaheer et al. NeurIPS 2020. (https://github.com/google-research/bigbird)\n",
            "summary_of_the_review": "I appreciated reading this paper because it introduces a novel task, which I think, can have a large-impact in improving our viewing experience of long internet videos. Especially I find it valuable they collected this novel ChapterGen dataset, which conveniently only relies on readily-available annotation, making it scalable to a much larger number of videos.\n\nHowever, there are many weaknesses that makes me believe this paper is not ready yet for publication as described above. In short, I would have voted for a clear accept if: (1) the paper put more emphasis on this novel dataset novel rather than the model  (2) tone down the large-scale claim for this dataset, or even better scale the dataset 10x more to at least 100K videos, which I believe is doable given the abundance of such videos. (3) Include stronger video chapter generation baseline based on fine-tuned state-of-the-art text summarisation method (i.e. BigBird).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}