{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors introduce a new method to train SNNs with a single time step. The main idea is to train an ANN, then convert it into a SNN with 5 timesteps, then re-train this SNN using surrogate gradient learning (SGL) and progressively reduce the number of timesteps down to 1.\nNote that having a single time step is almost a degenerated case for a SNN. It means stateless units, no temporal integration, no dynamics. In fact the resulting network could be better described as a feedforward ANN with Heaviside for the activation function.\nA corollary: the leak has no effect on the final network, so I wonder why the authors train it.",
            "main_review": "Strengths\n* The accuracies are impressive for 1 timestep only.\n* Having a single time step certainly decreases computational and memory access costs.\n\nWeaknesses\n* Not suitable for dynamics inputs.\n* Tedious procedure which involves multiple trainings.\n* The authors are not up to date with the literature. For example, they say:\n\"surrogate-gradient based BPTT training has resulted in SNNs with high accuracy, but the training is compute intensive compared to conversion techniques and the latency is still significant ( 100-125 timesteps).\"\n\"We choose T=5 as our starting point, since it is the minimum reported latency to achieve satisfactory performance on ImageNet.\"\nThese statements are simply not true, Fang et al managed to train a SNN from scratch using SGL (no conversion) on ImageNet using only 4 timesteps: https://arxiv.org/abs/2102.04159\n\nRecommendation:\n\nMarginally above the threshold.\n\nI acknowledge that their method works, but I think there must be a better way to obtain the final network. As I said earlier, this final network is a simple feedforward (stateless) ANN with Heaviside for the activation function. This network could be trained directly (no conversion) from scratch using a surrogate gradient for the Heaviside (eg the derivative of a sigmoid). Of course, I suspect this procedure would be sensitive to the initial weights and the shape of the surrogate gradient, but I see no reason why it would not work, so I urge the authors to try (in Table 3 they compared to other ANNb of the literature but I suggest the authors to do the training themselves using the above procedure).\n\nAlso, for the sake of completeness, I suggest trying a ResNet architecture on ImageNet in Table 5.\n",
            "summary_of_the_review": "An interesting method but which could probably be simplified.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes an iterative training method to gradually reduce the time steps for inference all the way up to 1. The authors have done experiments with CIFAR-10, CIFAR-100, and ImageNet datasets to show that deep SNNs can yield competitive classification performance even at a timestep value of 1. The authors have also conducted experiments on Atari pong based on RL. ",
            "main_review": "Strengths:\n=======\n1. The paper showed at t-step of 1 near SOTA accuracy can be achieved on datasets like CIFAR-10, CIFAR-100, and ImageNet.\n2. The authors did experiments with Atari pong, an RL-based game to show a new application of SNNs.\n\nWeaknesses:\n==========\n1. (a)The work is primarily based on two earlier works, 1. the hybrid SNN work [1]: from here the authors incorporated the idea of hybrid ANN-SNN training, and the diet-SNN work [2]: from here the authors incorporated the idea of threshold-leak trainable during SNN training for deep SNN models.  Apart from that, the authors did training with a time step value T1 (for a significant number of epochs) and then used the trained SNN weights to initialize and retrain the model in SNN domain for T2 epochs, where T2 < T1, and so on unless time step value of 1 is reached. This is just an extension of the general training strategy where for t step 5->1 with a step reduction of 1, the authors did training for 300x5 = 1500 epochs (in SNN domain only). Thus the core contribution of the work is not at all novel and makes the paper weak.\n\n(b) The concern of vanishing spikes for reduced t step or non-optimal threshold/leak is not new finding either. Hence, the overall novelty of the paper remains extremely weak.\n\n2. The authors referred to the training process to be similar to distillation in the temporal domain. However, this is a technically incorrect claim, as the authors are only using the trained weights of t-step T1, to initialize while starting the training for t-step T2 (T2 < T1). Here (as also pointed out by the authors) there is no notion of transferring the so-called \"dark knowledge\" via any form of loss propagation that takes the teacher (the model trained with t-step T1) into account. Also, while referring to the initialization of an iso-architecture as a student model, what is the teacher? So, this is not distillation, and the authors should refrain from saying it so. \n\n3. For ANN distillation, the teacher and student models do not necessarily need to be architecturally different. There have been earlier works [3] where the same models have been used both as teacher and student. This is no different from conventional ANN, and the authors should refrain from saying so.\n\n4. To me the comparison of Tables 2 and 3 is unfair. The authors here basically did iterative training and compared with methods most of which are non-iterative. For example, neither [1], nor [2] is iterative in any sense. Nor does the ANN comparison works. So, I am not sure whether the author's claim of SNN outperforming ANN (at least for the CIFAR-10 dataset) will hold got at all.\n\n5. Few choices are not clear. For example, why start with t=5 and then reduce by 1 or 2? What would happen if someone starts with t=20 or 10 and reduces to 1 with a larger step size? \n\n6. In Fig. 1(d) why does the light blue line (T1) is having high spike values at the later layer? Here I am assuming T1 represents direct training with t = 1 (I did not get what did the author mean by converged to 1, does it mean converged to 1 for starting with t=1 or converged with initialized weight for some other t). This is confusing because the authors said T1_5 means training for t=1 with starting weight of t=5. So, if my understanding of the notion is correct, then why direct training (T1) has more spikes at the later layer compared to for say T1_5 or T3_5?\n\nMinor concern: The authors claimed four contributions, however, the 2nd to 4th claims look like evaluations to me. The authors might want to rephrase that part. \n\n[1] Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation, ICLR 2020.\n[2] DIET-SNN: Direct input encoding with leakage and threshold optimization in deep spiking neural networks, arxiv 2020.\n[3] Revisiting knowledge distillation via label smoothing regularization, CVPR 2020.",
            "summary_of_the_review": "The paper proposes an iterative training approach to gradually reduce the SNN latency in terms of time step reduction. Overall, the paper's results are good. However, the paper lacks significant novelty and mostly remains as an \"empirical study paper\" of prior published works. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposed a framework that can reduce the training and inference latency of SNN to 1 timestep.",
            "main_review": "Strength: The proposed SNN can achieve good performance with only 1 timestep. \nWeakness: I’m concerned about the novelty. I’ll include the details below.",
            "summary_of_the_review": "Novelty and missing references:\n1. Missing references: In this paper, training threshold and leak is similar to the dynamic thresholding and intrinsic plasiticity in [1] and [2]. I think the authors should discuss a little about these existing works.\n2. Since the BP and training threshold and leak are existing algorithms, I hpoe the authors can make more clear about the contribution of this work.\n\nAnother major issue is the comparison with existing works:\nOnce the timestep reduced to 1, what the difference between the proposed network and Binary Neural Network (BNN)? To my understanding, they function almost the same and both of them should have similar energy efficiency. In the paper, the authors only demonstrate the energy comparison with regular ANNS. However, I think they should introduce the related BNN works, focus on discussing and analyzing the difference between the proposed network with BNN, and compare their performance and energy consumption.\n\n\n[1] Bellec, G.; Salaj, D.; Subramoney, A.; Legenstein, R.; and Maass, W. 2018. Long short-term memory and learning-to- learn in networks of spiking neurons. In Advances in Neural Information Processing Systems, 787–797.\n[2] Zhang, W., & Li, P. (2021). Skip-Connected Self-Recurrent Spiking Neural Networks with Joint Intrinsic Parameter and Synaptic Weight Training. Neural Computation, 33(7), 1886-1913.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}