{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes 2 different gradient-free stochastic algorithms, namely $\\texttt{DSZOG}$ and $\\texttt{ADSZOG}$, for solving finite sum nonconvex optimization problems with a large number of nonconvex functional constraints by using a form of quadratic penalty approach. They first re-write the problem in min-max form and solve the following reformulation:\n$$\n\\min_{{\\bf w}} \\max_{{\\bf p} \\in \\Delta^m}\\mathcal{L} ({\\bf{w , p}}) = \\frac 1 n \\sum_{i=1}^{n} l_i({\\bf w}) + \\beta \\sum_{j=1}^m p_j (\\max \\{f_j({\\bf w}), 0 \\})^2 -  \\frac \\lambda 2 || {\\bf p} ||^2\n$$\nThey use a zeroth-order stochastic gradient estimator for their updates. \nThe authors show the convergence of the method to the stationary points of the constrained function. The algorithms allow computing the gradient by only using a set of function values and constraints instead of computing the full gradients. They use batches to reduce the variance of the gradients. Later, they extend their algorithm for improved convergence rate by using acceleration and momentum techniques.  They provide empirical results comparing their method with existing Frank-Wolfe-based and projection-based approaches. ",
            "main_review": "Although there are some typos and grammatically incorrect sentences etc., overall, the paper is well written and easy to follow. Claimed contributions are clear. \n\nStrengths:\n\n1- The algorithms solve an important class of problems, which are nonconvex functions with nonconvex functional constraints. \n\n2- They are flexible in the sense that one can handle both stochastic (finite-sum) objectives and a large number of constraints without requiring heavy computations per iterations. The algorithm is single loop and does not require solving subproblems. \n\n3- Due to the zeroth-order oracle used in the gradient computations, they can handle both black-box and white-box functions.  \n\n4- The objective function and special important sampling technique used for constraints are inspired by Cotter et al. 2016. Authors add a strong concavity term to be able to satisfy the assumptions in the algorithm they use. \n\n5- Experimental results seem interesting and promising especially the one with fairness constraints. \n\nWeaknesses: \n\n1- My main concern with the paper is the fact that the contributions are overstated. For example, the authors claim that they propose a doubly stochastic zeroth-order gradient method to solve the heavily constrained nonconvex problem and abbreviate their algorithm as DSZOG. However, I think that they only use the ZO-SGDA algorithm in [Wang et.al. 2020] with $f({\\bf w, p}) = \\mathcal{L} ({\\bf w, p})$ and with domain of ${\\bf p}$ equal to simplex. This is consistent with their assumption that domain $\\mathcal{Y}$ is convex and bounded with domain radius $D>0$. Please correct me if I'm wrong. \n\n2- Moreover, theoretical results are almost identically obtained from ZO-SGDA with exactly the same assumptions on the objective function $f ({\\bf x, y})$. The only contribution for the DSZOG algorithm on the theoretical side, I would say, is the connection of the critical points of the min-max objective to the critical points of the constraint problem through KKT relations. I would be glad if the authors could clarify their contribution in the algorithmic part if there is any. \n\n3- In the proof of proposition, in the second line of inequality $(4)$, the authors use equation $(3)$ to upperbound $\\sum (\\max(f_j({\\bf w^\\ast)}, 0 )  - \\lambda p_j^\\ast)$ by $\\epsilon^2$. I'm confused by how the inequality is used. This is what I get with the same expressions and by using $||a+b||^2 \\leq 2 ||a||^2 + 2||b||^2$:\n\n$$\na =\\sum (\\beta \\max( f_j({\\bf w^\\ast}), 0 )  - \\lambda p_j^\\ast) \n$$\n$$\nb =\\sum \\lambda p_j^\\ast \n$$\n$$\na + b = \\beta \\sum (\\max(f_j({\\bf w^\\ast}), 0 )) \n$$\n$$\n\\frac 1 2  \\beta^2 \\sum (\\max(f_j({\\bf w^\\ast}), 0 ))^2 \\leq \\sum (\\underbrace{\\beta \\max(f_j({\\bf w^\\ast}), 0 ) - \\lambda p_j^\\ast}_{\\neq \\beta \\phi_j({\\bf w^\\ast}) - \\lambda p_j^\\ast })^2 + \\lambda^2 \\sum  (p_j^\\ast)^2 \\\n$$\nAm I missing something here? I would be glad if the authors could explain how they got this inequality.   \n\n5- What are the range of values for q for the gradient computations?\n\n4- That would be good if the authors could show the unbiasedness of the gradient estimates in all cases, at least mention it somewhere. \n\n5- Technical challenges and novelty for the accelerated variant of their algorithm are not obvious from their discussions. \n\n6- Step size values in the theorems seem so conservative. These are probably not taken into account when choosing the step-sizes in practice.\n\n\n7- Is there a special reason why the authors used the quadratic form $ \\phi_j({\\bf w}) := (\\max(f_j({\\bf w}), 0 ))^2  $ instead of ($ \\max(f_j({\\bf w}), 0 )) $ as in Cotter et al. 2016? Because, I think, with the latter choice, min-max problem corresponds to solving the constraint problem with lagrangian (except the quadratic term). Is there an augmented Lagrangian implication with the author's choice? \n\n8- How are $\\epsilon, \\epsilon_1, \\epsilon_2$ and $ \\epsilon_3$ all related ? This should be explained in the main text. \n\n\nMinor:\n\n* First page last pargraph: I think the sentence which starts with \"Based on ZOSCGD ...\" has to be re-written.\n\n* Section $2.2$ last 2 lines: \"Note different formulation ...\" --> \"Note the difference in the formulation\"  \n\n* Title of section $2.3$: \"Doubly zeroth-order stochastic\" --> \"Doubly stochastic zeroth-order ...\"\n\n* Title of section $2.4$: \"Accelerated with ....\" --> \"Acceleration with ...\"\n",
            "summary_of_the_review": "Overall, the paper discusses an important problem with an easy-to-follow structure. However, technical contributions are limited or overstated. I think the paper has value but it has to be written in such a way that the reader can appreciate the authors' contribution.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper  focuses on heavily constrained non-convex minimization problems, ie problems of the form: $$ f_0(x)=1/T \\sum_{t=1}^{T}l_t(x)$$ subject to constraints defined by a finite set of functions $f_j(x)\\leq 0$ with $j=1,..,m$.    \nTheir main innovation is that the introduction of a second type stochasticity concerning the choice of the associated $f_j$ which define the  respective feasible domain.  \nHaving this at hand,  the author(s) reformulate the original min problem to an appropriate min-max regularized one and apply these zeroth order approaches to solve the latter.\n \n",
            "main_review": "The paper seems quite interesting and enjoyable to read to read. Moreover,  the contributions are clearly stated and well written. Furthermore, the mathematical analysis as far as I have checked seems. correct and sound.\nMore precisely, the key novelty of this paper, namely to induce stochasticity to the constraints to  achieve scalability of the methods  seems interesting to me. However,  since I am not expert on zeroth order methods, I am not sure whether this idea is completely novel compared to the existing literature. So, I think that it would be good for the paper to highlight more their novelty and the technical challenges that they  tackled.\nA question that I have is the following:  Since the main issue that the author(s) want to tackle is scalability to complex constraints, why first. order methods, combined with stochasticity with respect to the domain does not solve the issue? \nIn addition that their methods should be numerically compared to first order methods as well. ",
            "summary_of_the_review": "The authors propose zeroth order methods with doubly stochastic estimators for solving heavily constrained non-convex minimization problems. This idea, as far as my knowledge goes seems novel and interesting. However, it would helpful to highlight the main technical novelties compared to the existing literature . ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents new zeroth-order method for large scale optimization with heavy constraints. Reformulating the problem as a minimax game, the paper proposes a doubly stochastic primal dual method DSZOG which stochastic gradient descent on the primal and randomly sample constraint for the dual ascent. Gradient is estimated by the zeroth order values. Moreover, the paper proposes an accelerated method (ADSZOG) which obtain even better convergence by using moving average and adaptive method. Empirical study justifies the superiority of the proposed method compared over the existing work.",
            "main_review": "In Proposition 1, the paper claims that the $\\epsilon$-stationary point for the minmax problem is equivalent to the $\\epsilon$-KKT point by Lin et al 2019. However, I don't get it. The minimax problem is not equivalent to the original constrained problem because we don't know whether the $\\alpha^*$ lies in the simplex or not. Hence, I am not sure the reduction to gradient of min function in Wang et al. 2020 would work. Plus, there is a typo in Proposition 1,  it should be L(w,p) not L(w,\\alpha). \n\n\nSome of the notations are not well-defined. For example, in page 3, the definition of $G_\\mu^{\\psi}$ appears to be problematic because the expression does not have $p$ inside.\nI would recommend using the  $i_t,j_t$ rather than $i, j$ to indicate that the indices are randomly chosen. \n\n\nIn page 3, Sec 2.3, the paper claim that \" we sample the constraint ..., which makes our method can find the most-violated constraint\". I don't understand, if the goal is to find the most-violated constraint, then why not use greedy approach such as frank-wolfe in the dual domain?  Can the author compare the random and greedy strategy? \n\nIn page4, please give a reference on how to compute projection on $\\Delta^m$ even though it seems easy via some search algorithm.\n\nFor algorithm 2, while the paper claims it is an accelerated method, the use of acceleration seems to be different from the existing  algorithm such as FISTA; there is no explicit extrapolation involved. Can the author elaborate which accelerated method the motivation is from?\n\nAssumption 3 is somewhat unintuitive. why can't $\\mathbf{z}_w$  or $\\mathbf{z}_p$ be zero?\n\nRemark 3, the last sentence, should be \"convergence ... is faster than ...\" not \"convergence rate is faster\".\n\nRemark 3, I don't understand why ADSZOG has better rate than DSZOG. The remark claims that ADSZOG has a rate $O(L^6/T)$ but Remark2 shows DSZOG has a rate $O(L^5/T)$. Should DSZOG be better?\n\n",
            "summary_of_the_review": "\nThe paper provide new algorithms for  constrained problem which only have access to the zeroth-order information. The results looks very interesting. \nHowever, the technique heavily relies on some existing work such as Guo et al. 2021. The adaptation of the existing technique to zeroth order optimization seems not a big surprise. Therefore, the paper should make more efforts in highlighting the difficulty and addressing the novelty.  Moreover, the writing of this paper still needs substantial improvement, and new concepts should be carefully addressed rather than being linked to literature. ",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper provides a zero-order methods to solve problems with large numbers of non-convex problems.",
            "main_review": "The paper is not well written, in term of notations, missing words, latex problems and written with a vague language, borderline of being incorrect on some statements. There should be more care dedicated to the paper before submitting it.",
            "summary_of_the_review": "Bordeline sentences\n- \"Recently, constrained optimization have become increasingly relevant to the machine learning community\" -> Why is it recent? What knowledge such a sentence brings to the reader?\n\nLatex problems; incorrect writting etc...\n- \"Due to the friendly of approximating the gradient and scalability to large scale problems\" : problem in the sentence\n- Improper use of \\citep and \\cite in several places, e.g., \"ZOSCGDBalasubramanian \"\n- Table 1: do these paper have rate of convergence?\n- \"effectively and efficiently\" -> why the redundancy?\n- \"Note different the formulation in Clarkson\"\n- \"To solve this problem, we use the stochastic manner\" -> the \"stochastic manner\"?\n- \"which makes our method can find\"\n- \"and white/black box function\" -> not sure if this is very correct to formulate like that. \"white/black\" box is not a property of a function, it is rather a setting that limit or not the type of oracle we can require on the function? I.e., evaluation only or asking for gradients.\n- Assumptions, Theorems etc... are abunding with notations and complex quantities with no space , making it very hard to read.\n\n\nTechnical assessments\n- It would be nice to have a discussion about convergence rate and what to expect.\n- The authors should also explain the novelty of their approach. Here, it is not very exciting in the presentation. It looks more like a combination of standard tools in Zero-order methods.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents two zeroth order methods, DSZOG and ADSZOG, for\nfinding approximate stationary points of smooth nonconvex finite-sum\noptimization problems with a large number of smooth inequality constraints.\nThe first method is based on a stochastic primal-dual augmented Lagrangian\n(AL) scheme in which zero-order estimates are used to approximate\nthe gradients of the AL function. The second method is a generalization\nof the first method that adds an addition extragradient step before\nevery primal-dual update. Computational results are then presented\nto demonstrate the superiority of the methods in terms of testing\naccuracy and training time.",
            "main_review": "*Strengths*: Overall, the paper is well-structured, the problem\nis well-motivated, and the background material is sufficient. The\npresented computational results look very promising and I appreciate\nthe level of detail given to the convergence analyses under relatively\nstandard assumptions.\n\n*Weaknesses*: I have three major issues with this paper that\nneed to be addressed before I can recommend acceptance:\n1. **Proper Dependencies on $\\varepsilon$**. Notice that the main notion\nof an $\\varepsilon$-stationary point considered in this paper is\nbased on Definition 2 (p. 6), but Theorem 1 and 2 do not pick $\\beta$\nand $\\lambda$ properly to ensure that its solution is such a point,\ni.e., we need $\\sqrt{m}\\lambda/\\beta=O(\\varepsilon)$. This detail\nis especially important as the assumed Lipschitz constant in Assumption\n1 (p. 5) $L$ depends on both $\\beta$ and $\\lambda$, e.g., if $\\beta=O(\\varepsilon^{-1})$\nthen $L=O(\\varepsilon^{-1})$, and the final complexity bounds depend\non $L$. I recommend making an explicit choice of $(\\beta,\\lambda)$\nand writing the final complexity bound in terms of obtaining an $\\varepsilon$-stationary\npoint as in Definition 2 (instead of Definition 4 as in Theorems 1\nand 2).\n\n2. **Choosing $T$**. The definitions of DSZOG and ADSZOG require\nthe user to pick $T$ before running the algorithms. However, the\nactual convergence theory for the algorithm require that $T\\geq \\max(C_{0}[g(w_{0})-g(w_{T})],C_{1})$\nfor some computable constants $C_{0}$ and $C_{1}$. As far as I can\ntell, this requirement cannot be checked in practice, since it requires\nthe user to know the value of $g(w_{T})$ for any $T\\geq0$. The authors\nshould make some remarks about how this can be done in practice, and\nin particular, how this is done in their presented experiments.\n\n3. **Acceleration**. I believe the use of the words accelerated/acceleration\nin this paper are a severe misnomer. To elaborate, note that the only\nplace where acceleration is claimed is in the updates of $z_{w}^{t+1},$$z_{p}^{t+1}$,\nand $H(\\cdot)$, and the only commonality between these updates and\nthose in the classical scheme of Nesterov is that they both apply\na convex combination of the previous iterate with some auxiliary point.\nTo achieve accelerated convergence, Nesterov employs a very specialized\nchoice of interpolation constants, while, in contrast, the authors here allow these\nconstants to be arbitrary. Furthermore, it seems that this kind of\n\"accelerated\" scheme induces an even worse complexity bound of\n$O(L^{6}/T)$ compared to the \"unaccelerated\" complexity bound\nof $O(L^{5}/T)$ (see Remark 3 on p. 7)! I recommend that the authors\neither remove any reference to acceleration, e.g., replace it with\nsomething more relevant like \"variance-reduction\", or present\na suitable justification for why their method is \"accelerated\"\nin a rebuttal.\n\nAside from the above points, I also have a few minor issues with some\nof the other material in the paper:\n\n1. [p. 6] What are the relationships of $\\epsilon_{1}$, $\\epsilon_{2}$,\nand $\\epsilon_{3}$ to $\\epsilon$ in Definition 2?\n2. [p. 6] I was unable to find Proposition 1 in (Wang et al., 2020).\nCan the author give a more specific location for this result, e.g.,\npage number and/or proposition number in that reference?\n3. [p. 6] What does $\\epsilon_{2}$ mean in Proposition 1?\n4. [p. 7] The ADSZOG bound in Remark 3 is actually worse than the\nDSZOG bound. Is this a typo?\n5. *Minor typos*:\n- [p. 1] ... friendly {**missing word?**} of approximating ...\n- [p. 2] ... These **make** the existing methods time-consuming\n...\n- [p. 3] The sentence following the definitions of $G_{\\mu}^{f}$\nand $G_{\\mu}^{\\varphi}$ is not grammatically correct.\n- [p. 6] Missing some condition on $g(w)$ in the last part of Theorem\n1, before the \"i.e.\".",
            "summary_of_the_review": "This paper is overall well-written and contains some very promising\ncomputational results. However, in its current state, I cannot recommend\nacceptance due to several major issues related to clarity, the presented mathematical\nbounds, and the improper use of the word \"accelerated\".",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}