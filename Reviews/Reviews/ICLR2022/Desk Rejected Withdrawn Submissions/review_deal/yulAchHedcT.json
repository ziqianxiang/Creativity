{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper propose a new way of using sparse oblique trees (optimized via Tree Alternating Optimization) to compress the neural networks. The compression is conducted on the last fewer layers of fully connected layers via replacing those with tree structure. Experimental results with VGG16 as backbone networks on CIFAR10 and LeNet5 as backbone networks on MNIST shows that the compression method reduce the inference time with marginally impact on model prediction accuracy. ",
            "main_review": "It is interesting to see model compression with tree based structures. However, some of my concerns are\n \n[1] The method proposed in this paper is similar to that in Deep Neural Decision Forests, ICCV 2015, which end-to-end method to train a networks with trees included. Although the goal of deep neural decision forests is not for model compression, comparison might be needed to ensure fine tuning pertained model with sparse oblique trees is better than other bench marks.",
            "summary_of_the_review": "Overall, the paper propose to compress neural networks last few layers with sparse oblique trees to do model compression, and the sparse oblique trees are shown to be better than classical random forest or XGBoost methods. The idea makes sense and is easy to follow. However, the idea itself seems similar to that proposed in  Deep Neural Decision Forests. It is recommended to compare the results in this paper to the literatures,  clarifying  the difference and demonstrating the improvements. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper investigates replacing full connected layers and potentially some of the later convolutional layers of a neural network with an oblique decision forest to improve inference times and reduce number of parameters while retaining accuracy of the original model. To achieve this the TAO algorithm (previously published) is used with the output of the retained part of the neural network as input features to the decision trees and the ground truth label as the target outputs. Therefore, the proposed work is actually more a hybrid neural net+decision forest training than neural network compression. Experiments are performed on the MNIST and CIFAR10 datasets using LeNet, VGG and ResNet architectures to demonstrate improvements in inference time and number of parameters.   ",
            "main_review": "The use of hard/sparse decision trees rather than soft trees to improve inference time is intuitive. It also appears that there is little work using hard decision trees in neural network compression. On the other hand, the motivation of the paper which revolves around neural net compression and teacher-student approaches is misleading in my opinion because in the end the method is a hybrid model training. It is stated that the original ground truth labels rather than the output of the teacher (neural net) is used too train the forest. I do not think this fits in the teacher-student paradigm. The potential advantages of using the neural net output as the teacher, e.g. soft outputs rather than 1-hot encoding, is not taken advantage of. Is the reason that the TAO algorithm needs hard labels? Furthermore, the hybrid model is not trained end-to-end because the decision forest part is not differentiable. \n\nExperimental results demonstrate a clear improvement in inference times and number of parameters for the simpler LeNet and older VGG models. However, the experiments using ResNet are limited and show a >1% accuracy drop on CIFAR10 at best. Furthermore, it is not clear whether the demonstrated results will hold in larger problems with more classes and more complexity such as ImageNet.   ",
            "summary_of_the_review": "While the idea is interesting and neural network compression is an important topic, I think that the paper needs to be further developed before it is suitable for publication. Experiments with more complex datasets are needed to demonstrate that the proposed gains generalize beyond simple datasets. I also believe the motivation for the paper should be clarified to state that it is a computationally efficient hybrid model rather than neural network compression. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper uses the Forest of Sparse Oblique trees in a teacher-student approach to replace several fully connected layers of a neural network. Experiments are limited but show promise in increasing inference speed without much decrease in accuracy.",
            "main_review": "Strengths:\n1. The method is simple and easy to understand.\n2. By replacing seven layers of VGG16 with the forest, a 3000x speed increase in inference has been observed.\n\nWeaknesses:\n1. Novelty is limited, being only in the way the forest of trees is used to replace the neural network layers.\n2. The applicability of the method seems to be limited to replacing FC layers since experiments are shown only on VGG and LeNet architectures, and not ResNet or DenseNet, or other more popular architectures. How does it work on those architectures? There are not too many FC laters to replace there.\n3. Experiments are limited. Some key experiments are missing:\n    - How does the method compare with replacing the seven layers with only two FC layers trained in a student-teacher setup? Maybe a smaller NN could do just as well in replacing those 7 layers.\n    - How does the method work on CIFAR100 or Imagenet?",
            "summary_of_the_review": "A simple method that shows some promise, but has limited experiments.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper presents a way to build more computationally-efficient vision models by replacing components of convolutional neural networks (CNNs) with forests of sparse oblique decision trees. The sparse oblique decision tree method TAO (Carreira-Perpiñán & Tavallali 2018) is used, and it generates trees where the split notes are a linear combination of a small subset of the features generated by the earlier CNN layers (which are kept). The forests replace later layers of the Lenet5 and VGG-16 networks on MNIST and CIFAR-10 respectively.",
            "main_review": "**Strengths**\n\ni) Includes a mix of FLOPs count and wall-clock time experiments.\n\nThe authors give a fairly complete picture of the computational cost of their method. Plots show parameter count, FLOPs, and inference time. Units for all of these are well-chosen for clarity.\n\nii) Show nicely complete cost/error tradeoff curves in Fig 2 & 4.\n\nFigs 10 & 11 also show this well. The best way to test and compare methods for efficient ML inference is to look at the full tradeoff between speed and accuracy when varying the hyperparameters of the method. Most of the experiments in the submission, even ones that are looking at some other variable, are well-designed to show how it affects the full tradeoff/pareto frontier rather than only showing the effect on one of the dependent variables in isolation.\n\niii) Extensive searches on hyperparameters and architecture choices.\n\nThe authors search over both tree depth and sparsity, including multiple choices of their method's hyperparameters in many of their experiments. This often shows very interesting effects: compare for instance the effect of the sparsity parameter lambda in CIFAR-100 experiments in Figs 10 & 11 with the smaller effect in experiments on other datasets. The submission also has experiments looking at the effects of model architecture, which layers are replaced with the trees, and the sizes of the trees and forests along multiple dimensions.\n\n**Weaknesses**\n\niv) Out-of-date baseline networks.\n\nThe experiments are somewhat lower quality due to the use of very out-of-date and weak baseline for the main experiments. VGG-16 is fairly far behind the state of the art in neural networks. It is also extremely overparameterized, especially in the fully-connected layers, So reductions in the computational cost of VGG-16 (in both parameters and FLOPs) are fairly easy to achieve relative to standard relatively modern networks such as MobileNets or even ResNets.\n\nAdmittedly the style of experiments that the authors use--which is a good choice, see (ii)--does need to search a lot of parameters, so the highest-cost state-of-the-art models in the literature might not be a good fit for these experiments. Though I doubt this is the case: the authors state in the \"Training time of a sparse oblique forest\" paragraph that they \"do not ned to train any neural networks,\" so I do not see how it would be infeasible to use a different baseline network. But either way, there are more modern networks that I'd expect to be *less costly* for the authors to use in their experiments than VGG-16, especially MobileNets, Squeezenet, or EfficientNets.\n\nAside from the main experiments, the appendices include one experiment on ResNet-56. Unfortunately, this presents a single datapoint rather than the full speed/accuracy curves in the main paper. There's also similarly a single-datapoint experiment for a \"low-rank compressed VGG-16\" at the end of Section 5. The authors' method does yield significant speedup, and has both lower cost and lower error relative to some of the other tree-based baselines such as XGBoost.\n\nTaken together, I'd describe these as fairly good proof-of-concept experiments. But really the ideal experiment should have all these elements in one: with the strong experimental procedure from (ii) and (iii), but used on networks that are better choices for a baseline. The single-datapoint experiments, as in the appendix, *suggest* that the method will also scale beyond very small-scale/easier compression targets, but do not fully show it.\n\nv) Lack of very simple baselines.\n\nOne of the main differences we might see in experiments on more modern networks, too, is that they are often designed to have some built-in scaling in their architectures. See for instance \"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\" by Tan & Le that includes a system for scaling network properties such as the channel count (a.k.a. \"network width\") to get networks of varying cost/accuracy. MobileNets have similar scaling choices.\n\nThis makes for an important baseline for work like this submission. Since one might ask not only whether the trees can lower cost relative to one default choice for the network, but also whether these savings will preserve more of the accuracy as compared to simply removing some units or channels from the same ordinary CNN network architecture. I.e. a gold standard baseline would compare the some of the cost-accuracy curves from Fig 1 in Tan & Le (or Fig 6, for CIFAR-10) to the authors' cost/accuracy curves of varying lambda and T. Roughly eyeballing the comparison between Fig 6 of Tan & Le vs Fig 4 of the submission, I don't see a clear advantage either way, but it's extremely hard to tell given a lot of the other differences (such as the baseline parameter count & error rates being extremely different anyway).\n\nInstead, this submission compares mainly against other tree-based methods applied in the same way. So the key top-level idea, of replacing later network layers with a hard decision tree, is not being compared across the full cost/accuracy curve with a baseline. Instead, the authors compare 3 tree approaches within this same top-level framework, and compare them with the single datapoint of the reference FC/conv layers from VGG-16. This is a weaker experiment than the most recent baselines for efficient neural networks such as Tan & Le.\n\nvi) Considers a limited set of benchmark tasks.\n\nThe main experiments are on MNIST and CIFAR-10. These are relatively \"saturated\" datasets with modern networks, as in (iv), so I'd describe these more as mostly proof-of-concept experiments. The CIFAR-100 experiments are one of the more complete sections in the appendix, though. ImageNet experiments are extremely common in published neural network compression literature these days; this submission doesn't include a task of similar difficulty. It doesn't look at tasks other than image classification, as in the highest-quality recent papers in this space.\n\nI'd rate that the CIFAR-100 experiments place the submission just over the minimum bar for acceptance in this respect.\n\n**Reasoning**\n\nMost of the individual ideas have been seen before in the literature with different permutations: replacing part of a neural network with a tree, reducing cost with conditional computation, and the tree inference itself that forms the main component. (The submission gives a good description of this prior work and how the authors' method differs.) The main contribution is how the authors assembled the ideas into this particular combined whole. This is valuable if and only if this is really the right way to combine the ideas: something that must be shown in the experiments.\n\nThe experiments taken together potentially show this, but it's really unclear if so. The experiments that follow a more complete procedure (with good hyperparameter searches, and full comparisons showing the speed/accuracy pareto frontier of mutiple methods) are on poor choices of network and benchmark. The experiments with better networks and datasets are less complete and are in somewhat disjoint pieces throughout the appendix. It lacks a solid single experiment combining the strengths of all of them, without a clear reason why.\n\n**Minor Comments**\n\n  * With this citation style, I've usually seen it in parens, and that's the case in the other ICLR 2022 submissions I'm reviewing. Without them, it's sometimes a little unexpected to see it in the sentence.\n  * \"to a leaf which outputs its class\" -> \"to a leaf that outputs its class\" on Page 3\n  * Odd grammar in \"a relatively small and shallow trees\" on page 4. Maybe meant to read \"a [forest of] relatively small and shallow trees?\"\n  * Suggest \"in a similar to FLOPs manner\" -> \"in a manner similar to FLOPs\"\n  * \"compression of softmax layer of VGG16\" -> \"compression of [the] softmax layer of VGG16\"\n  * Should remove comma from \"The TAO training itself, is parallelized on...\" on Page 7",
            "summary_of_the_review": "This is a paper that rests primarily on the quality of its experiments. The experiments do very clearly show that the method works, and how, but they don't really make a compelling case that the method work *well* or is definitely useful. Note this is more than a \"doesn't achieve SOTA\" complaint: I can't easily tell if the method clearly does better than fairly minimal and simple baselines.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}