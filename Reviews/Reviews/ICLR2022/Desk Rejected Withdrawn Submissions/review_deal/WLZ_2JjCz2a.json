{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a model compression scheme for GANs that adapts the recently introduced In-Time Over-Parameterization (ITOP) method to GANs, which allows sparse models to be trained from scratch (as opposed to the conventional dense training + pruning strategy). It is demonstrated that the proposed Sparse Unbalanced Training GAN (STU-GAN) outperforms competing compression techniques in terms of FID given the same compression rate.",
            "main_review": "**Strengths**  \n- Paper is reasonably well written and easily understood.  \n- Introduces some necessary tweaks to get ITOP working well with GANs, such as modifying weight EMA to work in the dynamic sparsity setting.  \n- Extensive parameter sweeps over sparsity ratios and other hyperparameters.  \n- Proposed STU-GAN outperforms competing model compression methods in terms of sample quality at equivalent compression ratios.  \n- Sparsifying the model during training could make GAN training much more efficient, which could reduce the computational barrier for training large GANs.  \n\n**Weakensses** \n- In the abstract it is mentioned that the proposed strategy chases high training efficiency gains, but these gains are never evaluated in terms of saved training time, reduced memory cost, or MACs. I think the computational savings during training is potentially a very strong contribution of the paper and would be happy to see more analysis in this direction.  \n- Reported scores do not include confidence intervals so it is difficult to tell how significant the results are. In several cases the reported values for STU-GAN are very close to the baselines. \n- Some what limited novelty. The sparsification mechanism is pre-existing and the problem of training GANs with unbalanced generator/discriminator pairs is not unknown. The majority of the contribution comes from demonstrating that ITOP can transfer to GANs (which is certainly useful) and the exhaustive experiments conducted.\n\n**Other Comments**\n- The paper places much emphasis on the unbalanced aspect of sparse GAN training, since it causes instability in training. I am curious what would happen if the static sparse GAN where trained with DiffAugment [1] in the unbalanced case, since it is shown to combat instability during training?\n\n[1] Zhao, Shengyu, et al. \"Differentiable augmentation for data-efficient gan training.\" arXiv preprint arXiv:2006.10738 (2020).",
            "summary_of_the_review": "Overall, I think this is a reasonable paper. It explores the application of an existing model compression technique to GANs and conducts extensive experiments to verify that it outperforms alternative methods. However, I think there is a major missing aspect of this work, which is the analysis of computational savings. Computational savings in training time and memory cost is one of the main appeals of being able to train sparse models from scratch, so it would be nice to know how much performance we can expect to gain from this technique. At the moment my recommendation is a weak reject, but I would be willing to increase it later if this further analysis is added.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "- The paper explores the possibility of directly training sparse GAN from scratch without pre-training, pruning, and fine-tuning steps. \n- In particular, they start training a sparse subnetwork and periodically explore the sparse connectivity during training. \n- The parameter exploration is achieved by prune-and-regrow, also named in-time over-parameterization, a strategy proposed by Liu et al. 2021. ",
            "main_review": "Strengths:\n\nCompressing GANs is valuable for edge devices. \nAccelerating GAN training is also very appealing.\n\nWeaknesses:\n\n- However, although the author repeatedly emphasizes the value of sparse GAN, there is no quantitative indicator in the paper to prove that their method does bring obvious benefits.  Does training become more efficient? How much accelerated?\n- Novelty is limited. The prune-and-regrow strategy is proposed by others. This paper is just applying this strategy to GANs. After reading this paper carefully, I didn't get any new insights. \n- The experiment is not very solid. For example, the result of BigGAN in the paper is obviously worse than its original performance. \n- As shown in Figure 8, the image quality generated by the sparse generator is very poor, which limits the practical application of this method.\n- I suggest that the author give exact numbers of parameters for generator and discriminator rather than only providing the sparse percentage in Table 1 and Table 3. ",
            "summary_of_the_review": "As an empirical article for GANs, the current version is not very sufficient in terms of experiments.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work aims at training a sparse GAN with unbalanced sparsity between generators and discriminators, without involving any dense or pre-training steps. \n1) The work presents the intuition of the unbalanced sparse GAN training method, which the traditional appoarches are suffering from the unbalanced sparsity allocation with extremely sparse generators.\n2) This work proposes an method called STU-GAN. STU-GAN trains a sparse unbalanced GAN with an extremely sparse generator and a much denser discriminator from scratch without involving any expensive dense or pre-training steps. \n3) Experiment result show the impressive results of the method on GAN compression.",
            "main_review": "Pros\n- Overall good presentation and clear idea delivery with motivation - method - experiment - conclusion\n- This work makes an inspring study about the parameterization of discriminator in the intuition section, which is not just useful for clarifying the motivation of this work (unbalanced training), but also be benefit for other following works to explore this area as a starting point.\n- The STU-GAN method shows good result on the comparison with other standard GAN compression method. The sparse training from scratch can even outperform the iterative pruning method as the paper claimed.\n\nCons\n- The notation can be improved for the clear idea presentation. For formula 3), a clear definition of \n\"1/2\" for \\theta_{s, t+1/2} is needed to show why the step is not a real number. \n- Since the merits of STU-GAN is outperforming the pre-training and pruning methods, the experiment section can emphasize the comparison first and then discuss other observations as the ablation.\n- It will be benefit to show the reproducibility of STU-GAN. Since this is an unbalanced training, the stability of this presented method can be shown with the learning curves or repeated experiments. \n",
            "summary_of_the_review": "Overall good and clear presentation. This work proposes a novel GAN  sparse training method which do not need the pre-training or dense training predecessor. This work can be further improved by clear notitions and better arrangement of materials. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work proposes a dynamical GAN pruning algorithm from scratch. Specifically, they fix the sparsity for both D and G, then they use static sparse D, prune and regrow the connectivity for G.\nThey compare the empirical results with static sparse GAN and LTH on GAN.",
            "main_review": "Strength:\n\n1. The proposed pruning method can be trained from scratch. \n\n2. The work provides ablation studies to show the relation between G&D sparsity and performance. \n\nWeakness:\n\n1. I don't see much novelty in this work: ERK init, pruning based on the magnitude of parameters, and gradient redistribution are proposed by Evci 2020. I don't think migrating the existing methods from supervised learning to the GAN scenario is good enough. \n\n2. Compared with previous GAN running works, it doesn't bring me a new understanding of GAN pruning. \n\n3. The improvements are marginal over the baselines (Tab. 1)\n\n4. Tab. 3 misses the comparison with sparse GAN baselines. ",
            "summary_of_the_review": "I personally don't recommend this paper.\n\nMainly for two reasons: 1. The lack of novelty for both algorithm and understanding on GAN pruning. 2. Marginal empirical improvements. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}