{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a novel and more expressive model TaCE for temporal knowledge base completion. TaCE has 4 modules - the Temporal Convolution module (generates a time-aware representation of (s,r) or (r,o) depending on the query), Static Convolution module (generates a time-independent representation of (s,r) or (r,o) depending on the query), Deep Learning module (output of previous two modules is passed through various fully-connected layers), and Link prediction module (this final module takes input from deep learning module and scores all entities on their likelihood of being the correct answer to the query). The superiority of the model is demonstrated via extensive experimentation.",
            "main_review": "Easy to follow paper. Proposed model's effectiveness is demonstrated via extensive experimentation. However, I have the following concerns.\n\n[Title of the paper]: Claims TaCE does Temporal Knowledge \"Reasoning\" but the text don't justify the claim.\n\n[Experiments]:\n0. Qualitative study on what type of cases TaCE performs better than the competing model will strengthen the claim of the need for a more expressive model. \n1. Table 3: TaCE w/o temporal component performs better than ConvE, while the underlying principle of both the models is similar. Why is TaCE better? Comments?\n2. Any specific reason for not sharing entity and relation embeddings used in temporal and static modules? Comment on the performance, space, and speed requirements of the new model - TaCE. \nAlong with Table 2 and Table 4, also consider reporting the size of the corresponding models. ​\n3. \"Awareness of time ordering and consistency\": The heading of the qualitative study suggested that the authors are going to talk about how well does the model understands which relations come before another (a time-aware model should understand that a person has to be born before he becomes the president). The authors may consider re-evaluating their choice of heading.  \n4. One disadvantage of such a convolution-based model is that they can't naturally predict time while the discussed tensor-based models can. What are the author's thoughts? Is there an efficient way to do time prediction? And how well does the model do in this task?\n\n[Not clear]\n0. Figure 3, h_s and h_r\n1. \"TaCE leverages the rates of four metrics by at least 7-10% against its temporal opponents, \"\n\n[Minor]\n0. TaCE is not the first temporal model to use 1-N scoring. Paper [a] and [b] both used it.\n1. I found a lot of Arxiv articles cited in place of the corresponding conference article.\n a. García-Durán, Alberto, Sebastijan Dumančić, and Mathias Niepert. \"Learning sequence encoders for temporal knowledge graph completion.\" EMNLP 2018.\n b. Jain, Prachi, Sushant Rathi, and Soumen Chakrabarti. \"Temporal knowledge base completion: New algorithms and evaluation protocols.\" EMNLP 2020.\nand more.\nConsider using an updated bib.\n2. The figure (esp 2,3,5,6) captions should be more descriptive.",
            "summary_of_the_review": "New improved method for Temporal Knowledge Base Completion. However some understanding of why this model performs better over others (via experiments discussed above) will strengthen the paper.\nI also have some concerns about the writing (highlighted above).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a novel temporal knowledge graph embedding approach that is based on convolutional neural networks. To create a feature on which the link prediction is performed, the presented approach first creates both a static and a temporal feature (both in their own respective convolution modules) and fuses them together in a third deep learning module. The so generated general feature is then used to calculate scores for the missing element in a tuple (s,r,o,t). The experimental section provides a performance overview of the proposed TaCE approach on standard benchmark datasets for TKGC tasks (ICEWS{14|05-15} and GDELD). As seen in the evaluation, the proposed approach performs better or on-par with most recent SOTA approaches in the field. The result presentation is followed by a more detailed analysis of the learned temporal embeddings that appear to contain a good semantic representation of temporal order. An ablation study further shows the huge impact that the temporal embedding module has on the overall results.",
            "main_review": "+ the overall quality of presentation is very good. The paper and line of arguments are easy to follow\n\n+ the presented approach is the first to apply convolutional neural networks in the context of a temporal knowledge graph completion task\n\n+ the proposed approach generates better results on the benchmarks than most recent TKGE approaches\n\n+ the qualitative analysis of the temporal embeddings indicate that meaningful semantics are learnt\n\n- it is a bit unclear how the timestamps are represented initially; is the timestamp embedding a vector representation of a timestamp that is processed in the same way as the other entities (i.e. in a symbolic representation) or is there an additional step involved?\n\n- can you explain in more detail how section 5.4. shows the ability of TaCE to provide order? Surely the example shows that quadruples belonging to ground truth, i.e. that are facts in the respective KG are assigned a high score by TaCE, but I do not see how in this case this necessarily is an indicator that the model has learnt the sequential relation between the quadruples. I agree that section 5.3. provides strong indicators that the semantics of the timestamps are well represented by your approach, but to me it remained unclear how section 5.4. further supports that claim. A more detailed explanation could – in my opinion – help clarifying this question\n",
            "summary_of_the_review": "The authors present TaCE, an approach for TKGC tasks that is based on the usage of convolutional neural networks. The novelity and the shown effectiveness of their approach plus the qualitative analysis of the temporal embeddings make this paper a clear accept. Not only do the authors provide a performant approach, but they also aim to explain the effectiveness of their proposed approach.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes to use timestamps as temporal convolution filters in embedding learning for knowledge graph completion task and shows good results compared with baseline models on different public datasets.",
            "main_review": "Strengths:\n1. The idea of using timestamps as convolutional filters is new and hasn't been tried before. \n2. The result on 3 public datasets compared with baseline models looks promising.\n3. The embedding visualization shows that the the model learn the semantic of temporal information in the knowledge graphs\n4. The training efficiency is good compared with the strong baseline TeMP.\n\nWeaknesses:\n1. The model only works for discreet times due to the convolution and the proposed model structure, which limits the ability of real-world applications where a lot of temporal knowledge graphs have temporal intervals. A lot of recent papers also address temporal intervals in temporal knowledge graphs\n2. What about knowledge graphs with incomplete temporal information, e.g, some triples have temporal information while other triples are missing temporal information. Can the model handle these knowledge graphs? How is the performance?\n3. The paper seems to be missing the time prediction task that a lot of existing temporal knowledge graph embedding models evaluate on, for example, as reported in this paper (Temporal Knowledge Base Completion: New Algorithms and Evaluation Protocols).\n4. The experiment could be more convincing if it could include other knowledge graph datasets, such as wikidata12k and yago11k/yago15k.",
            "summary_of_the_review": "The proposed method using temporal information as convolution filters is interesting and new. There's still room to address some of the weaknesses, such as the need to compare on more datasets, more evaluation tasks, and a more general notion of temporal information.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a time-aware convolutional embedding learning for temporal KGC. It converts timestamps as temporal convolutional filters fo fully interact with entities and relations. Overall, this paper is well written. ",
            "main_review": "Strengths: \n1. The paper is well written and is easy to follows. \n2. The authors conduct extensive experiments to verify the effectiveness of the proposed method. \n\nWeakness:\n1. The technique novelty is limited. Converting timestamps as temporal convolutional filters to fully interact with entities and relations is a simple way.\n2. Experimental results are not enough. Wiki and Yago are two popular datasets for TKGE. The authors do not provide the results on these two datasets.\n3. The codes are not available, it is difficult for others to re-produce the results reported in this paper. ",
            "summary_of_the_review": "This paper presents a time-aware convolutional embedding learning for temporal KGC. It converts timestamps as temporal convolutional filters fo fully interact with entities and relations. Overall, this paper is well written. \n\nStrengths: \n1. The paper is well written and is easy to follows. \n2. The authors conduct extensive experiments to verify the effectiveness of the proposed method. \n\nWeakness:\n1. The technique novelty is limited. Converting timestamps as temporal convolutional filters to fully interact with entities and relations is a simple way.\n2. Experimental results are not enough. Wiki and Yago are two popular datasets for TKGE. The authors do not provide the results on these two datasets.\n3. The codes are not available, it is difficult for others to re-produce the results reported in this paper. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}