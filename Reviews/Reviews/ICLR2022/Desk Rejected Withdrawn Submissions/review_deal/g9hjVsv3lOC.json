{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper uses a Gramian Angular Difference Field to convert the continuous EEG time-series data into images. The resulting images are used to predict attention scores using computer vision architectures such as CNN.",
            "main_review": "Strengths:\n- The paper is written well and builds upon prior work to propose a method for predicting attention.\n\nWeaknesses:\n- Gramian Angular Difference Field is a common method to convert time series to images and does not pose a novel method.\n- The same subjects are used in training and validation set which should cause overfitting and overestimation of performance metrics.\n- Accuracy is not reported. It was one of the metrics in the original paper introducing the dataset.\n- MAE is very close to the reported result in the original paper introducing the dataset.\n- The proposed method is only compared with this paper.\n- The paper mentions an improvement of .22 but that is over a scale of 30 so the improvement is less than 1%.",
            "summary_of_the_review": "Overall,  the paper proposes a method based upon prior work and does not contain any significant innovation. The main concept of the paper is a common phenomena (using Gramian Angular Difference Field to convert time series to images). The results are only incrementally better than the paper introducing the dataset and moreover, it is not compared with any other dataset. The improvement reported is less than 1% over the baseline. There is also a lack of metrics (only one is used in the paper). Also, the paper combines subjects before splitting into training and validation. Thus, I cannot recommend the paper for acceptance at ICLR.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a approach to estimae auditory attention from EEG data in human subjects conducting noise masking tasks. The manuscripts has several ethical and methodological problems as outlined in following sections of the review. ",
            "main_review": "The manuscript has the following problems:\n\n1. EEG dataset downloaded from an internet repository was recorded while violating basic ethical rules for experiments with human subjects. There is no trace of ethical or IRB review of those original experiments or any consent from the subjects. Some subjects were under-consent-age, which is an even bigger problem and a definite reason for the manuscript rejection.\n\n2. The authors did not provide detailed information about experimental conditions (EEG amplifier type, EMG noise prevention, etc.). The original experiment seems related to noise masking, only associated with the broad auditory attention concept. The formula for auditory attention appears to be more connected to short-term auditory memory and recall and not much to awareness, which is another weakness of the approach.\n\n3. On the EEG processing side, the authors completely ignored EEG preprocessing, so a reader cannot guess whether EMG/EOG is used in the final regression steps. Since the EEG traces were converted next into \"images,\" which procedure was susceptible to non-EEG artifacts and mainly in a case of a wearable amplifier, which usually delivers mostly EMG and EOG, an omission to discuss such a critical point is problematic.\n\n4. The machine learning models chosen to process \"EEG images\" are somewhat regular and poorly explained. Since the original dataset was tiny, why was no single subject out cross-validation not conducted to prevent the overfitting problem and prove the proposed approach strength? The current results look overfitted, and MAE scores are not that great. \n\n",
            "summary_of_the_review": "To summarize, the proposed approach to auditory attention estimation was presented using an ethically illegal dataset recorded from underage subjects without experimental consent from legal guardians and with confusing experimental settings (acoustic noise masking and short-term memory evaluation instead of auditory attention, for which cocktail-party setting would be probably the best choice). The proposed machine learning models were regular and freely available, further killing any novelty. A chosen 12-fold cross-validation probably did not prevent overfitting, and a single subject out cross-validation would probably defend the approach better.\nUnfortunately, the paper shall be rejected. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "Yes, Responsible research practice (e.g., human subjects, data release)"
            ],
            "details_of_ethics_concerns": "The manuscript violates ethical standards related to experiments with human subjects. The authors use EEG data from experiments with minors without any apparent ethical committee or IRB approval. The authors only stated in their submission:\n\"In this paper, we have used the PhyAAt dataset Bajaj et al. (2020), which contains the physiological responses of 25 subjects, collected from an experiment on auditory attention. The chosen subjects were university students aged between 16 to 34. All participants were non-native English speakers, out of which 21 of them were male and 4 were female.\"\nThere is also no ethical approval or IRB information in the paper available on arXiv either.",
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper presents a deep learning framework for detecting auditory attention using EEG data. Towards this, image data is generated from EEG signals acquired using a 14 channel device to enable the application of 2D- and 3D- convolutional neural networks. The performance Is evaluated using the mean absolute error, where the authors report a significance improvement over exiting studies. Hence, the major objective of this study is to predict auditory attention score using EEG data.",
            "main_review": "A lot of research is invested toward using non invasive signals such as EEG to predict various aspects of human behaviour including emotions, stress, and engagement level. On similar lines, this particular study aims to predict auditory attention using EEG data, where automation can be achieved using machine learning algorithms. It is indeed a significant area, where humans can benefit from the advances in technology. \n\nHowever, the authors are not able to celery justify the contribution of this work. Converting 1-D data to images has been used for sometime now to enable using 2D-CNN networks. The method used here is already being used in previous studies and the data collected has also been taken from a previous study. More details are needed for reproducibility in terms of data collection process, which 14 channel EEG device was used? How can the diversity of participants involved in the study be verified? Is there any domain expert involved in the study to verify the reliability of attention scores?\n\nAre the electrodes used non-invasive or invasive? since the authors claim that the electrodes were planted in to the head.\n\nThe values in terms of MAE are only showing a marginal improvement and there is no justification or discussion to back up why 2D-CNN gives better results than 3D-CNN and autoencoders. ",
            "summary_of_the_review": "The major limitation of this study is the lack of novelty in terms of study design and choices for methods used. There is no ablation study to justify the model parameters selected and hence it is difficult to establish the significance of these results. The authors also need to focus on the usability of such a design, the cocktail problem has generally been solved using blind source separation (BSS), where signals processing techniques can be used to separate various sources of noises even in hearing aids. What use cases arise where attention score related to brain activity will play an important role? Just counting the correct words out of total words to come up with a attention score is too subjective in nature and better parameters (from BSS) can be used to further justify these results. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "Yes, Other reasons (please specify below)"
            ],
            "details_of_ethics_concerns": "ICLR follows a blind review process, and the authors have submitted the manuscript on arXiv, hence not following the submission guidelines. ",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes an evaluation of different machine learning architectures to predict auditory attention from EEG signals. The approaches are evaluated on the Phyaat dataset. The time-series signals are transformed to the image domain using Gramanian Angular Difference Fields.",
            "main_review": "Strengths\nThe paper addresses a challenging problem.\n\nWeaknesses\n- The motivation for the work is not strong. The paper claims that since the world is moving towards automation, the proposed problem is well justified. The paper does not make a clear connection between automation and the need to predict auditory attention from EEG signals.\n\n- Similar work has been done before, as noted in the related works. Krishnan et al. 2020 is very similar to this paper. It is not clear how the propose evaluation extends the state of the art.\n\n- Data pre-processing using GAF (Section 3) is not new/novel. No new approach for the proposed problem is presented. It is using well-known techniques for pre-processing/evaluation.\n\n- At the end of Section 3, the paper states \"A way to deal with this was to pad all those images so that they become of the same size. However, we refrained from doing so as the padding might result in the loss of information from some images, possibly due to the variable image sizes. Instead, the models were designed in a way that they can incorporate images of different input sizes.\"  What does the last sentence mean? How were they designed in a way to handle different input sizes?\n\n- In Section 4, it is not clear if the experiments are subject-independent. This sections also claims that the experimental design was done as not much work has been done on this dataset. That motivation is not strong. It is also not clear why the training and testing data were not fixed. What exactly does this mean? Is this referring to 12-fold cross validation?\n\n- Sections 4.1 - 4.3 have a lot of details on well known methods (e.g., 2D CNN). It is not clear why so much detail/space is given to these. What is the motivation for using a random forest and XGBoost? The paper discusses these are the most promising regression models. What is this based off of? For what types of applications and what type of data?\n\n- It is unclear why there is a section for explaining MAE (Section 5). This is a common evaluation metric.\n\n- In Table 1, why is 2D CNN bold, when Bajaj et al performs better? The reasoning for the lower performance (i.e. resources) is not sufficient for a reason. It is not justified that more resources and a deeper model would give better results. This has to be validated through experiments.\n\n",
            "summary_of_the_review": "The paper addresses a challenging problem, however, the paper is largely an evaluation of machine learning architectures on one dataset. The original dataset paper outperforms any of the evaluations. Nothing new/novel is proposed in the paper. Motivation is lacking for why the problem is being addressed, why the architectures were chosen, and why the results perform worse than state of the art. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}