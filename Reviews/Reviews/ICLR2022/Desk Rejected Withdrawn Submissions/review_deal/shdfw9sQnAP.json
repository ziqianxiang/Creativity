{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This work proposes a robust FL framework that combines knowledge distillation (KD)-based FL and RobustFilter. The main benefits include: 1) KD can mitigate the \"curse of dimensionality for privacy and security\" by only transmitting a prediction vector rather than the entire model; 2) KD also does not require the model architecture to be the same for all clients. \nThe proposed framework is evaluated on FOUR datasets: Purchase, CIFAR10, SVHN, and MNIST. The model architecture are small-scale models that only contain a few CNN/FNN layers. \n",
            "main_review": "# Strengths\n1. The authors point out the benefits of the combination of KD-based FL and RobustFilter.\n2. Multiple attack baselines are evaluated.\n\n# Weaknesses\n1. I cannot see any ML-related contribution, given that knowledge transfer-based FL is also proposed by [1] and [2]. In other words, this paper only focuses on improving the robustness against poisoning attacks under an existing learning framework. It's better to submit the defense method to a security-related conference/journal. Even for the robustness-related algorithm, RobustFilter is not proposed by this work. So overall, this paper does not have any primitive novelty. \n\n2. the Introduction doesn't discuss important related works in KD-based FL and defense methods\n(1) \"federated learning cannot be used for aggregating heterogeneous models\": this is not true. Please check [1] [2] [3], they can also obtain personalized model architecture at the edge.\n\n(2) Due to lack of discussion of related works, it's unclear to readers whether \"Cronus is the only secure method that withstands the parameter poisoning attacks\". It seems many related works are not discussed. To help authors to improve the presentation, I provide a list of defense methods in FL at [D1-12]. [D-10] can also enhance robustness against model poisoning attacks. It's better to discuss the difference and do not overclaim the proposed method as the only secure method.\n\n3. This paper is not well-written. \n(1) the first two paragraphs are very redundant, having zero information to FL researcher. \n(2) \"Yet, they can be of heterogeneous architectures or even different families of machine learning algorithms.\" -- I am not sure whether KD \ncan support different ML algorithms. Please cite related works to backend this sentence.\n\n4. the source code is not provided for review.\n\n[1] Group Knowledge Transfer: Federated Learning of Large CNNs at the Edge. NeurIPS 2020\n[2] Ensemble distillation for robust model fusion in federated learning. Advances in Neural Information Processing Systems, 33, 2020.\n[3] HeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients. ICLR 2021\n\n\n\n----related works in defense---\n[D1] RFA: Robust Aggregation for Federated Learning\nhttps://github.com/krishnap25/RFA \n\n[D2] The Limitations of Federated Learning in Sybil Settings\nhttps://github.com/DistributedML/FoolsGold \n\n[D3] Attack-Resistant Federated Learning with Residual-based Reweighting\nhttps://github.com/fushuhao6/Attack-Resistant-Federated-Learning \n\n[D4] Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates (ICML 2018)\nhttps://github.com/LPD-EPFL/ByzantineMomentum/tree/master/aggregators \n\n[D5] Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent (NeurIPS 2017)\nhttps://github.com/LPD-EPFL/ByzantineMomentum/tree/master/aggregators \n\n[D6] Distributed Momentum for Byzantine-resilient Stochastic Gradient Descent (ICLR 2021)\nhttps://github.com/LPD-EPFL/ByzantineMomentum/tree/master/aggregators\n\n[D7] The Hidden Vulnerability of Distributed Learning in Byzantium (2018 ICML)\nhttps://github.com/LPD-EPFL/ByzantineMomentum/tree/master/aggregators\n\n[D8] CRFL: Certifiably Robust Federated Learning against Backdoor Attacks (ICML 2021)\nhttps://github.com/AI-secure/CRFL \n\n[D9] DiverseFL: https://arxiv.org/abs/2010.07541v3 \n\n[D10]FL-WBC: Enhancing Robustness against Model Poisoning Attacks in Federated Learning from a Client Perspective. NeurIPS 2021\nhttps://arxiv.org/abs/2110.13864 \nhttps://github.com/jeremy313/FL-WBC \n\n[D11] Revealing and Protecting Labels in Distributed Training. NeurIPS 2021\nhttps://arxiv.org/pdf/2111.00556v1.pdf\nhttps://github.com/googleinterns/learning-bag-of-words \n\n[D12] SLSGD: Secure and efficient distributed on-device machine learning. ECML&PKDD2021\nhttps://arxiv.org/pdf/1903.06996.pdf\n",
            "summary_of_the_review": "Given that both KD-based FL and RobustFilter are already proposed by existing works, and there is no ML-related contribution, I suggest NOT accepting this paper at the current version.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper addresses the security, privacy, and heterogeneity issues of parameter-sharing based federated learning (FL). The key observation is that the error rate of FL aggregation algorithms depends on the size of parameters to be shared between clients and the server, and this size is very large for deep models. Together with the inspirations from knowledge distillation and knowledge transfer, the authors propose to share the predictions of local clients to the server instead of sharing the parameters of local models. The technical solution is directly based on RobustFilter, a robust mean estimation aggregation algorithm. The core working mechanism is that the sample complexity of RobustFilter is now depending on the size of predictions (which is typically small) rather than on the size of parameters (which is very large). By pipelining these components, the proposed Cronus model is robust, against the attacks, and heterogeneous architectures.",
            "main_review": "++ Traditional federated learning faces many issues, and this paper proposes their ways of solutions for these issues.\n\n++ well written and good organization (but Sec 3 is so short and there is only one subsection for Sec 4.1/  Sec 5.1 and there is no 4.2/ 5.2), easy to follow.\n\n-- The proposed method has not much technical contribution. The aggregation rule is not new (RobustFilter); reducing overloads of local clients and transferring their distillation knowledge to the server is not new (He et al., 2020); using a public dataset to transfer prediction vectors is not new (Li & Wang, 2019). The authors propose a new attack called OFOM, but this is not enough for an ICLR paper in its current version. This paper could be a good one to describe a deployed FL system for industry applications.\n\n-- Many choices are not clearly stated. In algorithm 1, for the initial phase, why not train on the public dataset first? For the collaboration phase, the size of the public dataset D_p is typically much larger than that of the private dataset D_i, then what is the logic to merge D_p and D_i to train the local models? What is the influence of merging two imbalanced datasets?\n\n-- The benign accuracy is not good for the proposed model, especially on the CIFAR10 dataset, losing 9 points.\n\n-- The authors purposely designed the OFOM attacks for the existing methods, but there is no intentionally designed attacks for the proposed model. Is the proposed model perfect? What possible weakness of it? Is the comparison fair?\n\nSmaller ones:\n- In Table 1, adding a column for sample complexity and adding a row for MWU (Arora et al., 2012)\n\n- (Geoffrey & amd Dean Jeff,2014) --> this ref format is wrong\n\n\nReferences:\n[1] He et al., Group Knowledge Transfer: Federated Learning of Large CNNs at the Edge, NeurIPS 2020\n\n[2] Li & Wang, FedMD: Heterogenous Federated Learning via Model Distillation, NeurIPS 2019 Workshop on Federated Learning for Data Privacy and Confidentiality \n",
            "summary_of_the_review": "Not novel, the contribution is not enough for ICLR, accuracy losing much in benign setting, missing related works",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper introduces Cronus, a collaborative learning framework where the different parties participating in the collaborative learning task share with a central node (server) information about their local model’s predictions evaluated on a public (unlabeled) dataset. Then, the server computes the robust mean for those predictions using the algorithm in Diakonikolas et al. 2017 and send these values to the parties, which re-train their local models using their own private datasets and the predictions sent by the server on the public dataset. The authors argue that this approach is more suitable for defending against poisoning attacks in federated learning and that it reduces the risk exposure to privacy attacks. ",
            "main_review": "The large number of parameters of the neural network architectures that are commonly used for many applications limits the capacity of robust aggregation techniques to defend against poisoning attacks in federated learning. The authors aim to address this problem by reducing the dimensionality of the information shared with the server by means of the unlabeled public dataset. Although the motivation is interesting, I find important limitations in the proposed approach. \n\n1) Compared to federated learning, Cronus does not produce a unique collaborative model for all the parties, but every party has its own model (which can have different architecture). This can be limiting in some application domains where the parties may be interested in sharing a global model with the same performance. On the other side, the performance of the resulting model for each party depends heavily on the distribution of the local dataset and the public unlabeled dataset. However, this can result in a poor performance compared to more standard federated learning. For example, a given party cannot benefit from the information provided by other parties in regions of the overall data distribution that are not supported by its own local training set and the public data. This can be important, for example, in applications on security (attack detection) or healthcare, to cite some. In fact, from the results in Table 3, it can be observed that the performance of Cronus when there is no attack is reduced compared with the other competing methods. \n\n2) One of the objectives of federated learning is to train collaborative models in cases where the datasets from the parties are heterogeneous. However, Cronus is not designed with this purpose and it would fail, for example, in cases where the parties have only data with some class labels in their local training datasets. Actually, in the experiments, the authors do not consider any scenario with data heterogeneity, which has already been considered by a good number of papers in the research literature already. \n\n3) The robustness of Cronus has not been tested properly. None of the attacks used aim to compromise directly the design of Cronus. In this sense, for example, it seems that Cronus would be vulnerable to attacks where the malicious parties manipulate the local predictions evaluated on the public dataset that are sent to the server. \n\n\nOther comments: \n1) The experiments about the privacy are not well explained in the paper. The authors argue that Cronus reduces the risk against membership inference attacks, but what about property inference attacks as in Melis et al.? The fact that the server has access to the local predictions evaluated on the public dataset can leak information about the properties of the datasets used by the different parties. In this sense, I think that the conclusions drawn by the authors are perhaps too preliminary and the analysis of the privacy deserves further consideration. \n\n2) The structure and organization of the paper can be improved: the background in the first sections can be reduced. There are some points about Cronus and previous work that are repeated several times all over the paper. In contrast, for example, Section 3 is just a paragraph that does not give much details about the attacks proposed. Similarly, Section 5 (Experiments) could have been expanded and include some of the additional results that the authors are currently showing in the appendices. \n\n3) The experimental evaluation should be more comprehensive (see my comments before) and include cases with heterogeneous datasets. \n",
            "summary_of_the_review": "The proposed approach presents certain limitations for its use in practical applications, e.g. when the datasets from the different parties are heterogeneous, and there is a non-negligible loss in performance compared to other competing methods when there is no attack. On the other side, the evaluation of the attacks does not include attacks targeting the proposed method. The experimental evaluation and the organization of the paper need improvement. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "Cronus is another method for collaborative learning that combines knowledge transfer with a robust aggregation of updates (via the RobustFilter algorithm). The knowledge transfer allows replacing the aggregation of model parameters or gradients with the aggregation of softmax outputs as well as usage of heterogeneous models in the clients. The robust aggregation is used to prevent against poisoning attacks.",
            "main_review": "Strong points:\n1. Cronus is significantly **more robust against poisoning attacks** than other federated learning methods.\n2. The paper shows a new poisoning attack (OFOM) against MWU-based aggregations that substantially lowers the accuracy of the trained models (e.g., to almost a random model on CIFAR10).\n\nWeak points:\n1. The **novelty** of the paper is limited since it is an application of an already known robust aggregation technique - RobustFilter. Knowledge transfer has also been already proposed in federated learning (e.g., [1,3,5]).\n2. The framework assumes that an **unlabeled public dataset** from the same distribution is available to the server, a very strong assumption (similarly to: [2] - Please refer to the reviews). This makes the comparison with other methods, e.g., FedAvg more difficult to carry out. Moreover, clients train on combined private and public data (in the 2nd phase) but the private dataset has hard while the public data has soft labels. What if there is an overlap in the datasets? This requires a careful choice of the public dataset.\n3. The problem of **heterogeneous models** (also different classes of models, not only neural networks) has been addressed in other publications, for example, in [3]. This approach also does not assume that there exists a public dataset. It also aggregates hard labels, which does not require a robust aggregation algorithm, such as RobustFilter.\n4. **Robustness vs Benign accuracy trade-off**: table 3 shows that Cronus trades robustness against poisoning attacks for lower benign accuracy. \n5. **Dependence on the number of classes** in the learning task. The number of required parties to achieve the theoretical error bound is a few thousand to achieve the theoretical error bound for RobustFilter.\n\nOther comments:\n1. The paper states that: \"Despite its advantages, this approach has many known security and privacy weaknesses, and is limited to models with the **same** architectures.\" This assumption was relaxed, for example, in: [4]. The local models need to share the same architecture as the global model.\n2. Table 3 shows that the drop from benign accuracy to the worst accuracy by 2.1% for CIFAR10, while the text in Section 5.1 states that the maximum drop is by 4.5%. Furthermore, the drop in the benign accuracy is quite substantial for Cronus, e.g., by 8.3% in comparison to the FL with mean aggregation. \n3. How is the robustness measured in Table 3. \n4. The first paper on PATE states: \"Although applying distillation yields classifiers that perform more accurately, the increase in accuracy is too limited to justify the increased privacy cost of revealing the entire probability vector output by the ensemble instead of simply the class assigned the largest number of votes.\" (Section B.1). How does Cronus compare to the method that operates on hard labels instead of the soft labels?\n\n[1] FedMD: Heterogenous Federated Learning via Model Distillation. Daliang Li, Junpu Wang. https://arxiv.org/abs/1910.03581\n[2] Voting-based Approaches For Differentially Private Federated Learning. Yuqing Zhu, Yuqing_Zhu1, Xiang Yu, Yi-Hsuan Tsai, Francesco Pittaluga, Masoud Faraki, Manmohan Chandraker, Yu-Xiang Wang. https://openreview.net/forum?id=NNd0J677PN\n[3] CaPC Learning: Confidential and Private Collaborative Learning. Christopher A. Choquette-Choo, Natalie Dullerud, Adam Dziedzic, Yunxiang Zhang, Somesh Jha, Nicolas Papernot, Xiao Wang. https://openreview.net/forum?id=h2EbJ4_wMVq\n[4] HeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients. Enmao Diao, Jie Ding, Vahid Tarokh. https://openreview.net/forum?id=TNkPBBYFkXg\n[5] Ensemble Distillation for Robust Model Fusion in Federated Learning. Tao Lin, Lingjing Kong, Sebastian U. Stich, Martin Jaggi. https://proceedings.neurips.cc/paper/2020/hash/18df51b97ccd68128e994804f3eccc87-Abstract.html (also cited in the Cronus paper).\n[6] Nicolas Papernot, Martín Abadi, Úlfar Erlingsson, Ian Goodfellow, Kunal Talwar. Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data. https://openreview.net/forum?id=HkwoSDPgg",
            "summary_of_the_review": "The paper focuses on the protection of federated learning algorithms against poisoning attacks. However, the robust accuracy of the trained model is traded for the benign accuracy. The novelty of the method is limited since it leverages already known techniques.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}