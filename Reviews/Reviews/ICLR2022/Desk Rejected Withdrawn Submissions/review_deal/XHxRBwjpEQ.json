{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "- The paper proposes a defense against model stealing attacks via watermarking: given a model, can we identify whether it was constructed by performing model stealing attacks? Specifically, by observing outputs given a set of inputs with secret patterns known only to the original model developer.\n- The key idea of the proposed approach is to (a) train a feature extractor on non-watermarked data and (b) train a classifier on additionally on watermarked data; thereby, disentangling legitimate and watermarked features.\n- The authors show this simple trick helps create a watermarking technique more reliable and robust than previous approaches.",
            "main_review": "**Strengths**\n\n1\\. Well-motivated problem\n- I appreciate the problem setup: indeed, defenses against model stealing significantly lag behind the attack counter-parts. \n- Moreover, watermarking via 'backdoors' (Fig. 1) are problematic since stolen models can 'forget' watermarked regions.\n\n**Major Concerns**\n\n1\\. Threat Model / How effective are watermarks?\n- The primary concern I have with the paper is the threat model, which assumes the attacker has the same (unlabeled) data as that of the victim. However, this seems quite contrived since model stealing attackers do _not_ have the victim's dataset and typically use noise/OOD inputs. \n    - Are the authors aware whether the watermarks are successful in this setting as well? I reckon such typical attack scenarios would be challenging due to a larger discrepancy between extracted features. A reasonable way to verify this by evaluating watermarking accuracy on different mixtures of in-distribution and out-of-distribution attack query data used for model stealing.\n    - I would also appreciate a clarification: is the dataset used by the attacker the exact unlabeled copy of the victim's training dataset?\n- While I thank the authors for being transparent and verifying techniques to break the defense, based on the experiments in Section 4.3 (esp. quantization), breaking the defense seems quite easy -- by using hard- instead of soft-labels. Could the authors motivate the defense although it appears easy to circumvent? It would also be beneficial to experimentally support the claim \"to speed-up the stealing processing, adversary has to use soft-labels\".\n\n2\\. Technical Novelty\n- My next concern is to do with the technical novelty presented in the paper. Although the paper does improve recent watermarking results, I am concerned with a lack of technical novelty and insights that a broader ICLR community would find significant.\n- Specifically, the approach involves decoupling the training of a feature extractor from the classifier head. While I appreciate the simplicity, I find missing some analysis to justify the approach. I think a \"rigorous mathematical proof\" companion to the intuition presented in Sec. 3.1 would have helped the paper.\n\n3\\. Writing\n- Although the paper is easy-to-follow, I found the notation somewhat sloppy and hinder understanding. Some examples:\n    - $G_e = G_{e,s} + G_{e, w}$: while this seems like summing up elements in a vector space, I think the authors are trying to convey something else entirely?\n    - $f_{trad} = G_{d,s}(G_{e,s}(x)) + G_{d,w}(G_{e,w}(x))$: if $G_d$ is a classifier, $f_{trad}$ sums up softmax scores?\n- Some other things I found unclear when reading the paper:\n    - What is \"weight distribution shift\" in Fig. 3? Please define\n    - \"The watermark can be embedded in different layers in the protected DNNs and do not influence Watermark Acc\": But based on Fig 4., this does not seem to be the case? With 20% when first few layers are frozen and >20% only when last layer is frozen.\n\n**Minor Concerns**\n- Although appendix is mentioned in the main text, the submitted pdf does not contain an appendix.\n\n**Nitpicks**\n- Many typos and grammatical errors:\n    - \"could lost the decision boundary\"\n    - in fig 5-6: \"ratio of purned neurons\"\n    - \"extract an near\"\n- Results in Table 2 are not discussed",
            "summary_of_the_review": "The paper improves upon existing watermarking approaches by using a simple effective trick. However, I am concerned that the effectiveness might be limited to a contrived scenario and also that the approach is not accompanied with a more rigorous motivation or technical insights that might be of interest to the ICLR community.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a simple DNN watermarking scheme by ensuring that the model uses same features to detect the watermark and perform the intended task. The authors achieve this by first training the model on the original training set, then freezing the feature extraction layers (e.g., the conv layers), then fine-tuning the classification layers on both watermarked and original datasets. They outperform a recent method (EWE) that tries to achieve the same goal, while being much easier to implement. ",
            "main_review": "+ Simple and elegant solution to a well-studied problem. Prior methods like EWE use a complicated loss function with hyperparameters, whereas the proposed method is very straightforward and easy to apply for all. It also works better than EWE, scales better to more complex tasks and more flexible.\n\n- The authors only experiment with very predictable watermark types (white square in the image). Even a simple bruteforce search over some easy watermarks can find them. Easy-to-guess watermarks threaten the security of the watermarking scheme. I suspect that the feature sharing idea might not work as well as the watermarks get harder to detect, for example logos with different colors, random shapes, or watermarks similar to the Adi et al.'s paper (Turning Your Weakness Into a Strength).\n\n- The extraction attacks in the paper might be too simplistic. I would recommend evaluating different stealing attacks such as PRADA that depends on a small dataset which is augmented with adversarial perturbations or KnockoffNets that depends on out-of-distribution samples. These attacks are significantly different (and more practical) than the attack in the paper and the proposed defense might not perform well against them.\n\n- Judging by Figure 7, the attacker might as well discard the probabilities and only use hard-labels, which eliminates the watermark (as expected). This stems from the assumption that the attacker has the whole training set and tries to label this set using the target model. If the attacker has limited training samples (realistic) instead of the full dataset, discarding the probabilities would lead to an accuracy drop in the stolen model. A more realistic evaluation would be to understand these interactions for a practical attacker between the number of available samples/quantization strength/stolen model accuracy/watermark success.\n\n- The paper considers some adaptive attacks but here's an easier one, would the watermarking survive multiple rounds of distillation? Attacker steals the model, then distills this stolen model into another model and repeats this multiple times. After the first stealing the attacker does not need to query the target model anymore. I think one the implicit assumption of this threat model is that the computation is not a big problem for the attacker but access to labeled data is. So multiple distillation only increases the computational cost. Moreover, what about pruning only specific layers during fine-pruning defense instead of pruning the whole model. Since the watermark lives in the FC layers, trying to prune these layers might work better as an adaptive attack to remove the watermark.\n\n- One of the main claims is that when the watermark is installed using the proposed method, the watermark and the main classification task end up sharing features (which makes the watermark more transferable). However, asides from the extraction attack results, this hypothesis was never verified. Visualizing the FC layer activations on samples with and without the watermark for vanilla watermarked and feature-sharing watermarked models might be helpful to substantiate this claim further. You can also look into which FC neurons are more responsible for detecting watermarked models and show that there's a large overlap with the non-watermarked samples.",
            "summary_of_the_review": "+ Interesting and a simple solution to a relevant problem.\n\n- Not experimented with more complex (realistic) watermark types and extraction attacks.\n\n- Need more effort on adaptive attacks and verification of some claims.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work proposes a feature-sharing framework to improve the transferability of watermarks, when the model is stolen. Their approach is to enable the model to recognize watermark samples using standard features extracted from legitimate data.  They introduce a simple two-steps training strategy to extract features.",
            "main_review": "Strength\n\n+ This paper tackles a very interesting and important problem and it is a timely topic. The paper is easy to follow and the problem is well motivated by Fig. 1.\n\n+ To improve the transferability of watermarks, they propose a novel feature learning and watermark embedding method by considering both in-distribution and out-of-distribution samples and elaborate generation process. \n\n+They demonstrate and uncover the importance of watermark position for transferability through several experiments.\n\nWeakness\n\n- Writing can be significantly improved and there are many typos and mistakes, (e.g., In the Fig→ In Fig, T-test ⇒ t- t-est, X-axes ⇒ X-axis, Tab  5 ⇒ Fig 5, the obtains )\n\n- More experiments would be helpful with the larger dataset such as ImageNet to demonstrate the effectiveness.  \n\n- They only compare two methods in the experiments. More baselines are required to demonstrate the effectiveness of their approach.  \n\n",
            "summary_of_the_review": "They propose an interesting approach to tackle the very timely topic. However, the paper requires thorough proofreading. Also, the baseline method is too small to justify the effectiveness of their approach. In addition, the ImageNet result would be helpful.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No issue.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper tackles the problem of detecting whether a machine learning model is a “stolen’’ copy of another model. The paper first demonstrates that existing defense methods based on watermarks are not effective against black-box model extraction, as the watermark fails to transfer to the stolen model and as such cannot be detected. Then the paper proposes a methodology to address this gap. The main insight is to train the feature extraction and decision layers separately, and only embed the watermark at the decision step. The paper validates this approach by showing the watermark to be most impactful if applied at final layers. An empirical validation on image and text datasets shows the approach to detect stolen models >80% of the time and much better than two baselines. The proposed approach is shown to be robust to two defense methods but not to output quantization.\n",
            "main_review": "The paper is well structured and the methodology is overall clearly described. I think that the contribution is interesting and well-motivated. In particular, I appreciated Sec. 4.1 providing empirical results on the relationship between watermark location and transferability. \nMy main concern is related to the methodology presented in Sec. 3.2 - Identifying watermark in stolen models. Is the detection task framed as a binary classification task? It is not clear to me by reading the text how alpha is chosen. A small enough alpha may result in the null hypothesis always being trivially rejected and thus a WDR accuracy of 100%. What is the false positive rate, i.e., the probability of an un-watermarked model to be detected as watermarked, for the chosen alpha and why is this metric not reported in Tables 2 and 3? How would you generate un-watermarked models for a fair comparison? I would like to see an analysis of the trade-off between the false positive and true positive rates or if applicable, that authors report the Area Under the Curve (AUC) for the binary classification task.\n\nRegarding Sec. 3.3 Principles of API Watermarking, it would be useful in my opinion for a framework paper to elaborate more on the principles, for instance why these 3 in particular are deemed more important than other principles identified in the survey paper by Boenisch https://arxiv.org/pdf/2009.12153.pdf. Does the proposed framework satisfy these principles? \n\nIt is unclear whether the toy example is a contribution and if so, how it is different from the demonstration by Jia et al. (2020) which also showed black-box model extraction to not recover the watermark. There is furthermore no detail about the toy example: model used, data, etc.\n\nMinor points:\n- In equation (1) should it not be min over the expectation of J?\n- The information theory analysis does not seem very rigorous. For instance, when writing I(x, Ge,w(x)) it is unclear where the randomness comes from. If the randomness is over x (and G is fixed), then it follows that I(x; Ge,w(x)) = H(Ge,w(x)) - H(Ge,w(x) | x) = H(Ge,w(x)) because Ge,w(x) is a deterministic function of x. Why would H(Ge,w(x)) be low?\n- Why are results for the EWE baseline not reported for all the datasets?\n- I found the description of how EWE works to be unclear, I suggest rephrasing and adding more details if needed. \n- While results are reported for both in-distribution and out-of-distribution watermarking there is no discussion about the comparison between the two. For instance, in light of the results, which of them is preferable for a practitioner to deploy and why?\n- Can the authors provide intuition or an explanation for why the Watermark accuracy (Stolen model) presents a much higher variability for the EWE approach? I feel that a confidence interval of width 13.58x2=27 warrants an analysis.\n- Please include in the table captions the number of experiments and whether the standard deviation or 95% confidence intervals are reported.\n- Top of page 7 - “the Watermark Acc is always closed to 100%” - can the authors reference the appropriate Figure/Table?\nIn the third paragraph on page 7 there seems to be a contradiction between claims 1) and 3)\n",
            "summary_of_the_review": "I think that the contribution of this paper is valuable because it provides a principled solution to detecting whether a model is a “stolen” copy of a given ML model. I  recommend weak rejection because there are important missing details regarding the WDR metric casting doubts on the validity of the results. I would be open to increasing my score if enough and convincing details were provided about this, and if the relationship between the framework section and the rest of the paper was better clarified.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a technique for improving the transferability of model watermark. The proposed method is very easy to implement, which only requires fine tuning on the finial decision layer when training on the watermark dataset. The results show that the proposed method outperform baseline method and previous work in their evaluation metrics.",
            "main_review": "Strengths:\n- The proposed method is easy to implement and effective. \n- The experiments evaluated on both image and text domain indicating the generalizability of this method.\n\nWeaknesses:\n- The experiments mostly evaluated on small scale dataset (e.g., MNIST, CIFAR), and there is a performance drop when the dataset and image dimension become larger (in the TinyImageNet). Why didn't the author experiment with the full ImageNet dataset? Base on the existing experiments, I'm not sure whether this method can perform well in industry-level application (e.g., high-resolution image classification).\n\n- In section 4.1, the author investigated which layer to train using a simple CNN on MNIST dataset, and concluded that fine tuning on the last layer will results in the best performance. However, I wonder if this conclusion is still hold for larger datasets and larger networks. For example, training last 2 or 3 layers in resnet on CIFAR-100. Would this possible to have better performance than training only the last layer?\n\n- For evaluation on the same dataset, would different model architecture have different performance?\n\nOther comments:\n- I don't understand the insight or motivation in the experiment in Figure 3. Isn't it obvious that if you fix several layers and fine tuning the network, only the layers that is not been fixed can change their weights? Could the author explain more on their motivation of experiment in Figure 3?\n\n- Why the validation accuracy in Table 2 and 3 are different? Does experiment for in-distribution and out-of-distribution use different validation data?",
            "summary_of_the_review": "This paper proposed an useful method for model watermarking, but the experiments looks insufficient to me.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}