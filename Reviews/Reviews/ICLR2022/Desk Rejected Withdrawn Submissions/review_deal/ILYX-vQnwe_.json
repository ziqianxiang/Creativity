{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes to break down the question into several segments to retrieve knowledge for solving Visual QA with external knowledge bases. The methods utilize the segments as keywords to retrieve knowledge from the knowledge base, and treat the segments as nodes and dependency relations as edges, and then they use a graph neural network to integrate knowledge for answer prediction. They experimentally show new state-of-the-art results on the OK-VQA dataset.",
            "main_review": "* Strengths\n  - Breaking down visual questions for visual QA with external knowledge bases is a novel idea, and the subsequent components and overall systems are well-designed to achieve new state-of-the-art performance. The methods augment several keywords to query knowledge, including text in the image, object labels and attributes. It also leverages the question’s syntactic structure instead of the external knowledge graph to integrate knowledge.\n\n* Weaknesses\n  - KRISP (Marino et al. 2021), arguably the most important previous work, also has several similar features with the proposed methods such as segment-based knowledge retrieval and GNN-based knowledge integration. It would be better to clarify the differences between the proposed methods and KRISP, and discuss why they work better.\n  - In Table 1, MAVEx seems useful to increase the performance. However, it does not provide the numbers of the previous works including old state-of-the-art work, KRIPS, mixed with MAVEx at all. Since sole improvement of ‘Ours’ from ‘KRIPS(incl. graph pretraining)’ is only +0.2%, the comparison results seems critical with respect to improvement justification.\n   - On the process of breaking down visual questions, the questions may lose possibly important information if the key points of them are appeared in adjectives or adverbs not in noun or noun phrase. In this case the predicted answers would be wrong.\n",
            "summary_of_the_review": "This paper introduces a novel idea to solve outside-knowledge VQA task, and the authors design the overall system well to achieve new state-of-the-art performance.\nHowever, the significance of the contributions is not clearly validated with respect to comparative experiments.\nFrom the reason, I temporarily evaluate this paper as \"marginally below the acceptance threshold\".\nIn case that the my concerns are clarified and the significance gets clear, I will consider to change my score to \"marginally above the acceptance threshold\" or \"accept\".\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies leverage external knowledge for knowledge-based Visual Question Answering which is beyond the image. The proposed method breaks down questions into segments and then utilizes each segment as a key to connect to the external knowledge base.",
            "main_review": "- The proposed method seems to be quite dependent on the MAVEx knowledge retrieval system to both improve the performance as well as avoid the large search space of the word set. Since the main performance gain is based on a combination of the proposed method and MAVEx, the experiment should also include the combination of other existing methods and MAVEx. In the experiment, what about the comparison between the proposed method vs MAVEx using Wikipedia + ConceptNet + Google Images? In current results, it seems that the performance gain is mainly from MAVEx or from Google Images?\n- The experiment should also include a sophisticated ablation study which goal is to understand the contribution of each proposed component, e.g., knowledge sentence embedding, segment embedding, graph neural nets, and answer prediction. However, the current ablation study only shows different results with different parameters and it's not clear about the meaning either since their performance is very close.\n- In summary, this paper shows some incremental improvement in terms of performance. However, it will need to improve more baselines and experimental details to demonstrate the effectiveness of the proposed method.",
            "summary_of_the_review": "Please see the above review.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed a model which passes knowledge from sources between different pieces of semantic content in the question. \nEach segment of the questions is regarded as key to retrieving the knowledge from ConceptNet and Wikipedia. \nThen, the graph neural network is used to integrate the knowledge for different segments to predict the answer jointly. \nThe experiments on the OK-VQA dataset show that our approach achieves better results than the state-of-the-art methods.",
            "main_review": "[Strengths]\n1. The idea of converting passing the external knowledge to answer the visual question as an information retrieval task of the question segments is novel.\n2. In addition, the experiments show that the proposed method achieved a good result in the OK-VQA benchmark.\n\n[Weaknesses and Concerns]\n1. For the knowledge segments, is there any ablation study about the verbs' necessity in the proposed method?\n2. In the search word extraction, ViLBERT is used as the object linker. There is a concern that if a verb will be linked with an object in the image? Also, in the example in Fig. 1, the search word set is $('vegetable', 'carrot', 'red vegetable')$, where there are no verbs. So when will the verbs be used in this model?\n3. In the knowledge embedding section, $z$ is defined later than it first appeared. (In the content embedding section, it lacks a formal definition of the variable $z$.)\n4. Table 1 lacks explanations about what the 'oracle' means.\n\n[Minor]\n1. Figure 2 would be better to add numbers for each block or add arrows from the left top to right top.\n2. It would be better to add explanations about the right bottom in Figure 2 captioning.",
            "summary_of_the_review": "This paper obtains a novel idea and well demonstration of its approach.\nThere are some minor concerns about the paper details and the ablation study.\n\nEven some prior works also convert the fact-based VQA problem into an information retrieval task, this work focuses on learning the correlation between ''semantic chunks'' and the DBs.\nOverall, the contributions of this paper benefit the community.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper introduces “break-down VQA”, a new approach for knowledge-based visual question answering (KB-VQA), an ML task where the VQA system is asked to link some aspects of the question to some knowledge beyond the image, before answering. While current SOTA approaches for KB-VQA generally encode the entire question for either retrieving or filtering the external knowledge, the paper proposes to segment the questions into several semantic chunks and use them as semantic units to retrieve knowledge from external sources. The goal is to avoid confusion since different parts of the question could focus on different aspects outside and inside the image. The paper shows this visual question segmentation significantly boosts global accuracy, achieving new SOTA on the OK-VQA benchmark.",
            "main_review": "**STRENGTHS**\n- The paper is fairly well written, well-structured and easy to follow. The proposed approach is well described and almost all implementation details are provided in the paper. \n\n- Obtained results establish new SOTA on the OK-VQA benchmark, which is, in my opinion, the main strength of this paper.\n\n- The paper also provides a comprehensive ablative study which empirically validates many of the design choices of the approach. This ablative study is completed with qualitative analysis of some results.\n\n- The proposed approach is implemented on top of the ViLBERT model but could be easily adapted to work with other VQA backbones to extend them to KB-VQA task.\n\n**WEAKNESSES**\n- My main concern about this paper is the lack of clarity regarding the novelty w.r.t to the approach proposed in *[Wu et al. 2021]*. The paper clearly mentions that the main difference is the “breaking down” module during knowledge retrieval, but it is not clear if it is the only difference. The authors should clarify what are the similarities and the differences with *[Wu et al. 2021]*, and evaluate the impact of the newly introduced points. For example, in Table 1, the authors should add the result obtained by *[Wu et al. 2021]* without answer validation (i.e. accuracy of 37.6) since it provides valuable information regarding the benefits of adding the proposed visual questions segmentation.\n\n- The authors choose to implement their approach on top of the ViLBERT model, but they did not motivate this choice. It could be interesting to add a discussion (maybe in subsection 3.3) regarding the other alternatives to ViLBERT, or, even better, some ablations using other widely used models from the VQA literature (e.g. LXMERT). \n\n- In Section 4 (“training” paragraph), it is not clear to me which losses are used during training. The paper mentions a “standard VQA loss, together with the VQA loss on the final predictions”, which is a bit confusing for me. The authors should provide some clarifications about this point.\n\n- In Table 2, the authors provide ablations on the external knowledge sources (Wikipedia only, ConceptNet only, and both sources). It would have been interesting to also provide, for information, the accuracy score corresponding to the case where no external knowledge is used, i.e. when only content embedding is used to encode each segment.\n\n- If we compare the results provided in Table 2 with their equivalent in *[Wu et al. 2021]*, the gap is more significant when using only ConceptNet than using only Wikipedia. It would have been interesting to try to give some intuitive explanations about this difference.",
            "summary_of_the_review": "Despite the concerns mentioned above, I think the proposed approach could be an interesting addition to the KB-VQA literature, especially considering its quite impressive results. I also think the paper could be considerably improved by considering the comments made above, especially those related to the differences with *[Wu et al. 2021]*. Overall, I vote for “weak reject” for the moment, but I will consider leaning towards acceptance if some of these points are clarified.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}