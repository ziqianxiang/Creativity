{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In this work the authors propose two techniques to improve the performance of sparse attention mechanism. First, they introduce special tokens called as \"representative tokens\" that improves information flow between global and local tokens with use of a two step hierarchical attention mechanism. They also introduce SAOR, a sparse attention regularization that improves the robustness of the model to the sparsity pattern changes. They conduct experiments on LRA and MLM tasks to showcase the efficacy of proposed techniques.",
            "main_review": "Strengths:\n- Proposed hierarchical attention mechanism to solve the information bottleneck for sparse attention mechanisms significantly improves the stability w.r.t. number of global tokens used (table 7). The introduction of the representative tokens also has added benefits of improved performance for downstream tasks (table 6).\n\n- The experiments on LRA and MLM task showcase that proposed method can provide improvements on a variety of tasks. However there some concerns as discussed later.\n\nQuestions and concerns:\n\n- The motivation for SAOR is not clear. While the authors talk about the sensitivity to a change in attention patterns, it is not clear how this would manifest itself as sparse patterns are likely to remain fixed in practice?\n\n- Please elaborate this line on page 4: \"As this method is only related to number of attention blocks m and not affected by bottleneck size, the bottleneck sensitivity is solved?\"\n  - Is there a typo on page 4: \"where $m = (n - g) \\quad \\textbf{mod} \\quad w$.\" Is it supposed to be $m = \\frac{(n-g)}{w}$. Thus a representative token is inserted for each local window? If this is the case would the asymptotic complexity be quadratic due to attention computation on representative tokens? Though this will not be problem as the number of representative tokens $m \\ll n$  in practice.\n   - If the above is not a typo then what happens when $n_{windows} \\gg m$ would this not lead to similar information bottleneck?\n\n- The wall-clock training time for ERNIE-SPARSE on MLM and LRA tasks is not provided. My biggest concern would be that most sparse-attention methods are slower than full attention even for moderately large sequence lengths ($\\lesssim 4000$). Adding SAOR regulation is a significant overhead that would make its use very limited during training.\n\n- Robustness:\n   - Definition of robustness is not clear? What kind of robustness are we targeting?\n      - Is it robustness to adversarial examples. If yes, then the corresponding experiments are missing except Word Order Shift Attack (WOSA). Note some of the possible issues with WOSA later.\n      - Is it robustness to change of sparsity patterns? If yes, then the motivation for situations when the attention sparsity pattern could change is missing?\n\n- SAOR:\n   - Is the input rolled before or after applying positional embeddings?\n   - What is the overhead introduced by regularization? Train speed per-step and peak memory usage are missing for LRA benchmark.\n   - Biggest concern would be practical application, sparse attention methods already have a huge overhead even moderate sequence lengths. With SAOR additional overheads are introduced which make the training applications limited.\n\n   - Table 5: It is not clear the actual benefits are due to the regularization or due to larger effective batchsize due with token shifting.\n      - What happens if token shifting is retained but $\\alpha = 0$ for SAOR regularization?\n\n- Could you please elaborate on this: Page 8: \"Except for the global information some tasks require the model has more stability\" I do not fully understand the reasoning behind this statement.\n\n- Word Order Shift Attack:\n   - Could you please elaborate on how the adversarial dataset was constructed?  Was the rolling applied before or after adding positional embeddings? Note that even the full transformer will produce different outputs if rolling is applied before adding fixed positional embeddings and would not test the sensitivity to the sparse attention patterns.  Please also include the performance of vanilla transformer for this analysis.\n   - I am not sure if this is a good test if the rolling is applied before adding positional embeddings. For ListOps task the ground truth may not be valid any more. Similarly path finder could also be affected for the ground truth value.\n\nMinor issues:\n - Page 8: Take a a closer",
            "summary_of_the_review": "While the idea hierarchical attention to improve the robustness of sparse attention is interesting, I am concerned about the overheads introduced by SAOR regularization limiting the use to the proposed attention to only very long sequences. I am also concerned about the motivations of SAOR and the evaluation of the same.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "In this paper, the authors aim to resolve two important issues of sparse Transformer. Specifically, representative tokens and a regularization method are introduced to improve the effectiveness and robustness of sparse Transformer. Experimental results on long-context sequence and pre-training language modeling show the effectiveness of their methods.",
            "main_review": "Strengths:\n\nThis paper is high-motivated and aims to resolve important issues of sparse Transformer.\nTheir methods are easy to understand.\nGood results on various tasks are obtained and ablation studies show the contributions of these two methods (Table 5)\nSome questions and concerns:\n\n1. My biggest concern is that author try to claim their model is more robust but the way they show it is through the performance improvement. I am not sure whether this can prove their model is more robust.\n2. Why does the proposed model achieve better performance on GLUE tasks compare to RoBERTa (table 2)? To my understanding, the full-rank attention can see the global information and achieve ceiling performance.\n3. Could you try your method on the sentence duplication task mentioned in Reformer (Sec. 2.1) to see if the proposed model can obtain the performance?\n4. Some relevant works should also be included such as \"Luna: Linear Unified Nested Attention\".",
            "summary_of_the_review": " I think authors introduce two reasonable methods to resolve issues of sparse Transformer and obtain great results. But there are several concerns I mentioned above that should be resolved.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a hierarchical sparse transformer ERNIE with two components: hierarchical attention and sparse-attention-oriented-regularization (SAOR). Given a sparse attention pattern, for the hierarchical attention, the authors propose to add additional representative tokens for each group of tokens in the sparse attention. The representative tokens will first do a summarization over the tokens in the group, and then different representative tokens will do a round of dense attention. For SAOR, the author argues that different sparse patterns lead to different output for the same input sequence. And they try to mitigate the gap by further adding a regularization term that minimizes the KL divergence between the output from different sparse patterns.",
            "main_review": "My biggest concern of the paper is that the authors mix the use of robustness and expressiveness. Although the paper claims the goal is to improve the robustness of the attention mechanism, the whole paper is talking about how they improve the performance on standard benchmarks, but this is **expressiveness** of the model. The authors should evaluate how the model is robust to noises / adversarial attacks to show the robustness of the proposed model. Unfortunately, in the current version, there is only a short paragraph in the discussion section that is really about evaluating the robustness of the model under word order attack. Besides the above issues, another concern I have is that the paper has a lot of shaky statements without rigorous definition or concrete support. There are no mathematical analysis on why the prior design choice is not robust, why the proposed method makes the attention models robust, and to what extent the proposed method improves robustness. I list the detailed comments below.\n\n- “Another benefit of this method is it only needs to be used in the training phase and does not affect the inference of the model.” Why is this a benefit?\n- I think the paper is confusing “robustness” and “effectiveness/expressiveness”. Why does a better performance on LRA show the proposed method is more robust than the baselines?\n- “will leads to” -> “will lead to”\n- The paper lacks a mathematical analysis on why the previous design is bad and not robust. To what extent the proposed methods improve the robustness of the method, there is no analysis on this at all besides some empirical numbers, and all the numbers in the experiment section are from standard general benchmarks but not benchmarks for evaluating the robustness of the model. That being said, I like the synthetic dataset about word order shift attacks. However, there lacks detailed discussion on how this dataset is created, like how many steps did you “roll x”?. The authors only compare ERNIE with ERNIE w/o SAOR, both are the proposed methods, why not also evaluate the baselines on this task?\n- There are also various prior works on the information bottleneck that an information bottleneck increases the robustness of the model [1,2]. The authors may want to include the related work on information bottleneck and how IB improves the robustness in the related work section. But most importantly, all these prior works contradict with the assumption of this work.\n- Please define b_q, b_k, b_v in Sec. 3.1.\n- There is no citation in Sec. 3.1 on sparse attention. Which prior work on sparse attention do you refer to in Sec. 3.1?\n- “We can see that sparse attention is over sensitive to the pattern change” This sentence in Sec. 3.1 does not have support/proof, it just comes out of nowhere without pointers to any evidence. There is also no analysis on how sensitive they are.\n- Following the point above, you may want to actually show the empirical performance of sparse transformers with different patterns at least in the experiment section. I do not see any of this in the current experiment section.\n- “We propose to insert m representative tokens into input sequence, where m=(n-g) mode w”. This is wrong, it should be (n-g) div w.\n- “As this method is only related to the number of local attention blocks m and not affected by bottleneck size, the bottleneck sensitivity problem is solved.”  I do not see how/why the bottleneck problem is solved since the number of the “representative tokens” is still smaller than the sequence length, then this is still a bottleneck, no?\n- “Different with dense attention, sparse attention doesn’t guarantee interaction between all tokens as the design of sparse attention is to allow tokens can only attend to some of other tokens.” This is also not accurate. Tokens can only attend to some of other tokens **in one layer**.\n- A permutation of the input tokens from (t1, t2, …, t8) to (t7, t8, t1, …, t6) does not create the attention pattern on the right of Figure 2. There is no attention between (t1,t2) and (t7,t8) in the right Figure.\n- For the permutation of the input sequence in SAOR, there is no information on position encoding. Do you change the position encoding as well when you do the permutation?\n- Do you use the additional weights as your proposed method in the main results (not the ablation one)? Since you have this additional W_q, W_k, W_v in each layer, the proposed method actually has a lot more parameters than the baselines. Thus I’m not sure whether this is still a fair comparison, you may want to guarantee that the number of parameters are the same (or at least roughly similar) by decreasing the number of layers / hidden dimensions in your method.\n- How much additional time do you need for SAOR? You should also compare the runtime of your method with all the methods in the LRA experiment.\n- From the left figure of Figure 4, the dense transformer has much faster speed than ERNIE when sequence length is smaller than 3000? There is no discussion on this except for this sentence “The speed test data shows that the performance is consistent.” Besides, I assume the ERNIE should be at least two times slower than BigBird due to SAOR?\n- The experiment is also not quite consistent across datasets. For example, the authors only show standard deviation on the IMDB and Hyperpartisan dataset. Can you list the standard deviation of your method on the other experiments as well?\n- I am also confused by Figure 3. The authors aim to show the issue of the bottleneck, however, I do not see this issue from Figure 3. What I expect to see is a decreasing line for HST and ST (at least for ST if not both), because according to the assumption, the smaller the number of global tokens we have, the worse the performance should be. However, it’s pretty random on the five tasks. In the task Image, HST is even better with the #global tokens decreasing from 64 to 0.\n- The paper also does not have a complexity analysis on the proposed method. Note Appendix A.3 is NOT a complexity analysis. A complexity analysis is that you list the asymptotic complexity of your proposed method with respect to the sequence length, the number of representative tokens etc.\n\n[1] DEEP VARIATIONAL INFORMATION BOTTLENECK\n\n[2] Learning Robust Representations via Multi-View Information Bottleneck\n",
            "summary_of_the_review": "I think the paper in its current shape is not ready for publication. I suggest the authors think more about how they want to shape their contribution and also be much more careful about making statements without analysis and support.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}