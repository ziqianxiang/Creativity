{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This work analyzes and compares IGNNs and UGNNs in items of their convergence, representational capacity, and interpretability. Experiments are done on recovering synthetic labels and real-world node classification to study the aspect of long-range propagation, symmetric propagation, memory costs, and prediction performance.",
            "main_review": "Strengths:\n\nSome interesting theoretical results are shown in items of their convergence, the representational capacity of IGNNs and UGNNs which can be useful to understand the relation between IGNNs and UGNNs.\n\nSome questions:\n\nThe authors cite [1] as the first implicit GNN (IGNN). However, I believe the first IGNN is proposed in 2008 by Scarselli, Franco, et al. [2]. Some results of this paper may not be new. It seems Lemma 5.2 is also discussed in [2]. It states \"According to Banach’s theorem, (2) has a unique solution provided that is a contraction map with respect to the state.\"\n\nThe experimental results in table 2 are not compared to any recent SOTA methods. It is not clear if IGNNs and UGNNs are necessary or not from the experimental setup. I believe a comparison with SOTAs is important to convince why we need to pay attention to IGNNs and UGNNs and understand their property.\n\nOne of the benefits of IGNNs seems to be their memory efficiency. [3] also shows that it is possible to train 1000-layer GNNs with constant memory by using reversible residual connections. What is the advantage of IGNNs over reversible GNNs? Can they benefit from each other?\n\nIt is not clear what is the implication of the findings of this work to other GNN models. Does it provide any guidelines for designing typical message-passing GNNs?\n\n[1] Fangda Gu, Heng Chang, Wenwu Zhu, Somayeh Sojoudi, and Laurent El Ghaoui. Implicit graph neural networks. In Advances in Neural Information Processing Systems, 2020.\n\n[2] Scarselli, F., Gori, M., Tsoi, A.C., Hagenbuchner, M. and Monfardini, G., 2008. The graph neural network model. IEEE transactions on neural networks, 20(1), pp.61-80.\n\n[3] Li, Guohao, Matthias Müller, Bernard Ghanem, and Vladlen Koltun. \"Training Graph Neural Networks with 1000 Layers.\" ICML, 2021.",
            "summary_of_the_review": "The theoretical results of this work are interesting. However, a very important reference [1] is missing, and the motivation and implication of analyzing IGNNs and UGNNs are not clear.\n\n[1] Scarselli, F., Gori, M., Tsoi, A.C., Hagenbuchner, M. and Monfardini, G., 2008. The graph neural network model. IEEE transactions on neural networks, 20(1), pp.61-80.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors analyze two architectures (implicit and unfolded) GNNs and study frameworks in which the two approaches return similar or different results. In particular, the authors show that IGNNs can be considered as a proximal approach to UGNNs (provided that the activation function can indeed be written as the prox of some function, which in turns implies that this activation must be non decreasing, and provided that the weight matrix is symmetric) for specific penalty functions. \n\n\n\n\n\n",
            "main_review": "The paper is not bad but it definitely has to be reorganized and better connected to the litterature to highlight a clear definition and clearer comparison of the two approaches. Several of the results should be better explained. At this point the paper appears a little bit like as mishmash of results without a clear common thread. I think the authors would benefit from re-organizing the paper and re-submit it to a later conference. ",
            "summary_of_the_review": "=======================================\nGeneral comments :\n=======================================\n\nHaving a short paragraph of the form “UGNNs focuses on the minimization of a functionnal of the form (then add the general loss (3)) while IGNNs are based on a fixed point iteration of the form ” would clearly improve the readability of the paper. Honestly such a paragraph should appear right at the beginning, when you cite all the references in the introduction. This is a fundamental aspect of your paper and it is scattered throughout the paper instead of being clearly stated. \n\nIn short at the very beginning of the paper, I would say something like “The main point of this paper is to connect the training of UGNNs which occurs through the minimization of a functional of the form (3) to the training of IGNNs which occurs through a fixed point iteration of the form ” Then you should give an informal version of Theorem 5.1. and explain why this makes the UGNN approach more flexible than the IGNN . Then you can say that this is true only for symmetric weight matrices W_p and decreasing weight function. In fact I would put the first paragraph from section 5.1. much earlier in the paper. \n\nThe main advantage of IGNNs seems to be their use with non symmetric weights. However, if the activation is taken to be the identity, UGNNs can replicate the results obtained with non symmetric weights up to an arbitrary precision and leading to the same embedding as the IGNN up to an arbitrary symmetric transform. Then you can even say “In fact, there exists a meta-loss of the form ”\n\nIf you look at the comments above it really summarizes the contribution clearly in a couple of paragraphs instead of scattering them through the whole paper. I really recommend following this approach. \n\n=======================================\nDetailed comments :\n=======================================\n\nAbstract:\n- I would replace the word “approaches” by “architecture” which seems more appropriate to me\n\nIntrodution \n- paragraph 2: “to propagate signals arbitrary distances across nodes”, do you mean, “to propagate signals along arbitrary distances across nodes”?\n- The part “Although the original conceptions are unique, we consider a sufficiently broad design space of IGNN and UGNN models such that practically-relevant, interpretable regions of exact overlap can be established, analyzed, and contrasted with key areas of nonconformity” is quite unclear and appear unnecessary to me. \n\nSection 2:\n\n- What do you mean by adjustable weights?\n- The introduction of IGNNs and UGNNs based on how the embedding are computed seems to me a much cleaner way to introduce the two architectures than to through the extensive citations of the introduction \n\nSection 3\n\n- The sentence “IGNN models are predicated” does not mean anything. What do you want to say with predicate ?  “IGNN models are based on…” would be more adapted. The simpler and clearer the better\n\nSection 4.2.\n\n- Why do a proximal step with respect to phi and not rho for example ?\n\nSection 4.3.\n\n- The sentence “This result implies that, at least with respect to allowances for decreasing activation functions σ, IGNN is more flexible than UGNN” is not clear .. If you can derive IGNN from UGNN, isn’t the first more flexible than the second ? If it is just in terms of the activation I’m not sure I would use the statement “more flexible”. It seems (to me at least) more clear to say that the UGNN fixed reduces to the IGNN update only when the activation function is non decreasing. \n- line “although it thus far remains ambiguous how this relates to obtainable fixed points of UGNN ” is unclear. Replace with something like “so far we haven’t really explain if an when the proximal gradient iterations lead to a fixed point”.  \n- The sentence “although it thus far remains ambiguous how this relates to obtainable fixed points of UGNN ” is not clear. Do you mean that (8) is not guaranteed to reach a fixed point?\n\nSection 5\n\n- From what I understand deriving convergence guarantees on the general energy functional (3) is too difficult so you restrict it to (12). What are the motivations for this restriction? Aren’t there convergence results in the litterature for particular instances of (6) or (3). how do they connect to Theorem 5.1 ?\n\nSection 5.2.\n- In the statement of Theorem 5.4 you mention the Fenchel subdifferential while in the paragraph before you mention the Frechet differential \n- In the statement of Theorem 5.4. When you write U^{(k)}, is that the U^{(k)} from (6) ? this should be stated somewhere\n\nSection 5.3. \n- In the second paragraph, “expansive proximal operators actually undergird a variety of popular sparsity shrinkage penalties”  I have never seen the word “undergird” used in this way. I would use include or encompass \n\nSection 6\n- As a general comment, the introduction of the section is unclear. \n- In the introduction: the sentence “IGNN places no restriction that the propagation weight matrix Wp need be symmetric” is not clear, I would replace with “IGNN does not restrict the weight matrix W_p to be symmetric”. Why make complicated when you can make simple? \n\nSection 6.1. \n\n- “In this section we examine situations whereby ” —> “situations in which”\n- below the statement of Theorem 6, the sentence “given that T can be absorbed into the meta-loss output layer g from ” is unclear. In fact if what you mean is that “there exists a function g such that the minimizer of the meta-loss combined with the UGNN update will lead to the exact same embedding as the IGNN fixed point update”, then you should say it clearly. Also it is not clear to me \n- I would remove the last paragraph “Proceeding further, if we allow for nonlinear activation functions σ \n- …”. The paper already contain a lot of results. if you don’t have a clear result on non linear activation functions, maybe it is too early to discuss that part ? it is difficult to understand what you do anyways\n\nSection 6.2. \n\n- The whole paper (including the title) is about the distinction between IGNN and UGNN and then you introduce a result on the connection between general GCN and UGNN. This really makes the paper confusing. I would personnaly remove this section as it is not easy to connect it with the main message of the paper. \n\nSection 7\n- You indicate that “As expected, the highest recovery accuracy occurs when the generating and recovery models are matched such that perfect recovery is theoretically possible by design ”, is there any reason why those network do not achieve perfect accuracy?\n- Why do you say that the UGNN with asymmetric weights performs worse when applied to IGNN data? from the table it achieves 91% accuracy. Are you comparing it with the symmetric UGNN or with the asymmetric UGNN applied on UGNN data ? I don’t feel like the result of table 1 are sufficient to claim that some of the models are underperforming. \n- The sentence “indicating that truncated propagation steps can reduce performance when the true model involves long-range propagation ” is not clear. What do you mean by truncated propagation steps ? What do you mean ? if you implemented the networks, how can they have \n- For details on the architectures you send the reader to the appendix (I guess appendix C) but his appendix is not clear and does provide the details on the IGNN architecture. I can see you provide a link to the El Ghaou’s paper but it would be good to have a short explanation on the model. \n\nRemarks on the appendices:\n\nGeneral remark : you should really connect the appendices to the sections of the paper to which they are related. This can be done with one short sentence at the beginning of each appendix.  \n\nAppendix C.2\n- Your description of the architectures is unclear. You first say that the UGNNs are defined based on a modification of the TWIRL’s code. Then you mention using an identity function for rho and an indicator function for phi, which gives the relu function as the proximal operator, and you finally provide iteration (53). What architecture do you ultimately use for each of the models of section \n\nAppendix C.3\n\n- “In this task, we use model with …” what model ? \n\n\nA couple of additional typos:\n\n- Abstract line 2 : ‘between the efficient modeling long-range dependencies’ by ‘the modelling OF’ \n- I would remove the ‘(among other things) ’ on line 4\n- End of section 3: you want to replace “can but used to compute gradients” by “can be used”\n- intro of section 4: “Later, will describe the generic proximal gradient descent” with “Later we will describe”\n- section 4.3 I’m not sure you can say “it is illuminating “ I would rather say something like “it is helpful”\n- Proof of Theorem 2.2. first line: “if the loss E[Z] is small enough, the model achieves desired” —> “achieves the desired”\n- Section 5: I would replace “we first detail situations whereby convergence“ with “situations for which convergence … ” \n- Insection 7: “We also observe that UGNN with asymmetric weights” —> “that a UGNN”\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies the similarities and differences between two categories of graph neural networks, namely, implicit graph neural networks and unfolded graph neural networks. Specifically, the authors analyzed UGNN and IGNN theoretically and empirically from different aspects, including, convergence, representation capacity, memory cost, runtime, accuracy. They also analyzed the impact of a/symmetric propagation weights. \n",
            "main_review": "Strengths:\n1. It is interesting to see a comparison between two different paradigms of GNN. IGNN is based on an equilibrium equation while UGNN starts from energy functions. A general form of UGNN, which covers several special cases,  is also given.\n2. Both theoretically and empirically analyses are provided, with focuses on a few important aspects.\n3. The paper is well-written and organized.\n\nWeaknesses:\n1. The analyses and comparisons are appreciated, but the take-home messages are not very insightful and useful. After reading the paper, it is not clear which model to choose for a given task or dataset. I think giving more practical suggestions, instead of reclaiming the design criteria, could strengthen the contributions.\n2. The empirical analysis does not reflect the true power of IGNN. It would be better if larger datasets or some synthetic datasets  (like the one in the IGNN paper) are used for evaluation.  Also, the used datasets are small.\n\n\nQuestions:\n1. The abstract says that interpretability is also analyzed, but I did not see that in the main content.\n2. \"UGNN nonlinear propagation operators can be advantageous relative to IGNN when unreliable edges are present\" (in conclusion), from which parts can we conclude this?\n3.  Is it possible to marry the two worlds and get the benefits of both IGNN and UGNN with one unified framework?\n",
            "summary_of_the_review": "The contributions of the current version of the paper are not that enough and the takeaways are not very inspiring.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper provides several comparisons between the implicit and unfolded graph neural networks. The two models provide similar performances based on their analysis, and their numerical experiments support their findings.",
            "main_review": "Strengths: The authors identify a key difference between IGNN and UGNN: UGNN favors symmetric weight matrices while IGNN does not.\n\nWeakness: Overall the paper is not easy to follow due to confusing definitions and notations. Below are some concerns:\n1. The basic setup is confusing. It is unclear what machine learning problems the authors want to solve. Supervised learning? Unsupervised learning? Semi-supervised learning?\n2. Because of concern 1, the introduction to the GNN models is very confusing. For example, if it is supervised learning such as node classification, which is the machine learning task used in the numerical experiments, then the class labels are all given. As a result, Eq.(5) for UGNN becomes a general NN without graph-based information, and it is probably not a fair comparison between UGNN and IGNN.  \n3. The notations are not easy to follow: For example, Eq.(1) introduces parameter theta for a general GNN, but the authors do not specify what theta is in IGNN and UGNN. The number K is the iteration of the forward pass in IGNN, but the number K is the iteration of training or descent iterations in UGNN.\n4. The authors do not specify what meta-loss function is in both IGNN UGNN. There is no meta-loss definition for IGNN, while my guess for the meta-loss is the energy function defined in Eq.(3).\n5. The authors do not give a clear definition of the energy function and how the energy function is related to training. I guess that the “energy function” is defined in Eq.(3), which seems to be the meta-loss for UGNN. If that is the case, optimizing the “energy function” is equivalent to training the model, then Y^K is the prediction after K iterations of backward gradient updates. However, Y^K in IGNN is the output after K iteration of the forward propagation. There is no point in comparing these two quantities since they are different things, but that is the main content of section 4.3.\n6. The assumption made in the results are not justified. For example, the authors assume the function rho in Lemma 4.1. has Lipschitz continuous gradient. The authors may want to justify the assumption by introducing a few function rho used in practice that meets the assumption. Similarly, the assumptions made in Theorem 5.1 and 5.4 also need justification since the non-expansive operator and PSD are not very weak assumptions.\n7. The experimental setup is unclear: the authors do not provide enough details about the setup for the hyper-parameters (which is neither included in Section 7 nor Appendix). Given the similar performance between IGNN and UGNN, the experiments seem not to give strong support to the claims made by the authors.",
            "summary_of_the_review": "Although the paper did a lot of works on both theoretical and numerical perspectives, I do not recommend this paper because the overall logic of this paper is not clear, the definition and notations are so confusing, and the numerical results do not support the claims made by the authors due to limited introduction about the experiment setup. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}