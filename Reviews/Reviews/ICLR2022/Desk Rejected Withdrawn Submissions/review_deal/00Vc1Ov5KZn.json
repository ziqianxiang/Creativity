{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper explores video mixing strategies for self-supervised learning. Based on the existing approaches in the image domain (Mixup and CutMix), the evaluation is performed on the video domain. Additionally, the paper suggests mixing different modalities of video that lead to the proposed CMMC learning strategy. The overall approach is called Vi-Mix and studied on downstream tasks of video action recognition and video retrieval using three datasets: UCF101, HMDB51, and NTU-60.",
            "main_review": "Strengths:\n\n1) The paper is well-written and easy to follow. The related work section is quite brief but sufficient (I would suggest more discussion about video self-supervised methods).\n2) The suggested study of mixing strategies for video self-supervised learning is novel and it extends observations from supervised learning (VideoMix).\n3) The idea of mixing two modalities is very interesting and it opens more directions in further research. \n\nWeaknesses:\n\n1) The overall approach is highly based on already existing methods (Mixup, CutMix) and it can be considered as just a smart way of combining them.\n2) Most conclusions in the ablation study are made based on the results using the UCF101 dataset as a training set. However, UCF101 is a quite small and biased dataset that can lead to incorrect conclusions. It would be much stronger to use the Kinetics dataset to pre-train the model and then consider UCF101 and HMDB51 only for downstream tasks. \n3) There is no specification of how UCF101 and HMDB51 datasets are used. Which split of UCF101 is used for training? Which split of HMDB51 is used for testing? Are the results in Table 3 and Table 4 are for all splits or just for split 1?\n4) There are some concerns about the results in Table 1. It seems that results are presented for the models that are not fully converged. In Figure 1 the blue line (without mix) is still climbing even after 500 epochs. If we also compare results for MoCo from Table 1 (38.2, 15.3) and Table 2 (46.8, 23.1) we can see that they are significantly different. It raises the question: Does mixing really improve accuracy or does it just speed up convergence? \n5) The statement \"This is because cutmix operation destroys the temporal structure of the videos which is crucial for understanding actions in videos.\" does not apply to the results on HMDB51 in Table 1. Almost every mixing strategy improves over MoCo. It would be beneficial to see more discussion of this effect. \n6) Table 3 and Table 4 are confusing. Many methods explore additional data modalities during training such as optical flow, audio, and text. However, most methods are still using only RGB stream to obtain results for downstream tasks (except for MemDPC and CoCLR that present results for both RGB and Two-stream settings in their papers). Based on the results in Table 2 (last row) and Table 3 (second row) Vi-Mix is using the two-stream setting which requires the computation of optical flow during inference time. It makes the comparison to most of the other methods unfair. It is crucial to also present results for only the RGB stream for the state-of-the-art comparison. These results could be much lower considering Table 2 (only 55.8 vs 74.0 for UCF101). \n7) Also, the statements and insights of the paper would be stronger if other contrastive learning methods (e.g. SimCLR) and backbones (e.g. R3D, R(2+1)D) are considered. Also, the comparison with other methods would be easier. \n",
            "summary_of_the_review": "I like the idea of using mixing strategies for video self-supervised learning. I think mixing several modalities is also interesting and it can lead to further development of self-supervised methods. However, there are several concerns about the experimental setup that currently do not allow to be sure that insights and conclusions are strong enough to be useful for the community. Would like to see responses to my concerns to change the initial score. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the authors tackle the problem of self-supervised video representation learning. They extend existing image domain data augmentation, cutmix, to multi-modal video domain. They validate the effectiveness of the proposed method on publicly available benchmarks: UCF-101, HMDB-51, and NTU-60. \n\nThe claimed contribution of the paper are as follows.\n* In unimodal learning, they find that mixup is more effective than cutmix.\n* Cross-modal manifold cutmix is proposed for self-supervised video representation learning.\n* They claim this is the first attempt to perform mixing across channels.\n\n",
            "main_review": "This paper has the following strengths.\n\n+ The problem addressed, self-supervised video representation learning is an interesting, and unsolved problem yet.\n+ Adding video mixup shows good performance on downstream video classification and retrieval tasks.\n+ The paper is structured well. It is easy for readers to follow except for a few confusing sentences.\n\nHowever, this paper has weaknesses as well.\n- The proposed training pipeline is not end-to-end and quite heavy. We need to 1) independently train RGB and Flow encoders, 2) train the RGB encoder with cutmixed flow features (from the frozen Flow encoder), 3) train the flow encoder with cutmixed RGB features (from the frozen RGB encoder), 4) repeat step 2 and 3 again, 5) train on downstream tasks. It is fine to be complex if the performance gain is significant. However, the gain does not seem very significant. Is there any way to simplify the training pipeline? E.g., Can we jointly train RGB and Flow encoders? \n- The performance improvement from cross-modal cutmix is not significant despite we are relying on two modalities and quite complex training pipeline. In Table 2, Cross-modal mixup gives only 1~2 point improvement on UCF-101, HMDB-51 classification/retrieval tasks.\n- The proposed method does not show competitive performance compare with state-of-the-art. Despite very complex training pipeline, the proposed method does not outperform CoCLR when both are pre-trained on Kinetics-400 as shown in Table 3. It shows on par performance when much smaller UCF-101 (1/24 in scale) is used for pre-training. This might imply the proposed method does not learn more transferable/generalizable self-supervised representations than state-of-the-art methods. On skeleton based recognition task, the proposed method shows inferior result than CrosSCLR (xsub: 72.5 vs 74.5, xview: 79.1 vs 82.1).\n- They claim this work is the first attempt to perform mixing across channels. However, it seems that channel mixup does not improve the performance in general in Table 2. Why do we need channel mixup if it does not improve the performance?\n- There are a few unclear points in the paper. In Table 4, is it RGB + Flow result or single modal result? Please add modality and other columns (network, resolution, depth) in this table as well. In Table 2 caption, “All the models are trained on training samples of UCF101 for 500 epochs”. In the text, “All the models are initialized with weights obtained from pre-training with mixup for 300 epochs”. Which one is correct?",
            "summary_of_the_review": "To summarize, the proposed method shows insignificant performance gain despite quite complex training pipeline. It does not show promising results when large-scale training dataset is used for pre-training. This trend is quite opposite to the motivation of self-supervised representation learning. Therefore, I would recommend this paper a reject.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This submission proposes an augmentation strategy for self-supervised video representation learning. This augmentation (Vi-Mix) includes performing video mixup on RGB and optical flow streams individually by performing weighted averaging of two spatio-temporal stack of frames followed by a cross-modal Cutmix to mix such cross-modal representations in the hidden representation space. The self-supervised learning process has been done on Kinetics400 and UCF101 datasets and then the performance has been shown on downstream action recognition and retrieval.",
            "main_review": "Strengths:\n\n1- The idea of proposing new augmentation strategies for self-supervised video representation learning is interesting as this is one of the key components of self-supervised learning especially in the video domain that we have an additional dimension (time).\n\n2- Authors proposed different ablation studies showing the effect of using different mix-up strategies in different streams (RGB, optical flow, and both).\n\nWeaknesses:\n\n1- Authors mentioned some motivations for using cutmix as a way of cross-modal augmentation, however, I am not still convinced what is the benefit of mixup augmentation in each stream compared to the existing simple augmentation methods such as random temporal sampling as in CVRL, [1]. \n\n2- Although the authors mentioned that the benefit of using cutmix is doing augmentation across modalities, I'm curious how this augmentation can be generalized to other modalities such as audio or text. To me, this cross-modal augmentation only works on the modalities such as flow which also has spatial dimensions same as the RGB domain.\n\n3- Comparison to the state-of-the-art results in Table 3 are not convincing. The CVRL approach only uses the RGB stream, their final results outperformed the proposed approach with a large gap (10%). Although the authors mentioned that this improvement is due to the larger size of the input (224 instead of 112) and the architecture, I'm curious to see how the proposed approach behaves in the same setting (e.g., same resolution and same architecture). \n\n4- Comparison with the recent CVPR 2021 work [1] which has shown their results on Spatio-temporal MoCo-V2 and obtained better results than the proposed method is missing.\n\n5- The proposed training pipeline is composed of different stages (individual RGB and Flow training, 4 stages of training with Cutmix, and then another round of joint training). The benefit of self-supervised representation learning is how it can be utilized to train on larger datasets as we don't need annotations. However, here we can see that multiple stages of training and the large number of training epochs make the process much more challenging. \n\t\n[1] Feichtenhofer, Christoph, et al. \"A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n",
            "summary_of_the_review": "The paper proposed an augmentation mechanism for self-supervised video representation learning. According to my points in the weaknesses section, authors did not articulate the benefits of the proposed method compared to the recent existing approaches such as CVRL, and [1]. According to the experiments and comparison to the state of the art methods, I'm still not convinced about the benefit of using  mix-up augmentation while the random temporal augmentation used in methods such as CVRL or [1] outperformed the proposed approach. Another weak point is the training process which is not end-to-end and includes multiple stages of training. This brings more complication when dealing with large datasets.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper majorly explored the augmentation strategy, especially data mixing, on self-supervised video representation learning. The paper first evaluates the performance of existing data mixup and cutmix in video domain, then proposes the cross-modal manifold cutmix (CMMC) which integrates the extracted multi-modal features, finally implements extensive experiments to support the proposed augmentation strategy.",
            "main_review": "Strength:\n1. The paper provides a comprehensive evaluation and anslysis on data-level mix strategies in self-supervised video representation learning, including mixup, cutmix and some variants.\n2. The paper aims to jointly leverage the use of multi-modal data and mixing strategy to learn video representation. The proposed ViMix performs the mixing augmentation in feature maps, successfully avoid the problem of distribution inconsistency between multi-modal data, and improve the perofrmance.\n3. The experimental results and analysis provide some insights from the perspective of spatio-temporal characteristics of videos.\n\nWeakness:\n1. There are some previous works on video representation learning that perform multi-modal contrast and uni-modal data augmentations on the feature map like [1]. If perform uni-modal manifold cutmix but with cross-modal contrastive loss, how much performance gain will be obtained? The comparison is desired.\n2. In 3.2, the descriptions on the details of manifold cutmix operation is difficult to follow and somewhat confusing. First, when choosing the network layer, why set k <= l? What is the motivation? Second, how to ensure (c1,t1,s1)>=(c2,t2,s2)? When the network goes deeper, the spatio-temporal resolution descreases, the channel increases, it seems confilicting. Third, it seems Eq.9 defines both start and end points of mask M, and in this case, how to preserve the temporal and channel information?\n3. There are some mistakes in notations, like in 3.2, 'f_1() of modality 2' -> 'f_2()', in 4.4 'cross-modal mixup (+mixup)' -> 'CM mixup', in algorithm 1, 'z_1 = normalize(f_1q.partial_forward(x1_q, mix1, L))' -> '(f_1q.partial_forward(g_mix, mix1, L)'.\n4. Besides UCF-101 and HMDB-51, some other dataset preferring motions liking Diving could better validate the proposed method.\n\n[1] Patrick, Mandela, et al. \"Space-Time Crop & Attend: Improving Cross-modal Video Representation Learning.\" ICCV, 2021.",
            "summary_of_the_review": "Overall, the paper provides insights into the augmentation strategies in video representation learning, and the proposed CMMC is somewhat novel and effective. But there are some mistakes in the clarification, the motivations as well as technical details need to be stated more clearly. And it is better to support the proposed method with more benchmarks.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}