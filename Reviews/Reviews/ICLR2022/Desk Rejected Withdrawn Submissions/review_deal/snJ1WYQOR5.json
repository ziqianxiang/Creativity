{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents a neural-network-based approach for representing radiance fields. A neural architecture and training regime are proposed that can generalise novel views from sparse input imagery. The proposed approach renders more quickly than NeRF by virtue of directly representing the underlying plenoptic function and rendering as a single-evaluation operation, rather than employing a volumetric hybrid requiring ray marching as in NeRF. Results are shown for rendered scenes and include comparisons to NeRF and NeRF++, showing much faster training and rendering times.",
            "main_review": "\nPros:\n+ The approach contains several insights and the principle seems sound, though with a few important limitations that are not discussed (see below).\n+ The proposed approach renders more quickly than NeRF by virtue of directly representing the underlying plenoptic function and rendering as a single-evaluation operation, rather than employing a volumetric hybrid requiring ray marching as in NeRF.\n+ Insightful use of self-supervision via multi-view photo-consistency.\n+ Insightful addition of high-level features to complement photo-consistency.\n+ Comparisons to NeRF and NeRF++ are included, showing training time, rendering time, PSNR, and SSIM comparisons. Fast rendering is compatible with real-time operation.\n+ Ablation studies are included for the sampling strategy, use of features in proxy depth, and direction-based weighting. \n+ Code will be released.\n\nCons:\n- Results are for synthetic / rendered data only; use of captured imagery is limited to a supplemental video with no points of comparison or evaluation included.\n- The method relies on explicitly estimating distance to the scene and enforcing photo-consistency... it is not clear that this is compatible with non-Lambertian scene effects like transparency and specularity. Indeed, artefacts are visible around specular reflections in the supplemental video. This important limitation is not discussed.\n- It is not clear that the proposed approach will converge well in complex scenes. This is not discussed.\n- Significance of results is not always clear, e.g. in Fig. 5 the proposed and naively rendered images appear very similar; numerical results in Table 1 show only a small quantitative improvement; results in Fig 4 show modest qualitative improvements; perhaps the speed of this method is its most important strength, if so this could be better reflected in the presentation\n- Writing errors throughout are distracting and make the paper difficult to follow in places\n\nOverall the paper is interesting and contains some nice insights. However, limited results on captured imagery and issues with presentation, chiefly around highlighting / discussing limitations and strengths of the approach, make it difficult to recommend in its present form.\n\n\nMinor comments:\n\nThe authors may wish to refer to and appropriately contrast against the similar, arguably concurrent work by Sitzmann et al \"Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering\", to be presented at NeurIPS 2021, and available on arXiv.\n\nThe text at the opening of Sec 4 and Fig 3 mislead me into believing a physical 360-degree camera was to be used. I recommend clarifying in the text that this is only for synthetic imagery. \n\nIn Sec 4.3: \"NeRF and NeRF++ aim to estimate the radiance emitted by scene points at any position and direction, while our method is designed to recover the irradiance perceived by an observer from any point and direction.\" This characterisation of NeRF does not seem complete, and the distinction between the two methods could be clarified.\n\nQuestions for authors:\n\nExplicit depth modelling and photo-consistency are generally associated with a Lambertian scene assumption. Will the proposed approach handle non-Lambertian effects like transparency, reflection, and specularity? Why?\n\nHow does the proposed method converge in complex scenes? What are the conditions for convergence? What about scenes with a lot of occlusions? Or a lot of specularity or transparency? Or with few training views?\n\nHow does training time differ when using differing numbers of input views?\n\nWill the synthetic dataset be released?",
            "summary_of_the_review": "Overall the paper is interesting and contains some nice insights. However, limited results on captured imagery and issues with presentation, chiefly around highlighting / discussing limitations and strengths of the approach, make it difficult to recommend in its present form.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes an efficient deep learning-based novel view synthesis method. The model consists of two MLPs, with the first one inferring depths and the second one refines colors from the intermediate colors and depths. An Imaginary Eye Sampling (IES) training strategy is proposed to further improve quality. The experimental evaluations show that the proposed method outperforms previous methods on the proposed benchmark on both image quality and training/inference efficiency.",
            "main_review": "__Strengths__:\n\n1. The paper is easy to follow, and the related work section clearly introduces the relationships of different view synthesize approaches.\n   \n2. The proposed Proxy Depth Reconstruction (PDR) network does not require ground truth depth and can be trained with self-supervision.\n   \n3. In experiments, the proposed method outperforms FVS, NeRF and NERF++ on PSNR and SSIM. Moreover, it has an impressive efficiency compared with previous methods, e.g., 10x/20x faster than NERF and NERF++ on training, and about 200x/800x faster on inference.\n\n\n__Weaknesses__:\n\n1. The novelty of this paper seems incremental. Disentangling depth and color from views has been studied in DoNeRF (Neff et al., 2021), the difference is that DoNeRF needs ground truth depth. Moreover, the two MLP architectures (proxy depth reconstruction network and color blending network) are almost identical to existing methods NeRF and PointNet.\n   \n2. The proposed method may not work well on view-dependent effects, e.g., specular highlights, because the 2nd stage only relies on the intermediate depth and color, the differentiable ray-tracer is simply a 2D image warping function, and it does not consider the view/light directions. I was expecting the 2nd color blending network to learn accurate specular highlights, but in the supplementary video, the specular highlights on the floor of the bar scene look unreal, and the highlight positions do not change when the camera moves. I'd like to see videos of NeRF, NERF++ and ground truth for comparison.\n\n3. This paper lacks experimental comparisons with state-of-the-art methods, such as DoNeRF (Neff et al., 2021) and FastNeRF (Garbin et al., 2021).\n   \n4. In section 4.2, the author mentioned that the 5D input was encoded using Tancik et al., 2020, which results in a 512D input, however, NeRF's positional encoding has a much lower dimension, and I don't think it is a fair comparison. This paper can add another experiment by using the original NeRF encoding on the proposed method. Then we can tell whether the improvement mainly comes from the high-dimensional Fourier feature encoding.\n   \n5. Minor issues: \n   * Details about the values of hyperparameters $k$ and $\\lambda$ during training and testing are missing.\n   * How is the feature $f$ obtained?\n   * According to section 4.6, \"Ours w/o weighting\" is setting the weighting term $s_i$ to zero in Eq. 3, but then Eq. 3 will be 0.\n   * It may be better to move the discussion on DoNeRF (Neff et al., 2021) and FastNeRF (Garbin et al., 2021) in P14 to related work.",
            "summary_of_the_review": "The proposed method has an impressive performance, especially it shows better quality while being surprisingly efficient than NeRF and NeRF++. \n\nHowever, as I mentioned above, it may not work well on specular highlights like NeRFs, and I'm not sure whether the improvements shown in the paper come from the high-dimensional Fourier feature encoding or the model architecture itself. Moreover, I think additional comparisons with closely related works DoNeRF and FastNeRF are needed. Thus, I think this paper cannot be published at its current state, and I hope the authors address my concerns in rebuttal.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper presents an MLP based method for novel view synthesis from a captured light field. The proposed method first uses an MLP to estimate the depth of the scene at a new viewpoint and retrieve the color information from the captured views and then uses a second MLP to optimize the color information retrieved in the previous step. The training of 1st MLP utilizes the observed viewpoints to interplote the rays from virtual viewpoints and optimizes under the photo-consistency constraint, which is self-supervised learning and not requiring ground truth depth. Both MLP networks are trained with one scene at a time. The evaluation is done with a synthesized dataset and a real dataset (in the supplementary), comparing with NeRF and NeRF++. ",
            "main_review": "Pros:  \n\n+ The overall architecture is simple, clear, and easy to follow. \n\n+ The training and inference speed of the proposed method is faster than NeRF. \n\n+ The results show somewhat advantages over NeRF and NeRF++. \n\nCons: \n\nThe main concern is the training and generalization of the network. \n\n1. Training of the PDR network uses photo-consistency constraints. This is not always satisfying when there are non-Lambertian objects, such as specular or transparent objects. There is a lack of discussion about the limitation of the proposed method. \n\n2. The sampling strategy of the virtual viewpoint used in the paper is random sampling. \nWhat is the relationship between the number of virtual viewpoint samples and the performance? How is the performance degraded/upgraded if using uniform sampling? \n\n3. There is a lack of ablation study for the performance without the CBNet. How is the performance different from the one with CBNet? \n\n4. Because the PDR network is trained to predict a specific scene depth, it is not possible to generalize for multiple scenes. How about the CBNet? Is it possible to generalize for different scenes? ",
            "summary_of_the_review": "Although the proposed method is simple and fast, many details are missing and unclear. I don't think it is ready for publication with its current status. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents a learning-based view synthesis technique that is evaluated on a dataset of synthetic and real 360 degree omnidirectional images.\n\nThe approach is guided by the formulation of the plenoptic function, which describes scene radiance at any point in space, in any viewing direction (mapping from a 5D input space to a 3D RGB output space). The authors recognize that directly training a MLP or CNN to directly approximate the plenoptic function is challenging, in part because only a sparse sampling of the input space is available given a limited number of viewpoints, and because the plenoptic function contains high frequency content near occluders (motion parallax).\n\nIn order to lower the bandwidth of the plenoptic function, as other works before [1], the authors introduce a notion of a proxy scene representation. Interpolating output colors on a good approximation of the real surface lowers the required bandwidth for the view synthesis task [2].\nIn the presented work, the scene is represented as a depth-prediction network which predicts the scene depth along a given input ray. The depth network is trained first, using a color consistency loss over the set of input images. The predicted ray depths are then used to sample the input views, to train a color prediction network that converts a set of input samples to RGB intensity along a novel view ray.\n\nThe authors present results on a dataset of omnidirectional images. The quantitative results are established on synthetic data, and the supplementary video includes synthesis results on two real scenes.\n\n[1] e.g. Surface Light Fields (2000), Unstructured Lumigraph Rendering (2001)\n\n[2] A Frequency Analysis of Light Transport (2005)",
            "main_review": "The paper points out that a direct approximation of the plenoptic function (radiance received at the viewer)  is conceptually simpler, and potentially more performant, than an emitter-based formulation of view synthesis, which requires costly rendering and compositing of multiple emitting scene points. By eschewing volumetric rendering, and instead enforcing a surface-based scene representation, the authors demonstrate their formulation to be more performant than some recent view synthesis techniques. The presented \"Imaginary Eyes\" concept is a viable technique to generate additional input samples that fill the 5D input domain without requiring dense trianing data.\n\nUnfortunately, the presented results do not appear to reach the reconstruction quality of state of the art view synthesis methods. The description in the paper is not always always clear, and the amount of relevant comparisons is limited.\n\nMy primary concern about the exposition is that the paper does not make a convincing argument why the plenoptic fuction formulation would lead to better results than learning a scene representation in a different domain. The \"imaginary eyes\" sampling is interesting, but it is not fully evaluated how this formulation compares to volumetric accumulation along the viewing ray.\n\nThe authors claim to use a \"Differentiable Ray-Tracer\" to sample the real input images at predicted depth values. However, what is described in the paper is not a ray tracer. As described, the technique projects predicted 3D scene points into the input views and performs bilinear interpolation. It does not appear that any visible surface determination / ray-scene intersection is happening (the conventional meaning of \"ray tracing\").\n\nWhile the paper includes quantitative results comparing favorably to NeRF and NeRF++, these results were not computed on an established view synthesis dataset. The paper does not include results of the presented technique applied to more widely used datasets, such as the NeRF dataset, or the light field dataset by Kalantari et al. [3]. The used omnidirectional dataset appears to include limited motion parallax, and the the presented technique exhibits reconstruction artifacts on the real scenes that are shown in the supplemental video.\n\n[3] Learning-Based View Synthesis for Light Field Cameras (2016)\n",
            "summary_of_the_review": "The paper does not make a convincing argument why their formulation would improve upon existing view synthesis techniques. The experimental validation is missing key comparisons on established datasets, and the included results in the supplemental video show noticeable reconstruction artifacts.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}