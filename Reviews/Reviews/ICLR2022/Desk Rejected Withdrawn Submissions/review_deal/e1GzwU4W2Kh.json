{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper claims that the difficulty of having to search the large co-exploration space is often addressed by adopting the idea of differentiable neural architecture search, which faces a critical challenge of not being able to systematically satisfy hard constraints, such as frame rate or power budget. To handle this claimed issue of hard constraint problem of differentiable co-exploration, they propose ConCoDE,  \nwhich searches for hard-constrained solutions without compromising the global design objectives.  The authors provide mostly ablation studies to evaluate their proposed method.",
            "main_review": "The strengths of the paper are as follows:\n+ The paper targets an important topic of co-searching for both the networks and their accelerators;\n+ The authors provide a good summary of related works;\n+ The target angel of gradient manipulation is interesting;\n\nThe weaknesses of the paper are below: \n- The technical contribution is limited because the proposed methods involve quite some hyper-parameters limiting their practicality;\n- One main concern is that the authors do not evaluate their searched networks or accelerators with SOTA co-exploration/search works, making it difficult to evaluate the effectiveness of their method;\n- The target accelerator space is quite limited, and thus the authors should evaluate their method against random or grid search. ",
            "summary_of_the_review": "While the paper targets improve against SOTA co-exploration via differentiable search, the technical contribution and evaluation experiments are limited. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a novel method of differetiable network and hardware co-search, which is able to take the input of and satisfy the hard constraints such as the system throughput. As compared with the previous methods, this proposed one has more practical implication as hard hardware constraints is more common and the existing soft-constraint methods require tedious rounds of trial and error to satisfy the requirement. The experiments also show the results with similar quality to those searched without any constraints.",
            "main_review": "Strengths: \n- Overall, it is a nice to-the-point paper which points out the no hard constraint problem of the existing the co-search works and propose the solution to it. \n- Decent amount of experiments are conducted to verify the effectiveness\n\nWeakness:\n- Lacking the analysis of the searched networks and accelerators. \n- More justification on results' significance",
            "summary_of_the_review": "- Lacking the analysis of the searched networks and accelerators. \nWould be interesting to see the networks and accelerators searched under the hard constraint methods and how do they compare with those from the existing approach\n\n- More justification on results' significance \nBased on the figure 3, we can see that soft-constraint methods are able to produce design points with similar or even better performance. For sure it will take more trials to achieve so. Thus, it would be better to give some insights on the average time cost for the soft-constraint methods (with search parameters tuned) to produce the design satisfying the constraints and, how significant the reduced time cost for the proposed hard-constraint methods to produce the design satisfying the constraints in the real life application. Please note that the cost for searching a matched network and accelerator may be amortized throughout some application life span.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Conventional differentiable NAS cannot guarantee to meet the hard constraints such as frame rate. This paper presents ConCoDE to enable the differentiable co-exploration of neural architecture and hardware design when given hard constraints.\nSpecifically, it proposes to apply gradient manipulation in the training to push the gradient in the direction that 'agree’s with the hard constraint. Experiments on ImageNet show that ConCoDE is able to provide high-quality solutions that meet given hardware constraints.",
            "main_review": "Strength:\n- The writing is clear and easy to follow.\n- It's nice to see the experiments on different hardware constraints other than latency only.\n\nWeakness:\n- The main idea in this work is gradient manipulation, which seems to be an incremental medication of previously published efforts.\n- This work lacks comparison against other co-exploration works.\n\n[1] Once for all: Train one network and specialize it for efficient deployment. ICLR 2020.\n\n[2] Co-Exploration of Neural Architectures and Heterogeneous ASIC Accelerator Designs Targeting Multiple Tasks. DAC 2019\n\n[3] Neural Accelerator Architecture Search. DAC 2021\n\n- I think this paper is not well motivated. I am not convinced that the differentiable method is a must in practice. ConCoDE uses an NN-based estimator to estimate the hardware cost. Such estimator can be also used by other one-shot NAS frameworks. Meanwhile, \ngiven a new hardware constraint, ConCoDE requires retraining the network while one-shot NAS or Once-For-All NAS do not need to re-train the supernets. Therefore, this paper lacks the cost comparison against other one-shot NAS methods.",
            "summary_of_the_review": "This paper proposes to use gradient manipulation in the differentiable NAS to deal with the hard hardware constraints. My major concern is the novelty of the proposed method and its effectiveness compared to one-shot NAS methods. Thus I do not recommend this paper to be accepted.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper introduces a differentiable co-exploration of hardware accelerators and neural architecture search under various hardware constraints such as latency, energy, chip area. The authors revisited the idea of `gradient manipulation` in the context of constraint optimization. That is, if the gradient direction of `global loss function` and `constraint loss function` are concordant, the unmodified global loss  function is applied. Otherwise, a modified version of the global loss function is applied to the optimization process in order to deviate from violating the constraints.",
            "main_review": "**Strength**\n\n- Revisiting the idea of `gradient modification` in the context of constraint optimization is interesting and seems to work well for the target co-optimization problem.\n- The results show that the proposed constrained optimization seems to be working for the target search space.\n\n**Weakness** \n\n- Co-optimization of hardware accelerator and neural architecture search for only one application is far from a practical approach for architecting accelerators.\n- Many of the details from the hardware accelerator search space are not provided and it is hard to understand the complexity of the search space, the generality of the method, and more importantly reproducing the results.\n- The experimental setup for some of the evaluations are not clear nor quantitative. For example, it is hard to understand what a `best-effort hardware` means.\n- No comparison with prior work is provided.\n\n**Questions for Authors**\n\n(1) Would you provide the motivation for extreme specialization (e.g. co-optimizing the hardware accelerator and neural architecture search for one model)?\n\n(2) Please provide the details for the hardware accelerator search space. How complex is the search space? How many tunable hardware parameters exist in your search?\n\n(3) What is the accuracy of the hardware estimator compared to ground-truth data?\n\n(4) What do you mean by `best-effort hardware` for NAS only approach? \n\n(5) Do you think decoupling the search between neural architecture and constraint hardware search would deliver the same results? Or what would be the benefit if you don't perform any NAS, and just tune the hardware knobs in a constraint optimization settings?\n\n(6) How does your method works against some other baselines such as Mind Mappings---ASPLOS 2021 and ConfuciuX---MICRO 2020? (just for hardware search)\n",
            "summary_of_the_review": "The idea of co-optimization is interesting. However, with the current version of the paper, the contributions are not sufficient for publication. The motivation behind co-optimization of one neural architecture and hardware is weak and far from practical setting for accelerator design. Some of the evaluation setups are not clear and no comparison is provided with existing recent methods from the computer architecture community.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}