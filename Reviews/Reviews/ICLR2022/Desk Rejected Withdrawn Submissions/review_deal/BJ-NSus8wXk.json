{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a new method based on Q-learning that is able to detect out-of-distribution states during deployment time. In order to accomplish this, an energy-based regulizeer has been introduced in which it pushes down Q-values for out-of-distribution states during training time. In order to create out-of-distribution states during training, this paper utilizes some heuristic such as cropping applied to random locations to input images. To evaluate the proposed method, this paper utilizes 4 Atari games and Cartpole environment. ",
            "main_review": "I totally agree with the authors that the issue of safe deployment in RL has not gotten the attention it deserves. While this paper does a good job in motivating this problem, it doesn't make a convincing case for their method either.\n\n- First of all, I find their deployment scenario is very contrived. For instance, during deployment time, states generated from the other games which the agent wasn't trained on are used to emulate OOD states. This is very unrealistic case, why samples from Breakout should ever appear during Seaquest's deployment time? I am not sure under what circumstance this scenario would ever happen? In summary, the paper needs to introduce realistic scenarios for OOD to provide a clear picture about their method.  \n\n- Even if we ignore above issue, the proposed method only evaluated on small set of environments (i.e. 4 atari environments and cartpole to be exact) in which the paper fails to provide a good justification why these games have been chosen not other games. The authors said they are \"five most commonly used environments\" which I don't think so it is the case. Hence, the results from this paper are not conclusive and need more environments. \n\n- There are multiple issues with the results and experiments. 1) Looking at the learning curves, it seems the proposed method only improves Seaquest and it doesn't show any improvement with other environments. Assuming these learning curves show returns obtained by the learned policy during deployment time where x-axis shows the number of steps from the environment. If that is not the case, can authors provide such learning curves? 2) It seems experiments are done only for one seed as no standard deviation per different seeds are shown in the plots and tables 3) why these methods are only trained for 10 million steps (which is 40 million frames)? 10 million steps are not nearly enough to come to a conclusion about the performance especially about atari, previous works usually trained their agents at for 50 million time steps (200 million frames).\n\n- In regards to novelty, there are approaches in batch rl that are quite similar in terms of detecting and addressing OOD state-action pair. For example, both [1] and [2] proposed Q-value regularizers to address OOD data ( see Eq 6 in both papers). While the OOD part in those papers are about action not state, those method can be easily changed to apply to the OOD states. Even those methods can be used as baselines in this paper. Can author discuss them in details in the paper and potentially see if they can be used as baseline methods in the experiment section (i.e. combine their regularizers with DQN) \n\n- Can you explain what happens during deployment if your method detects an OOD state? does your agent simply stop and doesn't invoke any action? \n\nOther minor comments:\n- Can you explain in more details the relationship between negative free energy and p(S)?\n- Since {E_out} is a hyper-pramater, I'd suggest to use a lowercase greek letter.\n- ^2 is missing in $y(\\theta), line above eq 15.\n- You said code is released as supplementary material, but I don't see it.\n- Some results marked in bold in the paper are misleading for the same performing method. For example, Pong results in Table 1 ( first row),  PPO, Ensemble (10 NNs), and Ours all achieved 0 but only Ours is marked bold. \n  \n\n\n[1] Regularized Behavior Value Estimation https://arxiv.org/abs/2103.09575\n\n[2] Continuous Doubly Constrained Batch Reinforcement Learning  https://arxiv.org/abs/2102.09225\n\n\n",
            "summary_of_the_review": "Although this paper considers a very interesting problem in RL, their results and proposed method are not convincing and it requires more analysis and comparison with previous works as mentioned above. In summary, I don't think this paper is ready yet. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper considers the problem of detecting out-of-distribution states to avoid taking actions at states that the policy has not been trained on. The general approach is to synthesize \"out-of-distribution\" states, and penalize the value function at these states to be below some pre-determined threshold, and using this threshold to determine whether or not states at test-time are out-of-distribution. The approach is tested on a synthetic Atari suite.\n",
            "main_review": "While the problem of detecting out-of-distribution states is quite important and an interesting problem, the effort made in this paper seems lacking. I list my concerns in detail below, but two aspects seem especially concerning: first, that \"out-of-distribution\" states must be synthesized via this random cropping procedure, which may not generalize to other types of OOD states, and second, that the benchmark for evaluation, Atari games with other game states being OOD, is unreasonably synthetic. As a result, it is unclear to me whether the methodology of this paper will actually scale to more challenging / interesting cases for OOD detection, let alone in more real-world scenarios.\n\nStrengths:\n\n- The method in this paper is simple and seemingly easy to implement: it only requires to \"corrupt\" the images in the batch via some data-augmentation scheme, and add a simple conservative penalty on these states.\n\nWeaknesses:\n\n- The benchmark for evaluation here (when tested on Atari Game 1, to be able to tell that states from Atari Game 2 are OOD) is too simple, since image statistics for different Atari games are very different (e.g. different backgrounds). What I think would be a more compelling demonstration of being uncertain on OOD states is if the agent could distinguish different Atari game modes (which are much more similar visually) but whose different dynamics may make it difficult for the same policy to generalize. As it stands, one of my worries is that the current method (of generating noisy images and enforcing a pessimism penalty on these noisy images) only works because the test OOD images themselves are so visually different from the original game.\n- One thing that is untouched in this work, but is quite interesting, is what the RL problem setting affords an agent that is trying to detect if it is in an out-of-distribution setting. Unlike in supervised learning, where the agent only receives the observation $x$, and must make an immediate decision as to whether the setting is OOD, in RL, the agent additionally witnesses how the transition dynamics behave, what rewards the agent receives, etc. For example, if the transition dynamics behave differently at test-time than the agent originally witnessed during training, this provides a useful signal to determine whether the current state is OOD. The current scheme in the paper only considers what the state $s_t$ tells us about being OOD (that is, the supervised learning version of OOD detection), and not what the trajectory seen till date ($s_0, a_0, s_1, \\dots, s_t$) tells us. As such, the investigation spiritually seems more about OOD detection in supervised learning than in the sequential RL setting.\n- The paper only vaguely defines what \"in-distribution\" states are -- it would be useful to more explicitly outline what these are. Is it the state distribution that is actually reached by the current agent (e.g. $d^\\pi(s)$)? Is it the state distribution that is reachable by the agent in the training environment (for which, the replay distribution $d_{\\mathcal{H}}(s)$ might be a better approximation)? If the agent visits a state that has low (but non-zero) probability under $\\mathcal{P}$, is it \"in-distribution\" or \"out-of-distribution\"? How can \"in-distribution\"-ness be tested with a single sample in this setting (as is being done by a threshold function G(s))?\n- I would be curious to hear the authors thoughts on how sensitive such a method would be to the exact choice of training-time OOD states. It seems to me that the efficacy of such a method depends very heavily on this design decision: while the current choice of random cropping and noise injection works for detecting game states from other Atari games, it's not clear to me that this strategy would work well in other OOD-detection settings (even as a toy example, different game modes for the same game in Atari).\n- It would be useful in the experiments section to see a summary figure of Table 1, for a reader to quickly understand how the various methods compare instead of poring through a very dense table.\n- The line \"Note that all rewards encountered in Atari games are above zero, the lower bound of energy value for ID states is 0.\" is false. Many Atari games have negative rewards, e.g. Pong. \n- Given the author's claims that the regularizer doesn't affect the solution on in-distribution states, it is surprising to me that the regularized DQN outperforms the unregularized solution on ID states. Why do you suspect this to be case?\n- One suggestion to make the presentation easier to understand for the reader is to introduce these ideas in the max-ent RL framework instead of the standard RL framework, because the connection between Q-functions and energy-based models is already well-established. For example, in max-ent RL, $\\log \\sum_a \\exp(Q(s,a))$ is the value function $V(s)$, so the penalty becomes $\\mathbb{E}\\_{s \\sim U}[((V_\\theta(s) - E_{out})_+^2)]$, which is easier to interpret.\n",
            "summary_of_the_review": "(Copied from above) While the problem of detecting out-of-distribution states is quite important and an interesting problem, the effort made in this paper seems lacking. I list my concerns in detail below, but two aspects seem especially concerning: first, that \"out-of-distribution\" states must be synthesized via this random cropping procedure, which may not generalize to other types of OOD states, and second, that the benchmark for evaluation appears to be relatively simple and synthetic. As a result, it is unclear to me whether the methodology of this paper will actually scale to more challenging / interesting cases for OOD detection, let alone in more real-world scenarios.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose to regularize an RL loss with an OOD loss which drives down the Q values below a threshold for all actions of OOD states in order to be able to detect OOD states. They run experiments on a subset of Atari games and Cartpole. ",
            "main_review": "I think the paper could be greatly improved if better motivated. At the moment, I do not see why detecting OOD states is beneficial and how it could be done in practice. The authors provide the example of a self-driving car and the importance of being safe in unknown regions but do not provide any solution to that problem. What happens after an OOD state is detected? Still an action should be taken ? Is the system going to be handed to a human?\n\nAlso the definition of an OOD state is not clearly defined. What does it mean for an RL agent to encounter an OOD state? How this OOD state could be distinguished from a rare state? How is it possible that in deployment an RL agent encounters an OOD state? Does it mean that the agent has been trained on a safe environment and then deployed in a more challenging environment? Is it wise to deploy the RL agent in the first place? \n\nMy second concern is about the choice of OOD states in the experiments. I found it quite underwhelming to choose from other Atari games those training states. In that case we know for sure that the OOD states will no be encountered in deployment. How concretely will you choose OOD training states for a real-world example that could benefit the deployment of your RL agent?\n\nFinally, my third concern with the method is that it will bias the optimal behaviour in the real world. Indeed let say that the agent encountered a so called OOD state, now the Qvalues are very small but probably the agent will need to learn a policy at some point to overcome this difficulty but the problem is that its Q values have been completely biased  and relearning correct ones will take longer. Here the authors make the strong assumption that it will be never beneficial for the agent to learn a correct behaviour in those states and should delegate to another entity to take a decision.\n\nOverall, I have concerns with motivations and practical implementations of the idea in the real world.  The algorithm derivations are correct but they do not bring a novel or significant contribution to the RL community.\n      ",
            "summary_of_the_review": "The authors should better motivate their idea and provide a concrete example of how using their method could be beneficial in a real-world deployment.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concerns from my side.",
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposed a regularized loss for punishing the Q values for OOD states. They theoretically show this added loss will not influence the performance of vanilla DQN in both the tabular case and the DQN case. The theoretical analyses are intuitive with remarks that help the reader understand the importance of these results. They design experiments for evaluating the OOD detection performance and game performance. The rich ablation study helps the reader understand how the hyper-parameters are tuned.",
            "main_review": "This paper is trying to resolve the distributional shift problem between training and testing environments by detecting OOD states. This is an important issue for RL in terms of practical application. Since this problem is important, there are many existing frameworks or methods that are tackling a similar or even the same problem. However, this paper is trying to claim a new framework by arguing \"the study of handling OOD states in the RL environment remains underexplored\", which sounds questional. I list some existing frameworks that overlap with the proposed framework in many ways. If the authors have concerns over my suggestions, I am open to discussions.\n\n1. **Uncertainty-Aware/ Distrbutional / Risk-Sensitive Reinforcement Learning.**\nI merge these three RL frameworks since they are similar to each other in the sense they use Q values to estimate the uncertainty of states (or actions). They share a similar target to this paper, but they commonly apply uncertainty metrics (entropy or variance) to detect OOD states while this work uses \"smaller values\". However, I do not understand what's the gain here (see my questions.)\n\n2. **Offline Reinforcement Learning.**\nThe paper defines \"The general notion of OOD refers to inference-time inputs which deviate substantially from the data encountered during training\", this is almost the same definition in offline RL. it seems the author(s) is(are) not aware of this overlap, so they **rediscovered** the fact that \"RL agent can assign abnormally high Q values for OOD states.\". It is a well-explored, commonly studied conclusion under the setting of offline reinforcement learning, see figure 1 at [1].\n\n[1] Kumar, Aviral, et al. \"Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction.\" Advances in Neural Information Processing Systems 32 (2019): 11784-11794.\n\n3. **Constraint/Safety Reinforcement Learning.**\nThe regularization term or strategy has been well studied by constraint Reinforcement Learning. I do not follow the motivation behind \"unlike the underexplored states, the agent should not be encouraged to explore OOD states\", but if this is the case, Constraint RL, which imposes some soft/hard constraints on the OOD states, will do the job. \n\n[2] Yongshuai Liu, Avishai Halev, and Xin Liu. Policy learning with constraints in model-free reinforcement learning:  A survey. IJCAI 2021.\n\nApart from these Main concerns, I have several questions:\n\n1. Why \"unlike the underexplored states, the agent should not be encouraged to explore OOD states\"? In this way, the epistemic uncertainty will remain high in these states. Is this because of some safety issues like the motivation in Safety RL?\n\n2. In Figure 1, I do not follow why the outputs of the ensemble method are wrong while the outputs of the proposed method are correct? Commonly speaking, these states are unexplored, so an ideal agent assigns them huge uncertainty instead of a deterministic value.\n\n3. Why not use the uncertainty metrics (entropy or variance) to detect OOD states? What's the gain of using \"smaller value\"? As I mentioned, if the agent is encouraged to avoid the OOD states, just define a constraint on the uncertainty values like the way in Constraint RL. \n\nExperiments\n\nThe main results are detection results, which are novel for general RL works. This is ideal, however, I am a little surprised by the impressive performance, and have several concerns:\n\n1. During training, the OOD states are cropped and noisy images, however, during testing, the OOD states are states from other environments. There is a significant mismatch between training OOD states and testing OOD states. (i.e., The testing OOD states are actually OOD to the training OOD states). Why do we expect the method will work?\n\n2. I do not think PPO, A2C, and DQN are trying to detect the OOD states. They are methods for solving ID games. Other uncertainty-aware/ distributional / risk-sensitive / meta reinforcement learning (see [2]) methods should be compared here. QR-DQN is a good start, the author should include QR-DQN (and its advanced version like FQF or NC-QR-DQN) as a direct comparison instead of building compatible agents (the results in Table 9).\n\n3. Will the OOD states (in U) be available for the comparison methods during training? If not, does the advantage comes from the added information? \n\n4. Most importantly, I think the motivation of this work is building an unknown-aware RL agent. Maybe the model has a better OOD detection performance, but there is no evidence that this method will actually work in the testing environment. In other words, avoiding unknown states will not help to solve the RL problem. For example, the self-driving agent can not operate well by just avoiding unknown states, it **must** transfer the policy learned in the training environments to solve the testing environments, like how the meta-RL works. I strongly encourage the author to include their performance in a testing environment. In this case, Atari is not an ideal environment while Mojuco is a commonly applied environment (see [3]). If it is possible, please include the results.\n\n[3] Zintgraf, L., et al. \"VariBAD: a very good method for Bayes-adaptive deep RL via meta-learning.\" Proceedings of ICLR 2020 (2020).\n\nMinors:\n1. \"Note that Eneg is proportional to log p(s) with some unknown factor that may depend on the state s.\" I do not see why this is correct. Maybe I am confused. Please explicitly define the factor anyway.\n",
            "summary_of_the_review": "In summary, the paper is tackling an important question, but the author does not properly present the relation between their work and previous studies. There are some overlapped definitions, discoveries, and concepts with previous works. The author should cover more details. I strongly encourage the author to formulate their method under an existing framework instead of proposing something new. If this is indeed new, they need to explain the key differences from other frameworks that I listed above. For the experiment, I understand the author includes some novel designs, the results are strong but not convincing. There is a gap between the observed results and the explanations of these improvements.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}