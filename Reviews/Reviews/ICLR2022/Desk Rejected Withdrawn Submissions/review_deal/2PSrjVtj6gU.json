{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes an improvement to graph based neural network, by improving their attention mechanism (introducing recursive attention and jumping knowledge attention) to flexibly attend to its neighborhood. The paper shows solid experimental results over competitive baselines, as acknowledged by reviewers. The reviewers agree that the paper is clearly written, but overall have issues with the novelty of the approach. The paper combines multiple components (last residual connection module, improved attention mechanism) to show gains, but none of the pieces are very new."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposed the Graph Attention Multi-Layer Perceptron (GAMLP) model that combines features extracted from two streams: 1) node-adaptive features that are obtained via \"Graph-wise / layer-wise propagation\" (Section 2.2), 2) features obtained from label propagation. The author also proposed two ways for performing the node-adaptive attention: JK attention, and Recursive attention (Section 3.3). Extensive experiments are conducted on 9 transductive datasets and 3 inductive datasets. The results show that the proposed GAMLP(JK) and GAMLP(R) can outperform the baselines.\n",
            "main_review": "- Overall Novelty\n\nCombining layer-wise propagation and label propagation is a good idea. However, it is not clear what's new in GAMLP except ensembling two types of models. Fusing these two streams do not look very challenging and it is foreseeable that the model can achieve competitive performance as GBP (based on layer-wise propagation) or UniMP (based on label propagation). The proposed \"JK attention\" and \"Recursive attention\" are also not very novel and are largely borrowed from previous works like JK-Net.\n\n- Technical Quality + Writing\n\nThe technical quality of the paper is good. The author summarized the current landscape of scalable GNNs and categorized them as 1) sampling-based, 2) graph-wise propagation and 3) layer-wise propagation. The author also provided the reference to prior works such as label propagation and JK-Net. Also, the author clearly stated the general algorithm in Figure 2 and the detailed design in Section 3. However, the author can still improve the paper by providing insights on some technical designs, e.g.\n\n    1. Why choose the `cos(...)` formula in designing the \"Last Residual Connection\" module?\n    2. What's the fundamental differences between JK attention and Recursive attention? Why would you propose two variants?\n\n- Experiments\n\nThe author conducted extensive experiments in 9 transductive datasets and 3 inductive datasets. However, I find that the baseline selection is not consistent across the experiments. For example, GBP is not compared in Table 2. UniMP is not compared in Table 1.\n\nIn addition, the trend of validation and test accuracy reported in Table 5 is not consistent (models with higher validation accuracy tend to have lower test accuracy). The author needs to clarify the inconsistency.",
            "summary_of_the_review": "The general idea of GAMLP is neat and reasonable. However, I'm hesitant to accept the paper because I haven't seen a contribution that is clearly novel. In addition, there are some inconsistencies in the experiments.\n\n——— Post Rebuttal ———\nThe author has addressed my concerns regarding the experiments and the motivation of proposing two attention mechanisms (recursive attention v.s JK attention). Given the nice results and scalability of GAMLP, I’ve increased my score to 6.  However, I’m still concerned about the novelty of the paper because it proposed several ideas but none of them is really novel. The node-wise feature propagation idea looks like a direct extension of the previous layer-wise / graph-wise propagation mechanisms. The major difference is that the node-wise propagation will use the propagated features from multiple depths. Thus, I haven’t increased my score to accept due to lack of novelty. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes the scalable GAMLP architecture that can learn node-adaptive features and can utilize label propagation to improve the model performance. Comprehensive experiments are provided to demonstrate the effectiveness of GAMLP, discuss the importance of each component in GAMLP, and compare its efficiency against some classic and SOTA baseline models.",
            "main_review": "Strengths:\n1. The writing of this paper is very clear, the organization is easy to follow and the main ideas are delivered clearly.\n2. I appreciate the comprehensive experiments. \n3. The model performance is impressive. \n\n===\n\nWeakness：\n1. I feel there are some similarities between this paper and the work of SLE [1]. According to my understanding, both papers try to improve the SIGN model [2] by introducing a node-wise attention mechanism and utilizing label propagation to further enhance the performance. I am wondering, what is the key difference between GAMLP and SLE, and what are the advantages of GAMLP compared to SLE?\n2. I am also interested in the performance comparison between GAMLP and SLE.\n3. For the ablation study (section 4.3) Q2 (2), the authors compared the attention mechanism with different reference vectors. However, I am also interested to see how can the proposed attention mechanism affect performance?  For example, what would happen if each Wk in eq(3) is a learnable scaler? or what would happen if directly concatenate [X^(0)...X^(k)]?\n\n===\n\n[1] Sun, Chuxiong, and Guoshi Wu. \"Scalable and Adaptive Graph Neural Networks with Self-Label-Enhanced training.\" arXiv preprint arXiv:2104.09376 (2021).\n[2] Frasca, Fabrizio, et al. \"Sign: Scalable inception graph neural networks.\" arXiv preprint arXiv:2004.11198 (2020).",
            "summary_of_the_review": "My major concern is the novelty. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes an efficient method for inductive/transductive node labeling based on fast feature propagation, label propagation, and attention and yields good experimental results.",
            "main_review": "The paper introduces an efficient way to perform node labeling tasks on inductive and transductive setting. It achieves good results on the widely used benchmarks. Here are the questions / concerns:\n\n1- In some parts it is difficult to follow the paper. And at some points the information is missing. e.g., definition of T in eq(2) and s in eq (7). eq (7) and (8) seems to be identical. The dimensions are not explicitly mentioned which could have made the paper more clear.\n\n2- In terms of contribution, the paper seems to have incremental contributions with respect to previous work but not a significant one. Using a weighted average over the features from different layers seems very similar to attention graph pooling and also feature propagation is not different from previous work. Also using label propagation has been previously explored.\n\n3- The other potential limitation of the proposed model is that it is tailored to supervised setting and won't be able to perform in contrastive setting with linear evaluation protocol.",
            "summary_of_the_review": "The paper introduces some incremental contributions with empirical support. But at the end of day, the contributions are slight variations of previous work. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a simplified graph attention model that operates in two stages: first label propagation of the labels and features and then learning MLP on obtained predictions. The experiments show improvement on 12 graph classification datasets. ",
            "main_review": "The paper proposes an end-to-end model that first propagates separately node features and labels, combines them with attention weights, aggregates embeddings and trains MLP model. For node-adaptive attention mechanism the authors propose two schemes: Recursive attention and JK attention. Experiments clearly show state-of-the-art performance. \n\nDespite its advantages, I see the following limitations of this work: \n1) Limited novelty. All the blocks used in the model were presented in the past and showed superior performance. Hence it's expected to see superior performance when combining those. \n\n2. Figure 1 used to discuss limitations of the existing approaches is not plotted for the proposed approach. Does GAMLP solves the aforementioned limitations? \n\n3.Cora/citeseer/pubmed datasets are quite small to motivate such an approach. While there are OGB datasets, all of these data are homophily-based. It would be good to compare the model on heterophily-based data [1] (where LP may fail) and on data with heterogeneous node features [2] (where graph-agnostic MLP and GBM can lead to better results). It's also interesting to compare with solely MLP baseline and GBDT baseline [2]\n\n[1] New Benchmarks for Learning on Non-Homophilous Graphs https://arxiv.org/abs/2104.01404\n\n[2] Boost then Convolve: Gradient Boosting Meets Graph Neural Networks https://arxiv.org/abs/2101.08543 \n\n",
            "summary_of_the_review": "The paper is well-written and attains good performance metrics; however, contributions are limited and are composed of previously studied ideas. To improve the paper I would seek a good application of such model besides the standard node classification (e.g. such applications that require simplicity of ML models). ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}