{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper describes a series of experiments and theoretical considerations regarding the implicit encoding ability of the transformer decoder resulting from its causal mask. It has been shown that though, as the authors suggest, two or more layers are sufficient for the decode to encode word positions, explicit position encoding improves the decoder's performance. Nevertheless, the presence of implicit encoding seems beneficial. The latter finding was used to extend the architecture of the transformer encoder, leading to its better performance.",
            "main_review": "The paper is interesting and easy to follow. Its experimental part, reporting results averaged over a few random seeds, is convincing and seems methodologically correct. I have, however, several objections regarding the naming and correctness of theoretical considerations.\n\nFirst of all, the _position encoding ability_, as you define it, is a relative term, dependent on the sequence of tokens — first, the sequence of tokens $S$ is chosen, and then, one verifies whether the permutation-related condition is fulfilled. At the same time, the beginning of the proof assumes _any_ sequence $S$, suggesting that definition should be formulated in terms of any sequence $S$. This is, however, where another problem arises — there is no function with such a property.\n\nProof: Let $S$ be a sequence where $x_1=x_2=...=x_n$. Since there is no permutation changing this series, the requested function $func$ where the inequality holds does not exist. This, among others, suggest that proof cannot be correct.\n\nMoreover:\n- function with the property you refer to in Definition 1 and later is permutation _equivariant_ (not _equivalent_);\n- token embedding layer is not _order-invariant_ — it leads to a different order of attention scores and is _permutation equivariant_;\n- as you did not exclude identity permutation from $\\Pi$, it is not true that at least two vectors are swapped for any permutation $\\pi \\in \\Pi$.\n\n",
            "summary_of_the_review": "An interesting paper with a good experimental part and theoretical part requiring major changes. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethical concerns.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies the implicit positional encoding ability of Transformer Decoders with both theoretically empirically. The authors claim that Transformer Decoders with no less than two layers has positional encoding ability, and evaluate Transformer Decoders without positional encoding empirically. \n\nThis paper further proposes DecBERT, which incorporates autoregressive attention masks into the first two layers and achieves performance gains over baselines.",
            "main_review": "**Strengths**\n\n* The empirical study on Transformer Decoders with/without positional encodings is comprehensive. It shows that Transformer Decoders do have the ability to encode positions, although using explicit positional encodings leads to better results.\n\n* The proposed model DecBERT is simple and interesting. According to the authors, it achieves better results than baseline models.\n\n**Weaknesses**\n\n* The theoretical analysis in Sec. 3 is problematic. For example, the condition $\\pi\\in\\Pi$ in definition 2 should be $\\pi\\in\\Pi\\backslash\\{e\\}$, where $e$ is the identity element in the group $\\Pi$​; Otherwise no functions would have positional encoding ability by definition. Besides, to make Proposition 2 correct, assumptions must be made on the token embedding layer (e.g., different tokens have different embeddings, etc). Although I agree with the intuition behind this part to some degree, Sec. 3 is far from a rigorous theoretical analysis.\n\n* The inquiry on the positional encoding ability is limited and superficial. According the authors, the ability to encode position is to behave differently when the order of the input changes. However, the power of position encodings is much more than this. For example, position encodings can help the model know distances between tokens, which the authors do not take into considerations. Empirical results in Sec. 4 also show that explicitly encoding positions would be better (and using sophisticated positional encodings as Transformer-XL might lead to larger performance gains).\n\n* I don't think the analysis in this paper explains why DecBERT-Diff with PE achieves the best performance in the experiment. \n  * First, all the previous analysis focuses on causal attention masks with the same direction, but this model uses causal attention masks with two directions. \n  * Second, DecBERT-Diff with PE already uses explicit positional encodings, and it makes me wonder whether the implicit positional encoding ability of its first two layers do help. \n\n* Although the structure of this paper is clear, its writing quality is below average and impedes readability.",
            "summary_of_the_review": "Overall, I vote for rejection.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper use the auto-regressive attention masks in the first two layer of transformer models to provide implicit position  position encoding and conduct multiple experiments to verity the ability for position encoding. ",
            "main_review": "Strengths: \n  1) Simple;\n  2) Implicit position encoding can be combined with explicit position encoding(learnable/sincos position embedding);\n\nWeaknesses:\n  In this paper, the main NLU experiments in this paper focus on the sentence level task (such as GLUE). But for some token-level tasks are also very important in NLP. For example, SQUADv1.1/v2, a task that has been widely used as an evaluation benchmark in many past works (such as BERT and RoERBTa) and is easy to finetune by using transformers repo(https://github.com/huggingface/transformers). \n",
            "summary_of_the_review": "This paper is will written and organized. The core idea is simple and seems effective, but the result experiments does not convince me.\nI would expect a more standard result for the final comparison (ablation experiments are not required), such as the experimental setting of the most original setting for BERT-base or ablation setting for RoBERTa-base (256 batch size, 1m steps update). Since many subsequent works follow this setup, this gives the reader a better view of the specific enhancements.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper study the effect of implicit position encodings that is widely used in Transformer Decoder. The authors approved that two or more layers are required to achieve encoding ability when the explicit positional encoding is missing. The authors propose a new mode DecBert, where the Encoder uses the attention masking similar to Encoder. The effectiveness of the new architecture is verified on the GLUE benchmarks.There are also many experiments about different ways to encode the positional information.",
            "main_review": "Strength:\n\n- The proof of position encoding in Section 3 is clear written and easy to follow.\n\n- The purposed new model is novel.\n\n- The paper is well organized.\n\nWeakness:\n\n- The motivation is not clear. Why using explicit position encoding is a *limitation* for the Transformer?\n\n- Experimental results are not significant:\n\n    1. In Section 4.3, the author claims the Mul-PE is better than Sinusoidal-PIE, but the difference is just 0.03 (22.92 vs 22.95). Also, the statement that Add-PIE is better than Learnable-PE is also not significant because the difference is only 0.01 (23.36 vs 23.37)\n\n    2. The RoBERTa-ReImp results in Table 3 are lower than the original results in RoBERTa. For example, the RoBERTa base results on SST-2 is 94.8 in original paper but only 89.56 in Table 3.\n\n- The best model in Table 3 is DecBERT-Diff, which use opposite direction (right to left) masking. However, all study in Section 3 is about Decoder masking (left to right). More theoretical discussion is required about the direction.\n",
            "summary_of_the_review": "I recommend rejecting because:\n1.\tThe motivation is not clear.\n2.\tThe experimental results are not significant. \n3.\tThe analyze about right-to-left masking, which is actually used in DecBERT, is missing.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}