{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper introduces a new classification setting in which the data from one class is available only. Even though the data is available for one class, the authors assume that the probability scores of each instance to different classes are provided. The authors provide two real world applications for this specific setting; however they do not explain why/how such detailed scores can be provided/obtained. Most of the analyses in the paper are straightforward and direct result of the specific setting of the problem. The experiments are only performed on artificially created dataset from existing multi-class dataset, which further questions that such scenario could exist in real world. ",
            "main_review": "Strengths:\n1- The paper is well-written\n2- It is addressing an interesting multi-class classification setup\n\nWeaknesses:\n1- Fails to justify that such scenarios exist in real world and the required scores for the setting can be obtained\n2- The experiments are only performed using artificial data set.",
            "summary_of_the_review": "Even though the setting of the proposed multi-class classification is very novel, the authors fails to show the practical important of this setting.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper considers a multiclass learning problem in which the inputs are data from a single class, and in addition, for each example in the input data, the full posterior class probability vector (\"confidences\") is known. The method uses importance-reweighting to construct an unbiased risk estimator that can be minimized using ERM. In real applications, accurate posterior class probabilities might not be given, so the paper proposes a refined method that works with noisy posterior class probabilities, under some conditions. Finally, the method is extended to deal with the case where data come from a subset of classes. Theoretical guarantees are proved and experiments demonstrate the effectiveness of this method.",
            "main_review": "Learning from data with information about posterior class probabilities is in general a strongly supervised task. Even if one only considers data from a single class, it is still not a weakly supervised learning task.\n\nThe core idea in this paper is to use importance weights to construct an unbiased risk estimator. Such idea has been explored in learning under covariate shift, and more specifically, in Ishida et al. (2018). In this regard, this paper naturally extends Pconf classification (Ishida et al., 2018) from binary to multiclass.\n\nI have reviewed this paper in NeurIPS 2021. The consensus was that the paper had limited novelty and significance. Unfortunately the current version is basically the same as the one submitted in NeurIPS 2021, with virtually no improvements.\n\n---\n\nStrengths:\n\n1. Clarity. This work is well-executed and well-written. \n\n2. Quality. This paper is sound. Claims are well-supported by theoretical results.\n\n3. The paper is a natural extension of Ishida et al. (2018), but the formulation using noisy confidences is new, to my best knowledge.\n\n---\n\nWeaknesses:\n\n1. This paper is build on several strong assumptions. Assuming probability of each label given each example is very strong, even if all examples come from the same class. This assumption is far from being realistic. The authors acknowledged this limitation and proposed a \"noise-robust\" version of the method, but the noise setting (Theorem 3) is also very strong: it is assumed that for each instance x, the argmax of the clean posterior class probabilities is the argmax of the noisy posterior class probabilities. It is not surprising that the method works because with this assumption, predictions by taking argmax of noisy confidence scores will be correct. The authors should relax this noise assumption. Finally, there are some other assumptions that have some negative impact from practical point of view, for example, $p(y_s|x) > C_r$ for some $C_r > 0$ in Theorem 2, and $\\inf_{x \\in \\mathcal{X}} \\Delta(x) > 0$ in Theorem 4, though these assumptions are typically required for the theory.\n\n2. The experiments are not able to show this work's practical usefulness. The experiments circumvent the strong assumptions above by using synthetically generated posterior class probabilities. Specifically, the authors trained a probabilistic model on a held-out data set and then used the probabilistic model to stimulate confidence scores. This experimental design makes it harder to see the practical effectiveness of the method. It would have been more convincing if a more realistic setting is considered where the data was collected, not synthetically generated.",
            "summary_of_the_review": "This paper naturally extends Ishida et al. (2018). The work is correct and rigorous. But the results in this paper are not surprising given the strong assumptions and prior work. The assumptions are not realistic and limit the practical usefulness of the proposed method. The experiments are not able to mitigate this issue, nor do they demonstrate the practical effectiveness. Therefore, the novelty and significance are not enough to be accepted.\n\nAlso, the two weaknesses were the main concerns in the review for NeurIPS 2021.  Instead of addressing (or at least making attempts to address) the two concerns, the authors tried their luck at resubmitting the same paper to another conference. Such behavior just wastes reviewers' time and the whole community's resource, and should be deterred.\n\nConsidering all of the above, I vote for weak reject.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The proposed SC-Conf learning framework enables the use of empirical risk minimization for training a multiclass classifier by constructing an unbiased multiclass classification risk estimator with single class data and their confidences. The authors further analyze the effect of noisy supervision and give a consistent method by introducing unlabeled data. Theoretical and experimental results are provided to support their claims.\n\nContributions:\n1. The authors show that it is possible to learn a discriminative multiclass classifier that is statistically consistent with only data from a single class and their posterior possibility vectors.\n2. An unbiased risk estimator with data belonging to a subset of all the classes is also proposed to expand the setting of SC-Conf learning.\n3. The authors discuss the potential effects of noisy confidences and give a practical consistent method for learning with inaccurate supervision.\n4. Sufficient analyses on the consistency of proposed methods are provided. \n",
            "main_review": "Strengths:\n1. The authors show that it is possible learning a consistent classifier without data from all the classes, which can help in many real-world applications. Furthermore, the confidences used for consistent learning are not necessarily accurate, and this framework does not limit the use of different models, losses, and optimization algorithms, which makes this work more practical and robust.\n2. The assumption used in the analysis of noisy supervision aligns with many common types of noises in the field of multiclass classification.\n3. The analysis of noisy confidences is novel. Though the effect of noisy confidences is studied empirically in [1], to the best of my knowledge, this work is the first one that theoretically shows the connection between consistency and the magnitude of noisy confidences.\n4. Overall, this work is well-written and well-organized. The experimental results on four different datasets are also convincing and support the theory.\n\nWeakness:\nIn section 3.2, Theorem 2 shows that the lower bound of the confidences plays an important role in the estimation error bound, and the error bound approaches infinity if the confidences are small enough. However, the experimental performance of SC-Conf learning are close to or even comparable with those of supervised learning, which indicates that the effect of small confidences is limited. The authors are encouraged to give further theoretical analysis on this phenomenon.\n\n[1]. Takashi Ishida, Gang Niu, Masashi Sugiyama: Binary Classification from Positive-Confidence Data. NeurIPS 2018: 5921-5932\n",
            "summary_of_the_review": "In this work, a meaningful weakly-supervised learning framework is proposed to learn a consistent multiclass classifier without data from all the classes. The authors further consider the effect of noisy supervision. The theoretical analyses are comprehensive and rigorous. The experimental results support the claims in this paper. Overall, I vote for acceptance for this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper the authors introduce a novel weakly-supervised learning setting that is called single-class confidence classification. Instead of traditional multi-class classification, the idea is that only data from one class is observed, as well as confidence for that class. \n\nSeveral theoretical results are presented. In a first theorem, the authors show that the risk in this new problem formulation is the same as the risk of traditional multi-class classification (on population level). Other theoretical results discuss regret bounds and consistency for the newly-defined risk. \n\nIn the experimental results a few methods are compared on two image classification datasets, using a setup where ground-truth confidence scores are created in an artificial manner. ",
            "main_review": "This paper is well written and the main ideas are easy to follow. As far as I know, the analyzed problem setting is novel to the literature. \n\nHowever, I do find the problem setting very artificial, and very disconnected from real applications. This is clearly visible in the experimental section, which lacks a good application. Instead standard multi-class classification datasets are manipulated to reconstruct the problem setting that the authors put forward. The two applications that are mentioned in the introduction are also not convincing to me. I am quite familiar with climate research, and I don't see how the first application would really appear in practice. I am less familiar with market investigation, but also this example looks very unnatural. \n\nIn this paper the two differences with standard multi-class classification are:\n- for the data points class probabilities are obtained instead of class labels\n- data points are sampled according P(x | y_c) where y_c is a specific class, instead of sampling according to P(x,y). \n\nConcerning the first difference, I would argue that in the setting of the authors one has MORE supervision compared to traditional multi-class classification. This difference has been analyzed in many existing papers, see e.g. Waegeman et al. \"Supervised learning algorithms for multi-class classification problems with partial class memberships\", Fuzzy Sets and Systems, 2011. \n\nConcerning the second difference, I do not agree that only data of one class is observed in the problem setting that is introduced. For the obtained samples (x,r) the ground-truth class probabilities P(y | x) are assumed to be given, so one obtains data of ALL classes. In fact one is working in a specific covariate shift setting, because training data is sampled from P(x|y_c) instead of P(x,y). From that perspective it is obvious that Theorem 1 holds, since it only concerns an analysis on the population level. As soon as the population risks of (1) and (2) are replaced by training data risks, there is no equivalence any more. From that perspective, it would be interesting to include standard multi-class classification as a baseline in the experiments. This would allow to quantify what is gained or lost in performance by adopting the two changes listed above. ",
            "summary_of_the_review": "Well-written paper, but not convinced of the practical usefulness of the problem setting",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}