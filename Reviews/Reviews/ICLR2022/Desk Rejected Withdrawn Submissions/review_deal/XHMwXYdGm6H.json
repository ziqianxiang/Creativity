{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors analyze the recently-introduced negative sampling method to address missing labels in named entity recognition (NER). Their study addresses an important and common problem in NER training. The authors significantly improve upon the previous understanding of why negative sampling works. Specifically, the authors find that missampling rates (defined in the paper) and negative sample uncertainty (roughly “difficulty”, also defined in the paper), strongly influence the performance of the trained model. The authors use these insights to enhance the negative sampling method to yield even better results. \n\n\n",
            "main_review": "# Strengths:\n- presents insights that substantially improve our understanding of NER training with missing labels\n- a practical and effective method to improve NER training with missing labels. Practitioners will use this. \n- the paper is for the most part well-written\n\n# Weaknesses:\n\n1. ICLR isn’t the perfect conference for this paper. ICLR welcomes contributions on “representation learning for computer vision and natural language processing” (and other topics, of course). While this work involves some representation learning (neural networks are being trained), its innovations (pertaining quite specifically to labeling issues in named entity recognition) are of greater interest to an NLP community than to a general ML audience. This paper would be stronger if it were submitted to an NLP conference. \n\n2. The proof of Theorem 1 has a zero in denominator when n=1 (sentence length is 1), and the last expression in the proof (the bound) is negative if lambda is close to 1 and n=2. Could the authors improve the proof, and possibly also change the theorem itself to include something like “for sufficiently large lambda”?\n\n3. In Section 3.2.1, the authors define uncertainty in terms of a probability distribution generated by an oracle. To me, an oracle is a mechanism that can, should there is a single correct answer (e.g. label for a token), give the correct answer. So there is true ambiguity, an oracle can output a single answer, and will not have to hedge its bets by outputting a probability distribution. So an oracle is only affected by inherent ambiguity in a task, not by task difficulty. For example, a sentence like “France’s Bordeaux region’s soccer coach John Smith’s home was sold” may be a hard example for a BiLSTM, or even BERT. However, the sentence is unambiguous to a human, and it would be unambiguous to an oracle. So although this sentence is difficult for NLP methods, an oracle would assign a point mass to the probability distribution. It might be more useful to leave out references to an oracle, and just say that we can use a helper model P_h (instead of oracle model P_o) to define uncertainty H_{P_h} as the entropy of the label distribution generated by model P_h . This is also more practical: as the authors hint in the paper, we almost never have an oracle model, while an “approximate model” is easy to train, making H_{P_h} a quantity that we can compute in practice. \n\n4. Section 4 does not mention the sampling rate parameter lambda (introduced in https://openreview.net/pdf?id=5jRVa89sZk) anymore. Do the authors of the paper reviewed here still sample at a rate lambda, or is sampling solely done according to equation (5) in the paper? Could the authors clarify this? \n\n5.0 I am a bit surprised by the outstanding effectiveness of the original negative sampling method, and the enhanced negative sampling method presented here. Both methods work well even with a very high degree of missing labels (e.g. 50%, or more in the case of the method presented in this paper). Intuitively, I would expect that almost all missing labels (i.e. false negative labels) must be excluded through negative sampling for the methods to be so unaffected by missing labels. The original negative sampling method samples randomly, and claims in Figure 5 of https://openreview.net/pdf?id=5jRVa89sZk that negative sampling rates between 0.1 and 0.9 all work very well. I find that surprising, given that at a negative sampling rate of 0.9, almost all false negatives would still be present in the training data. Can the authors of the paper reviewed here make sense of this? And: what sampling rates do they find work well for enhanced negative sampling? What proportions of false negatives do negative sampling and improved negative sampling indentify?\n\n5.1 More specifically: in Figure 5 of the paper that introduced negative sampling (https://openreview.net/pdf?id=5jRVa89sZk), it appears that at a masking probability of 0.6 on CoNLL-2003, and a negative sampling rate of about 0.8 or 0.9, the F1 score is about 0.86. Extrapolating to a negative sampling rate of 1.0 (i.e. using all negatives, without subsampling, which would be the regular “BERT Tagging” model), I would expect the performance to be around 0.86 as well (the F1 does not appear to vary much with the negative sampling rate). However, Table 1 from the same paper states that the “BERT Tagging” model achieves an F1 0f 64.84 at 0.6 masking probability on CoNLL-2003. How could the F1 drop from 0.86 with 0.9 negative sampling rate to 0.6484 with a 1.0 negative sampling rate? In both cases, very few or no false negatives were removed. Since the paper under review here is advertised rethinking the original negative sampling method from https://openreview.net/pdf?id=5jRVa89sZk , the authors may already have reproduced Figure 5 in https://openreview.net/pdf?id=5jRVa89sZk themselves. \n\n5.2 I also encourage the authors of the paper reviewed here to include the “BERT Tagging” F1 score in an extra column in their Table 3, as the “BERT Tagging” model remains a natural (though suboptimal) baseline. Similarly for Figure 3. \n\n5.3 It may be useful to create something like Figure 5 from https://openreview.net/pdf?id=5jRVa89sZk , and include BERT Tagging (straight line, since it doesn’t use negative sampling), BERT Tagging with negative sampling (same data as in the original negative sampling paper, if the authors are able to reproduce this), and BERT tagging with the improved negative sampling proposed in the work reviewed here. \n\n5.4 It would be useful to measure and report what proportion of false negatives was excluded from training through BERT Tagging with negative sampling, and BERT Tagging with improved negative sampling. \n\n5.6 The above figures may not make it into into the main paper, but they should absolutely be in an appendix. \n\n6. Table 3 contains data for masking probabilities between 0.5 and 0.9. Having more than 50% of labels missing is quite extreme, though it may happen occasionally. For practitioners to help decide whether to use negative sampling or improved negative sampling, could the authors also include data for masking probabilities 0.0, 0.1, 0.2, 0.3 and 0.4? \n\n7. There is a growing literature on deep learning with noisy labels, independent of NER. Have the authors considered benchmarking against e.g. the label refurbishment method described in https://arxiv.org/pdf/2007.08199.pdf  ? It would also be good to include a description of what related work exists. For instance, using model uncertainty to remove examples from a training set with noisy labels is not new. \n\n8. The authors write “To avoid being influenced by “rare events” we erase the points supported by too few cases (i.e., 20).” I do not see any points removed from Figure 2. What was removed?\n\n9. I could not find supplementary information or code for the submission. Are the authors planning to release their code? \n\n10. A small stylistic issue: the snippet “Is is monotonically decreases with rising m” would read better as “Better is monotonically decreasing with as m increases”\n\n11. “find that unlabeled entities” should be “found that unlabeled entities”\n\n12. I did not understand the second part of this snippet: “However, Li et al. (2021) haven’t well explained why negative sampling works and there are weaknesses in their principle analysis” . Would “However, Li et al. (2021) haven’t well explained why negative sampling works.” alone capture what the authors are trying to express? ",
            "summary_of_the_review": "I rate the paper “Marginally below the acceptance threshold”. The paper makes an exciting read for members of the NLP community, and for NER practitioners in particular. It would be a good paper for a top NLP conference. However, the work may not be quite as relevant for an ICLR audience. \n\nIrrespective of the conference to be submitted to, Theorem 1 and its proof need some improvement to be fully correct, and I would highly recommend that the authors release the code and address other questions asked in my review above. Finally, I would be grateful if the authors could provide more information on what proportion of false negatives the original negative sampling and improved negative sampling methods remove from the training data. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper shows a new approach for Named Entity Recognition using negative sampling.  The proposed method of sampling negative instances considers unlabeled entities to prevent showing the models contradictive examples. In detail, the authors measure the uncertainty of an instance as the entropy of its label distribution. In practice, they use the same model to learn the NER task and sample negative instances. Experimental results show competitive results on various datasets.",
            "main_review": "Overall the paper is characterised by the following strengths:\nFirst, the proposed method is an elegant and easy approach. - - - This makes it easy to apply or extend. \n- The experiments test different essential aspects and cover various datasets.\n- The argumentation of the paper is clearly stated and supported with formal definitions when needed.\n- The paper is well written and easy to follow.\n\nAfter reading the paper, it leaves the following critical points open to discussion:\n- As far as one can understand, the experiments are run just with one random seed. However, this does not give additional information about the influence of random processes and, thereby, the approach’s stability. Thus, a multi-seed evaluation could enhance the experiments - despite the additional computational expenses. Further, such new insights can support the results (especially since the sampling of negative samples involves extra randomness) by answering the following questions: \n   - How stable are the model/results on the different tasks in general? For example, what is the mean value, standard deviation? \n   - How does the proposed approach affect stability compared to vanilla Negative Sampling?\n- At the end of page 6, it is stated that the results are statistically significant. Is it done with a statistical test? If yes, which one was used?\n- Negative sampling, pre-filtering, or labelling instances based on an oracle model are discussed more broadly than in NER like in Data Augmentation (Feng et al., 2021) or Curriculum Learning (Hacohen and Weinshall, 2019). Thus, additional references could give the approach a broader context.\n- Three reference approaches (Li et al., 2020; Yamada et al., 2020; Wang et al., 2021a) are missing in the resulting section, marking leading approaches on CoNLL-2003 and  OntoNotes 5.0 on the time of ICLR submission. \n\nThere are some details of the experiment missing:\n- What specific were BERT models (English and Chinese) used?\n- Which values were tested during the grid search on page 6?\n- An anonymised repository with the code of the experiments is highly appreciated.\n",
            "summary_of_the_review": "In summary, this paper makes an important contribution to how misleading cases can be handled. There are some weaknesses that should be addressed by the authors. Although these issues also affect crucial aspects, such as improving reproducibility, they do not affect the fundamental values of the paper.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors consider the unlabeled entity problem in the task of Named Entity Recognition (NER). Building on previous work (Li et al., 2021) that did negative sampling to reduce the impact of unlabeled entities, they analyze how negative sampling works and propose weighted negative sampling, which showed improvement on several datasets.",
            "main_review": "Strengths:\n\nThe paper is well written and easy to read, results are clearly presented.\n\nThe improvement on real datasets is only ~1%, but that can be significant in the NER application.\n\nWeaknesses:\n\nThe core idea of the paper is to weight negative sampling by using the current predictions of the text spans and their uncertainty (entropy). This is a standard technique in active learning, so the core idea is a bit low on novelty and feels incremental.\n\nThe analysis of negative sampling is interesting to read but quite simple: missampling rate and uncertainty unsurprisingly affect performance. With (Li et al. 2021) identifying the unlabeled entity problem, it is clear that the rate of unlabeled entities affects performance. It is also well known in active learning that uncertain items tend to be more useful for training models.\n\nThe experimental results show improvement compared to uniform sampling in previous work, but it's not clear which factors lead to that improvement: missampling rate or uncertainty? is the scheduling of T and the tradeoff mu important? An ablation study may be helpful.",
            "summary_of_the_review": "Promising results but a bit low on novelty and need more experiments.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper analyzes the effectiveness of negative sampling for NER and proposes an adaptive and weighted sampling distribution to improve it. In specific, the authors define two types of measurements: missampling rate and uncertainty, where missampling rate denotes the ratio of unlabeled entities in sampled negatives, and uncertainty relects the difficulty of negatives. The authors find that both of the two measurements have important impacts on the NER performance. Also, a theoretical analysis provides evidence why negative sampling works. Thus, they design the weighted sampling distribution to displace the uniform one. Several experimental demonstrates the effectiveness of proposed methods and analysis.\n",
            "main_review": "\nStrengths:\n1. The paper is well motivated and presented.\n2. The proposed method is simple yet effective.\n3. The authors have conduct extensive experiments to verify the effectiveness.\n\nWeaknesses:\n1. Some of the observation experiments are not very convincing. For example, Table1 shows that the more they mask the annotated entities, the lower performance the model gets. But, is the degraded performance from more unlabeled entities or less training data?\n\n2. The results on well-annotated datasets are not promising, but I think this is acceptable, since annotation is expensive here.\n\n3. Some references about wealy supervised NER are complementary to this line of research, which also talked about the missing entities issue in NER, such as [1,2].\n\n[1] Patra B, Moniz J R A. Weakly Supervised Attention Networks for Entity Recognition[C]//Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019: 6268-6273.\n[2]. Cao Y, Hu Z, Chua T S, et al. Low-resource name tagging learned with weakly labeled data[J]. arXiv preprint arXiv:1908.09659, 2019.",
            "summary_of_the_review": "see Strengths and Weaknesses.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work aims to handle unlabeled entity problem (also called incomplete annotations).\nUnlabeled entity problem occurs when some ground truth entity mentions are missed by human annotators or in automated generated annotations. If incompletely annotations are used for training and missed annotations are simply treated as negative examples, the trained NER models may be misguided. \n\nThis work has a very close connection to a previous paper (Li et al., 2021) \n\nThe contributions of this work is:\n1. the authors analyze why negative sampling proposed by Li et al. (2021) can solve the unlabeled entity problem;  More specificlly, they show that missampling rates (the proportion of unlabeled entities contained in sampled negatives) are controllable, and negative examples with different levels of uncertainty have effect of trained models\n2. an improved version of negative sapling (weighted, Equation 5) based on missampling rates and uncertainty score is used to replace the one used in Li et al. (2021)",
            "main_review": "Strengths:\n1. the paper is well written and easy to follow\n2. impressive results are obtained on standard benchmarks\n\nWeakness:\n1. Regaing the first contribution, there are some assumptions the authors make: 1) entity sparsity; this seems to be true on datasets with small amount of entity types, but may not hold on fine-grained NER, where incomplete annotations can be a real problem. 2) why only $\\lceil \\lambda n \\rceil$ are sampled (see Question 1)\n2. The second contribution seems very marginal, and it is not clear how much efforts are needed to choose these new hyper-parameters and achieve these improvements. (See suggestion 3)\n3. The incomplete annotations seem to be a specific case of 'noisy labels', however,  there are no methods from the category 'training with noisy labels' are mentioned in this paper. Also the chosen baselines which can deal with incomplete annotations are all CRF-based, which I believe by natural are more difficult to train on noisy labels than span-based NER models\n\nQuestions:\n1. Regarding Theorem 1, the size of sampled negatives is set as $\\lceil \\lambda n \\rceil$, why it is not $\\lceil \\lambda \\frac{n(n+1)}{2} \\rceil$? considering the NER is a span-based model.\n2. what is the difference between Biaffine (Yu et al., 2020) and your span-based model? What is the main reason you think your model outperforms Biaffine on CoNLL2003 (Table 5).\n3. Is the 'held-out training data' described in Section 3 and 4 similar to the 'clean label data' in distant supervision literature?\n\nSuggestions:\n1. The described unlabeled entity problem may also relate to nested NER: if only outermoster mentions are labelled and all contained mentions are not explictly labelled in the training data, can the proposed method be used to train a Nested NER model that can recognize both containing and contained entity mentions.?\n2. Suggest to explain the connections between your work and methods work on 'training from noisy labels'  (see missing references)\n3. Suggest to add sensitivity analysis regarding different choices of $\\tau$ and $\\mu$ in Equation 5\n4. Suggest to add more explanations regarding the calculation of $r_{i,j}$ in Equation 5\n\nMissing reference:\nCo-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels. Han et al., 2018. NeurIPS\nAdaptive Sample Selection for Robust Learning under Label Noise Patel and Sastry, 2021\nAnalysing the Noise Model Error for Realistic Noisy Label Data. Hedderich et al., 2021, AAAI (they also work on NER)",
            "summary_of_the_review": "This paper propose a very incremental improvement based on Li et al., (2021), and I don't gain much insights from it.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}