{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes an embarrassingly simple yet effective adversarial domain adaptation method to improve the results of domain adaptation, achieved by dynamically changing the domain labels according to the discriminator's predictions. It keeps the domain label unchanged when the feature extractor is not able to fool the discriminator, while changing the label to uniform when the feature extractor fools the discriminator. Experimental results on both synthetic and real data are shown, demonstrating the mechanism and efficacy of the method.",
            "main_review": "The method is simple and the improvements over the baselines are significant. However, the mechanism remains non-intuitive to me, and I doubt whether significant improvements can be achieved on stronger baselines. \n\n1. The concept of memorization, domain-invariant and alignment seem to be different, and are treated as the same in this paper. I think memorization is not relevant here.\n\n2. Why is the baseline not able to classify samples correctly in the source domain in Figure 3? Even training a classifier without the discriminator, when we reduce its loss to a small-enough value, the classifier should be able to classify correctly. Is the hyperparameters, including learning rates and total training iterations, carefully tuned for both the baseline and your method? Similarly for the second toy example.\n\n3. The method tries to resolve the class-imbalance issue, but it is not clear why the dynamic labeling from the discriminator solves this issue. The class-imbalance issue mainly affects the classifier, and the dynamic labeling is used to improve the alignment of the source and target domain. Even if the domains are aligned better, the classes are still imbalanced, so it is not clear why dynamic labeling makes the classifier better in the source domain in classifying classes with imbalanced number of training examples.\n\n4. The results on practical datasets seem promising, improving over the baselines significantly. However, the baselines seem a bit outdated and do not represent the SOTA results, according to https://paperswithcode.com/task/domain-adaptation. For example, on Office31, ASAN achieves 90.0. On Home-Office, the non-adversarial FixBi achieves 72.7 average accuracy using the same backbone, while the best result in this paper is 71.1. To make the results more convincing, I would like to see improvements on stronger baselines.\n\n5. (Minor) There seems to be many grammar issues or typos, for example, \"...the **majority data** clusters typically dominate typically dominate and **biase** the adaptation process\", \"... which allows each sample **can** be well studied ...\", \"...we **remain** the raw domain labels unchanged...\".",
            "summary_of_the_review": "The method is simple and experimental results seem good in the current form, but it does not seem intuitive why the method could resolve the class-imbalance issue, and some comparisons with stronger baselines might be missing. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Proposes “Dynamic Domain Labels”, a simple reweighting strategy for domain adversarial training for domain adaptation that treats instances that currently fool the domain discriminator as well-aligned and reduces their optimization importance. The proposed method is shown to improve the performance of multiple domain adversarial adaptation strategies on standard DA benchmarks.",
            "main_review": "Strengths\n\n– The proposed method is intuitive and extremely simple to implement, and appears to consistently lead to performance gains\n\n– The paper presents several interesting analysis experiments, such as visualizing the behavior of the method on toy problems, and comparing against other reweighting strategies\n\nWeakness\n\n– While the proposed trick is certainly interesting and intuitive, I found the paper’s title of “Unleash the potential of adaptation models” to be a little over the top – it would be good to rename it to something more factual and in-line with the paper’s contributions, eg.  “Improved domain adversarial learning via Dynamic Domain Labels”. \n\n– The paper’s contribution is relatively limited, as it presents a single training trick. That said, i) it does well to comprehensively analyze this trick and ii) domain adversarial learning is still a common DA strategy, so this method will likely find adoption in the community. Altogether, I do not consider this a major shortcoming.\n\n– The experiment ”DDL is well-suited to DA settings with Intra-class and Inter-class imbalance” is interesting but incomplete. A more detailed analysis of _why_ performance improves is required to back up the claims – is the gain indeed due to improved performance on underrepresented classes / within-class distributions? \n\n– The diagnostic experiment presented in Figure 2 when training with DANN is interesting. A similar plot demonstrating that the proposed method improves this behavior would have been interesting to see.\n\n– [Minor] The paper has quite a few grammatical errors and would benefit from careful proofreading. For eg. “biase” (bias – abstract), timely transfer (abstract), “to those easily overlooked samples” (abstract), “we remain the raw domain labels” (Page 4, last paragraph), \n\n– [Minor] Margin violation with Table 5\n",
            "summary_of_the_review": "Interesting paper presenting a simple and effective reweighting trick to ease optimization of domain adversarial training objectives. I have slight concerns (see weaknesses) but am willing to raise my rating based on the author response.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a simple technique to enhance adversarial domain adaptation methods. It mainly focuses on the binary domain discriminator and replaces the traditional binary cross-entropy with a dynamic one. Results on several datasets further demonstrate the effectiveness of the proposed technique.",
            "main_review": "Pros:\n1. this paper is well written and easy to follow\n2. the proposed method is quite simple but effective as shown in the experiments\n\nCons:\n1. [novelty] the authors argue the largest contribution lies in the dynamic domain label, however, using the soft label for domain discriminator is not new in this field. For example, IWAN (Zhang et al, 2018) and RADA (Jin et al., 2021) has shown that training domain discriminator with soft labels achieved promising results for domain adaptation. \n\n2. [results] the results of the proposed DDL technique, when combined with CDAN, are inferior compared with RADA (Jin et al., 2021) on many datasets, e.g., Office and Office-Home. Since the authors compare DDL with IWAN, the experiments on partial-set domain adaptation are required to fairly verify its superior performance. \n\n3. the authors are also suggested to compare [a] that uses entropy to measure the importance during adversarial training and [b] that uses mixup between source and target samples.\n\n[a]. Wang, Shanshan, and Lei Zhang. \"Self-adaptive Re-weighted Adversarial Domain Adaptation.\"\n\n[b]. Xu, Minghao, et al. \"Adversarial domain adaptation with domain mixup.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 04. 2020.\n\nMinor concerns:\nthe design of dynamic label assignment in Eq.(5) seems tricky, does the adversarial training need some warm-up epochs first?",
            "summary_of_the_review": "Generally, this paper provides a simple yet effective technique for adversarial domain adaptation, however, the novelty is fairly incremental. The results on several datasets also show its inferior performance to similar methods like RADA. Thus I vote for \"reject\".",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}