{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents Uncertainty Regularized Policy Learning (URPL), a model-free offline RL algorithm that aims to alleviate and avoid the overtraining issue in offline RL. URPL modifies REDQ by adopting an uncertainty regularization term, where the standard deviation across the ensemble Q functions is penalized in policy learning. The policy is selected by the surrogate metric: it is preferred that the policy entropy and the Q's standard deviation is low (i.e. stable learning area). In the experiments, URPL performs better than baseline algorithms in the D4RL benchmark (medium-replay).\n",
            "main_review": "This paper aims to mitigate the overtraining issue in offline RL: it is empirically observed that the policy performance in the real environment tends to increase and then decrease at some point as training progresses. To mitigate this issue, URPL makes two modifications in an online RL algorithm, REDQ. \n1. The first modification is an uncertainty-regularized policy update, where the standard deviation of ensemble Q functions is penalized in policy optimization. Still, I am not convinced that such a policy update method should be considered as a novel contribution given that penalizing the uncertainty in offline RL has been widely adopted. For example, MOPO and MOReL optimize the policy using the uncertainty-penalized reward function, and BOPAH (Appendix E in [1]) learns Q-function with the target of mean_i(Q_i) - beta * std_i(Q_i) where the mean and std are for the Q ensembles. What is the benefit of applying the uncertainty penalty to the policy update, compared to penalizing Q-function as in the existing methods?\n2. The second modification is adaptive policy selection: low uncertainty of the policy is preferred when selecting a final policy. However, this surrogate metric seems not well-motivated given that URPL is solving maximum-entropy RL in Eq (1,3): URPL is trying to maximize entropy (with rewards). Why should we choose a policy as opposed to the original objective of optimizing the policy?\n3. The proposed SLA metric for policy selection is heuristic and not justified by theory, thus it should be justified empirically through well-designed experiments. However, I am not convinced that the uncertainty term shows a consistent correlation with the actual performance of the policy in the experiments. For example, in Figure 3-4 (Halfcheetah), the policy performance is being improved continuously, regardless of the change of entropy/q-std. A similar trend is observed in Hopper-mixed-CQL of Figure 3. Also in Table 3, CQL_wo outperforms CQL_w in hopper and halfcheetah. These results imply that the proposed policy selection method based on the SLA metric can be applied to only limited situations (both for specific algorithms and domains). It would be great to provide experimental results that show that the proposed SLA-based policy selection method is generic enough in terms of domains and algorithms.\n4. Since URPL selects a policy based on the policy's entropy, it seems that it can be sensitive to the initialization of the policy's standard deviation. Is SLA still effective and showing a similar result even when the policy's initial std is set to much smaller or larger values?\n5. It would be great to see the result on more diverse domains in D4RL benchmarks. This would give more insight when the adaptive policy selection method is useful or not.\n6. In Table 2, it is said that the scores for BC/SAC-off/BEAR/BRAC-p/BRAC-v/AWAR are from the D4RL paper, which are based on -v0. However, the result for URPL is obtained using -v2 dataset, which does not seem to be comparable directly.\n7. Page 7: it is said that 'BCQ does not use stochastic policy...'. I don't think this statement is correct. BCQ's action selection is done by sampling an action from a conditional VAE and then perburbating the sampled action slightly, thus the policy is certainly stochastic.\n\n[1] Lee et al., Batch Reinforcement Learning with Hyperparameter Gradients, ICML 2020\n",
            "summary_of_the_review": "The proposed uncertainty regularized policy update does not seem novel enough, and it seems that the SLA-based policy selection method is useful only in limited situations. More thorough experiments are required including results on more diverse domains such as random/medium/medium-expert/etc in D4RL.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a novel offline reinforcement learning algorithm that leverages uncertainty to avoid regions of the state-action space with sparse data. In particular, they incorporate an uncertainty penalty into a Q learning algorithm. They empirically demonstrate that their approach outperforms several baselines.\n",
            "main_review": "The proposed approach does not appear to be very novel. For instance, conservative Q learning (CQL) already penalizes state-action pairs with high uncertainty in the context of Q learning (Kumar et al., 2020), albeit using a different strategy. Model-based offline policy optimization (MOPO) uses uncertainty penalties (Yu et al., 2020), though in a model-based instead of model-free approach. I’m not familiar with work that specifically uses uncertainty penalties in a model-free algorithm, but the approach is still of limited novelty in the context of this existing work.\n\nFurthermore, the experimental evaluation in this paper is missing a lot of baselines, especially more recent approaches that have been recently proposed. For example, the authors do not compare to MOPO, a state-of-the-art algorithm. Another recent approach is CODAC, which achieves state-of-the-art on several MuJoCo benchmarks:\n\nMa et al. Conservative Offline Distributional Reinforcement Learning. NeurIPS 2021.\n\nGenerally speaking, this paper only compares to a small number of older algorithms.\n\nFinally, their experiments are only on a limited set of benchmarks. Even if they are focusing on highly mixed datasets, I believe there are several other settings in D4RL that satisfy this criterion, e.g., the mixed datasets from other policies. In general, I would expect comparisons on a much more extensive set of benchmarks; for instance, there are also benchmarks in D4RL beyond the standard MuJoCo benchmarks that the authors could consider.\n",
            "summary_of_the_review": "Pros\n- Important problem\n\nCons\n- Limited novelty\n- Weak experimental results\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new method for offline RL which uses an ensemble of Q functions to penalize the policy for taking actions which have high variance in estimated value under the ensemble. The policy-learning objective also penalizes the entropy of the policy, thus encouraging the policy to concentrate. The authors also propose a method for early stopping in offline RL without real-world evaluations based on plots of the estimated policy uncertainty. They find that the proposed method outperforms prior work in experiments on three of the benchmark tasks from D4RL.",
            "main_review": "This paper is looking at an interesting area and the authors seem to have ideas that could work. However, this work would benefit from a more thorough understanding of the state of the field right now as well as a bit more rigor.\n\n**Strengths**\n- Penalizing the policy for taking actions with uncertain outcomes is a solid approach and, as the authors show, it can have good results.\n- On the studied environments the method achieves competitive results.\n\n**Weaknesses**\n- The writing of this paper is very weak. It contains many grammatical errors and typos, but more importantly it is frequently vague on technical issues. The authors should try to make each statement as precise and concrete as possible.\n- The technical novelty here is very limited. As described in the paper, the only contributions beyond the previously-published REDQ are the use of a variance penalty using an ensemble of Q functions and the \"Stable Learning Area\" (SLA) idea.\n- Many other papers, including some in the related work, have attempted various penalties for the uncertainty of the learned Q function or the divergence from the data. The variance-style penalty used here was also used in the model-based paper [1].\n- The SLA is never formally defined. As far as I can tell it is implemented only by having the authors look at the plots and make a judgement call. In an accepted paper I would expect to see a precise technical definition of this concept as well as some clear technical motivation which justifies it.\n- As written, the objective in Eq. 3 would maximize the entropy of the policy. However, the text implies that it minimizes the entropy of the policy (i.e. makes it more deterministic), which would align also with Eq. 4. I am not sure which was actually implemented.\n- The method was only evaluated on a tiny subset of the standard D4RL benchmark, and the results are only a marginal improvement over the baseline methods if that. This makes me concerned that the results might have been (perhaps inadvertently) cherry-picked and the performance would be less competitive on the full benchmark.\n\n**Questions and minor comments for the authors**\n- In Eq. 3, I assume that $\\tilde a_\\theta$ is an action sampled from $\\pi_\\theta$?\n- This paper may not be at the bar for acceptance, but you're working in an interesting field and there are lots of open questions. Hopefully this project was a good learning experience and now that you're up to date with the techniques in play you are very well positioned to address those questions.\n\n**References**\n[1]: https://arxiv.org/abs/1901.02705",
            "summary_of_the_review": "This paper has very limited technical novelty as it is very similar to a variety of previously-proposed offline RL methods, and the concept of the Stable Learning Area is not precisely defined or substantiated. The empirical results show a marginal improvement over other work and are only given for a small subset of the standard benchmark. The paper has significant flaws in its writing.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents Uncertainty regularized policy learning (URPL), a method for offline RL that aims to: (a) obtain a policy whose performance is as good as the best trajectory in a mixed quality dataset or better if diverse data is available, (b) adaptively select a good policy at any point/region of the training process to avoid over-training issue. URPL is an entropy-regularized actor-critic method that utilizes an ensemble of Q-functions and (a) sets the target based on the min of these Q-functions, (b) optimizes the actor by penalizing the uncertainty in the Q-functions. In addition to this, URPL presents an approach to select a policy by tracking a quantity based on a combination of entropy and uncertainty of Q values.",
            "main_review": "1. The proposed framework doesn't have any guarantees on the value of policy outputted by the procedure and appears heuristic at best. Why is the uncertainty in the Q-functions not used for setting the target and is just used for updating the policy? The notion of diverse offline data can be rigorously formalized in terms of coverage. The uncertainty based penalty can be related to lower confidence bonuses for classes of MDPs including Linear MDPs, Kernel Non-Linear Regulator etc. For analyzing actor-critic methods, one can utilize additional assumptions such as Bellman Completeness.\n\n2. The proposed algorithm appears to be rather narrow in its scope -- the authors say that they focus only on the diverse dataset case, and, experiment only on a subset of datasets in D4RL. My perspective is that any online RL procedure can be adapted to the offline case by appropriately utilizing aspects of pessimism. I'd want to see how the URPL works on all datasets in D4RL (MuJoCo tasks) rather than artificially limiting the subset of datasets where they offer competitive performance compared to baselines. Also, the common practice in D4RL appears to be presenting normalized scores rather than the unnormalized ones.\n\n3. Regarding the metric for policy selection -- I certainly like the paper's perspective on trying to define metrics that can guide policy selection in offline learning. A couple questions here -- how is the epsilon parameter set? Is there any theoretical support behind this metric?\n\n4. Hyper-parameters -- I do not see details of hyper-parameters used by URPL for their experiments. How did the authors perform hyper-parameter tuning? What are the important hyper-parameters and their ranges?",
            "summary_of_the_review": "The paper presents a promising approach for offline RL, but, falls short in terms of experiments (the paper deals with a subset of datasets in D4RL MuJoCo tasks); there is no theoretical support for the proposed algorithmic framework/off-policy estimator selection.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}