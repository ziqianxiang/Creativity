{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents three different network architectures for 3D MRI brain tumor segmentation. The existing state-of-the-art 2D network architectures were extended to 3D using pre-existing modules (such as residual, attention, and pyramid pooling blocks) to increase the learning capacity of a neural network. The proposed method has been validated on publicly available Multimodal Brain Tumor Segmentation Challenge (2020) dataset. The authors have provided their code, which is always good to see. Unfortunately, in my honest opinion, the paper neither provides any technical contribution nor any novel empirical insights to the existing problem.",
            "main_review": "Strengths:\n\nExtending the previous state-of-the-art methods (such as LinkNet, PSPNet, and combination of Residual U-Net with Attention U-Net) from 2D to 3D counterpart is an interesting work by the authors. It is also worth noting that the method has been validated on the publicly available BraTS 2020 dataset, which is a standard benchmark dataset for evaluating brain tumor segmentation problem.   \n\nWeakness:\n1) My biggest criticism is regarding the current literature review, and therefore experimental design seems completely outdated and ill-suited for contribution to the field of 3D MR brain tumor segmentation. Recent papers by Isensee et al., (2021, 2020) have clearly demonstrated that a well-configured plain U-Net (including its 3D variant) surpasses most existing approaches (to till-date) on a wide range of diverse tasks, including brain tumor segmentation. There have already been numerous high-impact publications that have shown state-of-the-art benchmark performance utilizing 3D-UNets. Some of the most notable one’s includes:\n\n-- Isensee, Fabian, et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nature methods 18.2 (2021): 203-211.\n\n-- Isensee, Fabian, et al. \"nnU-net for brain tumor segmentation.\" International MICCAI Brainlesion Workshop. Springer, Cham, 2020.\n\n-- Myronenko, Andriy. \"3D MRI brain tumor segmentation using autoencoder regularization.\" International MICCAI Brainlesion Workshop. Springer, Cham, 2018\n\n-- Feng, Xue, et al. \"Brain tumor segmentation using an ensemble of 3d u-nets and overall survival prediction using radiomic features.\" Frontiers in computational neuroscience 14 (2020): 25.\n\n-- Islam, Mobarakol, et al. \"Brain tumor segmentation and survival prediction using 3D attention UNet.\" International MICCAI Brainlesion Workshop. Springer, Cham, 2019.\n\n2) More importantly, recent BraTS challenges (2017-2021) have added more clinically relevant tasks/sub-challenges such as predicting the tumor recurrence or overall survival (Bakas, et al., 2018; Baid et al., 2021), emphasizing the clinical relevance of the brain tumor segmentation task. However, for some reason, the authors did not reference any of these works and chose to still experiment with a sub-task of the challenge. At this point in time, these incremental improvements in architectural designs have severely limited or no real-world value. Without sufficient experimentation on different sub-tasks or with many different organs such as in Isensee et al. 2021, there is no way for me to evaluate the impact of the paper and whether it is actually better than previous approaches. \n\n--  Bakas, Spyridon, et al. \"Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the BRATS challenge.\" arXiv preprint arXiv:1811.02629 (2018).\n\n-- Baid, Ujjwal, et al. \"The rsna-asnr-miccai brats 2021 benchmark on brain tumor segmentation and radiogenomic classification.\" arXiv preprint arXiv:2107.02314 (2021).\n\n3) Dataset description, implementation details, and experimental results have been very poorly presented. \n\n-- For instance, the BRATS 2020 challenge dataset consists of training (369), validation (125), and test (166) images. But, the author’s report in their paper is 250, and it’s not clear what is their train, validation, and test splits. \n\n-- Along with the Dice score, sensitivity, specificity, and Hausdorff distance should be reported, which is a commonly used evaluation metric on this challenge.\n\n-- I don’t see any explanation regarding their experimental results. In Fig. 10, how does the original image looks like and what classes do they belong to?\n\n-- Further, the paper doesn’t show any comparison against the state-of-the-art methods or any ablation experiments. \n\n",
            "summary_of_the_review": "In my, opinion this research work is neither reproducible nor has been written to the standard of the ICLR conference. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes three different deep learning architectures for the task of brain tumour segmentation. These architecture are trained and tested on publicly available Brain Tumour Segmentation (BraTS) 2020 challenge. The authors also provide their code, which would help reproduce their results.",
            "main_review": "**Strength**: \n* The paper tackles the problem of brain tumour segmentation which is important but hasn't been looked at by the ICLR community.\n* The paper proposes three different deep learning architectures, which is commendable and unique.\n* All different architectures are evaluated on a publicly available clinically relevant dataset. In addition to this, the code of the paper is made available. This would facilitate future comparison against these networks. \n\n**Major Weakness**: \n* The paper describes a good literature review of relevant methods but doesn't provide any comparison against these methods. As the BraTS challenge is one of the most prevalent challenges in medical image analysis with more than 50 participants every year [1], it is impossible to evaluate the novelty or the usefulness of the proposed architectures without comparison against any baselines. \n* The proposed architectures are modified from their 2D counterparts. A comparison against 2D networks would allow to access the usefulness of moving from 2D to 3D networks in medical image segmentation tasks which are usually 3D in nature.\n* The paper uses the latest version of the BraTS dataset (2020 challenge). In the paper, it is mentioned that there are 250 samples in the dataset. These samples were used to train the networks. There is no mention of validation or testing datasets.  Also, as per my knowledge, the BRATS 2020 challenge dataset consists of training (369), validation (125), and test (166) images. There is no mention of these datasets in the paper. If the paper used the publicly available BraTS 2020 validation dataset to evaluate their methods, it would facilitate future comparison against these methods.\n* The paper only reports a single Dice score value for each architecture. The BraTS challenge provides the dataset with multi-class tumour segmentation groud-truth. All methods in the BraTS challenge report three different Dice scores, corresponding to three different classes: Whole Tumour (WT), Tumour Core (TC), and Enhancing Tumour (ET). It would be helpful if the authors provided details about which tumour sub-class the reported Dice score corresponds to.   \n* The paper only reports Dice Score. While in the medical imaging literature, it is well-known that Dice scores have their limitations [2]. The BraTS challenge also uses Hausdorff distance (HD) to overcome these limitations, which measures the boundary distance of the predicted segmentation to the groud-truth segmentation. Reporting HD for the brain tumour segmentation task is essential.\n* There is no discussion about each proposed architecture's obtained results, pros, and cons, or when to use which architecture. \n\n[1] Bakas, S., Reyes, M., Jakab, A., Bauer, S., Rempfler, M., Crimi, A., Shinohara, R.T., Berger, C., Ha, S.M., Rozycki, M. and Prastawa, M., 2018. Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the BRATS challenge. arXiv preprint arXiv:1811.02629.\n\n[2] Reinke, A., Eisenmann, M., Tizabi, M.D., Sudre, C.H., Rädsch, T., Antonelli, M., Arbel, T., Bakas, S., Cardoso, M.J., Cheplygina, V. and Farahani, K., 2021. Common limitations of image processing metrics: A picture story. arXiv preprint arXiv:2104.05642.\n\n**Minor Points**: \n* The paper spends a lot of writing space describing each architecture. These details could be moved to a table, and it would save some space. The saved space could be better utilized in the discussion and conclusion section.\n* The last line of the related work section (\"We propose this paper to establish results for brain tumor segmentation in 3D.\") seems to be incomplete. ",
            "summary_of_the_review": "The paper lacks comparison against state-of-the-art methods mentioned in the literature review, doesn't provide any ablation study, doesn't report the usual metrics, and also details about the dataset are also missing. There is no discussion about the obtained results or the usefulness of the proposed architectures. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper modifies based on several existing deep networks to segment tumorous regions of the brain in MRI data. Three varied segmentation architectures are designed, i.e., Attention Residual UNET 3D, LinkNet 3D and PSPNet 3D.  They conduct experiments using BraTS2020 dataset consisting 250 data, where 80% is used for training and 20% for validation. There is no testing dataset.",
            "main_review": "Strength:\n-- I found this work with limited technical contribution. Authors develop 3 segmentation networks, namely Attention Residual UNET 3D, LinkNet 3D and PSPNet 3D. These are all based on existing deep network modules such as residual connection, attention, long range connection, pyramid pooling, etc. Authors just extend some of them into 3D, which can not be treated as significant contribution. \n\nWeakness:\n-- Limited technical contributions as state above.  The deep modules authors built upon have been explored for a long time. Authors do not propose novel modules significantly improve the performance.\n\n-- The experiments have severe flaws. Authors used the BraTS2020 dataset with 80% training and 20% validation, however, there is no testing set.  Results are reported on the validation dataset. \n\n-- The experimental results are limited. There are just 3 validation dice sores reported for the 3 developed networks.\n\n",
            "summary_of_the_review": "Considering the limited technical contribution, limited experimental results and the flaw of without a testing set, I recommend rejection of this work.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes three different 3D deep architectures (i.e., AT-UNET 3D, LinkNet 3D, PSPNet 3D) for tumor segmentation from multi-contrast brain MR images. The experiments are performed on BraTS2020 dataset, which shows satisfactory results.",
            "main_review": "Strenghs:\n1. This paper focus on accurate brain tumor segmentation, which is important in real scene, and 3D deep network has proven to be an effective solution for this problem.\n\nWeaknesses:\n1. My major concern is the lack of novelty. It seems the proposed deep architectures are just the 3D versions of previous well-known 2D networks with several modifications, and these modifications are also not well-motivated.\n2. As the authors mentioned, the proposed deep networks are to establish results for brain tumor segmentation in 3D. However, it is hard for me to see that the proposed networks are specially designed for medical images or brain tumor segmentation tasks.\n3. The experiments are highly insufficient. \n3.1) The authors mentioned that 250 brain MR images are used in experiments. However, as far as I know, the BraTS2020 dataset contains 369 training data, 125 validation data, and 166 test data. Did the authors select 250 of 369 brain MR images in experiments? Besides, what is the training/validation/test set in this work?\n3.2) Generally, the BraTS challenge evaluate the accuracy over three subregions, i.e., whole tumor (WT), tumor core (TC), enhancing tumor (ET). In this paper, the authors report a single \"Best Dice Score\" without any explanations. What does it stand for?\n3.3) This paper only provides the accuracies of three proposed deep networks, without any comparison with existing brain tumor segmentation methods. It is insufficient to prove the effectiveness of proposed methods.",
            "summary_of_the_review": "From my point of view, this paper is not ready for publish, due to the lack of novelty and insuffcient experiments.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper combine the residual block and attention block for brain tumor segmentation. The proposed method validated on the BraTS 2020 dataset.",
            "main_review": "1. The proposed method is not innovative. The proposed network is combine the residual u-net and the attention u-net.\n2. The experiment is very poor. The proposed method only compared with two old network. The proposed methods should compare with the state-of-the-art methods.",
            "summary_of_the_review": "This article is not a bit innovative， and the experiment is also very poor.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}