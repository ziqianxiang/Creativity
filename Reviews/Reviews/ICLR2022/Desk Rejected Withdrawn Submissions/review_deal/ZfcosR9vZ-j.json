{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper the authors propose to build the mini-batch of the OT minibatch based on a pyramidal structure of the data: the points are associated to a same mini-batch if they are globally in the same cells of this pyramidal structure. ",
            "main_review": "- 1) About the introduction:\n\nI think that the writing of the introduction could be improved. It is difficult to grasp the problematic, the context is not very clear since the authors go directly into technical details. Moreover there are very few references to support the different claims of the authors. Finally, the justification of the method is quite vague and, in my opinion, it is difficult to understand what is being proposed:\n\na) The method wants to build mini-batches that are \"geometrically consistent with their accompanying batch from the other distribution\". I think it is necessary to define clearly what the authors mean by \"geometrically consistent\". In my opinion it is a central point that justifies the pyramid approach and therefore requires more rigor.  \n\nb) \"our approach constructs mini-batches considering the coupling over the whole set of data points before splitting them into mini-batches\" -> It is quite not clear, it seems to suggest that one should first have the global coupling (which one wants to avoid calculating).\n\nc) \"We encode every data point into a multi-resolution pyramid structure and extract the implied coupling of data points from the structure\" -> without reference it is difficult to understand what this \"multi-resolution pyramid structure\" is.\n\nd) \"our approach improves the construction of mini-batches by considering global data structure\" -> what does the global data structure mean ?  \n\n- 2) About Section 2:\n\nThe authors propose to divide the data in bins and to build a \"multi-resolution histogram\" based on these bins. I find however that the paragraph \"Pyramid Match Kernels\" is not very clear and that it is difficult to understand what is done in practice. How are these subdivisions chosen ? How are they hierarchical ? I think it should be more described, I think there are many important details that are hidden.\n\nIn addition, many sentences are difficult to understand:\n\n a) \"Pyramid Match Kernels (Grauman & Darrell, 2005) have been shown\nto be an effective method for quickly estimating Wasserstein-1 (or equivalently “Earth mover’s”) distance\" -> It should be discussed how the pyramid match kernel allows \"quickly estimating Wasserstein-1\". There is no reference given here for this claim and it is not clear to which notion of \"estimation\" the authors are referring (statistical / approximation error).\n\n b) \"As each bin in layers $l > 0$ is a subdivision of a bin in layer $l-1$, bin relationships between layers can be modeled as parent-child relationships\" -> This sentence is quite difficult to grasp.\n\n- 3) About Section 3:\n\nI find that the description of the method that assigns a point to its minibatch is also quite difficult to understand (paragaph \"Using $\\Psi_X$ and $\\Psi_Y$...\"). Figure 2 does not shed enough light to understand what is done in practice. I find that section 3.2 which describes joint-PMB is also not rigorous enough:\n\na) \"If distributions X and Y are completely disjoint, existing pyramid matching binning schemes will\noffer no improvement over random batching.\" -> what does \"completely disjoint\" mean ?\n\nb) \"At each level of the hierarchy, cluster centers are determined by the available points in either PX or PY\" -> how exaclty are these centers determined ?\n\nc) \"Cluster assignments are then made for both point sets based on the determined centers, and the source of the cluster centers will alternate at the next level of the hierarchy\" -> How exaclty are these assigments determined ? \n\n\n- 3) About the method: \n\nI think there is a dimensional problem with the method and I doubt of its applicability in high dimension. I think this point needs to be discussed/illustrated as it is not clear that computing useful bins is efficient in high dimension (you need an exponential number of them to properly capture the whole space). \n\nMoreover the method requires to store the entire data (in order to compute the pyramid structure), which is the opposite of the minibatch philosophy and limits a lot its applicability to large-scale scenario. \n\nFinally, I think it is important to bring more elements on the approximation made by the method. Indeed there is no theoretical evidence that the proposed approach guarantees that the distance found is close to the true Wasserstein distance when the size of the minibatch increases, which, I think, is an important property.\n\n- Minor comments and typo:\n\n- $O(N^{3} \\log(N))$ should be $O(n^{3} \\log(n))$\n- Guassian mixture -> Gaussian mixture\n- I find some sentences are rather clumsy: \"so mini-batch-based optimal transport must be used\", the notation $\\mathbb{W}_p$ for the Wasserstein distance is not well chosen because as it does not show the measures $\\mu,\\nu$, \"As machine learning applications often deal with settings where $N$ is greater than 100,000 data points\"",
            "summary_of_the_review": "Overall the paper proposes to improve the creation of minibatches for the approximation of the Wasserstein distance. This is an important problem and the authors brings a method that seems interesting. However I find it difficult to understand, many points are not clear and lack of rigor. I also think, from a theoretical point of view, that it is important to justify that when the size of the minibatch is close to the size of the data then the estimate of the Wasserstein distance is consistent with that of the true Wasserstein distance. For these reasons I think this paper is not ready yet for publication. \n\n\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper focuses on the mini-batch optimal transport between discrete probability measures for large-scale data points. \nThe authors propose a pyramid-based mini-batch sampling scheme to construct geometrically consistent batches, called Pyramid Mini-Batching (PMD), which significantly improves the quality of optimal transport approximations compared to random mini-batch sampling.\nFurther, for the case of completely disjoint source and target distributions, they introduce the Interleaving-PMB and Joint-PMB approaches.\n\nThe effectiveness of the above strategies has been verified through experiments over various established optimal transport settings.\nThe main limitations and future work directions are also discussed. \n",
            "main_review": "- Using the tree structure of pyramid match kernels to construct mini-batches is an interesting idea, and it overcomes the limitation of random mini-batches to a large extent. The proposed method is novel and simple to understand.\n\n- The proposed PMB approach can deal with high-dimension optimal transport problems.\n\n- The organization of this paper is satisfactory, and the motivation is very clear.\n\n- The experiments are implemented on various synthetic and real datasets. The authors illustrate the effectiveness from many aspects.\n\n- The authors did not provide any theoretical guarantee about the convergence of the proposed mini-batch Wasserstein estimation to the true Wasserstein distance. Also, the theoretical properties of the proposed algorithm itself are not discussed.\n\n- The proposed method requires building a tree structure for the data. What is the computational cost for this step? In practice, how is the CPU time for the tree building step compared to the CPU time for the OT computation?\n\n- The tree-based methods usually suffer from curse-of-dimensionality, as their performance usually become worse as the number of dimension increase. Could the author provide some discussion on this point?\n\n- There seems to be some existing work on mini-batch optimal transport, such as [1] and [2], while only the naive random mini-batch OT method is compared in both methodological and experimental parts. There is a lack of comparison with other competitors.\n\n[1] Nguyen, Khai, et al. \"On Transportation of Mini-batches: A Hierarchical Approach.\" arXiv preprint arXiv:2102.05912 (2021).\n\n[2] Fatras, Kilian, et al. \"Unbalanced minibatch optimal transport; applications to domain adaptation.\" International Conference on Machine Learning. PMLR, 2021.\n",
            "summary_of_the_review": "The proposed idea is interesting, however, some points listed above need to be clarified.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes to use Pyramid Match Kernel as a proxy to improve the estimation of optimal transport from mini-batches. By using the special data structure, the paper claims that the estimation of discrete optimal transport is better than the conventional random sampling mini-batch. On the experimental side, the paper evaluates the new scheme on toy experiments, aligning Image-net, and gradient flow. ",
            "main_review": "**Strengths**:\n- Using a special data structure is a good idea for the mini-batch problem.\n**Weaknesses**:\n- The paper is not well-written. For example, two notations are used for the size of mini-batches $m$ and $N$ (in the last of page 4). Also, the paper mentioned generative modeling and domain adaptation as applications of mini-batch OT, however, the paper does not run experiments on these applications. Literature is poorly cited, there are many recent works about mini-batch OT and stochastic optimization for optimal transport that are not cited [1],[2],[3],[4],[5],[6], and other related methods and applications. \n- Mini-batch methods [1],[2],[3] are used as a surrogate loss for original optimal transport for estimating the parameter of interest (e.g. gradient of the neural net, transportation maps, etc). Their time and memory complexity only depend on the size of mini-batches $m$ (e.g. $m=N=100$) and the number of mini-batches $k$ (e.g. $k=8$).  In contrast, due to the additional data structure to use pyramid kernel, the space and time complexity of the pyramid mini-batch OT depends on $n$ which can be a few million. As mentioned in the paper, the time complexity for constructing mini-batches is $O(n L)$, and the memory complexity for storing the pyramid match kernel is $O(ndL)$.  This is extremely expensive in practice, for example, $n=100000$, $d=512 \\times 512 \\times 3$. I believe that this is the reason that authors could not have experiments on deep generative models and deep domain adaptation. At this point, the pyramid mini-batch OT is not practical and does not follow the idea of using mini-batches.\n- Since the author does not publish the code and the implementation of pyramid match kernel (using histogram) is written clearly in the paper, I assume that bins are divided uniformly. So, this is sufficient with a general ground metric of OT. For example, two face images are closed in $\\mathbb{L}_1$ norm that is not truly closed (different people with the same background).\n- On the technical side, pyramid mini-batch optimal transport is not formulated into a discrepancy between two measures in the paper. Also, the notation of the transportation plan is not also discussed for pyramid mini-batch optimal transport. \n- On the experimental side, the authors do not evaluate the new mini-batch method on standard experiments such as color transfer, generative models, and domain adaptation. I believe that the method of the paper is not scalable, hence, the author can only do some toy alignment problems.\n\n[1] Minibatch optimal transport distances; analysis and applications\n\n[2] Unbalanced minibatch Optimal Transport; applications to Domain Adaptation\n\n[3] On Transportation of Mini-batches: A Hierarchical Approach\n\n[4] Optimal transport mapping via input convex neural networks\n\n[5] Wasserstein-2 Generative Networks \n\n[6] Improving Mini-batch Optimal Transport via Partial Transportation\n\n",
            "summary_of_the_review": "I strongly believe that the method in the paper, pyramid mini-batch optimal transport, is not scalable and not practical. I suggest that the authors should include experiments on deep generative models, deep domain adaptation, and compare the proposed method with current mini-batch methods. Also, I believe that the paper should be polished considerably and discuss thoroughly the literature.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes a pyramid mini-batching approach to compute optimal transport costs and alignments between discrete distributions.",
            "main_review": "**Benefits**\n- Some empirical evidence on the superiority of pyramid minibatch approach w.r.t. other mini-batch approaches is present;\n\n**Minuses**\n\n- The exposition is not transparent, it is very hard to parse the main idea and understand how the algorithm works;\n- The approach is purely heuristical, there is absolutely no theoretical support to the proposed method;\n- There are no quantitative experiments directly supporting the main claim of the paper that the alignments are better than m-OT;\n- The paper has various too bold, but unfounded statements;\n- Missing discussion and comparison with some related works;\n\n*Clarity.* In my view, the main algorithmic part of the paper -- the pyramid kernels and algorithm are poorly explained. It is nice to have the pseudo-code of the algorithms, but this does not completely absolve the authors of the responsibility to clearly explain the main ideas in words. Unfortunately, the current explanations are very hard to parse and seem incomplete. I think they should be more thoroughly rewritten and detailed. In particular, interleaving PMB is discussed in brief and I do not really understand why assigning cluster centers by using both points is a good strategy. This might intuitively work in some cases, but not clear why it is a good strategy for the general case.\n\n*Experiments.* Since there is no theoretical support for the proposed algorithm, it is expected that the authors provide strong empirical evidence of performance.\n\nHowever, there is not a single convincing experiment that directly demonstrates to which extent the alignment by pyramid approach is better than by other approaches, e.g., minibatch OT. Is the alignment truly closer to the full-OT alignment? The conducted experiments try to estimate this through some indirect metrics, such as accuracy in domain adaptation, but this is not connected to the closeness to the OT. In general, it is not true that it is the full OT alignment that provides the best accuracy. The same applies to most other experiments in the paper using indirect metrics.\n\nThe only experiment demonstrating that the alignment might be indeed better is the experiment with computing the Wasserstein distance. However, it is again not directly related to the alignment. Why the authors do not directly compare the full-OT alignment with the pyramid minibatch one? This can be easily done on the artificially produced or some real-world discrete datasets of different dimensions. Also, some recent continuous benchmarks such as Korotin et al. (2021) “Do neural optimal transport solvers work? A continuous Wasserstein-2 benchmark” can be used to sample discrete pairs with known optimal transport maps in various dimensions.\n\n*Related work.* There are other mini-batch-like methods for OT. I believe a detailed discussion of these methods and proper comparisons should be included.\n\n[1] Nguyen, Khai, et al. \"BoMb-OT: On Batch of Mini-batches Optimal Transport.\" arXiv preprint arXiv:2102.05912 (2021).\n\n[2] Nguyen, Khai, et al. \"An Efficient Mini-batch Method via Partial Transportation.\" arXiv preprint arXiv:2108.09645 (2021).\n\n[3] Fatras, Kilian, et al. \"Unbalanced minibatch optimal transport; applications to domain adaptation.\" International Conference on Machine Learning. PMLR, 2021.\n\nIn general, the paper is very poor in terms of relevant references and the discussion of the related work.\n\n**Unsupported claims. ** *“WGAN (Arjovsky et al., 2017) minimize mapping functions over the Wasserstein objective like the experiments in this paper. … and we show how this approximation can equal or improve upon results produced by these methods,”* - I think this claim is not entirely correct and is misleading. In WGANs, two distributions are compared -- supported on the manifold (fake/generated continuous w.r.t. the Lebesgue measure on the manifold) and discrete data distribution. In WGANs, the optimization is performed by stochastic dual methods employing discriminators. In this paper, in all the cases the authors operate with primal-form optimal transport and discrete distributions exclusively. Therefore, the statement of the authors is unsupported.",
            "summary_of_the_review": "Due to the above-mentioned issues, it is hard to judge the usefulness of the approach and say whether the observed empirical performance generalizes to other practical tasks/datasets. Based on these serious drawbacks, I think the paper is not significant enough for the ICLR community. Thus, I recommend rejecting the paper.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No",
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}