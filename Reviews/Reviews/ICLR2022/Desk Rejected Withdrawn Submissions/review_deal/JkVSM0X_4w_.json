{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes switch spaces, a data-driven representation learning approach for representation learning. The motivation is to align the geometric inductive bias with the underlying structure of data. The experiments show the advantages of the proposed method on the KG completion task and item recommendation task.",
            "main_review": "Strengths:\n\nThe experimental results are solid.\n\nWeaknesses:\n1. For $N$ spaces, there are $\\sim N^2$ combinations of product space since each space is chosen from Euclidean, spherical, and hyperbolic spaces. $K$ is manually set. Although the proposed method can automatically select $K$ spaces from the given $N$ spaces, it relies on the given $N$ spaces and given $K$. Then for a new dataset, the search space is too huge with the increase of $N$ to find the optimal combination and $K$ value. For example, the reported inference time in Section 6.2 is in $(\\mathbb{P}^{100})^N$ case with the given $K$ value. However, in order to get the best possible combination, the method needs to try other combinations $(\\mathbb{E}^{100})^p(\\mathbb{D}^{100})^q(\\mathbb{P}^{100})^{N-p-q}$ as well as searching the optimal $K$ value from $1\\leq K<N$. The huge search space makes the method impractical for application.\n2. The novelty of the proposed method is a little bit trivial. The product spaces for representation learning have been proposed. The relation parameterization is also the existing model in previous work.\n\nQuestions:\n1. In Section 4.2, what does it mean that 'tail entity embedding is not used as it shares the same embedding matrix with the head entity'? Are the head entity and tail entity in each triple always embedded in the same product space? And in Section 4.3, are the user vector and item vector embedded in the same product space? From the Introduction, each data point will automatically choose its own product space, i.e., they are not necessarily in the same product space, but the square distance is defines as $d_\\mathcal{P}^2(x,y)=\\sum_{i=1}^Nd_{\\mathcal{M}^{(i)}}^2(x_{\\mathcal{M}^{(i)}},y_{\\mathcal{M}^{(i)}})$. If $x$ and $y$ are in different product space, i.e., in some $\\mathcal{M}^{(j)}$, $x$ has component $x_{\\mathcal{M}^{(j)}}$ while $y$ doesn't, how to define the distance?\n2. The RotH work also proposed RefH and AttH. RotH can be outperformed by ReH and AttH in some datasets. Why not consider RefE/RefH or AttE/AttH as relation parameterization?\n\nMinor comments:\n1. In Section 2, the paper said that 'Each component of product space has a constant curvature while the resulting mixed space has non-constant curvature'. I'm not sure whether the statement is correct since the definition of curvature in product space is unclear.\n2. In the caption of Table 2, if the experiment is consistent with Table 1 and Figure 3, then $K$ is set to $2$ on WN18RR and $4$ for FB15K-237.\n3. The results in Figure 3 do not show any insightful statistical performance. The analysis that 'It is obvious that KG triples are not randomly distributed' seems farfetched.",
            "summary_of_the_review": "The proposed method is theoretically grounded and achieves big improvements in experiments. However, the huge search space makes the method impractical. The presentation of the method exists some ambiguity. I marginally tend to reject the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a novel learning framework, named switch spaces, which applies sparse gating network to perform the combination of the embedding spaces. The goal is to ensure the input data can be embedded into suitable spaces that allows better representation by a data-driven approach. Experimental results showed that the proposed work outperforms the existing conventional models in knowledge graph datasets and recommendation tasks. ",
            "main_review": "First, this paper is well-written, and easy to understand. This idea of combining different embedding spaces, such as Euclidean and non-Euclidean geometric is indeed novel. The proposed approach is simple but effective, where the model is constructed by an embedding layer, a sparse gating network, and a space combination component. Experiments demonstrate that the proposed switch spaces performs better in many real-world applications, such as knowledge graph completion, and movie recommendations. However, those improvements seems to be effective, but not ground-breaking. Overall, I think this is a borderline acceptable paper. \n\n\n\n  ",
            "summary_of_the_review": "My recommendation for this paper is borderline acceptable. The reason is that there is no theoretic novelty or empirical ground-breaking results in this paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The submission improves upon the product space idea, i.e., the idea that a data point can lie in multiple spaces simultaneously, where each space can be of euclidean or non-euclidean (hyperbolic or spherical).\nThe major improvement is that it allows a data point to lie in only K spaces out of the N possible ones, by introducing a new sparse gating mechanism, similar to how routing is performed in a sparse Mixture-of-Expert (MoE) model.\nEmpirical results demonstrate that it outperforms the one-space and product-space baselines on KG completion and item recommendation.",
            "main_review": "Pros:\n- Good reproducibility, with clear writing and open-sourced code. \n- The proposed sparse gating mechanism is new.\n- It outperforms the one-space and product-space baselines and achieves good results on KG completion. Experiments on the hyperparameters are extensive.\n\nCons:\n- It is unclear why the submission decides to design a new sparse gating mechanism when many have already existed in the literature of sparse MoEs. Note that the gating mechanism proposed by this submission seems to have no connection with non-euclidean spaces.\n- There is also no comparison with the existing sparse gating mechanisms. Ablation studies of the gating mechanism can also be improved, e.g., (1) how having the random noise in Eq 5 helps balance the gates, (2) what if we randomly initialize the gating network and keep its parameters untrainable, and (3) do we need to replace the TopK operation in Eq 6 with a differentiable approximation.\n- Some concrete examples may be needed for the readers to understand how the *sparse-gated* product space is better than the regular (maybe soft-gated) product space. For example, in \"... parameters in both spherical and hyperbolic spaces will be updated even if the input data only aligns with the spherical space... \", it is not easy to imagine an example where the other spaces are completely unsuitable for representing this data point, and thus it is not clear how putting this data point in other spaces as well would harm the performance.",
            "summary_of_the_review": "The motivation is not clear enough, and the proposed new technique is not well justified (see Cons).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes Switch Spaces that generalize Product Spaces. The main idea is to use a sparse gating mechanism to choose a subset of spaces based on data. The approach is tested on knowledge graph completion and recommendation tasks. Improvements over a range of baselines are claimed.",
            "main_review": "Strengths:\n\nThe idea of adaptively choosing a signature is interesting.\n\nWhile I am not an expert in knowledge graphs, improvements on FB15K-237 are really impressive.\n\nConcerns:\n\n1. My main concern is regarding the practical applicability of the proposed approach in large-scale applications, such as recommendation tasks considered in the paper. Standard approaches first map users and items independently to a space and then use some similarity to evaluate user-item pairs. In contrast, as far as I understand, the proposed approach applies the whole pipeline to user-item pairs. Thus, we cannot, e.g., precompute item embeddings and then compute distances to items for a given query – we have to apply the whole procedure, including CNN. In contrast, the baselines embed users and items independently. So, performance improvements are not so surprising. Thus, the phrase “switch space models are efficient and have a constant computational complexity regardless of the model size” seems to be not correct.\n\n2. As the authors note in the appendix, the proposed “space” is not a metric space. Thus, the title and the main text can be misleading.\n\n3. It is assumed that all spaces have the same dimensionality b. This seems to be a restriction, while standard product spaces do not suffer from this problem. Moreover, in the experiments, only product spaces with this restriction are considered.\n\n4. I do not understand the motivation of Appendix B: the statement here is trivial and is not specific to a sphere.\n\n5. The reason why the proposed space is successful is not completely clear: removing some sub-spaces from the product space makes the geometry simpler.\n\n6. The code for recommendation task (MovieLens) is not provided.\n\n7. It is written that “manually checking the underlying geometry of real-world data is almost impossible due to its intricacy and large size” – however, in the paper proposing product spaces, an empirical heuristic for choosing a signature is proposed.\n\n8. Note that other improvements over product spaces have been proposed recently, e.g., \nShevkunov K. and Prokhorenkova L. Overlapping Spaces for Compact Graph Representations. arXiv:2007.02445, 2020.\n\nOther comments:\n- For readers not from the KG domain, more background on knowledge graphs will be helpful (how entities and relations are usually represented and how predictions are made). Similarly, background on RotE and RotH will be helpful.\n- It is not written whether the numbers for the baselines are taken from the corresponding papers or are independently obtained. \n\nMinor:\n- The notation used for “N choose K” is non-standard and first used in the abstract but defined on page 4.\n- The notation (h, r, t) is first used at the end of page 3 but is defined later on page 4.\n- The first sentence of the second paragraph in 4.2 is incomplete.\n- Some figures are located far from where they are referred to.\n- “wording embedding” should probably be “word embedding”\n- “i.e,” -> “i.e.,”\n- “Figure 7(Lower)” – missing space.\n- “our assumption is that the ...” should be “Our ...”\n\n",
            "summary_of_the_review": "Interesting idea with good empirical results. However, there are some concerns regarding the applicability of the proposed approach (see main review).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}