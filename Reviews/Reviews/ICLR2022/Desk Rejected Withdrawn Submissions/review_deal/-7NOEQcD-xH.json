{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper focuses on applying ensemble to policies in reinforcement learning, where the authors propose a method to train an ensemble of policies jointly. Specifically, the proposed method keeps an ensemble of N sub-policies, and determines the joint policy action distribution as the average of these N sub-policies, and the reinforcement learning agent interacts with the environment only with the joint policy. In order to further train each sub-policy, the authors also apply an importance weighted RL objective to each sub-policy. Furthermore, in order to encourage diversity within the ensemble, the proposed method uses a diversity loss term that encourages the action distributions between the sub-policies to be orthogonal. The overall loss for training the ensemble is therefore a weighted combination of these 3 losses.\n\nThe authors prove that the ensemble policy has larger entropy than the average entropy of sub-policies, and conduct experiments on two domains: grid-world environments and an algorithm trading benchmark. The experiment results suggest that the proposed method outperforms certain prior methods.\n",
            "main_review": "Overall I think this paper is well written and proposes some interesting ideas in using policy ensemble. However I do find many limitations that need to be addressed.\n\nPros:\n\n1. The paper is very well written. The overall organization of the paper is very clear and the results are easy to follow.\n\n2. I find the ablation study in the grid world environment very informative. It tells us that the ensemble really helps.\n\nCons:\n\n1. I’m not convinced about the novelty of the proposed method. The proposed method is largely a special case under the paradigm of mixture of experts ([1], [2]), where the mixture weights are uniform instead of a learned function. The mixture of experts model has also been applied to reinforcement learning before ([3]). Unfortunately, this paper does not include references to these prior works.\n\n2. The scope of experiments of this paper is fairly limited. As an empirical paper, this paper only includes experiments on two grid-world domains and one algorithm trading domain, which I don’t believe is enough. A good starting point could be the arcade learning environment ([4]), a commonly adopted suite of environments with discrete action space that has been used in many reinforcement learning papers. \n\n3. The paper lacks certain important baseline comparisons. As a special case of the mixture of experts method, it is important to compare to [3]. As an ensemble method for policies, it is also important to compare to other RL methods that use policy ensembles, such as [4] and [5].\n\n\n4. It is not clear to me why the orthogonal objective for increasing divergence makes sense. Two vectors in the probability simplex can only be orthogonal when they have non-overlapping non-zero components. I wonder if it makes more sense to use a divergence metric instead like in [5].\n\nGiven these limitations, I cannot recommend acceptance of this paper at its current state.\n\n\nReferences\n\n[1] Jacobs, Robert A., et al. \"Adaptive mixtures of local experts.\" Neural computation 3.1 (1991): 79-87.\n\n[2] Jordan, Michael I., and Robert A. Jacobs. \"Hierarchical mixtures of experts and the EM algorithm.\" Neural computation 6.2 (1994): 181-214.\n\n[3] Ren, Jie, et al. \"Probabilistic Mixture-of-Experts for Efficient Deep Reinforcement Learning.\" arXiv preprint arXiv:2104.09122 (2021).\n\n[4] Bellemare, Marc G., et al. \"The arcade learning environment: An evaluation platform for general agents.\" Journal of Artificial Intelligence Research 47 (2013): 253-279.\n\n[5] Hong, Zhang-Wei, et al. \"Diversity-driven exploration strategy for deep reinforcement learning.\" arXiv preprint arXiv:1802.04564 (2018).\n\n[6] Lee, Kimin, et al. \"Sunrise: A simple unified framework for ensemble learning in deep reinforcement learning.\" International Conference on Machine Learning. PMLR, 2021.\n",
            "summary_of_the_review": "The paper is well written, but the novelty is limited as the proposed method is largely a special case of the mixture of experts method. The scope of experiments is fairly limited and the paper misses some important baseline comparison. Therefore, I cannot recommend acceptance of this paper.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper applies ensemble learning in the policy training of reinforcement learning. They propose a simple mean aggregation strategy to aggregate ensembled sub-polices, and introduce a diversity regularization on sub-policies. ",
            "main_review": "Strength: \n* the mean aggregation method is clear and straightforward, which is wildly used in supervised-learning area. \n* The paper is well written and easy to follow. \n* Good ablation study has been done in the paper. \n\nWeakness: \n* the idea is simple but not novel. The mean aggregation idea is well studied in supervised learning, and it has been shown that it could provide better variance reduction of predictions. \n* To me, the entropy proof is not actually useful. Theorem 1 shows that the entropy of the ensemble policy $\\hat{\\pi}$ is not less than the average entropy of sub-polices. However, the reason that motivates us to use ensembled polices with regardless of its computational complexity, is if the ensembled policy could have better exploration than the usual single policy, not an average of polices. \n* Experiments are not extensive. The paper evaluates the idea on two Minigrid environments for showing better sample efficiency and one financial trading data for illustrating generalization ability. As we know, performance of RL algorithms varies over tasks. Better try to add more tasks or environments to make the empirical proof stronger, such as MuJoCo location motion tasks. \n* As stated in the paper that the idea or technique could be applied on any policy learning of RL algorithms, better try to add other methods not just PPO here, which only represents the on-policy learning algorithm. \n* From the experiment plots and tables, it seems that the proposed devesity encouraging regularization does not help much on the performance. The improvement seems incremental. \n\nActionable feedback is included in the weakness part. \n",
            "summary_of_the_review": "The idea is interesting, straightforward and clear. However, there are still places to make the paper stronger. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "In this paper, the authors propose a simple way of ensemble policy networks, with a term to encourage diversity.\nTwo grid-world environments and one algorithmic trading application are tested.",
            "main_review": "Strengths:\n1. Ensemble policy is a really interesting topic and maybe very fruitful.\n2. Some insights (diversity) are provided, which give confidence about your work.\n3. The financial trading task is very interesting.\n\nWeaknesses:\n1. The proposed method is actually well-known from literature, not surprising.  For example, the following one:\n    Wiering, M.A. and Van Hasselt, H., 2008. Ensemble algorithms in reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 38(4), pp.930-936.\n     The novelty and depth may be not good enough for ICLR.\n2. Actually, the tested tasks are really not very challenging:\n   A. Those two grid world tasks are simple.  \n   B. The algorithmic trading one sounds useful, actually also simplified, but is good enough for verification in a paper.\n3. However, for algorithmic trading, besides the two reported metrics (PA and Reward (cumulative return)), I believe many people are interested in the following trading matrics:\n    Annual return, Annual volatility;  Sharpe Ratio, Maximum Drawdown, etc.\n4. Also for trading, some widely used comparisons are expected to include:\n    A. Market index (benchmark performance); B. Equally weighted strategy, which is a little universal;  C. Buy-and-hold, say top-20%.\n    Without such information, the DRL agent's performance is not very convincing to the general readers.\n\n",
            "summary_of_the_review": "I worked on those experiment tasks and also used ensemble methods for DRL.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents an algorithm for training an ensemble of policies in deep reinforcement learning. An ensemble policy is defined as a mixture of sub-policies, and the PPO-like policy update for the ensemble policy is proposed. To control the ensemble policy behavior, the policy is updated using the KL divergence constraint on the ensemble policy. To strengthen the benefit of the ensemble policy, a regularization that encourage the diversity of the sub-policy is introduced.   The proposed method is evaluated on tasks with discrete actions. The experimental results indicate that the proposed method outperforms PPO and that the diversity regularization and the ensemble-aware loss improve the performance of the method.",
            "main_review": "Strong points of the paper\n- paper is well-written and easy to follow\n- proposed method is clearly presented\n\nWeak points of the paper\n- novelty of the proposed method is not clear\n- experimental results are not convincing enough\n\nDetailed comments\n\nThe idea of training an ensemble of policies is not novel, and it is also common to introduce the techniques that encourage the diversity of sub-policies. For example, recent studies such as [1,2] proposed algorithms for training ensemble of polices (In [2], authors call an ensemble of policies a mixture of experts.)  Compared to these previous studies, I do not see clear significant novelty of the proposed method.\n\n[1] K. Lee et al., SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning, ICML2021\n\n[2] R. Akrour et al., Continuous Action Reinforcement Learning from a Mixture of Interpretable Experts, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), early access.\n\n[3] Jack Parker-Holder et al.,  Effective diversity in population based reinforcement learning. NeurIPS 2020.\n\nEspecially, the algorithm presented in [2] also uses the PPO-like KL divergence constraints. I would like to encourage the authors to discuss the advantages of the proposed method compared with these existing methods.\n\nThe experimental results are not very convincing. I see some weak points of the experiments.\n- Although the proposed method is applicable to continuous control tasks, it is evaluated on a few tasks with discrete actions. I would like to encourage the authors to evaluate the tasks on continuous control tasks as well. \n\n- The proposed method should be compared with more recent methods, e.g. methods presented in [1,2,3].\n\n- Although the diversity is compared with DIAYN, DIAYN is not developed for solving a specific task. A recent study [4] proposed an algorithm, SMERL, that discovers diverse solutions using the unsupervised reward proposed in DIAYN. SMERL is more appropriate baseline for evaluating the diversity of policies.\n\n[4] S. Kumar et al., One Solution is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL. NeurIPS 2020.\n\n",
            "summary_of_the_review": "While the paper is well-written, the novelty of the proposed method is not clear. In addition, the experiments can be improved by adding continuous control tasks and more recent methods as baselines. I would like to encourage the authors to discuss the novelty of the proposed method and strengthen the experiments. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}