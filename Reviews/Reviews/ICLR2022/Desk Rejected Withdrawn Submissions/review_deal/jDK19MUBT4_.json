{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper the authors propose a data augmentation method, namely TailMix, to create more tail labels for extreme multi-label classification problems. The major difference between TailMix and other Mixup type method is that TailMix act on the label context vector rather than initial token embeddings. The method is applied on top of two XMC models, AttentionXML and LaRoBERTa, and empirical results show improvement on PSP scores on 3 XMC benchmark datasets.",
            "main_review": "The paper is easy to follow and addresses a well know problem. The proposed method first samples a label based on inverse label popularity and then generate new samples with M neighbors defined by co-occurrence graph. Experimental results show that TailMix can improve precision and PSP scores over baseline methods. However, I have several concerns about the novelty and the empirical settings of the paper.\n\n* The authors propose to use the context vectors rather than the token embeddings to generate new samples but do not provide enough motivation on this choice. How does the method perform if samples are generated by the initial token embeddings? \n\n* How scalable is the proposed algorithm in terms of output size? The label attention layer is linear to the output size L, which seems to be computationally prohibitive when the output size is large.\n\nExperiments:\n\n(1) The method is only compared with baseline, other Mixup methods should be included in the comparison.\n\n(2) Ablation study is needed to understand the difference between mixup by token embeddings or label context vectors.\n\n(3) Since the experiments are conducted on public XMC benchmarks, the authors should include recent SOTA models such as LightXML [1], X-Transformer [2] and XR-Transformer [3].\n\n(4) In AttentionXML the experiments are conducted on 6 benchmark datasets whereas in this paper only 3 with small output size are chosen. Is this because the proposed method is unable to scale to large output space?\n\nOther comments:\n* w_1...w_t used in Figure(1)(a) as token embeddings, in Eq(3) it's used for label attention weights.\n\n[1] T. Jiang, D. Wang, L. Sun, H. Yang, Z. Zhao, and F. Zhuang. LightXML: Transformer with dynamic negative sampling for high-performance extreme multilabel text classification. In AAAI, 2021.\n\n[2] W. Chang, H.-Fu Yu, K. Zhong, Y. Yang, and I. S. Dhillon. Taming pretrained transformers for extreme multi-label text classification. In KDD, 2020.\n\n[3]J. Zhang, W. Chang, H.-Fu Yu and I. S. Dhillon. Fast Multi-Resolution Transformer Fine-tuning for Extreme Multi-label Text Classification. arXiv preprint, 2021.\n",
            "summary_of_the_review": "The paper present a new method to deal with long tail distribution in XMC problems. Although I think the direction is promising, more work need to be done to make this paper publishable.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors proposed a new approach for eXtreme Multi-Label Classification (XMLC) that addresses labels imbalance and sparsity, named TailMix. It's based on a mixup, the idea of generating new samples by interpolating two samples existing in the training datasets. The TailMix performs the interpolation of labels and context vectors (intermediate hidden representations that are different for each label) of the same sample. The first context vector and label to \"mix\" is selected by sampling according to from positive labels according to the probability distribution calculated using softmax function on inverse labels propensity values of positive labels. The second selected context vector is the closed one to the first in terms of euclidean distance. A label proximity graph is used to narrow the number of candidates to speed up the selection process. The authors conducted experiments on 3 popular benchmark datasets using two different underlying architectures. In most cases, the proposed method achieves improvement in predictive performance in terms of precision@k, and propensity scored precision and nDCG.",
            "main_review": "Strengths:\n+ The paper has clear and simple motivation.\n+ The organization of this paper is good, and it's easy to follow.\n+ Comparison with different XMLC methods as well as mixup methods on 3 datasets, additional ablation study using two different underlying architectures.\n+ The proposed method improves over the base methods in terms of precision@k and propensity scored precision and nDCG.\n+ As far as I know, a mixup approach is a novel approach in the area of XMLC. The only work I recall that shares some minor similarities is:\nECLARE: Extreme Classification with Label Graph Correlations\nAnshul Mittal, Noveen Sachdeva, Sheshansh Agrawal, Sumeet Agarwal, Purushottam Kar, Manik Varma\n+ Proposed approach can be used with many different architectures.\n\nWeakness and doubts:\n- In similar semantic context vector selection, there is M parameter. I can't find information what is its value used for the experiments.\n- What if the label has less than M neighbors in the proximity graph? For very rare labels, this can be a problem. It can have just one or two or even no neighbors in the graph. Why not use some other fast-NN method like LSH, especially considering that labels are missing (propensity model). This makes me suspect that the proposed method may mostly improve on torso labels with at least a few examples.\n- Only one context vector is selected to be mixed at the time. Is there a reason for that, beyond the simple selection method from softmax? Why not mix a few at once, since they don't affect each other.\n- The ensemble of the method without TailMix and with TailMix seems to be a little unfair. The single version of the method with TailMix provides only minor improvement in most cases.\n- Experiments only on 3 datasets. Since the work aims to address extreme multi-label classification problems, I lack experiments on at least one or two bigger datasets, e.g., Amazon-670K, Wikipedia-500K.\n- The AttentionXML serves as one of the base architecture for proposed methods. The architecture uses PLT to partition label space, then learns separate Attention models for each level of the tree. I'm not sure from the text if many-level architecture is also used in this work since it's not mentioned later in the text. If it's just a flat model with a 1-vs-all type of output, it would be interesting to see how it performance with label-tree.\n- Some baseline mixup methods also use $\\alpha$ parameter to sample mixing ratio $\\lambda$ from the Beta distribution. Authors state that _\"For TailMix, we set α varying from 0.2 to 0.6 and report the best model for each dataset\"_. I wonder if it is also a case for other mixup methods, because otherwise, it's not a fair comparison.\n- I believe some information in related works are a bit inaccurate or missing. I understand that due to limited space, authors must simplify things, but false statements should be avoided:\n  - _\"DisMEC (Babbar & Schölkopf, 2017), ProXML (Babbar & Schölkopf, 2019), PDSparse (Yen et al., 2016), and PPDSparse (Yen et al., 2017) enforce sparsity using L1 penalty term to reduce the number of model parameters\"_ - DisMEC uses L2 penalty and then enforce sparsity by pruning weights with an absolute value below a given threshold.\n  - _\"Besides, Parabel (Prabhu et al., 2018b) and SLICE (Jain et al., 2019) employ negative sampling to balance the number of positive and negative samples.\"_ - I believe that the original Parabel doesn't use any type of negative sampling.\n  - _\"AttentionXML (You et al., 2019) utilizes Bi-LSTM and attention mechanism and builds probabilistic label trees (PLTs) for feature vectors\"_ - I don't understand what the _\"for feature vectors\"_ part means. As far as I remember, for AttentionXML, the tree is built by performing balanced k-means clustering on labels features, similar to Parabel.\n  - I find citations on hidden layer mixup methods a bit inconsistent: first only Sun et al. 2020 and Guo et al. 2019 are cited, and later TMix and HiddenMix are introduced in the experimental section, which are also a mixup method applied on hidden layers. I would discuss these baseline mixup methods in related work.\n- I don't understand why TailMix is so much faster than other methods for LaRoBERTa on Eurlex and Wiki10 but not for AttentionXML, what can be a reason for that? \n\nNITs:\n- No citation for probabilistic label trees (PLT): \nExtreme F-measure Maximization using Sparse Probability Estimates\nKalina Jasinska, Krzysztof Dembczynski, Robert Busa-Fekete, Karlson Pfannschmidt, Timo Klerx, Eyke Hullermeier\nProceedings of The 33rd International Conference on Machine Learning, 2016\n- No citation for Bonsai:\nBonsai - Diverse and Shallow Trees for Extreme Multi-label Classification\nSujay Khandagale, Han Xiao, Rohit Babbar\n- Some notations seem to be introduced quite late in the text. Introducing a multi-label setting earlier could help.\n- In the \"Evaluation metrics\" subsection, each measure could be separate equations instead of presented as just two.",
            "summary_of_the_review": "I believe that the paper is marginally below the acceptance threshold. The paper is generally nicely written and the proposed method is interesting and novel, but some clarifications for the method and experimental protocol are needed. Additional experiments would strengthen the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents a mixup inspired approach for better prediction on tail-labels in extreme multi-label classification. While mixup as a data augmentation technique has been applied in the instance space to create convex combinations of samples and corresponding labels. The proposed method uses this idea in the label space to account for the lack of training data in the tail-labels. This is done by creating a proximity graph for creating shortlists of candidates for mixup by co-occurrence. The proposed method when applied to existing XMC algorithms leads to some improvement in prediction performance.",
            "main_review": "=======Strengths======\n+ The idea to extend mixup from the sample space to label space using the proximity graph is somewhat novel.\n+ The idea is simple and easy to implement, and seems quite generic with potential application to a range of XMC methods.\n\n=========Weaknesses=======\n- The technical contribution of the paper is quite limited. While this may be fine as part of a suite of techniques for improving tail-label performance, but it does not seem sufficient for a stand-alone paper (at ICLR) purely based on this idea.\n- The experiments are performed on much smaller datasets, and it is unclear why much larger ones haven't been tried or reported. Is it that the method is unapplicable in the PLT setting, that is typically used for larger datasets with upto 3M labels.\n- It seems that the results for attentionXML have been under-reported. From the original AttentionXML paper [1, Table 6], the results on the three reported datasets are much higher.\n- Since the focus is on tail-labels, which seem to be classified well when using a propensity scored loss such as that given in [2]. Did the the authors use/try such loss functions instead of the vanilla BCE loss. As shown in Table 4 of [2], much better performance on PSP metrics can be obtained than what is reported in this paper.\n\n[1] AttentionXML: Label Tree-based Attention-Aware Deep Model for High-Performance Extreme Multi-Label Text Classification, https://arxiv.org/pdf/1811.01727.pdf\n[2] Convex Surrogates for Unbiased Loss Functions in Extreme Classification With Missing Labels, https://dl.acm.org/doi/pdf/10.1145/3442381.3450139",
            "summary_of_the_review": "The idea is simple, but does not seem quite impactful to be a stand-alone ICLR paper. There are many missing empirical evaluations, and incomplete results for existing methods such as AttentionXML are provided.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed a data augmentation technique to improve label-wise attention based methods on tail labels for extreme multi-label classification problems. In particular, the proposed method augment the samples of tail labels by mixing up the context vectors (output embedding of the label-wise attention layer) of the tail labels for samples which are close to each other and are assigned to the same label. \nThe experimental results show that adding the proposed technique TailMix to existing label-wise attention based methods such as AttentionXML and LaRoberta can improve the performance of the baselines on the tail labels. It also compares with other mix-up techniques and shows better performance. ",
            "main_review": "The paper is clearly written and easy to follow. The proposed mixup method TailMix should be novel as far as I know. The experimental results also show improved performance especially on tail labels by utilizing the proposed method. The authors also conducted comparison with other mixup methods. \n\nI also have several concerns and questions. \n\n1. A limitation of TailMix is that the method can only be applied to label-wise attention based XMC methods. Most of the existing XMC methods do not fall in this category.\n\n2. Table 2 shows the comparison with other XMC methods. As mentioned in the caption, the results of most compared XMC methods are copied from the publication or the XMC repository (Bhatia et al., 2016). In contrast, AttentionXML results are from authors' own implementation. However, its performance is much worse than the numbers reported in the paper  (You et al., 2019). For example, the PSP@1 for AmazonCat-13K from the original AttentionXML paper is 53.76, while Table 2 reports 51.29. I am wondering what causes the gap.\n\n3. By comparing AttentionXML+TailMix with AttentionXM from Table 2, I see PSP and PSN have some improvement, but P@1,3,5 seem to be same or even worse. I guess this is because the mixup operation is mostly applied to tail labels. What if the mixup operation is applied uniformly to the labels? Will it improve P@1,3,5 as well? \n\n4. In Table 4, there is no result for AmazonCat-13K. Table 2 and Table 3 have results for AmazonCat-13K. \n\n5. It would be good if the authors can provide some insights for the sampling strategy \"similarity\" and \"label proximity graph\". Selecting mixup samples based on \"similarity\" doesn't seem to be intuitive to me. \n\n6. Since the paper is improving the tail label performance, it's probably better to report the results of PfastreXML in Table 2. PfastreXML has superior performance in PSP and PSN although its performance on Precision is inferior. \n\n7. The comparison results with other mixup methods in Table 3 doesn't seem to show that TailMix has consistent superior performance than other mixup methods. ",
            "summary_of_the_review": "The paper proposed a novel and promising mixup technique for XMC to improve tail label performance. My main concern about the paper is its limitation; it can only be applied to label-wise attention based XMC methods. The results shown in the paper don't convince me that TailMix is consistently better than other mixup methods. Therefore, I think the paper is slightly below the acceptance bar. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I don't see any ethics concerns. ",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}