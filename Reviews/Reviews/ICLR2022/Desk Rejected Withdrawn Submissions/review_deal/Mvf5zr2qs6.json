{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This manuscript proposes a new metric called connectivity sharpness, and design a scheme called adaptive $\\beta$-decay for batch normalization layer which empirically works better in the large batch setting. ",
            "main_review": "Strengths: the paper is well-written. The idea of defining a new sharpness which is invariant to re-parameterization is novel. \n\nWeaknesses:\n\n1. In Theorem 3.1, it shows that the defined connectivity sharpness is invariant according to re-parameterization (9). However, it is just one particular case of re-parameterization. I think the analysis is particularly restricted to linear re-parameterization. It may not be invariant for nonlinear re-parameterization. So the Theorem 3.1 is not strong.\n\n2. There is no explanation why small connectivity sharpness implies better generalization empirically and theoretically. Figure 1 in Section 4 only shows the correlation between test accuracy and the sharpness measure. However test accuracy does not represent generalization.\n\n3. In Section 4, why only considering batch normalization layer? What about other layers? \n\n4. In Section 4, the authors show that the $\\beta$ decay scheme can reduce connectivity sharpness measure empirically. However, there is no clear explanation why this is the case. For example, the interpretation in Section 4.2 is not convincing to me.\n\n5. The improvement of testing accuracy is not statistically significant in Table 4 and Table 5. \n\n6. The presentation of Algorithm 1 (in Appendix D) is not described clearly. For example, what is Adam ratio (element-wise division)? What if $v^t=0$? More details and explanations are needed.\n\n7. There is no evaluation of connectivity-sharpness for small-batch training. Does the $\\beta$-decay scheme work for small-batch?",
            "summary_of_the_review": "This manuscript provides a $\\beta$-decay scheme to improve large-batch training. A plausible explanation is that this scheme can reduce the connectivity sharpness. The notion of connectivity sharpness is only invariant to a very small class of re-parameterization and may not be sufficient to practice. The connection between the $\\beta$-decay scheme and the proposed measure is not clearly presented. The empirical results only marginally improve the state-of-the-art.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new sharpness measure \"connectivity sharpness\".\n\nCompared to Hessian-based measures in prior work, the proposed connectivity sharpness is 1) cheaper computationally since it is calculated with gradients only, 2) favorably invariant to reparameterization, and 3) better correlated with generalization performance (i.e. test accuracy).\n\nWhen evaluating the connectivity sharpness of BN layers, the authors find that unregularized BN bias leads to higher value of sharpness, and accordingly proposes an adaptive bias decay that achieves better test accuracy on large batches.\n",
            "main_review": "Strengths:\n  - The proposed connectivity sharpness is easier to compute than the Hessian-based measures, and is scaling invariant.\n  - The experiments of adaptive $\\beta$ decay is promising; a clarification question: are numbers in Table 4 and 5 from single runs, or are they averaged of multiple runs? If it's the latter, what's the standard deviation?\n\nQuestions:\n  - How does the batch size $m$ affect the batch-based connectivity sharpness $C_m(\\theta)$?\n  - Experiments in Sec 3 are on VGG whereas Sec 4 uses ResNet. How does the use of skip connection affect the connectivity sharpness?\n  - Figure 1 and Figure 5-7: the points are spreading evenly, and look more like clusters with some outliers.\n    - Are the clusters caused by particular hyperparameters?\n    - There seem to be 16 setups (2 values for each of the 4 hyperparameters), but only 9 points shown in the figure. Could you explain which combinations of hyperparameters are left out and why?\n    - 2 values for learning rate / weight decay / bias decay seem a bit too few.\n  - Substructure separability: eq (11): the connectivity sharpness monotonically increases in the number of layers, but the test accuracy shouldn't be monotonic. Does this mean this violate the negative correlation between connectivity sharpness and generalization? I may be misunderstanding this, so to clarify:\n    1. by \"generalization\", do you mean the test accuracy, or the generalization gap (i.e. difference in train/test accuracy)?\n    2. If we add more layers to an existing network, how would the added layers affect the connectivity sharpness of the previous layers?\n  - I don't get the point of analyzing $C(\\beta)$ throughout training: the \"progressively sharpening\" and \"progressively flattening\" phases seem to follow directly from definition, i.e. $C(\\beta)$ is small if either $\\beta$ or its gradient is small, since $C(\\beta)$ is the product of the two. Could you explain a bit more what should be the takeaway from this analysis?\n  - Last sentence of the second paragraph on page 8: \"effect $\\beta$ decay appears across the entire network\": Table 2 seems to only show results on the FC layer, why is it the entire network? Which BN layers are $\\gamma, \\beta$ for?\n  - Sec 4.2, interpretation of $r_l$: the explanation could be made more rigorous.\n\nMinor points:\n  - Eq (4): the auxiliary variable $c$ hasn't been defined before.\n  - Minor grammar errors (e.g. articles, prepositions), e.g. the second paragraph on page 8.\n",
            "summary_of_the_review": "This paper proposes a new sharpness measure with some desirable properties, which motivates an adaptive layer-wise bias decay that improves the test accuracy from large batch training.\n\nHowever, as a mostly empirical paper, the current experiments are not convincing enough, and the same for the analyses in Section 4.\nI'm having trouble justifying the amount of contribution in the current form; perhaps the paper could benefit from better presenting the results.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work proposes a novel generalization measure for deep neural networks, called connectivity sharpness.\nThe connectivity sharpness has three useful features: 1) it is invariant to reparameterization of neural network parameters, 2) it can be evaluated for each subset of a network (“substructure separability”) to identify the most sensitive part of a network to the sharpness, and 3) it is much easier to calculate than the existing Hessian-based sharpness measures. Using this connectivity sensitivity, the authors identified that the “no bias decay heuristic,” which leaves $\\gamma$ and $\\beta$ parameters in BN layers unregularized, may have a detrimental effect on generalization performance in large batch training. Using $\\beta$ decay (weight decay on $\\beta$ parameters of BN layers), they showed the test accuracy of ResNet-50 on ImageNet classification improved from 76.56% to 77.24% with a batch size of 32k.",
            "main_review": "Strengths\n- The proposed connectivity sharpness is a novel sharpness measure that is much easier to evaluate than the Hessian-based sharpness measures and invariant to reparameterization of the parameters of neural networks.\n- This work provides an empirical data point that shows that the “no bias decay heuristic”, which leaves $\\gamma$ and $\\beta$ parameters in BN layers unregularized, perhaps have a detrimental effect on generalization performance of ResNet-50 models on ImageNet classification task in large batch training.  \n\nWeaknesses\n - The validity of the connectivity sharpness as a generalization measure is not well verified.\n    - Each plot in Figures 1, 5, 6, and 7 contains only 9 data points. It is difficult to evaluate the correlation with the generalization measures from the limited number of samples.\n    - Jiang et al. [1] conducted a large-scale comparison of 40 generalization measures using ~10,000 CNN models (different architecture, hyperparameters, optimizers) on image classification tasks. They concluded that the PAC-Bayesian generalization bounds, which capture the sharpness of loss function, is the most successful measure. \n    - Tsuzuku et al. [2] provided PAC-Bayesian generalization error bounds which is invariant to reparameterization. Based on the bounds, they also proposed a normalized sharpness measure. It is tempting that the proposed connectivity sharpness is reparameterization invariant, but such a sharpness measure already exists. \n     - The proposed connectivity sharpness has the advantage of being easy to calculate, but more data points are required to argue that is as useful as the existing sharpness measures. \n- The $\\beta$ decay seems to have only a marginal effect on the connectivity sharpness.\n  - Even if the connectivity sharpness is a valid generalization measure, the effectiveness of $\\beta$ decay is questionable.\n  - Table 2 shows that the $\\beta$ decay only reduces the final $C(\\theta)$ by 0.13, which is quite small compared to the scale of $C(\\theta)$ in Figure 1c (10-1000 for the test accuracy of 84-89%). \n  - This comparison of the scales might not make sense because Figure 1 uses VGG-11 on CIFAR-10 while Table 2 uses ResNet-18 on CIFAR-10 (different network architectures might have a different scale of connectivity sensitivity). But that is why we do not know how much of an impact a value of “0.13” would have for ResNet-18 on CIFAR-10, or other network architectures and tasks.\n- With only the results given in this work, it is hard to attribute the improvement of generalization performance to $\\beta$ decay.\n  - Tables 3, 4, and 5 only show a marginal difference in test accuracy (<1%) without providing error bars.\n  - Depending on the choice of initial parameters and hyperparameters, the results can vary greatly.\n  - To test the effectiveness of $\\beta$ decay, a wider hyperparameter search space and an analysis of sensitivity to test accuracy are required.  \n\nOther comments\n- “Substructure separability”: layer-wise Hessian-based measures (e.g. Hessian trace) also have this property. Although this property is useful for analysis, I do not think it is novel enough to emphasize. \n- Abstract and Section 4: Calling both $\\gamma$ and $\\beta$ in BN as bias parameters is confusing. Consider calling $\\gamma$ as scale parameter and $\\beta$ as bias parameter so that it is easy to understand that “bias decay” means “$\\beta$ decay”.\n- Related works (Sharpness/Flatness of minima):  Jiang et al. 2019 [1] and Tsuzuku et al. 2020 [2] should be cited.\n- At the end of Section 2: typo “generalizatThe I ion”\n - Subsection 4.1: “$C(\\beta)$ and $C(\\gamma)$ is reduced” -> “$C(\\beta)$ and $C(\\gamma)$ are reduced”.\n- Figure2 and 3: \n  - the labels of x axis should be $||\\beta||$, $||\\gamma||$, and $||\\theta||$.\n  - $w$ and $C(w)$ should be $\\theta$ and $C(\\theta)$\n- Table 2: “No bias decay” -> “No $\\beta$ decay”?\n- Table 4 and 5: “Test accuracy of adaptive $\\beta$ decay” is not descriptive. Consider using the caption “Test accuracy of ResNet-18 models on CIFAR-10/100 trained with $\\beta$ decay” or something like this.\n\nReferences\n- [1] Y. Jiang et al. Fantastic Generalization Measures and Where to Find Them. 2019. https://arxiv.org/abs/1912.02178\n- [2] Y. Tsuzuku et al. Normalized Flat Minima: Exploring Scale Invariant Definition of Flat Minima for Neural Networks Using PAC-Bayesian Analysis. 2020. http://proceedings.mlr.press/v119/tsuzuku20a.html\n",
            "summary_of_the_review": "The low computational cost of the proposed connectivity sharpness is attractive if it is as effective as the Hessian-based generalization measures. However, the effectiveness of the connectivity sharpness as a generalization measure has not been adequately tested. Also, the relationship between connectivity sharpness, $\\beta$ decay, and the final test accuracy has not been clearly verified. I would first recommend observing the correlation of the connectivity sharpness with the generalization gap in a broader range of training settings (e.g., different optimizers, network architectures, hyperparameters). Since the credibility of the story \"$\\beta$ decay fills the generalization gap\" depends on the results of this observation, I believe that the current manuscript does not contain enough contribution for publication. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper defines connectivity sharpness measure and proposes simple bias decay methods to fix the performance gap in large batch optimization by addressing the limitation that the sharpness of bias affect the sharpness of entire network.",
            "main_review": "Strengths:\n1. Proposed sharpness measure Connectivity Sharpness makes sense intuitively and is shown that it is a proper measure for sharpness and generalization as well as has nice properties like reparameterization invariance and Substructure separability.\n2. The paper shows that using adaptive beta decay would lead to better accuracy for large batch optimization.\n\nWeakness:\n1. The experiments are done on ResNet50, was wondering if the choice of networks would affect the effectiveness of beta decay.",
            "summary_of_the_review": "Overall, the paper is a good paper. It proposes a new measure and shows that it leads to better performance for large batch optimization, though it is not clear to me that if the conclusion generally holds for any networks.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}