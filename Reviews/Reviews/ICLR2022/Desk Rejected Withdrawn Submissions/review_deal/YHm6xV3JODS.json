{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper concerns the task of relational triplet extraction from text. They claim that there is a significant overlap between the relational triplets that are included in the train and test sets of common RTE Benchmarks, and that therefore the performance levels reported so far for these benchmarks is not reliable, where in practice the models do not generalize well to previously unobserved triplets. \n\nThey propose several strategies for manipulating the dataset to improve learning generalization / evaluation:\n1) remove some sentences from the training set that contain triplets from the test set to construct an overlap sifted dataset. the model degrades as the portion of unseen cases increases.\n2) `entity noising' - replace the entities in the training sentences with random words. The models should supposedly then learn entity-agnostic information to extract triples. \n3) `context noising' - replace a non-entity word with another similar word (synonyms from Wordnet) with a small probability, or swap non-entity words with some probability. (up to 5 swaps or substitutions per sentence to maintain the original meaning.) \n\nPerformance is evaluated on the original versions of NYT and WebNLG RTE datasets, and on the sifted train sets (showing degradation).\nThe data noising approach is evaluated also on selected sentences that contain 16 triples of (temporally) new facts which were manually extracted from Wikipedia. It is shown that entity+context improves performance both in terms of precision and recall. (However, performance is generally low when there is little overlap between train and test).",
            "main_review": "The topic is interesting and timely. The proposed techniques makes sense.\n\nSome concerns:\nI'm missing a discussion and reference to approaches proposed in related works about train-test overlaps in NLP evaluation. For example:\n- Lewis et al, Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets, EACL'2021\n- Elangovan et al, Memorization vs. Generalization : Quantifying Data Leakage in NLP Performance Evaluation, EACL'2021\nLikewise, context diversification by means of word swaps (with synonyms etc.) is not a novel idea.\n\nWith respect to the experiments:\n- Table 5 shows improvements using context noising + entity noising, but what about each of them separately?\n- If the goal is to test generalization, and you have two datasets, why not train on dataset a and test on dataset b?\n- The removal of BERT pre-trained information makes sense in a way. But, beyond possible memorization of triplets, pre-training allows to observe more diverse contexts and entities. I think that a more realistic evaluation would be: fine-tune BERT using the training data, with and without your augmentation techniques, and then show the differences. At least, I'd present this as additional experiments. \n\nTypos:\n======\n- nosing --> noising\n",
            "summary_of_the_review": "The paper highlights important concerns with respect to benchmark dataset evaluation of relational triplet extraction.\nTwo simple techniques are proposed to artificially reduce these overlaps - mainly, replacing some names and wording within the training examples. \nOverall, the paper is of interest. It lacks however more references to related works (there are quite a few similar works exploring failure to generalize and assess generalization due to overlaps, where the authors present their findings as groundbreaking).\nAlso, I'm missing an ablation study, and a discussion or evaluation of how this can be applied at large-scale, in realistic settings, namely, improve the generalization of models like BERT.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper challenges existing relational triple extraction benchmarks in the sense that existing solutions may just memorize patterns in training data. To verify the hypothesis, the paper created two versions of test datasets based on existing benchmarks by (1) synthetically replacing entities using language and word-embedding models, and (2) removing tripels that partially/fully appear in training data. The paper develops two regularization techniques: Entity Noising, which replaces entities with random noisy words, and Context Noising, which adds perturbations to the context. Experimental results on the modified benchmarks show that state-of-the-art methods suffer from extracting unseen triples, whereas Entity Noising and Context Noising significantly improve the performance of those models. \n",
            "main_review": "### Strength\n\n- (S1) This paper raises an interesting question on existing benchmarks.\n- (S2) The paper is easy-to-follow. \n\n### Weaknesses\n\n- (W1) The paper disregards the line of work in Open Information Extraction.\n- (W2) The augmented test set has an issue as a benchmark.\n- (W3) The design choice of the solution (Entity Noising and Context Nosing) is not intuitive or justified. \n\n\n### Major comments\n\nThis paper challenges existing relational triple extraction benchmarks and develops two regularization models to address the issue. I like the way the paper challenges something that has been used/believed in a research community, which should help progress the research. However, I believe the paper has the issues listed below:\n\n(W1)\nAlthough the paper claims extracting unseen triples as a scope of relational triple extraction, the problem seems to be more like Open Information Extraction (Open IE), which is completely disregarded in the paper.\n\nRecent Open IE methods [1-2, 4-5] are trained in a (distantly) supervised manner, and thus can be used for the relational triple extraction task tackled in the paper. It is worth referring to Open IE benchmarks, especially CaRB [3], which are manually annotated and contain a good amount of unseen triples (i.e., the main scope of this paper.)\n\n- [1] Lei Cui, Furu Wei, and Ming Zhou, Neural open information extraction. ACL 2018. (https://aclanthology.org/P18-2065/)\n- [2] Gabriel Stanovsky, Julian Michael, Luke Zettlemoyer, Ido Dagan. Supervised open information\nextraction. ACL 2018. (https://aclanthology.org/N18-1081/)\n- [3] Sangnie Bhardwaj, Samarth Aggarwal, Mausam Mausam, CaRB: A Crowdsourced Benchmark for Open IE, EMNLP 2019 (https://aclanthology.org/D19-1651/)\n- [4] Keshav Kolluru, Vaibhav Adlakha, Samarth Aggarwal, Mausam, Soumen Chakrabarti, OpenIE6: Iterative Grid Labeling and Coordination Analysis for Open Information Extraction, EMNLP 2020. (https://aclanthology.org/2020.emnlp-main.306/)\n- [5] Keshav Kolluru, Samarth Aggarwal, Vipul Rathore, Mausam, Soumen Chakrabarti, IMoJIE: Iterative Memory-Based Joint Open Information Extraction, ACL 2020 (https://aclanthology.org/2020.acl-main.521/)\n\nThe author(s) should clarify the similarities and differences between (unseen) relation triple extraction and Open IE. If they are the same, those recent Open IE methods should also be tested. \n\n\n(W2)\nThe augmented test set could contain invalid triples, which are not extrinsically verified (e.g., human annotation.)\n\nThe fact is that semantic similarities between entities do not ensure anything about the validity of triples, as the author(s) admitted in the paper. In p.4 the last paragraph, the author(s) claim that factual validity does not matter for relational triple extraction. If that is the case, such extractors cannot be used for KB construction that the paper aims to solve at the end. \n\nAnother issue is that something that is created by a language model can be inferred by that language model. In that sense, I’m afraid that the augmented test data has the same type of issues as the existing relational triple extraction benchmarks that the paper criticizes.\n\n\n(W3) \nEssentially, the goal of Entity Noising is to create more examples that are semantically irrelevant to the original examples (by replacing entities with *noisy words*.) This does not make sense from the generalization viewpoint. Essentially, it sounds like “Forget about the original training data.” In fact, from Figure 3, I presume that probably “learning less” performs better on the overlap sifted datasets. \n\nThe intuition of why adding more examples that are different from the original examples is not well-explained. That is, the paper should provide evidence that supports the statement “Adding such examples only alleviates the issues created by the original examples while preserving the generalized knowledge obtained from training examples (i.e., without having a “side-effect”).”\n\nAlso, since randomly initialized BERT, which should drastically lose the capability of the model, \nwas used for the experiments, it’s difficult to judge if using those data augmentation techniques is really helpful. \n\nFor Context Noising, I didn’t find technical novelty against (Wei & Zou 2019).\n",
            "summary_of_the_review": "This paper’s claim is essentially the scope of Open IE, which is completely disregarded in the paper. The augmented dataset creation and the solutions have critical issues or are not well-justified. Thus, I consider the paper cannot be accepted in the current form.\n",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposed a method for extracting unseen triples from text. They modified/augmented the original test data to test model's ability to generalize unseen triples at training time. By introducing the \"context noising\" and \"entity noising\" strategies, their model improves previous state-of-the-art models on the newly proposed generalization tasks.\n\nWhile solving the generalization problem on extraction unseen triples is a practical problem for building robust relation extracting (RE) system, I have a few concerns on the design of the experiments and the comparison the previous state of the art system. Please see my comments below.",
            "main_review": "I am not convinced by the authors' strategy of constructing an augmented test set by randomly replacing entities in the context. In the paper, they claim that \"... However, the true meaning of the entity words is fundamentally irrelevant to the relation between them given the context. Thus, the ability of an RTE model to extract relational triples should not be influenced by the authenticity of the given text....\" This might be true in some simple cases, e.g. replacing \"Joe Biden is the president of the US\" to \"A_Not_Important_Person is the president of the US\". However, entity representation does not only encode their surface forms, but also encodes the entity's information in the context, e.g. the type of entities (people vs location), the occupation of a person, especially for popular entities. If the input is invalid, I am not sure whether asking RE models to extract their relations is a fair task. For example, \"Christopher Nolan is the president of the US\" or even \"Pennsylvania is the president of the US\" are apparently invalid. \n\nThis makes their \"overlap sifted\" strategy more interesting where k% of the exposed triples are removed from the training set. The maximum percentage to be excluded is 15%. Can you potentially re-partition the training and testing data to make sure none of the triples overlaps?\n\nAnother question comes from the authors claim that \"... Therefore, for training RTE models for benchmark on overlap sifted datasets, we do not use a pretrained BERT but randomly initialize the backbone to completely deprive them of the BERT knowledge base and soak them thoroughly on the unobserved factual triples...\" I am not convinced why BERT should not be used for the experiments, as pretrained LM is good at learning contextual information for entities. At least, the authors should compare the results with and without pretrained BERT parameters.\n\nThe authors also do not compare their results with the previous state-of-the-art models on the \"overlap sifted\" data. The authors claim that they have trouble reproducing the numbers reported in the original paper, but it is still helpful to report the numbers you obtained in Table 6, just as that in Table 5.\n",
            "summary_of_the_review": "This paper clearly points out some problems in the existing RE datasets and models and proposes models to solve the problems. However, the design of their experiments needs major improvements. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper relates to the task of Relational Triple Extraction (RTE). The goal here is to promote the extraction of unseen triples from a text at the test time. It appears that the existing approaches don’t generalize well on unseen triples and part of the reason being the benchmark datasets designed so far contain test sets that comprise only 2% unseen triples. Therefore, to properly evaluate the generalization performance of baseline systems, this paper first proposes rectified datasets where the test set is augmented and the train set is sifted to emphasize unseen data. On such rectified datasets, the performance of many SOTA models reveals that previously attained high performance was due to the data bias but otherwise these SOTA models don’t generalize so well on unseen triples. This paper then proposes an approach wherein it adds noise in entities and surrounding text so as to promote generalization towards unseen triples. Experimental results on two benchmark datasets show that adding such noise does help in improving the performance.",
            "main_review": "__Strengths__\n1. An important and useful problem in the field of knowledge induction\n2. A very well written paper and quite easy to follow and understand\n3. Experiments are well planned\n\n__Weakness__\n1. Novelty is somewhat limited and the tricks are known.\n2. A few clarifications are needed (see my comments below)\n\n\n__Main Comments__\n\nOverall, I felt that this paper addresses a niche problem related to triple extraction and first proves their point that existing solutions appear to be good primarily because of data bias. In other words, most of the existing models seem to remember the triples and hence don’t perform well when extracting unseen triples. The paper proposed the idea of the augmented test sets and sifted training sets for the purpose of evaluating baselines models' performance on extracting unseen triples. The paper also proposes an idea of injecting noise in entity words and context words so as to improve the performance of the existing models on unseen triples. Table 5 and 6 show that such noise addition tricks help.  \n\nI have a few clarification questions below.\n\n1. In Table 4, the performance of the monotonically increasing (on entire set) as you go from D1 through D3 sift sets. For the case of TPLinker, it has already reached 98% (on NYT dataset). I was wondering what happens if you further go do say D4 set with even further less size of training data. Does it goes to say 99%. If it is so, this appears bit fishy  to me because just before Section 3.1, you said that this pattern could be due to less memorisation of the triples. However, if that were to be the case then shouldn’t the performance on unseen should also increase which unfortunately is not the case.   \n2. In section 4.1, where do you get the number $p_en^len$ from? Is it a hyper parameter? I could not find value for it in experiments?\n \n\n",
            "summary_of_the_review": "While this paper does justice to the aim that it sets for itself. However, I felt that the novelty of the proposed solution is somewhat limited. The trick of adding noise is a well-known idea in the literature. In that regard, I would say the key contributions of the paper gets limited to the idea of the augmented test set and sifted training set. Though the contributions of this paper are somewhat useful, I have my own reservation whether they are sufficient and novel enough to warrant a paper at the ICLR conference. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}