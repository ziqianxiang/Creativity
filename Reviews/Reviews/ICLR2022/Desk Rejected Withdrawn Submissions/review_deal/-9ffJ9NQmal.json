{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The authors propose Variational Inference for Concept Embeddings (VICE), a method to learn representations such that an odd object can be detected given a triplet (i.e. the odd-one-out task). The authors build on Sparse Positive object Similarity Embedding (SPoSE) which learns sparse, non-negative embeddings for images by placing a zero-mean Laplace prior. Claimed contributions include replacing it with a spike-and-slab Gaussian mixture prior, and a principled approach to choosing the subset of the dimensions of the learned embeddings. The empirical results show improvements over the SPoSE baseline.\n\nThe reviewers appreciated the empirical improvements over SPoSE and accept that a more informative prior might lead to improved results. However, the **motivation, novelty and significance** of the proposed method doesn’t meet the acceptance criteria for ICLR. After the rebuttal and the discussion phase the reviewers felt that the work necessitates a major revision (notwithstanding the remaining issue with limited novelty), and raised the following as the main improvement points: \n- Clarifying the motivation and significance.\n- Stronger empirical validation and generalization beyond the THINGS dataset.\n- Address the discrepancy with analyzing GMM priors, but using unimodal Gaussians in the implementation.\n- Comparing the chosen prior to other prior distributions and justifying the design choices."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents a technique for learning image embeddings from similarity data provided as odd-one-out judgments over triplets of images (i.e., ball is more similar to apple than car). The authors build on an earlier technique called SPoSE that learns sparse, non-negative embeddings for images by maximizing the probability of choosing the right pair (where this similarity is calculated using the dot-product of image embeddings) with an L1 penalty on embeddings. In this paper, the authors argue that 1) a spike and slab prior is more suited for this setting and 2) a more principled approach to choosing the number of dimensions in learned embeddings is possible.\n\nThey present an improved version of SpoSE (called VICE) that uses variational inference to learn embeddings with uncertainties associated with them and place a spike-and-slab prior on the embeddings to encourage sparsity. They then present a way to prune the set of learned dimensions. This calculates the importance of each dimension first (using the learned uncertainties and false discovery rate), then clusters these, and finds the subset of clusters that give the best validation performance.\n\nThey argue that their technique is more principled than SpOSE and show that it performs similarly to it in terms of prediction accuracy. However, in small data regime, their model outperforms SpOSE.\n\n",
            "main_review": "Overall, I think it is an interesting paper. I like the spike-and-slab prior and the variational treatment. I agree with the authors that these are better suited to this setting.\n\nMy main concern with the paper is that the motivation is not really clear. What would these embeddings be used for after learning? It looks like the authors have some applications in cognitive science in mind but I wasn't sure what these are exactly. How will these embeddings be useful in cognitive science experiments? I think a case study of where these embeddings were used would add significantly to the paper. Also, it would be nice to motivate this technique for the larger ICLR audience as well (not only cognitive scientists).\n\nMy other concern with the paper is that it's not very clear if the contributions are significant enough. \n\nVICE performs similarly to SpOSE in general, only except it outperforms SpOSE on smaller datasets. However, it wasn't clear to me how important this is. Mainly because the smallest dataset in the experiments, which has 67.500 triplets, doesn't seem very small to me from a psychology experiment perspective. (Assuming an experiment with 100 subjects, which is pretty large, perhaps you could collect 10.000 triplets). It would be very valuable to see the performance of VICE on even smaller datasets.\n\nI agree the pruning strategy is more principled than SpOSE's but it still seemed unusual to me in a couple of respects. First, it wasn't clear to me why you would cluster the dimension importance scores (and assume a max of 6 dimensions). I think more details on this in the paper would be useful. For example, are you clustering a single score per dimension, or large vectors of importance scores (one for each sample) per dimension? And why would you prune by cluster? If a cluster has features that are similar to each other, wouldn't you want to keep at least some from each cluster and prune the rest (because they are redundant)? And finally, how about just sorting the dimensions by importance and taking the top N? How well does this work compared to the clustering strategy?\n\nFinally, I'd also have liked to see comparisons to other techniques. I understand that the technique is built on SpOSE but there are many techniques that would be applicable to this problem. One class of methods that are especially suited to this setting are non-parametric Bayesian models (like Indian Buffet Process)[1] that can determine the optimal number of dimensions (without any explicit pruning strategy). One can also use a large multi-modal model like CLIP (OpenAI) to get embeddings for images and it might be interesting to see how well these do. Or if the authors think these models are not applicable, then a brief discussion of why they aren't would be valuable I think.\n\nBelow are some other minor points\n\n- In 3.5, the authors mention that VICE embeddings are passed through a rectifier to make them non-negative. This should be mentioned earlier in the paper. Also, if the embeddings are non-negative, why not use a prior that also makes this assumption (rather than a gaussian)?\n\n- In 3.2, they mention variances are initialized to be small. Why? Generally, people initialize these to be close to 1.\n\n- A minor organization suggestion. At the end of page 6, they mention separate pruning and tuning validation sets. It would be nice to mention this earlier (when talking about pruning strategy for example).\n\n[1] Navarro DJ, Griffiths TL (2008) Latent features in similarity judgments: A nonparametric Bayesian approach. Neural Computation.\n\n",
            "summary_of_the_review": "Overall, I think the contributions of this paper are unfortunately not strong enough for a clear acceptance. The paper would benefit from better motivation of the technique, more extensive experimental evaluation, and comparison to other existing models.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper address a problem in cognitive science: identify the embedding of objects in the brain's semantic space based on the subjective judgments of objects' similarity (odd-one-out task). It proposes a new model (VICE) to address a few issues of the previous model (SPoSE).\nThe new method uses a different form of prior (spike-and-slab) for the embedding, variational Bayes with Gaussian posterior, and appear to perform as well as or better than SPoSE in several metrics",
            "main_review": "The topic addressed by the paper is an interesting and important one for cognitive science.\nIt is nice that the paper pointed out a few limitations of the previous model, for example, the L1 penalization may not match the actual distribution. The experiments generally appear solid. The better performance with small training dataset as shown in Fig 3 is quite attractive.\n\nBut I also find the performance to be comparable to SPoSE in most cases and only better in a few aspects.\n\nOther major issues:\nI did not really get how 2.3.5.1 was done. Since you mentioned mu and sigma, I suppose they refer to the approximate posterior distribution in the variational bayes. But I did not get how you actually truncate it. where is the truncated Gaussian used? Is it only used for calculating p-value? Because this p-value has different interpretation than the one typically used in classical hypothesis testing, I recommend  you make a bit more explanation of what probability the p actually indicate.\nI think such hypothesis testing require the samples to be independent from each other. But because of the prior introduced, the posterior mean is biased. Does it still make sense to use such a p-value to assess non-zero embedding mean? Also, what does it mean to be \"predictive\" in this paragraph? \n\nIt seems also conceptually strange for any items with mean close to zero in all dimensions. What does the representation mean, if the p-value is high for such item over all dimensions? It sounds like the object would be treated as \"noise\"?\n\nIt was criticized that the criterion for determining how many dimensions the solution should have in SPoSE is heuristic and subjective, but I find that allowing users to choose q value in FDR to determine dimension importance also subjective. \n\n2.3.5.3 and 2.3.5.4: The choice of pruning by cluster sounds quite ad hoc. The purpose is only for reducing variance. But is there any theoretical rationale to justify this? GMM assumes each component is a latent cause. But why would dimensions of similar importance to be of the same latent cause and share the same fate of being either retained or pruned? \n\n\n\n",
            "summary_of_the_review": "I find the work generally interesting. But the advantage is not super apparent. It seems to mainly help when the training data is small.\nI find the detailed procedure a bit ad hoc. The phrasing of the procedure seems a bit confusing when it uses terms of frequentist statistics, but seem to not mean their original meanings.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a prediction model for the odd-one-out task [Zheng 2019], where the goal is to identify which pair people find to be the most similar within the triplet of pictures. The proposed approach learns a variational model to approximate the distribution over the triplet. The model is compared to SPoSE [Zheng 2019] and shown to outperform on small dataset sizes.",
            "main_review": "## Strengths\n\nThe paper clearly describes the main idea of applying variational approach to the concerned odd-one-out task. Compared to SPoSE, the proposed approach introduces a stochastic process to explicitly consider uncertainty, and switch the prior distribution to Gaussian mixture for better fitting to the data. The motivation seems clear and the approach looks reasonable.\n\n## Weaknesses\n\nThe paper has limited significance due to the narrow focus on improvement to SPoSE [Zheng 2019]. The paper seems to only concern the benchmark performance in the single dataset of [Zheng 2019], which might be simply overfitting the model to a specific dataset. Even though the results indicate successful in improving in smaller datasets (Fig 3), I do not understand how significant the result is without any generalization verification. In this sense, the paper fails to convey whether if the proposed approach has significant technical contribution.\n\nThere seems a recent work on proposing alternative benchmark.\n\n- B Roads and B Love, Enriching ImageNet With Human Similarity Judgments and Psychological Embeddings, CVPR 2021\n\nIt might be the case that there is a scientific value in improving the benchmark performance of the odd-one-out dataset [Zheng 2019], but as a reviewer, I do not have any background in judging the statement in Sec 1 saying “growing interest by cognitive scientists using SPoSE”.\n\nOn learning embedding representations, there are different attempts other than Gaussian.\n\n- L Vilnis et al, Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures, ACL 2018\n- O Ganea et al, Hyperbolic Entailment Cones for Learning Hierarchical Embeddings, ICML 2018\n\nAlthough I do not deeply understand the cognitive science context, I feel this paper misses the formal discussion in terms of representation learning in the natural language processing community. Even the simplest comparison done in [Zheng 2019] to NLP baselines (synset or NNSE) are missing in this work.\n\nThere seems somewhat recent relevant work.\n\n- A Laverghetta et al, Can Transformer Language Models Predict Psychometric Properties?, STARSEM2021\n- S Derby et al, Feature2Vec: Distributional semantic modelling of human property knowledge, EMNLP 2019",
            "summary_of_the_review": "The paper has a clear motivation to learn better concept representation than SPoSE [Zheng 2019], but that seems to result in the insignificant focus on the improvement to SPoSE at small data sizes. I feel that this work should better contextualize the scientific motivation among language literature, and consider generalization over the THINGS dataset. My initial rating is below the acceptance threshold.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No particular concern on this work.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper introduces a variational inference method for concept embedding in the odd-one-out task.  The objective is to learn representations that allow to predict the odd object from a triplet.  A variational inference problem is established to learn the representations through a Gaussian variational family with a mixture of two Gaussians as prior (spike-and-slab).  It is not clear how the selection of the variational family fits the prior.  Simultaneously, there is a dimensionality reduction procedure to improve the representations.  The experiments show improvement over the Sparse Positive object Similarity Embedding (SPoSE), and the method is also evaluated with random initialization and it shows to be stable.",
            "main_review": "Strengths:\n- The paper continues an interesting problem (odd-one-out) classification.\n- The idea of using variational inference to understand the problem is straightforward and interesting.\n\nWeaknesses:\n\n- In (8), it is not clear whether $p(\\mathcal{D}_{\\text{train}} \\mid X)$ is the same normalized similarity used in SPoSE (2).\n\n- What is the impact of using a Gaussian variational family and having a mixture of Gaussians as a prior?  My guess is that your variational distribution $q$ will end up wide and will try to encompass the mixture, or if the spike is too dominant it may collapse to it.  Why not use a mixture for the variational family as well?\n\n- What is the model you are training?  What is the definition of your encoder that produces the embeddings $X$?  Does it receives a single image or a triplet?\n\n- What is the training objective that you are using?  And what are the pieces of your model?  It is not clear whether you are training an encoder $q$ that receives a single data point and minimizes a batch of triplets w.r.t. the average of the normalized similarities (8), or if you are also using a prediction head (or classifier) for the odd-one-out task as well (12) to regularize the model.\n\n  In other words, are you training your encoder with only the similarities from the triplets, or you are using the odd-one-out classification task as well?\n\n- In Section 2.3.4, the prediction of the odd-one-out seems to be the normalized similarity pair-wise.  So, the reader will assume that the prediction is through doing the 3 pairs from the triplet and selecting the one with minimum similarity.  However, in Section 3.3.1, it seems that the prediction occurs as a one-hot-vector prediction.  Which one is it?  The usage of your model for the prediction task must be clearly stated and defined.\n\n- Why is the truncation of the dimensions needed?  One could still traverse the dimensions and see clusters of points through them. \n\n- Similar to the previous comment, the interpretability of the dimensions explores the maximum values to check for clusters in this neighborhood.  However, wouldn't it be interesting to see the traversals through the dimension to see how the samples change or not?  \n\n  Isn't it bias to look only at the clump of samples from the high values in the dimension (assuming that is the weight)?  One will expect to see similar clusters since the embeddings are close together for the maximum values. Similar to other parameters, why not to do an ablation study over the truncation too?\n\n- I found hard to follow the process of doing the dimensionality reduction.  It seems to be summarized, and I think its incorporation to the model impacts the final performance.  I suggest to improve the description and details of how this process works.  If space is needed, instead of reproducing SPoSE (Section 2.2), consider removing it.\n\n- How one can know whether the improvement of the representations come from the variational representation or from the different steps within the dimensionality reduction process?  I would recommend to do an ablation study to understand the contribution of the different steps on the dimensionality reduction to understand their impact on the final representations.",
            "summary_of_the_review": "The proposal extends SPoSE within a variational framework.  The proposal uses a Gaussian variational family with a mixture of Gaussians prior (the limitations of this selection or its impact is not discussed).  The paper defines a normalized similarity within triplets as a regularization on top of the KL-divergence for the variational inference problem.  The model and the training objective are not clearly defined.\n\nThe results show improvement over the SPoSE baseline.  However, the different components and parts of the proposal are not validated.  Thus, it is hard to understand where the benefit is coming from (e.g., either the variational model, or the different post-processing steps for dimensionality reduction).  The interpretabiltiy, while interesting, seems biased and the way the dimensions were selected may have influenced the observed results. \n\nOverall, the method's presentation needs improvement.  In particular, the definition of the networks used and the objectives used to train it (i.e., the explicit loss functions).  The different components of the proposal should be validated and an extensive ablation study is needed to understand where the improvements are coming from. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}