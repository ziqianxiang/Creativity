{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes Scale-Invariant Teaching approach for semi-supervised object detection. More specifically, the proposed approach enforces consistency between predictions from different image scales, uses soft labels w/ re-weighting to avoid false negatives and balance positive and negative examples, and uses exponential moving average to update the teacher model. Experiments on PASCAL VOC and COCO show that the proposed approach obtains very promising semi-supervised object detection results.",
            "main_review": "**Strengths**\n1. The proposed re-weighting strategy is interesting.\n2. The proposed approach obtains promising results.\n\n**Weaknesses**\n1. Novelty.\nThe novelty of the proposed approach is limited. 1) It's not new to use soft labels and exponential moving average for semi-supervised object detection (Tang et al., 2021) (Liu et al., 2021). 2) The proposed scale consistency can be seen as another kind of strong augmentation to train the student model. In addition, it's not new to enforce consistency between different image scales for object detection w/ limited labels [a] (although the techinical details of how to do scale consistency are not exactly the same).\n2. Experiments. \nThis paper needs more experiments to show the effectiveness of the proposed approach. 1) It would be interesting to show the results of removing the proposed scale consistency approach and adding image scaling as a kind of strong augmentation for student model training. 2) It would be interesting to show results of more object detections (like RetinaNet (Lin et al., 2017b), DETR [b], Deformable DETR [c], etc.) with more CNN backbones (like ResNeXt w/ and w/o deformable convolution).\n\n[a] Comprehensive Attention Self-Distillation for Weakly-Supervised Object Detection\n\n[b] End-to-End Object Detection with Transformers\n\n[c] Deformable Transformers for End-to-End Object Detection",
            "summary_of_the_review": "In summary, as stated in **Weaknesses** in the **Main Review** part, the novelty of the proposed approach is limited and more experiments are needed. Therefore, I would like to give a weak reject to this paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposed a method for the semi-supervised object detection task. The authors proposed a method that compromises several (relatively small) changes compared to the existing work: (1) leveraging the fact that we have multiple-scale features in the FPN and enforce consistency regularization between features across scales, (2) using soft pseudo-labels instead of hard pseudo-labels, and (3) re-weight the KL Divergence by the sample numbers (Sec. 3.3, inspired by an existing work). Combining these key changes, the authors demonstrate SoTA performance across existing benchmark datasets (though missing some experimental settings). ",
            "main_review": "**[Pros]**\n- comprehensive ablation study on the proposed method showing how each component impact the model's performance\n- The proposed method is simple \n\n**[Cons]** \n- I am slightly concerned about the novelty of the proposed method in leveraging scale-invariant features.  While this has not been done before for semi-supervised object detection, it's not a surprise method and its performance improvement is somewhat expected. I think this could be a paper for a performance-targeted conference, but probably not to the standard of an ICLR paper. \n- I am not sure if the proposed method can be easily extended to other detectors (see comments below)\n\n**[Questions]** \n- The authors claim that the proposed scale_invariant consistency regularization method can be easily extended to other detectors. While the authors use FPN as an example to illustrate the proposed method in Sec. 3.1, it's not clear to me if the same method can be easily extended to other detectors, e.g., SSD, YOLO, DETR, etc.\n- In Sec. 3.2, what exactly is the weight consistency regularization? From the equation (3) and the description in Sec. 3.2, it reads like the traditional consistency loss, but instead of using hard pseudo-labels like the one in Unbiased Teacher, the authors proposed to use soft pseudo-labels. They also use MSE for regression loss, which is pretty common for applying consistency regularization for regression tasks. \n- In Table 2, is there a particular reason why the authors decided to not compare the performance when data percent is at 0.5 or 1%?\n\n**[Minor comments]**\n- \"weight consistency regularization for aligning the name of the unsupervised loss\". Is this a typo for using \"name\" in this sentence? \n- \"which doesn't torture the geometric information\". I am not sure if I understand this sentence. Did the author mean to say \"contain the geometric information\"?\n- In Table 2, when Data percent is 100%, UBT's \"Iter\" should be 270k instead of 360k. ",
            "summary_of_the_review": "Given the amount of performance improvement as well as the technical novelty of the work, I am rating this work as \"marginally below the acceptance threshold\". The main two concerns for me are (1) the novelty of the method and (2) whether if we can actually easily adapt the proposed method to other object detectors.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed a new semi-supervised method that addresses the problems of existing self-training based methods. It comes with three key components: scale consistency regularization, weight consistency regularization with soft target and re-weighting strategy.\n",
            "main_review": "Strengths\n- The proposed semi-supervised method tries to address different problems via different strategies at the same time. Scale consistency regularization plays an important role in detecting objects with different scales. Using soft targets instead of hard targets makes is threshold free and simple to apply.\n- The experimental settings are well designed and results are well oraganized.\n\nWeaknesses\n- In the Introduction section, it says that the scale of objects varies smaller for classification tasks than object detection. This is true for some datasets, e.g. ImageNet and COCO. However, this is not a valid statement. We can still perform (multi-label) classification tasks on detection datasets. Thus scale invariance is also equally important for classification. It'll be interesting to see if the same scale invariance teaching can benefit classification tasks (at least some analysis).\n- The writing needs some improvement. \n- Notations needs to be defined before using. For example, SUP in Table 2, Baseline in Table 3.\n- Re-weighting strategy is used for dealing with class imbalance. However, the paper does not compare it with other methods to show the efficacy of the proposed method.\n- No geometric augmentations are used in all experiments. It'll be interesting to see how such augmentations can affect the performance. \n",
            "summary_of_the_review": "The proposed method works well but the novelty is not high enough. It also needs some improvement in both writing and analysis. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes improvements for Semi-Supervised Object Detection (SS-OD) methods. The authors suggest to use three mechanisms to extend current SS-OD frameworks. Specifically, they extend UBT (Liu et al., 2021) with three main contributions:\n\n1. They suggest to downsample the weakly augmented image by the same ratio of downscaled FPN levels. Then run inference on both the strongly augmented image and downsized image by the student. Finally, they apply consistency loss between the corresponding FPN layers of images inferred in the different scales. The loss used is the KL loss between the class probabilities, and L2 for the box regression,\n \n2. They use soft labels from the teacher similar to Humble Teacher (Tang et al., 2021). The authors claim that differently from the Humble Teacher work, they balance the background and foreground labels with the following gradient balancing mechanism. Another difference is that they do not ensemble the teacher model predictions.\n\n3. They use a mechanism to balance the gradients from easy and hard examples inspired by the work of Gradient Harmonizing Mechanism (Li et al., 2019). They use this mechanism only for classification of the unlabeled objects. This mechanism is similar to Focal-Loss work and aims to tackle the inherent foreground-background class imbalance in object detection.\n\nThe method presented is comprised of a single stage training - a single training phase for both the labeled and unlabeled data with two branches, one for each type of data.\n\nThe authors conducted experiments on Faster-RCNN with Res50 + FPN backbone.",
            "main_review": "### strengths\n\nThe authors present novel scale-invariance consistency loss for semi-supervised object detection. This component alone improves the 10% labeled data scenario by about 3 mAP on COCO (23.86- > 26.80). With the additional soft-labels from Humble Teacher (Tang et al., 2021) and Gradient Harmonizing Mechanism (Li et al., 2019) they surpass the current SOTA by a margin (5%-> +2.6, 10%-> +3.5, 100%->+1.2).\n\nThe authors shares important experimental details such as the update and lr decaying strategy for the EMA teacher model. \n\n### weaknesses\n\nThe documentation of experiments and method explanation need to be improved. For example:\n- Section 1 Paragraph 2: _\"Fig. 1a, the standard variance of the scale of instances in MS-COCO is 188.4, while that of ImageNet is 56.7\"_ what is the unit of measure? Also, the images were resized differently therefore the statistics are incomparable.\n- Section 3.1 _\"Towards handling the large scale variation, the s is selected from {1, 2, 3, 4}, which also matches the sizes of feature maps in FPN and the label assignment rules\"_ . It is not clear if the method uniformly samples from the set {1,2,3,4} or that a single number is used.  Later, in sec 4.3 it says that s=2 is the optimal scale. It is not clear which value was used for the experiments in table 1.\n- Section 4.2  _\"Fig. 1c shows that the discordance between different sizes is alleviated\"_ the setup is not clear, was it measured during the training phase? or did you augment the validation images to measure the discrepancy.\n- Section 4.3 _\"Fig. 5 shows that the fraction of valid instances...\"_ the term \"valid instances\" was not defined. It seems like an important and critical quantity that was not defined explained or further discussed. \n\n\n\n",
            "summary_of_the_review": "The ideas and presented in the paper are novel and present a decent improvement on current art with supportive experimentation. The paper present both novel and incremental improvements for extending current SS-OD approaches.\nHowever, the writing; method details; documentation and explanations need to be improved as was detailed in main review.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}