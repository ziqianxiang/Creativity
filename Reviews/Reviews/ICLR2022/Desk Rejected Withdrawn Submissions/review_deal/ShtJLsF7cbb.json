{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a novel GNN-based approach called TR-GAT for temporal knowledge graph embedding (TKGE).  It considers timestamps to be additional properties of entity links. Furthermore, it employs a time-aware self-attention mechanism to assign different weights to various entities in the neighborhood. The proposed method adapts well to TKGE tasks, and the authors evaluate it using temporal knowledge graph completion and time-aware entity alignment. The results of the experiments show that it is effective.",
            "main_review": "**Please provide a viable explanation for the apparent similarities with the  ENMLP-2021 paper ''Time-aware Graph Neural Network for Entity Alignment between Temporal Knowledge Graphs''.**\n\nThe proceedings of EMNLP 2021 are now available online. I just went through the published papers and discovered one with duplicate content with this submission:\n\n```\n@inproceedings{xu-etal-2021-time,\n    title = \"Time-aware Graph Neural Network for Entity Alignment between Temporal Knowledge Graphs\",\n    author = \"Xu, Chengjin  and Su, Fenglong  and Lehmann, Jens\",\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.emnlp-main.709\",\n    pages = \"8999--9010\",\n}\n```\nThe authors of the two papers both claim that they create five datasets for temporal entity alignment. But in fact, the datasets they used are the same. Please see Table 1 in the EMNLP paper and Table 2 in this submission for comparison.\n\nWhile in this submission, the authors also claim that \"To the best of our knowledge, there is no previous literature to perform entity alignment between KGs using a time-aware embedding-based approach\". \n\nThe experiments settings for temporal entity alignment in the two papers are also very similar, e,g., Table 2 in EMNLP vs. Table 4 in this submission, Figure 3 in EMNLP vs. Figure 4 in this submission, Table 4 in EMNLP vs. Table 5 in this submission, and Figure 4 in EMNLP vs. Figure 3 in this submission.\n\n---\n\nPros:\n\n- Unlike existing temporal GNN models that discretize temporal graphs into multiple snapshots and employ a combination of the GNN and recurrent architectures, the proposed method attaches time information into triples to form quadruples, which is more efficient for training.\n\n- This paper argues that time information is important for entity alignment in temporal knowledge graphs. The authors state that the proposed method is the first one to integrate time information into entity alignment approaches. Furthermore, they create several time-aware entity alignment datasets, which could be useful for future work.\n\nCons:\n\n- The authors only compare TR-GAT with baselines that do not use time information in the time-aware entity alignment evaluation. I believe that they should clarify whether it is feasible to associate time information with these baselines. If some baselines can be easily adjusted to incorporate time information, it is preferable to compare TR-GAT with these variants.\n\nQuestions and suggestions:\n\n- The best result of ICEWS14 on Hits@10 in Table 3 is not bolded.\n\n- TNTComplEx and ChronoR appear to have competitive results in Table 3 on ICEWS05-15 and YAGO15K. I'm curious if TR-GAT has any other obvious advantages (e.g., running time) over these baselines.\n\n- I would like to know why only the results evaluated on four datasets are provided in Table 4, as the authors create five time-aware entity alignment datasets. Although the results on YAGO-WIKI20K are shown in Table 5, they do not contain baselines. ",
            "summary_of_the_review": "The paper presents a new embedding model for the temporal knowledge graph. It also proposes a new setting and dataset for entity alignment, i.e., temporal entity alignment. Overall, I think the paper makes some new contributions although I have some questions regarding experiments.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper studies the problem of representation learning on KG enhanced by temporal information (or when the relations are time-aware). The authors take the GNN-based approach, and employ the graph attentional network to operate on the temporal neighborhoods defined by the KG. The proposed approach is examined via the link prediction and entity alignment tasks.",
            "main_review": "The time-aware nature of particular relationships in existing knowledge bases pose unique challenges to KG representation learning. Although there will be conceptual debates on how to perceive the connection between the time-aware relationship and the composition of timestamp + relationship, I am willing to follow the authors' argument and view the problem via the temporal graph representation learning task. However, this leads to several major concerns regarding the claims made in the paper, in particular,\n\nConcern 1. The authors claim throughout the paper that existing GNN discretize temporal graphs into static snapshots, and advertise their solution based on this critique.  However, this is very misleading: there are many existing solutions that directly handle the continuous-time information in temporal graphs, e.g. [1,2,3]. As a matter of fact, the referenced work all use some sort of attention mechanism with principled time encodings. I believe the solution in this paper is a special case of the TGAT approach in [1], which leads to the second concern as below.\n\nConcern 2. The novelty of the proposed approach is very limited compared with the state-of-the-art temporal GNN solutions. While there are obviously more advanced techniques to handle temporal information in graphs, I would encourage the authors to at least experiment with such as TGAT for the proposed KG embedding framework. Also, it appears that the proposed approach is a straightforward adaption of GAT to the temporal KG setting, so the technical contribution is incremental at most.\n\nFinally, the experimental validations are lacking in:\n\n1. Justifying the learnt attention weights in terms of how they assist representation learning on KG;\n2. Showcase how the proposed approach capture and characterize the timestamp signals. \n\n[1]. D Xu. Inductive Representation Learning on Temporal Graphs, ICLR'20\n[2]. E Rossi. Temporal graph networks for deep learning on dynamic graphs, Arxiv'20\n[3]. Y Wang. Inductive Representation Learning in Temporal Networks via Causal Anonymous Walks, ICLR'21",
            "summary_of_the_review": "The authors make inaccurate statements regarding the current temporal GNN methods, and the novelty and contribution of the proposed solution is very limited as it is a straightforward adaptation of GAT. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper presents a representation learning approach for temporal knowledge graphs. It focuses on two tasks of knowledge graph completion and entity alignment. The approach incorporates self-attention on both relations and time stamps. The approach doesn’t need to discretize a temporal graph into multiple static graphs. The paper considers this fact as one of the novelty aspects of the work.",
            "main_review": "The paper is well-written and well-structured. It is easy to follow.\nNovelty: There are some other approaches that do not discretize the dynamic graph to multiple static graphs such as [1,2]. It is better to have a detail comparison with these kinds of methods as well.\n\nModel: \nHow do you initialize the time embedding vectors? do you assign random vectors to them?\nIf so, how do you deal with cold-start problems with respect to time? What happens if we encounter a new time in evaluation?\n\nEquation (2): “Lrij and Lτij denote the sets of relations and time steps in the links from e_j to e_i, respectively”. How many relations there exists from entity i to j in your dataset that you consider a set for that? The same question for the time steps between i to j?\n\nExperiments:\nI am not fully convinced why you didn’t consider the textual information in entities and relations for entity alignment task. For example, in figure 1, if you consider the textual info of entities, then obviously “Boris Johnson” and “David Cameron” won’t be aligned. However, if you ignore the textual information, then obviously the time stamps would be very effective in distinguishing these entities from each other. I think ignoring this info from Entity linking task and concluding the effectiveness of temporal info causes a great concern about the validity of the task.\n\n[1] Towards Temporal Knowledge Graph Embeddings with Arbitrary Time Precision, CIKM 2020\n[2] Encoding Temporal Information for Time-Aware Link Prediction , EMNLP 2016\n",
            "summary_of_the_review": "I think the novelty of the approach is not clear in comparison with related work.\nAlso, the configuration of entity linking task doesn't seem reasonable.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a new temporal knowledge graph embedding model using time-aware entity representation and graph attention network. The proposed TR-GAT model can perform TKG completion as well as entity alignment tasks. The model is compared with several baseline models on public benchmark datasets for both tasks and shows some improvement especially on the entity alignment task.",
            "main_review": "Strengths:\nThe TR-GAT model can perform both TKG completion and entity alignment tasks which makes the model more appealing compared with single task models. The model shows good improvement on entity alignment task over baseline models on public datasets. The model can also handle missing temporal information which is a more realistic setting for real-world data. Overall, the model is straightforward and easy to understand. \n\nWeaknesses:\n1. There's a lack of detailed explanation of why a particular model component is chosen and not the alternatives, sometimes an intuitive example or reason behind the choice would be helpful, or if there was an implementation of an alternative but later was determined inferior, there should be a description of it. For example, why using attention as the aggregator in graph neural network? have you tried other gnn architectures? how are their performance?\n2. The example in fig1 seems a bit weird since it's clear from the entity labels that \"Boris Johnson\" and \"David Cameron\" are different entities despite them having the same neighbors or structures in the graph. Knowledge graphs are not just a graph, they also have labels, entities types, schemas, and other information associated with them that can help align and differentiate entities, temporal information is just another information that can help. So even for models without using temporal information, they may still be able to differentiate  \"Boris Johnson\" and \"David Cameron\" by simply looking into their labels. Perhaps, in the entity alignment baseline models, there should be a model that uses this kind of label or textual information. Also, all the baseline models in EA do not use time information, this seems to be an unfair comparison as the improvement might simply come from this introduction of additional (temporal) information. So it would be better if there could be a baseline model that uses temporal information and another baseline model that uses another additional information (such as textual or label information) to 1) see whether TR-GAT can outperform other temporal kge models in EA task and 2) understand whether temporal info is more useful or textual (label) info is more useful in EA task.\n3. Wikidata12k is also a popular benchmark dataset for tkg completion and it is missing in the experiment\n4. A popular task in tkg models and related papers is time prediction task which predictions the time interval of a particular statement (s, r, o). For example this paper (https://arxiv.org/abs/2005.05035). This task seems to be missing in the paper. \n5. It seems that the EA is possible because two KGs are trained simultaneously. In this case, can this (training 2 KGs together and having an additional margin rank loss) be applied to existing TKG embedding models and then create additional baseline models? How would these additional baselines perform compared with TR-GAT?\n6. The TR-GAT models doesn't not seem to beat existing models on ICEWS05-15 dataset though.",
            "summary_of_the_review": "The model is a straightforward extension of GAT model and is compared with existing baseline in two tasks, however, there are still a lot of questions and gaps as mentioned in the weaknesses.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}