{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "LATTICE QUANTIZATION provides a novel way to quantize weights.  This paper relies on weight correlation inside convolution filters so we can select better basis to represent weights (so called \"Lattice quantization\").  Firstly authors quantize numbers to lattice and then find an good lattice.  Due to weight correlation, a good lattice quantization can be more efficient than conventional uniform number representation (Cubic LatticeQ).  \n\nIn the same bit precision, Lattice quantization achieved less quantization error in convolution layers.  Less quantization in Lattice quantization also translates to better Neural Network accuracy (compared to conventional scalar uniform quantization).  \n\n",
            "main_review": "Some suggestions:\n\n\n(1)\tAlthough the idea is interesting, compared to other PTQ, experimental results are not impressive.  This paper uses on-line activation quantization, which helps reducing activation quantization error, but not easy to be implemented in GPUs or ASIC accelerators.   In MB2 W4A4 condition, BRECQ (https://arxiv.org/pdf/2102.05426.pdf) could achieve 66.57%; this work just achieved 46% even with the help of “on-fly” activation quantizer. Beside interesting quantization ideas, this work should demonstrate stronger experimental results, compared to other PTQ.\n\n\n(2)\tThe idea relies on convolution filter’s weight correlation so the applications are limited.  This method cannot be directly applied to recent NN architectures such as Transformer, BERT etc. It will be better to extend this work to a wider range of applications.\n\n\n(3)\tOther issues: writing in section 4.1 is not easy to follow. I cannot understand the reason: (f1b1 + f2b2 + f3b3), (f4b1 + f5b2 + f6b3) and (f7b1 + f8b2 + f9b3).  Better explain about the core idea “Lattice quantization” is needed.  \n",
            "summary_of_the_review": "In general, the lattice quantization idea is novel and interesting.  However the experimental results are not strong.  Theory and paper writing are not clear.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper explores the post-training inference quantization. Observing that the weight distributions between filters are often highly correlated, the authors propose a new quantizer based on algorithmic theory of lattices for the quantization of weight tensors. The method is verified on popular vision models with precision down to 4/3 bits.",
            "main_review": "I think the paper proposed an interesting and novel algorithm for the quantization of weight distribution. The results seem to support the approach. However, I find that the paper lacks technical details when describing the algorithm and the results, which makes the paper difficult to follow. \n\nThere is a list of questions I would like to be clarified.\n\n1.\tOn Fig. 2, why all the distributions have the same standard deviation of 0.01? The distribution seems to be quite different just from the printout.\n\n2.\t“The basis of the lattice is quantized with a uniform scalar quantizer”. Could the author explain some details on this part, such as what precision is used? Is the uniform quantizer is just a min/max quantizer?\n\n3.\tThe quantization set is the product of the base and integers from the bitwidth. Since base is quantized separately, what is the final precision after the product?\n\n4.\tIn the Algorithm 3, line 5, how Wq is computed. Could the authors add more details?\n\n5.\tThe description of the optimization strategies contains little information. Could the author provide details on the final optimization method used, i.e. the random search with restarts. Could they define restart? Also provide more information on why the optimization is difficult for this algorithm and why SGD does not work?\n\n6.\tThe authors claim the dimension of the basis is 3 for 3x3 layers and 2 for 1x1 layers. Could the author provide some results for the reader to understand more on the algorithm. In addition, why the first layer uses dimension of 1?\n\n7.\tOn Fig. 3, is the Cubic LatticeQ is just a min/max quantizer? The Vertical axis is MCE, what about MSE? The authors claim that MCE as loss is slightly better than MSE. Could they provide some results as evidence?\n\n8.\tFig. 4 is confusing to me. Could the author explain the plot in more details, such as what the axis are? How should the readers read and compare the plots?\n\n9.\tAnother major concern is that this paper only compared the results with two previous papers. There are some significant progresses recently in PTQ, for example (https://arxiv.org/abs/2102.05426), which shows much better low-precision accuracy than the results show in this paper. The author should provide more thorough comparison with SOTA results\n\n10.\tThere are several typos in the manuscripts.\n",
            "summary_of_the_review": "In summary, I think the paper proposes an interesting idea, however, needs to put more effort to present and explain the idea. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new post-training quantization method for DCNNs using vector quantization format w/o additional gradient updates. The proposed method calibrates the quantized model using sampled dataset only for activations. For the weights, lattice quantization, which seems to be a sort of vector quantization methods, is used without any data for the calibration/re-training step. For the activations, a small dataset is used for conventional activation quantization. This paper also includes the outperformed results compared to previous methods, AdaQuant and OMSE. ",
            "main_review": "### Strength\n- This paper proposes a quantization method based on a simple insight, “a quantizer shaped as a parallelogram is more efficient than uniform quantizers shaped as a square”. It could be helpful for many researchers in this compression area.\n- This paper proposes a new quantization method, a generalized form of uniform quantization. This method can express the original weights better with a few additional parameters, basis of lattices. Additionally, it could be solved by post-training step w/o dataset. \n\n### Weakness\n- This reviewer has a strong concern about the listed levels of Introduction. It is hard for this reviewer to agree with the authors’ criteria, which is more specific than Nagel et al.’s criteria. It seems to be distinguished in the aspect of usages of dataset and needs of gradient update. But, this reviewer thinks it is hard to distinguish levels 1a, 2a, and 2b. If sampled dataset should be needed for activation even we don’t use dataset for the weight quantization, it should be categorized as level 2 in this reviewer’s opinion.\n- In that sense, this reviewer thinks BrecQ should be included for the experimental target of this paper. BrecQ showed extensive experimental results for PTQ and uniform quantization. In addition, BrecQ showed a few results w/o dataset, which could be comparable to this paper’s results, but unfortunately, the results shown in the BrecQ paper are better than this paper’s results.\n- The new quantization method is used for the weight quantization w/o sampled data. Then, why is there no result on only weight-quantization (i.e. W8A32)? Because there are only results by combined methods, it is hard to evaluate the novelty of lattice quantization.\n- This reviewer is also curious on the acceleration of lattice quantization. Each weight of a kernel has different scaling factors. How could it be accelerated by any instructions of a certain H/W (CPU, GPU, or custom ASIC/FPGA)? For instance, BNNs can eliminate the number of multiplications in custom H/Ws and INT8-quantized networks can be accelerated by INT8 instructions because we can reformulate the multiplications between weights and activations with common scaling factors. In this case, it should be different, but there is no analysis or mention about calculation or acceleration. Should the quantized weights be dequantized right after accessing DRAM? Then, the network can be accelerated? Otherwise, there are other methods for lattice quantization? ICLR is not a H/W conference, but we need to evaluate the feasibility of model compression methods in the aspect of acceleration. At least, it should be described how the quantized weights and activations can be calculated efficiently in the aspect of algorithm. \n\n### Minor\n- Before Section 4: paralellogram → parallelogram\n- The end of page 1 : developper → developer\n- The first sentence of Section 7 : flexiblity → flexibility \n\n",
            "summary_of_the_review": "This reviewer's opinion can be summarized as follows:\n- Level criteria in Introduction section should be modified.\n- Other PTQ methods including BrecQ should be compared.\n- The proposed method should be evaluated in the aspect of acceleration.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}