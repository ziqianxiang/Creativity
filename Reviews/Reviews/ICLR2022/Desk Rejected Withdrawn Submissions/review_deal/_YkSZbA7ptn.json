{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposed structure optimization for graph classification. It first decode the structures underlying a graph by minimizing the structural entropy of the encoding trees. Then it designs a new feature combination scheme for the encoding trees and use it to develop a new tree kernel method and convolutional network method for graph classification. ",
            "main_review": "Strength:\n1.\tThe structure optimization and the labeling scheme for the “encoding trees” seem new in the context of graph classification. The paper also used this method to design both a new tree kernel and a new convolutional network.\n2.\tThe empirical performance is good on most datasets. \n\nWeakness:\n1.\tThe writing needs a lot of improvement. Many of the concepts or notations are not explained. For example, what do “g_\\alpha” and “vol(\\alpha)” mean? What is an “encoding tree”(I believe it is not a common terminology)? Why can the encoding tree be used a tree kernel? Other than complexity, what is the theoretic advantage of using these encoding trees?\n2.\tStructural optimization seems one of the main components and it has been emphasized several times. However, it seems the optimization algorithm is directly from some previous works. That is a little bit confusing and reduces the contribution.\n3.\tFrom my understanding, the advantage over WL-kernel is mainly lower complexity, but compared to GIN or other GNNs, the complexity is higher. That is also not convincing enough. Of course, the performance is superior, so I do not see it as a major problem, but I would like to see more explanations of the motivation and advantages. \n4.\tIf we do not optimize the structural entropy but just use a random encoding tree, how does it perform? I think the authors need this ablation study to demonstrate the necessity of structural optimization.\n",
            "summary_of_the_review": "The paper has some novelty but lacks of clarification. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper argues that the increase in model complexity may lead to better result, and there is no clear answer for making performance better while simplifying the model. For that reason, they propose a method that makes graph classification performance better while simplifying the learning process by changing the structure of graph to tree.\n\nThey first transform the graph to tree structure by minimizing the structural entropy of encoded tree, as a result, tree will contain the key structure of the graph (structural optimization). Then, they transfer the features of leaf nodes to root nodes as a novel feature combination scheme. Lastly, they apply tree kernel and convolutional network to classify the graph.\n\nThey achieve better performance than some old baselines with lower computational effort.\n\nThe paper’s main contributions are as follow:\n1)\tThey present the novel feature combination scheme.\n2)\tThey show structural optimization works better for graph classification task.\n3)\tThey develop novel tree kernel and convolutional network, outperforming some graph classification baselines.\n",
            "main_review": "Strengths:\n-\tThey analyze their algorithm’s theoretical complexities very well, and they are clear to the readers.\n-\tTheir new method/architecture intuitively makes sense.\n\nWeaknesses:\n-\tThe narrative and the start of the paper are quite weak. The reader won’t be motivated too much while reading. The authors need to sell the paper in a better way. \n-\tThe novelty is questionable, it is not quite clear if structural optimization method is new or not. However, it is in the paper title, which makes the narrative even weaker.\n-\tThe empirical analysis of the paper is super weak. The paper chooses very old graph classification baselines, with the newest one from 2018 (GIN). The authors should explore more recent baselines. Besides, there are many typos and mistakes in the experiment section and the result table. It is poorly written. Also test running times of the baselines should be given as well.\n-\tEven though the authors didn’t use the recent baselines, the table results are also questionable. The code is available online, but the paper suggests it is not available, so I assume the code is not the recent algorithm. The reader won’t judge the results coming from the code; however, the available code produces the following results for 3 different datasets. This weakness is written to help authors to realize if they did some mistake in implementation or gathering results. It won’t affect the final recommendation. Please run your experiments at least 100 times. (10 times x 10 fold cross validation)\no\tPTC: 0.6169\no\tMUTAG: 0.8556\no\tIMDBBINARY: 0.7530\n",
            "summary_of_the_review": "As a summary, the positives of the paper: it provides a new method that simplifies the graph and learns better representations of it, with a reasonable amount of theoretical analysis. However, the empirical side of the paper is very poor, and it needs further justification and improvements.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors proposed a method aiming to improve graph classification performance while simplifying the model learning process. Specifically, they first used structural optimization to transform the original datasets to encoding trees. Then they proposed the tree kernel, which derives from the canonical Weisfeiler-Lehman subtree kernel, followed by a convolution network (ETL). The authors validated the proposed model through experiments and compared with several graph classification benchmarks.",
            "main_review": "The authors proposed an interesting idea to transform the original data based on the structural entropy of the graph. The tree kernel and ETL is then proposed based on the transformed data. One of the major part here, section 3.1 structural optimization, is not written very clearly. In section 3.1, equation (1) is confusing. What is \\alpha here, and what is the vol() function? It would also be much more intuitive if the authors can show some examples of transformation. E.g. given some input graphs, what would be the transformed tree structure. Also, how is the tree depths decided here?\n\nI don't understand the proof for Theorem 2. And what is \"a naive application\" here? \n\nIn section 3.3, the \"convolutional network\" ETL concatenates layer representations across all heights/layers again, despite that the previous note-to-root propagation has already captured the information. Is would be good to include some ablation study to see the effects of this concatenation (versus only using the root).\n\nOther comments:\n\nIn section 3.2, before theorem 1. The authors mentioned that the tree kernel \"is also designed to count the number of common labels in two encoding trees\". Is the count used in anywhere in the model?",
            "summary_of_the_review": "The authors proposed an interesting idea to simplify the initial input graph data through structural entropy optimization. The tree kernel follows as a Weisfeiler-Lehman algorithm adaption for this particular tree structure. The ETL part need some more elaboration, e.g. the design choices and the effects to model performance. It is indeed unexpected to see this simpler algorithm performs better than GNNs on several datasets. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes the use of a tree representation of a graph for graph classification. The tree is one that minimizes the so-called \"structural entropy\" of the given graph. Based on this tree, the usual WL kernel on graphs can be straightforwardly extended to the definition of a kernel for the trees, and standard neural networks can also be applied on the trees. The authors argue that the tree is a simpler object than the given graph, but it yields comparable or even better classification performance.",
            "main_review": "This paper tackles the graph representation learning problem from a refreshing angle, by using a certain \"encoding tree\" surrogate that appears to have a lower complexity but nonetheless performs equally well. This angle is interesting and the tree surrogate naturally can be used under a kernel machine or under a neural network.\n\nHowever, a critical weakness of the paper is that the key technical details are missing. This issue causes a concern that the paper is hardly self-contained and a concern that details are unverifiable. The missing details include:\n\n- The definition of structural entropy: meanings of the notations in (1) are unclear.\n\n- The method to optimize structural entropy: it appears that the method comes from an unrefereed paper and its correctness is unknown.\n\nThese missing details are however the most important information for a reader to appreciate the contribution of the work. The use of the resulting tree, either under a kernel machine or under a neural network, is fairly straightforward. For the kernel machine, the kernel definition (definition 1) is a straightforward extension of the WL kernel for graphs; for the neural network (eq. 4), it is in fact a recursive neural network existent for several decades.\n\nOther comments:\n\n- The paper would be more readable if the authors explain in words what is the cause of the difference in time complexity between the proposed WL-ET kernel and the standard WL kernel. The current proofs (for Theorem 1 and 2) are too cryptic, and the key regarding the number of iterations is never elaborated. Moreover, the proofs mention a concept \"binary encoding tree\", which is never defined and whose link to a usual encoding tree is unclear.\n\n- Theorems 1 and 2 assert that \"(the method) is the simplest method\". This assertion has no ground and it is not a mathematical statement.",
            "summary_of_the_review": "This paper tackles graph representation learning from a refreshing angle, attempting the reduce the complexity of model rather than increasing it. However, the presentation lacks critical technical contents for comprehension, implementation, and verification of correctness.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}