{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents an easy-to-implement but effective approach to improve the out-of-distribution performance of CLIP models. Some of the results **may** seem to be intuitive from an ensemble learning perspective but the way that this could work as a weight-space ensemble instead of output-space ensemble definitely improves the practicality of this approach in a real-world setting. The authors also extensively conducted different experiments both on different kinds of ID / OOD dataset pairs (w/ and w/o ImageNet-based ID datasets), combining zero-shot CLIP with different versions of CLIP (LC or E2E) or non-CLIP classifiers (different versions of EfficientNet) as well as CLIP under different training budget to demonstrate the effectiveness of this approach probably holds across different scenarios.",
            "main_review": "Strengths\n\n* Very extensive experiments to demonstrate the effectiveness of the approach on a variety of datasets\n* Methodology is sound and easy to follow - main approach is also very practical to be used in real-world settings\n\nWeaknesses\n\n* [Not actionable - just a general comment] It's a bit tricky to determine the overall novelty of this approach - it seems that existing approaches like SWA (authors already cited these work) already strongly suggests ideas like this could work.\n* \"OOD\" itself is an overloaded term - it may refer to detecting novel classes, irrelevant input or distribution shift (https://arxiv.org/abs/2101.02447). This paper only discusses the case of distribution shift. It would be interesting to see if the proposed approach could also work on other OOD related scenarios.\n* It's not clear to me how the optimal \\alpha is determined in this paper across a number of different OOD datasets. Is it based on avg OOD performance? If that's the case it seems it's not really possible to choose alpha when using this. It might be good to clarify this part out - because for real world use cases it's possible users may need to tune alpha based on already well fine-tuned models (like what Appendix B.6 shown) and as the paper shows, fixed \\alpha values (like 0.4) (as suggested in Section 4) may hurt the ID performance a lot so the approach of parameter tuning can be important here.\n\n",
            "summary_of_the_review": "Similar to the section above.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a novel fine-tuning strategy WiSE-FT using pretrained CLIP model on downstream tasks to reduce performance drop in out-of-distribution (OOD) generalization. The proposed method simply interpolates the weights of pretrained model and that of the finetuned model. Extensive experiments on ImageNet and its five out-of-distribution datasets show that compared to standard finetuning, WiSE-FT improves the OOD accuracy by 2% to 10% while increasing in-distribution (ID) accuracy by about 1%. On six further distribution shifts, Wise-FT also exhibits superiority over standard finetuning in terms of OOD accuracy. \n",
            "main_review": "Pros.\n\n+ The problem dealt with is described clearly. The finetuning of a pretrained CLIP model on downstream tasks would harm the OOD accuracy, which is often lower than that of no finetuning model. \n\n+ Despite the method being so simple as a trick, some empirical analysis is given to explain why the proposed method WiSE-FT performs better than standard finetuning.\n\nCons: \n\n - The paper writing needs to be greatly improved. Many sentences are hard to understand, such as “fine-tunes model is more confident ID, the reverse is true OOD” in the 2nd line of section 5.1.\n\n - The title is inaccurate and incomplete. (i) “zero-shot model” is confusing and overbroad. In fact, using “pretrained CLIP model” would be more specific according to the actual content in paper. (ii) it is ambiguous to just use “robust fine-tuning” in title, which does not reflect any content of the proposed method that employs the weight interpolation between weights of pretrained model and that of finetuned model. \n\n - The method name “Weight-Space Ensembling” is overrated. It is more suitable to call “Weight-Space Interpolation” according to Eq.(1).\n \n - How to choose the hyper-parameter alpha in Eq. (1), which has a obvious effect on OOD accuracy according to Fig 1 (Bottom), is not described in paper. \n\n - More comparison experiments need to be conducted to verify the universality of proposed method. (i) More baselines. If using fine-tuned model combined with other tricks like data augmentation and different regularization terms, could the proposed method WiSE-FT still improve OOD accuracy. (ii) More pretrained backbones. Only CLIP has been considered in this paper. Experiments on more backbones should be conducted.\n\n - Incomplete experimental results. Why is only the model with final fc fine-tuned considered in Section 5.1? The results of end-to-end finetuned model should be concluded.\n\n - Table 2 should be as clear as possible. ",
            "summary_of_the_review": "The overall novelty of the proposed method is limited. I reject it.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper points out the shortcomings of current fine-tuning approaches and proposes a simple method called WiSE-FT to improve the out-of-distribution robustness. The method is straightforward — ensembling the weights of zero-shot CLIP and fine-tuned models. On several distribution shifting situations, WiSE-FT exhibits large improvements for out-of-distribution datasets while maintaining the in-distribution accuracy.",
            "main_review": "1) The author conducts extensive experiments to verify the effectiveness of WiSE-FT — it achieves large improvements on different distribution shifting benchmarks.\n2) Ablation studies of the paper explore different variants of WiSE-FT and offer reasonable empirical analyses of the advantages of weight-space ensemble.\n3) Since WiSE-FT conducts post-ensemble of weight space, there is no additional computational cost for the performance improvement. \n4) The technical novelty of the paper seems limited. The weight-space ensemble is a commonly adopted trick. WiSE-FT simply applies the strategy to models (zero-shot & fine-tuned) trained with different data. \n5) The paper directly used CLIP as a zero-shot backbone, however, to verify the effectiveness of WiSE-FT as a universal robust fine-tuning approach, the paper should conduct more experiments with other pretrained models to serve as zero-shot backbones.",
            "summary_of_the_review": "The paper proposes a simple and effective weight-space ensemble strategy and exhibits large improvements in terms of out-of-distribution robustness. But the method itself lacks enough technical contribution and needs to prove that it is universally applicable to different pretrained backbones beyond CLIP.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}