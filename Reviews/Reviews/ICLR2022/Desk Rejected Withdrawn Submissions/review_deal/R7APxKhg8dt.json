{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a CommonSense Contextualizer (CoSe-Co) to generate the commonsense knowledge that can be augmented in downstream tasks, such as multi-choice QA, open-ended QA to improve the performance. \n\nThe key idea is: 1) train a generative LM (i.e., CoSe-Co) on their collected <sentence, commonsense path> pairs data; 2) utilize it to generate commonsense paths (sequences of entities connected through relations) for a given textual input in downstream tasks; 3) concatenate the textual input and corresponding generated paths as the new input to the task-specific model.\n\nThe authors release a sentence and commonsense path paired dataset collected by their devised heuristic-rule based method.\n\nExtensive experiments show augmenting commonsense paths generated by CoSe-Co in Multi-Choice QA and OpenCSR brings the improvements over current SOTA methods. \n\n",
            "main_review": "**Strengths**:\n\nContribute a novel dataset consisting of <sentence, commonsense path> pairs together with their collecting method.\n\nPresent a general commonsense knowledge generator trained on this dataset to generate commonsense paths that can be augmented in downstream tasks. \n\n\n**Weaknesses**:\n\nMy main concern is about the technical novelty of the proposed method. Compared to the path generator in (Wang et al., 2020b), the difference is that CoSe-Co is T5-based LM rather than GPT-2 and can take the natural sentences as input.\n\nThe claimed significant improvement on multi-choice QA is about 3%. However, experimental results in Table 2 show the improvement is only about 0.58% on test split using full training data, compared to QA-GNN. Besides, when using 60% training data, the improvement over PGQA is also incremental.\n\nThe comparison with PGQA is a bit unfair since PGQA is based on GPT-2 (trained on 40 GB text data) while CoSe-Co is based on T5-base (trained on 7 TB).\n\n\n\n**Questions for the authors**:\n\nIn Section 4.2, the ratio of the novel entities in generated paths by CoSe-Co can reach 23.28%, which are not present in training paths. I am curious where these novel entities come from.\n\nDid you try to change the backbone of CoSe-Co to other generative LM such as GPT-2?\n\n \n\n",
            "summary_of_the_review": "My recommendation is to reject this paper. The main contribution is on the empirical side and it is more suitable to the CL conferences.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose a task agonistic CommonSense Contextualizer(CoSe-Co) to generate diverse and novel knowledge paths relevant to given sentences as input. They create new sentence-commonsense path paired data to train CoSe-Co. CoSe-Co generates paths that contain novel entities not present in KG. It demonstrates that their method outperforms current SOTA models on Multi-Choice QA and OpenCSR tasks and further results in better performance with low training data.\n",
            "main_review": "Strengths\n1. CoSe-Co achieves new SOTA results on Multi-Choice QA and OpenCSR.\n2. CoSe-Co outperforms PGQA and generates better commonsense paths showing in Appendix A.\n3. The authors perform thorough experiments and ablation studies to demonstrate effectiveness of their method.\n\nWeaknesses/Questions\n1. When creating sentence-path paired dataset, they query Solr and rank retrieved sentences based on similarity between sentence embedding and embedded representation of the query. Does that mean you filter the retrieved sentences based on the ranking? More explanation would be helpful.\n2. In 4.1, they set the length of paths in the range of 3 and 6. Just wondering if they explore other ranges. \n3. In 4.2, it mentions that CoSe-Co attained a novelty of 23.28%. How do you measure such novelty? Do authors read some examples and calculate the fractions and are 23.28% enough for a good fraction?\n4. In figure 3 (b), the greedy decoding result is missed. Since the BLEU scores for greedy decoding and diverse-path search are the same, it would be useful to see how different they are in diversity measurement. \n",
            "summary_of_the_review": "In general, the proposed method seems to be reasonable and outperforms existing methods. Their proposed framework, CoSe-Co achieves new SOTA on Multi-Choice QA and OpenCSR and they create sentence-path paired dataset that did not exist before. However, there are some points that they have not specified in the paper which I write down in the main review above.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose CoSe-Co, a generative model that generates relevant commonsense knowledge paths from a given natural language input sentence. First, the authors develop a new sentence to KG-path dataset curation technique and use ConceptNet to create a large dataset to finetune T5. After fine-tuning, this model can be essentially used as a source of commonsense knowledge for any sentence. Through experiments on multiple datasets, the authors establish that such a model is able to generate some meaningful knowledge and novel concepts that help solve downstream tasks more efficiently.",
            "main_review": "Strengths:\nOverall, the paper is quite well motivated and easy to read. Technically, the input format for CoSe-Co indeed makes it easier to apply to downstream tasks, which was a limitation of many prior works. Additionally, the dataset created by the authors can be a helpful resource as well. Lastly, diverse-path search is a good inductive bias for path decoding and would be relevant to the related community.\n\nWeaknesses/Questions:\n* On possible data leakage - Current models use ConceptNet as the KG which is probably fine (although it is a source of data leakage for CSQA dataset). Apart from that, you're also using Wikipedia for sentence selection. Do you perform any checks on data overlap with the training set of CoSe-Co? I think this can lead to wrong conclusions if not addressed correctly.\n* On querying partial paths - In this case, will the training data consist of sentence and the partial path that matched, or the complete path even though it was not used to match? If the latter, I wonder if that makes the dataset too noisy?\n* On ER-templates - The motivation for Q1 is not clear. Why are $(e_i, r_i, e_{i+2})$ and $(e_i, r_{i+1}, e_{i+2})$ chosen as templates? Please add some intuitions on why these were chosen (other than the ablation). Did you also try some other formats that did not work?\n* On diverse-path search - Do you mean only first entity is important? I believe any entity once selected can guide the path generation, with importance decreasing as the path length increases. So, even 2nd and 3rd entities could be quite important to decide the path direction. Or did you find that's not the case?\n* On choice of $l_1$ - Is there any reason $l_1 < 3$ was not chosen? 2-hop paths can be meaningful and also abundant.\n* On diversity - Isn't the metric used very noisy? Also, this is probably biased to generating just different lengths instead of diverse texts. If this has already been used for some diversity evaluation of sentences, please add those references.\n* On CSQA baselines - Please add some T5 baseline to the table, otherwise the comparison is probably not fair.\n\nNeeds clarity:\n* Please add some running example or illustration to explain the sentence-path paired dataset creation. Currently, the para is bit hard to parse and an example will make it easier to understand.\n* Please mention more details on the baseline method PGQA in appendix. I'd suggest adding a brief para explaining the working of the model and also clarifying how the heuristic filtering works for relations.",
            "summary_of_the_review": "The paper has interesting contributions in terms of the new training dataset + model proposed. Through many experiments the authors demonstrate the effectiveness of their method. But, some concerns about data leakage need to be addressed to ensure the gains are rightly attributed. Overall, I'm leaning towards acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a method for improving tasks relying on common sense by generating knowledge graph paths to provide additional context to QA systems. it is evaluated on a variety of tasks achieving strong results.",
            "main_review": "The results in the paper are good, but it is unclear why. As mentioned in section 4.3, the method proposed is a modification of the path generation component of Wang et al. (2020b), but otherwise the same. Wang et al, also used a pretrained LM fine tuned on KG triples, thus most of the motivation is the same. The only difference as far as I can tell, is the use of a different pretrained LM (T5 instead of GPT2), and possibly in the training data generation. However, the paper doesn't try to explain what the differences are, so it is unclear why the results are better. The masking might be novel, but it seems not to have much of an impact on the results (Table 3). On the whole though, it appears to be a better implementation of Wang et al. (2020b), but the ideas are the same, so I don't think another paper is warranted.\n\nA couple of other points:\n- In the evaluation of the triples, I think novelty should also take into account whether the triple is actually correct; not just that it doesn't appear in training\n- Finally, the paper is poorly written, with a lot of grammar errors and poor phrasings such as \"to scale the knowledge\" (abstract), \"model has to classify correct choice\" (introduction), etc. thus better proofreading is needed.",
            "summary_of_the_review": "While the results are positive, the paper does not justify why the method proposed improves on previous work, and in particular work also performing path generation.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}