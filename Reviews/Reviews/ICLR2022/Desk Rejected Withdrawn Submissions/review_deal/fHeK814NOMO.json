{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The work presents a modification to existing approaches of automatic learning rate adaptation (called TLR) via a second order approximation of the function mapping step size to the change in loss when optimized with SGD. This was easily the most controversial paper in the AC's stack, with 4 reviewers advocating accept and 2 reviewers strongly arguing for reject. The authors also went through considerable effort to address reviewer and AC concerns and uploaded multiple additional experiments and ablations to support the robustness and efficacy of the proposed method. Despite a long discussion and rebuttal period, reviewers were unable to reach a consensus. There were several different aspects of the work whose merits were thoroughly debated during the rebuttal period.\n\nThe first aspect regarded what the exact contribution of the work was. Initial reviewers who were very high on the work believed that the entire derivation from equation (1) to equation (9) was novel. However, as other reviewers correctly pointed out (1) to (7) is a standard derivation of adaptive learning rates and has appeared in several prior works. Instead it is primarily equation (9) that is the contribution. Given that multiple reviewers initially believed that (1 -> 7) was a novel contribution, I feel it is safe to say that the authors do not adequately discuss their contributions with respect to prior work. However, all reviewers in the end agreed that equation (9) is novel and potentially interesting (though some remain skeptical of it's utility).\n\nThe second topic of debate regarded the short horizon bias raised by reviewer hkZ3. The short horizon bias presents a fundamental barrier to meta optimization of the learning rate. To summarize, greedily selecting the step size to minimize the loss will result in the optimizer taking too small of steps in the flat directions of an ill conditioned loss surface. This results in faster training in the short term but slower training in the long run. The presented method seeks to greedily optimize the loss over short time scales and thus will be subject to the short horizon bias. The initial draft of the work did not include any discussion of this prior work. During the rebuttal, the authors initially argued that their method can help mitigate the short horizon bias before later concluding that it is a limitation of the method. There was debate between the AC and reviewers regarding whether or not existing methods of adaptive learning rate schedules were already at a fundamental barrier presented by the short horizon bias. One reviewer even mentioned that in their own research they have abandoned the general approach of adaptive learning rates because they cannot overcome this issue. This debate was never resolved, it's plausible to the AC that there is room for increasing the robustness of existing approaches while not addressing the short horizon issue. It is the AC's opinion however that the work would be significantly strengthened with experiments directly addressing the short horizon issue.\n\nThe final item of debate regarded the strength of the considered baselines. The authors claim that the second order term in (eq 9) largely removes the need for tuning relative to Baydin et. al. and that the method outperforms multiple baselines across multiple workloads, including Adam (Kingma et. al.), SLS (Vaswani et. al.), and SPS (Loizou et. al.). Indeed multiple plots are given showing that the authors have found a configuration of their method that consistently outperforms certain fixed configurations of the considered baselines. Furthermore, ablations are presented which suggest that indeed it is the addition of equation (9) that is responsible for this strong performance. Despite all of this presented evidence some reviewers remained skeptical, and believed that they could produce a different but fixed configuration of say Baydin et. al. (or even Adam) which matched the proposed method on all of the considered workloads. There are compelling reasons for reviewers to consider the presented experiments with skepticism. Indeed the deep learning optimization literature has for years struggled to make progress despite publishing hundreds of papers---see for example the results of [1] which perform an independent comparison of 100's of published methods and found that none convincingly outperform Adam. Given this, it is clear that the current standard for evaluating optimizers in the literature is inadequate if we are to reliably make progress.\n\nTo give a more relevant example of the difficulty of comparing optimization methods, suppose for the sake of argument that we were not evaluating the efficacy of TLR but instead the method of Vaswani et. al. (SLS). Vaswani et. al. makes many similar claims as the proposed method, namely the method consistently outperforms Adam across multiple workloads and enjoys a similar robustness to hyperparameters (e.g. their Figure 6). However, in Figure 3 and 4 of this work we see SLS no longer outperforms Adam on the considered workloads. If Vaswani et. al. had argued for acceptance based on the author's Figure 3 and 4, I don't think any reviewer would have recommended acceptance. This begs the obvious question, why does SLS consistently outperform Adam in the experiments run by Vaswani et. al., but not in the experiments run in the considered paper? There are at least two possible answers here, both of which are concerning. Either Vaswani et. al. is yet another method that generally doesn't outperform Adam or in comparing SLS with TLR in this work the authors did not properly tune SLS in their baselines. Furthermore, what is going to happen if future work tries to compare against TLR? Will TLR still look better than Adam or will independent review find that TLR is yet another method that on average performs about as well as Adam? As a reviewer trying to compare the two papers I see very similar evidence given supporting the two methods and thus I am left with an unresolved contradiction.\n\nGiven all of this, I am forced to conclude that there is insufficient evidence presented in this work that the proposed method generally outperforms related methods such as SLS and Adam. A natural question thus is, what would have been sufficient evidence? Indeed the presented experiments seem to be about as convincing as what is shown in previous published methods such as SLS. In a sense the AC is also arguing that Vaswani et. al. presented insufficient evidence that SLS generally outperforms Adam (looking at Figure 3 and 4 perhaps SLS in fact isn't as useful as Vaswani et. al. claim). In looking at the experiments presented in this work, related prior works, and the 100's of methods considered in [1] a common recurring theme is that when comparing with prior work authors consistently run their own implementation of baselines on workloads of their choosing, rather than directly comparing with published results. In doing so, this leaves open the question regarding whether or not the authors (perhaps inadvertently!) are only considering workloads and hyperparameter settings which favor their own method rather than giving a realistic assessment of the efficacy of their own methods relative to others. Thus, if the authors wish to argue that TLR generally outperforms SLS, a strong piece of evidence the authors could provide is to run TLR directly on the open sourced code provided by Vaswani et. al. Show the reviewer how TLR compares when added directly to (for example) Vaswani et. al. Figure 4. In doing so, the authors will have addressed any concerns reviewers may have about how well represented SLS is, as the authors will be comparing against SLS in a setting where there were actual incentives to make SLS look good.\n\n1. Schmidt et. al. Descending in a crowded valley – Benchmarking Deep Learning Optimizers, https://arxiv.org/abs/2007.01547"
    },
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper the authors derive updates to the learning rate (LR) that minimize the loss function that can be computed as functions of neural network gradients. These updates to the LR can be used to compute an adaptive LR schedule in an online, hyperparameter-free (or at least, less hyperparameter sensitive) fashion. They derive first and second order updates for the LR and then run experiments with it on various deep learning workloads.",
            "main_review": "*Strengths:*\n- Your Figures 1, 2 are great in that they actually showed what the computed LR schedule looked like.\n- The time requirements in Section 5 are very useful; it’s great to know what the real-life overhead one should expect from different methods!\n- Paper is very well written and formatted.\n\n*Weaknesses:*\n\nOverall I believe most of my issues stem from lack of comparisons to baselines with nontrivial LR schedules. The comparisons to other LR schedules in Appendix D are nice, and the authors should plot the baseline LR schedules next to the proposed LR schedules (such as in Figures 1, 2), and do so in the main text. The authors do not seem to include any LR schedules that include a warmup in their baselines, which is surprising given LR warmup is often common on the workloads considered in the experiments. Most of the computed TLR schedules look like a linear warmup followed by exponential or quadratic decay, so I would recommend trying this sort of schedule as a baseline, even just with default hyperparameter choices (warmup for 10% of training?). Figure 3 seems to imply that once the LR of the stepwise baseline schedule is dropped halfway through training, the baseline outperforms the proposed method. This to me indicates that by trying even a few more baselines LR schedules, the proposed method may not be competitive. While there is definitely appeal in the proposed method because it mostly does not require tuning, many heuristics used today do not actually require much guess/check work either beyond choosing the initial LR (which all proposed methods must do), that is they work “out of the box” (such as warmups or other popular decay schedules) or with very minimal tuning. As another example of this concern, it is surprising in Figure 3 that Adam does worse than SGD with momentum, given that on CIFAR WRN Adam almost always outperforms other optimizers; what LR schedule was used for Adam? How does Adam perform when using the same LR schedule as sgd_mstep? It seems unfair to compare to any optimizer with a knowingly poor LR schedule, especially given that commonly used ones (sgd_mstep) seem to outperform the proposed method.\n\nAdditionally, I struggle to determine which methods are better from many figures, because the subplots are too small/crowded. The authors could dramatically reduce the vertical axis on many such plots to paint a better picture. For example in Figure 4 it is unclear how the optimizers perform towards the end of training, because of the vertical axes (what were the final accuracies achieved for each optimizer?).\n\nWith any paper that introduces a training algorithm technique, I think it is critical to highlight if the goal of the proposed technique is to 1. improve training speed 2. improve generalization performance (or both). If any part of the goal is to improve training speed, then I believe that a better method of comparing performance is to instead report the number of steps (or total wallclock time) required for each technique to reach a competitive validation performance (for example, how many steps did it take for each optimizer to reach a competitive ImageNet validation accuracy (75.9%)?). If the proposed technique is supposed to improve generalization performance, then the baselines should probably include additional regularization techniques.\n\nSome other concerns:\n- “We should note that the per-layer rationale cannot be supported by backtracking\nline-search algorithms, which use single-valued learning rates.” I may not understand the point of this text, but why can’t other algorithms use per-layer LRs? Can’t you always just rescale the updates for different layers?\n- Storing an extra copy of gradients is actually nontrivial for a lot of larger models, making the storage cost of SGD with momentum similar to that of Adam. In these memory constrained cases, others have tried storing gradient statistics in lower precision to save space. While definitely not necessary, storing the older gradients in low precision could be an interesting additional experiment.\n- In Figure 3, it would be much more interesting to see different LRs instead of batch sizes (you have plots of the losses on CIFAR100 in the appendix for different LRs, but as we saw in other figures the loss does not tell the whole story in the case of overfitting), because it seems like an unfair comparison to use an untuned LR for the baseline methods (each baseline only gets a single initial LR) when the proposed method can change its LR online to correct for a poor choice of initial LR.\n- As a minor point, I believe citing this paper https://openreview.net/forum?id=H1MczcgR- would be useful, as it surfaces concerns with learning the LR.",
            "summary_of_the_review": "Overall, while the paper is very well written and formatted, I do not believe I can recommend this paper for acceptance without seeing better comparisons to more competitive baseline LR schedules (such as warm up followed by an exponential or quadratic decay, even with all the hyperparameters set to default values). The figures could use some cleanup as described above.\n\nHowever, *I do believe that the proposed method could be of significant value, but I would want to see it demonstrated on workloads where common LR schedules/heuristics do not hold up*; could the authors find or design an experiment where the dataset/model is too difficult to optimize with traditional LR schedules, but the proposed method succeeds? Perhaps poorly conditioned neural nets (such as removing normalization or skip connections) could be of interest here.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The authors propose an adaptive learning rate strategy that relies on treating the learning rate as a learnable parameter. The learning rate is updated using first- and second-order updates wrt the loss, evaluated after applying a step on the weights. The results are validated on common datasets and architectures such as CIFAR-10/100 and ImageNet.",
            "main_review": "The paper at the current stage is merely a repeat of a number of known approaches. Namely, Amid et al., 2020 already propose a very similar approach based on the inner-product of the current and past gradient to update the per-layer learning rate. Amid et al., 2020 extend this to coordinate-wise learning rates and use exponentiated gradient updates, which is better suited than gradient descent for updating non-negative values. They even consider adding momentum, EMA of the past gradients, normalized updates, etc. Methods that rely on updating the learning rate based on the interaction of the consecutive gradients date back to the Delta-Bar-Delta (DBD) method (Jacobs, 1988) and have been repeatedly proposed in different forms (Minai & Williams, 1990; Sutton, 1995; Schraudolph, 1999; Baydin et al., 2018). So in a sense, the paper has nothing new to offer. The only minor change that the current submission considers is a second-order update on the learning rate, which I do not expect to have a major impact, as discussed next.\n\nAnother important point missed by the authors is the problem of short-horizon bias associated with consecutive gradient-based adaptive learning rate methods. Specifically, Wu et al., 2018 show that all such approaches suffer from the effect of a short-time horizon where the method punishes for the variability in the loss surface (due to stochasticity and other factors) in the early stages of optimization. Thus, these methods usually do not allow a large enough learning rate initially to escape bad local minima. The paper does not acknowledge this problem nor provides any improvements over the earlier approaches to address this issue.\n\n\n\nMinai, A. A. and Williams, R. D. Back-propagation heuristics: a study of the extended Delta-Bar-Delta algorithm. In 1990 IJCNN International Joint Conference on Neural\nNetworks, pp. 595–600 vol.1, 1990.  \n\nSchraudolph, N. N. Local gain adaptation in stochastic gradient descent. In 1999 Ninth International Conference on Artificial Neural Networks ICANN 99. (Conf. Publ. No. 470), volume 2, pp. 569–574 vol.2, 1999.  \n\nBaydin, A. G., Cornish, R., Martínez-Rubio, D., Schmidt, M., and Wood, F. Online learning rate adaptation with hypergradient descent. In 6th International Conference on Learning Representations, ICLR, 2018.  \n\nEhsan Amid, Rohan Anil, Christopher Fifty, Manfred K. Warmuth. Step-size Adaptation Using Exponentiated Gradient Updates. In Workshop on \"Beyond first-order methods in ML systems\" at the 37th International Conference on Machine Learning (ICML), 2020.  \n\nYuhuai Wu, Mengye Ren, Renjie Liao, Roger Grosse. Understanding Short-Horizon Bias in Stochastic Meta-Optimization. In ICLR, 2018.  \n\n\n",
            "summary_of_the_review": "Overall, I believe the current submission misses multiple related work and has not much to offer in terms of novelty or addressing the issues associated with the gradient-based adaptive learning rate methods. Therefore, I vote for a reject.  \n\nUpdate: I forgot to add pointers to the missing references that I mentioned in my initial review. Apologies for that.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "In this paper, the authors propose a novel adaptive learning rate method where the learning rate is learned along with the weights through gradient descent. The main intuition is that the learning rate is part of the loss function (along with the weight gradients) and this combined loss is minimized through gradient descent (naive GD-TLR).  This method can be computationally expensive due to the fact 2 versions of the loss must be computed at each layer.  To mitigate this, the authors propose a variant of the method that updates the learning rate based upon a single loss (efficient GD-TLR). The efficient GD-TLR has the intuition that if 2 consecutive losses are in the same direction, the learning rate is increased (move to goal faster).  If the 2 consecutive losses are opposite direction, decrease the learning rate (bouncing around the well). In addition, this method can use different learning rates for different layers in the network.",
            "main_review": "This paper provides a useful formulation of the problem. Posing the learning rate as a learnable parameter does appear to be well-founded theoretically, but the results are a bit lackluster for the networks that were tested.  Testing a transformer-based model would be an interesting data point. \nAfter the description of the technique, the authors do a good job of comparing it to existing techniques. However, I would like to a concise summary of how GD-TLR improves on the shortcomings of those techniques. The main advantage is decreased computational cost as there is no Hessian required and can be simplified into a constant-time operation on the normal backward pass.  A single sentence making this explicit would be helpful for the reader.\nFor the simplified variant, efficient GD-TLR, the authors indicate that the technique is reactive to either too high or too low of a learning rate. This should have been the first hint that it will at best perform as well as a prospective technique that considers a known gradient profile. The authors should relate this in the intuition under equation 6 to the expected performance.\nThe experimental portion was a good representation of networks for computer vision classification.  But GD-TLR is likely most applicable to other problems where there might be more of an advantage. The authors' final comments do indicate that they believe an important result is that GD-TLR can work in scenarios where learning rate hasn't been optimized much.  It would be nice for the authors to actually demonstrate this rather than simply stating it.  I suspect this is true, especially for large transformer models, but the paper provides no evidence to this end.",
            "summary_of_the_review": "Overall a good paper with an interesting technique. Showing that the technique really can work in scenarios where learning rate hasn't been optimized could make this a very important result.  As written, there is much work to prove this out, so the idea remains a curious hypothesis.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper introduces a new algorithm to automatically adjust the learning rate. More specifically, the learning rate is optimized via the extra gradient descent step during the training. The paper first discusses the role of learning rate and formalizes the problem within a gradient descent framework. Then based on that, the authors give an expression of both the first and second derivatives of the learning rate where the neural network structure is taken into consideration. That further leads to a cost-effective implementation to update the learning rate. They also show that such implementation can be extended to the case where a different learning rate is applied for each layer.",
            "main_review": "Pros:\n* The paper is well-written and easy to follow. The logic structure of the paper is clear and necessary explanation is given. e.g. the intuitive interpretation of derived gradients is friendly to a reader like me.\n* The idea of incorporating the second-order gradient is novel and interesting. Most hypergradient descent methods require to introduce several additional hyper-paramters and carefully fine-tune them, but with a Newton-Raphson formulation, the update of learning rate become more concise.\n* The derivation of the algorithm is clear and the method is easy to implement. Furthermore, the per-layer learning rate scheme seems a natural extension to the original algorithm where a global learning rate is applied to all layers.\n* Both the experiment setup and the empirical results look good. The new algorithm outperforms the state-of-art methods in most cases.\n\nCons:\n* The trainable learning rate is not a fancy idea. There are lots of papers about hypergradient descent which consider hyper-paramters (including the learning rate) to be trainable. However, I have not seen a decent discussion about the connection between the proposed method and existing work in that direction.\n* The expression of derivatives of the learning rate with standard neural networks may not be applicable to more complex architectures (e.g. ResNet). Furthermore, I somehow feel $\\hat{y}$ is used as more like an intermediate variable so I am not sure if it is necessary to formulate the problem in this way. [1] has a similar conclusion for calculating the gradient of the learning rate. Is there any specific reason not to cite their conclusion?\n* I found the figures from the experiment sections extremely hard to read. For example, in Figure 1 the curves overlap with each other during training and I could not distinguish any one of them. If possible, please provide a table so the results will be much more readable. Also, some names of the legends are weird and not consistent with the ones in the main context. \n\nComments:\n* On page 4 there is a footnote explaining how to formulate the layer with bias term. I might miss something, but shouldn't $W = (W\\ b)$ instead?\n\n[1] Atilim Gunes Baydin, Robert Cornish, David Martinez Rubio, Mark Schmidt, and Frank Wood. Online learning rate adaptation with hypergradient descent. In International Conference on Learning Representations, 2018. ",
            "summary_of_the_review": "Overall this is a good paper and I tend to vote for acceptance. However, I do hope that the authors could provide some feedbacks on the reviews I made and polish the paper more in details.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors proposed an approach to optimize the learning rate of deep learning methods during gradient descent training. The approach uses first and second-order gradients concerning learning rate as functions of consecutive weight gradients. The gradient-based method suggests no manual tuning is required and considers the learning rate itself as a learnable parameter. The authors also investigate the use of the approach per layer. Finally, the authors studied the method’s performance considering variations in the initial learning rate and the batch size. The paper presents a set of experiments using MNIST, CIFAR10/CIFAR100, and ImageNet. Results consider MLP for MNIST, Wide-ResNet for CIFAR10/CIFAR100, and ResNet50 on Imagenet experiments. ",
            "main_review": "The paper presents an interesting approach with exciting results, quantitative and analysis. The ideas of the paper are promising and have the potential to open discussion to a different direction of how to adjust the learning rate on iterative learning algorithms. I have the following considerations and suggestions on how this paper can be improved:\n- The authors build the method in the context of gradient descent for optimizing a neural network loss function, and I’m wondering about the impact on different loss functions. \n- The paper suggests robustness to user-defined hyper-parameters such as batch size and initial learning rate, but no consideration is made about the number of epochs.  Considering the comparison with literature approaches in a long-term training regime the methods tend to be equivalent at some point of the optimization. The authors need to present some comments on this direction. If the authors consider fast convergence as a feature of the method and suggest ignoring the number of epochs, experiments and comments in this direction are needed.\n- Besides the considerations about the impact of the user-defined hyperparameter p, which controls the frequency of learning rate updates. It is unclear if the suggested 0.33 value is valid for different datasets, considering small/complex/sparse or other datasets characteristics.\n- Are the experiments considering one execution or different runs of training are performed? It is essential to introduce variance to confirm the experiment’s conclusions. Are all the methods starting from the same initial weights? Hyperparameters description? The experimental methodology description needs to be improved; some comments are made on the supplementary material but must be addressed in the main text.\n- Are the models trained from scratch? What is the performance considering transfer learning?\n- Improve the figure’s caption driving the reader to what needs to be understood from the figures. Please insert the axes description on the figure.\n- When considering the Time Requirements experiments, why are only experiments with CIFAR100 and the WRNET_16_4 architecture presented?\n- Did the authors identify any phase shift regime during the training process?\n- Some experiments suggest the proposed method with better performance at larger batch sizes. What is the impact of using bs minor than 64?\n- Could the method generate overfitting on small datasets?\n- What is the statistical relevance of the presented results considering statistical tests?\n- There has been little discussion on the limitations of the proposed approach. What are the possible technical limitations?\n",
            "summary_of_the_review": "The paper presents an interesting approach with promising results, quantitative and analysis. However, the authors need to provide more comments and address some concerns about the experimental methodology and experiments before publication.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "An intuitive overview of the fundamental ideas:\n• The fundamental problem that this work is concerned with is finding a suitable, “optimal”, way to adaptively set the learning rate for the\nNeural Net setting (whose fundamental challenge, as the paper points out, is it’s ’over-parametrization’)\n• The approach adopted in the paper is the proposal of a framework (WITHIN the existing GD-oriented ways in which we train our NN’s),\nwherein we consider the Learning Rate as a learnable parameter \n• The real novelty of this approach shows itself in how elegantly the mathematical formalism that follows from this approach, works out,\n",
            "main_review": "I try to briefly cover the interpretation of the obtained mathematical results in the paper below:\n– Since we are trying to treat α as a learnable parameter, and want to treat it on the same footing as any other NN parameter, this\nwould mean that we ought to write a GD-update for it (which would involve the gradient of an augmented loss, say: Lα(αt,wt)\nwrt the learning rate, which we would have to define), and would also involve an auxiliary parameter (say ηt) as a sort of meta\nLR for α – this is where a straightforward pursuit of the LR as a learnable parameter would lead to. So far, the novelty has\nbeen in the definition of the augmented loss, which was formed by combining the NN’s loss with the GD update for the weights that\nit was a function of (namely: Lα(α;w) = L(w − α.∇L(w)).\n– The real strength of the above approach, shines through when the relevant gradients are actually computed, namely the quantity:\n∂Lα/∂α |α=αt , for which we end up with the really powerful expression: −⟨gt, gt−1⟩, which, in addition to being an extraordinarily\ncompact, for such a general gradient computation (probably giving us a nudge that our initial definition of the augmented loss\nL was good) also affords to us a really intuitive (but robust and complete) qualitative interpretation of how we should update our\nLR per iteration, the reason this expression is so significant, is because so far, we haven’t even considered the GD-styled update for\nthe learning rate, and already, the expression for this newly defined gradient (computing which is O(1), since the dimensionality\nof our spaces are fixed when training) provides us with enough information to be able to make better LR-related decisions per iteration than using a constant LR/some weighting method to reduce our LR with progressing EPOCHS (either of which might\nbe schemes that do not accurately reflect the topology of the loss landscape)\n– After the above gradient computation, we look towards the second order gradient – both in hopes of obtaining a similarly elegant\nmathematical expression for the same and also with the aim that we might be able to leverage second order information to devise a\nstronger update rule for the LR, and as it so happens, we end up getting a really elegant expression for the second-order derivative\nas well, namely: ∂2L/∂2α |α=αt = 4/αt⟨gt, gt − gt−1⟩ = 4/αt(||gt||2 −⟨gt, gt−1⟩), which can also be computed along with the above\nfirst order derivative in O(1) time.\n– The fact that we have effectively O(1) access to relevant second order information for the framework that we have built for the\nLR, is very significant, making available all the power associated with second order methods, for basically free!\n– The next step would be to come up with a proper update rule utilizing the above information that we have derived about the\nLR’s GD structure – since we have access to both first and second order information in this case, an obvious place to look for\nmotivation for the functional form of the update, would be the Newton direction, which essentially, allows for the incorporation\nof second order information in an ’optimal’ manner (the Newton direction is a good direction move in when: 1) The function\nis well-approximated by it’s second order Taylor expansion and 2) The Hessian for the function is positive-definite) – also observe,\nthat in a sense, using Line-search methods with the Newton direction, is on a conceptual level (differing in the underlying\ndetails, like the requirement of positive-definiteness in LS, which may be relaxed in Trust regions) incredibly similar to using\nTrust regions (where we use the quadratic approximation model: mk(p) = fk + pT + ∇fk + 1 2pTBkp and take Bk = ∇2fk).\nThe above strategy is precisely what is adopted in the paper, the Newton direction is given by: pNk\n= −(∇2fk)−1∇fk, which may be re-created in the Naïve update rule: αt+1 = αt−η.∇Lα(αt;wt)  by setting η to −(∇2 αLα)−1 – in addition to this, the term is slightly altered to ensure numerical stability. \nA comparison with surrounding literature: The two specific papers that I am interested in talking about (wrt the current work), are: item 1 and item 3, since they are similar in spirit to the current work (in that, they also try to suggest ’optimal’, formalised methods for\nadaptively setting the LR) and have both been published in good conferences (the former in NeuriPS,19 and the latter in ICLR,18)\n2.1 Painless stochastic gradient: Interpolation, line-search, and convergence rates: This paper tries to solve fundamentally the same problem – i.e. it tries to find an ’optimal’ way to adaptively compute step size during training, though the approach adopted in their paper is fundamentally different from the current work, but with some interesting takeaways that could’ve influenced\nthe current work. I first summarize their approach:\nThe fundamental idea of the paper is to use line search on each iteration for computing the step-size, specifically, Armijo LS, which chooses it’s step size based off of the solution to the following optimization problem (the following is the expression suggested in the paper for the adaption of the problem to the stochastic setting). fik(wk − ηk∇fik(wk)) ≤ fik(wk) − c.||∇fik(wk)||2\nWhat follow are proofs of convergence bounds for this method in the case of strongly convex and convex functions under the assumption of the validity of the interpolation condition for the loss function and data of our NN (namely, if f(w) = 1nPn i=1 fi(w), then in the case of an optimum of f, w∗, i.e. ∇f(w∗) = 0, we also have: ∇fi(w∗) = 0∀i ∈ [n])\n• From the above, the major takeaway for the current work, was definitely the intuition to define our augmented loss in the general Line-Search setting (though of course, what the current work does is far from frame and solve a classical LS problem, since we straight off assume the direction of steepest descent, but clearly, the form of our augmented loss is the same as the general form of the LS optimization problem, namely: minα>0f(xk + αpk))\n• It would be interesting to see to which extent the mathematical framework of the current work could’ve been adapted to agree with the\ntheory of item 1. Since the starting point of both agree to a certain extent.\n• A clear strength of the current work over item 1 is the lack of requirement of the interpolation condition for results to hold good, and the\nsignificantly easier implementation of the suggested methods \n2.2 Online learning rate adaptation with hypergradient descent: In terms of the fundamental structure of the work, the current paper is\nincredibly similar to item 3. They also arrive at the same expression for the gradient, but that is where they decide to stop (and instead, focus their time towards the general formulation of the same update). Their way of formulating the update, where they didn’t explicitly define an augmented loss, instead they implicitly worked with ∂f(θt−1)/∂α , which at the end of the day is fundamentally the same thing as ∂Lα/∂α , but the more general formulation of the latter, (and a vested interest in second-order optimization!) is probably what led to the formulation of the second order update in the current work.\nReferences:\n1. Sharan Vaswani, Aaron Mishkin, Issam Laradji, Mark Schmidt, Gauthier Gidel, and Simon Lacoste-Julien. Painless stochastic gradient:\nInterpolation, line-search, and convergence rates. arXiv preprint arXiv:1905.09997, 2019.\n2. Rohan Mohapatra, Snehanshu Saha, Carlos A. Coello Coello, Anwesh Bhattacharya, Soma S. Dhavala and Sriparna Saha; AdaSwarm: Augmenting gradient-based optimizers in Deep Learning with Swarm Intelligence, the IEEE Transactions on Emerging Topics in Computational Intelligence; DOI: 10.1109/TETCI.2021.3083428.\n3. Atilim Gunes Baydin, Robert Cornish, David Martinez Rubio, Mark Schmidt, and Frank Wood. Online learning rate adaptation with hypergradient descent. In International Conference on Learning Representations, 2018.\n4. Pang Wei Koh and Percy Liang. Understanding Black-box Predictions via Influence Functions. In Internal Conference on Machine Learning, 2017.8\n5. Jorge Nocedal and Stephen Wright, Numerical Optimization, 2nd edition, Springer Series in Operation Research and Financial Engineering, 2006",
            "summary_of_the_review": "The strength of the framework put forward by the paper can be described in two points: 1) The ’optimality’ of the above suggested framework\nis intuitively obvious to anyone who understands basic NN optimization (since we are treating the problem of optimizing the LR, just\nas we would any other parameter of the NN) and 2) The mathematical objects required for the same (the gradient and the Hessian) turn out\nto have extremely elegant closed form expressions which require zero computational overhead to implement. Now, notice that points 1 and\n2 are talking about fundamentally distinct ideas, 2 is referencing the chain of mathematical argument, from the definition of the Augmented Loss (Lα), to the derivation of the expressions for the Gradient and the Hessian, while 1, references HOW we actually exploit the (first\nand second order) information obtained in 2, in an ’optimal’ manner – hence, one front on which an extension may be made is in our method of choice for step 1(hence, that opens up us up to, in theory, choosing from all the rich literature on second order method) – but, sadly this extension won’t be as easy as swapping out using a Newton step with some other second order method, since we further observe that even though 1 and 2 are distinct ideas, they are intimately coupled together by the definition of the augmented loss, which in turn was directly motivated by the form of the standard GD update (and also by the standard form of the Line-Search optimization problem, namely: minα>0 f(xk+αpk), where we had already made the decision to fit this work to directions of steepest descent i.e. pk = ∇fk), and hence in a way, brings all of this full circle in this being an isolated framework.\nThere aren’t any ’obvious’ ways to extend the fundamental theoretical framework of the paper, and any kind of work that takes motivation from the structure of this one, would have to start with first choosing the broader framework to embed this optimization of the LR in (in this case, it was GD wrt the NN loss)",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "10: strong accept, should be highlighted at the conference",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}