{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper targets a problem about formal query answering over an incomplete knowledge graph with a background DL-Lite_R ontology, where the formal queries are restricted to certain shapes. The paper extends existing methods for embedding query answering over knowledge graph without background ontology, namely Query2Box (Ren et al. 2020) and CQD (Arakelyan et al. 2021), with data sampling and a revised training objective. To cope with the background ontology, the basic idea for data sampling is to sample queries rewritten from the given query to train the embedding model, whereas the main revision to the training objective is to consider all queries generalized from the given query. The paper also provides two new datasets (LUBM and NELL) for the targeting problem and presents evaluation on Query2Box, CQD and their extensions mentioned above.",
            "main_review": "Strengths:\n(1) The paper provides two new datasets for the problem about formal query answering over an incomplete knowledge graph with a background DL-Lite_R ontology.\n(2) The paper also proposes two main extensions to enhance a method for embedding query answering to cope with background ontology.\n\nWeaknesses:\n(1) The usage of generalizations of a query will enlarge the set of certain answers to the query. In fact, all queries rewritten from a query based on a background ontology are specializations (from the right-hand side of the subsumption to the left-hand side), and thus the exact set of certain answers can be preserved. It is very unclear why generalizations are used in both data sampling and the revised training objective. In other words, the design of the extensions is questionable.\n(2) There are no evident implications from the main experimental results (see Figure 4 and Table 2); in particular, there is lack of evidence that the extensions are effective in improving the baseline methods Query2Box (Q2B) and CQD. If I do not misunderstand, the method O2B is enhanced from Q2B with the proposed extensions. However, O2B is inferior to Q2B in two (LUBM gen and LUBM spec) out of eight cases; i.e., the extensions are not effective in these cases. It is said in the paper that the proposed solution (O2B?) improves Q2B for almost 50% and CQD for almost 54% in terms of LUBM I+D, and that it improves Q2B for almost 20% and CQD for almost 25% in terms of NELL. However, these improvements cannot be checked from Figure 4 or Table 2. In contrast, CQD_gen and CQD_spec outperforms O2B_any a lot according to Figure 4 and Table 2.\n",
            "summary_of_the_review": "The paper has marginal novelty on two extensions of an embedding method for query answering. Both extensions rely on generalizations of a given query which may enlarge the set of certain answers to the query. This design of extensions is questionable and have not been addressed in the paper. The evaluation is insufficient to show that the proposed extensions are effective in improving the baseline methods Query2Box (Q2B) and CQD.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes ontology mediated Neuro symbolic query answering system that uses popular embeddings and knowledge present in the ontology to bring the inductive and deductive reasoning capabilities for query answering.  \nPaper proposes strategies to use ontology axioms to improve embedding training for both query based embedding methods and atom based embedding methods. Authors introduce two new benchmarks where this can be tested and show \npositive results that can be obtained on queries that span across deductive, inductive and inductive+ deductive scenarios",
            "main_review": "Strengths: \n1. Paper explores how inductive and deductive reasoning capabilities can be integrated in knowledge graph query answering. \n2. Paper is well motivated and the approach is described clearly.\n3. Paper proposes ways to generate training data to cover different aspects of the deductive reasoning.\n4. Describes ontology aware training objective description that can be easily incorporated into existing embedding methods(Q2B and O2B) and show it two existing methods.\n5. Benchmark datasets creation methodology and the two datasets introduced based on LUBM and NELL can be useful to the research community in pushing complex query answering research.\nWeakness:\n 1. Experimental results on newly introduced modification O2B by modifying Q2B does’t seem to yield consistent results across all the scenarios. Any insights around this ?",
            "summary_of_the_review": "Overall the paper is well written with new benchmarks and novel way to do query answering to support deductive and inductive reasoning. \nInitial experimental results show good gains for the new benchmarks and scenarios introduced. \nThis will open up new research to push the inductive and inductive reasoning for KG embedding methods. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Authors study the problem of ontology-mediated query answering (OMQA) over knowledge graphs (KGs). The problem definition is as follows: given an ontology,  an incomplete KG, and a monadic (positive) conjunctive query (CQ) which does not have a match in the given KG, predict/rank/score the most likely answers to the query. The proposed approach largely builds on existing work of Query2Box on answering monadic CQs over incomplete KGs, while extending this approach to make the embeddings, and hence their predictions, \"ontology-aware\".",
            "main_review": "*Strengths:*\n\nThe problem is meaningful, essentially importing the idea of OMQA to the domain of embedding-based approaches: one is interested in query answering over incomplete KGs using an embedding-based approach, while additionally making use of a logical background theory. This is commonly referred as rule injection in the context of KG embedding models. It is widely studied for, e.g., link prediction - performing link prediction while incorporating a set of given rules. \n\n*Weaknesses:*\n\nWhile the problem formulation is meaningful, the presented study appears rather shallow both theoretically and empirically. After describing the problem setup, authors state the following alternatives for solving the task: \n1. Train existing embedding models for logical QA on the data derived from the deductive closure of $G$ instead of $G$;\n2. Develop an ontology-aware embedding model that will be trained on $G$, but will have special terms in the training objective structurally enforcing $O$.\n\nThere is an obvious alternative which is completely missed: consider existing embedding models developed for link prediction, which possess rule injection capacity (i.e., can inject (subset of) rules of $O$) and train these following (Arakelyan et al., 2021)  for QA. The resulting model will be ontology-aware by construction, though disjoint from approaches (1) and (2). For example, BoxE can inject all rules from Fig 5 excluding (5). Since by (Arakelyan et al., 2021) it is possible to turn any link prediction model to a QA answering model, one would obtain an ontology-aware QA model. It is easy to see that this approach would result in much stronger baselines that the ones provided and so the presented results cannot be conclusive without such an analysis. Hence, the empirical evaluation is limited. Besides, I had a hard time to understand the experimental setup and the results are not discussed in detail: In many instances, it is not clear to me what to make out of these results, and there is generally a lack of insights.\n\nThis work is very incremental on Query2Box and lacks novelty.  The authors only modify the loss function and the query sampling strategy. In my understanding the main challenge in this approach is to incorporate the rules and there is a large body of work on this. While support for rules is generally limited, this work does not offer anything new in this respect either. In fact, the whole approach uses the Query2Box for model representation which itself has limitations in incorporating rules. It is possible to come up with examples, where Query2Box would fail to inject the intended meaning of the rules, and the current paper does not add anything new in this respect and neither does it include a formal analysis discussing their limitations.\n\nAnother obvious problem with the current approach is that it follows Query2Box approach and so is based on query sampling. (Arakelyan et al., 2021) obtain state-of-the-art results by using a simple framework requiring only neural link predictors trained for atomic queries, rather than millions of queries as in Query2Box. I could not find any strong arguments to justify a query sampling approach which comes with many additional problems/challenges. \n\nSome minor issues:\n\n- A knowledge graph is not the same as an ABox. There are semantic differences. (I understand what authors mean here, but it is misleading).\n- The presented monadic CQs are actually monadic positive CQs.\n- In Prelim's X is a variable, and $\\vec{Y}$ is a sequence of variables, and their union is not well-defined.\n- In Def 1, the term \"ideal completion\" is undefined.\n- In Def 1, ontology-awareness is not well defined (one direction of iff is missing)? ",
            "summary_of_the_review": "In light of the major weaknesses listed in my review, I recommend reject.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}