{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies programming by example problems from the visual inputs, and they aim to learn the programs in an unsupervised way, i.e., the program annotations are not available. Their approach includes 2 key design choices: (1) modularized neural network parameterization for predicting functions and their parameters; and (2) modified MCTS to search for programs with large parameter spaces. For evaluation, they construct a new benchmark of visual programs, where the input format is inspired by the Abstraction and Reasoning Corpus (ARC). They demonstrate that their approach outperforms enumerative search, and the performance gap compared to the model trained with supervised learning is reasonable.",
            "main_review": "In general, I think this work studies an interesting and important problem. Specifically, unsupervised program induction is highly challenging. While I understand that the complexity of programs that can be learned in an unsupervised way could still be limited, some claims about the program complexity in this work are not precise, and there are some missing evaluation and discussion, as detailed below.\n\n1. Firstly, while the proposed benchmark has some interesting characteristics, e.g., the functions have moderately large parameter spaces, the programs in this benchmark are not necessarily more complicated than those studied in prior works. For example, although the Karel benchmark studied in the neural program synthesis literature (Bunel et al., 2018; Devin et al., 2017) does not have parameters for functions, the predicted programs have control flow constructs, e.g., IFELSE, WHILE and REPEAT, and the model needs to predict conditions for such constructs. On the contrary, the proposed benchmark does not have any control flows, and the length is pretty short.\n\n2. For evaluation, I would encourage the authors to directly evaluate on ARC or at least a subset of ARC, instead of constructing a benchmark themselves. I note that the constructed benchmark is pretty small: usually at least we expect the dataset to include >100K samples, e.g., the Karel benchmark includes >1M training samples. Increasing the training set might also improve the accuracy. If there is any reason why the approach is not applicable to the full ARC and/or larger training and test sets, please provide a detailed discussion about the limitations as well.\n\n3. How do you measure the prediction accuracy? Do you compare the syntax match of the predicted program to the ground truth, or do you compute the execution accuracy by comparing the program outputs? I notice that there is only 1 input-output pair as the program specification, while existing programming by example benchmarks usually use more input-output pairs per program. Therefore, I wonder whether increasing the number of input-output pairs per program would also affect the measurement of the model performance.\n\n4. What are the results of Hit @1? I wonder whether these results still look fine for the proposed approach.\n\n5. The authors did not compare with strong baselines. At least the authors should try some RL baselines, instead of only comparing to enumerative search methods.",
            "summary_of_the_review": "This work studies an interesting and challenging problem of unsupervised program induction from visual inputs. However, some claims about the program complexity are imprecise. Meanwhile, the scope of the programs that can be searched by the proposed approach is not explained well.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a method for unsupervised visual program induction where the key insight is to split the inference in two stages - first search for the right set of functions and then find the right parameters for these functions. The authors demonstrate that their method performs well on a single task inspired from the ARC dataset.",
            "main_review": "The paper explores two fairly well established techniques in the program induction field:\ni. Searching - search is at the core of program induction and so all sorts searching techniques have been proposed over the years [1] including MCTS which is the chosen algorithm by the authors. \nii. Parametrization - finidng the coarse functional structure of a program and then optimizing/tuning function parameters is also another fairly standard method\n\nI do find it difficult to understand the core contributions made in the paper as the proposed techniques are fairly standard ones. The authors claim their method works with \"complex functions\" on \"challenging visual scenes\", however there are other works that have achieved much more promising results in terms of dealing with complexity [2, 3] which are not mentioned in the related work section.\n\nI find the assumed maximal program depth of 3 to be a severe limitation of the proposed method and certainly does not justify the claim of working with \"complex functions\". Moreover, the proposed method is applied to a single task inspired by the ARC dataset, but much of the abstract reasoning aspect has been replaced simply with drawing shapes. For that purpose, a solution based on neural rendering would most likely be much more performant and efficient.\n\n[1] Gulwani, S., Polozov, O. and Singh, R., 2017. Program synthesis. Foundations and Trends® in Programming Languages, 4(1-2), pp.1-119.\n[2] Ellis, K., Ritchie, D., Solar-Lezama, A. and Tenenbaum, J.B., 2017. Learning to infer graphics programs from hand-drawn images. arXiv preprint arXiv:1707.09627.\n[3] Ellis, K., Wong, C., Nye, M., Sable-Meyer, M., Cary, L., Morales, L., Hewitt, L., Solar-Lezama, A. and Tenenbaum, J.B., 2020. Dreamcoder: Growing generalizable, interpretable knowledge with wake-sleep bayesian program learning. arXiv preprint arXiv:2006.08381.",
            "summary_of_the_review": "Overall, I find the work in the paper as great first toward solving the Abstract Reasoning Challenge, but much more remains to be done in order to truly address the complexity of the visual scenes their and find a program induction based approach for solving it. Given the fairly standard setup for program induction and the limited experimental results I recommend the paper to not be published. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper addresses the problem of Abstraction and Reasoning Corpus program synthesis from input/output pairs. Specifically, this work aims to infer a sequence of functions and their parameters (e.g. BORDER[0, 0, 2, 3, RED], HLINE[1, 2, 2, BLUE]) that can transform an input pattern into an output pattern. To this end, the paper proposes a framework that decomposes the problem into inferring functions and predicting their parameters and then utilizes Monte Carlo Tree Search (MCST) to explore and obtain training data. The experimental results show that the proposed method outperforms pure search methods including beam search, DFS, and MCST. Ablation studies suggest that the proposed function and parameter decomposition yields better training efficiency and the proposed method can deal with programs that are longer than the training programs. I believe this work studies a promising research direction and proposes a reasonable method to address the problem. Yet, I am concerned with the unclear presentation of the proposed method, the limited novelty, missing discussion with relevant prior works.",
            "main_review": "## Paper strengths and contributions\n**Motivation and intuition**\n- Studying program synthesis from input/output patterns is promising.\n- The motivation for exploring synthesizing programs with parameterized functions is convincing.\n\n**Novelty**\nTo the best of my knowledge, utilizing MCTS to obtain programs and intermediate states for training is novel. This paper presents an effective way to implement this idea.\n\n**Ablation study**\n- Ablation studies are helpful for understanding how the proposed function and parameter decomposition could yield better training efficiency and how well the proposed method, DFS, and the supervised method can deal with programs longer than the training programs.\n\n**Experimental results**\n- The presentation of the experimental results is clear.\n- The experimental results show that the proposed method outperforms pure search methods including beam search, DFS, and MCST. \n\n**Reproducibility**\nGiven the clear description in the main paper and the details provided in the appendix, I believe reproducing the results is possible. \n\n## Paper weaknesses and questions\n\n**Program complexity**\nIn Figure 1, the authors argue that Karel programs are simple since there are only four possible action tokens in the Karel domain-specific language (DSL). However, all the works that I am aware of that use Karel programs actually focus on Karel programs with control flows such as if, else, while, repeat, etc. The difficulty of synthesizing such programs from input/output state pairs or demonstrations involves not only inferring the action sequences but also the conditions that cause the behaviors to diverge. Therefore, it is not reasonable to compare the complexity of programs simply based on the possible actions/operations in a DSL.\n\n**Clarity: the proposed method**\nThe description of the proposed method in the abstract and the introduction is unclear. There are many jargons and undefined terms such as \"modularized functions\", \"a function planning mechanism\". I can only understand them after reading the method section.\n\n**Complex functions**\nThis paper emphasizes many times that it is the first paper studying program synthesis with \"complex functions\". However, many prior works that explore synthesizing string transformation programs (such as \"RobustFill: Neural Program Learning under Noisy I/O\" by Devlin et. al. and \"Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis\" by Bunel et. al.) also deal with parameterized functions.\n\n**Infer program sketch**\nI believe that the proposed function and parameter decomposition is not novel as there are prior works exploring first synthesizing program sketches and then filling in their parameters, such as \"Learning to Infer Program Sketches\" by Nye et. al.\n\n**Unsupervised learning method**\nThe authors frame the proposed method as an unsupervised learning approach. Yet, I find it very confusing. While the proposed method does not directly assume access to the ground-truth programs, it obtains intermediate patterns and programs using MCTS and learns from them, which in my opinion still makes it a supervised learning framework.\n\n**Hit@50 and AvgTime of SEMF+{Beam, DFS, MCTS}**\nTraining Hit@50 and AvgTime are the same across all SEMF+{Beam, DFS, MCTS}?\n\n**T_max = 3**\nI feel assuming knowing T_max=3 is unrealistic.\n\n**Real/goal-oriented testing programs**\nThe testing programs used to evaluate the performance of the proposed method and the baselines are randomly generated. I believe it would be more interesting and convincing to evaluate the performance using some real (deliberately created by humans) or goal-oriented testing programs.\n\n**Novelty**\nOverall, I do not find enough novelty from any aspects while the overall effort of this paper is appreciated. \n\n## Other metrics\n\n### Relevance and significance\nReasonable contribution to a minor problem\n\n### Novelty\nMinor variations to existing techniques\n\n### Technical quality\nClaims not completely supported, assumptions or simplifications unrealistic\n\n### Experimental evaluation\nInsufficient or lacking evaluation in 1-2 criteria, but sufficient w.r.t. the other criteria\n\n### Clarity\nThis paper is difficult to understand in places because of typos, lack of organization, or other flaws. The paper would need significant/major improvement in writing.",
            "summary_of_the_review": "I believe this work studies a promising research direction and proposes a reasonable method to address the problem. Yet, I am concerned with the unclear presentation of the proposed method, the limited novelty, missing discussion with relevant prior works.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes a new method called Self-Exploratory-Modularized-Function (SEMF), a model coupled with MCTS, to solve the unsupervised visual program induction problem with a large primitive space. Specifically, the authors adopt a learning-and-searching approach, where in the learning part, primitive functions are modeled as a composition of the function class space and the function parameter space, and in the searching part, a modified MCTS is introduced to perform search and supply the learning sub-process with training data. The proposed method is tested on a specially generated dataset of visual input to test program induction without ground truth program supervision. Compared to other baselines, the proposed method shows notable improvement on hit rate and efficiency.",
            "main_review": "Using a learning-and-search-based method to address the large search space problem is a reasonable choice in program induction. Besides, the authors further consider decomposing the function space into the function class space for planning and the function parameter space for prediction. The idea of compositionality should be ideal in reducing the search space compared to simply \"flattening\" the functions. From this perspective, the methodology adopted in this work makes perfect sense. However, I would also like to point out that the approach is only incremental as learning-and-search is not new and neither the idea to decompose.\n\nThe modified MCTS also contributes to the improvement by properly constraining the search space.\n\nHowever, I'm also very confused on the definition of \"complex\" functions. According to the authors, they are defined as functions with parameters. Such functions have already been considered in earlier works and I do not think the major characteristics of the tasks considered in this work is functions with parameters, but rather a search space that is too large for basic search. Therefore, I would encourage the authors emphasize the search space size instead of functions being complex or not.\n\nAnother major reason for not giving a higher score is the limited scope of the experiments. While SEMF is effective in this task, what about performance of existing learning-and-searching methods? Is it your MCTS that actually contributes to your improvement while the base learning method does not make significant difference? DreamCoder has also been extensively used in program induction, how about its performance in the dataset your benchmark on? What about testing directly on ARC or PQA? What is the reason for introducing and testing on a new dataset without much empirical results rather than using an existing one?\n\nCan you give an intuitive explanation on why the proposed method works? And how can you apply the approach on a large search space just because of the number of primitive functions instead of a small number of primitives and a large product space of parameters?\n",
            "summary_of_the_review": "The work details a reasonable approach to address the large search space problem with learning, leveraging the idea of composition and model-based search. Despite of some issues, the contribution is practical (unless I missed some important points raised by other reviewers).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}