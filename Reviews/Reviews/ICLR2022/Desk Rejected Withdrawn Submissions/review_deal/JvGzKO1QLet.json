{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper extends the existing methods for learning causal discovery methods for videos where's the underlying causal structure is changing at each time-step. The paper evaluates the proposed method on two video prediction datasets. Experimental results show that the proposed method achieves better performance as compared to the various baselines considered. ",
            "main_review": "\nStrengths:\n\n- The paper is well written and tackles an interesting problem.\n- The authors extend the previous intervention-based causal discovery framework to non-stationary video sequences.\n\nWeakness:\n- It may be more useful to evaluate the proposed method as to how it performs on the downstream tasks (on more complex datasets) as compared to multi-step prediction methods (for example. following similar setup as in [1]).\n- Mean and variance across different seeds for all the results in table 1 and table 2 are not mentioned.\n\n[1] Attention over learned object embeddings enables complex visual reasoning, https://arxiv.org/abs/2012.08508\n\nRelated Work:\n- Some of the related work is not mentioned. For example: RIMs/NPS consists of an ensemble of \"variables\"  interacting with each other via self-attention. The \"graph\" is dynamic and dependent upon the time-step.\n\n[1] RIMs, https://arxiv.org/abs/1909.10893\n[2] NPS, https://arxiv.org/abs/2103.01937\n\nMinor Comments: These comments do not affect my rating.\n\n- In the introduction, the paper writes \" Brouillard et al. (2020) (DCDI recently proposes a differentiable causal model for a spatial graph to naturally capture the abrupt change of probability distributions during interventions\". This is slightly misleading. DCDI builds upon SDI (Ke et. al, 2019) [1] which authors have already cited in the paper.  Similarly, the idea of sampling from bernoulli distribution was used in SDI for learning the underlying causal structure.  \n",
            "summary_of_the_review": "The paper tackles an interesting problem. The reviewer is not convinced of experimental protocol and comparisons as compared to the other methods. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The authors propose a new model, called Intervention-based Recurrent Causal Model (IRCM), to discover causal structures of nonstationary video data. The model consists of two modules: recurrent network (RN), which is used to sample DAG structure and intervention set, and intervention-based causal model (ICM), which predicts the mean and covariance of multivariate Gaussian distribution for next observation and intervention sets. They evaluate the proposed method on two physical system simulation datasets in terms of both the counterfactual reasoning and future forecasting tasks.",
            "main_review": "- The example used for illustration of nonstationary causal structures might be inappropriate. Since the edges indicate interaction between objects in physical systems, that is, each edge is bidirectional, the resulting structure cannot be simply seen as a causal graph.\n\n- In the abstract, the authors claim that they extend the existing intervention-based causal discovery framework for videos to formulate *the instantaneous change* of the causal structure. However, in the subsequent sections (e.g., section 3.2) they also say that \"they neither consider the instantaneous edges nor edges that go back in time in this work.\" This might be a bit unclear and confusing. The authors should explicitly explain what the instantaneous change is.\n\n- In this work, the authors assume the non-Markovian setting and thus adopt a recurrent network to accumulate all the previous information for predicting the causal graph. This could introduce some spurious correlations/edges to the predicted graph. how to fix it?\n\n- Throughout the paper, the authors assume that all the variables, from which the causal structure is inferred, are given. However, this is not the case in most if not all vision tasks. It is intuitive to imagine that the badly-learned variables would, without doubt, lead to the unsatisfactory performance. It seems that this kind of issues are neither discussed nor even mentioned in the paper.\n\n- In Figure 2e, why are there two different $\\mathcal{M}^{1\\rightarrow 2}$? In $I^1$, if I understand correctly, it should indicate an intervention on $x_1^3$. If so, then why is there still an arrow entering it? So does $I^2$. Do their matrix representations in Figure 2e correspond to the causal graphs in Figure 2d?\n\n- Subscripts and superscripts are inconsistent throughout the paper. E.g., in Figure 2 I believe the subscripts indicate the time step, whilst the  superscripts indicate the time step at some other places. \n\n- In Figure 3, I think there also exist some arrows from $I^t$ to $\\mu^t$ and $\\Sigma^t$ and to $\\tilde{\\mu}^t$ and $\\tilde{\\Sigma}^t$. Because it is $I^t$ that leads to $\\phi$ and $\\psi$. At the end of this caption, it should be \"resulting in\", instead of \"resulting\".\n\n- In Section 3.2, the authors introduce a new concept of *agent*. What does it mean? sources? states? It would be a bit confusing to introduce something new but without any explanation. Also, the authors do not consider the instantaneous effects in the paper. I am wondering how practical this assumption is in the real world applications. Additionally, the authors claim that \"this feature makes our causal graph fully identifiable in the context of video sequence\". What does \"identifiable\" mean here? In what sense is it fully identifiable? All these are not that straightforward and need more clarification. \n\n- What is the range of K of the intervention family? Why do you use $k=2$, i.e., assume only one intervention family in this paper? What is the dimension of $\\boldsymbol{h}^t$ and how to determine it?\n\n- All elements of $\\mathcal{M}^t$ and $I^t$ are assumed to be mutually independent, which will lead to a huge search space when the number of variables is large. How to address this issue? \n\n- It seems that the regularization on DAG does not have guarantee on NO instantaneous causal relations. Therefore, I am wondering how the objective (15) work in this regard.\n\n- The authors use the extracted visual features from input videos in the previous state-of-the-art method to improve the model performance. If I understand correctly, each dimension of the extracted features will be seen as one variable, and the goal is to discover the causal structure over all these variables(i.e., dimensions), right? Then, this will go back to my previous question: what if \"the bad features\" are learned? \n\n- I would like to see more complex experiments on more realistic datasets. For example, it would be better to do some experiments on the simulated data with some distractors by replacing backgrounds with some natural videos [1]. The present experiments are too simple to verify the performance of the proposed approach.\n\n- Last but not least, I did not see any explicit assumptions on how nonstationary data change (e.g., which mechanism varies in time and which not, etc.). Without such assumptions over the underlying generative process, apparently it is generally impossible to predict the future observations. This is the most fundamental issue which has to be addressed or clarified in the paper. \n\nReferences:\n\n[1] Zhang et al. Learning Invariant Representations for Reinforcement Learning Without Reconstruction. 2021.",
            "summary_of_the_review": "Generally speaking, I think some technique details in the paper are unclear or even incorrect, which needs to be further clarified or corrected. The technical novelty is not that much because many are inspired by or draw on the work of Brouillard et al., 2020. Also, as aforementioned, I have some doubts about the feasibility of the idea of predicting the future observations in the nonstationary setting, without any explicit assumptions over the underlying generative process. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Causal discovery for video applications is an important problem. This paper focuses on discoverying non-stationary causal structures in real-world physical systems.  It proposes the Intervention-based Recurrent Casual Model (IRCM) which extends the existing intervention-based casual discovery framework for videos. Then it uses a recurrent model to sequentially predict the causal structure model based on previous observations. The method are evaluated on two physical system simulation datasets.",
            "main_review": "Strengths\n\nThe paper extends the previous intervention-based causal discovery framework to non-stationary video sequences (IRCM).\n\nIt applies recurrent networks to capture the long-term trajectory of causal graph models.\n\nIt achieve outperforms prior methods, CoPhyNet V-CDN on two downstream tasks: counterfactual reasoning and future forecasting on two standard benchmark datasets.\n\nWeaknesses\n\nThe key novel contribution seems to be the use of  recurrent networks that enables the sampling of DAGs.\n\nThe framework assumes there are no confounders which could be very limiting for real-world applications.\n\nThe framework does not assume ground truth causal graphs are available. It is not obvious to me Eqn. 15 can effectively learn the causal structure. \n\nThe evaluation also does not provide direct evidences that the framework can discover true causal graphs.\n",
            "summary_of_the_review": "My main concerns are the novelty and correctness. The main novelty seems to be the use of recurrent neural networks, Eqn. 4. It is not clear to me how Eqn. 4 can turn raw images into probability values (αt, βt) that one can sample the graph. Explaining using the motivating example in Figure 1 could clarify this. What makes it not learning spurious correlations? \n\nThe framework seems to be very problematic. It does not assume ground truth causal graphs, therefore there is no direct supervision. It also does not show direct evidences from experiments that the causal graphs are learned. It is quite puzzling why this is causal discovery. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper targets the problem of causal discovery in dynamic environments. Their motivation is that previous methods do not consider the non-stationary property of the environment or do not use the intervention tool. In their method, they combine Recurrent Neural Networks and Intervention-based Causal Model (ICM) to build a model named Intervention-based Recurrent Casual Model (IRCM). Due to the difficulty of evaluating causal graphs, the authors use two popular downstream tasks to benchmark the efficacy of the proposed model: counterfactual reasoning and future state forecasting.",
            "main_review": "1.\tGeneral Strengths and weaknesses\n\n\\+ The problem studied in this paper is definitely important in many real-world applications, such as robotics decision-making and autonomous driving. Discovering the underlying causation is important for agents to make reasonable decisions, especially in dynamic environments.\n\n\\+ The method proposed in this paper is interesting and technically correct. Intuitively, using GRU to extract sequential information helps capture the changes of causal graphs.\n\n\\- The main idea of causal discovery by sampling intervention set and causal graphs for masking is similar to DCDI [1]. This paper is more like using DCDI in dynamic environments, which may limit the novelty of this paper.\n\n\\- The paper is not difficult to follow, but there are several places that are may cause confusion. (listed in point 3). \n\n\\- The contribution of this paper is not fully supported by experiments.\n\n\n\n2.\tMain Questions\n\n(1) During the inference stage, why use samples instead of directly taking the argmax of Bernoulli distribution? How many samples are required? Will this sampling cause scalability problems?\n\n(2) In the experiment part, the authors only compare with one method (V-CDN). Is it possible to compare DYNOTEARS with the proposed method? \n\n(3) The authors mention that there is no ground truth to evaluate the causal discovery task. I agree with this opinion since the real world does not provide us causal graphs. However, the first experiment is conducted on a synthetic dataset, where I believe it is able to obtain the causation by checking collision conditions. In other words, I am not convinced only by the prediction results. Could the author provide the learned causal graphs and intervention sets and compare them with ground truth even on a simple synthetic dataset?\n\n\n3.\tClarification questions\n\n(1)  It seems the citation of NOTEARS [2] is wrongly used for DYNOTEARS [3]. This citation is important since DYNOTEARS is one of the motivations of this paper.\n\n(2)  ICM part in Figure 3 is not clear. How is the Intervention set I is used? If I understand correctly, function $f$ is a prediction model conditioned on history frames. \n\n(3)  The term “Bern” in equation (3) is not defined. I assume it is the Bernoulli distribution. Then what does the symbol $Bern(\\alpha^t, \\beta^t)$ mean? \n\n(4)  According to equation (7), each node $j$ has its own parameters $\\phi_j^t$ and $\\psi_j^t$. Could the authors explain why the parameters are related to time?\n\n(5)  In equation (16), the authors mention the term “ secondary optimization”. I can’t find any reference for it. Could the author provide more information?\n\n\n\n4. Minor things:\n\n(1)\tIn the caption of Figure 2, the authors say “For nonstationary causal models, (c)….”. But in figure (c) belongs to stationary methods.\n\n\n---\n[1] Brouillard P, Lachapelle S, Lacoste A, et al. Differentiable causal discovery from interventional data[J]. arXiv preprint arXiv:2007.01754, 2020.\n\n[2] Zheng X, Aragam B, Ravikumar P, et al. Dags with no tears: Continuous optimization for structure learning[J]. arXiv preprint arXiv:1803.01422, 2018.\n\n[3] Pamfil R, Sriwattanaworachai N, Desai S, et al. Dynotears: Structure learning from time-series data[C]//International Conference on Artificial Intelligence and Statistics. PMLR, 2020: 1595-1605.\n",
            "summary_of_the_review": "I vote for rejecting this paper. Please find my concerns in the main review part.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}