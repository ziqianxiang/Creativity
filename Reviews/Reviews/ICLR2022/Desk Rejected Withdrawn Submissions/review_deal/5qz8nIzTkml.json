{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes an algorithm to learn a fair predictor under the sampling bias. In particular, there covariate shift between training and test data. The algorithm tries to estimate the distribution of the data using the Horvitz-Thompson estimator. \n\nUsing the estimated distribution, the authors add the fairness constraint as a penalty to the objective function and solve the problem using a mini-batch gradient approach. \n\nThe main contribution falls into estimating the distribution of test data and finding the fair predictor under the covariate shift. \n\n ",
            "main_review": "Strengths:\n\nThe paper introduces an approach to estimating the fairness constraint under a covariate shift. \nIt also provides various numerical examples to demonstrate the performance. \n\nWeaknesses: \nThere are no theoretical results on the performance of the proposed method. The proposed method may not work all the time. For instance, in figure 2 and figure 6, when tau=0.5, the unconstrained approach archives higher accuracy and better fairness compared to the proposed method with delta = 0.2\n\nThe proposed method can improve fairness at the cost of decreasing accuracy. If we look at Figures 2 and 6, we realize that the proposed method decreases accuracy significantly. \n\nIt is not clear what does unconstraint methods mean. There is no citation for the unconstraint methods used as a baseline. Does the unconstrained method mean that the fairness constraint is removed or it means that the fairness constraint is added to the objective function as a penalty? \n\nI also want to ask the author to educate me about the difference between biased training data set and a biased test dataset. My understanding is that the paper considers covariate shifts. Covariate shift means that the distribution of feature vectors is different in the test and training datasets. however, \\Pr\\{Y|X\\} remains the same in both datasets. In this setting, when should we call the training dataset biased? when should we call the test dataset biased? \n\nThere are other works about fair learning under covariate shift (e.g., Robust Fairness under Covariate Shift by Rezaei et al.). Could you please elaborate on the advantage of your method compared to the work by Rezaei et al? ",
            "summary_of_the_review": "\nThe proposed method for fair learning under biased sampling is a heuristic method and it may improve fairness at the cost of decreasing accuracy. Also, in a few cases, the baseline method may achieve better fairness and higher accuracy. Given this observation, I believe the paper is lower than the acceptance threshold of ICLR. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper develops a fair machine learning framework that is robust to the sampling bias in the training data. They argue that when sampling bias exists a machine learning algorithm that is \"fair\" in the training set may not be fair in the test dataset. They impose the fairness constraints to a classification model using $L_q$-norm and propose a robust fair AI algorithm that improves the existing fair AI algorithms. ",
            "main_review": "\n### Major Comments:\n\n1) The definitions of $P^{tr}$ and $P^{te}$ are not clear. \nIf these are the empirical distributions over the samples of the training and the test datasets, then I am not sure how $P^{te}$ would be equivalent to $P^{tr}$.\nIf this is the case, the equivalence would be only satisfied if the training and the test datasets are the same.\nIs it possible that you mean both $P^{tr}$ and $P^{te}$ are estimators of an unknown true data generating distribution, and on average the empirical distributions of the training and the test datasets are not equivalent?\nWhat are the definitions for $p^{tr}(y, \\bm x)$ and $p^{te}(y, \\bm x)$? However, this is not what I understand from the paper.\nDoes the weight function in $s(\\bm x)$ has a similar meaning with the target-source density ratio in domain adaptation literature?\nThis paragraph is mathematically incomplete and far away from being rigorous.\n\n2) Please provide an exact definition of stratified sampling and why it causes sampling bias.\nThe definition for sampling bias provided in Mehrabi et al. (2019) mentions that the sampling bias arises when one group is frequently sampled than another group.\nIf I am not mistaken, stratified sampling is also used to retain the marginal probabilities of each subgroup in the sampled training set.\nI would suggest the authors add a comprehensive discussion on why stratified sampling causes sampling bias that is explained in Mehrabi et al. (2019).\n\n3) Overall Section 3 of the manuscript is incomplete and is not rigorous mathematically. I would suggest the authors revisit this section.\n\n4) I think the definition of $\\hat F_{fair, \\delta}$ is not provided, or is there a typo in (1)?\n\n5) \"A standard gradient descent algorithm with approximating .... works well.\" I am not sure what does this sentence means (works well compared to what?)?\nPlease provide a reference for that.\n\n6) What is the convergence rate of Algorithm 1?\nI am not sure if you can use the convergence rates provided in the literature for the stochastic mini-batch learning algorithms, because the gradient estimates are biased.\n\n7) The following sentence does not have any meaning mathematically, \"Our numerical study\nindicates that the Lq robust fairness constraint performs well for q \\in [1.5, 2] provided that Q(n) is selected carefully\"\nWhat does performing well mean? In terms of accuracy-fairness trade-off? \nI am not sure what does the authors intended to mean with \"provided that Q(n) is selected carefully\"?\n\n8) Please provide comparisons with other fair-robust methods in the literature. For example, (arXiv:2010.05166, Taskesen et al (2020)).\n\n### Minor Comments:\n\n1) I would suggest the authors provide the definition of the $L_q$ norm before (3).\n2) There is a typo at the end of Page 2. \"if If\" should be \"if\".",
            "summary_of_the_review": "Even though the paper studies a very interesting problem, I do not think that the mathematical claims in the paper are rigorous. The main concepts (e.g., sampling bias, stratified sampling) are not explained in detail. I think this paper has room for improvement and I encourage the authors to revisit the mathematical details and the presentation of the paper. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper studies fairness in AI where the unfairness is caused by sampling bias, which occurs when the population is not represented well in training data so that the probability distribution of training data is not aligned with the distribution of testing data. It develops a computationally efficient learning algorithm that is robust to sampling bias.  \n\n",
            "main_review": "1. ML models can be biased for many reasons. Sampling bias is only one of them. The paper only focuses on this type of bias, which may limit the adoption of the method. \n\n2. The paper proposes to use a bias function to model the sampling bias, which is assumed to be known and depends only on features $x$ but not $y$. It is not clear how strong these assumptions are and how reasonable the model is for capturing sampling bias. \n\n3. The paper presents an approach to training a model robust to sampling bias. However, there is no theoretical support for the approach. It’s not clear why the algorithm works and what the guarantees are for the robustness. Since the paper only considers sampling bias, it’s hard to claim the proposed method can outperform the existing approaches without analysis. \n\n4. The experiments show comparisons with two in-processing approaches and an unconstrained approach. The unfairness and accuracy of these methods are compared separately in two figures. It’s not clear the proposed method outperforms the existing methods. Although the unfairness is less under the proposed method (Figure 2), the accuracy is also less (Appendix A.4). Since there is a trade-off between fairness and accuracy, I suggest authors compare the trade-off by fixing one and comparing the other. \n\n5. Many in-processing approaches have been proposed in the literature, and authors only compared with two. Because the proposed approach lacks theoretical support, more extensive experiments need to be conducted to show the method's effectiveness.  I suggest authors compare with more methods such as “Fairness Constraints: A Flexible Approach for Fair Classification”, “A Reductions Approach to Fair Classification,” etc.\n\n6. Most of the existing methods for mitigating biases work well and they are not limited to sampling bias. What makes the approach better than the existing methods?",
            "summary_of_the_review": "My main concerns are as follows: 1) the paper is limited to unfairness caused by sampling bias; 2) the proposed approach lacks theoretical support; 3) more experiments are needed to show the effectiveness of the method. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes an in-processing fair training algorithm that is robust to sampling bias. If sampling bias exists, the training and test data have different distributions, so a fair AI model on the training data may not be fair on the test data. The sampling bias considered is different from unfair (historical and regulatory) bias and occurs when performing say stratified sampling. Given a fairness measure that can be compared with a threshold, the $L_q$ norm of the expected fairness is added as a regularization constraint for training. Experiments show how the proposed methods outperforms baselines in terms of disparate impact, mean score parity, and individual fairness on real datasets.",
            "main_review": "[Strengths]\n* Fair training under sampling bias is an important problem.\n* Using $L_q$ regularization to make the model training robust to sampling bias is an interesting approach.\n* The regularization supports both group fairness measures and individual fairness.\n* The experiments show that the proposed methods result in good fairness and accuracy for multiple real datasets, even with large sampling biases.\n\n[Weaknesses]\n* Overall, it is not clear which sampling bias is addressed by the proposed techniques. In the abstract and introduction, the authors mention stratified sampling, but then do not mention it much in the following sections. Stratified sampling actually seems like a fair sampling technique where different stratas can be viewed as sensitive groups. In Section 3, the authors list four types of sampling biases -- sampling, self-selection, population, and temporal -- they support, but then do not explain how each of them can be captured in an $s({\\bf x})$ function. The last paragraph in Page 3 then says that unfair (historical or regulatory) bias is not considered. How does this bias really differ from the above biases that are supported? Is there even a way to only debias the other biases? What do you mean by \"the biased probability model shares the same unfair bias and the same Bayes classifier\"? Not sure why we are assuming a Bayes classifier.\n* There should be more detailed comparisons with related work. In Page 2, the authors briefly mention DRO and Mandal et al. (2020), but stop there. Instead, there should be experiments comparing the proposed method with DRO. In addition, there should be a clear explanation how the $L_q$ regularization is a smoothened version of Mandal et al. (2020) in Section 4.\n* A concern is that many derivations leading to the $L_q$ regularization seem to be lacking enough justification. In Section 4.1, paragraph 2, if we do not know the bias function, why is it good to find weights that are close to equal weights while satisfying the fairness constraint? The claim comes out of the blue. In Section 4.2, paragraph 3, it is not clear why most of the mass of $Q^{(n)}$ should be placed near the boundary of $W_\\delta$. Proposition 1 in the appendix only says that the weights are on the boundary if the fairness function $\\phi$ is maximized, but what does that have to do with finding a good $Q^{(n)}$? Also, it is not clear why an inverse-gamma distribution is used. There are other distributions that have more mass near 0. Finally, how efficient is the $L_q$ approximation compared to $L_\\infty$?\n* In the experiments, there should be a discussion on how to tune the three hyperparameters $\\delta$, $q$, and $\\alpha$, which control the fairness-robustness. Although the authors show how each parameter affects the fairness-robustness, can all hyper-parameters be efficiently tuned for other datasets or other types of sampling bias? \n* In the experiments, there should be more comparisons with state-of-the-art in-processing baselines like Zafar et al., \"Fairness constraints:  Mechanisms for fair classification,\" AISTATS 2017.\n* In Page 6, bottom paragraph, why should the prediction model $\\hat{f}$ meet $\\epsilon^q$?\n* The problem solved in this paper seems similar to the domain generalization problem (i.e., out-of-distribution generalization problem), which also tries to improve the test accuracy when the testing distribution is different from the training. (e.g., Wang et al., \"Generalizing to Unseen Domains: A Survey on Domain Generalization\", IJCAI 2021.) It would be helpful to make a clear distinction of the problems if there are differences.\n* Some expressions are not clearly stated.\n   * Page 4: \"The constant $\\delta$ ... could be proportional to the expected amount of sampling bias\" : what is the amount of sampling bias?\n   * Page 4: \"Theoretically, the $L_q$ robust fairness constraints becomes to the robust fairness constraints ...\" : not sure what this means.\n",
            "summary_of_the_review": "While the paper proposes a simple technique for making a model robust against sampling bias, it can also be improved by clarifying the sampling bias that is considered, adding more comparisons with related work, and better justifying its derivations.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concerns.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}