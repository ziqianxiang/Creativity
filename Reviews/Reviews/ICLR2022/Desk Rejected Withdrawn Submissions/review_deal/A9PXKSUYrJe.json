{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper tackles the problem of learning with noisy labels. A new method that better exploits identified confident examples is proposed. The authors employ the loss values in multiple epochs for sample selection. Meanwhile, the mislabeled data are fully used for network parameter updates. Extensive experiments verify the effectiveness of the proposed method in this paper. ",
            "main_review": "**Strengths**\n\n(1) The experiments are comprehensive. The authors conduct experiments on multiple tasks with broad types of label noise. Most of the experimental results are promising. In addition, the ablation study of this paper is provided, which explains the roles of different components of the proposed method. The discussions on results are appreciated.   \n\n(2) The overall writing and organization of this paper are great. The technical details are clearly described. It is easy for readers to follow them.\n\n**Weaknesses and questions**   \n(1) The motivation of this paper is not very clear. Although the authors provide some insights, e.g., the issue of prior label correction methods, the motivation is still not supported well.\n- The issues of loss correction methods. The authors state that the estimation of the noise transition matrix highly relies on an extra clean dataset. However, this statement is not true. In fact, clean data can help the estimation but is not necessary (see, [1] and [2]).  \n[1] Xuefeng Li et al. Provably End-to-end Label-Noise Learning without Anchor Points. ICML 2021.   \n[2] Mingyuan Zhang et al. Learning from Noisy Labels with No Change to the Training Process. ICML 2021. \n- The authors exploit label correction for identified mislabeled examples, which is straightforward. This idea is shared by other methods since sample selection is argued not to use all training examples. Thus, I cannot understand the motivation well.       \n\n(2) The novelty of the proposed method. It seems the proposed method is a simple combination of multiple technologies, though it has promising performance. The authors should reemphasize the novelty of this paper.\n\n(3) Comparison of prior methods. One important concern in this paper is that it lacks the comparison of existing methods. For example, this paper uses the information of multiple epochs for sample selection, i.e., a sliding window. This idea is proposed in prior works on learning with noisy labels, e.g., [3]. Furthermore, the Markov assumption on the sequence is also investigated by [4]. The authors miss these important discussions.  \n[3] Geoff Pleiss et al.  Identifying Mislabeled Data Using the Area under the Margin Ranking. NeurIPS 2020.  \n[4] Xiaobo Xia et al. Sample Selection with Uncertainty of Losses for Learning with Noisy Labels. arXiv:2106.00445.\n\n(4) Minor comments:\n- The explanations of $w$ are not clear.\n- Typos need to be corrected. \n- It is not clear why the proposed method can extract clean hard examples. ",
            "summary_of_the_review": "This paper provides extensive experiments to justify the claim. However, the motivation and novelty are not very clear. Besides, the necessary discussions about related works are missing. The issues need to be addressed carefully. Before rebuttal, I recommend \"reject\".",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a method, called CREMA, for training DNN with noisy labels. This paper utilizes multiple techniques proposed in the past papers. For example, CREMA employs the standard co-training framework and a label-correction approach.\nThe technical highlight of CREMA is its own credibility calculation defined in Eq.(3). This credibility, w(x,y), is comprised of two terms, the stability and consistency of image feature vectors within the last n epochs. Both terms rely on the conditional probability, P(f_t|f_{t-1}, y), which is given by GMM or BMM. \nMultiple comparative studies were conducted with multiple datasets, which are commonly used in the past papers, and showed that CREMA could achieve the SOTA performance. ",
            "main_review": "[Strengths]\nS1. The idea of using the training results within the last n epochs for evaluating the credibility of each sample is not so common.\nS2. The multiple experimental results (probably) prove that the proposed CREMA achieves SOTA performance.\n\n[Weaknesses]\nW1. CREMA can be seen as a method based on the straightforward combination of the existing ideas. As noted above, it employs the standard co-training framework and a label-correction approach. Although the credibility based on the past n epochs is not so common, a similar idea can be found in RoCL[Zhou+]. Of course, the definition and the usage of credibility are different among CREMA and RoCL -- however, their underlying philosophies are very similar. As detailed in [Zhou+], samples with the wrong label will have a more unstable learning history than those with the correct label. Consequently, the technical novelty of this paper is not very high.\n\n [Zhou+]  Robust Curriculum Learning: from clean label detection to noisy label self-correction, ICLR2021\n\nW2. Relating to W1, I cannot understand the theoretical superiority of CREMA over RoCL. Especially, the calculation of credibility in RoCL utilizes the \"moving average\" technique, which is smarter than the sliding window technique. In addition, since there is no experimental comparison with RoCL, it is hard to understand how CREMA is better than RoCL. (Since the rebuttal period is short, I do not want to request the authors to conduct a rapid comparative study; if the authors can show (a) some theoretical limitation of RoCL and (b) CREMA's experimental result showing that CREMA is free from the limitation, it is OK. \n\nW3. The calculation of credibility fully relies on P(f_t|f_{t-1}, y). The paper says that this conditional probability is calculated by GMM or BMM. However, its implementation is not well described. The authors cite [Arazo et al., 2019] for GMM; however, GMM in [Arazo et al., 2019] was used not for modeling the probability of the feature vector t_t but for modeling the loss value distribution. I could not imagine how we could estimate this high-dimensional conditional probability before applying CREMA to actual tasks.\n\nW4. I compared several test accuracies in tables in the present paper to them in [Zhou+] and found too much difference between them.  For example, PENCIL on CIFAR-100 Assym-40% got  63.61 ± 0.23 in [Zhou+] and 32.81 ± 0.23 in this paper. Some convincing explanation for this gap is required. ",
            "summary_of_the_review": "Based on W1, it is still difficult for me to appreciate technical novelty sufficient for an ICLR paper. \nBased on W2 and W4, it is still difficult to confirm the practical usefulness of the proposed.\nBased on W3, I think the proposed method is not very practical. (If it is my misunderstanding, a clear explanation will be required.)\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I do not have any concerns. The authors say that they publish their code upon acceptance. ",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a novel robust method of separating clean and noisy samples from original samples. Based on the roughly separated clean and noisy samples based on data-label pair distribution, e.g. small loss, they especially design a memory based modulation scheme to dynamically adjust the contribution of each sample for selected clean samples in terms of its historical credibility sequence during training to alleviate the effect from potential hard noisy samples in clean set. Besides, for noisy samples, they further attempt to correct them to employ all samples' information. The entry point is nice and potential for the community.",
            "main_review": "Strengths:\n-  A novel algorithm against noisy labels is proposed to help distinguish clean and noisy samples. Partically, a detailed attention is paid on hard noisy samples.\n-  They proposed to help distinguish hard noisy samples based on the likelihood estimation of historical credibility sequence.\n-  Extensive experiments on noisy datasets across various types and levels of label noise have verified the effectness of the proposed method.\n\nConcerns:\n- The key concern about the paper is the lack of strong baseline experimentation, e.g., DivideMix [A], ELR,  to study the usefulness of the proposed method. As we known, DivideMix uses same strategy to overcome noisy label problem. They use a Gaussian mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples. They use semi-supervised technique to help further employ the noisy samples. They obtain SOTA results on the noisy label benchmarks. E.g., they obtain 92.9% test accuracy on CIFAR-10 with 80% symmetry noise. While the proposed method only has 29.66% test accuracy. I think this results do not support the effectiveness of the proposed methods. \n- The paper does not consider the noisy parts with clean labels. It just consider to deal with clean parts with noisy labels.\n-  Considering the limited presented results of MNIST in Fig 1 and 3, a more analysis of the proposed method on other datasets is necessary. Furthmore, the learned weights for clean samples should be presents to show the effectiveness of the proposed methods.\n-  The results would have been more improvements, if results were shown in a setting with feature-depend noise [C]. The hard noisy samples is more remarkable under this setting. \n\nMinor comments:\n- Ref \"Learning from massive noisy labeled data for image classification\" and \"Distilling effective supervision from severe label noise\" appears twice in the reference list.\n- What the meaning of $f_1$ and $f_2$ in Eq.(9).\n\n\n\n\n\n\n\n\n\n\n\n\nRefs:\n[A] Li J, Socher R, Hoi S C H. Dividemix: Learning with noisy labels as semi-supervised learning. In ICLR, 2020.\n\n[B] Liu S, Niles-Weed J, Razavian N, et al. Early-learning regularization prevents memorization of noisy labels. In NeurIPS, 2020.\n\n[C] Zhang Y, Zheng S, Wu P, et al. Learning with feature-dependent label noise: A progressive approach. In ICLR, 2021.",
            "summary_of_the_review": "To distinguish clean and noisy samples in noisy label problem is very important to improve the performance.  Although the paper proposed a new method to help distinguish hard noisy samples, the experiments is not adequate to verify the effectiveness of the proposed methods. I think the current version of the paper is not well prepared for publishing on ICLR. Overall, I vote for rejecting the paper.\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}