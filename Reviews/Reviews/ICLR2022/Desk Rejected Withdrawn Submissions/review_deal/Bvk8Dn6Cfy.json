{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies domain adaptation and generization of medical image segmentation by by leveraging domain-invariant image translations. In detail, a GAN is trained to generate a universal domain, while a segmentation consistency loss between the source and universal predictions is employed to preserve the semantic consistency. Experiments are conducted on the M&Ms dataset. Some qualitative results are provided.\n",
            "main_review": "The application of domain adpatation and generation into medical image analysis is interesting and important. However, there are many serious issues of this paper that enforce me to reject this paper:\n1. Title: this paper focuses on medial image segmentation, which should be reflected in the title. Otherwise, readers will consider this paper as a semantic segmentation but there is no such experiment.\n2. Introduction and related work: there are indeed some closely related methods that are not discussed and compared. Adding some recent surveys would make more sense, such as \"A review of domain adaptation without target labels\".\n3. Method: the technical novelty is quite limited. There are some similar methods that have been explored, such as \"Multi-source domain adaptation for semantic segmentation\", where semantic consistency loss is proposed for segmentation using KL divergence.\nSimply combining GAN and a segmentation consitency loss is far from sufficient.\n4. Experiment: only one dataset is employed, which cannot well demonstrate the generalization ability. Some ablation studies are missing. For example, what is the influence of segmentation consistency loss? Comparison with and without segmentation consistency loss and comparison with other type of segmentation consistency loss, such as KL divergence, is expected and suggested.\nSome feature visualization is also missing.\n5. Presentation: a proof reading is recommended. Usually, the space is not enough for most ICLR submissions. It is strange that this paper is less than 8 pages. The remaining space can be used to contain more experiments, details, and analysis.",
            "summary_of_the_review": "Limited technical novelty, insufficient experiment and analysis, unproper title, and insufficient comparison with existing methods.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes to do domain adaptation (DA) for new domains, by learning a model to translate images to a universal domain. To force the semantics (ROIs) are kept during the translation, a domain adversarial loss and a segmentation consistency loss is imposed. The proposed method performs slightly better than top methods of the Multi-Centre, Multi-Vendor & Multi-Disease (M&Ms) Cardiac Image Segmentation Challenge.",
            "main_review": "1. This work seems to be a variant of domain generalization (DG). Some DG works, e.g. [1] are using different augmentations to form different views of a single source domain, and using a segmentation consistency loss to force the model to learn unified representation of different views. This paper replaces the multiple views from augmentation with multiple source domains, and uses a GAN to generate images from the unified representation. I'm not sure if the GAN is necessary, as the segmentation network has been trained to extract unified representation on different domains.\n\n2. The experiments are far from comprehensive. The methods are only evaluated on one dataset M&Ms. The compared methods are all from the challenge participants. The authors should evaluate on more datasets, with popular DG and DA methods as baselines.\n\n[1] Domain Generalization for Mammography Detection via Multi-style and Multi-view Contrastive Learning. MICCAI 2021.\n",
            "summary_of_the_review": "1. DaSeGAN is too similar to domain generalization methods. The GAN part may or may not be necessary, if the network can extract unified representation on different domains.\n2. The experiments are insufficient.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "DaSeGAN implements a GAN architecture to improve the cross-domain adaptation of models built from heterogeneous data sources into generalizable, universal model distributions that can be applied to new and unknown target distributions. The prime motivator for this is the abundance of unlabeled data that is prohibitively expensive to label; improving domain adaptation would provide better leverage for utilizing such data. \n\nThe framework consists of a model that performs a segmentation task and a generator model that translates the task procedure into a universally invariant one. The original segmentation model learns weights that are tied to the universal model by a segmentation loss function to enforce consistent outputs for both source domain and unknown/target domains. Finally, a discriminator model provides the adversarial training to enforce the domain universality of the generator.\n",
            "main_review": "strengths:\n1. Methods and experimental setup are clear, reproducible, and stepwise.\n2. Figures and tables are self-explanatory and require little caption.\n\nweaknesses:\n1. The intuition of the paper appears to be an effective one: use a GAN to learn both universal mappings as well as task-specific weights. However, even in a GAN that is only learning one particular task rather than two at the same time, convergence can be difficult for various reasons, not the least of which would be an oppressive discriminator. \n2. My criticism of the split in the training of discriminator and segmentation would probably be alleviated if we had a bit more detail on how exactly the loss functions and layers were implemented; \nnot necessarily the source code, but an actual overview of the training pipeline would add a lot instead of just descriptions of each isolated component. Still, the application of GAN especially as a tool to leverage unlabeled data, particularly for this domain of research, is worth more exploration;\n3. For the result comparison, the author should also include other evaluation metrics such as modified Hausdorff distance. Perhaps the increase of the proposed model is significant in the statistical sense (not shown, though), but it is very marginal (1%). \n\n",
            "summary_of_the_review": "I would recommend a 5. A great deal of specificity on the methodology is lacking. We need a bit more detail on how exactly the models were trained. The improvement of the experimental results is not as significant as claimed in the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper aims to address a new setting containing both domain adaptation and domain generalization. To solve the combined problem, this work proposes a GAN-based framework to translate images into a universal domain, and introduce a segmentation consistency loss to preserve a consistent semantic between source and universal domains. The experiment part shows some improvements compared with other methods.\n",
            "main_review": "This paper tries to solve an important problem in the medical area from both adaptation and generalization aspects. The proposed method gives a feasible solution by using generative adversarial networks. The detailed strengths and weaknesses are detailed below.\n\nPros:\n- This paper introduces a different viewpoint of considering both domain adaptation and generalization tasks (i.e. unlabeled target data is not always available)\n- The organization of the paper is clear.\n- The translated images show a more similar appearance, which supports the assumption of 'universal domain'.\n\nMajor Cons:\n- What's the motivation to consider both adaptation and generalization? Is it necessary to consider adaptation and generalization simultaneously? When the unlabeled target data is available, we can have a more specific design for the target data distribution. But for generalization, typically it has a weaker assumption of data distributions. In other words, we can use the Unsupervised Domain Adaptation (UDA) method when target data is available and the Domain Generalization(DG) method when target data is not available. Please clarify the motivation and necessity.\n- As mentioned in the '**Our work**' part, the proposed approach is computationally cheaper. I cannot see any comparison regarding computational time/resources. If it's a contribution of this method, please give more details and comparison to support it.\n- As mentioned in the first paragraph of Related Work, compared to UDA methods, the proposed method 'only require a generator and discriminator network independently of the number of source and unlabeled target domains'. There are no analysis or experimental results to support this claim.\n- What's the main difference between the DaSeGAN and other GAN-based DG methods?  As mentioned in the related work part, the transformation(translation) is already used in existing DG literature.\n- The image sample shown in the left column of Fig. 1 is too fake.  It's so dark that not even visible to the doctor. Only simple contrast adjustment can achieve a more clear sample.\n- The translation in the DaSeGAN approach is not new and also the consistency loss is also well known, which weakens the novelty.\n- Compared to other methods, the main performance improvement comes from DaSeGAN-T, which means the images are translated into a universal domain. In this case, how to make sure the translated images are correct and sound for clinical applications? There is no guarantee that translated images do not lose any details from originals. The only constrain is the consistency.\n- The performance on Vendor C is very marginal, and for Vendor D, comparing DaSeGAN-T with others, no more than 1% improvements are achieved on LV/MYO/RV.  The 'significantly boosts' is overclaimed.\n- The compared methods are all come from the same challenge. It's not convincing to only compare challenge's methods. There are many methods in both UDA and DG, which should be included in the comparison.\n\nMinor Cons:\n- The writing style and logic of this paper are hard to follow.\n- There are some typos, e.g. last sentence in the first paragraph of the Introduction.  \"...without _or_ data rights to...\"\n- This paper seems not to be well prepared, the table also exceeds the margin.",
            "summary_of_the_review": "This paper proposes an interesting viewpoint, but lacks the motivation. The novelty of the proposed method is not strong enough. \nThe paper writing style is hard to follow and it seems this paper is not well prepared. It should be better if the authors can give a strong motivation and also includes more comparison of UDA and DG methods to enrich the diversity, only considering challenge's method is weak.\nIn a summary, based on my major concerns, I will clearly reject this paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}