{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This work is targeted at the task of dataset condensation for image classification, i.e., selecting or synthesizing N (=1, 10, 50, ...) images per class to dramatically lower the training cost with minimal performance drop. The author reviewed previous approaches: (1) corset selection (efficient, heuristics-based) (2) dataset condensation (expensive inner loop optimization, end-to-end learning). The author took the second route (dataset condensation), but made improvement in the efficiency via distribution matching in random/semantic embedding spaces.",
            "main_review": "**Strengths**\n\n- The writing and the presentation is concise and precise. Readers can learn a great deal about the subject of dataset condensation.  \n- The proposed approach avoids expensive nested loop optimization by using randomly initialized (or pre-trained) networks to perform embedding computation.  \n- This idea of leveraging random/semantic embeddings in this scope appears novel to me, although this have been used in other research areas.  \n- The authors did a good job in evaluation, visualization, and ablation analysis in general.\n\n**Weaknesses**\n\n- No significant weaknesses.",
            "summary_of_the_review": "Overall the paper is in good standing. Good writing, novel idea, comprehensive evaluation and ablation analysis.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes an alternative approach to the dataset distillation/condensation task. Common methods use bi-level optimization (on weights/gradients obtained from distilled data) and thus often require large compute and memory. The proposed method instead optimizes for a small fixed set of images such that they are not distinguishable with real data *with respect to* randomly initialized networks. The method, while not resulting in consistently superior distillation performance, achieves significant training speed gains. Applications-wise, the proposed method attains better continual learning performance (when used as the saved data \"subset\"), but doesn't really show much benefits on NAS (when used as quickly evaluating candidate architectures). ",
            "main_review": "Strengths:\n+ The proposed approach is quite interesting, and novel to dataset distillation research. I appreciate the authors show that such approaches can work.\n+ Explanation of the method is mostly clear.\n+ The experiments are extensive, showing both distillation and application performances.\n\nWeaknesses:\n- A couple claims are misleading and not really supported:\n  1. Sec 2.1 last sentence: \"... need to tune the hyper-parameters of the outer and inner loop optimization [..] for different learning settings, which probably means more training time ...\"\n\n      In both DC and DSA papers, it is clearly indicated that the same set of hyper-parameters are used for all settings. So the first part of this sentence is inconsistent with published work. The second part is also problematic. Research, especially criticisms on prior work, should not be based on guesses. Please provide evidence or revise.\n  2. Sec. 3.2 KIP paragraph \"... overwhelm their 500 images/class synthetic sets ...\"\n\n      The KIP paper clearly indicates that the cited numbers are for 50 images/class in their Table 2. Please do not mis-present prior work, intentionally or unintentionally. \n  3. Sec 3.2 BatchNorm paragraph \"Thus, the inaccurate mean and std will make optimization difficult.\"\n\n      Again, please do not make arguments based on guesses. Fixing the mean and std is simply just fixing an affine transform. It is unclear to me why this necessarily imply difficult optimization. \n\n- Questionable evaluation metrics\n  1. Cross-architecture generalization\n\n      The authors (maybe purposefully) picked a setting where the proposed method (DM) works well than prior work (50 img/class, BatchNorm, CIFAR-10), instead of the setting in prior work. From Table 1, we can see that prior work outperform DM in most other settings. Please report more complete results to show a full picture.\n  2. NAS\n\n     The authors evaluated rank correlation on the top 5% architectures selected. This is a quite confusing metric, because it only shows how well the relative rank (of performance) is preserved *within these 5% architectures*, and these 5% are all different for different methods. Why should we care about this metric? Shouldn't the overall rank correlation be much more important? If a method picks a terrible 5%, but ranks these 5% correctly, it will do very well on this metric, but be basically useless. It can also potentially explain the alarming fact that final found architecture performance is not consistent with this metric. \n- Missing discussion/comparison with generative modeling, and justification of the method vs. other generative modelling approaches.\n\n  The proposed approach is essentially a special case of generative modelling, where the learned data distribution is (1) class-conditional (2) only include a fixed number of images and (3) observed via transformation. The authors themselves also claim focusing on *distribution matching*, which is the core of generative modelling. In this sense, why not simply adapt existing methods? I imagine a GAN or VQVAE can be easily used. GAN formulation is quite similar to the existing algorithm (plus optimizing network). VQVAE is inherently discrete. Neither require bi-level optimization. Moreover,  both should only be at most a couple times slower, still achieving significant time reduction. \n\n   What's the justification of the proposed formulation over existing approaches? I can believe that using the classifier architecture may help, but comparisons are at least needed. \n\n- MMD connection misses more discussions\n  \n  Here the function class is composing augmentation and random network $\\psi_\\theta \\circ \\mathcal{A}$. If going with the MMD motivation, then actually the network (maybe and augmentation) should be optimized (which would actually lead to a formulation more similar to GANs). What was sup changed to expectation?\n\n- I would also love to see TinyImageNet learned images.\n\nMinor formatting issue:\n- Citations are not formatted correctly. Please use brackets/paratheses. ",
            "summary_of_the_review": "The paper proposes a nice alternative approach to dataset distillation/condensation. The authors show some empirical promises of this approach. However, I find many claims and experimental evaluations questionable. Additionally, while the method is clearly explained, more straightforward approaches are not discussed or considered. Hence, I don't think the paper is ready for publication in its current form.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed an optimization based synthetic training data generation method to reduce the number of training samples dramatically  and this small set is still able to train existing models with relative accuracy (claimed). The optimization is, generally speaking, minimizing the MMD under different embedding spaces. The embedding spaces are approximated with randomly initialized networks. ",
            "main_review": "I think this paper is overall well-written and easy to follow. However, a few places need to be clarified:\na. in Eq (2) (3), big L and D is not defined. \nb. Eq (3) needs more explanations: why the loss are accumulated over all iterations?\n\nSome technical concerns:\n1. how 1/10/50 samples are decided? in the optimization equation (6) the size of the synthetic dataset is not optimized and needs hand selection so I want to know what size of |S| could guarantee 100% training power of the original dataset\n2. the selection of embedding space using randomly initialized deep networks needs more explanation. the arguments in sec2.2 discussion is not convincing as the 2011 paper is pretty outdated and 2016 paper is based on a different context\n\nI think the presented method is simple and seems effective compared to the baselines: it has the highest relative accuracy and seems to be stable over different initializations. These were proved through well-designed experiments.  but I'm not convinced that this is of practical value for the following reasons:\n1. the performance degrades when the networks are more powerful (ResNets)\n2. even though the number of examples reduced quite a bit (to 1/10/50), but the performance drop is also significant compared to using the whole dataset. It is more so on challenging datasets and it seems to get worse as it becomes more challenging (tab. 1)\n\nSo I have mixed feelings about this paper: in terms of paper structure/clarify, I think this is a good paper. However, I think it is not useful (may be the technology is at its early stage and it might be inspiring for future works)",
            "summary_of_the_review": "I found this paper easy reading and the proposed method seem to be better than previous baselines. the experiments seem to be convincing.\nI'm skeptical on the actual value of the method in practical as the performance degradation compared to using the full datasets seem to be significant. In addition, there is no easy-fix provided if we want to get similar results to using full dataset even at the cost of increasing the budget. In addition, higher degradation is observed when applying to more challenging datasets and more advanced models. \n\nContinual training and NAS are two good applications for this dataset condensing techs. \n\nI'm also more interested in seeing how models trained on condensed dataset generalize to multiple different tasks (detection/segmentation) and robust to adversarial attacks. Because I think these applications potentially suffer more degradations caused by condensed synthetic training data.    ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}