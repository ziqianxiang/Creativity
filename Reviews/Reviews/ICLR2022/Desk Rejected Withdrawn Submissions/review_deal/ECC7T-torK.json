{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies how the model's different training epochs can affect the transferability of the block-box adversarial attack. Based on the experimental observation, this paper introduced the very non-robust features (VNRF) intended to explain the misalignment with existing understandings in this field. Based on this new observation, they show early stop and adversarial training can both improve the transferability of black-box adversarial attacks. \n",
            "main_review": "\n---\n\nStrengths:\n- This paper demonstrated that using an adversarially trained or earlier stopping of the surrogate model can improve the transferability of the black-box attack. The comprehensive experiments support this claim. For ViT architectures, existing work has shown that using ResNet as the surrogate model has limited black-box adversarial transferability aginst ViT [1]. The experiment shown in this work suggests this transferability can be achieved by adversarial trained ResNet. \n\n---\n\nQuestion/weakness:\n- Figure 1, the learning rate only decreased once at epoch 50, is that right? A separate line indicates the learning rate in this figure would be better for comparing the trend. \n- The RF/NRF can be extracted into a dataset in Ilyas et al., 2019. Can the VNRF be extracted using the same techniques as in Ilyas et al., 2019? The motivations behind this paper are the earlier stopping improves transferability. In section 3.1, explained the model learns in the order of RF/NRF/VNRF during the training process. If this is the case for RF/NRF/VNRF, can they be extracted from different training epoch checkpoints? This also applies to light adversarial training, can the VNRF be extract from these different settings? \n- For the weight filtering, the ASR evaluated after removing the filters, only shows the black-box setting, what about the white-box setting on the surrogate model? If the ASR has already decreased under the white-box setting, that could mean it is already a non-effective adversarial example rather than it affects transferability. Maybe add another line for the white-box or use their difference for Figure 2. \n- Figure references in appendix A.1 are broken. \n\n\n[1] On the Robustness of Vision Transformers to Adversarial Examples. ICCV 2021. \n\n",
            "summary_of_the_review": "Despite observation on transferability and empirical improvements, I'm not convinced by the justification (VNRF) to explain this observation. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work finds that the early stopped models achieve better adversarial transferability and proposes a Very Non-Robust Feature(VNRF) hypothesis to explain it.  Motivated by VNRF hypothesis, they propose light adversarial training to remove VNRFs for improving the transferability.",
            "main_review": "I have two concerns below.\n\n1)Concerns about the VNRF hypothesis:\n\nThe thinking of very non-robust features is interesting, but the hypothesis and analysis are just empirical. \n\n2)Concerns about the light adversarial training:\n\nThe insights of the light adversarial training are similar to many existing works such as Friendly Adversarial Training (FAT) [1] and Curriculum Adversarial Training(CAT) [2]. Hence, the authors need to compare them and analyze the difference.\n\n[1]Attacks Which Do Not Kill Training Make Adversarial Learning Stronger. In ICML’20.\n\n[2]Curriculum Adversarial Training. In IJCAI’18.",
            "summary_of_the_review": "The writing of this version is hard to follow and needs to be improved. Though I cannot recommend acceptance at this stage, I will increase my score if my concerns are solved properly.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper provides a novel perspective— non-robust features cause the transferability of adversarial examples, but very non-robust features (VNRF) can degrade the transferability. Inspired by this perspective, the authors show that light adversarial training and early stop strategy can significantly improve adversarial transferability.",
            "main_review": "The empirical conclusion is interesting, but the writing of this version is relatively hard for me to follow. In addition, I have several concerns and hope the authors can give some clarifications.\n- It is unclear for me that what is the relationship between the proposed VNRF and the $L_{1}$ norm of model parameters. Specifically, the connection between VNRF and the empirical conclusion of [1] is relatively weak.\n- It is a little confusing for me that ‘We believe there is an opposite correlation between the feature transferability and readiness to be exploited for the attack.’ I suggest adding a clear explanation.\n\nTypos:\n1. Abs:\n‘’Most lines of works attribute the existence of the non-robust features improves the adversarial transferability.’’ It is pretty hard to understand.\n2. Abs:\n‘As a motivating example, we test the adversarial transferability on the early stopped surrogate models, which are known to be concentrated on robust features than non-robust features from prior works.’ It is a little unclear for me, I suggest adding support, i.e., which work given the conclusion.\n\n[1] Defending against universal attacks through selective feature regeneration. CVPR2020.",
            "summary_of_the_review": "The empirical conclusion of this work is interesting, but the writing needs to be improved.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work observes that early stopping improves the transferability of adversarial examples, and proposes a Very Non-Robust Feature (VNRF) hypothesis to explain it. In addition, light adversarial training is adopted to remove VNRFs for improving the transferability.",
            "main_review": "1. Regarding the VNRFs hypothesis, (1) how to demonstrate that VNRFs are learned after NRFs? (2) if we compare the results in Figures 1 and 2, we can find that the improvement due to zeroing out filters with large $l_1$-norm (in Figure 2) is much less obvious than the drop in Attack Success Rate (ASR) in Figure 1. Doesn’t this suggest that there are other reasons in addition to VNRFs?\n\n\n2. In terms of light adversarial training, the idea of using/including weaker forms of attack has already been discussed in previous works, including curriculum adversarial training [1], fast adversarial training (Wong et al., 2020), friendly adversarial training [2]. All these works prevent the generated adversarial examples from overfitting to the surrogate model.\n\n\n[1] Qi-Zhi Cai, Chang Liu, and Dawn Song. 2018. Curriculum adversarial training. In Proceedings of the 27th International Joint Conference on Artificial Intelligence (IJCAI'18). AAAI Press, 3740–3747\n\n[2] Jingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and Mohan Kankanhalli. Attacks which do not kill training make adversarial learning stronger. In ICML, 2020.\n",
            "summary_of_the_review": "1. The VNRFs hypothesis needs to be further justified.\n2. The idea of light adversarial training is not completely new.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}