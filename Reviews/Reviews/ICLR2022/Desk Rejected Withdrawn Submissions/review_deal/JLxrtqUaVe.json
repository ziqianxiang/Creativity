{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper augments LSTM (and its variants) with attention over a sliding window, an architecture that would have been interesting 5 years ago, but nonetheless has been extensively studied by existing works (e.g., [1]). The motivation is cloudy, and the presentation could be significantly improved (e.g., half of the space is spent on equations and diagrams that are well-known to everyone in the community). Experiments are done on 4 toy-ish text classification datasets, with weak baselines, completely negligent about related works. \n\nThis would have been an ok course project. I vote for rejection.\n\nReferences \n[1] https://openreview.net/pdf?id=SkFqf0lAZ",
            "main_review": "See summary of the paper",
            "summary_of_the_review": "See summary of the paper",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a modification of the LSTM cell, where instead of feeding the output of the cell to the next time step to implement the recurrence, the authors propose to feed the result of an attention mechanism over the last $N$ steps. The authors claim that this method will help to improve the long term memory of LSTM cells. The experiments show that this new version of the LSTM is better than other variants in text-related tasks.",
            "main_review": "The paper is overall well written, clear and easy to follow. For the most part the introduction is good and clearly states the goals. The description of the modified LSTM cell is good and allows the reader to understand it precisely. \n\nThe experiments are carried out on three text classification tasks. On only one dataset are the results strongly better. On the other two tasks, the difference of accuracies with the baseline methods is pretty small, making the advantages of the proposed approach more difficult to assess without confidence intervals or tests of statistical significance. Also, what makes the third dataset different? Can we learn something from this different pattern?\n\nThe main issue with this paper is that the proposed architecture allegedly aims to improve the long-term memory of LSTM cells. First, the modification itself, consisting in integrating past hidden states with an attention mechanisms, limited to 4 to 12 past states, fails to convince. Why should it help for the long-term memory? It seems that it would help more for the short-term memory, so what is the induition of using short attention for solving long term memory? Then, the experiments do not show an issue with the classical LSTMs for long-term memory that would be solved by the proposed modification. It looks like all variants are better when the length of the input increases, but the difference between LSTM and HA-LSTM does not seem to increase with the input length. \n\nThe tasks are only very briefly described in the experiment section. They are all text classification tasks: why not test on a variety of tasks instead? If the point is to improve the long-term memory, toy problems could also very well illustrate the point. Moreover, the BLSTM layer size is adjusted to match the number of parameters of the unidirectional LSTM. It is not clear in the text whether the same was done for the HA-LSTM. The choice of $H$ for different values of $N$ is not given. For a fair comparison, all models should approximately have the same size, or it should at least be explicitly mentioned.\n\nFrom the equations, it looks clear that if $N=1$, the cell is exactly the LSTM. The other values of N tested are 4, 8 and 12:\n - why these values, and not 2 for example?\n - what happens when $t < N$?\n\nIf the attention is always fully on the last state, it also amounts to the classical LSTM. For a conference like ICLR, about learning representation, an analysis of how different is the attention than the one corresponding to the classical LSTM would be extremely interesting. If it indeed helps for the long term memory, how can we illustrate that with the attention weights? It could be interesting to design experiments with a fixed attention distribution (e.g. uniform over the considered context, ...)\n\nA few other things could be improved:\n  - In the results section, \"the poor performance might be a result of the initial value of the trainable parameters\": why not do several runs with different seeds to measure the effect? If indeed the initialization has that much of an impact, how can we conclude about the superiority of the proposed approach when the difference with ON-LSTM is so small?\n  - In the introduction, the authors imply that bidirectional LSTM help to retain long memory \"RNN might forget the initial information if the sequences are long. Therefore a bidirectional LSTM structure is proposed\". Bidirectionality does not help with past context, it is a way to include future context for a prediction at time $t$.\n  - \"the HA-LSTM structure is a bidirectional computation\": this is not clear. The HA-LSTM implements an attention over the last N cell outputs, instead of feeding only the last step. This looks still unidirectional.\n\n\nMinor comments:\n - formatting of the references in 2.1\n - \"flatten\" in eq. 12 is not clear.. I guess it reduces the rank of the matrix but not the dimension?\n - Beginning of Section 2: \"Computation\" -> \"The computation\"\n - Consistency: the ordering of the hidden states is different in Fig. 1 and 2\n - Fig 1 and 2 could be much smaller and merged into a single figure to leave room for more analysis\n ",
            "summary_of_the_review": "The paper presents a new modification of the LSTM cell, including attention in the cell which looks interesting and promising. However, the long-term memory problem it aims to solve is not supported by evidence. Without more analysis of the interest of the attention mechanism, it looks like a lot of added complexity on top of an already complex structure.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper investigates an extension of the LSTM with integrated attention mechanism.",
            "main_review": "Strengths:\n\n- It is probably a good idea to integrate attention inside gated recurrent models.\n- Provides implementation.\n\nWeaknesses:\n\n- No standard language model (LM) benchmarks such as enwik9 and many others, which would be the obvious benchmark for such a new model.\n- No other recent common models are compared. Most obviously Transformer and variants.\n- Lacks related work on similar models, e.g. Long Short-Term Attention (https://arxiv.org/abs/1810.12752),Recurrent Attention Unit (https://arxiv.org/abs/1810.12754), Multi-Zone Unit for Recurrent Neural Networks (https://ojs.aaai.org/index.php/AAAI/article/view/5958), LSTA: Long Short-Term Attention for Egocentric Action Recognition (https://openaccess.thecvf.com/content_CVPR_2019/html/Sudhakaran_LSTA_Long_Short-Term_Attention_for_Egocentric_Action_Recognition_CVPR_2019_paper.html)\n- No real analysis, e.g. on attention weights, or on testing on variations of the proposed models.\n\n",
            "summary_of_the_review": "While such idea is interesting to explore, the presented work lacks standard LM benchmark comparisons, comparison to other current state-of-the-art models, lacks related work, and lacks analysis.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work proposes HA-LSTM, which is a variant of the LSTM structure, to achieve reciprocity between the recurrent layer and the attention layer. The HA-LSTM structure uses the range-limited hidden state vectors as inputs to the attention mechanism and passes the corresponding outputs of the attention method back to the previous recurrent layer to update the cell state. \nIn experiments, the HA-LSTM is compared with other LSTM structures, and show consistent improvements.",
            "main_review": "Strength:\n\nThe paper is clearly written, with big, nice depictions.\n\nWeakness:\n\nConsidering the current popularity of \"attention is all you need\", I think the innovation of adding attention to lstm is limited. (There has also been works combining recurrence + attention, e.g., \"Modeling Recurrence for Transformer\")\n\nThe improvement over other LSTM structures are not large.\n\nI think comparison with the current popular transformer architecture is needed.\n",
            "summary_of_the_review": "Considering the current popularity of \"attention is all you need\", I think the innovation of adding attention to lstm is limited. I'd recommend to submit this work to an NLP conference or a workshop.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}