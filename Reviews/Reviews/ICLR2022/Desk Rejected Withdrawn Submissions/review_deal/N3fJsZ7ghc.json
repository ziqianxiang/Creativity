{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The key contribution of this paper is a method to obfuscate a neural network model by adding \"confusion\" neurons. The location of these confusion neurons is encrypted using standard encryption techniques. For correct inference, the encrypted locations are decrypted and the confusion neurons are ignored. However, any user without access to the decryption key will get highly degraded predictions from the obfuscated model. ",
            "main_review": "1) The problem being tackled (protecting a pre-trained model from a curious user) is relatively novel. However, the proposed solution has a number of limitations.\n\n2) At the time of inference, the locations of the confusion neurons are decrypted, and hence, gets revealed to the user. Thus, the proposed approach offers no protection against inference time attacks.\n\n3) The paper simply assumes that as long as the weights assigned to the confusion neurons are derived using a combination of random weights and weights sampled from the distribution of other neurons, it is difficult to detect the confusion neurons. However, this assumption may be fatally flawed and there may be many ways to identify the confusion neurons.\n\nFor example, for each layer, one can attempt to perform a two-class clustering (or) out-of-distribution detection based on the neuron weights to identify anomalies. Most regular neurons are likely to contain structural patterns in their weights, whereas the weights of confusion neurons are random. This does no even require any additional training data.\n\nIf some additional training data is available, the paper suggests a fine-tuning approach as an attack mechanism. Instead, one can also add channel attention modules and effectively learn the attention parameters, while freezing all the other weights.  \n\n3) Finally, the paper argues that even if there is a mechanism to perfectly predict the confusion neurons with probability p, there is a very small probability of correctly detecting all confusion neurons due to the combination effect. However, as explained earlier, one does not require a 0-1 decision about the confusion neurons. As long as a soft attention mechanism can learn to ignore the \"most confusing neurons\", the predictive power of the original model can be greatly restored.\n\n4) It is surprising that the paper makes no attempt to formally quantify the level of protection offered by the proposed scheme. For example, what is the conditional entropy of the original model given the obfuscated model (or the mutual information between the two models)? Neither does it quantify the level of degradation in accuracy for a given degree of obfuscation. Given the lack of any such theoretical analysis, it is hard to evaluate the real benefits of the proposed scheme.",
            "summary_of_the_review": "While the paper tackles a relatively novel problem, the proposed solution is far from satisfactory. Any paper on security that does not provide a concrete security analysis should not be accepted.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper studies how to protect pre-trained models from being maliciously copied when running on customers' devices. To do this, the authors propose to \"encrypt\" the pre-trained model by adding additional confusion neurons to the model, while keeping other parameter values unchanged. These confusion neurons act to increase the difference between the predictions of the original and encrypted models, which thus can contaminate and downgrade the availability of the encrypted model. In order to realize stable encryption results, the authors suggest adding the same proportions of confusion neurons to each layer of the pre-trained model, where the confusion neurons are initialized either randomly or with the values of existing neurons. Experiment results show that the proposed method can effectively downgrade the performance of the pre-trained model, and is resistant to fine-tuning.",
            "main_review": "Although this paper has studied an interesting and important topic, i.e., how to protect intellectual properties in the context of machine learning, I do not think the authors have responded well to the challenges.\n\nWhen I was reading the abstract of this paper, I thought the models encrypted by confusion neurons can perform inference while keeping encrypted. However, I later found out that during inference, the pre-trained model requires to be decrypted first. As a result, why do we still need such a complex encryption process? Once the model is decrypted on a device owned by a malicious user, he/she can obtain the decrypted model parameters through various methods, for example, directly copying the decrypted model parameters from the RAM.\n\nBesides, I could not understand why one has to encrypt the pre-trained model via adding additional neurons, as there exist many obviously simpler methods that can contaminate the model. For example, a potential solution is to assign random sign values for each model parameter. This potential solution has at least two advantages compared to the confusion neurons addition method: (1) it would not increase the memory cost for storing the encrypted model, and (2) it is foreseeable that this method can better resist fine-tuning. Therefore, I suggest the authors put more effort into explaining the superiorities of the proposed method.\n\nFinally, the writing of this paper requires to be improved. For example, Section 2.2 aims to describe the implementation of the proposed encryption method, however, the specific confusion neuron insertion strategies are not given until \"Section 3 Experiments\". This confuses me during reading. As a result, I suggest moving the whole Section 3.1, which discusses the neurons insertion strategies, to Section 2.",
            "summary_of_the_review": "In summary, I tend to reject this paper. However, I would like to see if the authors can resolve my raised concerns. Therefore, my current recommendation is \"marginally below the acceptance threshold\".",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "They propose the deep encryption method of neural networks by introducing confusion neurons to be embedded into the pre-trained model to protect the DNN model. They insert a number of confusion neurons to each layer of the network. And the position of confusion neurons are encoded into a short vector and can be encrypted. Users with the key can easily decrypt the confusion neurons. They conducted the several experiments to demonstrate the effectiveness of their approach, not only in images but also text datasets.  ",
            "main_review": "Strength:\n\n+ The paper is generally well-written and easy to follow. The organization is good. Also, they provide the good illustration of the effectiveness of the proposed approach in Fig. 2.\n\n+ They conducted the several experiments, and the comparative analysis on different tasks (image classification, object detection, instance segmentation, and text classification) to demonstrate the effectiveness and generalizability of the proposed protection performance.\n\n+ They also clearly present the threat model with the fine-tuned attack.\n\nWeakness:\n\n- The proposed idea is rather simple and straightforward strategy. \n\n- Further detailed explanations on optimization and approximation of Eq1. would be helpful. \n \n- Since no code is released, it is difficult to check the reproducibility.\n\n- More thorough evaluation against the fine-tuning attack would be helpful.\n\n\n",
            "summary_of_the_review": "They propose a simple yet effective, and interesting approach to protect the pretrained model, which is a very timely topic. They conducted extensive experiments. However, it would have been nice if more explanations and details were provided in Suppl. Also, since the code is N/A, it is difficult to check and assess the reproducibility and validity of the proposed method.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No issue",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work proposes a novel methodology for avoiding the unwanted usage of pretrained models, by injecting neurons that fires random output, and thus ruining the prediction of a given input. The authors show how much damage can be caused with different number of neurons in different settings (i.e. in different layers), and they claim it is possible to remove these neurons only with a decryption key (that is the position of the confusion neurons at each layer).\n",
            "main_review": "Pros:\n+ interesting intuition on how securing pretrained model, and avoid model stealing\n\nCons:\n- standard encryption is still the best option to secure a model\n- decryption is not even covered in the paper\n- poorly written\n\nMinor:\n- code is not available\n\nWhile it is interesting to study how to embed security mechanisms inside a deep neural network, this work is hindered by the following issues.\n\n**Standard encryption is still the best option to secure a model**\nThe encryption scheme provide a key, which is where to put a neuron that confuses the output. Hence, the client applies the key and removes such neurons, while an attacker (that is stealing the model) can not recover the key.\nI struggle to see where this can be better than applying a strong (and provable) and regular encryption scheme (like RSA for instance).\nAlso because, even if the removal of neurons is complicated, the attacker can still access most of the model.\nOtherwise, with a regular encryption algorithm, the attacker would possess a totally encrypted model, which is almost impossible to decrypt without the key (think about the RSA algorithm). Even if the authors study a possible approach to recover the confusion neurons, it is not well described and not motivated enough. For instance, they claim that a larger model is difficult to fine tune, but it is easier to apply more epoch than reversing an RSA algorithm. Also, the authors state that they are satisfied with fewer neurons addition, hence exacerbating the problem stated before, and the attacker can just fine tune the network.\n\n**Decryption is not even covered in the paper**\nOn top of the problems with the proposed scheme, the decryption is not even described in the paper, but it is just mentioned in words. Even if it is intuitive to understand, it is mandatory to have it inside the overall story. Also because the authors need to differentiate this scheme from the regular (and most suitable) encryption scheme that is generally used for protecting assets.\n\n**Poorly written**\nThe methodologies presented inside the paper are sometimes difficult to understand, not because the given intuitions are difficult to grasp, but because they are presented in a confused way. Like, the memory constraint (which is important), is introduced and poorly treated through the overall paper. \n\n**Minor comment: code is not available.**\nThe authors could have uploaded a simplified version of their work as supplementary material. This would have helped more in the judgement.",
            "summary_of_the_review": "I opt for rejecting this paper, because it is not clear how this scheme should be better than e regular encryption scheme.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}