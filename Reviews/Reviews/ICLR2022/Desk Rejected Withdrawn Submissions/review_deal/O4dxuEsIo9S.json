{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In the lottery ticket hypothesis (LTH) literature, re-training a found lottery ticket is an often-underestimated step, just re-using the dense training protocols. This paper discovers the hidden gem of “tweaking” better sparse training and improves the lottery ticket quality even with the same found sparse mask.",
            "main_review": "Pros. \n-  This paper is an inspiring challenger to many “commonsense” in LTH: once the sparse mask is obtained, traditionally, it has to be re-trained with the same initialization (or rewinding) and same training recipe as its dense network. This paper established the improved sparse re-training step as a hidden gem and showed the proposed tweaks could generalize to other sparse training methods in general. \n- Among the proposed tweaks, what impresses me most is the idea of changing the “lottery ticket initialization” by learned layer-wise scaling. While (Evci et al., 2020b) already pointed out typical dense initialization isn’t optimal for sparse NNs, they also found random re-initialization will destroy lottery tickets. This paper found a particularly clear idea between these two extremes by using learned linear scaling –– it can be learned to optimize the landscape for faster loss decrease while any linear coefficient can be absorbed by batch normalization. \n- Experiments are thorough and strong, showing improved sparse re-training clearly boosts the performance of “winning tickets” at high sparsity levels and large models. The performance margins are generalizable across datasets (CIFAR10, CIFAR100, TinyImageNet) and architectures (Vgg16, ResNet-18/ResNet-34, MobileNet). \n- The authors did a good job in connecting literature rationale with experimental observations, making their findings more comprehendible. I also liked the visualization in Figure 5 and Figure 11 and the ablation study results. Plus, due care has been made in distinguishing the techniques’ benefits to general (dense) training versus specifically sparse re-training. \n\nCons.\n- To be fair, many discoveries in this paper, although new to LTH, is not so “new” nor surprising in general deep learning literature, such as smoother neurons improving gradient flows (hence fixing sparse training, as Tessera et al., 2021 also found); as well as denser skipping connections, label smoothening, etc. ",
            "summary_of_the_review": "The submission is a solid work supported by convincing experimental results. While some proposed techniques are not essentially very new, identifying the sparse re-training bottlenecks and connecting all these dots is a non-trivial contribution. The work further provides many useful reflections and understandings on what is essential in sparse training. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose that the training and structure of models found by lottery tickets rewinding (LTR)/lottery tickets (LT) should be modified from that of the dense model, unlike what was typically done in at least the original LT paper, and propose a set of training and architectural changes to improve retraining of these sparse models. The architectural changes consist of adding extra skip connections around each convolutional layer within the ResNet blocks, and changing the ReLU activation function to Swish/Mish. The training changes consist of scaling the LT initialization according to the sparsity of the layer and replacing the one-hot classification labels with naive or knowledge-distilled soft labels. The authors demonstrate improved generalization for the combination of all of these changes as compared to dense training and standard LTH retraining baselines with VGG16, ResNet-18/34 and MobileNet on CIFAR-10/CIFAR-100 and dense/standard LTH with (unclear what model) on Tiny-ImageNet. The authors claim significant generalization improvement for very sparse networks, e.g. +4.93% at 97% sparsity for ResNet-18/CIFAR-100.",
            "main_review": "Strengths:\n- Compelling results for high-sparsity as compared to LTH rewinding\n- Figure 1(a) shows significantly higher improvement at high sparsity with the extra skip connections than at low sparsity/the baseline dense model. Using extra skip connections could be benefiting sparse NN more than dense from this result.\n- Figure 6 shows significant improvements with MobileNet/CIFAR-100 in particular, but also to some extent ResNet 18/34 at high sparsity.\n- Figure 2 presents some compelling results for the claim that the activation function changes from ReLU to Swish/Mish result in sparse NN that have smoother loss landscape - *however* this is already known for dense NN, as presented in Fig. 5 of the swish paper for example.\n\nWeaknesses:\n- While the authors motivate most of the changes in improving the smoothness of the loss landscape, there is little evidence (outside of the swish/mish activation functions) aside from some plots of the \"loss surface\"/\"loss landscape\" in Figure 5 and Figure 11 - but these visualizations are not explained at all, and clearly must be projections/transformations of the loss landscape. Without knowing how they were produced we can't judge how representative they are. Why not use the Hessian analysis as used for Swish/Mish to quantify this?\n- Many of the proposed improvements seem to improve the performance of the baseline dense model and very-low sparsity models very significantly, making the question of if these results simply improve all DNN training rather than sparse/LTH training important. The authors attempt to address this question in Figure 1 by claiming the improvement increases for high sparsity, but it's not convincing for any of the results except for perhaps skip connections (Fig 1(a)), and in fact Fig 1(d) seems to show that label smoothing only helps the baseline dense or very low-sparsity models.\n- There is no clear table of results, only graphs, making it hard to judge the actual significance of the results. The CIFAR10/100 experiments should have multiple runs with different seeds to judge the variance/significance of the presented differences which are marginal in much of Figure 1.\n- There should also be a table of results presenting an ablation study over the many confounded methods (AT+TRT) used by the authors, like what the authors tried to show in Figure 1.\n- Graphs often presented without proper labelling/explanatory captions. For example, what's the x axis in figure 8 and 6? Assuming sparsity, but should be clear.\n- Graphs seem to be in an odd order, and far from where they are referenced, or are just not referenced in the text (Figure 8 for example).\n- The initialization component of the method itself is not a contribution since as the authors cite, it was proposed elsewhere.\n- Section 3.1: The motivation that having extra skip connections smooths the loss landscape is known, but why does this matter more for sparse NN than dense? The explanation that it might prevent collapsed layers (layers with catastrophic pruning) makes sense for (uncommon) extreme sparsity, but not so much for reasonably sparsity levels. \n- End of Section 3.2: The argument that scaling of the LTH initialization will not affect training because of batch norm is not valid, since batch norm takes a number of iterations to collect mini-batch statistics and will not be effective on the first number of iterations which can be very important for training. It is also a confusing argument since the authors learn coefficients to scale the LTH initialization to improve training as in Zhu et al., apparently acknowledging this.\n\nMinor comments:\n- Section 3: \"However, those are not necessarily optimal for training sparse networks.\" Why? There is no justification or reference made for this statement here. If referring to the results, then state as our results demonstrate. \n- Section 3.1: claims \"ReLU is an obstacle to sparse retraining because it leads to high activation sparsity in the subnetwork.\" No results are shown/cited here to back up this motivation, and it's not clear why activation sparsity should be more of a hindrance than with dense NN if the pruned connections were not salient. Should refer to appendix A1.\n- Last line of 2.1 doesn't make sense, perhaps it means \"in dense CNNs\"?\n",
            "summary_of_the_review": "This is overall a poorly presented paper, with significant issues in the motivation and the presentation of results in particular, and is simply not of a standard of an ICLR paper in the current state. This is unfortunate as it does contain what could be some promising results, in particular for the architectural changes improving training at high sparsity. I would encourage the authors to work further on the presentation of the motivation and results, narrowing down the scope of their method to those changes with the most promising results vis-a-vis sparse NN, and incorporating the feedback for submission elsewhere, but I cannot recommend this paper for acceptance as is.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The authors propose several \"tweaks\" to sparse re-training of lottery tickets that have been identified by iterative magnitude pruning (IMP). These tweaks include \n1) the replacement of ReLU activation functions by smooth activation functions,\n2) the use of soft-labels instead of one-hot labels, \n3) learned layer-wise rescaling of the initial parameters to enable effective gradient steps initially, \n4) the injection of new skip connections to avoid layer collapse. \nThe tweaks are compared and combined with weight rewinding.\nExperiments on CIFAR10, CIRAR100, any TinyImageNet suggest a superior performance of the tweaks over vanilla IMP.\n",
            "main_review": "Strengths:\n+ IMP with the proposed tweaks seems to outperform the vanilla IMP across architectures and studied datasets.\n+ The authors also tried training with a teacher network.\n+ The change of loss surface with regard to the tweaks is analysed indicating that flatter minima are identified with the help of the tweaks.\n\nWeaknesses:\n- No error bars are reported. In consideration that pruning results are subject to high levels of noise, this makes it impossible to assess whether the gains over LTH are significant.\n- None of the proposed tweaks is novel but is a standard approach used in deep learning.\n- All experiments are limited to image classification problems and none of the datasets is really large scale (like ImageNet). For a purely experimental paper that claims to achieve new state-of-the-art results, this experimental evidence seems a bit weak. It is thus unclear whether the proposed tweaks are useful in different settings and tasks. \n- Only comparisons with vanilla LTH are presented. However, a multitude of other techniques have been proposed to address shortcomings of IMP (see e.g. synflow [7] to prevent layer collapse or [1,3]). The claim that the authors achieve a new state of the art is thus not really justified.\n- It is unclear what the specific role of the single tweaks is and whether they are actually superior to similar proposals in the literature.\na) For instance, is it really the smoothing affect that helps when replacing ReLUs by smoother activation functions after pruning? Maybe similar results could be achieved by training from the start with smoother activation functions? Also the impact of this tweak does not seem to be consistently positive.\n(The authors are not the first to try out activation functions that are different from ReLUs in the context of the LTH.)\nb) Skip-connections: A couple of architectures naturally have skip-connections. Are this skip-connections helpful in this context? \nI would expect a deeper analysis of what their specific role is and what happens to them during pruning. E.g.: Are these skip connections actually pruned away or preserved? \nThe experimental results indicate that additional skip connections can also harm the performance of relatively dense networks.\nDo the authors suggest that skip connections should only be added after aggressive pruning?\nIf skip connections are beneficial: Do additional skip connections actually prevent layer collapse and are all other parameters removed from a skipped layer? (Is the layer thus effectively removed?) \nYet, skip connections can have multiple benefits also in training deep dense neural networks, which leaves the questions: How have the blocks with skip connections been initialised? Do these parameter initialisations respect different layer densities and different pruning stages?\nIn dense training, skip connections can be quite helpful and stabilise training if used in the right way. \nc) Re-scaling of initial parameters: Is the proposed learned re-scaling more effective than orthogonal repair [1]? Does it add value in comparison with [2], which does not even require learning of the scaling factors? \n- Technically, the claim that the authors are the first to explore LTH sparse re-training under different conditions than dense training seems to be an overstatement. All pruning at initialisation methods do that. [2] specifically addresses the parameter initialisation. [4] makes several proposals, e.g., adapting the Drop-Out rate. Adding also links instead of only removing links is also not such an unusual idea. See, e.g., [6]. The authors really have to show that their proposal works better than others.\n\nPoints of minor critique and open questions:\n- The visualisation of the loss landscape is not explained at all. What are the axes?\n- A couple of details are missing on how exactly the tweaks are performed. For instance, how are the layers re-scaled? How are the scaling factors trained? How many skip-connections are added? Which smoothing parameter is used for the soft-labels?\n- All of the tweaks introduce additional hyper-parameters that have to be tuned in case of different applications.\n- The authors did not test whether they find actually meaningful lottery ticket architectures (see [5]).\n\nLiterature:\n[1] A Signal Propagation Perspective for Pruning Neural Networks at Initialization, ICLR 2020.\n[2] Robust Pruning at Initialization, ICLR 2021.\n[3] Winning the Lottery with Continuous Sparsification, NeurIPS 2020.\n[4] Learning both Weights and Connections for Efficient Neural Networks, NIPS 2015.\n[5] Sanity-Checking Pruning Methods: Random Tickets can Win the Jackpot, NeurIPS 2020.\n[6] Sparse Training via Boosting Pruning Plasticity with Neuroregeneration, NeurIPS 2021.\n[7] Pruning neural networks without any data by iteratively conserving synaptic flow, NeurIPS 2020.\n----------------------------------------------------------\nPost-rebuttal update:\n----------------------------------------------------------\nAs the authors have presented additional experiments and compared their approach with multiple other methods, I am convinced that their tweaks can improve sparse training.\nI have raised my score to 6 accordingly. However, I reserve doubts that the paper is interesting enough for acceptance at ICLR, since it does not present novel ideas in the context of deep learning. Furthermore, the details of the approach are still not well understandable (see my my points of minor critique which have not been addressed in the updated draft).",
            "summary_of_the_review": "A multitude of techniques are presented to improve Iterative Magnitude Pruning (IMP) to identify lottery tickets. The presented experiments are encouraging that vanilla IMP can be outperformed but the proposed tweaks are neither particularly novel nor are they compared with similar ideas in the literature or their impact on sparse training thoroughly analysed. Therefore, I believe that the work is not ready for publication at ICLR in this form.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper aims to improve Lottery Ticket Hypothesis (LTH). The paper shows that the combination of several techniques, such as different activations, skip connection, and knowledge distillation, can improve the performance of the sparse sub-network obtained from dense networks. The paper validates the method on some networks such as ResNet and MobileNet. Most experiments shown in the paper are conducted on CIFAR10 and CIFAR100.",
            "main_review": "### Strengths\n1. The proposed methods in the paper can help improve the performance of the sparse networks obtained using LTH.\n2. The proposed methods are well ablated.\n3. The paper shows various experiments on CIFAR10 and CIFAR100 to validate the methods.\n\n### Weaknesses\n1. The paper claims that LTH is one of the most effective methods of deploying computationally expensive models on devices. However, the paper didn't show any benefits when deploying the proposed model on devices. Some techniques introduced in the paper, such as additional skip connection, can increase the latency of the model. \n2. The technical novelty of the paper is rather limited. The paper seems didn't propose new techniques; instead, it combines different tricks, such as different activations, skip connection, and knowledge distillation, to improve the classification accuracy.\n3. Some techniques used in the paper, such as changing activation or adding skip connection, actually change the architecture of the sparse network. It's not a fair comparison with other LHT methods. I guess using more aggressive network changing methods such as network architecture searching could even improve the performance.\n4. The experiments are not extensive, considering the paper is more like empirical work. Most experiments in the paper are conducted on CIFAR10 and CIFAR100, which are very small datasets. The author claims experiments on Tiny-ImageNet, but there is only one experiment on Tiny-ImageNet (Figure 7), and the architecture used is not clarified in the paper. \n5. I would suggest the authors have more experiments on ImageNet. Besides the networks used in the paper, I would suggest having deeper and more challenging networks, such as ResNet 50 and MobileNet V2.\n",
            "summary_of_the_review": "While the proposed methods can achieve better results than existing LHT methods, the novelty is rather limited. The paper is more like an empirical report with a combination of different training tricks. Also, the experiments on limited network architectures and small datasets are not convincing. Therefore, I lean towards rejection.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}