{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a method for tuning the learning rate of deep learning models. The method is evaluated on a benchmark of three problems, one of which is a private dataset. Results show that one of the proposed methods (method 1, midpoint of viable learning rate) is able to identify good performing learning rates. ",
            "main_review": "\nStrengths:\n- The method is simple and somewhat justified. The targeted problem is significant.\n- The paper is well written and the figures help explain the method.\n\nWeaknesses:\n- Presentation of the method was somewhat confusing, spread over multiple sections, when it fact it is quite simple. A reorganization or algorithmic presentation of the method could be helpful, and there is room for it (page limit is 9, not including references). See the following questions for precisions:\n-- What does it mean to \"know\" a point of interest? What is the threshold? What is the threshold used to stop the pre-evaluation loop of learning rates? Is there an argument to make in favor of the loop involving three different methods (two of which should likely be removed because they don't perform as well as Method 1)? A comparison with a pure grid search perhaps?\n-- I also had to search through the paper to find the number of mini-batch steps (20), perhaps it could be rewritten in Section 3.3.\n-- When this pre-evaluation stops, what is the next step? Train a full model with the selected learning rate?\n\n- There is no comparison baseline at all. \n-- How does your method improve upon classical hyperparameter optimization methods such as random search, Bayesian optimization or successive halving? If they are not valid comparison baselines, you should provide some justification to that end. \n-- What about the cited blog posts? You should compare with those.\n-- A key missing citation is \"ONLINE LEARNING RATE ADAPTATION WITH HYPERGRADIENT DESCENT\" by Baydin et al. 2018. They effectively remove the learning rate parameter by performing hypergradient descent.\n-- Some comparison with methods that tune the step size of SGD such as Adam or Adagrad would also be useful. It's not like there is no literature to make SGD less sensitive to it's learning rate parameter.\n\n- Some of the methods proposed in Section 3 seem arbitrary and poorly justified. For example, the argument in Gugger's blog post was that since the training loss curve was obtained by progressively increasing the learning rate while maintaining the underlying model, then saying when the training loss reaches a minimum the \"optimal learning rate\" was overshot by a factor because some training had already occurred beforehand with the previous learning rates. However this method is no longer used in your paper, how do you justify still having to cut to the middle of this training loss curve? \n\nGrammar mistake:\n\nSection 3.3 paragraph 2: a course grid -> a coarse grid",
            "summary_of_the_review": "\nI recommend rejection of the paper because the method is poorly justified but also because there are absolutely no baselines and I hope I've shown in my review that there was no lack of methods to choose as baselines for comparison. \n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a method to select a learning rate by training a model for a small number of steps over various learning rates and resetting both the model parameters and dataset between each trial. The authors claim that an optimal learning rate can be chosen from the log of rates vs. losses. The paper compares different configurations of the proposed method using three models, BERT trained over the AG News dataset, BART trained on a proprietary Samsung dataset called ACC, and ResNet18 trained over the Hymenoptera data dataset. ",
            "main_review": "The paper presents an interesting approach but needs conceptual and methodological improvements. The novelty of the proposed method is limited. The authors should clarify the paper's contribution in detail and relate it with the literature. The paper looks like a work still under development. I have the following considerations and suggestions on how this paper can be improved:\n- Address the paper innovation and main contributions in the introduction section.\n- Does the proposed approach work for large datasets? What is the computational cost of all these procedures? Is it worth comparing to traditional and similar approaches? What is the underfitting x overfitting tradeoff found?\n- The authors could use a figure to explain the proposed approach better.\n- The authors should improve the experimental methodology description.\n- What are the theory and insights behind the proposal?\n- When testing the viability of a range of learning rates by training a small number of steps, the authors used 20 steps for each learning rate. Why?\n- How are the sets separated for training, validation, and testing? If only two sets (training and test) have been used to find the hyperparameters, how do the authors know the results are unbiased?\n- The authors suggested that \"the capability of learning from a freshly initialized model is at a maximum, and if we increase the learning rate past that point we risk instability, without any improvement in the model's ability to learn\". Do they verify that experimentally?\n- What is the comparison with the state-of-the-art? What is the computational cost associated with each case? What is the influence of batch size and the number of epochs? What is the influence of the other hyperparameters in this choice? What is the relationship between training for longer epochs and using the proposed methodology?\n- Improve the figure's caption driving the reader to what needs to be understood from the figures. Please insert the axes description on the figure.\n- I am wondering about the impact on different loss functions. \n- The authors should investigate the impact of initial learning rate vs. batch size vs. epochs, robustness to different model hyperparameters, comparison to similar and SOTA approaches, and time requirements.\n- Are the experiments considering one execution or different runs of training are performed? It is essential to introduce variance to confirm the experiment's conclusions. Are all the methods starting from the same initial weights? Hyperparameters description? The experimental methodology description needs to be improved.\n- More experiments are needed considering different datasets and models with different sizes and complexities.\n- Are the models trained from scratch? What is the performance considering transfer learning?\n- What is the statistical relevance of the presented results considering statistical tests?\n- There has been little discussion on the limitations of the proposed approach. What are the possible technical limitations? ",
            "summary_of_the_review": "The paper presents an interesting approach but needs conceptual and methodological improvements. In addition, the novelty of the proposed method is limited. The authors need to provide more comments and address some concerns about the experimental methodology and experiments before publication.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors present a simple method which evaluates different learning rates for very few iterations using the same seed for batch sampling and initialization. The observed validation performance is then used to select a fixed learning rate which is used to train the model until convergence.",
            "main_review": "The motivation of the method is clear and the methodology itself is simple to use, two clear advantages of this methods. However, the weakness of this paper is the poor empirical evaluation (unclear protocol, few datasets, no baselines).\n\nThe experimental protocol in this paper is not sufficiently described. I have several questions. What kind of optimizer is selected and how do the authors define \"learning until convergence\"? There is absolutely no baseline. The authors mentioned a few candidates among which the learning rate which obtains smallest validation loss is obvious.\nBesides that, there are many adaptive learning rate schedules or optimizers (cosine decay, linear decay, Adam) that might serve as a baseline. I acknowledge, that these approaches require to choose a learning rate as well. Yet, it is unclear whether a default learning rate with an adaptive learning rate performs better than a fixed learning rate or whether the proposed learning rate selection scheme works with adaptive mechanisms as well. I recommend that the authors show that they can outperform Adam with default hyperparameters or a simple method such as a random hyperparameter search limited to 20 steps for a learning rate of Adam. A connection to hyperparameter optimization could be pointed out.\n\nThe approach is evaluated only on three datasets. It is not clear why exactly those were chosen. I recommend to further investigate this approach and evaluate it on more datasets.\n\nThere is very little content in this paper. First, the paper has barely 8 pages and is mostly filled with oversized figures. But also the conveyed message is rather short and could be easily explained on a single page. This is not necessarily bad, but I think there is sufficient space to evaluate the proposed method on more datasets and possibly more diverse settings (finetuning vs learning from scratch, CNN vs FFNN).\n\nEven though the paper features many plots, the plots' quality is poor. In most cases the axis labels are missing and the axis tickers are placed inside the plots.\n\nA suggestion for the full approach: why not use active learning combined with a model predicting the \"learning rate/loss curves\" accurately and sample-efficient. The current approach seems to waste resources without a clear benefit.\nFurthermore, it is not clear whether this approach is useful. The authors could add an ablation study.",
            "summary_of_the_review": "The authors present a simple method that might be computationally inexpensive to find an appropriate learning rate. However, the claims are not sufficiently supported by empirical results.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work provides a technique to find a range of learning rates and 3 methods to find the optimal value from that range. The suggested methods are quite simple and seem to be promising based on some initial empirical results. However, the manuscript is not complete in my opinion and there are many open questions to answer before it is publishable.",
            "main_review": "The results in Figure 9 are based on fully trained (converged) models for every learning rate in the grid (as written in the 3rd paragraph of Section 4) but this is not affordable in real applications. In the introduction, the learning rates seem to be selected based only on a number of (20) steps which is more practical. \n\nThe description of the methods in the 2nd paragraph in Section 3.3 is not clear. I could only roughly get the idea but not the exact steps for the actual computation to find those points of interest. Also, the gain of this smarter learning rate proposal than grid search is not shown in the experiments.\n\nIt makes sense to me that the performance of learning rates are not comparable without resetting the model weights and data batches. But having additional experiments to support this claim will be more convincing.\n\nFollowing above, the initializations of the model weights and data batches will also affect the performances.Ideally one should evaluate several replications of different initializations. The authors may also want to provide some data points on whether these replications are necessary or not.\n\nIn the end, this work is closely related to the previous works, Smith (2015), Gugger (2018) and Surmenok (2017) but no comparison is done at all.\n\nMinor: Figure 2 left has no axis labels. Figure 3 has no ticks and labels on the x-axis. ",
            "summary_of_the_review": "As I already mentioned in the summary, the manuscript is not complete in my opinion.  There are many open questions (as suggested in the reviews) to answer before it is publishable. The writing and plotting of the paper also need to be improved for consistency and clarity. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}