{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This work aims to learn a shared embedding space in an auto-encoder architecture (student network) which potentially encodes the knowledge from various source (teacher) networks. These teacher networks are trained on the same/similar datasets for different tasks.\nThe  auto-encoder is optimized such that the reconstruction from the auto-encoder has same activations in the source networks as the original input. For this a weighted loss function which minimizes the layerwise mse loss between the activations of the original input and the activations from the reconstructed input. \nThus the work claims to induce knowledge embeddings where the loss (eq. 1) is aggregated for different source networks. \nThe method is constrained in the sense that the input formats of the student network is identical to that of the teacher networks.\nExperiments are performed on subsets based on objects of the COCO datasets for the segmentation network where the training stability and number of source networks from which knowledge can be transferred.",
            "main_review": "Novelty: The novelty in terms of architecture and the task explored remain limited as there exist work eg., [a] which also aim to learn a shared feature space in an autoencoder in the multi-task setup. The related work should also be cited and compared to. \n\nExperiments: \n1. The choice of the architecture for the auto-encoder is not clear. How are the number of layers for encoder/decoder chosen? Also, if the reconstruction is nearly perfect owing to the number of parameters in auto-encoder, how does this impact the learning (gradients/parameter updates) of the auto-encoder network and the shared embedding space?\n2. The experimental setup for multitask learning through knowledge distillation has been demonstrated in prior work [a,1] and the experimental setup consider in the paper is very different from the related work. \n3. The claims of scalability of the approach and the quality of the embeddings learnt by the method is not demonstrated in the experiments. \n4. There are issues regarding the training instability when considering deeper source networks. This should be discussed in more detail. This can potentially be resolved with the proper weighting of the loss functions? \n\na. Deep Asymmetric Multi-task Feature Learning\n[1]. Variational Information Distillation for Knowledge Transfer",
            "summary_of_the_review": "There are concerns with respect to the experimental setup and the choice of the datasets. The experimental setup is different from the related work [1]. Please clarify on what basis or parameters is the choice of the dataset partionining and the experimental setup made. As of now it seems to be arbitrary. \nThe experiments do not particularly demonstrate the advantages of the proposed approach over the existing methods. \n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces MIKE, a Multi-task Implicit Knowledge Embedding model, to transfer knowledge from pre-trained networks to a single autoencoder.  Both the pre-trained networks and the autoencoder are based on EfficientNet. MIKE  considers that the inputs of the pre-trained networks and the autoencoder must be identical. The input X is passed through the autoencoder to return a reconstruction X’. Then both X and X’ are fed to the pre-trained networks to produce two sets of activations taken from all the hidden layers within the pre-trained networks. The loss function is based on the difference between these two sets. After training, the encoder of the autoencoder can be reused for downstream tasks. ",
            "main_review": "Pros. \n\nN/A\n\nCons. \n\n+ The paper is not well-structured, and it is hard to follow, e.g., using different terms (e.g., source, teacher, segmenter) to refer to the same concept (e.g., pre-trained).\n\n+ The paper lacks the motivations. Why does MIKE need multiple pre-trained networks? Why doesn’t MIKE utilize a single network pre-trained on 11 different datasets? \n\n+ It’s is not really true when considering non-overlapping classes as different tasks. Why does MIKE use the loss function in Equation 1?\n\n+ The baseline model is the single autoencoder which is not a good baseline. MIKE is incremental from the model proposed in [1] and does not compare with this model.\n\n[1] Deep Feature Consistent Variational Autoencoder. WACV 2017.\n",
            "summary_of_the_review": "The paper lacks the motivations and is not well-structured. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a method for representation learning where embedding spaces from many different models can be merged into a single shared embedding space. Given a collection of source networks with different embedding spaces, the method involves training an autoencoder using two losses: (1) the reconstruction loss, and (2) loss from difference between source networks’ representations for the original input and the corresponding reconstructed instance. To evaluate the proposed method, experiments are conducted on the image segmentation task from COCO for 11 different task category sets, using EfficientNet based models, and evaluated using IOU. ",
            "main_review": "The idea presented could be interesting for scenarios that involve distilling representations from multiple networks into one, eg. for models from many different domains, different tasks etc. The approach, which is an extension of prior work from Larsen et al., (2016); Hou et al., (2017) to the use of multiple networks from which to distill, is clearly presented. However, the paper takes only a small step in validating the idea and doesn’t quite conduct extensive enough experimentation to make a convincing argument in favor of it. \n\nSome questions that might be useful to consider, especially for evaluating the quality of the learned representations (this is a very small subset):\n1. How does this approach compare to simply training/fine-tuning a multi-task model? Establish the trade-off between using this process vs. just gaining access to some data. This is even more important when considering pre-trained models.\n2. How does combining representations from truly different tasks impact performance? In the current study, the task remains image segmentation. What about representations from networks trained on image classification, object detection etc.? \n3. What about vastly different domains -- radiology images + imagenet + street scenes etc.? Why not just jointly train on these domains rather than distilling?\n3. Even within segmentation, consider experiments using multiple datasets for more robust conclusions.\n\nIt’s not clear what the baseline in Figure 3 is, but it seems to be comparing the single-task version? Is it presented as analysis for why the general approach is useful? \n\nIn addition to further experiments, it might be worth revising the narrative to make it clearer with stronger motivations and perhaps motivating examples as well -- what is a scenario where the source network and query data are available but no training data at all? \n\nOverall, the ideas presented could be interesting but the experiments are far too preliminary.\n",
            "summary_of_the_review": "Recommending rejection because experiments are unconvincing and preliminary.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors introduce a novel method for learning an encoder producing image embeddings from a series of source models (or teacher models), a method which they call MIKE (Multi-task Implicit Knowledge Embeddings). The encoder is obtained by training an auto-encoder to reconstruct an image. The original input and its reconstructed version by the auto-encoder pass through the source models and the loss is computed as the difference between the activations of the original input and the reconstructed version through the source models. The core intuition is that a reconstruction that preserves the patterns which are salient to the source network should produce activations that are similar to those of the original data.\nThe authors demonstrate the effectiveness of the method on a preliminary small-scale setup.\n",
            "main_review": "Strengths:\n- The method seems novel and simple.\n- The intuition and motivation are well explained.\n\nWeaknesses:\n- Though the preliminary results are encouraging, the paper is half-baked and requires more experiments to demonstrate the claims and analysis to understand the method.\n- Results are sparse, preliminary and the broadness of the setup doesn’t allow to draw general conclusions.\n\nQuestions:\n- I don’t get what the “baseline” (blue) and the “single task” (orange) setups are in Figure 3. What are their respective averages?\n- What does “trained against” mean?\n- How are the 3 tasks chosen in Figure 4?\n- What’s the main takeaway of Figures 5 and 6?\n- Experiments only consider segmentation source models (i.e. same tasks but on different domains or targets). Could you elaborate on how the method would work (and how well) if you consider source models with different tasks (image recognition for instance)?\n",
            "summary_of_the_review": "The results of this paper are preliminary and the paper is half-baked.\nI would suggest resubmitting to the next relevant conference when the paper is ready.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}