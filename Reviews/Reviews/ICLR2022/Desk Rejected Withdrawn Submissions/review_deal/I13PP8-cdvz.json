{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper targets the problem of learning invariant and task agnostic representation for free-hand sketch data (including handwritten digits) using Graph Neural Network. Authors show that the proposed method is spatially robust and claim to generate \"new digits\".\n",
            "main_review": "Pros:\n 1. The method aims to learn spatially invariant representations which aligns with humans ability to learn from sensory inputs (slight oversell here).\n\nCons: There are significant amount of concern over the proposed methodology, its correctness guarantee and the experiments performed.\n 1. Writing quality: The quality of writing is quite average or sometime below that. The paper is filled with loosely framed sentences that needs to be far better in order to be acceptable.\n 2. Methodology: The framework, as a whole, is quite underwhelming. It consists of two parts: the second part is just an out-the-box usage of Message Passing Neural Network (MPNN); the first part is a carefully hand-designed encoding of sketches. It is claimed to have \"spatial invariance\" (only translation and rotation, others are still missing) merely because of the hand-crafted encoding - it isn't really learning the invariance itself.\n 3. The specific hand-designed encoding of sketches isn't very attractive either. Pairwise distances between \"intra\" and \"inter\" control points is quite a brute-force way of preserving the geometry of the strokes, which is algorithmically quadratic.\n 4. The definition of \"strokes\" used by authors is semantically inconsistent in different data formats. When raster images are \"preprocessed\" by detecting fork points, we basically retrieve \"segments\" (at least that's what Fig.1 is suggesting), not really \"strokes\" which is the case in QuickDraw data. Moreover, such preprocessing of raster sketches can itself involve significant amount of error when the sketches are not so simple - this was never addressed by the authros.\n 5. The methodology section is written assuming the input is an image (named 'X') followed by raster-to-vector extraction process is mentioned. However, one of the datasets used in experiments (QuickDraw) does not even require such preprocessing. I suggest that a major reorganization of the methodology would be required with the preprocessing (raster-to-vector) step rewritten as part of data preparation (for raster datasets only).\n 6. The usage of QuickDraw dataset is quite minimal with only few \"sub-parts\" used in experiments. How did the authors separate the sub-parts since QuickDraw doesn't have any such labels?\n 7. There is no explanation on why the classification accuracies are so low (for the proposed method). Even the baseline CNN accuracy (on QuickDraw) seems to be lower than I expected.\n 8. How can accuracy be \"zero\" (as in some of the setting mentioned in the paper)? How many classes are we talking about? Even with random predictions, a classifier would achieve a rough accuracy of 1/(# of classes).\n 9. The idea of new digit generation is either flawed, or not presented properly. Section 3.3 (along with the algorithm) is hard to follow with no proper justification or explanation of the method. In general, I am not at all convinced about the theoretical correctness of such \"generation\" process. Even if its correct, what is the utility of generating such new digits (which, to me, looks quite random in nature)? What distribution do they come from? It's fairly handwaving to say that the new characters are \"visually distinguishable\", since any random stroke would also be visually distinguishable from the data distribution. The authors must argue how they are statistically related to the original data distribution.",
            "summary_of_the_review": "Overall, this paper is rather below par for a conference like ICLR. The motivating argument sounds rather generic/overselling and certainly not sketch-specific. Methodology and experiment all have major(ish) problems, but the writing itself is already a fatal flaw.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "In this paper the authors propose a novel graph-based sketch representation. Each stroke is represented by a fixed number of control points in which bipartite distances between the control points form the vertices of a graph. The graph edge is constructed from bipartite distances between control points of 2 connecting strokes. The presentation is translation and rotation invariant by design. The authors demonstrate the robustness of their representation against spatial transformation on the MNIST and Quickdraw datasets. Experiments on the robustness in adversarial attack scenario and the ability to generate novel sketch patterns are also reported.",
            "main_review": "Pros: \nThe idea of representing sketch as a graph of strokes is interesting. The innate invariance to translation and rotation is a desirable feature (although it has its own caveats, more details below).\n\nThe authors report some interesting features of their proposed sketch modelling design. Their analysis on the behaviour of the model when altering the graph geometry and vertices/edges is valuable. \n\nCons:\n(i) The proposed stroke representation method does not seem to be able to model spatial relationship between non-connected components in a sketch. If two strokes are connected to each other or via other strokes, their spatial relation is encoded via the graph edge, however if they are totally isolated (e.g. a sketch of a donut), their relative distance is lost in this representation. In other words, there exists multiple sketches having the same representation.   \n\n(ii)  Another issue with the design comes from its own rotation invariance feature. An obvious example is digit 9 and 6 having the same representation (although in practice, the preprocessing step Lake etal 2015 may results in a different set of strokes, thus different graphs).\n\n(iii) The scale of the classification experiments on Quickdraw is a bit underwhelming. The authors only use 10 classes out of 345s to test their model, hence the scalability of their model is still questionable. Also, it is surprising that the CNN baseline performs so poorly on the spatial robustness test (0.01%). Does the authors train this baseline using spatial transformations as augmentation methods at all?\n\n(iv) The authors claim their method is task-agnostic and can be used for any downstream task, yet do not report any experiment using the learnt features for a downstream task. So I think this is overclaimed. ",
            "summary_of_the_review": "Overall I feel the proposed sketch representation interesting but lacks several important features and experiments to validate  thoroughly.  ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper presents a novel way of representing sketches as undirected graphs where strokes are represented by graph's vertices and stroke connections represented by the graph's edges. The method's inputs are rasterised sketches, upon which the Lake et al.'s (2015) method is applied to decompose the image into strokes and the strokes into control points. Specifically the graph vertice is represented by the pairwise distances between control points while the graph's edges are represented by the pairwise distance between the control points across strokes. \n\nSince the method does not enforce positional information into the graph, it is rotation and translation invariant by design. An advantage the authors demonstrate in experiments for both MNIST and subsets of Quickdraw.",
            "main_review": "- Strengths -\n\nThe authors take a method for inferring strokes from raster images and design a novel graph representation around this procedural inference. The main advantage of their design is of course that the graph representation is invariant to translation and rotation. With this \"limitation\" in place, the optimisation process naturally encourages representations to be aligned to stroke relationship and key point relationships.\n\nThe paper is well written and at most times easy to follow and I found it specially useful that the authors gave brief overviews of complex methods they borrowed from other authors such as Lake et al.'s stroke inference and Li et al.'s Gated Graph Neural Networks.\n\n- Weaknesses -\n\nThe experimental setup is overly simplified and does not paint a full picture on the usefulness of the method. MNIST, whilst widely used, is usually reserved for smaller experiments; furthermore the baseline network used for MNIST is not well established. \n\nThe Quick! Draw! Dataset is promising, but the authors chose to take very small subsets of it, why was that choice made? The authors say it is because of the “abstractness of some categories”, but other studies have performed well on the full set with 345 categories. The authors took a subset with 1000 samples per class for ~10 classes. Such a small subset is not statistically significant to test on (they take 100 samples per class for testing as well), specially since there are tens of thousands more samples available. Other studies have also taken the same 1k sample size from each class, but have done so for all 345 categories.\n\nThe “smallness” of the chosen sets is further aggravated by the choice of metric to show spatial robustness: “if one of the transformed samples fails a model, the model is then considered not robust against spatial transformations wrt the input image”. Considering that the authors apply 775 combinations of translations and rotations to the model, that makes this a very hard metric to beat, except that the presented model is invariant to translations and rotations by design, which means it will get a perfect score regardless of the number of rotations and translations applied. This “unfairness” could be alleviated by (1) making the metric much more reasonable, using for example the mean accuracy across all transformations, and (2) by including transformations that the presented model is not immune to by design, such as stroke drops and scaling.\n\nI believe the best way of demonstrating that the models “are able to accomplish all tasks” would be to perform different traditional tasks where comparable literature existis. The choice of tasks here is ok, but maybe the authors should reconsider calling the model “task agnostic” or “robust to all tasks” if the presented tasks have no baseline to compare against (except for the classification task). \n\nAnd considering baselines, the authors only compare their model against the Multi-Graph Transformer and an Inception network not designed for sketch recognition, missing other sketch specific studies that were applied to QuickDraw recognition before such as SketchMate and LiveSketch\n \n- Could Be Improved -\n\nThe abstract could be revised. The first sentence says that end-to-end vision models do not present spatial invariance, but traditional convolutional layers do offer spatial invariance as long as pooling operations are not used. \n\nThe abstract and introduction also mention many cognitive studies, but few have citations attached to them. “Unlike the human vision system […]”, “following the insights that the human vision […]”.\n\nThe motivation is not very convincing: “in this paper we assume that non-sketch inputs […] can be ignored, and it is feasible to convert images into sketches”. Sketch research does not need to bounce back to general computer vision to be relevant, consider how sketches are easier to do and have been a part of human expression for a very long time.\n\nThe contribution of generating hypothetical “new classes” is a consequence of the search space being limited to the graph representation. The authors mention that the newly generated digits share a similar visual style to the MNIST ones, but that is only to be expected given the natural limitation of the single stroke representation employed. To drive this idea across to the reader the authors should consider showing what digits from the same class look like, showing if they look indeed like a new digit class.\n\nThe boundary constraint is not clear enough in the text. How is it implemented?\n\nThe experiment for altering the graph geometry (section 3.2.1) is very interesting, but it also shows one of the weaknesses of the model: the invariance to rotation can lead to loss of meaning. This happens with both “7”s presented as one is upside down and the other looks mirrored. It is normal for methods to have limitations, but the paper could be improved with the authors discussing and highlighting them. On that note, it’d be interesting to see how the model behaves in a confusion matrix, as the expected outcome for a rotation invariant method would be that 9’s and 6’s would be confused with each other quite a bit.\n\n",
            "summary_of_the_review": "While the authors present a novel representation that is interestingly invariant to rotation and translation by design, in my view they failed to design the experiments in a way that would highlight the full benefit of their model while also assuaging the fear that this representation might be too restrictive for general sketch recognition. My initial recommendation then is to reject the paper whilst hoping that I'll see this work in the future with an expanded and improved experimental setup.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes to represent images as sketches (i.e. networks of strokes), and to encode these sketches as a graph where nodes correspond to strokes, and edges link intersecting strokes. The motivation for this representation is to provide invariance to spatial translation and rotation, which is achieved by describing each node and edge of the graph solely in terms of distances between point samples along the strokes. The resulting graph is then processed by a Graph Neural Network (GNN), trained for different tasks.\n\nThe representation is evaluated on simple sketches extracted from digits (MNIST) and hand-drawn doodles (QuickDraw). On a classification task, the proposed representation achieves lower accuracy than CNNs and than a recent method based on Graph Transformers that encodes the absolute coordinates of the strokes. However, the proposed representation outperforms these two baselines when the sketch is transformed by translations and rotations not present in the training set. More precisely, the proposed method maintains its accuracy on transformed sketches thanks to its invariance, while the accuracy of the other methods drops significantly.\n\nAnother experiment shows that the representation supports optimization of the graph structure and values, allowing to deform a sketch to resemble a target class (e.g. deform a 6 to make it look like a 0). Finally, an experiment shows that the representation can be used to generate new digits that can be distinguished among themselves and among other digits, effectively creating digits for an hexadecimal basis after having been trained to classify the MNIST decimal digits. In both experiments, the stroke-based representation produces images that preserve some form of structure, while a bitmap-based representation would likely break the lines apart when morphing between digits or when creating new ones.",
            "main_review": "**Strengths**\n- The proposed representation is simple and achieves its goal of transformation invariance. \n- The representation also carries the structure of the strokes, ensuring that images generated/modified by the model look like sketches.\n\n**Weaknesses**\n- The method is only demonstrated on very simple sketches composed of small number of strokes. How does it scale to more complex sketches?\n- The related work section is quite short. Other papers on sketch representations could be discussed. For example, this recent work also used GNNs for sketch classification, although it relies on absolute coordinates and as such wouldn’t be transformation invariant: https://www.youyizheng.net/docs/Sketch_TOG.pdf\n- I would also have liked a discussion of other attempts to use graph-based representations of visual content to achieve invariance. While I don’t know much about this topic, the following references might be a good start:\n\nLearning Graphs to Model Visual Objects across Different Depictive Styles\nhttps://link.springer.com/content/pdf/10.1007/978-3-319-10584-0_21.pdf\n\nWhich is based on\nhttps://www.di.ens.fr/willow/pdfscurrent/cho2013.pdf\n",
            "summary_of_the_review": "Unfortunately, I don’t feel very well equipped to evaluate this paper, because I am a researcher in computer graphics / computer vision applied to sketches rather than a researcher on visual representations. \n\nAs long as the application to sketch classification is concerned, the results shown in the paper are not yet at the level of complexity of the ones shown in state-of-the-art work (see reference above). \n\nThe value of the representation thus rather seems to lie in its invariance to transformation, but I lack knowledge about other transformation-invariant representations to judge the magnitude of this contribution. The references on graph-based representations cited above suggest that such representations have a long history in visual modeling, but I am far from an exepert.\n\n--- Additional questions ---\n\nWhile the experiment on classification under transformation is convincing, I would have like to know how well the baseline method performs if trained on a dataset that has been augmented with similar transformations.\n\nIn section 2.4.2, I am not sure to understand how the stroke s is recovered when minimising Eq. 5. I assume that the descriptors (i.e. distances) on the edges and nodes of the graph can be optimized by backpropagating through the GNN, but how to then find the sketch that corresponds to these descriptors? Maybe using Multidimensional Scaling?",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}