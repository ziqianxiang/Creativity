{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This work benchmarked out-of-distribution (OOD) detection methods that do not require finetuning. The results indicates that large scale pre-trained models can bridge the gap between methods require fine tuning and methods that do not require fine tuning, and that Mahalanobis Distance method works particularly well with large scale pre-trained models. ",
            "main_review": "Significance\n\nStrength\n\nIn general, I value benchmarking works a lot as they are a lot of work and provide useful and systematic insight into a particular problem. And I do appreciate the authors’ effort in this work. \n\nWeakness \nHowever to provide robust insight and comprehensive into a particular problem through benchmarking, it is necessary to include all the representative methods and tasks in the prior work. This is why benchmarking projects are hard. In this work, CIFAR10/100 were experimented as in distribution dataset, which is even fewer than some of the non-benchmarking papers in OOD detection, and only three methods were considered while there are so many other competitive methods in this field. Just to name a few: \n\nSastry, C.S. &amp; Oore, S.. (2020). Detecting Out-of-Distribution Examples with Gram Matrices. <i>Proceedings of the 37th International Conference on Machine Learning</i>, in <i>Proceedings of Machine Learning Research</i> 119:8491-8501 Available from https://proceedings.mlr.press/v119/sastry20a.html.\n\nPapernot, N., & McDaniel, P. (2018). Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning. arXiv preprint arXiv:1803.04765.\n\nBenchmarking projects on related topics are usually at a much larger scale, here are some examples:  \n\nTaori, R., Dave, A., Shankar, V., Carlini, N., Recht, B., & Schmidt, L. (2020). Measuring robustness to natural distribution shifts in image classification. arXiv preprint arXiv:2007.00644.\n\nYe, N., Li, K., Hong, L., Bai, H., Chen, Y., Zhou, F., & Li, Z. (2021). OoD-Bench: Benchmarking and Understanding Out-of-Distribution Generalization Datasets and Algorithms. arXiv preprint arXiv:2106.03721.\nThus I think this submission is not significant enough for the ICLR community yet. \n\nNovelty\n\nAs a benchmarking project, the only novelty in my opinion is the use of the pretrained models, which is somewhat expected given the current trend in deep learning, so I’d say this is only marginally novel. Everything else does not look novel to me. \n",
            "summary_of_the_review": "The main reason of my rating is insufficient number of in distribution datasets and more importantly, insufficient coverage of competitive methods considered in this work. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper studies the effects of large-scale pre-trained models and different model architectures on unsupervised OOD detection methods.\nThis is achieved by comparing OOD detection performance of 3 well-known OOD detection methods on different both large scaled pre-trained and randomly initialised networks.\nThe main findings of the paper: 1) model architecture significantly affects OOD detection performance, 2) large scale pre-trained models improves unsupervised OOD detection and matches supervised OOD detection performance, 3) one of the existing methods called Mahalanobis outperforms other OOD detection methods.",
            "main_review": "- Despite the interesting findings and analysis in the paper, I have some concerns about novelty of these findings and sufficiency of the experiments:\n1) Effect of different model architectures on OOD detection have already been explored in previous work to some extent. For example, Mahalanobis and Generalized-ODIN (CVPR 2020) papers investigate different architectures such as Resnet, WRN, and DenseNet and report different performance for different architectures. So, it is already a known fact that different architectures effect OOD detection performance. The paper expands the architecture list by adding ViT. However, in my opinion, this doesn't add much novelty to the ultimate message of different architectures effect OOD detection.\n2) As mentioned in the paper, Hendryks et al. (2019a) have already shown that large-scale pre-training improves OOD detection. The paper mentions 3 points that distinguishes this paper from Hendryks et al. (2019a): 1) using even a larger version of ImageNet dataset for pre-training: why we should consider this as a significant addition to the previous work? I think this message should be given clearly. As is, using even a larger dataset for pre-training doesn't seem significant. 2) evaluating performance of more OOD detection methods: using only 2 more methods is really not enough for a paper that analyses different methods in different settings. Especially, more unsupervised OOD detection methods should be added to the analysis such as [1, 2, 3] 3) investigating the impact of different architectures: this doesn't seem really novel to me as I explained in the above comment. \n[1] Task-agnostic Out-of-Distribution Detection Using Kernel Density Estimation\n[2] Unsupervised Out-of-Distribution Detection by Maximum Classifier Discrepancy\n[3] CSI: Novelty Detection via Contrastive Learning on Distributionally Shifted Instances\n\n- The paper focuses on unsupervised OOD detection problem. So, there is no access to target OOD dataset. How are the hyperparameters of the methods determined in this case? For example, how input perturbation strength parameter of Mahalanobis is determined?\n\n- The paper states that RQ1 is uncovered/answered by the confidence calibration analysis. Is the answer to RQ1 is that OOD detection performance vary significantly across different architectures because they have different calibrations? What is the evidence to reach this answer? For example, is there any pattern which shows OOD detection performance increases as the calibration error decreases.",
            "summary_of_the_review": "I have 2 major concerns about the paper:\n* Novelty: Two main contributions mentioned in the paper are the analysis on effect of different architectures and using large pre-trained models for OOD detection. However, such experiments have already been done to some extent in previous works as mentioned in my main review. Despite there are some differences and additions in this paper, I didn't get any new message or finding that were not available in previous work.\n\n* Lack of comprehensive analysis: I would expect to see more detailed analysis for the setting investigated in the paper. More specifically, 1) more recent unsupervised OOD detection methods should be added to the analysis, 2) more network architectures should be investigated e.g. different transformer architectures in the concurrent work by Fort et al.\n\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper aims at indentifying and explaining the differences in performance between known out-of-distribution (OOD) detection techniques and between their realizations with different architectures.\nConcretely, it investigates the performance of MSP, Mahalanobis OOD detection and Energy score with six commonly used computer vision architectures.\n",
            "main_review": "Having a systematic evaluation suite for out-of-distribution detection is certainly an important asset that the community is up to my knowledge currently missing. However, I do not see that this submission provides it. Especially since it does not propose a new method itself (which is not by itself a reason for rejection), the regarded techniques and provided evaluations are by far not comprehensive enough for such a review paper at ICLR.\nFor example, the evaluated out-of-distribution datasets are fewer than those shown in previous works (e.g. Hendrycks et al., 2019b, Cheng et al. 'Informative Outlier Matters'). The list of six evaluated architectures does also not cover a comprehensive collection. By considering only three OOD detection methods, the paper does not give a good overview of the field.\n\nThe overall presentation is ok, with issues in one section mentioned below. The experimental methodology follows (from what can be inferred reading the paper) good practices.\n\n\nQuestions, required clarifications, suggestions:\n\nIn the original Lee et al. (2018b), Mahalanobis distance based OOD detection adds a gradient to the input (size depends on the test out-distribution) and uses regression over Mahlanobis distances within the separate layers which is tuned on the test out-distribution. Not using the test out-distribution (i.e. supervision in the context of this submission) is good, but the differences should be mentioned. In Lee et al. (2018b), a version which does not rely on OOD samples but uses adversarials is shown and might work better than not using input noise and all layers, so it should at least be mentioned. In 2.2, it says that \"f(·) denotes the output of an intermediate layer\" -- I assume that in the experiments (-> assumption confirmed only in the discussion of learned representations), the penultimate layer of the models is used, but this should be specified and the choice should be motivated (maybe just with a citation).\n\nBesides distinguishing 'supervised' and 'unsupervised' OOD detection approaches, for the 'supervised' ones it is important to distinguish if an approach needs data from the same distribution (not test set) as it is tested on or if it only needs access to a general surrogate out-distribution while it is test-distribution agnostic.\nTo me, it seems like from a practical standpoint, 'supervised' methods needing access to a vast surrogate out-distribution is little different to the use of a large pre-training dataset like ImageNet-21K. Potentially, training Outlier Exposure with properly downscaled ImageNet-21K as surrogate training OOD might work to some extent. I think a comparison of the assets necessary to implement these methods would benefit the paper.\n\np. 2: 'In Outlier Exposure, it requires careful curation relative to the training classes.' Up to my knowledge\t, this has not been demonstrated. In fact, the 80 Million Tiny Images dataset that is used by the original OE actually contains a significant amount of objects that belong to the CIFAR in-distributions and even some near duplicates (while only exact duplicates that were the source of CIFAR had been removed).\n\nVision Transformer (ViT) should be cited as Dosovitskiy et al. 2021 (https://openreview.net/forum?id=YicbFdNTTy); the transformer of Vaswani et al., 2017 was not directly usable for vision.\n\nIn 2.4, the paragraph Confidence Calibration is rather difficult to understand due to bad grammar and stucture. Also, it does not become clear why the Mahalanobis distance and energy should be converted into confidences like this. For example, multiplying the score with a constant positive factor (similar to a temperature rescaling) at first would also yield \"values [in]\nthe interval [0, 1] while maintaining the relative order of original values.\", and since there is (or is there?) no inherent scale, these formulas need to be justified.\n\nAUPR depends on the number of in- vs. out-of-distribution test samples which should be mentioned and the numbers should be given in the appendix. Also, it should be noted explicitly what is counted as true, positive etc. in the calculation of precision and recall, as there are in general two non-symmetrical alternative definitions.\n\nWith the limited space in mind, Since the three metrics AUROC, AUPR, FPR95 are usually quite similar as a comparitive indicator of detection performance (in most cases, the ordering of the methods/architectures in the same in all three metrics), I would suggest presenting the results in one metric of all datasets in the main paper, and moving the other two metrics into the appendix.\n\nIt is good that an evaluation the confidence calibration of the different methods is included.\nComparing ECEs of models with significantly different accuracies is in this case not very informative, since if all models are generally overconfident, it would be almost impossible for a model with only 1.48% test error can only really accumulate calibration error over very few samples.\nObtaining confidences from Mahalanobis and Energy scores does not make much sense to me, as mentioned above. If this has been done and analyzed before, previous observations should be cited and compared to, and if not, such a novel evaluation of Mahalanobis and Energy detectors should contain more in-depth analysis, including for example a version of Figure 1 with them.\n\nThe qualitative analysis in the first paragraph of 'Learned Representations' is mostly just the definition of the Mahalanobis score. Also, detoriation on a class of OOD\tinputs does not depend on the distributional properties like being Gaussian but only on the distribution of the Mahalanobis distances to the empirical mean.\n\nRegarding that the main contribution is \"Sneakoscope, a unified evaluation suite\", is it an evaluation pipeline with accessible code?\n\nBesides Fort et al., 2021, also Koner et al.'s OODformer (https://arxiv.org/pdf/2107.08976.pdf) cover pre-trained transformers.",
            "summary_of_the_review": "Having a systematic evaluation suite for out-of-distribution detection is certainly an important asset that the community is up to my knowledge currently missing. However, I do not see that this submission provides it. Especially since it does not propose a new method itself (which is not by itself a reason for rejection), the regarded techniques and provided evaluations are by far not comprehensive enough for such a review paper at ICLR.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper provides a series of evaluations of \"unsupervised\" OOD detection along three axes: 1) w/ and w/o pre-training, 2) model architectures, and 3) detection scoring mechanisms. Correspondingly, the authors identify that 1) large-scale pre-training is powerful and \"compares favorably with supervised detection methods\", 2) model architectures can affect detection performance, and 3) Mahalanobis Distance as a metric yields the best performance. The paper summarizes its contributions as \"provide new baselines for unsupervised OOD detection methods\".",
            "main_review": "I have the following concerns/questions/comments along the three axes of Sneakoscope's evaluation, and also regarding the novelty/contribution of this work.\n\n### With and with-out Pre-training (Major)\n1. My first fundamental question, and also my biggest concern about the major claim made in this paper, is whether pre-training is really an \"unsupervised\" OOD detection method. Consider these two cases: 1) One fine-tunes an ImageNet-pretrained model on CIFAR-10. 2) One first trains a CIFAR-10 model, and then fine-tune with OE using ImageNet as training outliers. I do not clearly see the essential differences between 1) and 2), and I'm much more hesitant to say that 1) is an unsupervised method while 2) is a supervised method.\n\n2. The paper argues that \"massive amount of OOD data\" \"may not always be available\" (page 2, first line), and seems to suggest that pre-trained models are more accessible/feasible than OE. However, given that essentially both pre-training and OE requires large-scale extra training data, I could not imagine a case where pre-trained models are significantly more likely to be available than OE. In natural imagery domain where there are abundant pre-trained models, practitioners can meanwhile always use public dataset like ImageNet, OpenImages to serve as training outliers for OE. In other domains like medical imagery and satellite imagery, while it might be difficult to collect extra data for OE, it is also less likely that a pre-trained model is available.\n\n3. It is argued in the paper that for OE \"one needs to carefully and laboriously curate the outlier dataset so that there is no overlapping between in- and out-of-distribution data\". While this statement conceptually makes sense, I would like to see fact-based justifications. Are there any references or experimental evidences that if there are samples in the outlier set that overlap with in-distribution data then OE will not work?\n\n4. Finally, I have concerns regarding the comparison between pre-training and OE. In the \"Comparison with Supervised Results\" paragraph under the Table 3, it is stated that the results with pre-training and Mahalanobis \"is better than dedicated fine-tuned results reported in [1] and [2]\". I assume this conclusion is obtained by comparing the results of Big Transfer in Table 3 with the results in [1] and [2]. However, [1] and [2] both used Tiny Images as the outliers while Big Transfer uses ImageNet-22K as the pre-training data. It would be a more direct and convincing comparison if the Big Transfer is pre-trained using Tiny Images, or [1] and [2]'s results are reproduced by using ImageNet-22K as the outliers. Even if the same data is used for both pre-training and OE, the comparison could still be somewhat unfair since OE-based methods typically fine-tune for a few epochs [1,2], which means that the model will only see a relatively small amount of data in the outlier set for very few times (perhaps just once if the outlier dataset is large enough). However, in pre-training, the model will see the whole dataset over and over again for a large number of epochs.\n\n\n### Model architectures (Somewhat minor)\n\n1. I do like this idea of investigating model architecture's effect on OOD detection since it is somewhat unexplored in the community. However, I expect more informative findings/analysis than what is presented in the submission. Specifically, while in Table 2 it is observed that there is variation across architectures, it is not clear whether there is a consistent pattern as to which architecture is better or if there could be any intuition for why certain architectures are better. Also, are the ResNet, DenseNet, Wide-ResNet used in Table 2 all have comparable capacity in terms of e.g. #parameters? If not then this makes the effect of architecture more unclear since the capacity can also play a role here.\n\n\n### Mahalanobis Distance v.s. other scoring mechnisms (Minor)\n\n1. I suggest the authors be more careful about concluding that Mahalanobis detector \"outperforms\" MSP and Energy score. Just as the authors notice, in Table 2 when without pre-training, Mahalanobis only outperforms MSP and Energy when the OOD samples are much different from the ID data (SVHN, textures, and gaussian noise), but not when the OOD samples are from CIFAR-100. Since SVHN/textures/gaussian noise are by construction so different than CIFAR-10, they might present many non-semantic shifts (e.g., differences in imaging conditions or low-level statitics) [3], and it is unclear whether Mahalanobis is really a powerful detector or it is just more sensitive to the non-semantic shifts in the OOD data. Moreover, Mahalanobis is known to have scalability issue when the number of ID classes is large [4]. If necessary, consider putting a few sentences talking about these limitations of Mahalanobis.\n\n### Novelty/contribution compared with previous works (Somewhat major)\n1. I find the separation of this work from previous works that incorporate pre-training not strong enough. Specifically, the separation of this work from [5] is identified as that [5]'s \"pre-training scale is limited to a down-sampled version of ImageNet-1K\" and \"is considerably smaller than the ImageNet-21K pre-training scale evaluated in this work\". While this is true, it does not make an obvious novelty since the idea is still pre-training, and the large-scale pre-trained models used in this work simply haven't came out when [5] is published. To increase the novelty, instead of just shifting to models pre-trained with more data, the authors could pre-train multiple models with varying size of dataset (e.g., ImageNet-1K/5K/10K/21K) and evaluate the OOD detection performance w.r.t. pre-training size. I believe this kind of exploration will contribute more to the OOD detection research from the aspect of pre-training.\n\n\n\n[1] Deep anomaly detection with oultier exposure.\n\n[2] Energy-based out-of-distribution detection.\n\n[3] Detecting semantic anomalies.\n\n[4] Scaling out-of-distribution detection for real-world settings.\n \n[5] Using pre-training can improve model robustness and uncertainty.",
            "summary_of_the_review": "As discussed in the Main Review, I do have some significant concerns on the major claim of this paper (that pre-training is an \"unsupervised\" OOD detection method), and the novelty/contribution compared with prior works is somewhat limited. These concerns currently prevent me from recommending acceptance.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}