{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes to perform meta learning via subspace structure i.e., each task's parameters are restricted to belong to a particular sub-space and the (parameters for the) sub-spaces are meta-learned. The meta-learning as well as task-specific learning are performed via gradient descent over appropriate objective.\n\nTheoretical guarantee for convergence of the meta-learning gradient descent is presented. Samples bounds are also presented.\n\nDetailed simulation results are presented comparing various baselines and over standard meta-learning benchmarks.",
            "main_review": "Strengths:\n1. Work has some novelty: Subspace structure seems to be new in meta-learning literature. However, in related multi-task learning literature this structure is very popular e.g., \"Multi-Task Feature Learning\" Argyriou et.al. etc.\n2. Exhaustive simulations are performed illustrating the efficacy of the proposal.\n\nCritical Comments:\n1. It is mentioned that k^*_\\tau dependence is ignored in gradient computation. While it is clear that this facilitates computation, it's adversarial affect on convergence/generalize are not known. Neither of the theorems in sec 3.3 consider this error. It will be helpful if some idea of why this is justified in terms of convergence/generalization is provided. Can some technical issues be alleviated with manifold optimization tools? \n2. It is mentioned that gradient of v^*_\\tau is O(T_{inner}dm^3). Does not the computation depend on loss? In any case, a sentence or two about this would help the reader. Also, mentioning overall complexity would help.\n3. There seem to be two issues in theorem 1:\n     a) Is there a intuition why Expected number of times a subspace is selected is T/K ? It seems to depend on task similarities etc.\n     b) Even in this case it seems results in Fallah et.al. etc. are with norm of gradient and not norm-squared as in LHS of theorem 1 result. How can this discrepancy be explained?\n4. Discussion in sec3.4 also may be important as not all task parameters may lie in a small subspace.",
            "summary_of_the_review": "Overall, the paper is organized well and easy to read. The subspace structure seems to be novel. However the optimization and convergence analysis seem to have minor issues that are highlighted above. The simulation results are exhaustive and encouraging.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper focuses on an important but practical learning problem, i.e. meta-learning under task heterogeneity scenarios. To achieve the problem, the paper proposes a multi-subspace structured meta-learning (MUSML) method which embeds the tasks into different subspace. In addition, the paper theoretically proves the convergence and generalization performance of proposed method, along with the positive experimental results.",
            "main_review": "Pros: \n1. The learning problem studied in the paper is pretty important and practical. \n2. The paper provides the theoretical proof for the convergence and generalization performance of proposed method.\n\nCons:\n1. Section 3.2, multi-subspace structured meta-learning is not well driven and clarified.\n2. In abstract and introduction, the paper refers the recent structure meta-learning methods fail to take advantage of negative correlations between tasks, but the authors did not well analyze how the the proposed MUSML handle this problem.\n3. The paper gives an inadequate analysis for the experimental results.\n4. The latest methods of few-shot learning should be compared in the experiment section to demonstrate the effectiveness of proposed method.\n5. The related work section does not well clarify how the proposed method address the issues of existing works.\n6.  The methodology section is not so informative and well analyzed. For example, how these subspaces are learnt? How the negative correlation is pursued in the learning process.\n7. The authors pointed how Kong et al, 2020; Tripuraneni et al. 2021 can not handle nonlinear model or general loss, but these two extensions are not so difficult for a shallow technique. These issues can be further extended and well analyzed.\n\n",
            "summary_of_the_review": "The paper studies a practical problem, i.e. meta-learning under task heterogeneity scenarios, and the experimental results confirm the effectiveness of proposed method. However, there are some key weakness needed to be improved, e.g. inadequate result analysis and the proposed method is not well driven. The main technique ideas are more closely related with MAML and prototype networks, the former performs inner/outer meta learning, and the prototype network performs learning subspaces, and the technical novelty is not so solid. Overall, I do not recommend this manuscript as an accepted paper for this top-tier conference.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a meta-learning approach by learning the multiple subspaces.  The authors say this algorithm is model-agnostic, which can be used for o both linear and nonlinear models. The experiments show that the proposed framework outperforms the SOTA methods.\n\n",
            "main_review": "Although the meta-learning method proposed in this paper has been experimentally proved to outperform the SOTA methods, in Section 3, the method is not described clearly, especially the author has a series of assumptions and does not give a detailed explanation. \nIn addition, this paper has some grammatical errors.",
            "summary_of_the_review": "I expect the authors to explain the method clearly. So far, I don't really understand the essence of the author's method. Therefore, I set the score to 5.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Responsible research practice (e.g., human subjects, data release)"
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "To handle task heterogeneity, this paper proposes a MUlti-Subspace structured Meta-Learning (MUSML) algorithm to learn the subspace bases. Its convergence and performance are established and analyzed, respectively. Experimental results verify the effectiveness of the MUSML algorithm.",
            "main_review": "This paper tries to overcome the disadvantages of both the centroid-based clustering methods (Jerfel et al., 2019; Zhou et al., 2021) and the moment-based methods (Kong et al., 2020; Tripuraneni et al., 2021), and proposes a MUlti-Subspace structured Meta-Learning (MUSML) algorithms. Although the authors provide theoretical analysis and experimental results, I still have the following comments.\n\n+ves: \n\n+ The idea of considering multi-subspace in the structured Meta-Learning is interesting. \n+ This paper establishes the convergence and analyzes the generalization performance.\n+ Experimental results verify the effectiveness of the MUSML algorithm.\n\nConcerns: \n\n- The problems of this paper are unclear. The first one is that the centroid-based clustering fails to (1) take advantage of negative correlation between tasks (e.g., w and -w may be assigned to different clusters) and fails to (2) handle tasks that are distant from all clusters (e.g., tasks tao’ in Figures 1(a) and 1(b)). ------In (1), what is the advantage of negative correlation between real-world tasks? what real-world tasks do satisfy the negative correlation? Please show some examples. ------In (2), the proposed method still fails to deal with the case because it contains some tasks, which also can be distant from all subspaces. For example, there is a task tao’’, which is not in the two subspaces of Figures 1(c). In fact, the task tao’ (or tao’’) is an outlier in the clusters (or subspaces). We can remove it or consider it as a new cluster or new subspace. \n\n- The second problem is that the moment-based methods are not easy to be extended to nonlinear models (e.g., neural networks) or general losses (e.g., cross-entropy loss). Why are they hard? Please explain them. As I known, it is not difficult to extend them to nonlinear models, for example, Deep Subspace Networks [ref-1]. Moreover, deep (subspace) clustering methods are also popular in machine learning and computer vision.\n\n- In this paper, there is no motivation to propose MUlti-Subspace structured Meta-Learning (MUSML) algorithm. In fact, the authors only show the problems in the second and third paragraphs. However, I do not see any reasons to explain an reasonable logic in the whole paper.\n-Although figure 1 shows the different formulations of the task structure, it is not good to show the advantage of the subspace structure. I suggest that given a same distribution of the tasks, single cluster and multiple clusters are not to cover all the tasks, luckily subspace structure can cover them.\n-In the Eq. (4), how to get 1-(enta_t*beta_1*beta_2)/2>1/2?\n[ref-1] Adaptive Subspaces for Few-Shot Learning, CVPR 2020.\n",
            "summary_of_the_review": "I recommend the score 5 (marginally below the acceptance threshold) because both the considered problems and the motivations are not clear, and the theoretical analysis is simple.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Responsible research practice (e.g., human subjects, data release)"
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}