{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "Most of the works in the literature of transfer learning focus on statistical similarity of source and target domains and they propose approaches requiring a retraining phase during the transfer process. This work, however, studies the connection of topological similarity and transferability with aim of guaranteeing vanilla transfer learning which does not require any retraining phase. It shows that topological similarity gives a necessary condition for statistical similarity not a sufficient one. Using this fact, it proposes a method to minimize the TV distance of source and target using linear homeomorphisms while keeping the topological properties intact.",
            "main_review": "The strength of the paper is the fact that it investigates the connection of the topological similarities and transferability which has not been understood well in the literature. Furthermore, it proposes a computational tractable method to easily transfer knowledge of source to the target which does not require any retraining which is common in most of the transfer learning approaches.\n\nThe issue is that given a pair of source and target task, how can one estimate the bottleneck distance of source and target domains to see if they are close to each other or not. Because in the proposed method, they assume that source and target tasks are already close in the sense of topological properties. It seems that they only claim that they want to utilize the topological similarities between source and target, but they just assume that they have similar topological properties and just focus on the linear homeomorphisms to keep these topological properties intact.\n\nThe paper is well written and easy to follow but the novelty of the work is limited and does not fit into competitive AI conferences such as ICLR.",
            "summary_of_the_review": "I recommend to find a method to estimate the bottleneck distance of source and target to identify transferable pairs of source and target tasks.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper discusses a topological similarity perspective of vanilla transfer learning. The idea is based on a lower bound relationship between the bottleneck distances of persistence diagrams and total variation distance. The paper then discuss how to \"learn\" a linear operator between domains that minimizes total variation distance with a randomized algorithm. ",
            "main_review": "**Strengths**\n\nThe paper discusses the connection between persistence diagrams, a popular tool in topological data analysis, and total variation distance statistical divergences. This motivates using source domains with a similar topological structure to the target domain.\n\n\n**Weaknesses**\n\n- Why not directly discuss the problem using minimizing total variation distances? It is sensible to find a linear homeomorphism to reduce the TV between $\\phi(S)$ and $\\phi(T)$ already, as you are not distinguishing them into different classes. \n\n- Is there any reason to include the topological argument here? The paper also does not evaluate anything related to TDA, such as the persistence diagram. It might be helpful to obtain the persistence diagram, compute the bottleneck distance before and after applying linear homeomorphism, which would then show that your method really works as intended?\n\n- It is hard to define what it means to be \"training\" since we can technically call any optimization procedure \"training over an objective function\". This paper also performs constrained optimization (approximation), even though $A$ is not acquired via gradient descent. The experiments also use \"positive\" and \"negative labels\" from which we have information about the target domain. \n\n- How is the generative linear classifier applied \"before\" applying the linear homeomorphism module? Would you learn a new linear classifier, which is the same as \"after\" with the same \"positive/negative\" splits? This part is quite unclear, so I am not sure how significant are the gains. For example, on CIFAR-10, training a full ResNet-50 can have at least 90% accuracy, whereas the proposed approach only has about 70% F-score (they are not identical, but given the tasks are binary classification here, they are not fantastic either).\n\n- This also brings about the \"training cheapness\" of the argument: the paper claims that this method is about 20 times faster than \"full retraining\", but there are fine-tuning methods that do not require full retraining of the model? Moreover, the method only works on one class, whereas fine-tuning methods scale much better with more classes, so with many classes, fine-tuning can still be faster?\n\n**Comments**\n\n- Why is A constrained to be an orthogonal matrix? Technically, any full rank matrix can work?",
            "summary_of_the_review": "The paper proposes an approach to vanilla domain transfer based on a relationship between persistent diagrams and total variation distance. Empirically, the paper did not compare with reasonable baselines under the same setting (such as fine-tuning, or other ways to optimize total variation distances), and did not experiment on the proposed application domains. Theoretically, I do not see where the topological argument is crucial (the same argument just with minimizing statistical divergences would work just as well, and preserving topology is not sufficient as we can allow any distance preserving transformation).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the authors discuss how to transfer a trained model to a dataset in a different domain without retraining. First, the authors formulate a method for model transfer without retraining based on topological information. Next, they show that the optimization problem in this formulation is APX-Hard. To this end, the authors present an approximate solution method as a practical approach and evaluate the effectiveness of the approximation method experimentally. In addition, the relationship with Topological Data Analysis is discussed.",
            "main_review": "Strength\n- Transfer without retraining is extremely useful when the computational power of the operational system is low, and the fact that we have demonstrated an effective method is very useful for the spread of AI.\n- This paper theoretically demonstrates the difficulty of finding solutions to optimization problems to be solved and a method for mitigating this difficulty, making a significant contribution to the development of transfer learning.\n\nWeakness\n- The overall structure needs to be reviewed. For example, the following points should be made:\n\t- In section 4, the definition of symbols, especially subscripts, is unclear. For example, the meaning of the subscript i in /phi_i is unclear, and it is not clear whether it means the i-th component of the vector /phi or /phi(x_i). Similarly, I don’t know what A_{d+1} means. We think it is the d+1th row of matrix A, but we cannot be sure because it is not stated. These prevent us from understanding the most important equation of this paper, equation (2), so we cannot determine what the formulation you want to claim is, or whether it is correct or not.\n\t- I think the method the author is advocating is that for the feature space of the original domain data /phi(X), we consider the feature space of the new domain data /phi(Y) and determine f so that the density distribution of f(/phi(X)) mapped by homomorphism f and the density distribution of /phi(Y) are close. This may be due to the above problem, but I don’t think equation (2) shows that. This optimization(2) contains only part of the matrix A. Therefore, the solution is highly flexible and A cannot be determined. Also, since u_{d+1} is determined by the source /phi(X) and target /phi(Y), it is not reasonable to transform it in the extended space. If it is reasonable, then the authors will need to provide the information necessary to understand it. If these are not validated, the discussion on computability will be meaningless.\n\t- Section 3 discusses TDA, but although it shows the use of topological information in the claim, it does not show the use of an argument about TDA. Although TDA is discussed in the supplement, it is not directly related to the main argument and is not a natural structure of the paper.\n- Methods of transforming the source distribution to make it closer to the target distribution have been discussed in [1] and elsewhere. These should be discussed as well, not just compared to the case where nothing is done. This is not to say that the contents of this paper are not applicable to the same task, but rather that there should be a discussion of how the concepts in this paper can be naturally derived for your task.\n- I don’t think this method makes sense for arbitrary transfer learning, and it should be discussed when there is a relationship between source and target.\n\n[1] Y. Wada et.al., Robust Label Prediction via Label Propagation and Geodesic k-Nearest Neighbor in Online Semi-Supervised Learning. IEICE Transactions on Information and Systems, Vol.E102-D, No.8, August 2019.\n",
            "summary_of_the_review": "While the contribution on the computability argument is acknowledged, the formulation of transfer learning itself is questionable. Although there is a possibility that it is correct, it is difficult to judge with the information given. Including this, the overall structure of the paper should be improved, and it is not within the scope of a minor revision. Therefore, I recommend that you review the whole thing and revise it before resubmitting.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}