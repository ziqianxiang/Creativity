{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper introduces a synthetic multi modal dialogue state tracking task where the task of the system is to talk to a user and answer questions describing a scene over multiple turns.  The introduce the task of multimodal dialogue state tracking, the synthetic dataset with dialogues and a transformer model which combines video and dialogue features to do answer user questions. \n\nThey compare their network on their task and show that their method can keep track of the objects mentioned in the conversation better than baselines.  ",
            "main_review": "The paper starts off with an ambitious goal of defining a new multi-modal task, a benchmark and a novel solution for it. But perhaps since it tries to do so much, it doesn't do any one of them particularly well. In the end you come out with a few vague ideas from the paper but nothing rigorous vs the alternatives. \n\nStrengths:\nSince this paper defines a new task it has multiple novel ideas. The idea of doing multimodal dialogue state tracking, a dataset as a benchmark and a novel video + dialogue transformer model were new and interesting to me. \n\nWeaknesses:\nListing weaknesses by each of the contribution:\n1) Multi-modal dialogue state tracking task definition: The authors only briefly define this in section 2. Considering that this would be a new class of tasks, I would like to see much more discussion on why the task was defined the way it was. Single start - end times with the objects themselves as slots. Perhaps the definition the authors describe was well suited for the synthetic task they settled on but there is no indication that such a representation is useful for a real world multi modal dialogue mentioned in Figure 6. \nEither this should have been clearly motivated by a real world task or there should be a detailed discussion of various ways in which the dialogue state could have been represented and why the one they chose is the best one. Simply proposing a paradigm without any pros and cons is not scientific research\n\n2) Synthetic benchmark: Again here, for a new task it would be useful to know why the benchmark chosen (DVD) is the apt one. Looking at the dialogues generated in Table 12 and 13, its clear that the utterances are neither natural nor human comprehensible. Such a benchmark could still be useful but needs validation to show that it is indeed useful. Here also I would have liked to see more variation on the synthetic data generator instead of simply stating the default chosen. What happens if we trained on 130k dialogues instead?\nAlso stating that the released synthetic benchmark data-set has 13k dialogues which is larger than related DST benchmarks is wrong (appendix B) as its comparing apples to oranges\n\n3) Video Dialogue transformer: The paper focuses a bunch of time showing that their Neural network indeed performs better than others on this task and contains the correct  ablation studies and relevant baselines but given the task is essentially visual reasoning, showing the performance on the prior benchmarks would be useful. \n\nOther comments: \nThe paper only reports state accuracy but knowing the system answer accuracy would be interesting as well. ",
            "summary_of_the_review": "As stated due to the above concerns, the paper tries to do too many things and none of them well. It would be useful to break this into at least 2 more focused papers and show reasoning behind some of the chosen decisions. \n\n\n============= UPDATE ====================\nThank you for your responses. I think they take a step in the right direction. This comes back to my point that the contributions should be more thoroughly justified. \nMy initial concerns regarding the formulation of the task and the choice of benchmarks still stand. In light of these responses, I'll revise my score to 5.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper tackles the challenging task of multimodal dialogue state tracking. They synthesize a new dataset (DVD-DST) for the task based on previously proposed approaches. They propose VDTN, a multimodal model that jointly encodes video segments and text. It has two decoders: one to produce a new dialogue state in text format, and a visual decoder to reconstruct original video segments for self-supervised training. They test VDTN (and some baseline models) on DVD-DST and show its superiority. They provide several ablation studies to gain insight on what each component is contributing to the accuracy. ",
            "main_review": "Strengths:\n- Overall I think it's a well-done paper. The task is relatively unexplored and it's nice to see new contributions. \n- The paper is well-written, tasks definitions are clear, and experiments seem reasonable.\n\nWeaknesses:\n- Experiments are on synthesized datasets. It's a well-known fact that testing on synthetic datasets result in overestimation of real performance and don't translate to real-world usability. Despite that, the state accuracy is only ~28%. I understand VDTN is supposed to be a baseline for future improvement, but how such low results on synthetic data are justified? I couldn't find any discussion on that in the paper.\n- Another concern is the baseline models. None of those models are meant to be used for MM-DST. As authors have found, simply concatenating visual and textual embeddings don't perform well. Is it possible to compare to some of the multimodal models proposed over the years (e.g. [VilBERT](https://arxiv.org/abs/1908.02265), [SimVLM](https://arxiv.org/abs/2108.10904), [VLP](https://arxiv.org/abs/1909.11059), etc.)\n- My last concern is the usefulness of the dataset. I find QA over videos an interesting concept with potentially a lot of use cases (finding a segment in a youtube video you're interested in) but the videos in this dataset are limited to only objects moving around. From current work, it's not clear how you can scale and what the limitations are. If I want to tackle a new domain, say videos of humans, what are the challenges besides defining new slots? I think a discussion on that in the paper would be interesting.  ",
            "summary_of_the_review": "Overall I think this is a good paper taking a step in the right direction. \nI have a few concerns regarding the evaluation and the dataset mentioned above. As of now, I'm leaning towards a weak reject, but I'm willing to improve my score if those concerns are addressed.  \n\n=========== UPDATE ===========\nThanks for your responses. My concerns regarding testing on synthetic datasets (instead of real crowdsourced data) and not comparing with previous multimodal models still remain. Thus I will keep my score.\nAlthough I agree the task is hard, theoretically it should be possible to generate enough training data to cover all the paths objects can take. By evaluating on the same simulated data, you can possibly achieve close to 100% accuracy, while the current accuracy is far from that which makes me think either the data generation approach is not exhaustive enough, or the modeling is insufficient. With the current information in the paper, it's hard to tell.\nModels such as UniVL (https://arxiv.org/abs/2002.06353) have Video and Language Pre-Training. I think a comparison with a truly multimodal model will shed more light on what components of the new models is benefiting the task.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduced a multimodal dialogue state tracking task that can track visual objects and their attributes mentioned in dialogues and provided a synthetic benchmark for it. It also designed a neural network model for object-level representation learning and multimodal interaction and achieved large performance gains compared to conventional DST models.",
            "main_review": "--------------Update--------------------\n* There exists another paper \"SIMMC 2.0: A Task-oriented Dialog Dataset for Immersive Multimodal Conversations\" https://arxiv.org/abs/2104.08667, which is a multi-modal task-oriented dialogue dataset and has rich annotations. This paper is first on arxiv in 8 Apr 2021 and has a Multi-modal DST task. Given that, the current paper's novelty and contribution are significantly weakened. \n----------------------------------\nStrengths\n* This paper introduced a multi-modal DST task combining video with dialogue text data and also provides a synthetic multimodal DST benchmark.\n\nWeakness\n* The motivation to introduce multi-modal DST is unclear to me. The purpose of the traditional DST components in task-oriented dialogue systems is to track users' goals, e.g. restaurant-name, -price, -people etc, which are used to query knowledge base (database), and the query results are used for response generation. However, I don't realize the real-world value to track visual objects in task-oriented systems. The authors may consider clarifying their motivations. \n* The proposed neural network is not quite novel. For example, using a decoder to recover visual representations in a self-supervised learning task to improve video representation learning is not a novel objective function. There exist lots of similar techniques. Wu et al [1] use dialogue history and dialogue state to recover user utterance to improve representation learning. \n\n[1] Wu et al. Improving Limited Labeled Dialogue State Tracking with Self-Supervision. Findings of EMNLP 2020\n",
            "summary_of_the_review": "\nAlthough this paper provides a synthetic multi-modal DST benchmark and a neural network to deal with this task, this task itself is not clearly motivated and is far from real-world scenarios, and the proposed neural network is also not quite novel. I think this paper's contribution to the dialogue research community is small and inclined to reject it.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper solves the problem of dialog state tracking in the setting of multimodality. It developed DVD-DST, a synthetic video-grounded dialogue benchmark with annotations of multimodal dialogue states, designed a transformer-based model VDTN for the task, and demonstrated the performance in a response prediction task using the decoded states. \n",
            "main_review": "Strengths: \n1. Multimodal Dialog State Tracking is an important problem to solve with increasing numbers of papers on multimodality in the research community.\n2. The VDTN model structure seems reasonable by encoding the information of both visual knowledge and dialog history knowledge using one transformer network.\n3. Both the experiments comparing baseline systems on the DVD-DST dataset and the ablation study seem good to me. \n\nWeakness: \n1. The definition of MM-DST is relatively straightforward for researchers working on DST. I do not consider it a significant contribution.\n2. There are several works associated with Multi-modal dialogs (e.g.,  https://arxiv.org/pdf/2007.09903.pdf,  https://arxiv.org/pdf/1911.07928.pdf). I would suggest the authors adding more content elaborating on the differences.\n3. Since the benchmark is synthetic based on the generation procedure of DVD. I would suggest the authors elaborating on the quality of the datasets. Otherwise, it is not straightforward to check the performance of the VDTN model.\n4. The baselines are not necessarily the best DSTs as of now (check  https://github.com/budzianowski/multiwoz Dialog State Tracking). Adding comparisons with more SOTA DST models should help illustrate the model performance, especially that the current benchmark is new and synthetic. \n\n\n============= UPDATE ==================== \nThanks for the authors' response. I will keep my original assessment. ",
            "summary_of_the_review": "Overall, the quality of this paper is mixed. On the one hand, this paper is well-structured, aiming to push the frontier of multimodal dialog state tracking given that most current dialog state tracking tasks are still primarily text-based while multimodal dialog system is clearly a future topic. On the other hand, there are multiple weaknesses, as mentioned above.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "There is no human subject involved and hence, no ethical concerns.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}