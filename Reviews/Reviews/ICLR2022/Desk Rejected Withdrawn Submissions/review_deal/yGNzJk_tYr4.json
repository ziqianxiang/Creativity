{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper introduces causal sensitivity (CENT), a new metric to estimate sensitivity of neural NLP models to spurious correlations. The key idea is to first assign random labels to the documents and then perturb some of these documents (randomly sampled to mimic an RCT) so as to inject certain spurious patterns that correlate well with the assigned pseudo-label. Models are then trained on this new dataset where documents with one pseudo-label also contain a spurious pattern that is now predictive of this pseudo-label. On the test side, test set documents are assigned random labels. Similar to the training perturbations, the predefined spurious pattern is injected into all examples  that have a particular pseudo-label in the test set to form a new test set. Difference in model accuracy on the two test sets (perturbed and unperturbed) provides a measure of how sensitive a particular NLP model is to a spurious pattern. The authors use this to now augment their training sets with examples that include a specific spurious pattern and show that augmenting data in this way leads to models that are more robust out of domain when said associations appear in the test sets.",
            "main_review": "- The paper is well motivated and looks at an important problem.\n- I think this is an interesting problem formulation but the problem formulation is at odds with the motivation. The experimental design appears to be begging the question rather than setting out to verify a hypothesis. I am not sure I see a clear contribution from this work. The key idea is that we are injecting spurious patterns into the training set documents corresponding to one label and showing that training on this data produces models sensitive to said patterns (which is expected in supervised learning) and lower performance when these patterns don't exist in the test set. The authors propose using this difference in test accuracy as a metric to quantify a model's reliance on spurious correlations. But what does this difference tell us? Certainly a model trained to predict pseudo-labels will have different model parameters post optimization than what we want to study---a model trained to predict actual class labels. We can't use findings from the former and say that the same hold for the latter. And if instead of specific models, we are hoping to make generalized findings about classes of architectures, I don't think this setup is suited.\n- The second stated contribution is the analysis that shows the effectiveness of data augmentation to improve model's ability to generalize better in the presence/absence of certain spurious patterns. But don't we already know that? I'd appreciate some clarity here if I'm missing something. Also, it doesn't appear that you have controlled for the dataset size to disentangle the performance improvements gained by data augmentation generally (from the source distribution) versus data augmentation in the way you've described.\n- The experiments are limited to one data domain. I would've liked to see this be replicated across several datasets.\n\n-----------------------------------------------------------------------------------------\nIn addition to the questions I've asked above, I'd appreciate some thoughts on these:\n- One of the claims in the paper is that data augmentation is only more effective at improving robustness against spurious features that a model is more sensitive to. How does this stack up against the findings in [1,2]?\n- Why do you expect that \"sensitivity of the model should be independent of injection probability\"?\n- I don't think you can claim that RoBERTa's better performance compared to BERT indicates a larger pretraining corpus improves downstream robustness. While I do believe that larger pretraining corpus improves downstream robustness in general, RoBERTa and BERT don't just differ in their pretraining corpus but BERT also uses a next sentence prediction task in its pretraining that is not used in case of RoBERTa. Is there additional data in your paper that supports this assertion?\n- I'm not sure I get what you mean by \"priority matters when dealing with spurious correlations\" (Section 4.1)?\n- How does analyzing the types of features a model might be sensitive to enable \"fair comparison between different models and features\"?\n\n1. Kaushik, D., Hovy, E., & Lipton, Z. C. (2019). Learning the difference that makes a difference with counterfactually-augmented data. ICLR 2020\n\n2. Kaushik, D., Setlur, A., Hovy, E., & Lipton, Z. C. (2020). Explaining the efficacy of counterfactually augmented data. ICLR 2021\n-----------------------------------------------------------------------------------------\nPresentation suggestions:\n- Introduction: \"For example, BERT only achieves an accuracy less than 10% on a challenge test set HANS\". Give some context of why 10% is bad on this task to a non-NLP reader who may not have prior knowledge about NLI or how HANS was created. One sentence should be enough.\n- Introduction Page 2: \"typical nerual NLP models\" -> \"typical neural NLP models\"\n- Page 9: \"Approaches that go beyond simple data augmentation is required to combat such spurious features.\" -> \"Approaches that go beyond simple data augmentation are required to combat such spurious features.\"\n- Overall: Footnotes should appear after the punctuation (comma, period, etc.). The only exception to this rule is a footnote appearing before em dash.",
            "summary_of_the_review": "I think the paper is well motivated but the problem setup is not in line with the motivation. I don't see a clear contribution from this work and would appreciate a response from the authors on the questions I have posed above. If I'm missing something, I'll be happy to update my score post author response.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work aims at defining and quantifying NLP models’ sensitivity to certain text perturbations (phrased as spurious features throughout the work). Specifically, the authors assign random labels to some text data, perturb the data under a specific label with some probability as a treatment, and check whether the NLP model can reconstruct the random labels based on the perturbations. A successful reconstruction shows a high sensitivity of the model to the perturbations. The authors further explore the correlation between this quantified sensitivity to perturbations and a defined robustness of the model. They find a significant inverse correlation -- a model that’s sensitive to certain perturbations would likely perform poorly on a test set with these perturbations injected.",
            "main_review": "The writing of the paper is overall clear, and the authors explore a good number of text perturbations. However, I think the concerns that some readers might raise and complain include:\n\n(1) The authors frame the sensitivity experiment as a way to quantify the model’s sensitivity to spurious features. I don’t think the experiment is related to the common impression of spurious features, as the labels are fake (random) and the text perturbations (so-called spurious features) are the only thing directly correlated with the fake label. So these “spurious features” are actually the core/true features in this experiment. This is misleading and I think framing the setup as sensitivity to certain perturbations can be better.\n\n(2) I think some aspects of the defined sensitivity are missing and need further elaboration. For example, should sensitivity also depend on the length of training? I would imagine even the same model with different length of training would have different levels of sensitivity to the perturbations.\n\n(3) For Hypothesis 2, I don’t think it’s enough to measure the performance gain on the perturbed test data. How’s the performance on the original unperturbed test data?\n\n(4) Throughout the work, the text perturbations used are automatic, and the robustness evaluations are also based on these automatic perturbations. They are not natural (e.g., shuffling words) and could have two consequent concerns: (a) the sensitivity-robustness findings may not hold in real applications, and (b) the automatic perturbations may break the true features for the original task in the text.\n\n(5) Overall, the takeaway and impact of the work is rather unclear to me. Do we know whether the analyses will be feasible for real spurious features (e.g., lexical overlap or negations in natural language inference, or yet unknown spurious patterns in other tasks)? Why/how could this work facilitate the construction of a more robust and generalizable model?\n",
            "summary_of_the_review": "Overall I think this work might need more justifications to its experiments (as mentioned in (1), (2), (3) above). The takeaway, application, and novelty of the work are also not very clear to me.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "\n# Summary \n\nThe paper measures the sensitivity of NLP models to spurious features. There are two main measurements. In the first kind of experiment, all examples are assigned random labels. Some training examples are augmented with a synthetic feature, at probability p. The synthetic feature is correlated with the pseudo label. Models are trained to predict the random label. At test time, models are evaluated on test sets with and without the synthetic feature, and the difference in accuracy is defined as sensitivity to the feature. In the other kind of experiment, models are trained with real labels, and evaluated on test sets with and without the spurious feature. \n\nThe experiments show that models are sensitive to the spurious features, to different extents. Most importantly, the sensitivity to the spurious features is inversely correlated to robustness to the features. This may be expected, but is good to see in a careful setup. Sensitivity is also positively correlated to how much a model benefits from augmentation with that feature during training. This is more surprising, and is a useful empirical finding. \n\n",
            "main_review": "# Evaluation\n\nThe paper is well-written and targets important questions in NLP these days. The findings are mostly expected, but some are more surprising than others. This isn't necessarily a shortcoming, since it is good to see these findings in a careful setup. The formal framework is grounded in concepts from the causality literature, although I did not find this especially useful or important for the methodology and analysis. It may be a matter of personal taste though. \n\nMore importantly, the work suffers from a major limitation, as many studies in causal inference -- the use of simulated data. The benefits are obvious: once can clearly define a randomized control trial, without confounding. However, the shortcomings are also significant, since the work is more detached from how these models and datasets are actually used. There are other approaches in causal inference for deconfounding besides simulation, such as working with real data but stratifying it. I wonder if alternative or additional setups should be considered, to bring the work closer to more naturalistic settings. In this context, the robustness experiments are relevant, but (a) their relevance can be made clearer earlier; (b) even with them, the sensitivity experiments are still detached. \n\nThat being said, the work does provide a clean setup to answer important questions. I'd be willing to change my evaluation depending on how the issues are dealt with. \n\n\n# Comments and questions\n\n1. Details on the spurious features/perturbations are provided in appendix A, but these seem important enough to me to discuss in the main text in some detail.\n\n2. It makes sense that a model becomes more sensitive to changes in the spurious correlation when it has seen that spurious feature more frequently at training time. So, I don't exactly understand the expectation that sensitivity of the model should be independent of injection probability.\n\n3. There are several issues stemming from the choice to use pseudo-labels. I think some of them are accounted for by the robustness experiments, so when reading, some forward-referencing could be helpful. Still, some of these issues may still be problematic even with the robustness experiments. \n\na. Working with pseudo labels also mean that we have to be careful about making statements about what NLP models end up doing in a more naturalistic case. For instance, claiming that the results show \"that neural NLP models succumb to these spurious features eventually\" is problematic. The sensitivity experiment shows that NLP models are able to identify these spurious features eventually. But \"succumb\" implies that these models would use them in an actual task, which isn't shown here. The robustness experiments complement this issue, showing that when the feature exists the model performs worse on the original labeling task.\n\nb. When using pseudo-labels, and without the spurious feature, there is no way to solve the task above a random 50% accuracy baseline, right? So, the sensitivity measure, which is the difference in accuracies, may be less meaningful, since it is hiding the actual performance. \n\n4. The model-wise comparisons are quite interesting, and well-put in perspective of other work. \n\n5. A few relevant studies that may be worth discussion:\n1. The idea of injecting synthetic features is pretty common for studying robustness to such features in and out of distribution. For example, see the synthetic bias experiments in [1,2,3].  In those experiments, however, the label is true and not pseudo. It may be worth discussing this. \n2. Measuring sensitivity to perturbations is of course quite common. In addition to studies mentioned thus far, it would be good to distinguish the present work from work applying other kinds of perturbations, such as [4].\n\n\n\n[1] He et al., Unlearn Dataset Bias in Natural Language Inference by Fitting the Residual\n[2] Clark et al., Don’t take the easy way out: Ensemble based methods for avoiding known dataset biase\n[3] Sanh et al., Learning from others' mistakes: Avoiding dataset biases without modeling them\n[4] Abdou et al., The Sensitivity of Language Models and Humans to Winograd Schema Perturbations\n",
            "summary_of_the_review": "The paper presents a clearly designed set of experiments to assess sensitivity of models to spurious features. It provides an interesting analysis of the relationship between sensitivity and robustness, although it suffers from a limitation of using simulated data in some cases. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper studies the sensitivity of NLP models to spurious features using tools from causality literature. The authors propose a simple experiment: take a text classification dataset, replace its labels with random labels, and add special features ($f$) that appear in some fraction of the instances with one of the labels (l'). They then compare the performance of several models trained on these training sets on a test set where all instances with label l' contain $f$. They interpret the results as the sensitivity of these models to spurious correlations. They then perform another set of experiments with different combinations of perturbations in the training and/or test sets, showing an interesting correlation between some of the results of these experiments.\n",
            "main_review": "The paper asks a very interesting question: how much are leading NLP models effected by spurious correlations. The authors make a potentially interesting comparison between different features such as duplicate punctuation marks, showing that some are learned better than other. The analysis at the end is also potentially interesting, though I am not sure I follow the intuition behind hypotheses 1 and 2.\n\nMy main concern with this paper is that the experiments do not necessarily address the research questions. The authors define _spurious correlation_ to be _prediction rules that work for the majority examples but do not hold in general_. They contrast them with _target features_, which are never formally defined, and I assume relate to features that always hold? This distinction itself is somewhat questionable, as Gardner et al. (2021) showed that all single-word features can be thought of as spurious. I realize that the justification of these terms might be beyond the scope of this paper, but when you consider the experimental setup, I am not sure why the proposed perturbations are a good model for spurious correlations. First, unlike the definition in the paper, they *always* hold at test time, and never appear with a different label. Second, it is unclear why is the fact that models are able to use these features necessarily a bad thing. I would expect any reasonable learner that sees some feature with only a single label, to learn that this feature is correlated with this label. This is particularly true when no other meaningful features are found, as in the first experiment. Third, it is not clear what are the target features that these supposedly spurious features are compared against. By design, the first experiment does not contain any feature that is correlated with some label not by chance, so there cannot be any _target_ features. In some of the cases in the second set of experiments, the real labels are used, so other features exist. However, the authors do not try to separate between the spurious features that exist in them, and those target features that they mention (again, Gardner et al. would argue that the second group might be empty). To conclude, while the experiments are potentially of interest, the current framing of the paper is not supported by these experiments. I would recommend reconsidering this framing and proposing alternative research questions that these experiments address.\n\n",
            "summary_of_the_review": "The experiments are interesting, discussing to what extent models can rely on single word features that only appear with one label under different conditions. However, the connection between them and the research questions is questionable.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}