{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors propose a mechanism through which a cortical microcircuit motif may approximate backpropagation, while maintaining biological plausibility. By introducing a dual-compartment approach of pyramidal cells, along with a tightly coupled population of inhibitory neurons, an explicit error signal can be tracked and minimized by an STDP-type kernel. The authors show that when following this learning rule a simple network will minimize the error signal, and thus become \"self-predicting\". ",
            "main_review": "Major Comments:\n- In section 2.2 the authors introduce synaptic currents in the form of conductance based synapses. However, in equation 3, they then state that the [E_syn - y(t)] term is treated as a constant.\n\t- E_syn is not stated anywhere in the paper, so it's unclear if dropping this valid. \n\t- If this term is being replaced with a constant, then these more directly reflect current based synapses (with the exception of B_t(t) for E-type synapses), and this needs to be clearly stated. \n- Evaluating equation 9 for layer N-1, $\\tilde{a}^N$ incorporates $a_i^{target}$. Thus, if the target is not a delta function, this seems to indicate that a real valued signal can be passed between the units simulated in this work. If this is true, it's a major issue with the biological plausibility of the network. \n- The proposed circuit has excitatory connections from a given layer's pyramidal cells to the paired somatostatin neurons. However, the pyramidal to somatostatin connections alluded to in the article typically emerge from higher (rather than local) excitatory neurons. How might the network function if this hierarchical offset were followed? \n\nMinor comments:\n- A sub-sub section break may be warranted after the paragraph containing equation 8, as this is a shift from the E-type synapses to the BP approximation\n- It's unclear how the tasks in section 5.2 are being encoded to function with the spiking neural network architecture. How are the continuous-valued inputs being presented?\n- The calculation of error signals as the difference between proximal and distal inputs is similar to other recent work (eg: Payeur, ..., Naud Nature Neuro 2021. Sacramento, ... Senn, NeurIPS 2018). Comparisons to this recent work should be included.\n",
            "summary_of_the_review": "The comparison of distal versus proximal inputs, together with a tightly coupled inhibitory population, are a common and well founded motif in attempts to reconcile biological networks with backpropogation. Aside from the comments above, there don't appear to be any technical issues with the article. However, given the similarity to previous approaches, the article may not be novel enough to appeal to the broad interests of ICLR. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a bio-plausible learning framework for training Spiking Neural Network (SNN) using backprop-based rules. The multi-layered SNN consists of pyramidal cells and somatostatin-expressing (SOM) cells modeled by the leaky integrate-and-fire (LIF) neuron. The weights in the network are either trained by local spike-timing-dependent plasticity (STDP) learning rules or in synchrony with other weights. The authors performed experiments on MNIST and CIFAR 10 datasets and showed competitive accuracy compared with other SNN training methods.",
            "main_review": "Pros\n\n1. The proposed method can effectively train deep convolutional network architectures and obtain competitive accuracy compared to other SNN training methods on MNIST and CIFAR 10 datasets. In addition, the spike train approximator experiment shows the potential of the proposed model for temporal and sequential tasks.\n\n2. The authors introduce SOM neurons as the top-down feedback interneurons of their network. Such an architecture with feedback interneurons has been previously proposed without specifying the type of interneurons used [Sacramento, João, et al. 2018]. This detail is appreciated in a biological context since somatostatin-positive (SOM) interneurons are known to project connections to the dendritic areas of pyramidal neurons, while parvalbumin-positive (PV) interneurons are known to target perisomatic areas [Royer, Sébastien, et al. 2012]. This contribution should be clarified in this biological context because it is not otherwise crucial in the deep learning context of the paper.\n\n\nCons\n\n1. The authors need to position their work correctly with respect to existing works in the field. Many studies have attempted to reconcile backprop with biological plausibility and spiking neurons in recent years. For example, [Lillicrap, Timothy P., et al. 2016] showed that random feedback weights in a network could support error backprop. [Guerguiev, Jordan, et al. 2017] and [Neftci, Emre O., et al. 2017] used multi-compartment spiking neurons similar to this paper that receive the feedforward and feedback information in segregated compartments. In addition, [Bellec, Guillaume, et al. 2020] used eligibility traces that reflect intermediate neuronal states and extended the versatility of bio-plausible BP-based learning to a broad spectrum of temporal applications. Recently, bio-plausible BP-based learning has also been introduced for neuromorphic processors ([Shrestha, Amar, et al. 2021], [Renner, Alpha, et al. 2021]) that showed the application advantage of such approaches. However, none of these works have been discussed in the paper.\n\n2. Without reviewing and discussing related works in the field, the authors also missed the chance to show what specific problem they solved for bio-plausible backprop. Several important problems limit backprop from a bio-plausible learning method for SNN, including the weight-transport problem, the requirement of separate feedforward and feedback pathways, and the need for future information in the temporal domain. The proposed method in the paper failed to solve the above problems better than other existing works. For example, the weight-transport problem is ignored by directly copying the feedforward weights to the feedback connections. A feedback pathway is still needed to backprop errors layer by layer.\n\n3. There is no functional role for the SOM neurons in the proposed architecture. Even if we remove the SOM neurons, the network can still learn by directly propagating the error signal e to the pre-synaptic layer without adding the post-synaptic outputs. Thus, the authors need to justify the reason behind such an architectural design.\n\n4. The authors recognize that the one-to-one correspondence between their pyramidal neurons and their SOM interneurons is somewhat limiting for the connectivity of their network architecture. However, they justify their design decision with a reference suggesting that SOM interneurons receive top-down feedback encoding the difference between targets and predictions [Leinweber, Marcus, et al. 2017]. There are two problems here: i) First, the paper identifies the PV and not the SOM interneurons as the main targets of this top-down feedback (see the first paragraph of their Results section). ii) Even if the SOM interneurons are also targets for that top-down feedback signal, there is no justification for the one-to-one connectivity, which is the argument that the authors themselves were trying to support. This kind of one-to-one neuron correspondence is not uncommon in similar approaches but should be ideally removed in real biologically plausible applications.\n\n5. The authors clearly state that the neurons of the proposed framework disregard Dale's law, i.e., they project outputs of mixed signs to their post-synaptic targets. Although this is a common approach in several previous SNN approaches, it is in contrast to the biological plausibility that this work claims.\n\n6. The structure and writing of the paper need considerable improvements to reach the standards of the conference. The authors need to carefully motivate their work in the introduction and add a related works section to appropriately place the work with respect to the existing works in the field. The authors need to give more intuitions on the design choices in the method sections instead of only providing equations and abundant biological definitions. The method section should be clear enough so that the readers can reproduce the method using just the write-up.\n\nSacramento, João, et al. \"Dendritic cortical microcircuits approximate the backpropagation algorithm.\" 2018.\n\nRoyer, Sébastien, et al. \"Control of timing, rate and bursts of hippocampal place cells by dendritic and somatic inhibition.\" 2012.\n\nLillicrap, Timothy P., et al. \"Random synaptic feedback weights support error backpropagation for deep learning.\" 2016.\n\nGuerguiev, Jordan, et al. \"Towards deep learning with segregated dendrites.\" 2017.\n\nNeftci, Emre O., et al. \"Event-driven random back-propagation: Enabling neuromorphic deep learning machines.\" 2017.\n\nBellec, Guillaume, et al. \"A solution to the learning dilemma for recurrent networks of spiking neurons.\" 2020.\n\nShrestha, Amar, et al. \"In-Hardware Learning of Multilayer Spiking Neural Networks on a Neuromorphic Processor.\" 2021.\n\nRenner, Alpha, et al. \"The Backpropagation Algorithm Implemented on Spiking Neuromorphic Hardware.\" 2021.\n\nLeinweber, Marcus, et al. \"A sensorimotor circuit in mouse cortex for visual flow predictions.\" 2017.\n",
            "summary_of_the_review": "Overall, the reviewer recommends not accepting the paper. Although the authors demonstrate good performance for the proposed method, the overall quality of the paper does not reach the standards of the conference. First, the authors need to position their work appropriately with respect to existing works. Without a thorough review of other related works, the paper does not identify the specific problems it solved for bio-plausible backprop. Second, the biological plausibility of the proposed SNN is questionable, and the motivation behind the bio-inspired architecture design is not clear. Third, the structure and writing of the paper need considerable improvements to make it more readable. More detailed problems of the paper are listed in the cons in the main review.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors proposed a bio-plausible learning framework for training SNNs, which has two components, namely pyramidal-SOM-based architecture and dedicated learning rules. The key to the proposed architecture is the auxiliary SOM cells, which mimic the behavior of the same layer’s pyramidal cells one-on-one. The proposed learning rules are based on STDP self-supervised learning rule. Unlike STDP, the proposed method trains an SNN in a supervised learning manner, propagating error signals through local feedback connections constructed with pyramidal and SOM cells. The authors also discussed the equivalence between the proposed learning rules and the BP-based rules. The proposed learning framework is validated with MNIST and CIFAR10 datasets.",
            "main_review": "**Strengths**:\n\nLeveraging more bio-plausible concepts for the research of SNNs is meaningful to me. Effective training schemes for SNNs are highly desired to make SNNs thrive in the AI and machine learning society. I appreciate the efforts made by the authors. \n\nThe manuscript provides thorough mathematical derivations to support the proposed learning rules. Besides, it also provided necessary biology/neuroscience evidence to support their design.\n\n**Weaknesses**:\n1. The title and abstract of the manuscript are different from the ones on the Openreview. I have to read through to confirm that I have reviewed the correct manuscript.\n2. The experiments for validation are very limited. It is hard to understand how does each part of the proposed learning rules impact the training quality. \n     * Above Eq.(4), the authors mentioned: “simplified the double exponential function to single exponential decaying function.” Why? How does the original double exponential function work with the proposed learning rules?\n     * The authors should explain what do C, P, and the numbers mean under table 1.\n     * There are so many potential ablation studies that can be done to reveal the importance of some key components of proposed learning rules. However, the authors did not show that. Are there any specific reasons I overlooked?\n3. The mechanism of the proposed SOM cell is unclear to me. Why does SOM cell contribute negatively? The pyramidal cell can propagate in a  top-down manner by itself, and SOM cells mimic the behavior of the same layer’s pyramidal cells one-on-one. I think the authors should give more intuitive ideas about why SOM cells are needed, especially since they impose special constraints on the SNN’s connectivity. \n4. Do the proposed learning rules need to be customized for different SNN neuron models? For example, can a potential researcher directly deploy the learning rules with the IF or SRM models?\n5. What’s the limitation of the learning rules? \n     * Does it suffer from over-excited neurons or “dead” neurons? \n     * How does the spiking threshold impact learning?\n     * How easy to make the training converge? \n",
            "summary_of_the_review": "I think the research direction and motivation of the manuscript are meaningful, and the SNN community would appreciate it.  However, the experiments section is weak to me. The authors should provide more experimental evidence to show the effectiveness of the proposed architecture and learning rules. Also, the authors did not discuss the limitation of the proposed approach. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a bio plausible learning method applied to spiking networks. The results are original and potentially interesting, but the presentation is globally disorderly and hinders the global understanding of the scientific contribution of this paper.\n",
            "main_review": "\nFirst of all, at the scientific level, the authors present an architecture model in the form of a microcircuit whose complexity does not allow its total description in the paper. This bottom-up approach is explained by detailing the neural models and a time-dependent learning STDP rule. These equations are used from the existing computational neuroscience literature and the interesting and original part of this paper comes at the moment of relating the dynamics of the networks of this microcircuit with the gradient back propagation algorithm. It seems on the one hand that this kind of work has already been discussed (e.g. Optimal spike-timing-dependent plasticity for precise action potential firing in supervised learning JP Pfister, T Toyoizumi, D Barber, W Gerstner), but also that instead of occupying less than a page, this result deserves to take a more important if not a more central place in the result. \n\nThe experimental results seem to confirm the modeling choices made in the paper, but it seems to me that the comparison with existing works is not detailed enough to be convincing. Minor point: it is usual to indicate in bold the best result and not the one proposed in the paper.\n",
            "summary_of_the_review": "Finally I would like to point out that the paper as a whole has too high objectives for the material that is described. The introduction does not do less than gather Spiking NNs, backpropagation of the gradient and Karl Friston's theories of free energy minimization. However this is done with vague terms and high generality. For example, I do not see the point of stating that \"our brain is the only system that is truly intelligent\". This is an unverifiable fact and most neuroscientists would agree (which is rare enough to mention) that it is false. There are also many inaccuracies in the description of neuro-biological data, e.g. associating the STDP learning rule with necessarily an LTP or LTD. On the other hand, there are many typographical and syntactic errors that hinder the reading and presentation of this paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}