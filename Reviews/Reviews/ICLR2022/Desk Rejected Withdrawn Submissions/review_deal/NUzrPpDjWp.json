{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies the problem of scalable adversarial attack for graph neural networks. Due to the huge search space  of graph adversarial attack, usually quadratic complexity w.r.t the number of nodes, it is desired to design an efficient attack method that can scale up to large graphs. In this work, it proposes to perform adversarial perturbation on the small graph obtained by graph coarsening. Experimental results have demonstrated the effectiveness of the proposed method.\n",
            "main_review": "The studied problem of scalable graph adversarial attack is important in practice and it is interesting to see how the authors tackle the problem by reducing the huge search space through graph coarsening. However, there are some concerns that should be addressed:\n\n1. Although the strong empirical performance has demonstrated the effectiveness of the proposed method, it will make this work more solid to add some justification and theoretical understanding on attacking with graph coarsening. Essentially, why attacking the coarsened graph can degrade the performance of GNNs trained on the original graph? Maybe the authors can borrows the recent paper [1] which shows graph coarsening serves as regularization in training GNNs.\n2. The current coarsening method uses kmeans clustering to form super nodes while there are other coarsening methods preserving various properties of the original graph, e.g., spectral properties [2]. It is of interest to discuss other choices on coarsening methods.\n3. Since this work focuses on scalable attack, it is suggested to include more results on larger datasets, e.g. Physics and Ogbn-arxiv, in more tables instead of just one table. \n\n[1] Scaling Up Graph Neural Networks Via Graph Coarsening. KDD 2021.\n[2] Graph reduction with spectral and cut guarantees. JMLR 2016.",
            "summary_of_the_review": "The idea of using graph coarsening in the attacking process is interesting but there are still some concerns regarding the analysis and experiments. The author are suggested to address these concerns to make this work more convincing.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new scalable attacking algorithm for GNNs, using the graph coarsening techniques. The attack is applied to an initially coarsened graph and is iteratively refined specific edges are identified to be added or removed. The key advantage is the scalability of the algorithm.",
            "main_review": "Strengths: the idea seems effective in practice. On small graphs, the proposed method is on par or better than SOTA methods. On large graphs, both efficacy and scalability are well demonstrated. \n\nWeakness: the idea does seem a bit straightforward. The proposed algorithm lacks theoretical guarantees. I would have been much more excited if the algorithm can be more closely coupled with the loss optimization, so that certain bound on sub-optimality can be provided, even if under strong assumptions. \n\nI have some technical concerns regarding presentation, complexity analysis and empirical evaluation. They are listed below. I am willing to increase my score once they are resolved.\n\n1, why only use the first layer representation for kmeans? For the second/third iterations, is the clustering still based on the initial surrogate model's representation, or is any intermediate surrogate model (on the coarsened graph) used?\n\n2, For the main algorithm, some technical part is unclear. Is the whole clustering-rebuilding-selecting-splitting through $l$ iterations only to add/remove one edge? So if the budget is $m$ edges, do we need to run the algorithm for $m$ times?\n\n3, For the deletion of an edge between clusters, does it mean all edges between nodes from the two clusters are removed? If so, what if this is beyond the given budget?\n\n4, There is a requirement when adding edge -- weights need to be $<|\\bar{v}_i||\\bar{v}_j|$ -- what is the reason for this? And what if the weight is bigger than the bound?\n\n5, $h$ is the size of hidden layer -- did you mean the number of layers?\n\nComplexity analysis:\n6, The most important issue is that meta-gradient computation does not seem to be counted in the analysis. I assume the gradient computation needs to be done at every iteration, before the selecting step. This can be expensive as the intermediate graph is very likely dense. In the worst case, the intermediate graph can have $O(Kl)$ nodes and $O(K^2l^2)$ edges. \n\n7, I can accept that $h$ and $d$ be assumed constant, but $l$ is indeed $\\log_k{N}$ and should not be dropped anywhere in the analysis. \n\n8, kmeans has number of iterations, and the whole algorithm iterates for $l$ times. These should be included in the complexity. \n\nI do not think these issues are fatal. But they need to be cleared. \n\n9, I think the following random attack should be compared with (note it is different from DICE): since a surrogate model is used to predict pseudo-labels on test data, we can randomly add/remove edges near the test nodes with low confidence, based on the surrogate model. The rationale is that a low-confidence prediction can be easily flipped due to edge perturbation within its neighborhood.",
            "summary_of_the_review": "The idea is well motivated and proven effective. Some technical issues (presentation, complexity analysis, experiments) need to be addressed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a procedure for scaling adversarial attacks on GNNs by first coarsening the graph (via clustering the node embeddings) and then searching for edges to perturb on the coarsened graph. After the edge is selected the coarse clusters are recursively split until an edge connecting two single nodes is found. The edges are selected greedily based on the meta-gradient. The attack is global and aims at reducing the overall classification performance. A loss function which additionally uses the estimated labels for the unlabeled nodes is also investigated.",
            "main_review": "Major comments:\n- The authors state \"There are only a few works developing efficient attack methods in terms of time complexity.\". Please provide a reference and add a discussion on how those methods are related to the proposed approach.\n  - Approaches that directly study scalability (such as [1, 2, 3]) should be discussed in more detail. In particular, in [1] the authors propose to \"decouple the large-scale graph into several smaller graph partitions and cast the original problem into several subproblems\" which is similar in spirit to the proposed approach (it has also been applied to poisoning). In [3] the authors scale their RL attack to graphs with 2.5 million nodes. \n  - Relatedly, the authors should discuss and compare with the approach proposed in [4] where an attack scaling to even larger networks is studied. Note that this paper was published in the DLG-AAAI'21 workshop and was available before the ICLR submission deadline (like all of the aforementioned papers). An extended version [5] was recently accepted to NeurIPS'21.\n  - Similarly, a fairly successful approach is to run a global evasion attack and simply use those as poisoning edges (i.e. retrain on the graph with perturbed edges). In practice this often leads to good results (see e.g. Table 4 in [1] or Table 3 in [5]). Therefore, the authors should compare their approach with SOTA evasion attacks as well (at least on those datasets where scaling is possible). See [6, 7, 8] for a comprehensive list of attacks. For example, the PGD attack (Xu et al., 2019), and by extension FGSM, should easily scale to graphs at least as large as PubMed.\n- I agree with the statement by the authors that \"Adversarial attacks should be unnoticeable to the original data.\". However, the chosen threat model does not reflect this. The authors follow Zügner et al. and add a global budget constraint, however they omit the local (per-node) constraints which are equally (if not more) important. Given that most nodes have a very small degree, if we omit the local constraints it is easy to cause misclassification since it is easy to end up with nodes that have more adversarial than clean edges. Specifying local constraint relative to the node degrees as in Zügner et al. is prudent, although it is not clear how this should be properly adapted for the coarsened graphs. Relatedly, it is not clear whether the comparison with Nettack and Meta is fair. Are the local constrains also disabled for these methods?\n- The statement \"Furthermore, graphs are discrete data, that using gradient-based methods such as gradient descent to make small perturbations on the graph data is not applicable.\" is incongruent with the discussion that immediately follows. Specifically, the authors propose to use meta-gradients to greedily select which edges to perturb. This *is* a gradient-based approach, the fact that you use meta-gradient (i.e. taking the training into account) does not change this. The approach is still using simple gradients w.r.t. the adjacency matrix which are computed for some function (which happens to include the unrolled training of a GNN). I agree that in general gradient-based attacks are suboptimal for graph data, and the issues with such attacks are also inherited in the proposed approach.\n- The statement \"Previous works on graph coarsening assume the graph is not attributed (i.e., without features X) and focus on preserving different properties.\" can be misleading. There is a rich literature of methods for attribute graph clustering (e.g. [9,10,11,12] to name a few) all of which can be used for coarsening. The proposed coarsening is also based on clustering.\n- It is not clear what the phrase \"clusters the nodes averagely\" means. Can you please elaborate.\n- The discussion of the runtime complexity is appreciated. Additionally, including a discussion of the memory complexity would be helpful. \n- The attack is evaluated only on vanilla GNN models. There are many other proposed defenses which should be evaluated to make the paper stronger (especially given the limited technical novelty). See [6, 7, 8] for a comprehensive list.\n- The ablation study w.r.t. $l$ is appreciated. It would be helpful if the sensitivity w.r.t. some of the other hyperparameters is also studied.\n- The paper is easy to follow, however the overall presentation can be improved and polished (see below).\n\nMinor comments:\n- The authors state \"We use the popular GCN in Kipf & Welling (2017) ... \". However, the GNN in Eq. 2 is not the popular GNN but rather its linearized version. Explicitly specifying *linearized* can avoid confusion.\n- To improve readability the figures, tables and algorithms should be placed in close proximity to the text where they are discussed.\n\nReferences:\n1. Feng et al. \"Scalable Adversarial Attack on Graph Neural Networks with Alternating Direction Method of Multipliers\"\n2. Li et al. \"Adversarial Attack on Large Scale Graph\"\n3. Dai et al. \"Adversarial Attack on Graph Structured Data\".\n4. Geisler et al. \"Attacking Graph Neural Networks at Scale\" \n5. Geisler et al. \"Robustness of Graph Neural Networks at Scale\"\n6. https://github.com/gitgiter/Graph-Adversarial-Learning\n7. https://github.com/safe-graph/graph-adversarial-learning-literature\n8. https://github.com/ChandlerBang/awesome-graph-attack-papers\n9. Xu et al. \"GBAGC: A General Bayesian Framework for Attributed Graph Clustering\"\n10. Yang et al. \"Effective and Scalable Clustering on Massive Attributed Graphs\"\n11. Wang et al. \"Attributed Graph Clustering: A Deep Attentional Embedding Approach\"\n12. Zhou et al. \"Clustering Large Attributed Graphs: An Efficient Incremental Approach\"\n",
            "summary_of_the_review": "Given the limited novelty and the lack of technical contributions the paper needs a much stronger empirical evaluation to warrant acceptance (see suggestions in the main review). In addition, the discussion of the related work is inadequate and incomplete. Therefore, I recommend rejection. I am open to increasing my score if the authors address my concerns.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The authors propose a meta-learning-based poisoning attack on graph neural networks (GNNs). The method works by first coarsening the graph to a size at which it is feasible to conduct a meta-learning attack. Then, the method recursively splits the coarsened nodes until the perturbation with highest score (meta-gradient) is between two nodes in the original (non-coarsened) graph. The authors perform attack on well-known small-scale graphs as well as larger ones (e.g., Arxiv, Physics), where the poisoning attacks lead to increased error when training GCN.",
            "main_review": "### Strengths\n* Scalability is a key weakness of existing GNN attacks, so making progress here is important.\n* The coarsening-based approach by the authors is interesting and seems novel.\n* The results indicate that the attack is indeed successful.\n\n### Weaknesses\n* The description of the algorithm is confusing and key details remain unclear (see detailed comments)\n* The authors do not evaluate existing defenses against poisoning attacks and does not even provide references to them.\n* It is unclear whether and how the method can scale beyond Arxiv.\n\n\n### Detailed comments\n* The attack method is not clear to me after reading the sections multiple times.\n  * What exactly does $l$ do? It does not appear in Algorithm 1 except for determining $K$.\n  * How are the meta-gradient entries computed for the coarsened graph? To my understanding this would require us to re-train the surrogate model on the coarsened graph. However, Algorithm 1 does not say anything about this.\n  * The section about inserting or removing edges between coarsened nodes is confusing to me. To my understanding, the coarsened adjacency matrix is never directly modified, i.e., edges are only inserted / removed between nodes in the graph. So why do we need the distinction between adding and removing edges between coarsened nodes? We could simply take the absolute value of the meta-gradient to decide whether to expand the respective coarsened nodes.\n* The authors do not evaluate whether their attacks break existing defenses for GNNs (e.g., GNNGuard [Zhang and Zitnik 2020], ProGNN [Jin et al. 2020]).\n* The method appears to *barely* scale to Physics and Arxiv, as the number of inner training iterations for the meta learning is set to just $T=1$. Can the method scale to larger graphs, and if yes, how?\n* The paper is missing a reference to Geisler et al 2021: Attacking Graph Neural Networks at Scale. AAAI'21 DLG Workshop.\n* The authors seem to have copied all results except their own from [Zügner and Günnemann 2019]. This is acceptable for the attack results but problematic because it includes the clean accuracy. It makes the comparison between the authors' perturbed results and the clean accuracy less clear, since it is unlikely that the authors' setup is identical to the one of [Zügner and Günnemann 2019] (e.g., the authors use PyTorch, while the 2019 paper uses Tensorflow). Also, it is confusing since some DICE results are copied and others are original (on the additional datasets).\n",
            "summary_of_the_review": "The idea of the approach (improving scalability of attacks via coarsening) is interesting and promising. In my view, however, the drawbacks outweigh the merits currently:\n* While the attack results look decent, the authors do not compare to (or even mention) any of the existing GNN defenses. Thus, it is unclear whether the attacks could be defended currently.\n* The description of the approach has left open questions for me (see main review), making it difficult to judge the soundness of the method. Hence my score of 2 for \"Correctness\", which I am willing to reconsider given a better understanding of the approach.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}