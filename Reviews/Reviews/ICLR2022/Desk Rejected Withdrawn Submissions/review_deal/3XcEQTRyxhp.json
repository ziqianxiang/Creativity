{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper focuses on image preprocessing for Self-Supervised learning (SSL) of image representations.\n\nThe author's main idea is to provide cropping strategies facilitating generalization from datasets with images well-framing objects,  close to the image center,  to datasets with images of complex scenes with several small objects.  \nIn particular, the proposed strategies take into account the relation between objects and the surrounding scene. Which they call “object-aware” cropping. \nAmong the analyzed strategies the Obj+Obj+Dilated based on the BING method is the one obtaining the best results.\nThe authors show that their strategies significantly reduce the gap with respect to supervised methods on the same datasets.  \nThe paper reports results also on COCO and PascalVoc showing that in these cases too improved mAPs are obtained. The authors do a thorough analysis of the performances of four main SSL methods on a restricted version of about 212K images from the OpenImages dataset, namely MoCo-v2, CMC, SwAV, and BYOL. The methods used to derive the proposed strategies are BING, Edge-Boxes, and the method from Vo et Al-2019.\n",
            "main_review": "Strengths:\n1. The paper is well written and sufficiently clear. \n2. A significant amount of experiments supports the main idea.\n3. The idea of revisiting augmentation is interesting.\n\nWeaknesses :\n1. the exploitation of the main idea is poor.\n2. The paper lacks a clear description and evaluation of the idea.\n3. Some assumptions are not convalidated.\n4. The difference between the iconic images in ImageNet and other datasets was already noted in several papers.\n. \nIn object detection, it was evident that object proposals were a crucial component to improve the detection. There is an interesting paper by Hosang, Benenson, Dollar and Shiele “What Makes for Effective Detection Proposals” (2016) revisiting the main approaches on proposals, and discussing their significant correlation with detectors performance. Now, the authors of the present papers, besides generating a well-balanced dataset of 212K images, besides pretraining the considered approaches (MoCo-v2, CMC, SwAV, BYOL) on this refined dataset, also apply a cropping strategy Obj-Obj+DilateCrop, which is a specific object proposal method. \n\nThe strategy, which is the best one according to the authors, consists in taking an object randomly from 10 proposals from BING and choosing a second image by dilating the BING proposal. Now BING is a proposal method originally trained on PascalVOC2007 (see the comments and the evaluations of BING on the above-cited paper).  \nUsually, SSL learning representations use random cropping (what the authors call scene-scene) choosing only the scale/size parameters, to learn statistical and causal information according to specific transformations.  With “object-aware” cropping, the authors provide some explicit information via a  trained proposal method. \nThis explicit information might significantly change the meaning of SSL representations. The object-aware cropping, in fact, provides exemplars of objects selected by a trained proposal method, namely BING, plus part of the scene obtained by enlarging the proposal bounding box.  \n However, to what extent the SSL is changed is never discussed. In order to take the “object-aware” cropping as a good or bad idea, it would be necessary to analyze the pro and contra and the latent factors involved. The pro is better performance on a specific reshaped OpenImages dataset, for the four methods (MoCo-v2, CMC, SwAV, BYOL) trained on the reshaped OpenImages, and other results on COCO considering also Dense-CL, CAST, and Self-EMD. Yet, what kind of latent information is leveraged? Which features are enhanced? Which are the contra? Does the method generalize to other sets of images in OpenImages not considered in the author's redesigned OpenImages? What meaningful representation is learned with the “object-aware” cropping?  These questions are not tackled in the paper.\n\nOther comments. \nOn page 5 it is written that three object proposal methods are selected: BING, Edge-Boxes, and the unsupervised object proposal method of Vo et Al.  However, in the supplementary, it is written that Edge-Boxes is slower than BING therefore no comparison ever mentions Edge-Boxes. Concerning the reference to Vo et Al., it should be noted that they use the Randomized PRIM’s algorithm experimented in Vo et Al. on VOC_6x2 containing only 6 classes. So how the randomized PRIM algorithm is extended to all the classes in OpenImages? How it is implemented?  These concepts should be made precise in the paper. \n\nIn the caption of Table 4  and also on page 8, it is written “we outperform CAST (Selvaraju et al., 2020) which also relies on localized crops”.  But CAST uses unsupervised saliency maps to intelligently sample crops, and to provide grounding supervision via a Grad-CAM attention\nloss, while in the present paper the crop is guided by a proposal method explicitly trained with SVM.\n\nMinor: sometimes we find VOC, sometimes VOC-2007, sometimes Pascal-VOC, in any case, it should be specified always if 2007 or 2012 and mentioned entirely.\n",
            "summary_of_the_review": "The paper starts from a largely known and analyzed problem related to ImageNet iconic images as opposed to more complex datasets, and proposes a new augmentation strategy. The strategy called “object-aware” cropping is based on using object proposal methods and adjusting the outcome to enlarge the field of view of the proposal so as to include parts of the scene.\n\nThe proposal methods are BING, Edge-Boxes, and Randomized PRIM’s algorithm (referred to in Vo et Al2019.). The first two are supervised methods, the last one is an iterative tree-growing procedure and has been experimented on a few classes. The authors do not talk at all about the second method and do not explain how to lift the last one to the number of classes in OpenImages.\n\nIn particular, none of the methods for augmentation are vaguely formalized nor their implementation described with suitable algorithms. \n\nDespite a certain amount of experiments supporting the idea, the idea is not novel. \n\nAs a matter of fact, the idea of using proposals has been already widely explored in supervised object detection. The only contribution, not formalized, is to enlarge the object proposal bounding box.\n\nThe paper does not explore the consequences of this augmentation strategy on SSL, it only reports the effects of augmentation on some SSL methods about some datasets. \n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes using object proposal algorithms as a guidance of cropping operations instead of random cropping for self-supervised learning in wilder datasets. In conventional image datasets such as ImageNet, there was only a single object in the center of the image. On the other hand, more challenging datasets such as OpenImages, such assumption does not hold and therefore randomly cropping parts of the image is not a good choice. The paper experimentally demonstrated that random cropping is harmful and the object-aware cropping is much better as shown in Table 1. The proposed method is shown to be slightly better than state-of-the-art similar approaches as shown in Table 4.",
            "main_review": "Pros:\n+ The idea is very simple and easy to reproduce.\n+ The performance improvement from the baseline approaches worth sharing among the researchers.\n\nCons:\n- The idea is too simple and straightforward. I do not see any technical innovation, but it is more like the egg of Columbus.\n- The performance improvement from the state-of-the-art (in Table 4) is negligibly small. Although I admit that the proposed method is much simpler than previous methods, I wonder how such small improvement would contribute to the community.\n\nConsidering the trade-offs between the pros and cons above, I am afraid that the quality and the technical novelty of the paper is below the standard of ICLR.\n\n\nHere are some other comments to improve the paper.\nThere are a few mistakes such as missing spaces before brackets, unnecessary comma and period in Figure 2 after “Ours,” etc.\n\nI like the idea itself, but to add more technical novelty, for instance, the authors might want to discuss the optimal cropping strategy rather than using “random” crop in the proposed obj-obj+dilate crop.\n",
            "summary_of_the_review": "Although the idea is interesting and promising, the technical novelty and the performance improvement from previous works do not cross the threshold of ICLR.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I do not see any ethical issues in this paper.",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper analyses the effect of the cropping strategy in self-supervised contrastive learning, especially in datasets like OpenImages and COCO, which have multiple objects in an image. The authors propose to replace one of the scene-level crops with an object-based crop using object proposal methods. Clear improvements are achieved on different self-supervised learning methods.",
            "main_review": "Strengths:\n- The proposed object cropping strategy is simple, straightforward, and motivated. How to better utilize the uncurated images in self-supervised learning is an open challenge, especially when most of the attention is still on learning representation with curated ImageNet.\n- The paper is well-written and organized. The figures and tables are clear and informative. It's pretty easy to read and follow.\n- The proposed object-aware cropping approach introduces consistent improvements over different pre-training datasets and pre-training methods, including MoCov2, BYOL, and DenseCL.\n\nWeaknesses:\n- Table 1 can be more complete, e.g., to list the results of BYOL with Object-Object+Dilate crop here. \n- According to the classification results on OpenImages and Imagenet, SwAV shows the best results among different self-supervised learning methods. Do the gains still hold when using SwAV as the baseline? Or at least the authors should provide some description about how to apply the proposed cropping strategy to the SSL methods using multi-crop, e.g., SwAV and  Gansbeke et al. (2021).",
            "summary_of_the_review": "The paper is well motivated and presented. The proposed method is simple and effective. The authors have provided comprehensive analysis and experiments to support the claims.\nJust a few minor concerns have to be solved.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose an alternative to the random cropping used in many self-supervised approaches.  Their object-aware cropping provides improvement on COCO and OpenImages.  They show improved performance on these two datasets over standard MOCO-v2, BYOL, other SSL methods, and supervised Imagenet pretraining on several tasks.",
            "main_review": "Strengths:\n* The intrinsic question and issue the paper raises in important.  How do SSL methods perform on “non-Imagenet” datasets?  If they fail, then why?  While there has been work done on this topic in the past (which the authors do NOT reference), it has not been studied exhaustively.  So the issue being raised is important, however, I believe the paper falls short on addressing it.\n\n-------------\nWeaknesses:\n\n* There is a bit of muddiness around the goal and motivation of the work.  The claim seems to be that the random cropping paradigm used in most SSL works is suboptimal.  However, because the focus of those works is often only Imagenet, they “get away with it”.  Instead, a smarter cropping approach is proposed.  Then what needs to be established is twofold- the standard approach fails on other datasets (which is shown for OpenImages in Table 1) and the proposed approach is better on both Imagenet (although perhaps not substantially, but it shouldn’t be worse) and other datasets.  The focus of much of the early analysis, however, is on OpenImages (not on the method itself).  Instead it seems the main question is “why does MoCo-v2 struggle on OpenImages”?  This is a valid question, but also quite limited- there are many other datasets out there.  Certainly this may have been the motivating observation, but if the goal is to understand why SSL performance varies dramatically across datasets, this type of analysis needs to be performed on many datasets.  Additionally, if the goal is to understand why MoCo-v2 struggles on OpenImages (and other datasets), simply performing data analysis isn’t sufficient to motivate cropping as the source.  There is good reason to believe it is a contributor, but this needs to be explored more systematically, for example, by varying the distance over which the crop could occur.\n\n *  Part of the claim of this paper is implicitly that current SSL methods work on Imagenet because of the properties of Imagenet (or perhaps they are overfit to Imagenet) and that different approaches (i.e. theirs) are needed to handle SSL more broadly.  COCO and OpenImages are both fine datasets to use, however they are still relatively similar to Imagenet in a lot of ways.  Because this is the focus/motivation of this approach, I believe it’s necessary to show similar performance on a wider range of dataset types.  It would be helpful to show performance on other datasets which are “scene dominated” (e.g. Cityscapes- which has many more than 8 objects per image) or non-natural scene imagery (e.g. something medical or remote sensing focused).  Additionally, one still needs to demonstrate that this approach performs as well or better on Imagenet.\n\n* Using BING and Edge-Boxes which are trained on Pascal-VOC is problematic.  Because these proposal methods are trained on an external dataset, they necessarily include information about that data distribution.  Therefore by using those proposals, you are brining in significant new information.  Even though class information is not used in those models, “objectless” is necessarily included in the identification of the bounding boxes.  Furthermore it implicitly puts more training emphasis on \"important\" non-background regions which will cause additional capacity to be devoted to these regions.  This is not a self-supervised task therefore because the methodology used to focus the cropping is based on labels.  There may still be value in using an approach like this, but it is important to acknowledge that by use of that information embedded in the bounding boxes implicitly, this is no longer a self-supervised approach.\n\n* The OpenImages-subset is fairly substantially modified from the original dataset.  Many SSL methods focus on one-shot learning as a task to demonstrate the quality of the representation learned, but here the authors have eliminated classes with few samples.  In a real-world scenario where one is using SSL methods, they likely don’t have class labels so doing a dataset reduction like this is likely infeasible.  Selecting this subset could still be justified, but the motivation needs to be explored more.  For example, if using this subset as opposed to the entire dataset has a profound effect on the results, that says something about SSL methods generally.  How then would this impact the usage of this approach in the real world where someone doesn’t have the class labels a priori?  Also, how might the result on a modified Imagenet look like if a similar subsetting were used?\n\n*Section 2.1: “train a linear classifier or fine-tune”.  Which approach do the results in Table 1 refer to?\n\n* Section 2.1, Figure 1.  I think it is also important to show the OpenImages performance starting from (supervised) Imagenet pertained weights, not just random.  Past works have shown that even when the downstream task is switched, (supervised) Imagenet weights are often comparable to SSL weights on the in-domain dataset.  This included in Table 2, but for completeness sake, I think it would be useful to include this line in Table 1 as well.\n\n*Section 2.1.  Because the method has not been discussed in detail, the results of the proposed method shouldn’t be discussed in 2.1.  It’s fine to include them in the same table (as opposed to two separate tables), but the new method needs to be discussed before the results are referenced.   The last line of Table 1 can be referenced later.\n\n* Figure 3 isn’t a main-text figure in my opinion.  OpenImages is a well researched public dataset.  It would be sufficient to put that figure in the appendix and simply reference the results of the analysis conducted. \n\n* Table 2 is not understandable from the table and caption alone and requires going into the text, which is suboptimal.\n\n* Table 3 is verbose and could be dramatically condensed.\n\n* The methods and results are hard to follow.  If the same training protocol is used throughout, then it can be stated once in a single section.  However, if different approaches are used for detection vs. segmentation, I would put each of those detail elements in the section in which the results are discussed - We did X in this fashion and here are the results.  \n\n* Sometimes related work makes sense near the end, but here I believe it needs to come toward the beginning of the paper.\n\n* In the related work, again the focus is explicitly around COCO and OpenImages.  There has been significant work done on non-Imagenet datasets where performance has been good/reasonable.  This somehow needs to be addressed.\n\n* Varying the delta parameter is an important experiment.  However, I believe the missing baseline is to vary the related parameter in the absence of the detector.  That is, if you used just basic MOCO-v2 but restricted the random crop (i.e. required overlap to be in a certain range), how would the performance change?  This should be done on Imagenet and the alternate datasets.  \n\n* Hardware used for training is not stated.",
            "summary_of_the_review": "The paper raises and important question around how current SSL methods perform on non-Imagenet datasets and propose an approach to deal with an observed deficit they found in OpenImages  and (to a lesser extent) COCO.  However, the paper falls short in multiple ways.  First, the proposed approach is not tested on Imagenet (so it is not clear if it is comparable or worse or if they have just found a method that works better for OpenImages/COCO).  Second, their method has not been demonstrated to be broadly useful.  COCO and OpenImages are still natural scene datasets which, while slightly different, are relatively similar to Imagenet.  Performance on dramatically different datasets (cityscapes, medical imagery, remote sensing imagery, faces,  rooms) needs to be demonstrated.  Finally, and very significantly, using the BING detector is problematic in the way in which their evaluations are conducted.  Because that detector is trained on Pascal-VOC, it is necessary brining in information about that data distribution.  Therefore performance could be as a result of this.  It is also unclear whether this detector is even needed or if varying the amount of overlap from the centroid would be sufficient.  It also makes this not a self-supervised approach because annotations were used in the creation of the detector which directs the selection of the cropping mechanism.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}