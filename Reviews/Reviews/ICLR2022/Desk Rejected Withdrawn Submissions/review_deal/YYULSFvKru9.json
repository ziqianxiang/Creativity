{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes StARformer, an architecture improving on Decision Transformer by incorporating S-A-R embeddings generated via Step Transformer layers in the style of Vision Transformer. StARformer thus encodes a locality inductive bias and a more expressive observation encoder. For each timestep, the previous action, reward, and current state patches are processed by a shared ViT module, whose outputs are intertwined with convolutional state embeddings in an autoregressive transformer. In offline RL and imitation learning settings on Gym and Atari, StARformer outperforms DT. Additionally, StARformer is more robust to the context size hyperparameter of DT. Ablations show step-wise reward outperforms return-to-go, having both convolutional/transformer state encoders help, and DT with ViT works poorly.",
            "main_review": "Strengths\n1. Performs an investigation to improve the performance of supervised approaches to offline reinforcement learning. Such works could enable better stability and performance for RL training.\n2. Incorporates ideas from video transformers to the similar setting of reinforcement learning from image observations. This might help expand the diversity of ideas considered for RL tasks.\n3. Shows decent improvements over DT in Atari, almost 2-3x in terms of mean return in some settings.\n\nWeaknesses\n1. The method is quite complex; it additionally adds many more parameters (10x reported in the paper) and also likely FLOPs/wall-clock time (not reported). Although Section 5.4 seeks to investigate ablating model capacity, they do so in a nontrivial way by adding ViT to DT. While it is interesting DT performs worse than ViT, another comparison that would more closely answer the capacity/compute question would be to increase the size of DT via layers/width, or to train with a larger batch size. In fact, STaRformer also performs worse with ViT when used like the proposed \"DT with ViT\" (Table 3). Keeping fair comparisons by scaling models correctly is an important consideration in transformers literature, and some results show that base transformers scale best when scaled properly. In particular, I think the complexity of the proposed approach limits the possible future significance and applications.\n2. I also have some confusion over the proposed insights of the paper. A large proposed contribution seems to revolve around \"strongly-related local causal relations\" due to close relationships between elements within in a timestep, which sounds reasonable. However, in Table 3, it seems removing the convolutional encoder performs about as well as DT. If I am not misunderstanding, is the key idea that both convolutions and attention patches improve diversity in the representations which leads to the performance improvement?\n3. I didn't understand how conditioning was being performed in this method, or by \"DT with step-wise returns\". If no target return is being input and no procedure is used to optimize the rewards (ex. beam search or MCTS), then I think the method resembles behavior cloning, and not necessarily offline RL. The results showing STaRformer performs about the same in both \"offline RL\" and \"imitation\" settings seems to support this, although I don't really understand how STaRformer improves on behavior cloning baselines in the offline RL case.\n4. It is not clear what datasets are being used here. In Section 4, Agarwal et al. 2020 and Fu et al. 2020 are cited -- both of these papers propose multiple datasets per environment. The DT paper used the 1% dataset for Atari and medium, medium-expert, and medium-replay datasets for D4RL; it should be made clear which settings are being evaluated in the paper.\n\nOverall, I am excited about new work applying ideas from other transformers work (cross-attention between modalities in a two stream architecture, although the data is sort of repeated here), in particular the consideration that RL can be tackled with video transformer-like models. While the improvement over DT is interesting, I think the insights are not very clear to me; if they can be clarified or I am misunderstanding details, then I will be open to raising my score during the rebuttal period.",
            "summary_of_the_review": "Since the baseline comparison could be improved (Weakness 1) as well as some of my confusions over insights from the paper/where the improvements are coming from (2 and 3), I currently recommend rejection.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The presented paper is an extension to the \"Decision Transformer\" (DT) architecture\nwhich approaches Reinforcement Learning problems through sequence modelling\nusing attention mechanisms. The authors claim that the Decision Transformer \napproach exhibits problems when trained on long sequences. They propose\nto explicitly include state-action-reward tuples from previous timesteps\ninto the sequence model by preprocessing them separately by another attention module.\nThe authors state that this simplifies attending to more recent information which is more\nimportant for action prediction. \nIn contrast to \"Decision Transformer\", the image encoding mechanism is additionally changed. \nThe proposed method significantly outperforms DT on image-based (Atari) \nenvironments while yielding similar or slightly better performance \non state-based (Gym) environments.\n",
            "main_review": "Strengths:\n\n* Manuscript is in general well-written, provided figures Fig. 1, Fig. 2 \nhelp to understand the approach.\n\n* Performed ablation experiments are rather comprehensive \n(except the points I list in \"Weaknesses\").\n\n* Empirical results significantly improve compared to the SotA baseline \n\"Decision Transformer\" on Atari environments.\n\nWeaknesses:\n\n* The contributions of this paper stay unclear and it should be stronger \nhighlighted what exactly the weaknesses of DT are which were addressed.\nWhy is modelling long sequences problematic in DT? Can the DT not\nlearn to attend to states, rewards and actions from the recent past\nmore than from the far history?\n\n* Background and explanation regarding \"Reinforcement Learning\nas Sequence Modelling\" in the framework of \"Decision Transformer\"\nis insufficient; it is not clear how the formulation leads to a \nmaximization of reward, this needs further explanation.\n\n* Experimental setting and goal of \"Imitation Learning\" without rewards stays unclear to me.\nDoes it replicate the behaviour from the offline RL dataset? \nHow does it lead to \"optimal\" behaviour of any form?\nI would suggest to properly explain all experiments in the Appendix in more detail.\n\n* Significance of results in Table 1 is hard to evaluate, given that\nthe evaluations were performed on only three random seeds and numerical \nresults are close between \"DT\" and \"SF\", especially in the Gym Environment setting.\nRunning the experiments on more random seeds would strengthen the results,\nalthough I understand that computational constraints may be an issue.\n\n* The main performance increase can be observed on the Atari environments.\nHowever, in contrast to DT, in these environments also the image embedding mechanism differs.\nThe Gym environment model is most comparable to the original Decision Transformer, \nas the \"patch embedding\" extension is not in effect here. \nOn the Gym environments the proposed model does not seem to give a significant \nperformance increase (see above). Furthermore, the ablations given\nare not sufficient in my opinion to certainly attribute the performance increase\nto the StAR embedding (see below).\n\n* From the set of ablations given I cannot infer where the performance\nincrease over Decision Transformers on the Atari environment *actually* come from,\nwhether it is the special way of embedding the images patch-wise or the\n(additional) StAR representations or both. In addition, it stays unclear\nwhich actual model modifications were performed for the ablation studies.\nMy main questions are:\n  * How does the (conv -> ViT) method differ from the proposed main method?\n  * For the \"DT with ViT\" ablation, did you use an architecture which is closer\n    to the \"conv\" or to the \"ViT\" architecture? I'm asking because the \"ViT\"\n    ablation already performs worse to the \"conv\" embedding strategy,\n    thus it may be no surprise that \"DT with ViT\" does not reach the performance\n    of \"SF\" in Table 5.\nIn my opinion (1) it should be ensured that DT is extended by exactly the same\nembedding mechanism used in \"SF\" (Table 5) and (2) to have an ablation\nin Table 3 which uses the same (non-patchwise) embedding as in DT for both\nthe State Embedding and S-A-R Embedding.",
            "summary_of_the_review": "The main strength of this paper is the performance increase in Atari environments\ncompared to \"Decision transformer\". Unfortunately, in my opinion, this can\nnot clearly be attributed to the StAR representations as appropriate baselines are missing or unclear.\nThe fact that the performance increase on (state-based) Gym environments is marginal\nindicates that the observed performance increase may not solely be attributable\nto the StAR representations but also to the patchwise image embedding mechanism. \nFor this reason, my current recommendation for this paper is \"weak reject\".",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a transformer architecture applied to sequential decision making which explicitly encodes local relationships between temporally close state-action-reward tuples. Starformer combines existing conv methods and local causal modelling to learn local and long-horizon representations used for action generation in RL. \nTo this end, they divide the action prediction task into 2 parts - local representation learning (using a “step transformer”) and long-horizon representation learning (using a “sequence transformer”). The authors perform ablations testing the utility of the components and perform experiments on Atari and Mujoco. They also perform ablations on various reward formulations. The main novelty of the paper is the explicit encoding of state-action-reward tuples into a causal representation which the authors claim improves sequential decision-making.\n",
            "main_review": "Strengths:\n1. The method section is well written and easy to follow. \n2. Figures are clear and concise.\n\nAreas for improvement:\n1. It is unclear to me that modeling local relationships between state-action-reward tuples is the reason why starformer outperforms the baseline. From my understanding, there are 3 possible factors that affect the performance of starformer - (1) What is claimed by the authors, i.e., local causal modeling, (2) Conv representation of states (which is not novel and is used by both the baseline DT and Starformer) and (3) The number of parameters (Starformer has ~10x the parameters of the baseline). \nTherefore, a comparison between Starformer w/o conv and DT is essential to rule out the effect of Conv. The reason for this is that Starformer w/o conv only utilizes the single novel contribution of the paper i.e., the State-Action-Reward representations per time step while DT only utilizes Convolutional layers. Such a comparison can be made by looking at Table 3 row 2 and Table 1 row 4. Clearly, the results are mixed, with DT beating Starformer w/o Conv in 2/6 tasks while the others being rather close. What worries is me that DT has an order of magnitude fewer parameters than Starformer w/o Conv, but still achieves high performance. This has left me wondering whether the idea of locally modeling interactions does really help. I believe a table displaying performance along with parameter size will help clarify this. \n2. The authors claim that their step transformer allows for the modeling of causal relationships between state-action-reward tuples. There is no experimental evidence to support this. To my understanding, being temporally nearby doesn’t imply any causation. Of course, interventions are likely impossible in the environment and so making claims of causation is unsubstantiated. I would, however, be satisfied if the authors are able to visualize the latent space of the representations learned by their method and are able to generate some causally consistent interpretations of the knowledge acquired by the agent. For instance, state-action-reward tuples in “dangerous” zones of the games cluster together. While proving causation is impossible, such interpretations would lend credence to their claims - without this the paper is incomplete. \n3. The lack of implementation details is also worrying. When comparing the Starformer with a modified version of the DT, the authors do not enlist the parameter size of the modified DT saying merely “it is comparable”. The lack of error bars especially on parameter size ablation experiments is particularly disconcerting. While Figure 2 is informative, it is impossible to quickly glean the salient features of the method at a glance. A pseudo-code would be helpful. \n4. The authors claim that Starformer is able to model short-horizon causal relationships between temporally adjacent state-action-reward tuples. However, Figure 3 makes me question this as DT (in spite of fewer parameters) outperforms Starformer at the shortest sequence length task 6/9 times.\n5. Finally, I question the generalizability of their results. A 10x larger network performs rather poorly in continuous control (arguably a more difficult domain) - offering minimal improvement. Thus, it’s impossible to say whether Starformer will generalize to domains other than the 6 Atari games on which it is tested. More experiments on the remaining environments in the Atari suite are necessary. Additionally, I would like to see the ablations, i.e., Table 5 for the Mujoco experiments. \n",
            "summary_of_the_review": "Overall, the paper does present some interesting ideas, namely that state-action-reward representation learning is beneficial for solving sequential decision making. However, I am not convinced that the claims made by the authors are substantiated in the experimental results.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces a new transformer architecture specifically for sequence modeling for RL. It combines a local representation which they call StAR-representation of each (s, a, r) triplet with a long-term sequence prediction (i.e. across different timesteps). The paper shows results in several Atari and Gym benchmarks.",
            "main_review": "Positives: \nThere do seem to be some real improvements in results from using this new architecture, at least on Atari\n\nMajor issues:\nThe paper does not really introduce a lot of new ideas. The major proposed idea is this joint SAR embedding as input to a transformer, but that is really the only unique idea in the paper. \n\nThe ablation studies are somewhat confusingly presented and I am not sure they do a good job of showing the effect of various components of the new model and where the improvements are coming from. In fact, I think they don't support the conclusion that the main architectural novelty claimed in the paper is actually what is causing the performance improvement. \n\nMy summary of the architectural changes from DT to StAR are:\n* The step transformer\n* The state embedding (the patch embeddings fed into the SAR embedding)\n* The integrated state embeddings (the convolutional embeddings)\n\nParts of this are ablated somewhat in Table 3 where it shows variants of the last two changes, but there is not a version which uses the state embedding from the baseline DT model (just a linear embedding of state I believe). Especially because Atari is very image based, \n(and these are the only environments where this model really gets any improvement) this seems really important. And the ablations they do actually seem to really back up the idea that the state embedding is really what is making the difference over DT. When you compare the two image ablations with DT (btw, it is very inconvenient that DT was not a row in Table 3 to make this comparison), the ablations perform pretty similarly or worse than the DT baseline except for on Pong. \n\nThe ablations also do not have variance runs (or at least they're not shown), so conclusions are pretty hard to draw since you're essentially comparing on a very high variance with a single seed. Were variance runs done on the ablation? If so, what are the variances here and how many seeds are used?\n\nThe StAR embeddings part of the model is not clearly ablated (either a model which has StAR embeddings without the simple state embedding from DT or a model which uses the new state embeddings but without the StAR embeddings). I think having these would make it even more clear where the improvements are coming for. \n\nThere is an experiment run where there is additional state encoding in the baseline. But I don't find this as convincing as an ablation on StAR because this embedding is not identical at all to the state encoding in the StAR model (which uses two kinds of state encoding). There is also the problem that ablating from the main method as apposed to adding components to a baseline is just usually better because you get the benefit of any parameter tuning from your main model as baselines often get much less parameter tuning (see [1] for more discussion about this problem).\n\nI think a direct ablation of the SAR encoding would also have been really helpful since it's I think the main contribution of the paper. Since the sequence is mostly just the patches from the image, was a transformer even necessary here? What about just a joint linear embedding of the (a, r, s) tuple? Wouldn't that also give us this locality property?\n\nThe fact that there's only one baseline is a bit concerning. Would have been good to see a comparison with other sequential models (e.g. LSTMs, RNNs) or even some non-sequential models with the state history stacked in the input. I would have expected a bit more work done in baseline comparison.\n\nI don't think the paper ever actually says what the learning algorithms actually are? I think all it says is offline RL and imitation learning. But what algorithms? Behavioral Cloning? Off policy Q? Going back to baselines, is the architecture sensitive to choice of learning algorithm? Is it possible that with different choices of learning algorithm that the proposed architecture would not work as well.\n\nI know that the environments were chosen to compare more directly with DT, but I think more difficult Atari environments or environments where long-term policies are very important would have been more convincing. The Atari environments chosen are all pretty reactive and do not seem to really require much long-term sequential modeling. A lot of them involve pretty short term decision making (shooting ships that are currently on screen, hitting the ball of the paddle without missing, etc). A game with more long-term decision making like for instance Motazumas would have actually better tested these ideas. I am not convinced that any of these games require long term strategies. Similarly, the Gym environments are all locomotion where the best results can be achieved using single state prediction.\n\nRelatedly, the experiment in Figure 3 does not really convince me of much. First, like with the ablations, I am unsure that there are any variance runs on these experiments, so it's hard to know if these results would even hold up on multiple runs. Secondly, even taking these numbers at face value, the proposed pattern here seems very weak (I see it on a few games such as Assualt, Boxing and Breakout, but it's a lot less clear on the other environments).  Putting that aide to, I don't think this result does or could show that this means that the model is \"modeling long-term sequences.\" First of all, it's not even clear that you NEED a sequential model for these games. That to do well you need to even model long term sequences. Secondly, the length T here is the INPUT sequence, not the output. So if anything, all this says is that the model can handle long sequences better. I don't think this is a pedantic difference; a model could \"handle\" a long sequence better by simply ignoring most of the sequence and only attending to the last few states and thus do better than the baseline. But that's clearly not \"modeling\" long term sequences. I think it is not proven that the model is actually discovering or using any long term sequences. I think to show this, you would actually want to qualitatively or quantitatively show that it is solving some task better that REQUIRES long term sequence modeling. Even a toy task might demonstrate this, such as a memory-type task where it has to use information from multiple time steps to take the correct action.\n\nMinor issues:\nThe paper also does not include nearly enough hyperparameter and training details to be at all reproducible. There's a single table in the appendix. It says that most hyper-parameters are the same as Chen, but which ones are not the same? There are definitely missing parameters (e.g. number of training epochs/steps/episodes?) \n\n[1] Deep Reinforcement Learning that Matters. Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, David Meger",
            "summary_of_the_review": "Overall, there are a few interesting results, but the paper does not add a lot to the field. The ablation studies do not convincingly show that the claimed novel contribution is really responsible for the results. There is only one compared baseline. And the environments chosen in the experiments are pretty simple and do not really require long-term sequences, and so do not tell us a lot about how to do long-term sequence modeling in RL.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}