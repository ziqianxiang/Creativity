{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper tackles the problem of few-shot image generation. The method first train a generator/discriminator using a large-scale dataset in the source domain and transfer the trained models to the target domain where we only have few (e.g., 10 images) examples. To help produce diverse samples for the adapted generator, this work introduces momentum relation distillation module. For help ensure the discriminability of the adapted discriminator, this work include training images from the source domain as part of the real/fake examples. The paper demonstrates the performance on several few-shot image generation tasks, including FFHQ to sketches, caricatures, and other attributes (e.g., babies, sunglasses) and LSUN cat to dog.",
            "main_review": "Strength:\n+ State-of-the-art generative adversarial networks often require a large amount of diverse training data to achieve satisfactory results. The problem of few-shot image generation is an important and timely topic.\n\n+ The proposed approach is technically sound. \n\n+ The paper provides a thorough list of experiments, including direct comparisons with most of the existing few-shot generation methods. The paper also presents ablation study on the proposed design choices (mainly on momentum and discrimination).\n\nWeakness:\n\nNovelty:\n-\tMy main concern on this paper lies in its limited technical novelty. In essence, this paper takes [Ojha et al. 2021] as a baseline and \n-\tA) replace the cosine similarity with SSIM\n-\tB) effectively increase batch size using momentum update (as used in contrast learning methods)\n-\tC) keep using the source images when finetuning the generator.\n\n-\tThese seems to be reasonable practices and seems to work slightly better than the baseline [Ojha et al. 2021] quantitatively (particularly for FID). However, as FID score could potentially be misleading particularly in the few-shot setting., I am not entirely convinced that the proposed method materially advances the state-of-the-art.\n\nExposition:\n-\tThe paper claimed that it’s contribution is on applying knowledge distillation techniques to the few-shot generation problem. However, after reading the paper, I realize that this contribution is mainly from Ojha et al. 2021 where they distill the knowledge from the source model using a correspondence loss. I think it will be more appropriate to clarify the core novelty of this work.\n\n-\tI understand this is probably following the naming convention from prior work, but I think the title “few-shot image generation” could be misleading the readers. In all the reported cases, one need to first pre-train a StyleGAN in the source domain with numerous training examples. Also, the target domain needs to be semantically or structurally similar to the source domain. Perhaps “few-shot transfer” or “few-shot adaptation” would be much more precise because here the generator is not trained from scratch. \n\nExperiments: \n-\tIn Section 4.1, it would be great to highlight the specific example and refer the readers to them when discussing the points. For example, from Figure 4, I could not draw the conclusion that “CDC prefers to frontalizing faces”. \n\n-\tAs discussed in the paper, looking at the FID scores could be misleading and therefore we also need to examine the diversity of the generated samples by measuring the intra-cluster pairwise LPIPS distance to get a complete story. Why do Table 1 and Table 2 report different target domains (the first two column)? \n\n-\tFrom the visual comparison with CDC [Ojha et al. 2021], I don’t really see that the proposed method has a clear improvement. For example, in Figure 6, I would prefer some of the samples generated by CDC in three settings. \n\n-\tHow many samples were used to compute the FID score? If it's only using the 10 examples for finetuning, then it's unlikely that the scores are reliable. ",
            "summary_of_the_review": "This paper presents some interesting modifications on CDC to improve the quantitative scores on fidelity and diversity measures. However, the technical novelty seems limited, and the experimental results (particularly visual results) are not very convincing. I am thus leaning slightly negative about this paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work studies few-shot domain transfer of GANs, which adapts a GAN model trained on the source domain to synthesize images on the target domain with only several reference images. To alleviate the overfitting problem, a similarity regularizer is added to the training of the generator. Meanwhile, due to the lack of real images for the target domain, the discriminator is asked to distinguish real/synthesized images from both the target domain and the source domain.",
            "main_review": "**Strength**\n\n- Good FID results compared to the baseline approach, CDC.\n\n**Weakness**\n\n- First of all, a tiny suggestion, \"few-shot image generation\" in the title may not be accurate since the model is already pre-trained on a large-scale source domain.\n\n- Another suggestion, which is also one of my major concerns (limited novelty), \"knowledge distillation\" is also not accurate.\n\n   - From the generator perspective, the contribution compared to CDC is marginal. Concretely, a momentum trick, together with a memory bank, is used to enlarge the queue size, and \"SSIM-A\" is proposed to compute the similarity. Other parts are almost identical to CDC. (**Proof:** Eqs. (2-4) are proposed by CDC, Eq. (5) is the \"SSIM-A\" criteria, which is marginal to SSIM. Eq. (6) rewrites Eq. (2) with \"SSIM-A\". Eq. (7) is the momentum trick proposed by MoCo. Eq. (8) is identical to Eq. (4).)\n   - From the discriminator perspective, the only difference to naive fine-tuning is that the discriminator is also asked to differentiate real/synthesized images from the source domain, to alleviate the overfitting problem. This is not distillation *at all*.\n\n   Could the authors explain *what* knowledge is distilled?\n\n- Experiments:\n\n   - Fig. 5 is meaningless. Two pairs of D-score (green-pink, and red-blue) are not comparable at all, since they are produced by two different discriminators. Hence, I can tell nothing from Fig. 5.\n\n   - Fig. 6. Why not use the same reference image for the one-shot comparison? The target image is already reported in the original paper of CDC.\n\n- Evaluation metrics:\n\n   - How many samples are used for FID calculation?\n   - Why not use the precision-recall metric [1] to evaluate the diversity? The \"pairwise LPIPS distance\" reported in this paper seems not convincing.\n\n   [1] Improved precision and recall metric for assessing generative models. Kynkäänniemi *et al.* NeurIPS'19.\n\n- The paper is hard to follow and there are many typos. I would recommend polishing the writing and the language. Following are some casual examples:\n\n   - What is $D_P$ in Eq. (4)? Does it stand for the patch discriminator? Then, what is $D_T$? Are they both fine-tuned from the pre-trained discriminator?\n   -  \"few-show\" in Figure 1 caption.\n   - \"superior\" in the fourth line under Figure 6 (on Page 9) should be \"superiority\".\n   - The last sentence of \"Quality evaluation\" of Sec. 4.2 is not even a sentence.\n\n- Minor Suggestion: The framework uses three figures, *i.e.*, Figs. 1-3, which are not informative.\n",
            "summary_of_the_review": "I would suggest this work go through another review cycle. Please refer to the **Weaknesses** in the **Main Review**.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The authors proposed to adopt knowledge distillation for few-shot image generation. To deal with the the problem of over-fitting, they contributed from two aspects, with one the momentum relation distillation (MRD), and the other the source discrimination distillation (SDD). The former is to produce diverse samples by creating a dictionary of feature tensors with momentum update for relation distillation, and the later is to ensure the fidelity discrimination by making use of source data. The proposed model outperformed previous SOTA models by a large margin as they reported. The paper is well written and the logic is clear. ",
            "main_review": "Strengths:\nThe problem of few-shot image generation (or more generally, few-shot learning) is a very interesting and meaningful topic in current machine community, and also the industry, especially in some domains such as medical imaging, astronomical imaging, etc. The model proposed in the work achieved SOTA results on few-shot image generation, which is good and having contributions from the engineering view. \n\nWeaknesses:\nMy main concern is that the work may be incremental, which is based on StyleGAN2, and integrates several several methods in to a large framework. For example, 1) the string baseline is from Ojha et al. (2021); 2) the momentum relation is also based on previous works, where an extra generator is used accroding to some \"memory bank\" works in order to improve the consistency of the model adaptation from source to target domain; 3) for alleviating overfitting of the discriminator, a discrimination distillation loss is introduced to the final model. The authors should claim more about the innovation of their work.\n\nSome other issues need to be addressed:\n1) The hyper-parameters $\\alpha$ and $\\beta$ in the full objective seems to be empirical, with $\\alpha=10^4$ and $\\beta=0.1$. Why the value of $\\alpha$ and $\\beta$ are set so differently?\n2) The results in the ablation study showed that the baseline model has already achieve the SOTA performance comapred to other methods such as CDC and EWC for few-shot image generation. It is better to show what the baseline model is and the training details should be made clearer.\n3) Apart from the performance level (meansured by LPIPS and FID), it will be good to give some examples to show how each part in the model affects the generated image quality, especially the MRD and SDD.",
            "summary_of_the_review": "The submission is well written but the proposed model is based on several provious works which makes the work incremental.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}