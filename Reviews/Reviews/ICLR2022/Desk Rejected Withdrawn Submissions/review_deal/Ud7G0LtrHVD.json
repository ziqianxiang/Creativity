{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "Motivated by the question \"How does ViT perform when individual input image patches are perturbed with natural corruptions or adversarial perturbations, compared to CNNs?\", this work conducts a study on the robustness of vision transformers to patch-wise perturbations, and concludes that vision transformers are more robust to naturally corrupted patches than CNNs while they are more vulnerable to adversarial patches.  In addition, the authors conjecture that ViT's stronger robustness to natural corrupted patches and higher vulnerability against adversarial patches are both caused by the attention mechanism, i.e., the attention model can help improve the robustness of vision transformers by effectively ignoring natural corrupted patches, whereas when vision transformers are attacked by an adversary, the attention mechanism can be easily fooled to focus more on the adversarially perturbed patches and cause a mistake.",
            "main_review": "The strengths of the paper are as follows,\n+ This work provides a new perspective to understand the robustness of ViTs, which is to connect ViTs' robustness with their attention mechanism; \n+ The comparison experiments on the robustness of ViTs and CNNs can potentially inspire future innovations in robust ViTs or effective attacks on ViTs;\n\nThe weaknesses of the paper are below:\n- As this work is more of an investigation paper, in which the authors conduct experiments under different settings in order to provide a new understanding regarding the robustness of ViTs and CNNs, the experiment settings in terms of ViT model types and attack types should be further enlarged to deliver more convincing and useful insights to the community; For example, the authors consider only the vanilla transformer models while more recently advanced ViT models (such as Swin, LeViT, and mobile-former) are not considered; the attack types can be extended to include Auto-Attack (Croce & Hein, 2020), CW attacks (Carlini & Wagner,2017).\n\n- There is a limited new technical contribution, e.g., the paper would be strengthened if the authors provide useful applications driven by their experiment observations/conclusions. Specifically, it would be more insightful if the paper attempts to leverage their investigation observations to develop new techniques.",
            "summary_of_the_review": "This paper studies the robustness of ViTs and CNNs from a new perspective, i.e.,  patch-wise perturbations, under different settings. It can be more useful and insightful to the community if the investigation settings can be further improved to be more comprehensive, or the drawn observations are leveraged to develop new robust/attack methods.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work conduct the study on the robustness of ViT to patch-wise perturbations and find that ViTs are more robust to naturally corrupted patches than CNNs, and this is caused by the self-attention mechanism. However, the attention mechanism is found to be easily fooled to focus more on the adversarially perturbed patches and cause a mistake.",
            "main_review": "Strengths:\n\n> + The experiments settings are clear and make sense. e.g., using the same training recipe for ResNet and ViT to build the fair base models.\n> + The observation on DeiT has lower fool-rate than ResNet in natural corruptions but higher fool-rate in adversarial attack is interesting and  relatively new.\n> + The paper logic is clear, e.g., the sensitivity to the patch positions echo the claim \"the architecture bias of ResNet where pixels in the center can affect more neurons than the ones in corners... each patch within ViT can equally interact with other patches regardless of its position\".\n\nWeaknesses:\n> + Limited novelty: No new and dedicate attack or defense method are proposed for ViT, so I feel it more like a technical report.\n> + Inconsistent visualization implementation: Some visualization designed for Transformer cannot be applied to ResNet, e.g., rollout attention in Figure 4. However, if ResNet uses a different visualization method (e.g., average of feature maps along the channel dimension is visualized as a mask on the original image), I think it would be better to add the same one to ViT to be fair otherwise we cannot make sure the visualization implementation is the same for ViT and ResNet.\n> + Only limited types of ViT and CNN models are included: Including some CNN-VIT hybrid models [1] can make it stronger to show whether such hybrid model can be better or worse than both CNN and ViT.\n\n[1] Xiao, Tete, et al. \"Early convolutions help transformers see better.\" arXiv preprint arXiv:2106.14881 (2021).\n\n",
            "summary_of_the_review": "The limited novelty (i.e., it is more like a technical report regarding the robustness of ViT and CNN) is the my main concern.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper investigates the vulnerability of ViT model under patch based adversarial attacks and corruptions. The paper compares one type of CNN based models: ResNet50, Res18 with one type of ViT variant: DeiT-small eDit-Tiny. The paper find ViTs are more vulnerable under adversarial patches, but they are more robust to naturally corrupted patches. The paper shows analysis on patch perturbation variants and attention visualization.",
            "main_review": "Strengthens:\n1. The paper is clear written.\n2. The experiment results in Table 3 suggest the proposed conclusion.\n\nWeakness:\n1. The model variants are limited. The paper only compares with DeiT,  however, there are the standard ViT[1], ViT trained with regularization[2], and other ViT variants such as the STOA twin-transformer[3], and the newly proposed MLPMixer is missing. Thus the experiment is too limited to support the conclusion.\n\n2. The paper should consider to use the finding to improve the robustness of the ViT model. Only test on patch corruptions is not that interesting.\n\n[1] An image is worth 16x16 words: Transformers for image recognition at scale\n[2] How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers\n[3] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
            "summary_of_the_review": "Overall the paper's experiments are limited and not ready for publish.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies Vision Transformers' (ViTs') robustness against patch-wise perturbations and finds that ViTs are more robust to naturally corrupted patches while they are more vulnerable to adversarial patches. It further analyzes this phenomenon via the lens of the self-attention mechanism and provides more ablation studies about both ViTs' and CNNs' sensitivities against patch positions and patch alignment of adversarial patch attacks.",
            "main_review": "**Strength:**\n1. This paper is among the early works to discuss ViTs' robustness against adversarial perturbations at the granularity of patches, which can be a complementary part for the community to understand ViTs' robustness as compared to CNNs.\n2. This paper is well written with a clear logical flow and the visualizations of the self-attention mechanism are impressive. \n\n**Weakness:**\n1. The major concern for this work is the unsold experiment setting. According to Sec. 4.2, all the reported patch-wise adversarial attacks are based on 100 randomly selected images from ImageNet-1k, which is even 10 times smaller than the total number of classes. According to [1], the robustness gap between different classes can considerably large, thus the overall robustness on ImageNet can hardly be covered by such a small set of 100 images. Therefore, the reported robust accuracy may suffer from a large variance compared with the one measured on the whole ImageNet-1k validation set.\n\n2. The patch-wise adversarial attack can be viewed as clustered sparse attacks[2] under $L_0$-norm constraints. A natural question is whether ViTs are more vulnerable against common $L_0$-norm based attacks than CNNs, which helps explain the importance of patch-wise perturbation. \n\n3. Only ResNets and DeiTs are considered in the evaluation, while the robustness of other CNN-ViT hybrid designs like LeViT[3] and CvT[4] are missing. Discussing the robustness of these hybrid models against patch-wise perturbations can significantly help understand the reason between the robustness differences of CNNs and ViTs.\n\n4. The connection between the conclusions made by this paper and the ones from previous works is not well discussed and analyzed. According to [5][6][7][8], ViTs are found to be more robust than CNNs against existing adversarial attacks. It's not clear what's the boundary or key difference between these commonly adopted perturbations and patch-wise perturbations, which lead to a different robustness ranking here. Deeper analysis is helpful in addition to the observations from self-attention visualizations.\n\n5. The insights of this work for guiding the future designs of ViT-customized adversarial attack and defense methods are not well discussed. Considering (1) the current patch-wise attacks are perceptually notable, and (2) the adversarial patches are manually selected, it's not clear how to further develop the observations to a practical attack method, which may be further integrated to the adversarial training process.\n\n\n[1] Analysis and Applications of Class-wise Robustness in Adversarial Training, Q. Tian et al., arXiv'21.\n\n[2] Sparsefool: A Few Pixels Make a Big Difference, A. Modas et al., CVPR'19.\n\n[3] LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference, B. Graham et al., arXiv'21.\n\n[4] CvT: Introducing Convolutions to Vision Transformers, H. Wu et al., arXiv'21.\n\n[5] Understanding Robustness of Transformers for Image Classification, S. Bhojanapalli et al., ICCV'21.\n\n[6] On the Adversarial Robustness of Vision Transformers, R. Shao et al., arXiv'21.\n\n[7] Adversarial Robustness Comparison of Vision Transformer and MLP-Mixer to CNNs, P. Benz et al., arXiv'21.\n\n[8] Reveal of Vision Transformers Robustness against Adversarial Attacks, A. Aldahdooh et al., arXiv'21.",
            "summary_of_the_review": "Based on the above weakness, my major concerns are the lack of solid experiments and deeper analysis about the identified phenomenon. I tend to reject this paper now while I'm willing to adjust my score if the above concerns are properly addressed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}