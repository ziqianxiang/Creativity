{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "Introduces a new model-agnostic image attribution method: optimize a mask in the wavelet domain to keep the same model prediction (predicted probability) while being as sparse as possible. The wavelet-sparse image reconstruction, termed cartoon, is the final explanation. Qualitative comparisons to other attribution methods and some preliminary ablation experiments (in the Appendix) are provided.",
            "main_review": "Finding explanations in a different representational space is an intuitive idea carried properly here using wavelets. Similar ideas have been explored in feature visualization, e.g., using generative models or restricting the image space to low-frequency images, but the approach is novel for explanations and justified nicely based on distortion theory.\n\nPaper is easy to read and written cleanly. I appreciate the short but complete background on RDE and the full description of the method. Perhaps one or two sentences with the intuition on why a sparse representation in wavelet space produces cartoon-like images could be added. It should also be stated that the method works only for differentiable models (and for that matter, differentiable transformations f and discrepancy measures); just adding the word differentiable once somewhere in the intro might be enough.\n\nThe method description does feel long at times given the simplicity of the method and similarity to pixel RDE. Namely, in Sec 4.1, the introduction of channels makes notation convoluted and might confuse readers, it would be enough to say it is a differentiable data representation function f(h): R^k-> R^n; the restriction of each R^c level having the same mask value seems arbitrary and could just be stated as an implementation detail (Sec 5.1). Also, figure 2 with the illustration of the advantages of a change of basis seems out of place, it is not a necessarily counter-intuitive, insightful or foreign concept for researchers in ML or an exclusive advantage of this method—for instance, LIME with superpixels could be considered a change of basis; perhaps it is worth moving it to the appendix and bumping another result from appendix (Fig 7 or Fig. 12 perhaps?) to the main paper or moving it forward in the main paper to Sec 3 where it might fit better.\n\nCurrently, however, the main limitation of the paper  is a lack of comparison to attribution methods that tend to produce smooth explanations, e.g., occlusion methods, LIME/SHAP with superpixels, GradCAM and L2X with attention maps (I do not know if the last one has been shown to work in natural images), among others that I might be unaware of. Comparisons to at least some of these might be a requirement to put these results in context.\n\nMinor comments: \n- Sec 1, last lines: empty spaces. Latex doesn't like wrapfigures near page breaks or section titles. You can move it up or down on the text and latex will comply. p.\n- Sec 4.1.2, p. 2: all partitions are disjoint, no need to say disjoint partitions; plus the last sentence in p.2, sec 4.12 is confusing: you are not computing the mean for each partition but for each subset/member/part of the partition. You could have just said \"we compute empirical mean ... for each member A_i of the partition.”\n- Sec 4.1.2, p. 2: I do not see how “distribution V_s chosen as Gaussian adaptive noise depends on s”. The mask s is unused to get the partition or compute the mean/variances.\n- Fig 4b is hard to see in print. I wonder whether inverting the colors might help.\n- perhaps in appendix you could add how do you initialize the masks.\n\nQuestion:\n- Do you have any intuition on why using a zero baseline for the obfuscation does not produce meaningful representations? what do the reconstructed images look like? will a non-adaptive Gaussian (mean and std computed across coefficients fro  all orientations and scales) also fail?",
            "summary_of_the_review": "This paper introduces a nice idea and is written properly. The method will be part of the arsenal, along with other attribution methods, when trying to explain black-box differentiable models. Moving away from pixel space in search of more human-understandable explanations will also certainly produce further research. However, comparison to other, more appropriate, attribution methods is needed to evaluate its results properly.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "\nThe paper focus on finding which input features were important \nfor the model predictions. It extends previous work that used the rate-distortion \nframework with a discrete wavelet transform.\nInstead of distorting input pixels, the distortion is done in the wavelet space.\nThe advantage of the wavelet space is that a sparsity penalty can be added \nwhich yields piece-wise smooth images.\nThe authors claim that the \"piece-wise smooth explanations are more\ninterpretable than jittery pixel-spare explanations\".\n",
            "main_review": "\nThe problem of finding which input was important to a deep neural network remains a hard\nproblem. The rate-distortion framework is a promissing alternative to\ngradient-based attribution methods. Also, changing the basis from input \npixels to something more high-level (wavelets / concepts / etc. ) seems reasonable. \nHowever, I have concerns about the rigor of the evaluation and the quality of the\npresentation. The figures are too large. Almost any figure could \nbe equally well be presented in half the space). While the figure size might \nbe a rather minor issue, it affected the space available to the evaluation\nsection. All experiments\nfrom the evaluation sections (5.2 Experiments and Analyzsis) refer to \nthe appendix. For example:\n\n> We discuss the sensitivity of CartoonX to these hyperparameters in Appendix A.3. In Appendix A.5, we also shed light on the evolution of ImageNet classifier [...]\n\nThe current presentation makes it hard for a reader to get an overview of the evaluation.\nThe descriptions in the main text are to short to judge if the evaluations were\nexecuted correctly and the descriptions in the Appendix are too scattered.\nA simple fix would be to shrink the figures and move the Appendix A3-A8 to the\nmain paper. \nI also want to mention that the call for papers clearly states that \"Note\nthat reviewers are encouraged, but not required to review supplementary material\nduring the review process\". \n\nBesides this major presentation issue, I also want to critize the paper for claims \nnot sufficently backed by experiments. In the following, I want to \nprovide details on two claims. The first is:\n\n> \"Surprisingly, we find that our method is particularly well-equipped to explain misclassifications, often showing “what the neural network actually saw”\n\nThe evidence presented is anecdotical. Figure 1 shows two examples that seem\nlike they support this finding. However, we do not know what the model has\nlearned? While it seems reasonable that the attribution highlights the chair's\ntexture, is that really what the model has learned or just our human\npreconception? \nAs this is claimed as a major contribution, I would like to see a \nquantitative evaluation, for example, on a synthetic\ndataset which allows to test the model's behavior, e.g. the BAM dataset\n(https://arxiv.org/abs/1907.09701).\n\nThe second claim is:\n\n> We demonstrate that our piece-wise smooth explanations are more interpretable than jittery pixel-sparse explanations and that they can reveal relevant piece-wise smooth patterns that are not easily visible with existing pixel-based methods\n\nEspecially, claims about interpretability should be best backed by a human\nsubject evaluation. \n\nFinally, I have many unanswered question about using discrete wavelets transformation: which bias is \nadded by using DWT? How well is it aligned to the data distribution? And what would\nhappen if it is not applied to natural images? \nPrevious work attributed to internal network features such as TCAV.\nHow does the DWF basis compare to using the network's own features as an explanation basis? \n\nAs a minor issue, I would suggest that to reduce the paper's file size. \n42MB is too large. A simple command can be found here: https://tex.stackexchange.com/questions/18987/how-to-make-the-pdfs-produced-by-pdflatex-smaller \n",
            "summary_of_the_review": "I argue to reject the paper. Both the paper's presentation and the rigor of the\nexperiments are not sufficient. \nAdditionally, quality of the evaluation makes it hard to judge the technical and empirical significance.\nThe technical idea of using DWT is relatively straighforward and only contains some novelty.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper generalizes the framework of Rate Distortion Explanations (RDE) to representations systems other than the canonical basis. In a nutshell, RDE searches for a sparse mask in the representation space that occludes as much as possible while minimizes some notion of distortion of the resulting classifier. The resulting explanations, since they are sparse in the wavelet domain, are smooth and \"cartoon\" like. Qualitative comparisons are included for correct and incorrect cases of image classification.",
            "main_review": "The contribution of this paper is clear, natural, interesting and insightful. The paper is very well written and easy to follow. \n\nMy main concerns are the following:\n\n- While the obfuscation strategy seems to work well in practice, I have difficulty understanding if this is correct. The authors comment that other works propose similar masking with GANs, producing samples that remain in the data manifold, but choose not to employ this strategy because \"it is hard to tell whether a GAN mask did not select coefficients because they are unimportant or because the GAN can easily inpaint them from a biased context\". Could the authors clarify?\nMore importantly, the issue I have with the chosen strategy is not whether the samples are \"on the data manifold\", but rather on the distribution that the model was trained on. If the model is given sample a sample x', that is close to an original input x (say, a masked version of it) but outside of the distribution used to train the model, then f(x') can be very inappropriate to infer anything about f(x). Indeed, deep models are particularly brittle in these settings (as adversarial examples demonstrate).\nThe obfuscation strategy employed by the authors alleviates this issue this by introducing *some* distributional assumption about the pixels to input, which seems to alleviate this problem in practice while still not producing samples from the data distribution. A clearer explanation of this aspects, and its limitation, would be useful to the reader.\n\n- The motivation behind the extensions of RDE are clear, and the results of the new method also align well with the intuition behind explanations in a wavelet domain. However, the comparison between Cartoon and Pixel RDE seems only perceptual and subjective. Given these two \"explanations\", which one is more correct? More precisely, given the higher efficiency of wavelet systems to encode information, I would have expected this to reflect in some kind of efficiency in terms of explanations. Two ways of doing this come to mind: \n(i) for the same level of sparsity (in original and wavelet domain), which approach leads to the minimal distortion of the classifier? In fact, and because of this reason, I would have expected CartoonX to be faster (and not comparable, or even slower) than Pixel RDE, because  a higher level of sparsity could have been employed (as indeed explained in Appendix A.2). \n(ii) Presenting the impact on the logit units as a function of the number of obfuscated pixels, as done in the original RDE paper. Does CartoonX provide a faster decay/higher efficiency? \nIt is my opinion that these quantitative comparisons are important.\n\n- In the definition of the obfuscation strategy, the authors define j disjoint partitions A_i, over {1,...,k}. Defined in this way, and because one could have j \\neq k, my understanding is these can include more than one pixel. Should j = k?\n\n- This reviewer really likes the comparison with image compression in Fig 3. In fact, maybe this could be termed \"Supervised Image Compression\".\n\n- This reviewer finds the comment that \"CartoonX is less susceptible to explanation artifacts\" a bit hand-wavy. Naturally, the explanations provided by CartoonX will be smooth and have less edges, because of working in the wavelet domain. Conversely, the explanations of Pixel RDE will be \"less blurry\". This reviewer is not sure which one is truly preferable - personally, I would prefer the one with fastest rate of decay of distortion of the classifier.\n\n- The presentation of some failure cases is a useful part of the contribution. Can the authors elaborate more them? i) what approximate proportion of cases are failures? ii) do the authors have any clues or intuition as to why these happen?\n\nMinor comments:\n- page 2, last paragraph: \"marks\"->masks\n- While it is very clear why this method is termed 'CartoonX', the authors might consider adding a short comment in the introduction about this, in the lines of cartoon images being a specific class of 2D signals that has been well studied, and for which wavelet provides a 'good' representation system. This will be useful for readers not familiar with harmonic analysis. Along this line, I'm curious as to why the authors choose to use a wavelet system instead of an -optimal- shearlets representation. \n- All references authored by Stephane Mallat, unlike all other references, are reversed (that is, authored by \"Mallat Stephane\").",
            "summary_of_the_review": "This paper presents a nice and novel idea. I would have liked to see a more quantitative perspective supporting the claims that these results provide better explanations.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work builds on the recent RDE (A Rate-Distortion Framework for Explaining Neural Network Decisions, Macdonald et al, 2019) framework by doing analyzing distortions in the Wavelet domain. The authors give a (mostly) clear explanation of their work and provide lots of qualitative examples, as well as their reasoning about the interpretation of their explanations. This work does not provide any quantitative comparisons with other approaches. ",
            "main_review": "In the first two sections the authors give an overview of the literature on \"network explanations\". Then the authors then describe the RDE framework. It is straightforward and clear. Then they give their own reformulation, which moves the masking to the components of a signal decomposed on to any basis. In the fifth section, the authors describe their explanation approach. They claim to develop a cartoon-like explanation image from an (image, network) pair that explains the networks predictions on the image. To construct their explanation, the obfuscation is computed in the wavelet domain, inverted, clipped, and then the RDE step (l1 penalized update to the mask entries, with clipping) is applied for a fixed number of steps. They use Daubechies wavelets for their explanations. The work showcases many qualitative comparisons with other methods, and the authors provide reasoning about the usefulness of the explanations and also give failure cases.\n\nI have a few questions about some of the choices made here. A big choice is the use of the wavelet basis, namely Daubechies. It seems to me that if you really wanted \"cartoonish\" images you would want a *piecewise-constant* explanation---Daubechies is more about support (which makes them efficient in JPEG, for example) than piecewise-anything as far as I'm aware. However, the Haar wavelets do provide an efficient way to represent piecewise-constant functions (that's how they're defined basically). So then I would expect Haar to be the more natural choice. Moving on, I would argue that since the inverse wavelet operation has to be applied here, the \"intuitive\" advantages of other explanation methods (like SmoothGrad or even RDE) are lost as the effects of the inverse transform are more complicated than backprop or output distortion to this community. The explanation images contain a lot of background pixels. One of the consistent qualitative metrics used by this community is \"coherence\" --- active explanation regions should reside on common objects, especially those concerned with the prediction. These examples do not show this property. The analysis of the \\lambda parameter also shows that even by varying the sparsity it is not easy to achieve coherence with this approach. I think that looking more closely at why this is, and providing some level of quantitative evaluation (either by A-B testing or by the rate rate-distortion experiments) would help this paper significantly. \n",
            "summary_of_the_review": "This paper does seem to be the first to do network explanations in the wavelet domain, which seems promising. However, it is not clear that the resulting explanation are piecewise-smooth in any way. In fact, most examples contain a high degree of blurred background. This work also fails to incorporate any quantiative evaluation. It is worth noting up front that this area does not seem to have a universal quantitative evaluation philosophy, though older works such as (Ribeiro, 2016) have done some modification of A-B testing for quantitative evaluation and some recent works such as (Macdonald, 2019) and (O' Shaughnessy, 2020) provide quantitative evaluation looking at the information rate of the explainer's selections. In general the area does seem to be moving towards quantitative methods. Due to the lack of quantitative evaluation and unconvincing qualitative results, I think this paper should be rejected.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}