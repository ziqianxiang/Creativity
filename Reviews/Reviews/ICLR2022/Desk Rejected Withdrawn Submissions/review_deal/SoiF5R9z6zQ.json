{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper describes a method for augmenting sparse LiDAR point clouds with dense point clouds obtained using depth completion algorithms, for the purpose of 3D object detection. The LiDAR data is used as a supervision for depth completion. The paper also describes a method for sparse LiDAR data and dense predicted point cloud fusion, and a novel point feature extractor - the CPFE (Color Point Feature Extractor), for processing the fused point cloud. The paper presents compelling 3D detection results and an extensive ablation study.",
            "main_review": "As mentioned above, the paper presents several novel components, namely, the LiDAR and predicted point cloud fusion via 3D-GAF (3D Grid-wise Attentive Fusion), and the CPFE - Color Point Feature Extractor, intended for processing the merged point clouds. The paper presents very compelling results improving over the state of the art almost in all cases.\n\nThe main weaknesses of the paper are with respect to unclear or incomplete algorithm description, as detailed below.\n1. In the attentive fusion description, the paper states that \"For grids where raw clouds are sparse, pseudo features should be enhanced. For grids where pseudo clouds are inaccurate, pseudo features should be weakened\". Specify which criteria are used to decide whether the raw point cloud is sparse, and whether the pseudo point cloud is inaccurate. Specifically for the latter, is the accuracy measure also learned? What happens when the pseudo point cloud accuracy estimation is unreliable? Also, it is necessary to specify exactly how the enhancement and weakening are performed.\n2. In Section 3.3, 3D GRID-WISE ATTENTIVE FUSION, please explain why n - the total number of grids in a 3D RoI, is equal to 6 × 6 × 6.\n3. In the Color Point Convolution description, the paper mentions \"neighbor search\" for a pseudo point. However, from the description in the rest of this section, it seems to me that no search is performed. Rather, I understand that each pseudo point aggregates information from a predefined set of neighbors. Otherwise it won’t be able to perform the search and aggregation in constant time. If my understanding is correct - please correct the description so that it does not mention “search\", it is misleading. Also clarify how the K = 9 neighbors of a point are chosen.  Otherwise, specify which criteria are used for the neighbor search.\n4. In the Color Point Convolution description, \"Position Residuals\" paragraph: How is p_i defined? is the last element of h_i^k not equal to the norm of its first 5 elements? Why this redundancy? Also, which semantic information do 2D residuals carry that is not already encoded by 3D residuals, such that the method needs both types of residuals? \n5. In the Color Point Convolution description, the \"Feature aggregation\" paragraph - why fully connected layers are applied to position residuals and not to point features f_i?\n6. In the same paragraph, feature concatenation is not permutation invariant - please comment on how this affects the algorithm results.\n7. In the \"Multi-Level Feature Fusion\" paragraph, the paper mentions multi-level feature concatenation - how this multi-level features are defined and obtained? Add a clear definition for this in the text.\n8. In section 3.6, define all loss functions explicitly, for completeness of presentation.\n9. Similarly, provide precise definition of all the parts of the original network components proposed in the paper (not the existing methods used for depth completion and 3D detection).\n10. In section 4.1: the premise of the paper is that some ground truth depth is provided, to train the depth completion network. However, it is hard to obtain such GT depth information in real work scenarios and at scale. How can the proposed method be extended for case when there is no GT depth provided?\n11. In section 4.2, list all the data augmentation approaches used in the experiments, for completeness of presentation.\n12. For visual comparison with previous methods, in Figure 3 and Figure 9, and for the comparison in Table 2, only Voxel-RCNN was used. Please explain why results of other methods, specifically - the SFD by Deng et al. (2020), were not shown in the comparison.\n\nSome places where additional small clarifications are needed.\n1. Page 1, \"Nevertheless, with more data, more annotations and more time, current multi-modal methods perform less accurately than LiDAR-only methods\" - please support this statement with a reference to a prior work where the latter was shown.\n2. In \"Figure 2: Comparison between raw LiDAR point clouds and pseudo point clouds.\", (a) and (b) - it could be helpful to specify the distance between the vehicles shown in the images and the ego.\n\nTypos and small comments.\n1. Page 2, \"pseudo points on objects are much more than raw points in all distance ranges\" => \"there are many more pseudo points than raw points on objects at all distance ranges\".\n2. In Figure 3, for axes shows in green for the results of the proposed method, make them green also in the Voxel-RCCN result visualization, for clarity. In the same figure, the blue arrows are light blue or cyan, not indigo.\n\nMissing references:\n- \"Is Pseudo-Lidar needed for Monocular 3D Object detection?\" Park et al., ICCV 2021.\n- \"SPG: Unsupervised Domain Adaptation for 3D Object Detection via Semantic Point Generation,\" Xu et al., ICCV 2021.\n- \"Real-Time Anchor-Free Single-Stage 3D Detection with IoU-Awareness,\" Ge et al., Arxiv 2107.14342, 2021.\n- \"Sparse Auxiliary Networks for Unified Monocular Depth Prediction and Completion,\" Guizilini et al., CVPR 2021.",
            "summary_of_the_review": "The paper presents a novel method for 3D object detection and shows compelling detection results. However, the algorithm description is unclear at many places - to improve the paper further, some algorithmic choices need to be described in more detail, or more clearly justified, per comments above. Without these changes, it is hard to follow the algorithm description, and impossible to reproduce it. Therefore I recommend \"marginally below the acceptance threshold\".",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents an algorithm to better fuse the lidar point cloud with the synchronized rgb image. The main idea is to generate the dense pseudo point cloud from the rgb image with an off-the-shelf depth completion before doing the fusion. The method is validated on Kitti detection challenge where it shows noticeable improvement. ",
            "main_review": "+ Well-written paper with illustrative pipeline diagrams \n+ Thorough ablative analysis of each proposed component.\n+ The proposed method outperforms existing baselines by a noticeable amount.\n\n- My concern is the dependency on the depth completion method. The papers says they can swap the algorithm from Hu et al. 2021 to TWISE and get comparable results. I 'd like to see in depth quantitative and qualitative analysis on this: how about using older depth completion methods? How does the final accuracy is influenced by the depth completion? Any failure cases?\n-When does this method fail?",
            "summary_of_the_review": "The proposed algorithm seems to be a very reasonable to tackle the domain gap between lidar and rgb sensors. It is also written and has state of art performance on KITIT. As long as the authors provide justified analysis on the dependent of depth completion algorithm, the paper should be solid in my view. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposed an attention mechanism to fuse lidar and rgb inputs through a sparse-to-dense mechanism for the rgb+lidar to create pseudo-lidar. This enables point cloud based augementation which help generalization. The results demonstrate a marginal improvement the state of the art for multi-modal 3d object detection. ",
            "main_review": "The authors tackle a relevant problem in multi modal fusion and show improvement over state of the art. \nStrengths:\n- Clear description of contributions and experiments to support the claim.\n- 3D-GAF and CPFE modules are novel.  \n\nWeaknesses\n- Heavy reliance on depth labels which may or may not be available. The authors do not addres training sparse to dense without depth labels which is a common in practice as dense depth labels in outdoor environments is very hard to get. \n- Over reliance on Kitti and especially the car class. Demonstrating on multi modal benchmarks beyond Kitti is important to validate on another benchmark such as TOR4D. \n- Incomplete metrics such as mAP in table 1. \n- No mention of runtime and computational complexity. in the main paper.  Ideallyt ta\nble 1  should have an additional column  discussing the runtime for each method\n -Abalation studies  on the  grid size  and  additional hyperparameters in the 3DGAF and CPFE modeules  will be revealing about its inner workings. Additional visualizations to support that the attention mechanism is behaving as intended. \n\n\nI would also like to underdstand if the authors intend to share code as  a lot of  details  can be made clear with author implemented code release",
            "summary_of_the_review": "Although I like the paper and the approach, the experiments are lacking. The authors need to discuss the computational complexity of the approach and need to clearly demonstrate the benefit from a performance+runtime perspective. The fact that the method relies so heavily on depth labels is also a negative. If not, the authors should discuss the perfomance on Waymo benchmark. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Considering the sparse nature of Lidar point, this paper proposes to enhance sparse point clouds with dense pseudo points using a learned depth completion network. The model uses a standard Lidar-only RPN to propose objects and then combine features from pseudo points and real points within the proposal boxes to predict a score and box refinement. Two new modules are proposed to effectively extract 3D geometric and 2D semantic features from pseudo and real lidar points.  The overall pipeline is efficient and achieves state-of-the-art results on KITTI.  The overall experimental evaluations are comprehensive but there is a concern about data leaking between depth completion training set and 3D detection validation set that needs to be addressed during the rebuttal. \n",
            "main_review": "I went through the method and experimental sections in detail. Here is a summary of strength and weakness\n\nS1 The idea of using depth completion to improve Lidar-based 3D detection is novel. As the authors pointed out, while previous works like pseudo lidar families [Wang et al. 2019 You et al. 2019], also use the idea of 2D to 3D lifting to generate dense pseudo points, they rely on noisy monocular/stereo depth estimation and have considerably lower accuracy compared to Lidar-based methods. This paper, on the other hand, manages to improve state-of-the-art Lidar-based 3D detectors using image-based depth completion. The results demonstrate that pseudo points generated from high-resolution camera sensor inputs are useful for 3D detection even in the presence of high-end Lidar. \n\nS2 The proposed Color Point Feature Extractor (CPFE) is novel and effective. In order to extract features from both 3D and 2D efficiently, the authors propose to perform a neighbor search (implemented efficiently using a HashMap)  to extract 3D geometric features and 2D semantic features simultaneously. \n\nS3 The authors present the ideas really well. The ablation studies for the effectiveness of those two key modules (Color Point Feature Extractor and 3D-GAF) are solid. The choice of using pseudo points inside RoI regions also prevents a large amount of potential false-positive pseudo points. The authors do a good job showing the readers the challenges they overcome to get the simple depth completion for 3D detection idea work.  \n\nW1 My main concern is that there is one key issue with the experimental section. As mentioned in [1], the depth completion benchmark (which the author used for the depth completion network training) has overlap with the KITTI 3D detection validation set. As stated in [1] Section 4.1, more than 32.5% of images in the KITTI validation set are already seen in depth completion training. This gives an unfair advantage to the proposed method due to the data leaking. The authors need to rerun a certain set of core experiments (e.g. Table 3 and Table 4) with the depth completion network trained on the dataset that excludes images in the validation and testing sets.  \n\nW2 Another ablation study that is currently missing is to compare the proposed CPFE module with a more standard 2D / 3D feature fusion that uses a separate image-based network (preferably pretrained on other larger datasets) to extract image features and combine them with Lidar features through pointwise concatenation [2].  While I understand that CPFE is probably faster, the alternative approach may better utilize image semantic features through a pretrained 2D backbone. \n\nW3 In my understanding, the proposed synchronized augmentation is not novel. A lot of previous work already uses it by default (e.g. PointPainting [3]) to do multimodal data augmentation. I don’t think the author should claim this as a novel contribution. \n\nW4 In Section 4.1, the author mentioned that they experimented with the KITTI dataset as the other dataset doesn’t provide depth completion labels. In my understanding, the KITTI dataset also infers this label from temporal sparse Lidar points. It is better to add a discussion about how this proposed method can generalize beyond the KITTI  dataset and be used by other researchers. \n\n[1] Simonelli et al. Are we Missing Confidence in Pseudo-LiDAR Methods for Monocular 3D Object Detection? CVPR 2021\n\n[2] Sindagi, Vishwanath A., Yin Zhou, and Oncel Tuzel. \"Mvx-net: Multimodal voxelnet for 3d object detection ICRA 2019 \n\n[3] Vora, Sourabh, et al. \"Pointpainting: Sequential fusion for 3d object detection. CVPR 2020\n",
            "summary_of_the_review": "The proposed method is novel and the presentation is really clear. My biggest concern is the data leaking mentioned in W1 and I will consider adjusting the rating based on the author’s response. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes to fuse estimated pseudo-dense point clouds with raw sparse point clouds for improved 3D vehicle detection. Some contributions claimed by the authors include:\n1. SynAugment for jointly augmenting input raw sparse point clouds & estimated pseudo-dense point clouds;\n2. Color Point Feature Extractor for extracting features from pseudo-dense point clouds & the color images;\n3. 3D Grid-wise Attentive Fusion for fusing features from image & pseudo-dense point clouds & raw sparse point clouds.",
            "main_review": "Strengths\n\nS-1: The benchmark results and ablation studies demonstrate the effectiveness of SFD (to certain extents, see W-5 & Summary Of The Review).\n\nWeakness\n\nW-1: High computation cost. These add-on modules from the pseudo-stream & fusion head significantly reduce the FPS of the baseline Voxel-RCNN from 25.2 FPS to 10.2 FPS. For 3D vehicle detection in driving scenarios, efficiency is also critical.\n\nW-2: Echoing W-1, these add-on modules also increase the overall model sizes compared with the Voxel-RCNN baseline. Therefore, the comparisons with Voxel-RCNN are unfair in terms of model capacities. I would suggest increasing Voxel-RCNN model sizes to match the model sizes of SFD for fair comparisons.\n\nW-3: The RPN only relies on sparse raw point clouds within the LiDAR stream, and thus can still miss objects due to long distances & sparse points. The pseudo point clouds are only useful for ROI feature pooling after 3D box proposals have already been estimated. For example, if an object is missing in the RPN stage due to sparse points, then the current fusion framework can't save it back. In some sense, it violates the original purpose of leveraging dense depth to better capture such challenging detection scenarios.\n\nW-4: Table-1, the reference of Voxel-RCNN is incorrect.\n\nW-5: The benchmark results on KITTI only have the vehicle category. The results on other categories (eg., pedestrian) are not shown.",
            "summary_of_the_review": "Overall speaking, the proposed SFD brings some improvement for 3D vehicle detection. However, considering the greatly increased computation cost, model sizes, incomplete benchmark categories, and moderately improved performances, I'm holding a conservative attitude of accepting this work for ICLR.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}