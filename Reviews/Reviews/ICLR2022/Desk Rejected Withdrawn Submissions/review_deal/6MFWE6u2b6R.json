{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies black-box attacks to graph neural networks. The authors propose to use bandit optimization to obtain the final attack which has been proven to be efficient i.e., it suffers a sub-linear regret.",
            "main_review": "Strengths\n* The is the first work to use bandit optimization to deal with black-box attacks to graph neural networks.\n* The paper is written well and easy to follow.\n* Extensive experiments are conducted to demonstrate the empirical performance of the proposed algorithm.\n\nWeaknesses\n* Although the idea is new and interesting, it seems to me this paper is a natural application of bandit optimization technique to the field of black-box attacks to graph neural networks. The paper first formulate the problem as a combinatorial optimization problem, which is then tackled using bandit optimization technique. It should be noted that the aforementioned technique has already been extensively researched in the research field e.g., [1]. I find it difficult to consider its contribution to be significant when a study is only an application of an established approach to a new sector. It might be better if the authors could help specify the difficulty of such application and how the difficulties are tackled during the process.\n\n[1] Online convex optimization in the bandit setting: gradient descent without a gradient.",
            "summary_of_the_review": "Overall I like the idea proposed in this paper. However it seems to me it is an application of an established technique to a new problem setup which makes me hard to consider its contribution to be significant.\n\nDetailed comments:\n\nEq. (8): $(S^t_{v})^*$ ->  $S^*_{v}$\n\nLemma 5: bound -> bounded",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies the problem of adversarial attack on GNNs in a black-box setup, by perturbing the graph structure. The authors formulate the problem as a bandit optimization problem and propose an online optimization method to obtain the adversarial perturbations. The proposed method is evaluated on both node classification and graph classification tasks.",
            "main_review": "On the positive side, this paper studies an interesting problem of black-box adversarial attack on GNNs by perturbing the graph structure. \n\nHowever, this submission has missed a significant portion of GNN attack literature and, more critically, has major technical flaws.\n\nIn terms of literature, this submission claims that existing black-box attacks to GNNs only \"consider perturbing node features\" and positions itself as the first black-box attack that perturbs the graph structure. In fact, there have been quite many black-box attack methods focusing on the graph structure. See the Table 2 in the review paper by Jin et al.\n\nThe technical flaws I identified are listed below.\n\n(1) The definitions in Eq. (4) and Eq. (5) are conflicting. To see this, by Eq. (5), we can write down\n\n$L(a_v) = \\max \\left(f(a_v)_{y_v}  - \\max_y f(a_v)_y, -\\kappa \\right),$\n\nand\n\n$L(a_v + s_v) = \\max \\left(f(a_v + s_v)_{y_v}  - \\max_y f(a_v + s_v)_y, -\\kappa \\right).$\n\nFirst, in the case both $f(a_v)_{y_v}$ and $f(a_v+s_v)_{y_v}$ are small such that $L(a_v) = L(a_v + s_v) = -\\kappa$, we can immediately $L(a_v) - L(a_v + s_v) = 0$ no matter how large the largest logit is. However, Eq. (4) says $L(a_v) - L(a_v + s_v)$ equals to the largest logit of $f(a_v+s_v)$. \n\nIn cases $L(a_v) > -\\kappa$, it is also easy to see that $L(a_v) - L(a_v + s_v)$ will depend on $f(a_v)$, while Eq. (4) says that it only depends on $f(a_v + s_v)$.\n\n\n(2) Right after Lemma 3, it says the binary solution $s_v^t$ is obtained by sampling from $\\hat{s}_v^t$. However, in the Algorithm 1, $s_v^t$ is obtained by setting the top-B values of $\\hat{s}_v^t$ to 1 and others to 0. This invalidates the condition stated in Lemma 2, which I believe is needed for the derivation of the main result.\n\n(3) I did not take a careful look at Section 4, due to the technical flaws mentioned above. However, with a simple glance it seems all the analysis is restricted to the perturbation of edges on a single node v, which is a strange setup in adversarial attack.\n\n\nMinor typo: the first sentence of Section 2.4, $a_u$ should be $a_v$.\n\nReference\n\nJin et al. Adversarial Attacks and Defenses on Graphs: A Review, A Tool and Empirical Studies.",
            "summary_of_the_review": "This submission has missed a significant portion of GNN attack literature and, more critically, has major technical flaws. I do not think it is ready for publication at its current status. And it does not seem the flaws can be fixed easily during the author response period. Therefore I recommend a reject.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No major ethics concern for this paper.",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper aims to study the structural attack on graphs from the perspective of bandit techniques. The authors first formulate the attack process as an NP-hard binary optimization problem and relax it as a bandit optimization problem. Then a bandit-based attack algorithm is developed with a sublinear regret bound. Experiments on both node and graph classification tasks demonstrate the ability of the proposed algorithm towards a successful attack on GNN models.",
            "main_review": "Strengths:\n- The paper is easy to follow with clear motivation and is well written. The perspective of solving the adversarial attack problem with bandit optimization is interesting.\n- The black-box setting of adversarial attacks is of great importance, especially in real-world applications.\n- The empirical performance with theoretical guarantee has merits.\n\nWeaknesses:\n- The main concern of mine is the black-box setting, which is the most important motivation of this work but without clear justification. Typically, I would expect a \"black box\" attack to depend only on observed outputs of the model, as the setting of RL-S2V, but not the logit score of the label of a given node. As introduced by [1], this belongs to the practical black-box attack setting. Meanwhile, the accessibility of data, such as adjacency, feature matrix, and labels, should also be included in the paragraph of Attacker’s knowledge. The authors should make clearer clarification, perhaps with potential rigorous mathematical definitions on this setting due to its importance.\n- Another concern is whether the proposed method is practical in real-world scenarios. Presumably, having access to the whole matrix X, e.g. all user information on a social network, as well as having the ability to manipulate all nodes in the graph seem to be quite strong assumptions. I am curious about how the proposed method performs under the practical setting in [2].\n- The bandit technique used in this paper seem to be unstable with a relatively large regret. Thus, why the authors choose not to use the better methods from bandit online learning such as [3] should be discussed.\n- For experiments, the concerns include:\n    - Demonstration of the time efficiency of their attack. Their theoretical study is pretty nice, but it would also be helpful to provide a comparison of run time among different attack algorithms for better demonstration.\n    - Since RL-S2V uses less information than the proposed method, the authors are suggested to compare with GeneticAlg, which is also from [1] but seems to be under the consistent setting in this paper, for fairer comparisons.\n    - The maximal number of perturbed edges seems to be set arbitrarily especially on two image graphs. I doubt this would be a conflict with the unnoticeable constraint during the generation of adversarial examples, especially considering the average nodes in two image graphs are considerably small.\n\n[1] Adversarial Attack on Graph Structured Data, ICML 2018\n\n[2] Towards more practical adversarial attacks on graph neural networks, NeurIPS 2020\n\n[3] Optimal Algorithms for Online Convex Optimization with Multi-Point Bandit Feedback, COLT 2010\n",
            "summary_of_the_review": "My concerns are mainly from two aspects:\n- The description of the setting of the black-box attack is vague and needs further clarification. \n- The proposed method seems to be not so practical in real-world scenarios.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The current paper proposes an interesting solution to the binary optimization problem of finding the optimum black-box attacks on graph neural networks. The goal is to find a strategy to change the structure of the graph i.e. remove or add edges to a graph, in order to deteriorate the performance of a GNN learning a node and graph classification task, while observing only the output of the model for each specific node as well as its neighborhood. Moreover, there is a certain budget of edges to change, which acts as a constraint to the suggested optimization problem,  which is a version of knapsack and hence NP-hard. The algorithm proposed solves a continuous relaxation of the problem with a bandit-based method that approximates the gradient. The proposed method enjoys sublinear regret on the number of examples required to query from the GNN and surpasses reinforcement learning and optimization-based approaches in experiments with three well-known GNN models on 5 common datasets.\n",
            "main_review": "\nPros\n*Adversarial techniques on graph neural networks are an important topic with increasing popularity.\n\n*The formulation of the problem based on multi-armed bandits is novel in the context of graph neural networks, and a theoretically grounded approach opens the road for further principled methods on the problem.\n\n* The experiments include a number of datasets and two baselines from recent works.\n\nCons\n\n*The constraint B is satisfied by keeping the top B-edges in line 5 of the algorithm. However, the satisfaction of constraint C is unclear. Can the authors explain this or highlight it? Because it is not easily deducible from the text. \n\n*The authors mention that their method is scalable, however as the method samples random vectors with float values, when the number of nodes becomes large enough as in real social networks i.e. >100k nodes, this vector (which is not sparse) will be 10^10  and will not fit in regular memories.  The authors can address this with text or with an experiment on the limit of the method’s scalability. \n\n* Could the problem be cast as a combinatorial multi armed bandit where the $s_u$ is not relaxed? there is literature on relevant methods that might be promising and more straightforward [1,2]. Can you comment on the superiority of your method compared to these or the hinders on applying such methods?\n\n[1]Chen, Wei, Yajun Wang, and Yang Yuan. \"Combinatorial multi-armed bandit: General framework and applications.\" International Conference on Machine Learning. PMLR, 2013.\n[2]Wen, Z., Kveton, B., Valko, M., & Vaswani, S. (2016). Online influence maximization under independent cascade model with semi-bandit feedback. arXiv preprint arXiv:1605.06593.\n\n\n*To my understanding the attached code addresses only GCN and does not include the image datasets. Moreover, there is no readme ( I assume the main is attack.py as it also contains the baselines) and the reproducibility seems possible, but not easy.\n\n\nMinor comments:\n* The loss inside (4), defined in (4) and defined in (5) should have different names in order to distinguish them clearly. Moreover further intuition on the loss function would be beneficial, and an explanation on why (4) is given though (5) is used.\n\n*Rephrasing on some of the sentences in the first paragraph of page 4 as it is hard to read e.g. “PGD requires that the gradient information needs to be available.” -> “PGD requires the gradient information to be available”\n\n*Lemma 2 and its proof could be added in the appendix or omitted if it is covered in the citations.\n\n*It would be nice to provide the proof of Lemma 3 for one point gradient estimation when |N|>1 except if it is proved in a corresponding citation which should be clarified. \n\n*The step (e) in lemma 4 should be clearly stated that relies on the identity of Var(x) =  E[x^2]-E[x]^2. \n\n*Is an expectation missing on L(u^t) in (20) ?\n",
            "summary_of_the_review": "The paper is quite interesting overall, the proofs are clear and the experiments show a clear superiority over the previous approaches. My main concerns revolve around the scalability of the model and the budget-constrained. I am willing to increase my rating, once the authors clarify the concerns raised and answer the comments.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}