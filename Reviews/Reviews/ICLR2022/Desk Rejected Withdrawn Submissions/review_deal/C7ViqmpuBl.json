{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper applies established spectral kernel results to learning the kernel in the generalized formulation of self-attention proposed by Tsai et al. (2019), in an attempt to explore how kernel learning can help improve transformers.\n",
            "main_review": "**Weaknesses:** \n\n- The idea is rather trivial. \n\n- The paper is missing references to nonstationary spectral kernel methods (e.g. [1, 2]). [2] introduced families of spectral kernels that are dense in the family of all continuous bounded kernels (stationary and nonstationary), including families that have the stationary spectral kernels used in this paper as a special case. Much of the work in this paper could have been applied to these more general families. I would encourage the authors to discuss these spectral kernels in relation to their endeavour.\n\n**Strengths:** The paper reads well and, except for the omission noted above, the literature review is thorough. Experimental results seem encouraging and support that kernel learning may indeed be helpful in some long context tasks.\n\n\n[1] Samo, Y.L.K. and Roberts, S., 2015. Generalized spectral kernels. arXiv preprint arXiv:1506.02236.\n\n[2] Samo, Y.L.K., 2017. Advances in kernel methods: towards general-purpose and scalable models (Doctoral dissertation, University of Oxford).\n\n[3] Remes, S., Heinonen, M. and Kaski, S., 2017. Non-stationary spectral kernels. arXiv preprint arXiv:1705.08736.",
            "summary_of_the_review": "Although the idea isn't ground-breaking, the paper in its totality is a worthy addition to the literature in my opinion.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors proposed kernelized transformer to improve the time and space complexity from quadratic to linear in terms of sequence length. The proposed method applied finite approximation of Gaussian kernels to achieve complexity reduction. The authors proposed several variations of approximation and analyzed the complexity accordingly. The experiment results show the efficiency of the proposed method.",
            "main_review": "Pros: Overall the paper is  easy to follow. Th authors proposed to utilize several different linearization method to approximate the feature map of Gaussian kernel. Analysis of time and space complexity are provided. Sufficient experiments are conducted.\n\nCons:\n1. The quadratic problem of the original transformer is caused by computing the $L \\times L$ pairwise similarity score. The author proposed to achieve linear time complexity by approximating real valued kernel in Eq. 1 as $\\kappa(x1, x2)\\approx \\langle \\phi(x1),\\phi(x2)\\rangle$. It’s unclear why such approximation can significantly reduce the complexity of original self-attention work. By using linear kernel (dot product between two vectors), it’s naturally the same as the author proposed. \n\n2. The authors have emphasized the proposed method can significantly reduce the time complexity, it would be more convincing and obvious if the authors provide similar figures like Figure 3 Memory consumption vs. Acc.\n\n3. There are some works about directly approximate the Gram matrix to linearize the complexity.\nAs the authors have studied random Fourier features as the finite approximation of Gaussian kernel, it’s natural to think of another popular approximation method: Nystrom approximation.  \n _Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nyströmformer: A nyström-based algorithm for approximating self-attention_  \nThe author should discuss about such existing works and provide some analysis and comparison.\n",
            "summary_of_the_review": "Overall, the justification for the proposed method is a bit unclear, and some existing works need to be discussed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "\nIn this paper, the authors employ random feature map kernel learning for the approximation of self-attention matrix in Transformers. ",
            "main_review": "\n\nStrengths.\n\n1.  Random feature map learning in Transformers may have potential applications. \n\n2. The paper is well written and well organized. \n\n\n\nweakness\n\n1. Random feature map learning is not novel. The technical contribution of this paper seems to be marginal.  \n\n2.  The feature map learning increases time cost compared with Performer. However, a comparison of the time cost is missing in the experiments to justify the benefit of the feature map learning. \n\n3. In Figure 1, it seems that this paper incurs almost two times memory cost to obtain less than 5% average accuracy compared with Performer.  I am not sure of the significance of the benefit. ",
            "summary_of_the_review": "Overall, I think the technical contribution of this paper is marginal.   The empirical benefit is not significant.   So I think it is marginally below the acceptance threshold.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper incorporates kernel learning in a spectral sense into linear transformers. Several random features based algorithms including equal-weight GMM, Fastfood, GAN for RKS and PRF. The experimental results demonstrate the effectiveness of the applied random features based algorithms for efficient Transformers.",
            "main_review": "Pros:\nThis paper is well-written and easy to follow. The studied algorithm is quite comprehensive and the experiments demonstrate the superiority.\n\nCons:\nThe main issue of this paper is its novelty. The essential issue in linear transformer by random features is how to construct random features to approximate the softmax kernel, to achieve better approximation quality or prediction performance. \n\nIn theory, I agree with the author that “kernel learning has never been explored within the framework of Transformers”. However, the authors use off-the-shelf random features based kernel learning techniques for efficient Transformers.  There is merely insights and technical contributions in perspective of kernel learning, at least.\n\nTheorem 1 follows with the same framework with Perez et al. (2019), with the extra store of the current position in the head. Lemma S.2 in fact can be easily obtained by Bochner’s theorem: E[exp(w’z)]=exp(-||z||^2/2) for w ~ N(0,I_d).\n\nIn experiment, according to Table 2, if we consider the kernel learning algorithm by random features, e.g., GMM-RKS, GMM-PRF, GAN-PRF, there is no significant improvement on test accuracy when compared to Performer, except for the Text dataset. \n\n",
            "summary_of_the_review": "In my view, the contribution of this paper is relatively weak and the improvement in experiments is also relatively limited on most datasets. Such two issues make me infirmatory on this paper. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}