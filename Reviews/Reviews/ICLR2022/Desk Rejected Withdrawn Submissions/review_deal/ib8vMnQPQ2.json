{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a PIM-aware quantization training approach accompanied with various optimizations to facilitate the training convergence under quantized setting and mitigate the inherent circuit non-idealities during inference.",
            "main_review": "The idea of quantization (whether in the context of PIM or digital implementation) is an interesting and active line of research. Many of the ideas presented in this paper seem to be working for the limited set of models studied in this work (generality / scalability of the proposed methods still need to be evaluated). Finally, I commend the authors developing a real-chip and evaluate their methods under real scenarios. However, in my opinion, that should not be considered as a novelty or contributions for this work. The non-idealities and circuit noise can be modeled and tested (under various corner cases) without developing an actual chip.\n\nThese are some of the questions about this work that would help me to better understand the approach and evaluate the contributions:\n\n(1) While evaluating large deep neural networks (for example transformer models) may not be possible on actual chip due to the hardware limitations, but the efficacy of the proposed approach can be evaluated on these models to better understand the benefits and limitations of your technique. Would you please provide the model accuracy + estimated performance for such large models?\n\n(2) Overall, the proposed GSTE method makes sense and seems to be effective for the limited set of models/dataset evaluated in this paper. Maybe I am missing something here, but I couldn't grasp the overall benefit of GSTE. Are there any ablation studies showing the benefits of GSTE?\n\n(3) In Table 3, in my opinion, the baselines for comparison are not fair and well-justified. There are various quantization-aware training that generally preserve accuracy under deep quantization setting.\n- [Ultra-Low Precision 4-bit Training of Deep Neural Networks](https://papers.nips.cc/paper/2020/file/13b919438259814cd5be8cb45877d577-Paper.pdf?ltclid=)\n- [WRPN: Wide Reduced-Precision Networks](https://arxiv.org/pdf/1709.01134.pdf)",
            "summary_of_the_review": "While the ideas presented in this paper are interesting, however more studies and evaluations are required to better assess the generality of the proposed methods in this work and its applicability to large class of neural networks.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a method for training quantized networks to incorporate PIM quantization for quantization and non-ideal effects in ADC. \nThe straight-through estimator is customized for PIM quantization-aware training algorithm. \nBased on the theoretical analysis on PIM quantization aware training algorithm, a rescaling technique is introduced to facilitate training convergence. \n\nAn empirical batch normalization calibration and adjusted precision training are proposed to suppress the adverse effects of non-ideal linearity and stochastic thermal noise involved in real PIM chips. \nExperimental results show the proposed method can achieve comparable inference accuracy on PIM systems, compared with some baseline methods. ",
            "main_review": "-- strength\n- The proposed approach is interesting and makes sense. Regarding as the real quantization problem in PIM systems, the authors customize the straight-through estimator for PIM quantization aware training algorithm for PIM systems. Based on the theoretical analysis on PIM quantization-aware training algorithm, the authors introduce the rescaling technique to facilitate training convergence. The experimental results verify this benefit. \n- The proposed method can achieve comparable inference accuracy on PIM systems, compared with some baseline methods.  \n\n\n\n-- Weakness \n- According to the reviewer's understanding, a rescaling technique is introduced to facilitate training convergence, based on the theoretical analysis on PIM quantization aware training algorithm. While batch normalization calibration and adjusted precision training are proposed based on empirical analysis. There is a gap among the training dynamics based on the customized straight-through estimator, the batch normalization calibration and adjusted precision training. In other words, it is mutual independent among training dynamics, batch normalization calibration and adjusted precision training. Besides, it is unclear how to perform batch normalization calibration and adjusted precision training. The authors do not provide an overflow for the proposed method for training quantized networks.\n- The authors claim they show real chip results. However, they also claim they evaluate the proposed method using physical models of a state-of-the-art SRAM PIM chip prototype and experimentally confirmed the identical MAC and inference results of the model and a real physical chip. All descriptions about real physical chip are missing, including technology nodes and MAC size. \n- In Table 4, the authors report efficiency (TOPS/W). However, the authors do not mention any details about how to obtain this performance. Besides, they impose stochastic thermal noise as a Gaussian distribution. The reviewer is not sure whether this assumption is reasonable since real thermal noise is more complex and high related to environment and PIM layout. \n\n",
            "summary_of_the_review": "This paper seems to propose three independent techniques to handle three issues, respectively, so that comparable inference accuracy can be achieved on PIM systems. \nFirst is a rescaling technique is proposed to facilitate training convergence.\nSecond is a batch normalization calibration is proposed for the non-ideal effect in ADC. \nThe last is a adjusted precision training is proposed for noise injection and imperfect linearity. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper propose a quantization-aware training algorithm by considering the injected noise during the PIM execution. To mitigate the variance explosion problem caused by the noise during the forward propagation stage of the DNN, the author introduce a scaling factor during the backpropagation stage of the DNN training.  Additionally, the paper adopts a calibration algorithm to mitigate the errors in the BN statistics. Finally, the paper studied the possibility of employing difference precisions for training and inference, given the fact that the non-idealities only impact the least significant bits during the involved quantization mapping. Experiments have shown that the proposed training approach can achieves superior accuracy performance compared against the baseline algorithms.",
            "main_review": "Strengths\n+ The authors consider a practical issue from the perspective of DNN hardware implementation. The problem is well-defined and well-motivated.\n+ The solution is presented clearly. The paper is easy to understand.\n+ Real hardware performance is measured.\n\nWeakness\n- The paper lacks explanation of some key noise components in PIM. For example, the major noise source such as thermal noise, shot noise, random telegraph noise are not explained in the paper (although the authors mention the thermal thermal noise briefly). These noises play an important role in PIM and there are some well-developed mathematical models to estimate these noises (e.g., [a1]).\n- The paper lacks some justification on why the nonlinearity caused by the imperfection can be modeled by a simple scale factor \\rho (equation 5d). \n- Some prior works (e.g., [a1][a2]) have been conducted on PIM aware quantization. Authors should clarify their contribution by comparing against these works.\n- Some important datasets and DNNs are missing. The paper should include more challenging datasets (e.g., ImageNet) and more popular DNN architectures (e.g., MobileNet, VGG, Transformer).\n- The hardware evaluation results are superficial. This is because different training approaches will only impact the DNN weight values and the validation accuracy, but not the hardware performance of the DNN execution. That's why the energy efficiency are the same across different datasets and DNN depths.\n\n[a1]: He, Zhezhi, et al. \"Noise injection adaption: End-to-end ReRAM crossbar non-ideal effect adaption for neural network mapping.\" Proceedings of the 56th Annual Design Automation Conference 2019. 2019.\n\n[a2]: Long, Yun, et al. \"Q-pim: A genetic algorithm based flexible dnn quantization method and application to processing-in-memory platform.\" 2020 57th ACM/IEEE Design Automation Conference (DAC). IEEE, 2020.",
            "summary_of_the_review": "Overall, I think the paper ignores too much details of PIM. I understand that ICLR is a ML conference, but this paper is on PIM, more details should be revealed to the reader to better understand the paper. \n\nMoreover, I think this paper may better suit the audience of the hardware conference like DAC, given the fact that multiple similar works have been published there.\n\nFinally, Theorem 1 is actually an assumption, it should not be called a theorem.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}