{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes Mixture of Neural Temporal Point Processes (NTPP-MIX) which utilize the existing NTPPs for event sequence clustering. Similar to Xu & Zha (2017), the prior distribution of coefficients for cluster assignment is modeled by a Dirichlet distribution and the conditional probability of a sequence is modeled by the mixture of series of NTPPs. The authors introduce a variational EM algorithm coupling with Stochastic Gradient Descent (SGD) to train the model. Lastly, to further improve its capability, the authors also propose a fully data-driven NTPP based on the attention mechanism named Fully Attentive Temporal Point Process (FATPP). ",
            "main_review": "First, the paper is well organized and clearly presented. The main contribution is the combination of NTPP and the event sequence clustering framework proposed by Xu & Zha (2017), where the intractable posterior distribution is approximated by a variational distribution p(Z, pi). However, the proposed framework is more like a \"linear combination\" of NTPP, clustering framework, and a standard variational learning strategy. Besides, the contribution of the proposed FATPP is also incremental comparing to other attention-based point process model. ",
            "summary_of_the_review": "In short, this paper is well-written, however, there is a lack of sufficient novelty for this topic. Unfortunately, I suggest that the paper is marginally below the acceptance threshold.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper contains two main contributions:\n1. An approach for clustering event sequences by learning a mixture of neural temporal point process (NTPP) models. \n\n    The clustering approach follows the lines of the Bayesian treatment of GMM in Section 10.2 of ([Bishop, 2006](https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/)). Specifically, the cluster-conditional density for each cluster $z \\in \\\\{1, ..., K\\\\}$ is modeled with a separate NTPP as $p(\\\\{t_1, ..., t_N\\\\} | z) = p(\\\\{t_1, ..., t_N\\\\} | \\theta_{z})$. Then, the posterior over the cluster assignments and parameters of the different NTPP models are jointly optimized by maximizing the ELBO.\n2. A new NTPP model, where the conditional intensity is defined fully via self-attention. \n\n    Most existing NTPP models encode the event history $\\mathcal{H}\\_{t\\_i}$ into a single vector $\\mathbf{h}\\_i$. In contrast, the proposed approach defines the intensity for mark $u$ at time $t$ as a function of **all** past events $\\mathcal{H}\\_{t\\_i}$ using an attention mechanism.",
            "main_review": "## Strengths\n- The proposed approach is theoretically sound and elegant.\n- Paper is well-structured, clearly-written, and the main ideas are easy to understand. The description of the experimental setup is very detailed, which should make the results easy to reproduce.\n\n\n## Weaknesses\n### 1. Efficiency of the proposed model\n\nAs far as I understand, the proposed model is computationally much more expensive than existing transformer-based NTPP approaches. This aspect is not discussed in the paper and isn't analyzed in the experiments, even though it can be important in practice. \nTo explain this, let's look at how the conditional density $p(t_i | \\mathcal{H}\\_{t\\_i}) = \\lambda^*(t_i) - \\int\\_{t\\_{i-1}}^{t\\_i} \\lambda^*(t) dt$ is evaluated for existing transformer-based NTPPs and the proposed model.\n\n- AMDN ([Karishma et al., 2021](https://arxiv.org/abs/2008.11308)):\n    - we need to perform a **single forward pass** of the transformer to embed the history $\\mathcal{H}_{t_i}$ into a vector $\\mathbf{h}\\_i$\n    - we evaluate $p(t_i | \\mathbf{h}\\_i)$ in closed form\n- In transformer Hawkes process (THP) ([Zuo et al., 2020](https://arxiv.org/abs/2002.09291)) \n    - we need to perform a **single forward pass** of the transformer to embed the history $\\mathcal{H}_{t_i}$ into a vector $\\mathbf{h}_i$\n    - we evaluate the integral $\\int_{t_{i-1}}^{t_i} \\lambda^*(t) dt$ with Monte Carlo integration, which is more expensive than the previous approach\n- In FATPP (this work)\n    - we need to perform **multiple forward passes** of the transformer to evaluate the conditional intensity $\\lambda^*(t)$ at points randomly sampled in the interval $(t_{i-1}, t_{i})$ to approximate the integral $\\int_{t_{i-1}}^{t_i} \\lambda^*(t) dt$. Of course, these can be done in parallel. However, even a small number of MC samples is used (e.g., 20), this increases the memory footprint of the model by a factor of 20.\n\nMoreover, sampling cannot be done exactly in the FATPP model. In AMDN, we can sample the next inter-event time analytically. In THP we can use thinning since the conditional hazard decays over time (so an upper bound on the hazard is easy to obtain), which still leads to exact sampling. In contrast, FATPP would require inverse transform sampling with multiple forward passes and 2 approximations: (1) approximating the compensator with Monte Carlo and (2) inverting the compensator with numerical root-finding to obtain a sample.\n\nAt the very least, the above limitations should be mentioned in the paper. It would be even better to demonstrate the runtime / memory-consumption plots for different models on different dataset sizes. \n\nPlease let me know if I misunderstood any aspects of the proposed model in my description above and should update my assessment.\n\n### 2. Choice of baselines during experimental evaluation\n\nThe main motivation for the proposed FATPP model is its flexible parametrization of the intensity function. However, experiments in Section 5.1 do not compare to existing flexible NTPP models, such as FullyNN ([Omi et al., 2019](https://arxiv.org/abs/1905.09690)), LogNormMix ([Shchur et al., 2020](https://arxiv.org/abs/1909.12127)), or the transformer-based AMDN model ([Karishma et al., 2021](https://arxiv.org/abs/2008.11308)). The implementation of all these 3 models is publically available, so the comparison should be rather straightforward.\n\n## Minor comments\n- Another simple baseline for sequence clustering with NTPPs was proposed in Section 5.5 of ([Shchur et al., 2020](https://arxiv.org/abs/1909.12127)). The learned sequence embeddings could be clustered with K-means to assign sequences to clusters. This approach could be added to Section 5.2.\n- Equation 2: $\\int_0^T$ instead of $\\int_0^{t_L}$ since we're talking about a TPP on $[0, T)$.\n- Figure 3 is rather hard to read because of the juxtaposition of all intensity curves. It might be helpful to provide plots in the appendix, where the intensity is plotted separately for each model.\n- Table 3: please elaborate the meaning of \"crash\" in \"complex time intervals ... make RMTPP-MIX crash\".",
            "summary_of_the_review": "The paper addresses a relevant problem. The proposed approach is simple (which is an advantage), and the presentation is very thorough and clear. My main two concerns with this work are: (1) lack of discussion of the limitations of the proposed model, especially related to efficiency, and (2) missing baselines in the event sequence modelling experiment.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The submission proposed a Mixture of Neural Temporal Point Processes (NTPP-MIX) framework that can utilize existing NTPPs for event sequence clustering, which is trained by variational EM and stochastic gradient descent. Besides, the paper proposed a NTPP model (Fully Attentive Temporal Point Process) based on the attention mechanism. ",
            "main_review": "The derivation of variational EM algorithm seems solid and the construction of FATPP is rational. The major concern of the paper is the choice of hyperparameters and the experimental validation:\n\n1. How to choose the hyperparameter $\\epsilon$ in experiments that has a crucial impact on the cluster estimation? In appendix, the authors only provide the value of $\\epsilon$ for each model without providing a rule for it.  For example, in an imbalanced dataset, e.g., 100 sequences in one cluster and only 2 sequences in the other one, the $\\alpha_1$ will be significantly larger than $\\alpha_2$, and it is prone to remove the second cluster according to the rule of updating $K$ in the paper, which results in an incorrect clustering result. If the ground-truth number of clusters is known in advance, the appropriate $\\epsilon$ can be found easily, but how to specify it for the ground-truth-unknown real data?\n\n2. If my understanding is right, the reported LL in Table 1 is training LL. However, a larger training log-likelihood is not enough to indicate FATPP contributes to the success of NTPP-MIX because FATPP can be overfitting as it is very flexible. For example, a overfitting backbone NTPP can learn quite different parameters from two sets of similar sequence data in one cluster (Bias-Variance), and then the NTPP-MIX framework will tend to divide them into two different clusters because the two sets of learned parameters are quite different, which adverse to the clustering task. \n\n3. If my understanding is right, the reported LL in Table 3 is training LL. The training LL is not an appropriate metric for clustering in real data. A larger training likelihood can be obtained if the backbone NTPP is more flexible and the mixture framework is prone to output more clusters, e.g., the likelihood will achieve the maximum when $K=N$ (each sequence belongs to an individual cluster), but this cannot demonstrate the framework provides the correct clustering structure in the real data. \n\n4. Missing baselines: Neural-HP [Mei & Eisner,2016] in Table 1; Neural-HP-Mix and THP-Mix in Table 2,3,4. \n\nBesides, the update of $q(\\pi)$ should be E-step not M-step.  ",
            "summary_of_the_review": "The theoretical part in the paper seems solid, e.g., the derivation of inference algorithm and the construction of FATPP. However, the strategy for updating hyperparameters is not satisfied, e.g., the rule of updating $\\epsilon$ and $K$; besides, the metric in experiments is inappropriate to support the main claim of the paper. ",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a general framework (NTPP-MIX) for clustering event sequences, which can simultaneously infer NTPP and cluster assignments. The authors also propose a more flexible intensity function (FATPP) based on an attention mechanism. The learning algorithm is similar to that of variational inference for mixture models, which is general in that one can use any MLE-based TPP models. The effectiveness of the proposed method is demonstrated using synthetic and real-world datasets. ",
            "main_review": "Strong points.\n- The task addressed here may be significant; there are a few studies on event sequence clustering.\n- This paper is well-written, except for some undefined variables and typos.\n- There is sufficient explanation to reproduce the experimental results.\n\nWeak points.\n- Learning algorithm based on variational inference seems not to have technical contributions.\n- There are some concerns about intensity modeling.\n- I am wondering if the claim about the flexibility of FATPP is valid or not.\n\nDetailed comments.\n- The idea of NTTP-MIX is a mixture model, as in (4), which is very simple,  and its learning algorithm based on variational inference is very naive. So, this framework is incremental. Also, Section 3.2 describes the general procedure of variational inference for mixture models, which is not novel except for using NTPP as the likelihood. If I am mistaken, could the authors tell me about the novelty and significance of the proposed algorithm?\n\n- In NTPP, it is generally difficult to integrate the intensity function involved in the likelihood (2). How does FATPP handle it? If FATPP requires numerical integration, the authors should discuss its computational costs, and so on. \n\n- In the manuscript, the authors state that basic NTPP modules need to be flexible enough to capture diverse processes. I agree with that point; however, is it true that the prior works do not have enough flexibility. The intensity-free models (Shchur et al., 2020; Omi et al., 2019) are flexible; is it possible to use them as modules? If that is the case, what is the advantage of FATPP? Besides flexibility, can the authors point out any other advantage of the proposed method in event sequence clustering? \n\n- In addition, in Table 1, I am not sure if it is appropriate to compare the likelihoods for the training data because the intensity function can be made more flexible by using larger neural nets. \n\nMinor comments.\n- The authors should define the variables: $L$ and $t_L$ in Section 2.1; $K$ in Section3.1.\n- Equation (6) is evidence lower bound (ELBO); the authors would mention it.\n- In line 3 of Algorithm 1, epcoh should be epoch.\n",
            "summary_of_the_review": "This paper addresses the interesting problem and is well-written; however, the method is somewhat incremental, and the authors' claim is concerning (see detailed comments). Accordingly, I tend to reject it.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}