{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This study investigates the problem of hyper-parameters tuning in Federated Learning (FL). The proposed approach to overcome the use to optimally tune the hyperparameters at the client level based on the sharing of a hyperparameters reference table pre-computed on a pre-defined dataset. Hyper-parameters are eventually refined through Bayesian optimization to accommodate to the specifics of the local datasets.\n\nExperimental results are shown on standard benchmarks, where the proposed scheme is compared with respect to global optimization and hand-tuning, as well as to local adaptation techniques based on random and grid search.\n",
            "main_review": "The main idea of accounting for heterogeneity in FL through hyper-parameter adaptation at the client level is interesting, and has some connections with recent literature work on FL optimization (see comment below). Nevertheless, I feel that there are a number of  critical aspects of this work, which should be addressed and clarified: \n\nRelationship with the state of the art. The impact on FL of clients performing different amount of local work has been thoroughly investigated in Wang et al (https://arxiv.org/abs/2007.07481). In that work, the authors show that a sufficient condition for FedAvg to converge to a stationary point of the federated loss function is that clients should perform identical amount of SGD steps with same learning rate.  While that study was not cited in this work, the proposed framework seems to find justification in the problem arising from the same number of epochs executed at each client site (section 3.1), while the use of the same number of SGD steps does not seems to be contemplated (while being currently among the reference approaches to unbiased aggregation). In this sense, the study seems to miss an important comparison and benchmark. \n\nMoreover, the optimization scheme here proposed leads to a biased final model as clients perform heterogeneous amount of local work and learning rate (Wang et al).\n\nGeneralization. One of my main concerns is that the approach assumes the existence of a reliable proxy dataset that can be used for hyper-parameters tuning. This is to my opinion an overly simplifying assumption, which is hardly met in most of real-world applications of FL. While a proof of concept can be easily demonstrated on public benchmark datasets, such as FEMIST or Cifar10, this is far from practical in any sensitive application of FL with rare, complex, or even unique datasets.\n\nAssessment. Training and testing sets do not have the same data distribution by construction. For example, the testing dataset with FEMNIST is made with the data of clients that are not participating to the learning process instead of allocating for every client a fixed percentage of samples to training and the rest to testing. Indeed, the federated loss is data dependent, which makes the training and testing loss function different in the setting considered in this work. Therefore, all the plots should be with the training loss and not the testing accuracy. Indeed, every optimization guarantees are given with respect to the training data.\n\nOther remarks. For Figure 1, the protocol used to hand tune the learning rate should have been provided instead of citing a reference. The final learning rate for H-Oblivious should also had been given. Indeed, the current learning rate might not be good enough. Lastly, what is the number of epochs run by each client to perform their local work?\n\nLastly, client data heterogeneity can also come from its features distribution and not only from its label distribution as assumed in this work with HI(c). Lastly, FedTune requires additional communication from the clients. They have to send to the server their HI which for some applications may be a sensitive information.\n\n",
            "summary_of_the_review": "The overal idea is interesting and addresses a relevant problem. However, the proposed scheme seems to not properly account for current knowledge and advances in FL optimization, while being not practical in real world applications.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper explores and discusses the challenges faced by hyperparameter tuning in the federated learning setting, and motivates customized hyperparameter tuning for every client. The paper introduces a hyperparameter tuning method for for federated learning, in which the central server performs the task of hyperparameter tuning for different types of client heterogeneity and sends a results table of optimal hyperparameters to the clients, who can then choose its own optimal hyperparameters based on its own degree of heterogeneity.",
            "main_review": "Strengths of the paper:\nThe paper studies an important problem in federated learning, i.e., hyperparameter tuning. It is commendable that the paper has designed some experiments to motivate the need for customized hyperparameter tuning in federated learning. The algorithm designs are also clearly motivated, including the use of synthetic dataset, sending the entire table of optimal hyperparameters for different degrees of heterogeneity, etc.\n\nWeaknesses of the paper:\n- One major problem with the paper is that it did not acknowledge the existing works on hyperparameter tuning for federated learning, and mistakenly claims that this is the first attempt at hyperparameter tuning for FL. Specifically, the following papers are related works alone this line:\n[a] Federated Bayesian Optimization via Thompson Sampling, NeurIPS 2020.\n[b] Federated Hyperparameter Tuning: Challenges, Baselines, and Connections to Weight-Sharing, 2021.\n[c] Robust Federated Learning Through Representation Matching and Adaptive Hyper-parameters, 2019.\n\nTherefore, some of the claims in the paper needs to be softened. For example, the claim that \"To the best of our knowledge, there is no customized hyperparameter optimization work addressing the data heterogeneity and privacy issues, which are the key properties of FL\" needs to be adjusted, because the paper [a] above has indeed designed hyperparameter tuning methods for federated learning while considering the issues of data heterogeneity and privacy. Moreover, comparisons with these previous works should be done, either through empirical comparisons or qualitative discussions. In addition, there are also a few concurrent works on hyperparameter tuning for FL:\n[d] Evaluation of Hyperparameter-Optimization Approaches in an Industrial Federated Learning System, 2021.\n[e] Differentially Private Federated Bayesian Optimization with Distributed Exploration, NeurIPS 2021.\n\n- I'm not fully convinced by the choice of the proposed algorithm to let the central server perform all the tasks of hyperparameter tuning. Firstly, one of the original motivation for the FL setting is the growing computational power of edge devices, which now allows them to perform expensive computations such as training deep neural networks. In this sense, why not simply let every client perform its own hyperparameter tuning task since they have large computational power? Moreover, I think if the central server simply carries out the hyperparameter tuning tasks without knowing the degree of data heterogeneity of the clients, some of the hyper tuning tasks (i.e., some entries in the HRT) can be wasted (i.e., not used by any client).\n- (more of a suggestion than weakness) If I understand correctly, one independent hyperparameter tuning task is done (by the central server) for every combination of data quantity and HI index in the HRT, as a result, the algorithm isn't really taking advantage of the \"collaborative\" nature of FL. That is, there is no collaboration or knowledge transfer among clients during the hyperparameter tuning algorithm. Perhaps you can consider using some transfer learning or meta-learning algorithm to speed up the hyperparameter tuning process, for example, you can transfer information across different hyperparameter tuning tasks corresponding to different entries in the HRT, to speed up the tuning process of the server. You can refer to reference [a] above for such ideas.\n\n- (minor) Second last line of Section 2.1: \"complimentary\" should be \"complementary\"\n",
            "summary_of_the_review": "The main concerns I have include (1) absence of discussion of and comparison with related works on hyperparameter tuning for FL which resulted in some over-claiming, and (2) the choice of letting the central server carry out all hyperparameter tuning tasks (rather than delegating the tasks to the clients) isn't very convincing within the FL setting.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents \"FedTune\", a technique for performing client-specific hyperparameter tuning. The proposed technique builds a lookup table based on two client-specific metrics, \"data distribution heterogeneity\" (measured by Heterogeneity Index, HI), and \"data quantity heterogeneity\" (measured by sample size). The lookup table is transmitted server --> client and clients use the best set of hyperparameters learned using a proxy dataset. Experimental results on four datasets show competitive performance with hand-tuning client-level techniques and improvements over random search, with considerably fewer exploration steps.\n",
            "main_review": "In general, the work feels like a promising, albeit incomplete, step in a useful direction for FL. At times I found the writing to be unclear; for example, the paper makes repeated reference to \"privacy restrictions\" in FL and uses them as a motivation for the proposed approach, without ever formally characterizing these restrictions (are they restrictions on data sharing, or more specific restrictions such as DP?). The degree of client-level tuning seems quite minimal and is restricted to HI (a measure of the heterogeneous distribution of labels *only*) and sample size. I also have concerns about the \"tuning\" method really relies on a proxy dataset, effectively performing client-level optimization offline and hoping that the two, very limited client metrics (HI and sample size) fully measure similarity between proxy samples and client datasets. The discussion of experimental results is too limited for a paper whose main contribution is empirical.",
            "summary_of_the_review": "### Major Comments\n\n* A major concern with this work is the use of only HI and client sample size to tailor client-level hyperparameters. HI strictly measures the distribution of class labels at each client, and thus captures nothing about the feature distribution or the conditional distribution p(y|x). As a result, it's hard to imagine that this fully characterizes the factors that influence hyperparameter performance on the client level.\n\n* Some important experimental details are unclear; for example, I don't see where it is specified how it is determined which \"cell\" a client is assigned to in the hyperparameter table, and which parameters are tuned. Additionally, no details are given on the model/algorithm being tuned in the main text.\n\n* The authors' claims regarding learning rates are unclear in 3.2.2. For example, they claim that \"the higher the heterogeneity, the noisier the training process...which is why we observe that an increase in data heterogeneity requires higher learning rates\" -- in fact, I would expect the opposite, due to the fact that large learning rates increase the risk of overfitting to noise.\n\n* I found the writing in several places to be confusing; collectively this adds up to some arguments that are difficult to understand and follow. For example, the paragraph beginning with \"Such pattern...\" on P6 seems to suggest both \"it is challenging to utilize the patterns of hyperparameters for estimating optimal hyperparameter values\" but also that we can do so using a simple quadratic curve. Similarly, the \"privacy restrictions\" of FL are never clearly defined, beyond that client data should not leave the device, but the authors' discussion implies stronger, unstated restrictions. Throughout, confusing constructions make it hard to follow the arguments fully; I also feel that far too little space is dedicated to experimental results, which should be the main result in the paper (since there is no theoretical component).\n\n### Minor Comments\n\n* In 2.1 the authors suggest that prior methods of tuning can cause \"fairness\" issues; however, it is never discussed how the current method alleviates these issues. Please clarify. It seems that assuming a fixed (HI, n_train) tuple \"works\" for all clients could equally cause fairness issues (the hyperparameters may be better for some clients than others, even within a cell). This seems particularly likely when a single, public proxy dataset is used.\n\n* It is not exactly clear why versions of hyperband, NAS, etc. couldn't possibly be modified to work in the FL domain (e.g. running a local version of each at each client). Please clarify further why \"privacy constraints\" prevent these methods from being used.\n\n* Please clarify why \"conventional wisdom for mitigating data heterogeneity impact is difficult to be adopted in FL\" (p.4).\n\n* It isn't clear why client-side hyperparameter tuning cannot achieve linear complexity in the number of clients, as the authors suggest in multiple places; indeed, it seems a client-side tuning algorithm (which meets the authors' privacy constraints) would be the main competitor to the proposed approach.\n\n* Please clarify how the \"forgetting factor\" in FL is different from catastrophic forgetting in a federated context, if it is -- and if not, please state this.\n\n* I would suggest to provide and explicit algorithm in place of (or in addition to) Fig. 3.\n\n\n### Typos etc.\n\nThere are many typos in the paper; this is not an exhaustive list.\n\nP1: \n\n* \"privacy preserving across\" --> privacy preserved across\n\n* Despite of the support --> Despite the support\n\n* such data decentralized --> such a data decentralized\n\nP3: \n\n* Stray \"0\" at end of sentence in third paragraph\n\n* \"severs which averages clients\" --> servers which average clients\n\n* 3.1 section title: \"Why ...is so unique\" --> \"Why is...so unique\"\n\nI would recommend renaming \"synthetic\" dataset (which implies it is artifically-constructed data, not real) to \"proxy\" or \"public\" data (which is more in line with existing literature on this topic)\n\nP6\n\n* \"temporally\" --> temporarily (I think)",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the authors experimentally demonstrate that hyperparameter tuning based on individual client’s profile can improve FL performance, compared to the current global hyperparameter tuning model. Motivated by this observation, the authors propose a heterogeneity-aware hyperparameter training algorithm. The gist of the algorithm is to offline training hyperparameters based on data distribution heterogeneity and data quantity. BO is used to train this hyperparameter table and then the table is sent to individual FL clients. Experimental results show good accuracy of the proposed algorithm, compared to hand-tuning results. ",
            "main_review": "The paper identifies one unique property of FL – different clients may benefit from different hyperparameters in training. The idea proposed is simple and effective. It shows good empirical performance. \n\nOne assumption/simplification the paper makes is that optimal hyperparameters of different clients are independent. In other words, regardless of how other clients are distributed, the optimal hyperparameters for one client based on its heterogeneity index and data quantity remain the same. While I understand that this simplification is probably necessary to avoid an explosion of search space, I wonder how valid this is.  Can you elaborate on this or evaluate this? \n\nIn addition, there are many different aggregation schemes in the literature. I wonder what is the impact of aggregation schemes on profile-based hyperparameter training. Could you elaborate on this? \n",
            "summary_of_the_review": "Overall, the paper presents the importance of heterogeneity hyperparameter tuning in FL and proposes a simple yet effective algorithm for it. The evaluation results are promising. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}