{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposed an ensemble method with both cross-layer and cross-ensemble member sharing. In the experiment part, the author demonstrates the effectiveness of their model compared with the state-of-the-art models on both CIFAR and ImageNet dataset.\n",
            "main_review": "Pros:\nThe experiment is very comprehensive: \n(1). compared models are all state-of-the-art models.\n(2). not only did experiments on CIFAR but also did experiments on a larger scale dataset -- ImageNet;\n(3). study the effect of scaling on model performance;\n\nCons:\n\n(1). The idea of the article is not innovative: cross layer parameter sharing is very similar to NPAS[1]. The main idea of this article is more like extending the NPAS to ensemble member parameter sharing.\n\n(2). The writing of the article is very poor and makes people feel very confused:\n\na. In the “defining a single ensemble member” paragraph, the author states that “round robin fashion, as described in the next section”. But there is no description in the next section. I only found a brief introduction in the next paragraph and it is not clear what is the round-robin fashion. \n\nb. Figure 2 does not contain much useful information. This figure doesn’t demonstrate round bin, cross-layer, and cross-ensemble member sharing, template, parameter group. \n\nc. The experimental setup is not clear either. In the CIFAR experiment: \n(i). Why use a different model architecture for Thin Mixture Ensemble? \n(ii). Why are FLOPs different in Table 4? Could you keep FLOPs at the same level and compare the performance?\n(iii). I think “prediction time” is an important metric but this paper doesn't compare it.\n(iv). In the ImageNet part, this paper mentioned: “the results are in Table 4.2”. There is no Table 4.2;\n(v). Why are ensemble members different in Table 5?  I understand that this paper wants to control parameters but # ensemble members is an important hyperparameter, which should be the same among all the models. There are multiple ways of controlling parameter numbers such as change models architectures.\n  \n\n[1]. Neural Parameter Allocation Search\n",
            "summary_of_the_review": "Although this paper did comprehensive experiments to demonstrate the strength of this model, the writing is confusing and the idea is not novel. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose MixtureEnsembles - a parameter-efficient ensemble where the layers of ensemble members are parameterized as linear combinations of shared parameter “templates.” The authors demonstrate that this method is more parameter efficient than standard ensembles, and is often more powerful than standard ensembles or other efficient variants.\n",
            "main_review": "Overall, I believe that the proposed method is promising, and is a strong addition to the growing literature on parameter-efficient ensemble methods. In some sense, mixture ensembles are similar to “implicit” ensembles like Dropout (which also share parameters). However, mixture ensembles are sufficiently novel in that the shared components are 1) deterministic and 2) linear combinations of entire weight matrices, rather than Bernoulli mixtures of single parameters.\n\nI believe that the proposed idea is promising; however, there are several ways in which this paper could be substantially improved. In summary, there are issues of clarity, issues with imprecise/unscientific language, and opportunities for improving the experiments. Overall, I lean towards acceptance, and my score can be improved if the authors address my comments below.\n\n**Clarity**: In general, the details in Section 3 are confusing, especially since the underlying idea of mixture ensembles is relatively simple. I would suggest clearly outlining the mixture ensemble procedure before going into the specific (e.g. assigning templates via round robin). Moreover, there are many details that are not well explained. For example, do layers within a single model use a disjoint set of templates? I think that an algorithm block that describes the “round robin” process would be useful (either in the main text or in the appendix).\n\nMoreover in Section 4.2: “To encourage maximum ensemble diversity for our ImageNet experiments, we assign disjoint templates within the parameter bank greedily.” It is not clear how this differs from the round robin setup. More details in the appendix would be useful.\n\n**Imprecise/unscientific language** The paper uses lots of vague and imprecise terminology. The following could be greatly improved with precise scientific language:\n- Section 3: “In this way, each ensemble member gets a pseudo-independent set of weights.” This is not true at all - the weights are linearly **dependent** by definition! Please be careful with language like “independent” which has precise mathematical meaning.\n- Section 3.2: “Each ensemble member is trained independently with a standard cross-entropy loss.” Same issue.\n- Section 3: “This flexibility [to use more parameters] is a significant advantage over competing methods, such as BatchEnsemble and MIMO.” What do you mean by “flexibility”? Is it “flexible” because performance of the models is less dependent on hyperparameters like width? Be specific.\n- Section 1: “This formulation allows us to *precisely tune* the total number of parameters in the ensemble.” Same issue. What does “precisely tune” mean?\n- Section 4: “This suggests that MixtureEnsembles are also an effective regularizer.” This is vague. MixtureEnsembles are a different class of models than explicit ensembles (there are explicit constraints on the weights of a mixture ensemble), and so claiming that a MixtureEnsemble is a regularized version of an ensemble is not well-justified.\n\n**Missing related work**: The authors claim that “prior works have only addressed sharing within a single network and have not studied sharing in network ensembles.” I would refer the authors to the “PatchEnsembles” work of Cheng et al (2019), which uses a similar parameter-sharing mechanism. (This parameter sharing mechanism is for implicit non-deterministic ensembles; nevertheless it is a relevant work to cite). I would also cite the “HashNet” work of Chen et al (2015), which introduces a parameter sharing mechanism for single nets.\n\n**Additional experiments**: Below are additional experiments that I would like to see in order to improve my score:\n- I am curious to what degree the independent batch norm parameters play a role in the success of mixture ensembles. For example, Frankle et al (2021) show that only training batch norm parameters (and keeping all other network parameters random) is often sufficient to obtain high accuracy. It would be interesting to test mixture ensembles on a small non-batch-norm architecture (like VGG).\n- It would be useful to include experiments on CIFAR10-C in addition to CIFAR-100-C.\n- Scaling experiment (Figure 3, left): it is nearly impossible to draw any conclusions from this figure. I believe the text that “MixtureEnsembles scale down most gracefully” cannot be supported by the evidence that you provide. This figure needs to be substantially improved in the following two ways: 1) it needs more than 3 data points for each method, and 2) you need to be explicit what “scaling gracefully” looks like on this graph (is it related to the slope of each line)?\n\nIn addition, all experiments would benefit from error bars. I do not expect you to supply these error bars during the review period, but I would strongly recommend running multiple trials for the camera ready (if this paper is accepted).\n\n**Other missing details**: Unlike standard ensembles, the networks in mixture ensembles share parameters. Practically, I imagine that this requires lots of GPU memory, since models must be trained simultaneously and cannot be trained sequentially. The authors should comment on this in the paper.\n\n\n**Small notes**:\n- (Very picky) Table 3 adds percent signs after ECE values, but there are no percent signs after accuracy values in Table 1/2.\n- Figure 3 (right): I would use a different set of line colors than in Figure 3 (left), since the two figures are comparing different sets of models. At first glance, I assumed that e.g. orange represented MIMO, as in Fig. 3 left.\n- Section 6: “MixtureEnsembles well across a wide range of parameter size operating points” - should be “MixtureEnsembles **perform** well”?\n\n\n\n\nRefs:\n- Ensemble Model Patching: A Parameter-Efficient Variational Bayesian Neural Network (Chang et al., 2019)\n- Compressing Neural Networks with the Hashing Trick (Chen et al., 2015 ICML)\n- Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs (Frankle et al., ICLR 2021)\n",
            "summary_of_the_review": "The idea behind mixture ensembles is promising, with compelling empirical results. Overall I am leaning towards acceptance, since it is a nice addition to the efficient ensemble literature. However, I would strongly suggest improvements regarding clarity, replacing vague/imprecise language with scientific terminology, and including a few additional experiments.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper generalizes cross-layer parameters sharing techniques to cross-member parameters sharing to create a more efficient ensemble system.",
            "main_review": "1. The novelty of the paper is limited. It seems this paper is just a simple generalization of cross-layer parameters sharing techniques of previous works. The contribution is only in engineering. It does not give any fresh ideas/techniques.\n    \n    1.1 As for the performance, the paper's results are far from the state-of-the-art results on both CIFAR and ImageNet datasets. The relative improvement of the method compared to previous works (MIMO, BatchEnsembles) is small, sometimes it produces no gains (Table 5 on ImageNet).\n\n2. The argument in Sec. 2 RELATED WORK / Parameter Pruning: \"Still, pruning methods cannot leverage parameter sharing\" is not right. SLIMMABLE NEURAL NETWORKS [1] also sharing parameters. \n\n3. In Table.2 why Thin MixtureEnsembles outperforms MixtureEnsembles? It is somehow counterintuitive.\n\n[1] https://arxiv.org/abs/1812.08928",
            "summary_of_the_review": "The novelty of the paper and the improvement in the performance are both limited, so I vote for rejection.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes MixtureEnsembles, an extension of the soft-sharing trick (Savarese & Maire, 2019) to ensemble learning. The whole method is simple and seems to be easy to implement, and can moderately improve ensemble performance in the low-parameter regime.",
            "main_review": "## Strengths\n- The whole method is simple and well-motivated, and the paper is easy to follow.\n- The empirical studies cover a range of scenarios and the ablation studies are insightful.\n\n## Weaknesses & Concerns\n- The major issue of this paper is that the novelty is limited. Basically, it is an extension of the soft-sharing trick (Savarese & Maire, 2019) to ensemble learning. But, the performance gains in diverse settings are marginal (see Table 1 2 3). Though the authors argue that their method saves the storage cost of network parameters, in practical usage, the practitioners concern more about the run-time memory cost (mainly stemming from the hidden feature maps) and the FLOPS. Obviously, the proposed method cannot address these two aspects. So, I hold concerns about the effectiveness and the practical value of the work.\n\n- Deep ensemble is actually empowered by the stochasticity from SGD and random initialization. However, you \"train the ensemble simply by feeding the same input batch to each ensemble member.\" By this, I think you will miss the stochasticity induced by SGD. \n\n- By the way, it seems that you choose to train all ensemble candidates in parallel. Can you train these candidate networks sequentially as in training a Deep Ensemble? Would the sequential training suffer from \"catastrophic forgetting\"?\n\n- I am curious about why the stochastic over the scalar masks \\alpha suffices for inducing function-space diversity. In my opinion, every ensemble candidate is actually using the shared parameters plus low-rank perturbations. This is conceptually similar to the BatchEnsemble (though with various implementations). Now that BatchEnsemble cannot effectively induce diversity, why can the proposed method?\n\n- The current results cannot convince me to choose the proposed method instead of the more flexible naive Deep Ensemble.",
            "summary_of_the_review": "Given the limited novelty and results, I recommend rejecting the submission. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}