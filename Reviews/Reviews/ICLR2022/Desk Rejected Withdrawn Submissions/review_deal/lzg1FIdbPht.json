{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents a generative autoencoder-based model as a protection against adversarial attack.  The generative model is biologically inspired by biological vision, the LGN, and the sparse coding properties of primary visual cortex.  The method is a two staged approach, using a denoiser and category internal models for defense.  The paper presents both qualitative and quantitative results showing that the method has strong overall robustness.\n",
            "main_review": "The paper addresses an important question in adversarial machine learning, specifically, can exploit biological perception (which does not get fooled by adversarial perturbations) in an artificial neural network.  Overall, the paper reads fairly well but some of the sections could be presented in more detail, specifically some of the related work.  Several typos exist e.g. sect 3.1 is misspelled.  \n\nThe paper does a thorough job of comparing against many different types of adversarial attacks and comparisons with other methods.  While the performance is on par with the SOTA, the method itself seems to only have incremental novelty.  The method should be evaluated on at least one other dataset that has the complexities of the real world.\n\nThe main issue is that it does not seem to be a scalable approach.  Having an internal model for each category seems intractable.  Selectivity to certain concepts or categories can be captured by a sparse population of neurons via sparse coding or other sparse inducing opimization.  The selectivity of such neurons naturally emerge via sparsity.  Futhermore, as noted in the conclusion,  \"we adopt a set of simple internal generative models to encode different categories in the dataset. This reflects the sparse coding that is based on the engram cells in Layer 2/3 of the primary visual cortex\".  Using a generative sparse coding approach (Olshausen and Field) would probably fit better than a set of autoencoders.  \n\n",
            "summary_of_the_review": "I think the paper draws upon some interesting concepts in biological perception and utilizes them in an adversarial defense method.  While the paper shows good performance across multiple attacks, the method was only compared to attacks on MNIST.  It is not clear if this would translate to real world images.  Furthermore, the method seems to have difficulty scaling via a large number of classes.  \nSome areas of the paper showed clear biological inspiration, while other areas seemed to ignore important concepts touched on in the paper (like sparsity).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/a",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a brain-inspired defense against adversarial examples consisting of a denoising stage followed by a set of internal models of different input classes. The defense is extensively evaluated on MNIST and outperforms state-of-the-art defenses.",
            "main_review": "Strengths\n1. The experiments on MNIST are comprehensive, comparing against several defenses with many attacks. The proposed method appears to achieve state-of-the-art performance in terms of overall robustness.\n2. The connection between the proposed defense and computations in the brain is interesting and appears novel.\n3. The latent space visualization in Figure 3 is insightful.\n\nWeaknesses\n1. Correspondence with brain: the authors appear to be mapping the LGN to the denoiser and primary visual cortex to the internal models. It would be ideal if the authors could cite evidence that the LGN plays a denoising role in the brain. Furthermore, the primary visual cortex extracts other types of features beyond engram cells (for example, the V1 visual area has neurons sensitive to certain orientations). The authors may want to describe what these other neurons correspond to in the internal models.\n2. Experiments are conducted only on MNIST. It's not clear to what extent the proposed defense relies on the property that all points within each class in MNIST are fairly similar. It would be ideal if experiments were conducted on a more challenging dataset such as CIFAR-10. This would also help argue the biological relevance of this work since most biological visual input may not be MNIST-like. The experiments on MNIST are valuable since they outperform the state-of-the-art against worst case attacks. Nevertheless, previous work (in particular Schott et al.) already demonstrated that existing defenses were not fully robust on MNIST and proposed a better alternative defense (ABS and biABS). The proposed method does not perform significantly better than biABS based on Table 1. Thus, as it stands, the proposed method seems to have limited value.\n3. In Table 1, it would be ideal to also show results for $L_2$ and maybe $L_1$ targeting PGD defenses (i.e. Madry for $L_2$ and $L_1$). Adversarial training can be applied to other perturbation norms than $L_\\infty$.\n4. Prior works have proposed autoencoder-based defenses to adversarial examples. Nevertheless, the specific approach used by authors is novel and achieves good performance.\n\nMinor Comments:\n1. In the abstract, \"deep learning severely suffers from robustness\" might be better phrased as \"deep learning severely suffers from lack of robustness\"?\n2. In the introduction, the authors write that we understand the human visual system better \"with the recent progress in neuroscience\". However, most findings cited by the authors have been known for quite some time (generally at least 5 years).\n",
            "summary_of_the_review": "Overall, the paper shows state-of-the-art robustness on MNIST against general adversarial attacks. Unfortunately, the paper only considers MNIST, and the experimental results do not seem to be much better than the existing state-of-the-art. Thus, I recommend further improving this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work describes and tests a robust classifier consisting of a denoising autoencoder followed by class-specific autoencoders. The class-specific autoencoders are trained on the objective of reconstructing examples pertaining to their designated class or minimizing the output's pixel space l2-norm otherwise. The inference is conducted by comparing the l1 norm of the class-specific autoencoders' outputs. The accuracy of the proposed classifier in the face of diverse adversarial attacks is compared to an adversarially trained classifier (Madry et al, 2017) and to the ABS model (Schott et al., 2019), which uses the reconstruction error of class-specific VAEs as its classification score.",
            "main_review": "Strengths:\n* Defending against adversarial attacks by generative modeling as an attractive research direction. Emulating the robustness of human observers requires a solution of this form rather than overfitting the classifier to particular adversarial attacks as done in adversarial training.\n* The authors considered a diverse array of adversarial attacks.\n\nWeaknesses:\n* It's unclear whether the proposed model improves over the Schott ABS model. The empirical results are very mixed.\n* The accuracy on clean MNIST is low (96%). For MNIST, such an accuracy level can be achieved with no feature discovery at all, merely matching pixel-based templates. This result alone eliminates the proposed model as a serious contender and it casts doubt on whether this approach can scale up to CIFAR-10 and ImageNet.\n* Conceptually, the proposed model does not go beyond the ABS model and seems to be less theoretically motivated. In particular, the ABS model classifies by its ELBO estimates, approximating the generative classification signal, p(image|class). The inference in the proposed model seems arbitrary (why use the norm of the reconstruction and not the norm of the reconstruction error?)\n* The neurobiology papers cited are at best weakly related to the claims made in the text. In particular, there is no evidence of exemplar-specific cells (typical to the primate hippocampal formation) in the primary visual cortex.",
            "summary_of_the_review": "While the research direction is important, this work as it is does not demonstrate evident progress over state of the art. Furthermore, the required advance over the Schott ABS model is not to squeeze a few percentage points in adversarial performance measures: The outstanding challenge is to generalize robust generative classification from toy datasets such as MNIST to complex real-world categories (e.g., ImageNet classes). The proposed approach is not promising in this respect.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed a method against adversarial attacks which consists of a two-stage model following the human brain's visual processing paradigm. Stage 1 is a denoise network to pre-process the input data. Stage 2 has several auto-encoders one for each category to decide which category the input image is. Experimental results show the proposed method has some effect against adversarial attacks. \n",
            "main_review": "strengths: \n1. This paper is well organized \n2. and is interesting by simulating the pipeline in the human brain for visual signal processing against adversarial attacks.\n\nweaknesses: \n1. the proposed method is not realistic. The authors only did experiments on the Mnist dataset which has only 10 classes, thus 10 autoencoders are required at stage 2. What if we have a more complex dataset like Imagenet, how will the authors train the model? What's the cost? Could the proposed method be used in real life? \n2. threat model is not clear. Actually, I'm not sure where is the classifier in this design, what is the attack scenario? where will the attack happen?  Does the attacker know the DNN model? Is the proposed method a white-box attack or a black-box? How did the attacker generate the adversarial samples? \n3. results are not exciting. I don't think the results are good enough, first, the biDIM method seems to achieve the best Minimal Accuracy, but it is only for gray images. Second, the proposed method is an image pre-processing type defence method, why not the authors compare with the same type of defence methods but adversarial training? ",
            "summary_of_the_review": "Overall, the idea of this paper that tries to simulate the pipeline in the human brain for visual signal processing against adversarial attacks is interesting, but the proposed method is not realistic and the results are not good enough.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}