{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper explores a way to parametrize general Riemannian manifolds with neural networks. The authors propose a parametrisation of the Riemannian metric using symmetric matrices, and then also propose to compute manifold operations: Exponential, Log Maps and hence geodesics by solving various versions of the geodesic equation. This paper then evaluates their framework on two problems: embedding of graphs and fitting geodesics. Evaluations (stress distortion for graph embedding and visual inspection for geodesics) are shown comparing with some similar works like Beik-Mohammadi et al. (2021) etc. \n",
            "main_review": "Pros\n\n- At a very high level, the problem tackled in this paper (i.e. to parametrize general Riemannian manifolds and fit them to arbitrary data) seems very challenging and interesting \n\nCons \n\n- The general structure and flow of this paper lacks coherence and clarity. Amongst other things: the related work is reported just before the conclusion. Equation (2) is not explained nor motivated and other main aspects like training procedure and just the lack of a clear exposition of the overall pipeline make it very hard to understand the main contributions of this paper. Figures 4 (what non-trivial geometry is visualized here?) and 5 need more concrete captions to understand what is the significance of the result. \n- Results from Table 1 are not convincing. The stress-distortion for Deep Manifolds seem quite high in comparison to other approaches (Sphere 100 and Tree 40)\n",
            "summary_of_the_review": "Overall I find the main message of this paper to be somewhat confusing and the empirical results do not seem significant. At this point I am voting for not accepting this paper. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a new model for learning a Riemannian metric of a data manifold, under various data observations. The model is tested in graph embedding and geodesic fitting problems.",
            "main_review": "I find the main idea of the paper quite interesting and novel to the best of my knowledge. The proposed model seems to have a lot of freedom to make metric estimation despite the fact that its topology is poor (just the Euclidean one). I also like the idea of outputing an SPD matrix by modifying a neural net and then use it to construct a metric in Euclidean space. The tested problems are important and certainly of interest for the ICLR community. I am still sceptical about some parts and struggle to understand a few points, below are questions, suggestions and critisisms.\n\ni) I do not understand why it is claimed that *all* Riemannian manifolds can be approximated metrically by the proposed model, since it is clearly stated in Theorem 2.3 that it works for *compact* manifolds. That excludes Hadamard manifolds for instance.\n\nii) I understand that the reason for needing compactness assumption is to be able to find a Lipschitz constant for the exponential map (in the proof of A.2. I guess you mean exponential is $C^1$ and not continuous). Is there any estimation of the Lipschitz constant $K$ in terms of the diameter of the compact domain? Can it be exponential for instance?\n\niii) In the part that you compare the required dimensions with competitive approaches, I understand that $A^T A$ metric needs $\\frac{m(m+1)}{2}$ many dimensions and the pullback metric needs $m^2+5m+3$ where $m$ is the dimension of the manifold. In theorem 2.3 you need to double the dimension of the manifold in order your embedding to work, thus you should need $\\frac{2m(2m+1)}{2}=2m^2+m$ which is worse than both previous expressions, what do I miss?\n\niv) I struggle to understand how backpropagation is done in the applications, I see that the loss for graph embeddings is in equation (7) and I can also see that the gradient of the Riemannian logarithm arises from the differentiation of Riemannian distances. Can you also indicate exactly what loss function is used for geodesic fitting and the formulas for backpropagation?\n\nv) I am sceptical about the application of the method for recovering geodesics on manifold of negative curvature. Theorem 2.3 says that we can approximate a compact subset of such a manifold and recover the geodesics there, but in negative curvature geodesics spread so sharply that can change vastly after the spotted compact region. Have you implemented the method in negative curvature? Euclidean space is of $0$ curvature and $SE(2)$ is of non-negative if I am not mistaken.\n\nvi) In the discussion is mentioned potential extension to non-trivial topologies and the main difficulty is said to be computing $\\log$ in different charts. Then why don't just limitate inside the injectivity radius and have only one chart given by the exponential map? (geodesically normal coordinates)",
            "summary_of_the_review": "I am favourably disposed towards this paper since I find the main idea interesting and the techniques novel. However I am not very familiar with the related literature in order to provide a very confident review, also I did not check the math derivations carefully due to time limitations.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper propose a neural network that predicts a Riemannian metric. This is then optimized with an MDS-like loss function to find a metric under which geodesic distances match observations. The work is applied to find embeddings of graphs.",
            "main_review": "The paper propose to learn a smooth neural network that output a Riemannian metric. This then allows for computing geodesics, and hence distances under the predicted metric. These distances can then be used to define loss functions (e.g. MDS-style stress). The result is a data representation alongside a Riemannian metric under which geodesics and distances better reflect the given data. This is, amongst other use cases, used for embedding graphs, which is a notoriously difficult problem.\n\nThis is an interesting contribution. I, in particularly, liked the results on graph embeddings. This is nice.\n\nHowever, I have a lengthy series of issues with the paper, which I provide below (in somewhat random order):\n\n* The name of the method, *Deep Riemannian Manifolds*, is problematic. A long series of papers (some of which are cited) use neural networks to parametrize Riemannian manifolds (Chen et al., 2018a; Arvanitidis et al., 2017, 2019, 2020; Beik-Mohammadi et al, 2021; Hauberg, 2019; Kalatzis et al, 2021). Claiming that this is the first paper to do so, is dishonest and misleading.\n\n* From the Intro, \"In this paper, we overcome these limitations and introduce the first method for learning Riemannian\nmanifolds from geometric information\" seems rather misleading given that the cited methods in the point above do the same. Unless by \"geometric information\" is meant \"distance observations\". In which case, the paper \"Isometric Gaussian Process Latent Variable Model for Dissimilarity Data\", Jørgensen & Hauberg, ICML, does what is being mentioned.\n\n* The paper claims to be the first to use neural networks to learn Riemannian metrics. Beyond the above-listed papers, which do so implicitly, then the paper \"Latent Space Non-Linear Statistics\", Kuhnel et al., 2018, also does so explicitly.\n\n* Propositions 2.1-2.3 are trivial (e.g. prop 2.1 simply says that the composition of smooth functions is smooth). I understand that there is value in listing these results, but presenting them as propositions sends the (highly misleading) signal that there is a contribution associated with the propositions.\n\n* When comparing different metrics (Sec. 2.3) to each other, I do not understand the pull-back metric. If I understand correctly, this requires learning an embedding and then pulling back the metric associated with the embedding map. Obviously this is a much more complex task then directly learning the metric. This seems like a strawman to this reader.\n\n* In Sec. 2.3.1, \"pullback metrics can not be considered valid metrics since they may not be correct\". I assume this refers to the potential lack of positive definiteness? If so, then this is not true for models with stochastic generators.\n\n* Section 3 detail how one can differentiate through the geometric calculations to have end-to-end learning. This is a nice summary of the situation, but it is presented as novel contributions. Similar constructions are used in *theanogeometry* (https://bitbucket.org/stefansommer/theanogeometry/src/master/), *StochMan* (https://github.com/MachineLearningLifeScience/stochman/) and the code released alongside the paper, 'Variational Autoencoders with Riemannian Brownian Motion Priors', Kalatzis et al., ICML 2020. What is being described as a novel contribution in the paper is, hence, standard practice in existing open source tools.\n\n* In the Intro, Chen et al. is cited as being the first to study latent spaces of generative models geometrically. To the best of my knowledge, that was first done by 'Metrics for Probabilistic Geometries', Tosi et al., UAI 2014.\n\n* In Sec. 5, \"Then, we compare against the most recent baseline given in Beik-Mohammadi et al. (2021), which is the only prior work which learns a Riemannian manifold to model the environment.\" This statement seems odd given that this paper is largely an application of prior work on VAEs. What is meant by \"the environment\" here?\n\n* Following up, \"We find that their method is insufficient for more difficult problem settings since it only relies on topological information\" Which topological information is that? As far as I understand, the cited paper just use the pull-back metric (a geometric object) in a standard VAE. I don't see which topological information is being referred to.\n\n* I honestly did not understand Theorem 5.1. It would perhaps be good to have a less informal statement.\n\n* The model of Beik-Mohammadi used in Fig. 5 is just a VAE. Such a model can easily capture a Euclidean phenomena, so I would suggest that perhaps the poor results in Fig 5 are simply due to a poorly fitted VAE?\n\n* In the discussion of related work, I missed references to papers that learn Riemannian metrics from data. There is early work from both Lebanon (Learning Riemannian Metrics, UAI 2003) and Hauberg et al. (A Geometric Take on Metric Learning, NeurIPS 2012) that aim to do the same, but not using neural networks.\n",
            "summary_of_the_review": "The paper is an interesting extension of existing work, and, in particular, the work on graph embeddings is a neat contribution. The paper itself, however, systematically misrepresent existing work (possibly in an effort to increase perceived novelty). Many of the claimed contributions are well-known and appear in the *cited work*. This is unfortunate, as the paper comes with useful contributions, but it is difficult to recommend a paper that requires a significant rewrite to accurately represent the foundational work on which the paper builds.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors proposed to use a neural network to model a Riemannian manifold. The input coordinates x (in a global chart) is first mapped onto a high dimensional real-vector space; then it is reorganized into a symmetric matrix; then it goes through matrix exponentiation; finally it gives a SPD matrix, which is the local metric g(x). This gives a universal modeling of any Riemannian manifold with simple topological structure.  The author constructed basic recipes to compute the geodesic, and to compute distances, besides basic operations, based on automatic differentiation.\n\nWith a target towards applications in robotics, the authors looked into two different experimental settings of the proposed deep metric framework\n(1) On embedding simple graphs, the authors showed that the proposed deep metric can better preserve graph distance (when optimizing a MDS-style stress function for dimensionality reduction);\n\n(2) On fitting given geodesics, the method is shown to be better than Beik-Mohammadi et al. (2021), in the sense that the geodesics corresponding to the learned Riemannian geometry coincides more precisely with the given geodesics, on a few small scale dataset of robot trajectories.",
            "main_review": "I find that the paper is interesting to read, with well polished language. The presented visualizations have high quality. However, I am concerned about a few weakness from both theoretical and practical standpoints, based on which I tend to vote for a rejection.\n\n\nFrom a mathematical standpoint, this paper used basic tools from differential geometry. However the formulae are not consistent, with many un-introduced notations (only an expert can guess what they are but definitely not the general ICLR audience). See below 'minor points' for details regarding the equations. Within the formal results, Lemma 2.1, 2.2 are trivial from the universal approximation theorem. Theorem 2.3 is straightforward from lemma 2.2, and the last theorem 5.1 is informal.\n\n\nFrom a machine learning perspective, the novelty is limited. Learning Riemannian metrics is not new and has been studied in related subareas including metric/manifold learning. The authors' contribution is more on combining existing tools including automatic differentiation into a unified framework.  This is in contract to the authors' claim on ' the first method for learning Riemannian manifolds from geometric\ninformation.' The authors are suggested to be more modest in their claims, and connect with related work.\n\nAnother weakness is that the experiments are mostly did on toy datasets through visualizations. It is hard to argue the usefulness of the proposed deep metric. In the first quantitative experiment, it is easy to observe that the proposed deep metric did not beat the baseline methods, generally speaking.  In the experiments on the three real graphs, there are common rank-based measures for these embeddings,\nsuch as Spearman's coefficient.  At least one of these measures should be included, besides the authors' own distortion measure, for the comparison to be convincing and accetable.  The three real datasets have 30 nodes at most. A question I wish the authors can address in the rebuttal is whether the proposed Riemannian metric learning method can be made scalable? This could be major limitation on the application side.\n\n\nSection 2.3.2 the argument in this section is not convincing by merely looking at these figures. For revision, the authors should explain how this point whose metric tensor is shown is selected, and whether the results are statistically significant through quantitative measurements. Moreover, the results should be accompanied with the spectrum of the metric.\n\n\n'The metric requires approximately n2 + 5n + 3 dimensions by the Nash Embedding Theorem'. I checked that the Nash embedding theorem is an existence theorem. How this bound of the dimensionality of the isometric embedding is obtained? This should either be stated in more detail, formally, or it should be removed.\n\n\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPlease see more detailed points below:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nsection 2.1\n\n\"The difficulty lies in representing outputs on the Lie Group Sn+\"\nIt is not clear why it is difficult to represent a Lie group.\n(in other words, which structural properties of Lie groups are hard to represent?)\n\neq.(1)\nExplain what is '\\exp', '\\circ' (give \\exp in closed form expression)\n\nSection 2.2 'Prop'->'Proposition'\n\nTheorem 2.3 introduce 'd_g' and 'd_{g_{nn}}'\n\nTheorem 2.3 what's the difference between 'ut' and 'sym'? It is best that they can be defined in expressions.\n\nSection 2.3 first paragraph: introduce 'D_x'\n\nSection 2.3.1\n'adding a small positive noise to the diagonal restricts representation.'\nHere, not clear on how to add this 'noise'\n\nSection 3 after equation 1: explain all symbols in the equation.\n\nSection 3 'and the full result for general BVPs': 'and the full results for general BVPs'\n\nSection 3 equation 3: explain 'log(x,y)', is it the same thing as log_x(y)?\n\nThroughout the draft:\n'autodifferentiation':  'auto-differentiation'\n\nSection 4 paragraph 1: explain  what is 'graph constructs' in 'to capture other types of graph constructs'\n\nSection 4 and the whole draft: there should be some space before/after the slash, for example, 'canonical manifolds –- e.g.'\n\nTheorem 4.1 needs citation\n\nSection 4.1 equation 6, d_X(x,y) is inconsistent with d_g(x,y) used earlier, as the index is used for the Riemannian metric.\n\nSimilar for equation 7.\n\nSection 4.1: the distortion measure 'distortion(f)' is defined with respect to \nan embedding f. As on the right of equation 6, max is taken over all the pairs \n(i,j). Use another index for the sequence f_i of embeddings.\n\nTable 1, 2 and Figure 3: to avoid confusion, please be consistent on the\nabbreviation of the proposed method. For example, name it as\nDeep Metric (DM), everywhere for simplicity.\n\nSection 4.2.1 table 1: font too small.\n\nSection 4.2.2 second paragraph: there is an extra \"Gr.\" at line 3.\n\nSection 4.2.2: 'better embeddings than the preexisting methods': 'pre-existing'\n\nSection 4.2.2 Here, it is important to mention the statistics of the investigated graphs, as they are not well known.\n\nsection 5.2.2: it is best to have a baseline method here to be complete, if possible.\n\nReferences, 'bayes', 'bvp', etc should be capitalized",
            "summary_of_the_review": "This current paper provided a universal framework to model Riemannian manifolds with neural networks.\nDespite that it is well written with interesting illustrations, it has the following limitations.\nTheoretically, the mathematical formulae can be improved with better consistency. \nThe novelty in machine learning is limited, leaving the main contribution to the application.\nPractically, the graph embedding method should be better evaluated on larger datasets.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}