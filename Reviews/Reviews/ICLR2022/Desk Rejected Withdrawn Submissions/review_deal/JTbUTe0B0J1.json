{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes network pruning spaces and a detailed empirical evaluation of whether we can efficiently find pruning strategies to identify winning subnetworks. The paper proposes 4 conjectures and a new metric (mean computational budget, mCB) to indicate good pruning strategies. Experiments are conducted with ResNet model on CIFAR-10 and ImageNet datasets. ",
            "main_review": "The paper has following strengths:\n1.\tThe idea behind this paper to study or analyze the pruning behavior is very interesting. In my opinion, the study of trying to understand pruning behavior is novel. Quickly identifying how to prune existing networks is also an important problem.\n2.\tResNet-50 pruning results shown on ImageNet are promising.\n\nThe paper has following weaknesses:\n1.\tMost of the statements in the paper are conjectures without any theoretical proof. Moreover, even if we just look at empirical results, statements like “Conjecture 4: The limitation of performance in each pruning regime can be predicted by a functional form” are completely unsupported and must be removed from the paper. \n2.\tThe main concern I have is on the claim that mCB=1.0 is optimal. The authors make the claim that mCB=1.0 can indicate the winning ticket without any additional proof/analysis. They support it by showing some results in Fig. 5. However, Fig. 5 seems to be plotting data only from the winning subnetworks. If we want to look at the raw accuracy drop data (for all sampled networks) vs. mCB, we should see the plots in Fig. 2 (row 5) and Fig. 4 (row 4). Clearly, mCB=1.0 has nothing to do with winning subnetworks (especially on Fig. 4: models with mCB=1.0 exhibit very high accuracy drop for c_params = 0.02). Specifically, for each value of mCB, there are several subnetworks that achieve high accuracy drop while several others achieve low accuracy drop (e.g., see blue and orange points going high up above the green points in both Figures for almost all the cases). Therefore, mCB=1.0 does not seem to indicate winning subnetworks (in Fig. 2 and Fig. 4). In fact, the optimal value seems to be changing from case to case (e.g., 1.1, 1.2, 1.3, etc., and that is just for one dataset) and, like mentioned before, even at those values, we are not guaranteed to consistently see subnetworks with the least accuracy drops). \n3.\tI tried to see if there are other patterns like in Fig. 2, mCB is positively correlated with accuracy drop (at least for high pruning constraint). This would indicate that the lower the mCB, the lower the accuracy drop and thus it can be used to indicate winning tickets. Unfortunately, in Fig. 4, the same metric is negatively correlated with accuracy drop (particularly for high pruning constraint), and thus, the higher the mCB, the lower the accuracy drop. This again suggests that mCB cannot be used to indicate winning subnetworks. Note that, this change in behavior is for a single dataset. If the behavior of the proposed metric changes for the same dataset (when changing only the experiment setting from “constrain FLOPS” to “constrain parameter count”), it is highly likely that the metric won’t be useful when other, more complex datasets are involved. \n4.\tFor the ResNet result, the authors have gone only up to 25% FLOPS and 50% FLOPS. In Fig. 2, there was no correlation between CIFAR-10 accuracy drop and mCB values for 25% FLOPS constraint, so it is not convincing how mCB can be reasonably used for ImageNet. The results are good; however, it is unclear if mCB is the correct explanation for those results.\n",
            "summary_of_the_review": "I do like the problem addressed in this paper (and it is a hard and novel problem to address). I think with more comprehensive empirical or theoretical support (and by identifying a better metric than mCB), this paper can be transformed into a solid contribution. But in its current form, there are too many weaknesses. For some possibilities on theoretical angle, please see recent works such as https://arxiv.org/abs/1906.06307 (ICLR 2020) and https://arxiv.org/abs/1910.00780 (CVPR 2021).",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper focuses on network pruning. It studies that given a target pruning ratio, how to find a layer-wise pruning strategy that gives the best accuracy-efficiency trade-off. It adopted the method in [1] to first randomly sample pruning recipes from a randomly distribution (uniform ratio + random variable with a controlled std), and compare the empirical distribution function (EDF)'s of the accuracy drop. Through sampling and training a series of network, the paper conjectures that there exists an optimal FLOPs-to-parameter ratio bucket that within this bucket, there are higher probabilities of finding good pruning recipes. \n\n[1] https://openaccess.thecvf.com/content_CVPR_2020/papers/Radosavovic_Designing_Network_Design_Spaces_CVPR_2020_paper.pdf",
            "main_review": "Strength: \n1/ This paper adopted a EDF to understand the distribution of pruning recipes and the impact over the accuracy. This is a meaningful analysis.\n\n2/ This paper achieves very strong performance on pruning ResNet50 on ImageNet dataset.\n\nWeakness:\n1/ This paper presented a lot of experiments, but the logic link between experiments and the conclusion is very vague and misleading. In Figure 2 and 3, row-1, the result seem to show that a close-to-uniform distribution gives better performance than distributions with larger variances. But derivation that mCB is a better indicator of performance is very unclear. As shown in Figure 5 (right), the value of optimal MCB is fitted from observations. But is there a way to predict the optimal value before hand without training so many models? \n\n2/ I do not see enough ablation/comparison to support that sampling around the optimal mCB gives better results. To support this, using the EDF to compare sampling around uniform-0.01-flops, uniform-0.01-params, and uniform-{std}-mCB should be compared. Also, on ImageNet, though the proposed method achieved better performance than other papers, it is not clear whether this is a result of sampling around mCB or simply because it sampled 300 network, which is a large number. And specifically, what insight is transferred from CIFAR experiment to ImageNet? \n\n3/ Also, does this apply to different network other than ResNet50? I do not see discussion and evidence to support this. If it does not transfer, or there is not an easy way to find optimal pruning recipes, how do other works benefit from this?\n\n4/ A clarification question, when generating random ratios for each layer, the paper mentions \"we start with an uniform pruning ratio and modify {ri} with N small random numbers\". How is the random numbers sampled? From Gussian or Uniform or other distributions? I did not find this detail. ",
            "summary_of_the_review": "This paper use EDF to study the distribution of pruning recipes and their impact on accuracy. The insight of the paper is vague and unclear. Despite it ran a large mount of experiments, the conclusion and main results (strong imagement-ResNet50 pruning) is not well-supported by experiments. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work investigated the performance distribution of neural network models pruned under different pruning ratio settings. The authors then propose the pruning strategy of maintaining an optimal FLOPs-to-parameter-bucket ratio for better network performance. The pruning strategy is then compared against prior works on the Resnet-50 model.",
            "main_review": "Pros: The approach is a good exploration that deviates away from setting layer pruning ratios according certain empirically devised rules. To my knowledge, this approach is novel.\n\nCons:\n\nIdeally we would want to sample \\textbf{r} with high STD in order to explore a network space thoroughly. However, under a setting with high pruning ratio, the probability of creating detrimental network bottlenecks is high. That is the chance for at least one out of 50 layers to sample a very small width and cripple the network would be very high under such a setting. Therefore randomly sampling subnetworks in a pruning space with high STD will be dominated by those crippling bottlenecks. I believe this the the reason behind the right shift of the CDF curves in the first row of Figure 2. And likely this effect dominated the whole study. Therefore I believe the conclusion drew by the authors that maintaining an approximately optimal FLOPs-to-parameter-bucket ratio is prefered is flawed. \n\n\nIn Figure 2d the STD value is changed from 0.1 to 0.08 for one of the plots in Row 6. A similar thing happened in Row 5 of Figure 4. Please clearly state such changes in the manuscript.\n\nPlease define the parameter bucket.\n\nP6: “STD-0.01 spaces become higher than STD-0.05”, this sentence is vague.\n\nIt is very difficult to read the 3rd row of Figure 2 even on my big monitor. Maybe a better visualization scheme is needed here.\n\nThe legends of the Row 3,4,5 of Figure 2 are mixed up with the data. Please try to make them stand out.\n\nConjecture 1: please define “A good structure”\n\nThere is no logical reasoning given why Figure 3 proves Conjecture 1c.\n\nThe authors select the best model from 300 candidate models through two rounds and compare the best model against prior works. I believe such a comparison is not proper.",
            "summary_of_the_review": "This manuscript has significant technical and presentation issues. Please see the main review for more info.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}