{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes to solve the overfitting problem of adversarial training. They found that the traditional adversarial training method will generate highly biased adversarial samples, which will cause the decision to be non-smooth. The authors put forward Perturbation Diversity (PD) to ensure the adversarial samples can be distributed evenly in the latent space. They proved the effectiveness of this method with experiments as well as theoretical analysis.",
            "main_review": "This method is interesting. The authors test their proposed method from different aspects, embedding space, time complexity, accuracy, and they also prove it is effective from a theoretical perspective (although I didn't check this in detail). Moreover, the paper and the idea are clearly presented and easy to follow.  My main concern is the performance against AutoAttack (AA) -- according to Table 4, the proposed method cannot consistently improve the robustness against AA, it would be better if there are more insights on this.\n\nMy main questions are:\n\n1.\tAccording to Figure 3(c) and 3(d), the authors generated the training data with FS and FS+PD,  but generated the test data with PGD. I am wondering why not also test with FS. It would be better if the TSNE results of FS test data can also be presented.\n\n2.\tIt seems this method’s performance is related to the batch size. Because this method wants to get the biggest volume for each batch, there will be fewer orthogonal vectors if the batch size is small, resulting in a smaller volume. Each time it generates a new batch, it will forget the directions of the previous batch. Is it possible that a larger batch size will promise more evenly distribution from the whole dataset perspective? How to choose the best batch size?\n\n3.\tI am wondering why PD requires less computation budget compared to other methods. As shown in Fig. 3(e), generating adversarial samples with AT+PD or FS+PD should compute the PD based on AT or FS. Why can it be less time-consuming when adding more restrictions?\n",
            "summary_of_the_review": "I don't see the obvious flaw of this paper. The proposed method makes sense to me and achieves reasonably good results.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Authors of the paper argue that adversarial training leads to training examples over-concentrated near the decision boundary, which hurts generalization. Authors propose a different way to generate adversarial examples during adversarial training which makes a more diverse training set and argue theoretically and experimentally that it would improve robustness and generalization.",
            "main_review": "The paper proposes a novel method to improve adversarial training and proves a theorem which gives an upper bound on robust error rate for the proposed method.\n\nThe paper have following drawback:\n\n1. First of all I would question the assumption that standard adversarial training indeed leads to over-concentration examples near the decision boundary.\n\nI would suggest to clarify that there are at least two decision boundaries worth talking about - “true decision boundary” of the underlying data and “model decision boundary” which is learned by model. Any adversarial example would be near the model decision boundary by construction, however it’s not clear whether this is a problem or not. What’s more important in my opinion is whether adversarial examples are crossing the true decision boundary.\n\nAuthors provided figure 1 as an illustration to decision boundary issue, however one can also argue that size of adversarial perturbation is way too large on figure 1 and that’s really the issue.\nSo there has to be some discussion of perturbation size and whether it’s an issue of perturbation being too large.\n\n2. I have concerns over evaluation of the proposed method.\n\nEvaluating adversarial robustness of defenses is a hard problem, because there is always a question whether attack is not thorough enough (see https://github.com/evaluating-adversarial-robustness/adv-eval-paper for details on thorough evaluation of adversarial defenses).\n\nThere are multiple signs that evaluation of the proposed method is not done thoroughly.\n\nThere are various cases in tables, which show that model accuracy is higher under PGD100 attack compared to PGD20 or PGD40, which suggests that implementation of an attack might be incorrect.\n[if it’s a PGD with random start then there is natural randomness which may affect results as well]\n\nGenerally speaking with an increase of epsilon, accuracy eventually should go to 0. Looking at top left figure of figure 4, I have the impression that AT+PD model will eventually have some lower non-zero bound of accuracy if the curve would be continued for a larger epsilon.\n\nTo resolve these issues I would suggest authors to become familiar with https://github.com/evaluating-adversarial-robustness/adv-eval-paper apply that ideas for evaluation.\n\n3. The strongest attack actually shows that the proposed method makes defense weaker.\n\nAccording to table 4, the strongest attack is “AA” and under the strongest attack, proposed method actually shows lower accuracy compared to baseline.\n\n4. It’s not clear how useful and practical is Theorem 3.1\n\nThe theorem states that “robust error ≤ non-robust error + expression”. However it’s not clear if this expression is bounded in any way and evaluation of the expression seems to require a lot of hard to compute values. I would suggest authors to compute the exact value of expression for few examples to show that numerical value is actually useful.\n\n\nMinor issue: Figure 4 could benefit from more detailed explanation. Reference to fig 4 in section 4.2 mentions CW2 which is not shown on the figure.\n",
            "summary_of_the_review": "Issues with evaluation. Results under the strongest considered adversarial attack are worse compared to baseline.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper used the so-called DDP theory to generate diverse adversarial examples to do robust training to achieve better robust accuracy. Both theoretical and numerical results are provided.",
            "main_review": "Strongness:\n(1) The paper is well-written and well-organized so that it is very easy to follow.\n(2) The motivation is well established and also mathematical correct.\n(3) It has both theoretical and numerical results.\n\nWeakness:\n(1) The assumption of the theory is very strong. Also, the theory did not reflect the key introduced component, i.e., diversity of the adversarial examples, of the paper.\n(2) The gain as compared to FS is marginal, particularly for strong attacks.\n(3) More datasets and models need to be evaluated. ",
            "summary_of_the_review": "See above.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a new method for promoting diversity of adversarial examples. The authors provide theoretical justification and comprehensive experimental results to demonstrate the method’s effectiveness.",
            "main_review": "Strengths:\n- A new method for promoting diversity of adversarial examples\n- 2D illustration to motivate the discussion\n- Theoretical justification of using perturbation diversity\n\n\nWeaknesses:\n- It would have been more instructive if the authors had provided the results of Auto-Attack (AA). AA is an established method that consistently outperforms PGD, CW, and FGSM, and can overcome obfuscated gradient. From Table 4 it is clear that other attacks severely overestimate the robustness, which raises the question of whether other experimental results based on these attacks are still valid. Unfortunately, AA results are missing in most of the experiments.\n- In the CIFAR-10 experiment in Table 4, the proposed method does not improve over the baselines under the strong attack of AA. \n- 2D experiments in Figure1 are a little bit contrived - the authors have used perturbations that are large enough to alter the underlying classes of the data, which is not what people do in real experiments. If the perturbations were properly constrained the AT method would have learned a valid decision boundary. FS and PD have additional constraints so they didn’t suffer from the large perturbations.\n- A lot of the claims in paragraph 3 of the Introduction section are based on observations from the 2D experiments. They may not hold in high dimensional space. \n- The setting of the TSNE experiment is not clear. I suspect that it is a visualization of a classifier trained on CIFAR-10, but I cannot find any relevant information in the TSNE section.\n- The proposed method is based the determinant point process (DPP) theory (Kulesza & Taskar, 2012; Pang et al., 2019b), but the authors didn’t discuss these works in the related work section. \n",
            "summary_of_the_review": "While the proposed method for promoting perturbation diversity is new, and it’s nice to have a theoretical justification, experiments are not well conducted (and potentially have misleading results) -  as is shown in Table 4, the attack methods (except AA) severely overestimate model robustness, and most of the results are based on these attack methods.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}