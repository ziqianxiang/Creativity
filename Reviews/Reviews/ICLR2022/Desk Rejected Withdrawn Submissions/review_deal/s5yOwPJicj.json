{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes the use of an external storage in addition to the episodic memory in continual learning. The authors describe a procedure for executing training and data transfer between the RAM and the external storage asynchronously, and three swapping policies to decide which training instances to transfer from the RAM to the storage.",
            "main_review": "The strengths of this paper are really obvious to anyone that reads it. The text is clear, and well-structured; the main ideas presented in the paper can be understood effortlessly; and the experimental evaluation is quite thorough. (One minor issue with the text: there seems to be a broken citation in the 3rd line of page 4.)\n\nHowever, I am not convinced that this paper offers a significant contribution worthy of a publication. The two main ideas of the paper are i) the asynchronous swapping mechanism, and ii) three different swapping policies. \n\nAsynchronous programming is not something novel or particularly complicated, and, at least to me, it seems obvious that if you'd like to involve an external storage in replay-based continual learning, you would not execute training steps and I/O sequentially. It was also obvious that storing more data would lead to higher accuracy and less forgetting.\n\nMoreover, the obvious way to execute the swapping is at random. The authors do propose an entropy-based approach (which can be considered a minor contribution) and a combination of the random and entropy-based swapping. Still, by examining Table 2 it does not look like there is significant performance gap between the random approach and the other two.\n\n\\\nQuestions:\n- I expected DER++ and RM to perform significantly better on the ImageNet Subnet and ImageNet-1000 benchmarks. Do you have any idea why they perform so badly?\n- Did you try swapping out the instances with the smallest loss? (This idea came to mind when I was reading the paper.)",
            "summary_of_the_review": "The paper is very well-written but, unfortunately, I'm not sure that there is a significant contribution that makes it worthy of acceptance.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces a new episodic memory (EM) management technique CarM in continual learning. Unlike previous approaches, the authors show the hierarchical memory architecture which consists of EM and external storage. At training time, the main model that learns the tasks from stream data utilizes the EM to prevent the catastrophic forgetting, and using the scoring function from the main model, the samples to be swapped are chosen. In addition to proposing a new memory architecture, the authors adopt asynchronous sample retrieval technique which can reduce the training time compared to the synchronous version. In the experiment, the baseline methods equipped with CarM achieves much higher accuracy than their original version, and in the ablation study section, the authors show the effectiveness of each component in CarM.",
            "main_review": "Pros:\n1. Unlike the previous approaches, the settings for CarM is more practical, and it has a novelty on proposing the hierarchical memory architecture.\n\n2. The data swapping policy using entropy and accuracy is quite impressive. This technique can be also used for measuring the importance of image samples in continual learning.\n\n3. Using CarM can increase the overall performance of baseline methods.\n\nCons:\nBefore talking about the cons, I respectfully disagree about the comments that authors said in Abstract and Introduction section. First, the authors said that EM is usually in-memory buffer, and it is stored in RAM. However, in modern mobile devices, their memory size (e.g. RAM) is at most 4GB, but the resolution of image is excessively high, so it is hard to store almost 20000 images in RAM. Second, the authors said that swapping the samples between RAM and storage does not incur significant I/O overhead that affects the overall system efficiency. However, in case of operating system, the more frequently the swap area of the storage is used, the more the system efficiency tends to decrease. To avoid this problem, most of the systems usually adopt large size of RAM. Therefore, if CarM always try to swap the samples between RAM and storage, this approach is highly similar as the case for using swap area frequently in operating system. \n\nNow, I'll specify the cons about this paper.\n1. The overall experimental results are quite impressive. However, I guess the total memory size of CarM is much larger than the original version of the baselines. So, it is hard to know whether the increase of accuracy is due to the large size of memory or the effectiveness of algorithm. I think comparing CarM with baselines that use large size of memory is much fair approach. For example, the accuracy of 'all' of DER, BIC, and RM  in Figure 4(c) is same as in Table 1, which means DER, BiC and RM uses all the training data, while the original version only uses small subset of training data.\n\n2. Deciding which samples to be swapped using the scoring function is a kind of memory retrieval technique. Therefore, it would be great to compare the proposed method to other retrieval methods (e.g. MIR [1] )\n\n[1] Online Continual Learning with Maximally Interfered Retrieval, Aljundi et.al., NeurIPS 2019",
            "summary_of_the_review": "The overall methods and performance is good, but it would be great to fairly compare the proposed method to other baselines.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposed a new strategy for constructing the replay buffer during continual learning. By assuming the split of memories in a continual learner as the large & slow and the small & fast, the authors newly introduce Episodic Storage that memorizes a large number of past tasks' instances, which is different from Episodic Memory that standard CL methods use. The paper introduces swapping rules to use them. ",
            "main_review": "- The initial intuition in the paper about different roles in memories for continual learning is reasonable. But, the paper is composed of too naive or technical modifications with a strong assumption on the large memory. Since the model keeps 1.5x~10x or even full instances of the past tasks compared to baselines, the performance should be fantastic. \n\n- Data swapping policies are not significant. in most cases, the performance of Entropy and Dynamics are very near to the Random with a marginal gap. It would be hard to say the gain in Entropy and Dynamic is statistically significant (e.g., p-test).\n\n\n[Minor]\n- Some citation is missing on page 4, Line3.\n",
            "summary_of_the_review": "The performance gain is simply from memorizing much more instances compared to existing rehearsal-based methods. I understand there are accessible memories with different properties (i.e., slow&large and fast&small) and effectively utilizing them can be helpful for deploying the practical embedded continual learner, but in standard simulation-based continual learning setups, this expansion is not much attractive in the sight of scientific research.\nFurther, suggested swapping policies fails to show benefits, and result in statistically insignificant performance to Random. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "- The paper proposes Carousel Memory (CarM), a new design for episodic memory (EM) in continual learning (CL) systems based on replay or rehearsal of previously observed samples. \n- EM buffers are stored in high-speed memory for fast retrieval but are usually limited in size and the number of samples that they can store. The paper proposes to exploit the abundance of internal storage in some devices to alleviate forgetting by applying asynchronous strategies for swapping samples between memory and storage without necessarily slowing the training due to the difference in access speed.\n- In particular, they propose a data swapping mechanism, based on a gating function, to dynamically replace a subset of in-memory samples used for model training with other samples in the data storage memory.\n",
            "main_review": "- The paper is well written and easy to follow. Background and literature review are on point and up to date. The experiments are thorough and well-detailed.  \n- **Questions**\n    - 1) I guess my main question is: what if you use a bigger memory buffer? You observed that __\"data swapping delivers better accuracy over conventional memory-only approaches using much smaller memory\". I think you should dig a bit more on this. I understand that the point of this paper is showing how to use storage instead of RAM memory to achieve this, but what happens if you try with larger EM sizes such as the total one (memory+storage) that you use when you train with CaRM? I guess that would also give you some information on the importance of your swapping policy. \n    - 2) Actually, It is not really clear what is the actual size of the storage memory. For instance: looking at plot 4c what is the meaning of \"original\"? How many samples are you storing? And how much storage memory? \n    - 3) All policies (random, entropy, dynamic) have similar results. Entropy looks slightly better than the others, but I don't think it has too much significance. The fact that ransom policy works so good makes me think that your method improve performance because because of the size and variability of your buffer. For instance, what if you don't use CaRM and you use the entropy policy to decide which samples stay in the buffer? \n    - 4) Data swapping is not improving iCaRL performance as it does in other methods. Authors explain this behaviour by studying different ways of computing loss on old data.\n        - In the original paper, iCaRL uses only soft labels obtained from an old classiﬁer.\n        - DER++ use both hard labels (i.e., ground truth) as well as soft labels and then weighting the loss as a linear/convex combination of the hard and soft labels.\n        - Having convex combination of losses with soft and hard labels generally helps in BIC and DER++. Modifying iCarl in the same way looks that having hard labels is important when using iCarl + CaRM. In general the coefficient of soft labels does not have to be high.\n        - Is that because old models are enforcing wrong labels and you always need a signal on the true label to avoid confirmation bias? Can you give more insights on this? \n    - Figure 11 x-axis have 10 classes for some methods and 100 for others, the caption does not explain why. Is it an error? Same for figure 12.\n",
            "summary_of_the_review": "- The paper proposes an implementation for increasing the size of the memory buffer using the available storage. This implicitly increase the samples variability in the replay buffer by swapping samples asynchronously. I think it's a clever idea, but it is an engineering solution more than a scientific contribution. Code is available with config files for reproducing experiments.\n- At the moment I'm intrigued by the paper and I believe that the experiments are well carried out and give nice insights, but I don't think that the scientific contribution is enough for ICLR.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}