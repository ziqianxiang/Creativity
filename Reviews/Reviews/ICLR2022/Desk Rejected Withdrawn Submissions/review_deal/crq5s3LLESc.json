{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies the problem of domain adaption, which it attacks from the perspective of Wasserstein distances between label distributions. The key idea to reduce various flavours of DA to the setting of distributional comparison tackled by OT is to consider soft labeling functions $f:\\mathcal{X}\\rightarrow \\Delta_M$, where the co-domain is a simplex of size equal to the union of labels of the two domains. With this, one can define a meaningful ground metric on the simplex, and therefore an OT distance between label pairs. The paper then provides theoretical results linking this distance with the (expected) label shift, and the risk of a hypothesis on the source and target domains. Switching gears, the paper then proposes an optimization objective that combines aspects of this distribution comparison with other terms explored previously in the literature, parametrizes the dual entropy-regularized OT problem with neural networks, and optimizes with gradient descent. The paper presents results some quantitative results validating the theoretical statements and practical experiments on various domain adaptation tasks.",
            "main_review": "Strengths:\n* Some interesting insights in theoretical results, but their relation to similar bounds from prior work is not discussed in detail\n* Results on domain adaptation tasks could potentially be remarkable, but they come without error bars or uncertainty estimates\n\nWeaknesses:\n* Notation. Despite deep familarity with OT & DA, I found this paper unusually hard to read, partly because of dense and unintuitive notation.\n* Correctness. Some aspects of the theoretical results are somewhat haphazardly written and misleading. See \"Technical Issues\" below. \n* Presentation. Could be improved, various parts of the paper are unclear or not properly explained/motivated (see Notation issues), and the relation to related work should be explained much more in detail (see Related work). \n* Coherence. There is a staggering disconnect between the theory developed in Section 2 and the actual method implemeted in Section 3. See 'Coherence Issues' below. \n\nNotation/Writing issues:\n* (for some reason, OpenReview is not rendering  \\mathbb{P}\\_{f^S} properly, so I will write it as $P_{f^S}$ in the following)\n* Very similar symbols used for very different concepts make reading this paper harder than it should be. For example, $\\mathbb{P}^S_{f^S}$ is a joint distribution (on $(x,f^S(x))$), but $P_{f^S}$ is a distribution on labels only (defined as $P_{f^S}=f^S\\sharp \\mathbb{P}^S$ in Prop 2). \n* The equation reference in algorithm 1 for the last step seems to be wrong (it says 10, but (10) is the objective for $\\phi$, not $h$. \n* In general, Algorithm 1 needs much more detail to be useful and reproducible. \n* In page 4, it is claimed that the bound in Prop 3 is \"more reasonably expressed\" that those of prior OTDA and DA work. How so? This should be backed by an explanation.\n\nTechnical issues:\n* Proposition 2 is misleading. The object $W_{d_Y}(\\mathbb{P}^S_{f^S}, \\mathbb{P}^T_{f^T})$ is not really a Wasserstein distance, despite its symbology and how it's referred to in the paper. Note that $\n\\mathbb{P}^S_{f^S}$ and $\\mathbb{P}^T_{f^T}$ are joint distributions on $(x, y)$, but the ground cost $d_Y$ takes only into account the (probabilistic) labels y. As a consequence, it is easy to see that one can have $(x,y)\\neq(x',y')$ but $d_y((x,y),(x',y'))=0$. Easy example: $x_S \\neq x_T$ and $f^s(x_S) = f^t(x_t)$ (e.g., two different images with same label). In other words, $d_Y$ is not a proper metric, and $W_{d_Y}$ is not the Wasserstein distance (or, any distance, for that matter). This part is fixable by simply referring to this object was the OT cost between the distributions. Note that this does not apply to \n$W_{d_Y}(\\mathbb{P}^S_{f^S}, \\mathbb{P}^T_{f^T})$ which is instead defined on marginal (label) distributions. All of this makes me a bit uneasy about the validity of this result, but the proof seems to hold through nevertheless.\n* The weighted 'distance' $\\bar{d}$ introduced in Section 3.2 is not a proper metric in general (e.g., it's very easy to construct trivial examples for which it doesn't satisfy identify of discernibles or triangle inequality). Furthermore, it's motiation is unclear: to upweight the distance if the examples have the same label seems superfluous, since isn't that the point of $\\mathcal{d}_Y$?\n* Equation (10) is unsual in that it combines aspects of the continuous OT problem (the Kantorovich potential is a function) and the discrete one (sums over finite samples). This is certainly not the formulation of Genevay et al. (2016) who either tackle a fully discrete formulation (potentials are vectors), or a continuous one using an RKHS parametrization of the potentials. In particular, Genevay et al.'s formulation maintains the two Kantorovich potentials, while this paper only has one. The closest I can think of to this objective is the semi-dual regularized problem considered by Cuturi & Peyre (2018), which nevertheless is still phrased over (finite-dimensional) potential vectors. Using a neural network here seems like an overkill, when a vector parametrization could to the trick. In any case, I would like to see some other reference for using this semi-dual NN objective, and in particular, the correctness of doing so without any constraints on the potential $\\phi$ + convergence guarantees. \n\nExperimental Issues:\n* The results in Tables 1 and 2 are practically meaningless without error bars, especially considering the stochastic (and potentially unstable, thanks to the min-max objective) nature of the actual algorithm implemented here. \n* There is no discussion on the effect of the choice of weighting terms $\\alpha, \\beta$ from Eq (8), or the number of update steps on the dual problem, in the performance of the method. \n\n\nCoherence issues:\n* Section 2 (up until Thm 4) mostly considers the Monge formulation of OT, between the *label distributions*. Indeed, there is no mention of what happens when the features of the two domains are not directly comparable (e.g., have different dimensionality, or suffer some transformation like rotation). There is no mention of embedded spaces in Props 2 or 3. Section 3, on the other hand, immediately assumes a joint embedded spaces, considers a Wasserstein distance between the *joint (feature-label) distributions*, and introduces two additional loss terms ($\\mathcal{L}^{ent}$ and \n$\\mathcal{L}^{vat}$) with very little motivation. Equally unmotivated is the choice of a weighted 'metric' (not a metric - see above) in section 3.2.\n* The Experiment section is also split into disconnected sections, each adressing independently the Theory (Sec 2) or Algorithm Sec 3). \n\nRelated work issues:\n* Despite continuous assurances on the contrary, the proposed method is very similar to JDOT (Courty et al. 2017) and DeepJDOT (Damodaran et al. 2018). Indeed, the last paragraph in Section 3 concedes that the main difference between the latter and this paper is the use of a weighted ground 'distance', the clustering loss, and the use of the dual rather than primal entropy-regularized OT problem. The last of these comes at a price: replacing a min-min problem with a min-max one, which are known to be less stable that their min-min counterparts. For all these reasons, I would have ecpected a more in-depth comparison to DeepJDOT. In addition, the adaptation bounds are very similar to those of (Courty et al. 2017), but they are not explcitly compared here. ",
            "summary_of_the_review": "In my view, this paper falls short in two main aspects: it provides disconnected theoretical guarantees and practical algorithm, and fails to thoroughly evaluate the method, not providing confidence intervals and ignoring various important parameters in the ablation study. Some of the questionable choices in the implementation of the method and the presentation of the theoretical results make it hard to put this paper in context of related works, both theoretically and empirically. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": " \nThis paper tackles a hard domain adaption (DA) setting where both data and label distributions have shifted. The paper proposed an optimal transport (OT) based DA method. Theoretical analysis of the method is delivered. The empirical results on the standard DA benchmark are presented. \n",
            "main_review": "##########################################################################\n\nPros: \n \n1. The paper has a strong results, 3% improvements on Office-31 nad 4% improvements on Office-Home looks significant for me.  \n \n2. The paper proposes a solid theoretical analysis. I personally like the discussion of closed set/partial set/partial set/universal DA. Theorem 5 reveals an interesting relationship between “domain shift” and different types of settings. \n \n3. This paper articulates its method’s difference with prior work, especially DeepJDOT which I appreciate. \n \n##########################################################################\n\nCons: \n \n1. Although the paper already does a good job of presenting theoretical results. It would be better to provide the following details: \n\n    (1) The constant term in inequality (i) in Proposition 3. What is the physical meaning of this term? Is this term usually large or small in the real dataset? Does it mean some intrinsic hardness of adaptation between two domains?\n\n    (2) Could the author elaborate more on theorem 4? I understand that author trying to say that assuming L(hS,fS,pS) = 0, then L(hT,fT,pT) and W(gS#pS, gT#pT) has a trade-off. However, the trade-off is not clear to me. Let’s say if we know A+B = C, then definitely increase of B will lead decrease in A and vice versa. However, in theorem 4, we have A + B >= C, so I am not sure whether there is a trade-off between A and B. I know your sort of empirically prove that there is a trade-off via your experiment shown in Figure 3. But I would like to hear some more theoretical justification. \n \n2. I am not sure why the similarity-aware version of the ground metric significantly improves the performance (as shown in table 6). Could the author articulate why it is so effective? Also, I suggest that the author make the two paragraphs “Similarity-aware version of the ground metric” and “Evaluating the weights” to be adjacent such that I can have a clue what s(xS,xT) is, right after I finish reading the first paragraph. \n \n3. I notice that the author include an ablation study in table 6 on a subset of tasks on Office-Home. Could the author provide this ablation study on all the Office-Home and include it in the main paper. I feel that will be the most convincing. \n\n4. Can the author also make a comparison between LDROT and RWOT (like you did with DeepJDOT)? RWOT is the prior state-of-the-art OT-based DA method. And, RWOT also uses weighted OT. \n##########################################################################\n \nMinor comments\n \n1. The label shift seems ready being a standard concept in the field of DA which means, P_s(y)!=P_t(y) while P_s(x|y) = P_t(x|y). So I just the author naming their newly defined “label shift” (in definition 1) differently such as “transported label shift”.\n \n2.What is the \\bar{h} in eqn (8) ? \n \n \n",
            "summary_of_the_review": "Overall, I vote for accepting. I like the idea that formulating the DA problem as an optimal transport problem.  The solution is not limited to certain assumptions like data shift or label shift. My major concern is about the clarity of the paper and some ablation results (see cons). Hopefully, the authors can address my concern in the rebuttal period. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper deals with domain adaptation (DA). It develops theory to define the label shift between two domains via optimal transport (OT). It also studies the properties of DA under various DA settings (closed-set, partial-set, open-set, and universal settings). Numerical method is proposed to handle covariate and label shift using OT.  Furthermore empirical evidences show  the effectiveness  of the approach. ",
            "main_review": "- The main theoretical novelty is Proposition 3 which establishes the target error bound as a function of the source error, the label shift LS and the shift due to the use of the hypothesis functions $h^S$ and $h^T$. Interestingly the LS is an upper bound of the norm over the difference of class proportions in source and target domains. Driving LS close to 0  will minimize the class proportions mismatch. When the data marginal distributions are mixtures of well-separated Gaussians the decrease rate of the mismatch is attainable.\n\n- When commenting Proposition 3, the paper states \"the label shift in our inequality is more reasonably expressed\". Can the authors elaborate more on that?\n\n- The authors overlook the paper [1] which deals with OT for conditional domain matching and label shift or [2] which addresses label shift in deep domain matching using OT. Can the authors relate their results to the ones in [1] and [2]?\n\n- As an interesting contribution, the proposed label shift is studied under open-set or partial-set settings where the output spaces in source and target domains are not aligned. How the derived bounds relate to the existing learning bounds (for instance [3])?\n\n- Based on the developed theory the paper presents another interesting contribution: a learning algorithm for unsupervised DA. The objective function to be minimized includes the standard source error, the shift loss which accounts for the covariate and label shift using OT and a clustering loss which enforces the target model yield the same prediction for source and target examples lying on the same cluster. The used objective function raises some questions: a) how minimizing the overall objective function ensures minimization of the target error bound in Proposition 3, especially minimization of LS? b) the use of the clustering loss is disconnected from the theoretical derivation. Can the authors elaborate more on the clustering loss and its impact on the performances? c) Does the clustering loss apply to both $h^S$ and $h^T$?\n\n- The learning scheme involves several hyper-parameters ($\\epsilon$; $\\tau$, $\\alpha$, $\\beta$, $\\theta$...). How sensitive are the prediction performances to the choice of those parameters?\n\n- The paper in Section 4.1 refers to point (iii) of Proposition 3. It is not clear how the MNIST and SVHN samples are described by mixtures of well-separated Gaussians.\n\n- I wonder if it is useful to include in the main body the experiment on learning domain-invariant representation under label shift as this phenomenon is now known and reported in the literature. It should rather be interesting to include ablation study instead. \n\n\nOther comments\n- In Eq. (3), to comply with the notations, use $p^S_Y$ and $p^T_Y$ in the Wasserstein distance\n- In Paragraph \"Implication on target performance\",  $h^S$ should read $h^S = h \\circ g$\n\n\n\n \nReferences\n\n[1] Rakotomamonjy, Alain, et al. \"Optimal Transport for Conditional Domain Matching and Label Shift.\" arXiv preprint arXiv:2006.08161 (2020).\n\n[2] Le, Trung et al. \"LAMDA: Label Matching Deep Domain Adaptation\". ICML 2021.\n\n[3] Fang, Zhen, et al. \"Learning Bounds for Open-Set Learning.\" International Conference on Machine Learning. PMLR, 2021.",
            "summary_of_the_review": "The paper proposes a theoretical framework to study DA under covariate and label shift using OT. Though the experimental results are better compared to competitors, the paper overlooks recent important  works on the topic. Also the related learning algorithm scheme is not sufficiently connected to the theory and ablation study on the choice of the various hyper-parameters is lacking. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper considers the problem of domain adaptation when there is a shift between the feature marginals and between the labeling mechanisms (label conditionals). The contributions are divided into a theoretical and an algorithmic component. At the theoretical level, the shift between the domain labels is defined based on a mapping connecting the source and the target domains. This term is proven to be an upper bound on the shift between label marginals, and it contributes to bounding the classification risk on the target domain among other terms. Also, the tradeoff between aligning the feature marginals and the performance on the target and source domains is quantified in the case of a label shift. At the algorithmic level, the authors propose the LDROT algorithm that is claimed to reduce both the data and label shifts. Empirically, some theoretical findings are tested, and the performance of LDROT is demonstrated on several datasets.",
            "main_review": "The paper is well written, but the notations are cumbersome. The motivation to handle a general DA setting is interesting, but I do not think this paper has succeeded in proposing a solution for that problem. In fact, there are some nonnegligible drawbacks both on the theoretical level and concerning the connection to the proposed algorithm.\n\n# Strengths\n\n* Theoretical results concerning the Wasserstein distance in difficult DA settings: open set, partial and universal.\n* The result concerning the special case of well separated Gaussians, which examines the tightness of an established bound.\n\n# Weaknesses\n\n* Problem with the reconstruction term in Proposition 3 $r = \\sup_{L,K:L\\\\#P^{T} = P^{S}, K\\\\#P^{S} = P^{T}}E\\left[d_{Y}(f^{T}(K(L(x))), f^{T}(x))\\right]$: The problem comes from taking the supremum rather than the infimum, which will result in a worst case association between $x$ and $K(L(x))$. More formally, consider for example the case where $P^{S} = P^{T} = P$ is a probability distribution on $\\mathbb R$, having a density that is symmetric with respect to the origin (e.g. a Gaussian, or a mixture of two Gaussians with the same variance, with equal weights and each being centered at $-a$ and $a$ for $a>0$).  Then we have $-Id \\\\# P = P$, where $Id$ is the identity map. Let $f^{T}(x) = ([x\\geq 0], [x<0])$ (where $[.]$ is the Iverson bracket notation) , let  $L = -K = Id$, and let $d_{Y}$ be the $L_1$ (total variation) metric. We have  $L\\\\#P^{T} = P^{S}, K\\\\#P^{S} = P^{T}$ . Then for $x \\in \\operatorname{supp}P$, we have $$d_{Y}(f^{T}(K(L(x))), f^{T}(x)) = d_{Y}(f^{T}(-x), f^{T}(x)) = |[x\\leq0] - [x\\geq0]| + |[x>0] - [x<0]| = 2 |[x<0] - [x>0]| = 2$$.\n    Consequently, the reconstruction term is at least $2$ which is nonnegligible and makes the bound vacuous. The construction can be generalized to the binary case with equal class proportions and a symmetry with respect to the origin in $\\mathbb R^d$. \n\n* Label shift reduction by LDROT: In the end of paragraph **Remark on the shifting term**, it is claimed that the term $W_{d_Y}(P^S_{h^S}, P^T_{h^T})$ contributes to reducing the label shift. However, the label shift is equal to $W_{d_Y}(P^S_{f^S}, P^T_{f^T})$ according to Proposition 2, and there is not guarantee that $W_{d_Y}(P^S_{h^S}, P^T_{h^T})$ behaves like $W_{d_Y}(P^S_{h^S}, P^T_{h^T})$.\n\n* Lack of connection between the theory and the practical part: in the empirical part, two major modifications are added:\n\n    * Introducing the clustering loss for the target domain: the only connection I see with the theory is the result about mixtures of Gaussians with well-separated components. However, this result concerns the tightness of the bound on $LS$, not the performance on the target domain. Moreover, in the supplementary material, Appendix D.1, it is indicated that minimizing the Wasserstein loss between \n$W_d(P^S_{h^S},P^{T}_{h^{T}})$ term encourages the source and target representations to group in clusters. This should be sufficient and more coherent with the theoretical developments. Otherwise, an ablation study showing to which extent the clustering term boosts performance is necessary in order to see how much performance gain is due to the developed theory.\n   * Using weights as a function of similarities between points to improve the Wasserstein distance estimation: this modification is added in an ad-hoc manner and further increases the gap w.r.t. the developed theory.\n   * $d_X$ and $d_Y$ being respectively the cosine distance and the KL divergence, are not metrics.\n\n* Using $d_Y$ when comparing joint distributions: A confusion when $d_Y$ is used to define a Wasserstein distance between the joint distributions. From what I understood in the proof of Proposition , if we denote $z^T = (x, f^T(x)) \\sim P^T_{f^T}$ , we have  $$W_{d_Y}\\left(P^S_{f^S}, P^T_{f^T}\\right) =\\inf_{H\\\\#P^T_{f^T} = P^S_{f^S}} E_{(x, f^{T}(x))  \\sim P^T_{f^T}}[d_Y(f^T(x), H_2(x,f^T(x))]= \\inf_{H\\\\#P^T_{f^T} = P^S_{f^S}} E_{z^T  \\sim P^T_{f^T}}\\left[d_Y\\left(\\pi_{2}(z^{T}), \\pi_{2}\\circ H(z^{T})\\right)\\right]$$ \n\n    where $\\pi_{2}$ denotes the projection on the second component for a given couple $(x,y)\\in X^D \\times Y_\\Delta$ . Hence the previous Wasserstein distance should be denoted for example $W_{d_Y\\circ \\pi_{2}}$, since $d_{Y} \\circ \\pi_{2}$ operates on $X \\times Y_{\\Delta}$. The same problem applies to Proposition 3 for the term $W_{d_Y}\\left(P^S_{h^S}, P^T_{h^T}\\right)$. Also, a similar issue concerns the equality in Equation (9), and can be clarified by considering $d_X \\circ \\pi_1$ instead of $d_X$ when comparing $P^S_{h^S}$ and $P^T_{h^T}$, where $\\pi_1$ is the projection on the first element.\n\n* For the classification results, OT-based methods are sufficient if the cost function didn't involve other terms. However, since it involves the clustering term on the target domain (along with the smoothing VAT term), a comparison to papers having used this loss should be carried out, at least to [1].\n\n# Other comments\n\n* Similarity to recent work: The objective function is very similar to a previous, albeit recent work [4]. The final version of the paper should make the readers aware of this similarity.\n* Paper introducing the \"Kantorovich Potential Network\" term:  To the best of my knowledge, the exact term was first introduced in [3]. This latter paper should then be mentioned when the network is introduced.\n## Minor comments\n* Appendix A: $(\\mathscr X, \\mathbb P)$ and $(\\mathscr Y, \\mathbb Q)$ are probability spaces, not measures. Also, $Q$ should be denoted $\\mathbb Q$\n* In paragraph **Evaluating the weights $w_{ij}$**: \"we find $\\mu_i$\" --> \"we define $\\mu_i$\"\n* In paragraph **Entropic regularized version of shifting term**: \"is a positive regularized term\" --> \"is a positive regularization term\"\n\n\n# Questions:\n* Was the comparison to other methods carried out using the same architecture? In this case, the cost function explains the performance, but otherwise, it is difficult to tell if the network architecture did not contribute to the boost.\n\n* Suggestions\n\n  * The notations are cumbersome in my opinion. For example, replacing $\\mathbb P^{S}$ and $\\mathbb P^{T}$ by $\\mathbb S$ and $\\mathbb T$ would partially mitigate this problem.\n  * In Theorem 5, Points $i)$ and $ii)$ of Theorem 5 can be presented as particular cases that can be immediately deduced from $iii)$.\n  * More theoretical comparison to JDOT [2] should be carried out as it is also a paper that develops a theory for reducing both the distribution and label shifts.\n\n\n# References\n\n[1] Shu, R., Bui, H. H., Narui, H., & Ermon, S. (2018). A dirt-t approach to unsupervised domain adaptation. *arXiv preprint arXiv:1802.08735*.\n\n[2] Courty, N., Flamary, R., Habrard, A., & Rakotomamonjy, A. (2017). Joint distribution optimal transportation for domain adaptation. In *Advances in Neural Information Processing Systems 30* (pp. 3730–3739). Curran Associates, Inc.\n\n[3] Li, M., Zhai, Y. M., Luo, Y. W., Ge, P. F., & Ren, C. X. (2020). Enhanced transport distance for unsupervised domain adaptation. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition* (pp. 13936-13944).\n\n[4] Nguyen, T., Le, T., Zhao, H., Tran, H. Q., Nguyen, T., & Phung, D. (2021). Tidot: A teacher imitation learning approach for domain adaptation with optimal transport. IJCAI.",
            "summary_of_the_review": "I recommend rejecting the paper based on the following points that I detailed in the main review:\n* The looseness of the bound of Proposition 2 concerning the performance on the target domain.\n* The fact that theoretical result justifies that minimizing $W_{d_Y}(P^S_{f^S}, P^T_{f^T})$ is guaranteed by minimizing $W_{d_Y}(P^S_{h^S}, P^T_{h^T})$\n* The lack of connection between the proposed theory and the algorithm LDROT, especially adding the clustering loss part.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}