{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors propose a new meta-strategy solver whose stopping criterion is based on regret criterion for the current empirical game. Embedded within the PSRO framework, it is then claimed that this procedure converges. The key idea behind this is that within the empirical game the profile space can contain strategies with lower regret than the target profile.\n\nThe authors then propose a method of tackling games with larger numbers of players for which current approaches are limited due to the explosive growth in required computation with the number of players. Specifically, the authors propose using a backward search method to avoid simulating the complete payoff matrix which is where the computational burden is normally occurred.\n\nThe ideas of the paper are supported by a range of experiments that demonstrate the beneficial characteristics of the approach.",
            "main_review": "The paper is well-written and easy to follow. The explanation of the pressing issues are well-explained as is the rationale behind the proposed approach.   \n\nFrom an intuitive perspective, the approach is well-justified, however little is done in the way of providing concrete theoretical results that prove better performance. Since, the contribution of the paper is mainly down to an introduction of two features, the overall contribution feels a little slight. The proof of the convergence guarantee is also very straightforward. Nevertheless, the paper is supported by decent empirical evaluations which give positive support for the ideas within the paper. \n\nAs the authors state, some key ideas for the backward profile search technique were established in Brinkman & Wellman (2016). Elaborating more on why the adaptation of these ideas to the PSRO setting would be quite useful for evaluation the novelty of that part of the contribution. ",
            "summary_of_the_review": "The paper makes a useful contribution based which is justified by way of intuitive arguments and empirical evaluations. A drawback of the paper is that the extent of the contribution feels quite slight; including the theoretical contributions. This is magnified by the fact that the paper does not elaborate as to why the proposed BPS approach requires a meaningful degree of effort to adapt it to PSRO.\n\nA positive aspect of the paper is that the ideas are supported by a range of encouraging empirical results.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a variant of the PSRO family of double oracle algorithms, using replicator dynamics (RD) with strategy-profile regret to generate the next empirical game strategy to best-respond to, rather than the classic choice of trying to find a Nash equilibrium.  The RD process is terminated by reaching a threshold regret. The authors report on experiments showing this process has better performance in a couple of existing baselines, that using a regret-based stopping condition is better than a fixed number of update steps, and that the algorithm is not highly sensitive to the stopping threshold.",
            "main_review": "The work in this paper is an interesting continuation of an existing strand of research. The algorithm is clear, but I have some concern about the applicability of theoretical statements, and the strength of the empirical results.\n\nFinally, it wasn't clear to me what importance was being placed on solving two-player zero sum games, generating a strategically interesting set of strategies, and finding a NE (or some other eq'm concept?) for multiplayer games. As a result, the main text seems to largely be motivated and described in a multiplayer context, while the experiments seemed to be focused on two player zero-sum games. The results and much of the setup is centered around finding a strategy which is close to a NE, but there are occasional mentions of strategic diversity. The goal should be clear throughout the paper.\n\n\nMore specific comments follow below:\n\npage 5: is lambda_min generated for each game? That is, every time we call RRD, do we run for many iterations to discover what lambda_min is, then either use a stored history or re-run to get the result for some larger lambda?\nOne issue I see is that if lambda_min is generated once early on, I'm not aware of any reason that cycles couldn't be introduced as the game is expanded, so that lambda_min might increase. If it's generated every time, how expensive is this?\n\nAlso related to lambda_min -- it's apparently part of the whole generation process, but its computation isn't well specified. Run once, or when? If once, with what initial empirical game (1 strategy doesn't seem like it would give an informative choice)? For what \"large number of iterations\"?\n\n\"Since RD may not converge to an equilibrium in certain games\"\nThis seems like a weaker caveat than expected: in my understanding, RD is not guaranteed to converge in two player zero sum games (at least without adding additional regularization), let alone multiplayer games. An alternative phrasing like \"Since RD will generally not converge to an equilibrium\"?\n\nProjection P seems to be defined wrt a generic norm. Did the authors intend this to be generic, or assuming some particular projection? If it is supposed to be a specific choice, that choice should be clarified. If P is intended to be generic, the choice used for the experiments should be stated.\n\nTheorem 1 does not seem to directly apply to PSRO RRD.  Steps in the reasoning use a best response, while PSRO is using an approximate best response. Also, the set of all policies generated by the an RL-based approximate best-response oracle policies is only finite in an unsatisfying way: possible floating bit configurations of NN parameters, or some argument about the possible results given random data and a limited number of training step.\nIgnoring the finite closure, it seems like it should be easy to update from a best response to an approximate best response.\n\nThe name epsilon-closeness suggests to me that epsilon is a parameter of the property, but it is instead a property of the argument, sigma. Also, closeness suggests a distance to me, rather than the propety of being closed. Consider calling it o-closed instead?\n\n\"We first observe that RRD yields a rapid convergence to a low-regret value compared to others.\"\nRephrase to be more clear that this is a relative to the other methods? The absolute values seem quite high, which is possibly being masked by the choice of clipping the y-axis: Leduc poker has a 1 unit ante, so a regret of 1 seems like rather a lot of regret. This is even more strongly an issue in Figure 3, which looks like minimal improvement over the initial (random?) strategy profile.\n\n\"According to the consistency criterion, we compare MSSs with the same RRD-based regret\"\nThis could be clarified in the text. Figure 1b seems to have a number of different regret measurements, not just RRD. In contrast, Figure 1a is using a single measure: regret.\n\n\"and authenticate the faster convergence of RRD over DO and PRD in terms of the strategic coverage of the empirical game.\"\nHow is this a measure of strategic coverage? In what way is this critical?\n\n\nNit: for Figure 1a, suggest ordering the legend in the same order as the lines, by swapping RRD with threshold and RRD with fixed number of iterations. For Figure 2, use the same colors/legend for both plots.",
            "summary_of_the_review": "There are few issues of clarity and a few issues with the theory and evaluation that I think would need to be addressed before publication.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Policy-Space Response Oracle (PSRO) is an algorithm that incrementally approximates Nash Equilibria (NE) in an *original* game by maintaining a portfolio of strategies for all players, over which the final player strategies (the NE) must mix appropriately. PSRO updates the portfolio by employing Double Oracle (DO)-like iterations on an *empirical* game. There are two steps in this process: 1) The individual rewards of the empirical normal-form game are estimated by averaging simulations of the original game using each tuple of pure strategies from the portfolio. Then, 2) new strategies are added to expand the portfolio. They are found using reinforcement learning as an (approximate) best-response to some probability distribution over the opponent's profile. How to set this distribution is important: it will be used as a training target for RL.\n\nPreviously, the distribution (hence training target) was set as the NE of the opponents (with some minor modifications to incentivize exploration). This paper instead proposes to i) use an approximate NE as the target: it is found by following a few steps of replicator dynamics until a stopping criterion (regret or number of iterations) is met. Then ii), for a game with more than two players, authors provide a Double-Oracle-like scheme to select which rewards should be computed by simulation to save time, as the number of all non-sparse individual rewards to estimate is exponential in the number of players.",
            "main_review": "\nThe paper is relatively clearly written for an expert in the topic, but is perhaps less accessible for an outsider. A few more iterations on the text could improve clarity, for example simplifying the text of abstract to not use abbreviations unncessarily and improving the flow of the text.\n\nThis is mainly an experimental paper. My main concerns are following:\n\ni) The main takeaway from the paper is that picking the opponent portfolio distribution as an approximate NE yields lower regret than an exact NE (for example, Figure 1b). I find this very surprising, and I suspect there are some confounding factors going on that deserve more thorough investigation and explanation. I suspect this is due to inexact values of the empirical game. The set of NE in the empirical game can be **very sensitive** to the choice of rewards, and perhaps some strategies dominate others due to variance of the simulated rewards. This effect might be even more pronounced, when old estimates are kept for the next iterations (such as in the OpenSpiel implementation of PSRO [1]). While this saves the time to recalculate the payoffs from O(n^2) to O(n), it could bias the search in the solution space and require more of pretty much identical DO strategies to compensate for the reward variance. \nAs this shouldn't take too much time, I would like to ask the authors to rerun the experiments with exact rewards on the small games and show whether the observations in the paper still hold. Either way, this does not invalidate the findings on _large games_: it is prohibitively expensive to compute the rewards of the emprical game. But maybe more efficient mechanisms could be made (such as simulating more outcomes for the older strategies in the portfolio).\n\nii) How do the results generalize? The evaluations are run only on one sequential imperfect-information game (Leduc). It is not clear whether the additional complexity of having more hyperparameters to tune is worth it. Indeed, authors acknowledge this in the appendix as there is \"No Free Lunch for the exploration strategies\". For a practioner the results do not seem to be convincing enough to justify the additional cost.\n\niii) Why should one pick to use RD? I believe other alternatives could be used as well, such as parameter-free Regret Matching with convergence guarantees as a function of time, and are perhaps more efficient in this scenario.\n\niv) I'd like to suggest that the profile search in multiplayer (3+) games is out of scope of the paper and should be perhaps made into a separate publication. I understand the authors probably wanted to put as many interesting things in a single paper, but now there are \"too many things going on\" to properly analyze and review.\n\nOther notes:\n\n- \"Our MSS exhibits many desired proprieties for strategy exploration\" -- too vague, unclear what are \"desired properties\".\n- PRD pseudocode: \\sigma_i -- missing i \n- derivative of strategies -- please add reference to prev equation (with number).\n- \"a projection operator that maintains the feasibility of mixed strategies,\" -- a projection to strategy simplex\n- \"Since RD may not converge to an equilibrium in certain games\" -- Note which games (or provide reference)\n- \"To handle this issue, we select by first running RD for a sufficient large number of iterations T' to obtain \\lambda_\\min and only choosing \\lambda \\geq \\lambda_\\min\" -- how is this different to running RD for \\tau iterations \\tau < T' ? It seems like a roundabout way to claim that RRD will have some bounded regret. Couldn't the regret be directly computed from RD bounds?\n- Everything is defined on NFG games, but Leduc Poker was evaluated -- did you turn the EFG to normal-form? (If yes, this is exponentially expensive and should be noted -- especially the RRD!). If not, then the algorithms are not properly defined.\n- What is the cost of RRD compared to finding NE with LPs?\n- Geometry of strategy space assembles the shape of a spinning top -- missing reference\n- \"plot the regret of RRD profile and the regret of NE based on the same empirical games in each run\" -- in the whole game, not the \"empirical game\" right? because in empirical game NE regret would be zero (since it's the NE of that empirical game)\n- Figure 5: what game is this? How robust are the results to selection with lambda? Does it hold generally across games? Is it an artefact of having imprecise values in the emprical game?\n- Make very clear when regret is being measured on which game it is -- Figure 7? Unclear. \n- Observation: \"Spikiness\" may be related to the projection\n- Sanity check: Are the regrets identical in iteration 1? Show full figures without truncation.\n- \"We evaluate MSSs in terms of the regret of MRCP (averaged over 5 different initial strategies for hex and starting from the uniform strategy for RGS), which is theoretically justified evaluation metric for PSRO when its computation is feasible\" -- I don't see why this is justified.\n\n[1] https://github.com/deepmind/open_spiel/blob/93c2ebd9479389aff3a00be1c83ff70615f357e9/open_spiel/python/algorithms/psro_v2/psro_v2.py#L398\n\n[2] Grandmaster level in StarCraft II using multi-agent reinforcement learning, Vinyals et al.",
            "summary_of_the_review": "The paper proposes to study an interesting modification of PSRO and shows improvements in its regret performance. I like the idea of the paper, however I think the paper needs one more iteration. The authors should improve clarity, make more consise explanation of why approximate NE are better than the exact NE in the empirical game, evaluate on more imperfect-information games (as these are the main use-case for PSRO), and perhaps study alternative choices for finding the approximate NE.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper introduces Regularized Replicator Dynamics (RRD), which the authors claim can practically outperform exact Nash and other related methods as the meta-strategy solver in PSRO. ",
            "main_review": "Strengths\n\n1. RRD is simple to implement and explain\n2. The techniques in the paper are extremely general and can basically be applied to any game verbatim\n\nMajor questions/concerns\n\n1. Why replicator dynamics, as opposed to any other iterative method with perhaps more theoretical guarantees (such as no-regret learning, or linear programming)? If the \"stability\" of the method (i.e., meta-profiles not changing much between PSRO iterations) is the key (as suggested in the experiments section), it seems to me that any iterative method that can be warm-started (e.g., regret matching) can lead to this as well, or it can be added as an explicit objective to a linear program. These would perhaps be more preferable, as, unlike RD, they are guaranteed to find Nash (at least in zero-sum games). I would like to see at least a deeper/more mathematically rigorous discussion about such alternatives, both theoretically and practically, to tease out exactly what it is about the MSS that gives good convergence.\n\n1. In the description of BPS, what, exactly, is a subgame? There are two possibilities in my mind:\n\n(a) *A subgame is an empirical game.* That is, a subgame of an empirical game is a tuple of collection of strategies in the empirical game, one collection per player. If this is the interpretation, the number of payoff matrix entries that need to be computed still scales poorly with the number of players: if we have lots of players each with 2 strategies {L, R}, then BPS will start with 1 strategy for each player (say L), and at each iteration (until convergence) will add strategy R for a new player. Thus after $t$ iterations of BPS our subgame will have 2 strategies for each of $t$ players and 1 strategy for each of the remainder, for a total of $2^t$ profiles. Is the hope that the number of iterations required for convergence will be so tiny that $2^t$ is controlled? That seems unlikely to me. Indeed, in the experiments (Table 1), using BPS does not have a large impact on the number of iterations, only about 10% decrease.\n\n(b) *A subgame is a collection of profiles*. If this is the interpretation, we still run into trouble: a collection of strategy profiles may not make a game. For example, if two players each have two strategies {L, R}, the collection of profiles {LL, LR, RR} do not form a game. What, then, does it mean to compute a Nash equilibrium in a subgame?\n\n3. A lot of the experiments have large relatively error bars that make the results not significant or only slightly significant. I would like to see a more rigorous test with more games and more samples, if the main claims of the paper are to be empirical.\n\n1. The construction in the proof of Theorem 2 (\"No free lunch\") is circular: the game $\\mathcal G$ depends on the strategies $s_t$ and $s'_t$, which depend on the sequences $\\{X_t\\}$ and $\\{X'_t\\}$ of empirical games, which depend on the game $\\mathcal G$. \n\nMinor questions/comments\n\n5. Theorem 1 explicitly relies on RRS converging to a $\\lambda$-equilibrium, which may not happen. This should be explicitly stated as a condition. \n\n1. In the experiments, what size board is used for Hex, and how is the regret computed? I assume a very small one, as it is easily solved by these techniques and regret is exactly computable.",
            "summary_of_the_review": "Paper's main claims are not very well justified: why RRS and BPS as opposed to several reasonable alternatives? ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}