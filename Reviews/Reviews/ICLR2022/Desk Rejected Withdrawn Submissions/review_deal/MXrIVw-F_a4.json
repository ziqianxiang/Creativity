{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "This paper presents Fast Learnable Once-for-all Adversarial Training (FLOAT) which transforms the weight tensors without using extra layers, thereby incurring no significant increase in parameter count, training time, or network latency compared to a standard adversarial training. Compared to existing SOTA, FLOAT is better in many metrics including training time, training parameters and hyperparameters, storage cost, potential inference latency, speed, and task accuracy. \n\nThis paper received highly mixed scores 8-6-5-3. During the private discussion, Reviewer DN3 stated that she/he was willing to raise score from 3 to 5. Although I am not sure why the reviewer did not actually make the change, I'm consider the rating increase as happened (i.e., \"factually\" 8-6-5-5).\n\nAfter reading this paper, AC agrees that FLOAT solved an important limitation of the previous state-of-the-art method OAT: reducing the FiLM overhead by using a more efficient model conditioning method, i.e. adding configurable scaled noise on model weights. The lukewarm part is, the method is no doubt heavily based on the OAT paper. Even one can argue the \"method as a whole\" is novel, the contributions (despite interesting) remain slightly incremental.\n\nIn view of the above, AC currently places this paper as a borderline rejection."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents an algorithm named FLOAT(S), which adds conditional perturbation to weights during training (add noise when training with adversarial examples). This method immediately improves over its predecessor OAT by removing extra FiLM layers. As demonstrated in the experiments, FLOAT achieves better training efficiency than OAT, and perhaps more surprisingly, it has better robustness over OAT.",
            "main_review": "## Strengths\n\n- Better results compared to prior work\n\n## Weaknesses\n\n- Although the method as a whole seems new, each of its parts is known. Specifically, adding parametrized noise tensor to weights improve robustness is shown in prior work, and using different BNs for clean and adv inputs is known to be useful.\n\n- I'm concerned regarding the continuum of the trade-off. First, it is not necessarily a trade-off as shown in Fig 6 and 7: sometimes one can achieve higher robustness and accuracy by changing $\\lambda_n$. Additionally, almost in all figures, FLOAT has flat robustness changes followed by a sharp change. That is, practically speaking there are only two knobs on the trade-off curve.\n\n- It is not clear how applying to Slimmable improves the understanding of the proposed method. They seem pretty orthogonal to me or perhaps I'm missing something.",
            "summary_of_the_review": "Overall, I think this paper is a bit incremental even though it improves over prior work.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper tackles the problem of in-situ trade-off between model robustness and accuracy, which is of great importance in real-world applications. The proposed method, FLOAT, makes improvements on the previous state-of-the-art method OAT (NeurIPS'20) by using a more efficient model conditioning method. Experimental results show that FLOAT outperforms OAT in terms of accuracy, robustness and model size. The FLOPs of FLOAT, OAT, and the PGDAT baseline are almost the same. ",
            "main_review": "Pros:\n1. The problem setting of in-situ trade-off between accuracy and robustness is of great practical importance for real-world high-stakes applications such as autonomous driving. \n2. The proposed method solve a major limitation of previous state-of-the-art method OAT: OAT leads to unneglectable overhead in model size by using the extra FiLM layers for model-conditioned adversarial training. The proposed FLOAT uses a more efficient model conditioning method, which adds configurable scaled noise on model weights. \n3. The experimental results show FLOAT outperforms previous state-of-the-art OAT and PGDAT by considerable margins on three popular benchmark datasets, while largely reducing the model sizes. \n4. Although the proposed \"configurable scaled noise\" method is inspired by He et al. (2019) and the model conditioned learning is inspired by Wang et al. (2020), the overall framework of FLOAT still has its own novelty. Specifically, it is good to see the learnable additive noise in He et al. (2019), which is originally proposed to improve model robustness, can also be adapted for efficient model conditioning. With that said, I do suggest the authors to discuss and acknowledge more on He et al. (2019).\n5. The paper is well written and easy to follow. \n\nCons:\n1. The results in Figure 7 shows OAT is outperformed by PGDAT on CIFAR10 and STL10. However, the original OAT paper showed they have similar performance. I suggest the authors to try more hyperparameters on OAT to better match the results in the original paper. \n2. Please discuss and acknowledge more on He et al. (2019) in both related works and method sections. ",
            "summary_of_the_review": "The problem setting is of great importance to real-world applications. The proposed method solves a major limitation of previous state-of-the-art method, and empirically achieves good performance. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper argues feature-wise linear modulation (FiLM) methods to perform in-situ calibration are too computationally expensive. Motivated by this, the authors propose the FLOAT method which adds scaled binary noise to the weight tensor instead of using extra layers. This paper also extends FLOAT to balance the 3-way-tradeoff between robustness / accuracy / complexity. The included experiments showcase the improvement in resolving the tradeoff between robustness and accuracy. ",
            "main_review": " \nThis work is heavily based on the previous work Wang et al. 2020 <Once for all adversarial training>. At first glance, I could not even tell there are fundamental differences between this work and the previous work. Indeed, the improvement is natural but very limited. This severely diminished the novelty of this work. \n\nThis paper contains lots of redundant information e.g. Algo 1 and Algo 2 are mostly similar with minor tweaks, and could very well be condensed. Figure 2 and 5 are redundant, and could be merged as well. \n\nI do see the improvement from FLOAT over OAT in terms of computation time. In figure 7, the performance of FLOAT seems to be better than OAT and PGD. But Figure 12 in the appendix tells me this is highly dependent on a good pick for lambda, which might not be generalizable. \n\nThe authors tend to abuse some highly theoretical terms in this empirical paper without providing enough background/description such as Pareto Frontier. \n",
            "summary_of_the_review": "\nOverall, I think the paper is not at the publishable stage, I suggest the authors condense the information and add supporting theoretical thoughts behind the proposed modification.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper generalized the conventional once-for-all adversarial training (OAT) approach [Wang et al., 2020] to FLOAT, the fast learnable once-for-all adversarial training method. The key technical innovation lies at the incorporation of $\\lambda$-conditioned noise into model weights so as to achieve the purpose of OAT but not request additional layers to perform conditioning. To demonstrate the effectiveness of the proposed approach, extensive experiments are provided across datasets and model architectures.  ",
            "main_review": "Strengths:\n\n+ Simple but effective model noise injection approach\n+ Extention to slimmable networks is plausible\n+ Sufficient empirical justification across multiple dataset and model types.\n\nWeakness:\n\n- The technical contribution is marginal compared to the OAT baseline. Eq. (4) is similar to $\\lambda$-conditioned weight smoothing with a learnable variance scaled by $\\alpha^l$, right? If so, during training, is only one Gaussian noise realization generated? Or are multiple noise copies used to smoothen the model weights when $\\lambda = 1$?  \n\n- I suggest incorporating experiments evaluated by non-PGD/FGSM attacks (e.g., CW attacks or ensemble attacks like AutoAttacks) at testing time.  In other words, it is worthwhile to examine if the superior RA performance (e.g., Fig. 7c) is correlated with test-time attack types. \n\n- Question on Fig. 7: Fig. 7a and 7c showed that the OAT baseline is worse than PGD-AT. Then, what is the benefit of proposing OAT in the literature? Does it outperform PGD-AT in other metrics? If so, should they be considered in the comparison with FLOAT?\n\n",
            "summary_of_the_review": "In summary, the paper contains some interesting results but the technical novelty is marginal and the effectiveness should be further justified through experiments. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}