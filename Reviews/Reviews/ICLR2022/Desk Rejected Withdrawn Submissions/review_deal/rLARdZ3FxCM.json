{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a novel projective manifold gradient layer for deep rotation regression. The designed layer employs the Riemannian optimization to tune the estimated rotation matrix $R$ and then finds the inverse images on the manifold and ambient space (i.e., $\\mathcal{M}$ and $\\mathcal{X}$).",
            "main_review": "1) This paper is well-written and easy to follow. Figure 1 is very clear. \n2) The core idea is well-motivated. To be honest, I'm not an expert in 3D vision. For me, it is similar to the projected gradient descent (*PGD*), which is widely used in constrained optimization, to some extent. PGD aims to project the unconstrained solution back to the feasible domain (which could be regarded as a kind of inverse image), while RPMG intends to find the inverse image of $\\textbf{R}_g$ and $\\hat{\\textbf{x}}_g$. It would be better to add some references about PGD. \n3) It would be better to show more figures like Figure 3 within different $\\lambda$, for the ablation experiment. \n4) As the authors claim that '*no matter where x is in, the projection operation will shorten the length of our prediction because $|\\textbf{x}_{gp}|$ < |\\textbf{x}| is always true*' in Section 3.3, is it an empirical conclusion or a technical conclusion? By the way, it should be $\\|\\|\\textbf x\\|\\|$ to represent the norm of a vector. \n5) Could the authors provide more descriptions about the implementation of $\\psi(\\cdot)$? I'm just curious about how to define the inverse function of $\\phi(\\cdot)$. \n6) It should be $\\arg \\min \\limits_{x_g \\in \\pi^{-1}(\\hat x_g)} \\|\\|x_g - x\\|\\|$ in Figure 1. \n7) Some theoretical analyses may help to further improve the paper.",
            "summary_of_the_review": "I'm not an expert in 3D vision so that I cannot judge whether the conducted experiments are proper. \nThe paper is technically sound from the optimization aspect, since the proposed layer attempts to restrict each optimization step to approach the manifold. \nThe authors fail to investigate the convergence property.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work proposes a simple loss to improve the performance of deep neural networks for rotation estimations. Conventional way to train a network for 3D rotation regression is usually to minimize the Frobenius norm of rotation matrix differences; the network usually outputs vector representations of rotations (e.g. 3D for euler-angle representation, 4D for Quaternion representation), which are further mapped to 3x3 rotation matrices for loss computation (via manifold mapping and rotation mapping). However, due to the projective nature of manifold mapping, different network outputs can result in the same rotation matrix. It brings difficulties in network backpropagation training. \n\nIn this work, the authors propose to train the network at the immediate stage of the network outputs (without mapping them to rotation matrices). In particular, they propose to compute an intermediate rotation matrix between the predicted rotation matrix and the ground truth rotation matrix during each training step. The vector representation (e.g. Quaternion) of the intermediate rotation matrix is then computed to construct an L2 loss with the network outputs. They also propose a regularization term to enforce the length of the predicted vector to be close to that of the tangent vector of the ground truth rotation matrix, to avoid the length vanishing problem. \n\nThe training strategy is applied to various applications: 3D point cloud pose estimation, 3D object pose estimation from real images in the form of both supervised way and self-supervised way. The experimental results demonstrate improved performance over the conventional training loss functions. ",
            "main_review": "### Strengthes\n\n1: It is valuable to address the problems encountered for deep rotation regression, due to its wide application scenarios. \n\n2: The proposed solution achieves large improvements (> 2 times better in error reduction) with Quaternion, 6D and 9D representations (while achieves similar performance (usually less than 1 degree) as that of 9D-Inf representation). \n\n### Weaknesses\n\n1: Regarding the experimental evaluations, would it be possible to provide more results with Quaternion-Inf and 6D-Inf as that for 9D-Inf? Do they perform similarly as that of RPMG-Quaternion and RPMG-6D? \n\n2: Can you elaborate more about the experimental settings (i.e. for experiments with different rotation representations, for the same task with the same dataset)? Do they have the same settings during training? I found that from Table-1, Table-2, Table-4, Table-5 and Table-6-left, the proposed approach only achieves slightly better performance (<=1 degree better) than that of 9D-Inf representation from Levison et al., \"An analysis of SVD for deep rotation estimation\", NIPS 2020. I am wondering if this could be caused by different experimental settings or other process noise during experiments? \n\nSimilar for the experimental results presented in Table-3, the difference between MG-6D (i.e. when lambda=1) and RPMG-6D is also not quite significant. I am wondering if it is really beneficial to regress to the intermediate rotations (i.e. x_gp) compared to \\hat{x}_g directly, given the slightly better performace (i.e. < 1 degree improvements), which might be caused by the experimental settings or other process noise. Can you elaborate more on it? \n\n3: Can you please explain why the 9D-Inf representation fails for the Instance-level self-supervised experiment? The 9D representation actually also works very well for this task.  It is weird that it works well for other experiments (both supervised and self-supervised settings) and fails on this one. \n\n4: The experimental results demonstrate that 9D-inf representation usually performs much better than the 9D representaiton. However, for similar tasks in Levison et al. 2020, both representations usually have similar performance. Sometimes the 9D representation even performs better than 9D-inf. You'd better explain the reasons for such an inconsistency.",
            "summary_of_the_review": "Prior works (e.g. Levison-2020) already show that Quaternion representation, Euler-angle representation etc. are not able to deliver good performance for deep rotation estimations due to the discontinuity problem, so prior works propose to use 9D/10D representations. Based on this fact, we argue several concerns of the proposed method as follows:\n\n1: The proposed method can outperform Quaternion representation and Euler-angle representation. However, from prior works, we already know 9D/10D representation work better than that of Quternion representation and Euler-angle representations. In this case, the large improvements of the method over prior Quaternion representation or Euler-angle representation does not make any difference, since we can use prior proposed 9D representation directly. To my understanding, the main drawback to use 9D representation is to use more parameters (6 more at maximum) to represent rotation than Quaternion/Euler-angle. However, this would bring negiliable effect to the whole pipeline (with different representations). So for the following concerns, we base on the assumption that we could use 9D representation, instead of Quaternion representation or Euler-angle representation. \n\n2: The experimental results show that the proposed approach performs similarly as that of 9D/9D-Inf experiments. In fact, it is usually less than 1 degrees better. I am wondering if this is caused by the experimental settings. Therefore, I would like to read more elaborations about the training configurations (e.g. hyper-parameters, number of training epochs). The main concern is that I think this slight better performance is not significant enough to prove the proposed approach can outperform the 9D/9D-Inf experiments. If it is the case, why not to use 9D/9D-Inf representations as what Levison-2020 already did? \n\n3: Similar concept to regress to the immediate output of the network directly has been appeared in Levinson-2020 (i.e. 9D-Inf representation) already. The main difference is that the current method propose to regress to an intermediate rotation between the predicted R and the ground truth R. However, the results from Table-3 demonstrate that to regress to the intermediate R is only slightly better (~0.5 degree better) than to regress to the ground truth R, which is not significant to show that it is beneficial to do so (in my humble opionion). As what I have concerned, this small difference might be caused by process noise. \n\nGiven those concerns, I would vote for a boarderline (tends to rejection) at the moment. I would like to read the rebuttals with more elaborations on my concerns, to help me make the final decision.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper considers the task of regression of 3D rotation. In the paper, a new type of Riemannian gradient is devised, making it is possible  to not only map the output of the network on SO(3), but also the gradient back propagating from the loss to the backbone network also on SO(3). The proposed method does not rely on specific rotation representation, and is applicable to quaternion, 6D representation or 9D rotation matrix. Preliminary results show interesting performance.",
            "main_review": "Strengths:\n- The paper is comfortable to read, and the reviewer appreciate work in summarizing the previous works.\n- Introduction of Riemannian gradient for rotation regression sounds novel.\n- The proposed method does not rely on specific rotation representation.\n- The problem is well formulated.\n\n\nWeakness:\n1. Motivation: Why Riemannian gradient is a desirable choice?  I understand mapping the raw output of a network to SO(3) is a desirable choice for regression, but forcing the gradient to be on SO(3) does not sound natural to me.  Let us say the \"manifold mapping\" maps a raw output x from Euclidean space to SO(3), the during back propagation the gradient of loss w.r.t. the raw output x is used to update the weights in the remaining neural network, which reside in Euclidean space. So why do we need a gradient on SO(3) to update Euclidean based weights?\n\n2. The paper argues that the most of the manifold mapping function are many-to-one, and thus the many gradients would result in the same update in final output rotation. This is true, but which type of gradient is the \"best\" is pretty open question. So why the preferred gradient with the smallest norm used in this paper is better than other choices?\n\n4. Experiment: The reviewer highly doubts if it is proper to claim \"start-of-the-art performance\".\n    According to what is present in Section 4, this paper does not compare with any related works, but only a few baselines in its own setup. This also gives raise to the concern of how significant the contribution is.\n\n4. The experimental setting is simplified too much compared to most of the related works. For example, why does this paper only experiment with category of \"bicycle\" and \"sofa\". \n \n5. A few wordings in the paper are unclear to the reviewer. For example:\n - Page 2\n      End of 1st paragraph:  what does \"This in fact becomes a supervision problem\" mean?\n      End of 2nd paragraph: \"One thing to note is that this projective gradient tends to shorten the network output, causing the norms of network output vanishing.\" --> what does \"shorten the network output\" means?\n\nSuggestion: \n- Regarding the gradient of neural network in regression SO(3), it seems that the related work (Liao et al. 2019) is on the similar direction. It would be nice to see the discussion with that paper.\n",
            "summary_of_the_review": "This paper tackles an interesting aspect of SO(3) rotation regression, i.e. improving the gradient backpropagate through the deep neural network. However, the motivation has not fully persuaded the reviewer so far, and the experimental settings appear to be too simple. The reviewer would appreciate it if the above concerns can be addressed. \n\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This study focuses on the task of regressing 3D rotations using deep learning. It follows an existing approach of the literature which consists in including in a neural network a function that maps an arbitrary feature vector to SO(3) (namely “Quat”, “6D”, “9D”).\nThe authors propose a variant of standard backpropagation to train such network. The variant consists of two main ideas:\n\n- “Projective manifold gradient”: the authors try to analytically compute the smallest update step in feature space which would lead to a target update step on SO(3). This step is then fed as a pseudo-gradient to backpropagate a training signal to preceding layers of the network.\n\n- “Regularization”: the approach above actually does not work in practice (collapse of feature vectors). The authors therefore introduce a regularization consisting in a direct supervision of feature vectors, pushing them towards a predefined manifold M and preventing the collapse. The topology of M seems implicitly assumed to be quite similar to the one of SO(3).\n\nThe authors present regression experiments performed with point clouds and images inputs, where the proposed method achieves a comparable or better performance than baselines based on classical backpropagation.",
            "main_review": "Strengths:\n============================================\n- The authors show good results obtained using their backpropagation method, compared to the  baselines.\n- The paper is readable (with some efforts due to the weaknesses listed below) and the motivations of the authors are clear.\n\nWeaknesses:\n============================================\n\n- The main justification provided by the authors for their work is that\n“vanilla auto-differentiation [of a variable on SO(3) manifold yields] off-manifold gradients for predicted rotations” (first page), i.e. that “the gradient of a loss function with respect to the output rotation is often not on-manifold” (second page).\nThis statement makes no sense from a mathematical standpoint, and is misleading.\nIt makes no sense because the gradient of a loss function $L: R \\in SO(3) \\rightarrow L(R)  \\in \\mathbb{R}$ is necessarily tangent to the manifold (this is what the authors refer to as Riemannian gradient).\nWhile it is true that the gradient of an extended loss function defined on 3x3 matrix space\n$L: R \\in \\mathbb{R}^{3 \\times 3} \\rightarrow L(R)  \\in \\mathbb{R}$ can be arbitrary, the statement is still misleading because the authors consider a case where the loss is combined with a mapping function\n$R: \\theta \\in \\mathbb{R}^n \\rightarrow R(\\theta) \\in SO(3)$ (e.g. the “9D” mapping).\nThe Jacobian of such mapping (expressed in ambient space $\\mathbb{R}^{3 \\times 3}$, and where it is well-defined) necessarily admits a nullspace that includes the directions orthogonal to the manifold. Therefore a small displacement in the direction of the gradient of $dL/{d\\theta}$ will always correspond to a small displacement tangent to the manifold, regardless of the direction of the gradient $dL/dR$.\nNote that the method used to estimate such gradient  (vanilla auto-differentiation) is irrelevant here.\n\n- I personally find that the terminology confusing. I suggest the authors to refrain using the word gradient (e.g. “find a better gradient”, “we choose our gradient”, ...) when what they are considering is in fact an optimization step.\n\n- Section 2.1 the authors introduce the notion of Riemannian metric (sometimes denoted $G(\\cdot)$ and sometimes $x \\rightarrow G_x$).  It is not used in the paper however, especially because the authors seem to always assume a manifold isometrically embedded into an ambient Euclidean space (this is not explicit in the paper).\n\n- In Definition 1, the authors mention a notion of “classical gradient” which is not well-defined. It seems to correspond to the gradient of a function $\\hat{f}: \\chi → \\mathbb{R}$ extending smoothly $f$ on the ambient space $\\chi$, but a proper definition would help to clarify this.\n\n- Section 2.2 “for quaternions or 6D and 9D that lie on non-Euclidean manifold”: this statement is inconsistent with the fact that the authors implicitly identify these spaces to respectively 4D, 6D and 9D Euclidean spaces in section 3.2. In this section they consider orthogonal projections that rely on the usual Euclidean distance in these spaces.\n\n- Section 3.2, the authors discuss about the non-triviality of inverting the projection $\\pi$, but ignore the question of inverting $\\phi$, which is not necessarily injective and may therefore lead to multiple valid $\\hat{x_g}$. This is the case in their quaternion example, where $\\hat{x_g}$ can correspond to two different points on the 3-sphere for a given target rotation.\nNote that instead of considering $S^3$ as representation manifold in the quaternion case, they could have considered instead the 3D real-projective space, which would have avoided this difficulty.\n\n- The authors seem to be assume a topology for the representation manifold $\\chi$ somehow close to the one of SO(3). Expliciting the constraints on $\\chi$ may help to clarify the method.\n\n- Section 3.2, while the authors aim to propose a strong theoretical framework, they end up using approximations in practice (ignoring $k>0$ for quaternions, $k1, k3 > 0$ for 6D and chirality constraint for 9D), which is somehow disappointing and would benefit from additional discussions.\n\n- Section 3.3, “the projection operation will shorten the length of our prediction because |x_gp| < |x| is always true”: this statement is false in the general case. As an example, it would not always be true for orthogonal projection on a sphere not centered on zero. The authors should make more explicit the fact that this statement only applies to their specific example.\n\n- The experimental comparisons may be unfair. Compared with the baselines, the proposed method includes two additional hyperparameters ($\\tau$ and $\\lambda$). These hyperparameters have been carefully tuned to achieve good results (including some advanced warm-up technique for $\\tau$), and the authors do not mention the use of a validation set (“we divide the airplanes into a train split and a test split” page 7), which suggests a tuning on the test set, which might be unfair compared to the baselines.\n\n- In section 4.2, the authors reproduce some experiments of Levinson et al. and obtain different results, which suggests some variance in the results across different trainings.  A study of the significance of the results may strenghten the claim of the authors.\n\n- Section 4.3: “without ground truth rotation supervision” is a bold statement given that the flow loss is actually a weighted chordal distance on SO(3). For details, see:\n\t- Kazerounian K, Rastegar J, “Object norms: A class of coordinate and metric independent norms for displacements”. In Flexible Mechanisms, Dynamics, and Analysis ASME DE-Vol 47:271–275 (1992)\n\t- or section 5.3 of Brégier et al, “Defining the Pose of any 3D Rigid Object and an Associated Distance”, in IJCV (2017).\n\n\n- I personally do not find the experiments and the ablative studies convincing enough regarding the usefulness of the projective  gradient step, compared to the simple use of the regularization term. Indeed, results for MG-6D are reported for a single lambda value in table 3, which makes the comparison with RPMG-6D difficult. Moreover, ‘9D-Inf’ (whose approach is somehow similar to MG-9D) achieves comparable results with RPMG-9D in Table 1 and 2, without the extra hyperparameters tuning.\n\n\nTypos and minor issues:\n================================\n- Wrong citation for Adam optimizer (not (Adams et al., 2020)).\n- I could not find in the appendix a proof for equality (2), contrary to what is announced in Definition 1.\n- Section 2.3 (Forward and backward pass): the authors should define the notations used when providing the expression of gradient using chain rule.\n- Section 3.1 (definition of R_g in the case of Riemannian optimization): $R_R$ is undefined.",
            "summary_of_the_review": "I would recommend rejection of the paper in its current form.\nThe approach is based on theoretical developments that contain false and confusing mathematical statements. The experiments may be biased in favor of the proposed method due to hyperparameters tuning, and the superiority of the results obtained with the proposed method may not be statistically significant.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}