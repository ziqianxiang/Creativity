{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The main goal of the paper is to introduce a scalable algorithm for generating hypergraph embeddings to be subsequently used for node classification and hyperedge predictions tasks.",
            "main_review": "The problem is important and timely, and has attracted significant interest in recent years due to its applicability. Existing sota methods still lack in performance.  \n\nThe present work puts forth a nice idea of jointly embedding the edges and the hyperedges, and departs from the traditional approaches for hypergraphs that do a clique or star expansions.\n\nThe experimental results are fairly convincing and appear to be fair, though additional data sets could have been employed, with various levels of size and sparsity. The proposed methods clearly outperforms competition in terms of running time, and most often also in terms of accuracy.\n\nWould have been good to assess performance on a stochastic block model for hypergraphs, to see how close to the ground truth one gets in such synthetic models.",
            "summary_of_the_review": "The experimental section could be further strengthened.\n\nLater edit:\nIt appears that the authors did not compare to other task-relevant sota methods, as other reviewers have pointed out.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper focuses on the problem of representing nodes in hyperedges in a low-dimensional Euclidean space. The approach is a heuristic algorithm that hierarchically reduces the problem to simple graph embedding (here and henceforth I use \"simple graph\" as a technical term).",
            "main_review": "My main concern for the publication is novelty and unfair experimental comparisons. The proposed algorithm is a simple heuristic that does not look any better than previous work – GraphZoom has some theoretical insights from theory of spectral sparsification while MILE has some social science theory-inspired heuristics. The contraction operation seems extremely similar to HARP [Chen et al., 2017] as an extension to hypergraphs.\n\nThere is not much to discuss about the technical quality and exposition of the paper, since the contribution is a heuristic naïve baseline. The performance is aggravating for a method that uses node features – 70% for Cora means very poor embedding quality (simple GCN achieves 80%+, modern 83-85%). I am particularly skeptical, since GraphZoom reports 83+% accuracy on that standard dataset. Overall, it it confusing that some baselines use the node features, while some do not. I would suggest to add some indication of that into the results tables.\n\nOne other experimental problem I have noticed is the lengths of random walks used in node2vec. Instead of the standard 80, allowing to explore the graph more, here it's merely 20. I can imagine this severely cripples that baseline.\n\nOverall, this is a paper without any theoretical contributions, so I believe it should be evaluated on its empirical effectiveness. The effectiveness was not sufficiently demonstrated in the paper, and I have serious doubts about the efficacy of the proposed approach, as it fails to perform on too-simple-to-fail datasets, such as Cora. ",
            "summary_of_the_review": "The overall contributions of the paper are marginal – there is no theoretical insight to the proposed folding technique, and the experiments are mostly comparing against simple graph embedding baselines.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper provides a multi-level approach to learning embeddings for hypergraphs which can be used for downstream tasks of node classification and hyperedge prediction. The proposed method, NetVec, is based on the principle of coarsening a hypergraph followed by employing existing techniques (in this case, node2vec) of learning embeddings and refining the embeddings for the original hypergraph. Authors have shown that the coarsening and refinement procedure let them get embeddings for hypergraphs with more than a million nodes in a few seconds. The proposed method is tested on multiple datasets for node classification and hyperedge prediction and improves performance over the baselines considered.",
            "main_review": "Strengths:\na) The paper takes one of the most important problems in the space of hypergraphs. Most of the solutions that work for hypergraphs do not scale well because of the intrinsic complexity of hypergraph structure.\nb) The proposed framework of obtaining node embeddings works well for all the datasets in the case of node classification and most of the cases in hyperedge prediction. The paper tests NetVec on a variety of datasets, and the numbers outperform the baselines used.\n\nWeaknesses:\na) I am concerned about the novelty of the proposed framework. Working with a coarsened hypergraph and refining the solution for the original hypergraph is not new. There is a separate line of work focused on this aspect of hypergraphs, mainly in the context of clustering [1]. It is possible that the existing coarsening methods may not suit well in your settings, but the paper never talks about them.\nb) Choice of baselines: The baselines used, especially in the case of hyperedge prediction, are not sufficient. Node2Vec is a graph-based method, so comparing against it doesn't reveal much. There are other recent methods of hyperedge prediction that this paper does not discuss [2, 3]. Comparing the accuracy of the proposed method that uses both higher-order information and node features against a technique that uses neither is a weak comparison.\n\nSuggestions for improvement:\na) The rationale behind choosing node2vec to obtain an initial set of embeddings is not clear. More advanced methods (GCN and its later versions) use attribute information to get embeddings for graphs, which node2vec does not consider.\nb) Since the paper proposes a coarsening/refinement framework and not any embedding/classification/hyperedge prediction method, an additional but more fundamental way of evaluation could be the visualization of embeddings.\n\nClarifications:\na) How did you apply graph-based baselines on hypergraphs? Do you take clique reduction? Star expansion? or some other reduction?\nb) In Table 2, how come node2vec takes more time than NetVec(l=2)? My understanding is that NetVec(l=2) runs a node2vec in addition to coarsening and refinement steps. Can you please add some explanation on what I am missing here?\nc) In the Introduction, don't say, \"Two hypergraph problems have been studied in the literature: node classification and hyperedge prediction.\" There are a lot more hypergraph-based research problems in the literature.\nd) The \"importance\" defined in section 3.1.1 will assign low importance to the nodes that are part of large hyperedges. Can you explain the rationale behind this choice in the context of your datasets?\n\nQuestions during rebuttal period: Kindly respond to the questions asked in clarifications and your take on the points raised in weaknesses and suggestions for improvement.\n\n1. Papa, David A., and Igor L. Markov. \"Hypergraph Partitioning and Clustering.\" Handbook of Approximation Algorithms and Metaheuristics 20073547 (2007): 61-1.\n2. Yadati, Naganand, et al. \"NHP: Neural hypergraph link prediction.\" Proceedings of the 29th ACM International Conference on Information & Knowledge Management. 2020.\n3. Kumar, Tarun, et al. \"HPRA: Hyperedge prediction using resource allocation.\" 12th ACM Conference on Web Science. 2020.",
            "summary_of_the_review": "The novelty of the work is under question mainly because the proposed framework a) is not compared with the existing coarsening schemes b) lacks comparison with the SOTA methods for the end tasks c) is shown to outperform methods that are not suitable for the tasks you considered.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a heuristic multi-level approach for computing node embeddings of hypergraphs. ",
            "main_review": "**Strengths:**\n\nHypergraphs are a generalization of graphs and they arise in many application domains. This paper proposes a multi-level approach that scalably computes hypergraph node embeddings.\n\n**Major weaknesses:**\n\nUnfortunately there are two major issues with this paper. First, the proposed method contains too many, if not all, heuristic constructions that, in my opinion, may not make sense in general. Second, the empirical evaluations are missing important information about the datasets and details of the experiments, and hence, the reported results may be potentially misleading. \n\nIn what follows I provide details.\n\n*About the proposed methods:*\n\n1. Why is mean aggregation (instead of other possible aggregation methods) used to compute the feature vector of a hyperedge? Some discussions are needed to justify the choice.\n\n2. When assigning nodes to hyperedges, why is the cosine similarity used? Can we use other similarity measures? My guess is that the authors use the cosine similarity because in their experiments for node classification, the datasets are citation networks and the node feature vectors come from bag-of-words encoding. In these datasets, cosine similarity aligns well with class labels. However, no explanation is ever provided for the choice of cosine similarity in general. Discussions on the choice of similarity measure in the coarsening step should be provided.\n\n3. When assigning nodes to hyperedges, and when the hypergraph does not have node features, the authors suggest to compute some \"importance\" measure, e.g., w_j/d_j. But what if w_j = d_j for all j? The authors suggest to break the tie randomly, but in this extreme case when we randomly assign every node to an arbitrary hyperedge in the coarsening step, will the method still provide meaningful embeddings? More explanations and discussions are needed.\n\n4. How many iterations of refinement (i.e., Laplacian smoothing) is suggested? Table 5 in the appendix (not in Section 4) seems to empirically demonstrate that around 30 iterations is sufficient for the citation networks, but why 30, not 5? Moreover, why does the improvement stops after around 100 iterations? Is it due to the fact that Laplacian smoothing converges reasonably after 100 iterations, or due to the fact that for these citation networks, local network structure (i.e., locally smoothed feature vectors) is already very informative about the class labels, so global network structure (i.e., fully smoothed feature vectors) is not useful? Some discussions should be provided either in the main text or in the appendix.\n\n*About the experiments*\n\n1. The paper does not provide a citation for the datasets in Table 1. The authors claim that they \"[use] the standard hypergraph datasets from prior works\", but I could not find verify the source of the datasets. For example, the Cora dataset that the authors provided in the supplementary material does not match with the Cora dataset used in [Yadati et al. (2019)] or the one used in [1]. The original Cora dataset has 2708 nodes, why is the Cora dataset used in this paper has only 1434 nodes? I tried to keep the largest connected component, but I still get slightly less than 2400 nodes.\n\n2. Since the source of datasets is not clear to me, I cannot verify the validity of the empirical results. I suggest the authors use the same publicly available datasets used by [Yadati et al. (2019)], and follow the same train/test splits. This will ensure consistency for comparing methods and results reported across papers. Moreover, it will improve reproducibility. Please also include other datasets in the supplementary material, e.g., Citeseer, Pubmed, DBLP, right now the supplementary material only contains the Cora datasets. \n\n3. The authors should compare their method with [1] and [2]. [2] is particularly relevant because [2] is also an *unsupervised* node embedding method for hypergraphs. Maybe more importantly, Table 2 in [2] shows that the method in [2] is significantly better than the method proposed in this paper, for all the node classification tasks. But, as I mentioned in my previous point, the empirical settings may be very different. This highlights the importance of using the same benchmark hypergraphs and the same train/test splits. For example, for the Cora dataset, please report a test result when using 20 labelled nodes from each of the 7 classes. [1] applies GCN to the clique expansion of hypergraphs, so [1] is relevant and the authors should compare with [1].\n\n4. Once node embeddings are obtained, how did you perform node classification? Did you train a multi-class classification model? If so, what \nloss function did you use?\n\n**Minor**\n\n1. In high-order network analysis, people usually refer the number of nodes connected by a hyperedge as the order of that hyperedge. It might be helpful to add this as a footnote, as the term \"degree\" of a hyperedge sounds a bit less common.\n\n2. The authors claim generality as a contribution in this work. However, without specific experiments on graphs (not hypergraphs), I could not see how generality is particularly important.\n\n3. In Section 2.1, it might be more clear if graph embedding methods are categorized into supervised and unsupervised approaches.\n\n4. Please discuss the details about parallel implementation in the appendix.\n\n**Additional references**\n\n[1] Hypergraph Neural Networks. Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, Yue Gao. 2019 \\\n[2] A nonlinear diffusion method for semi-supervised learning on hypergraphs. Francesco Tudisco, Konstantin Prokopchik, Austin R. Benson. 2021.",
            "summary_of_the_review": "The multi-level hypergraph embedding method proposed in this paper, while mildly interesting, is neither novel nor surprising. Moreover, the method contains too many heuristics without proper justifications. As an empirical paper, the empirical evaluations are rather weak: A number of important details are missing, making it hard to verify the validity of the results.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors introduce NetVec which is a hypergraph embedding system. The main idea is to use a multi-level embedding approach on the star expansion of the hypergraph. Promising experimental results are given for node classification and hyperedge prediction. The authors argue that NetVec is capable to embed very large hypergraphs.",
            "main_review": "The proposed system uses a multi-level coarsening strategy reducing the size of the input hypergraph. For this, a hypergraph is represented by a bipartite graph which is an heterogeneous graph structure. The authors assert that NetVec allows to maintain the hypergraph structure through successive levels of coarsening.  The assertion should be proved, i.e. that the coarser graph is a representation of a hypergraph. Indeed, the proposed coarsening algorithm (Algorithm 3 given in appendix) takes as input a simple graph so it is not easy how it works for a bipartite graph representing a hypergraph. It should help the reader to see how the algorithm works over a hypergraph without node features. Also, the output hypergraph is an approximation of the input hypergraph and properties of the transformation should be given.\nThe second main assertion is the scalability of NetVec. The authors assert that the time complexity of some steps of NetVec are linear in the size of the bipartite representation of the hypergraph. They should give more elements proving their assertions. Also, the time complexity of the complete system NetVec should be studied. \nFor the experiments, it should be made precise whether co-authorship or co-citation hypergraphs are used. It is said that the refinement algorithm allows to improve the hypergraph embeddings. Again it is not clear how the algorithm works for bipartite graphs. It should be explained why such an improvement. Last, the choice of the parameter vector w is not discussed.\n\n*** Detailed comments\n\n* Algorithm 1. The input of Coarsening is the bipartite representation of a hypergraph. Consequently, the description of Algorithm 3 should be coherent. The coarsening algorithm should be given in Section 3.1.1, it should take as input a bipartite graph representing a hypergraph, it should clearly output a bipartite graph representing a hypergraph. What is f? How do you compute the embedding of a bipartite graph with a graph embedding designed for simple graphs? What is $\\tilde{u}$?\n* Section 3.1.1 It is not easy to understand the algorithm without a clear distinction between the two types of nodes. The assertion at the end of the section (\"One important point ....\") should be proved. Also it is said that \"a coarser graph represents the original hypergraph\". In my opinion, the coarser graph is an approximation of the original hypergraph and properties of such an approximation should be studied.\n* Section 3.1.2. As said before, you should comment how you compute the embedding of the coarsest hypergraph using a system designed for simple graphs.\n* Section 3.1.3. It is said that the vector w is made of weights and then that the algorithm can be improved using a hyper-parameter vector w. This is not discussed further in the paper. Thus it is not clear how to choose such an hyper-parameter and it is not said how t it is chosen in the experiments.\n* Section 3. A subsection should be devoted to the study of the time complexity of NetVec.\n* Section 4. Dataset description. Please precise which hypergraph is considered (co-authorship or co-citation). Please make precise what is the number of edges. Is it the number of edges in the star representation?",
            "summary_of_the_review": "The paper introduces a new system for computing unsupervised hypergraph embeddings. The considered problem is significant and difficult. The main idea is to use a multi-level embedding approach on the star expansion of the hypergraph. This seems promising but, in my opinion, the descriptions of the algorithms are rather vague and do not allow to be fully convinced by the main assertions of the paper. Also, a more complete study of the time complexity should be provided in the paper. Therefore, despite promising ideas and promising experimental results, I think that the paper is not ready for publication at ICLR.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Other reasons (please specify below)"
            ],
            "details_of_ethics_concerns": "Easy to find a previous version of the paper on the Web",
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}