{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents an approach for regressing specific image attributes. For example, predicting\nthe pose of a human face (orientation in degrees) or age of a human (in years). The approach followed\nin the paper assumes that a given input image is invertible into a GAN's latent space where there\nexists a hyper plane seperating semantic attributes. The manuscript finds that the distance to the\nseperating hyper plane can be used as feature to regress human understandable values (age in years)\nor to sort based on the semantic property (ex: young to old)\n\nExperiments are shown on single object distributions like faces, cats, and plant leaves.\n",
            "main_review": "Strengths:\n1. The paper is well presented and easy to follow.\n\n2. Sufficient supporting empirical experiments are provided (age, pose on human faces, orientation/pose\non cats, health of plant leaves)\n\nWeakness:\n1. My main concern is on the limited novelty. Prior works (like interfacegan) have already shown that\nsemantic hyper planes exists in a GANs latent space and traversing along the normal towards the hyper\nplane effects that specific attribute. Its obvious that distance to the separating hyper plane is\nhighly correlated. This paper finds that the corelation is linear.\n\n2. A limitation of this approach could be multi-dimensional attributes which may be hard to binary\nlabel them. Or in other words if the latent space is not disentangled its separating hyperplances would\nbecome unreliable. It would help the paper to add some results on scenes like LSUN bedrooms where the\nlatent could be highly entangled. An example would be to predict the brightness of the scene (dark vs bright).\n\n3. Importance of layers. The paper talks about computing an importance score for each layer that consumes\nsome dimensions of the latent space. Why per layer? Why not per-dimension? Wouldn't this be more generic?\nAnd in such a case, it would be very similar to the approach presented in the style-space-analysis cvpr21 paper.\n\n",
            "summary_of_the_review": "I believe this paper is slightly below the accepatnce threshold with my main concern being limited novelty.\n(also see weakness)\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The work proposes a novel approach where the latent spaces of GANs (particularly StyleGAN) is utilized to perform few-shot regression. \nThe method combines the ideas from the prior work on finding (linear) directions in the semantic space of the GANs for different attributes and using the features from the latent space to perform the task of regression. \nExperiments to demonstrate the quality of the learnt features are performed for the task of pose and\nage estimation for human faces and cars. The proposed approach achieves state-of-the-art performance for few-shot regression and demonstrates competitive performance on different datasets.",
            "main_review": "+The paper is very well written and easy to follow.\n\n+The proposed approach is technically correct and the claims are supported through extensive experiments.\n\n+The experiments demonstrate the effectiveness of the approach in the case of limited supervision (Fig. 4).\n\n+Human evaluation is included for scenarios where the continuous supervision is not available.\n\n1. A comparison of the covergence time to obtain task specific latent codes against the prior work eg., on GAN inversion can be included.\n2. The latent space considered is a unimodal Gaussian? How does this assumption of the linearity of the semantic direction hold when considering multimodal priors or latent spaces eg., multivariate Gaussians?\n",
            "summary_of_the_review": "Overall, the proposed method is of relevance to the community where the goal is to exploit the latent codes from deep generative models to the downstream tasks such as image editing.\nThe proposed approach is sound and well-supported with detailed experiments. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to use linear latent directions of trained GANs (specifically StyleGANs) for regression tasks. In particular, the authors observed that distances in the latent space of GANs are highly correlated with the magnitude of visual semantic attributes. Based on this observation and the fact that different StyleGAN layers control different attributes, a weighting method is proposed to adjust distances calculated in the latent space. The adjusted distance in the latent space can be further calibrated to real-world values by fitting a linear model on small number of labeled images. Empirical studies demonstrated that the proposed method achieves superior few-shot regression performance. Results are presented under calibrated (i.e. predicting real-world values including facial age and head pose) and uncalibrated (i.e. image sorting for human/cat face attributes and sick/healthy leaves) settings.\n\nThe contributions of the paper include: \n- A scheme to convert a pertained generator (StyleGAN) into a few-shot regressive model which achieves SoTA performance.\n- An approach to analyzing layer-importance and a weighting strategy for improved regression.",
            "main_review": "Strengths:\n+ The paper builds upon several existing works on latent space methods in StyleGAN, and proposes a new application where a well-trained GAN can be useful for. It is also well-written and clearly puts the work into the context of GAN literatures.  \n\n\n+ The proposed method showed improved performance over models that require expensive labeling of fine-grained scores (e.g. age, angles of head pose).\n\n\n+ The idea of using gradient information to improve distance measurement for the target attribute is interesting, and based on the presented results it consistently improves the calibrated scores.\n\n\n+ The ablation studies are comprehensive which includes the effects of different design choices, the inversion methods, and regularizers for the linear regression model.\n\n\nWeaknesses:\n- The proposed method assumes the availability of \"disentangled\" semantic direction. As mentioned in section 5 of the main paper, \"for some attributes, finding a disentangled latent direction is infeasible\", which limits its potential use cases. In fact, the attributes considered in this paper are restricted to age and pose for calibrated regression. Furthermore, the assumption also raises concerns including \n    1. How to quantify the amount of disentanglement and what's the effect on the proposed method?\n\n    2. Is the proposed method sensitive to different methods of finding semantic directions? \n\n\n- I find some experiment setups missing or unclear: \n    1. In section 4.2, the age editing direction is discovered by StyleCLIP (global direction), which is supposed to be a direction in S space. However, the major development of the proposed method is in W or W+ space (eq 2 and eq 3). Additional details are needed to clarify the actual implementation.\n\n    2. The methods for finding semantic directions are different for pose (InterfaceGAN) and age (StyleCLIP). Some discussions are required to justify such choice. This concern is also related to the above question about the sensitivity of the proposed method to semantic directions. \n\n\nOther comments:\n\n- One major novelty of the proposed method is to calculate the importance score for each StyleGAN layer. However, the ablation study on this approach versus simply calculating Euclidean distance is performed only on a single attribute (i.e. Figure 10). \n\n- In addition to the limitation caused by out-of-distribution samples (e.g. faces with heavy makeup), even for in-domain samples, if the encoder fails to correctly reconstruct certain details (e.g. lip color, eyebrow shape, lighting), the proposed pipeline may not precisely predict semantic strengths. It would be nice to discuss such effects.\n\n\nMinor comments:\n\n- The result using CLIP in the uncalibrated setting (user study) may not be optimal, and can possibly result in a weak baseline. Instead of calculating scores using the cosine similarity between image and text, the score can be defined as the softmax probability of a zero-shot binary classifier using CLIP [b]. For example, when sorting images based on their \"smile\" intensity. A zero-shot binary classifier can be defined by \"Smiling face\" and \"Sad face\" with proper prompt engineering [c].\n\n\n[a] Tewari et al., \"StyleRig: rigging StyleGAN for 3D control over portrait images\", CVPR 2020.\n\n[b] Radford et al., \"Learning transferable visual models from natural language supervision\", arxiv 2021.\n\n[c] Brown et al., \"Language models are few-shot learners\", NeurIPS 2020.\n\n",
            "summary_of_the_review": "Overall, I vote for weak reject. The empirical result that StyleGAN with simple linear semantic direction can out-perform supervised regression model is inspiring. However, on the technical aspect, similar approach of using gradient information to calculate channel importance has been proposed in [I], and despite its simple framework, there are still some concerns about whether the proposed method is sensitive to the choice of semantic direction trainer. On the experimental evaluation, some details are missing. Hopefully the authors can address my concerns in the rebuttal period.\n\n\n[I] Wu et al., \"StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation\", CVPR 2021 (arxiv 2020)",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this submission, the authors observe that some features, such as yaw angle and age, are encoded linearly in StyleGAN's latent space.\nThey then propose to leverage this property for regression applications, focusing specifically in few-shot settings.\nTo do so, they use an off-the-shelf method to put images into the latent space. They learn weights for each layer that defines a distance in the latent space. Finally, this distance, applied to an input image and a known semantic hyperplane relevant to the regression task, is then calibrated to compute the predicted value.\nThe proposed method can also be used without calibration to compute layer importance and define semantic distances and rankings between images.",
            "main_review": "Strengths:\n- Exploring the quality and the properties of GAN latent space is in itself a relevant topic, and the paper delivers interesting insights in that regard.\n- It proposes a simple method for few-shot regression method, demonstrating that a GAN-based latent space surpasses alternatives for this task, as well as learning unsupervised distances.\n- The paper is mostly clearly written and simple to read.\n\nWeaknesses:\n- The submission emphasized that the proposed method is efficient relative to the number of needed annotations. But it also requires that a semantic hyperplane P is known: in the experiments (Section 4.1), P is obtained \"a large collection of binary-tagged images\". A discussion about how large this collection needs to be, how these requirements limit the usability of the method, and potentially about alternative ways to obtain this hyperplane would be useful.\n- The paper mention that linearity has been observed in all tested attributes, but that it is \"unlikely to hold universally\", which I agree with.\nHowever, it would be useful to expand further on why the method could be expected to work or to fail in any given scenario. In that regard, the authors could want to provide discussions about why the latent space could be linear in the main paper, expending on appendix E.\n- The paper claims that the latent space is linear w.r.t. all tested features. It is supported by results in Figures 1 and 8. However, R^2 is reported only on a single feature. Adding R^2 for more features and datasets would give stronger evidence. Also, for the settings with very few calibration samples in Figure 8, it seems unfair to evaluate 2nd, 3rd, and 5th-degree polynomial by fitting them on only 2 samples. Could the authors add results for 3, 4, and 6 calibration samples?\n- While different inversion spaces are tested in appendix A.3, demonstrating the superiority of the proposed method, the study doesn't include the original Z space. As far as I understand, this space is implicitly dismissed in the paper Section 3.2 along with the more expressive W space, which is observed to be insufficient for accurate reconstruction. But as highlighted by the authors, the best space for inversion is not necessarily the best space for regression, and it could well be that the more compact Z space is already good for the task. It would seem relevant to the paper to provide some indication about how the Z space behaves. It would at the very least serve as a baseline, or for later comparison with other GAN models that don't have a W space, or to give more insights about how the linearity of features can be explained.\n- As noted by the authors, equation 2 is an ill-posed optimization problem. They argue that optimizing it via gradient descent however will keep w* close to w+. I do find this behavior plausible and very likely, but it should nevertheless be empirically validated for the sake of completeness. Have the authors maybe tried to add a regularisation term that minimizes the distance between w* and w+ and checked if there was any difference?\n\nAn additional question:\n- It is hypothesized in Appendix E that the latent space (W) would mostly ignore density related considerations and focus mostly on ease of generation. It seems to be implied that distances in the W space should reflect the magnitude of the visual differences between the corresponding images.\nBut in section 4.2, it is observed that the visual difference between a person at ages 8 and 13 is much greater than the difference between 30 and 35, but that latent-space linearity still holds in this case.\nThis seems to contradict the previously stated hypothesis as age 8 should be further away from age 13 than age 30 to age 35.\nCan the authors comment further on that discrepancy?\n\nminor typo and clarity:\n- section 4 §1 : \"using randomly samples subsets of the data.\"\n- section 3.2 $3: \"a random latent code w ∈ W in the same space as the semantic hyperplane\" might not be a rigorous formulation?",
            "summary_of_the_review": "The submission tackles an interesting topic, and the empirical results show interesting findings about the GAN latent space. Its more directly applicable results are also very promising.\nHowever, the method in itself is somewhat incrementable when compared to prior work on discriminative tasks. More importantly, relevant details are skipped over in the main paper, and some non-trivial claims that would warrant at least some discussion are treated with simple hand-waving.\nFor these reasons, I rate this submission as marginally below the acceptance threshold.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}