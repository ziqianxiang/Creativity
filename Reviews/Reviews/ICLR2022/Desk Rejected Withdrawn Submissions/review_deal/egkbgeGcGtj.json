{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This work proposes a unified multi-dataset pretraining mechanism for semantic segmentation. Several training strategies are proposed to make the model be better trained, including pixel-to-prototype contrastive learning and cross-dataset mixing. Moreover, the proposed MDP can be seamlessly extended to the semi-supervised setting. \n",
            "main_review": "Strengths:\n1 This work is well written and the idea is easy to understand.\n2 The idea of performing multiple datasets pretraining for semantic segmentation is interesting compared with previous works.\n3 The effectiveness of the proposed MDP is verified on four popular datasets, and extensive ablation experiments are provided.\n \nWeaknesses or Questions\n1 The technical contribution of this work is weak. It seems that these work ensembles many popular techniques.\n2. How long will the MDP take? Could you provide the training time comparison of MDP and ImageNet pretraining + individual dataset fine-turning?\n3 The authors emphasize that applying tricks to improve performance is out of the scope of this work. Based on my experiences, it is easy to achieve competitive performance or even better performance without too many tricks with the imagenet pretraining + fine tuning pipeline. There are many off-the-shelf pretrained models, and finetuning will cost less time compared to this work. Based on the above consideration, I think imagenet pretraining + fine tuning pipeline is more practical to many applications? Could the authors provide the advantages of MDB?\n",
            "summary_of_the_review": "Overall, the task of this work is interesting. I incline to accept this work if the weaknesses are well addressed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces an approach to train a semantic segmentation model on multiple datasets. The authors proposed a pixel-to-prototype contrastive learning strategy to learn class-level representations. They also use two cross-dataset mixings to learn a better inter-class relationship.The proposed approach is evaluated on PascalVOC, ADE20K, COCO-Stuff and Cityscapes. The proposed pre-training strategy shows competitive results w.r.t. to a standard ImageNet pre-training. ",
            "main_review": "**Strengths**\n- Learning from multiple datasets is an interesting topic because collecting annotated data for semantic segmentation is time-consuming and hard to scale up. It is not an easy task because of the inconsistencies of labels between datasets.\n- The proposed approach does not require extra manual annotations to \"align\" the annotations.  \n- The proposed approach looks generic and can be used with a lot of model architectures. \n- The proposed approach learns class-level representations (prototypes) by using a contrastive learning approach. \n- Using prototypes looks like an interesting approach to deal with the class imbalance problem. The results in the appendix show a large improvement for some of the tail classes.\n\n**Weaknesses**\n- I wonder how similar the datasets should be to lead to a good training. Is it possible to use any dataset with semantic segmentation annotations or not? For example, a lot of medical images have semantic segmentation. Is it a good idea to combine ADE20K with a medical image dataset? I think it is important the authors discuss what datasets can be combined. The authors show results on a set of datasets. I think it can be a good idea to show that this approach generalizes and can be used on another set of datasets. \n- The claim \"MDP transfers most of the computation budget to the pre-training stage, while is able to avoid cumbersome data mining for each downstream task and is beneficial for fast deployment.\" (page 2) does not seem justified. I think the authors should add results that confirm this claim.\n- The model has a lot of hyper-parameters and seems sensible to them: “We conclude that too large or too small memory bank size will cause performance degradation” (page 8). I wonder if it is possible to use the same hyper-parameters on another set of datasets. If not, how to choose parameters?\n- The hyper-parameter tau is used in multiple losses. I wonder what is the motivation to use the same hyper-parameter value. The losses have different goals so it makes sense to use different values. \n- The claim ““better intra-class compactness and inter-class separability” does not seem to be justified. I did not see intra-class compactness and inter-class separability measures. \n- I wonder how the quality of the prototypes is correlated to the downstream performances. I guess that if the model is not able to learn good prototypes, the downstream performances are not good. \n- “In this setting, pixels with the same label are treated as positive pairs, while those with different labels are regarded as negative pairs.” (page 3-4). I wonder what will happen if datasets have different levels of annotations. For example in Pascal VOC, the walls are annotated as background but not in ADE20K. I feel this definition can lead to some conflicts.\n- Overall, I feel it is difficult to know what the final model is. The paper presents a model, and then several improvements. I think it is more important to focus on the final model than the steps that lead to this model. For example, I do not think section 3.1 is very valuable and could be moved in the appendix. Some information could be also given in the introduction. \n- I wonder what pre-training is used on ImageNet because there are several pre-training procedures. [a] shows that it is possible to improve ResNet performances on ImageNet by using a training procedure used to train Transformers. \n\n[a] Wightman R., Touvron H., Jégou H. ResNet strikes back: An improved training procedure in timm. In arXiv, 2021. http://arxiv.org/pdf/2110.00476 \n",
            "summary_of_the_review": "Overall, I think there is no major technical contribution. The proposed approach combines some existing techniques, but the combination seems novel. The problem tackled is interesting and can potentially have a nice impact in the semantic segmentation community. However, the paper should be improved. There are some claims that are not well justified. It is also not clear if the proposed approach generalizes to other settings. The model is evaluated on several datasets that are quite similar. I think it can be interesting to explore what datasets can be combined, because it can improve the potential impact of the approach.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposed a pretraining method for semantic segmentation that utilizes multiple semantic segmentation datasets.\nThe paper proposes a pixel-to-class prototype contrastive loss, applies cutmix and mixup for cross-dataset images, and extend the method to semi-supervised setting.",
            "main_review": "\n### 1. Paper Writings\n\nThe paper is hard to read and should be rewrite, some examples of writing and inappropriate usage of math formulas are listed as below:\n\n1. The abstract should be rephrased:\n    1. The method name, MDP is inappropriate as it implicitly enlarges the scope of the paper. However, it only study multi-dataset training for semantic segmentation rather than for many vision tasks.\n    2. Does this paper unify anything? Why it is a unified framework?\n\n2. Caption of Figure 2 : '... different views, ... $\\overline{x}^t$ (not shown in the figure)', it is $\\widehat{x}^t$ rather than $\\overline{x}^t$ that is not shown in the figure.\n\n3. In Sec. 3: 'which would be explained in detail in the following sections' -> 'which will be explained...'\n\n4. Math formulation is weird In Sec. 3.1:\n    - If $\\mathscr{Y}$ is the set of label spaces of different datasets and $\\widehat{y}_i \\in \\mathscr{Y}$, then $\\widehat{y}_i$ should be one of the label space rather than a label map.\n    - The usage of $N$ is confusing. At first $N$ indicates the number of datasets. Then in Eq. 1, $N$ indicates the number of pixels in one image.\n\n5. The pipeline is not clearly delivered in the main text, is $\\widehat{F}$ obtained by $E_q$ and $\\overline{F}$ obtained by $E_k$?\n\n### 2. Experiments\n\nThe experimental results are tricky and need further explanation.\n1. The paper remove the auxiliary head of DeepLab v3+ in experiments, which significantly lowers down the baseline. For example, the common setting of DeepLab v3+ in MMSegmentation achieves 42.7 mIoU with ResNet-50 backbone and auxiliary head as shown in the [model zoo page](https://github.com/open-mmlab/mmsegmentation/tree/master/configs/deeplabv3plus#ade20k). However, the paper only achieves 42.69 mIoU on ADE20K and lowers down the supervised baseline from 42.7 to 39.36 mIoU.\n2. The experiments about semi-supervised training need further clarification and explanation.\n    1. It seems that no previous semi-supervised methods [1, 2, 3, 4] are compared.\n    2. Not quite understand 'we choose COCO-stuff as unlabeld data'? Does it mean the pre-training stage does not involve COCO-Stuff, and then the model is finetuned on other datasets except COCO-stuff and tested on COCO-Stuff? Is seems to be not a commonly used setting in semi-supervised semantic segmentation[1, 2, 3, 4].\n\n### 3. About Claims\n\n1. In related work: it says 'MSeg yields low accuracy and poor generalization', such a description is inconsistent with the claim in the paper of MSeg (good zero-shot performance and robust model). The paper does not even compare with MSeg and does not analyze the zero-shot performance. It also conducts experiments on a smaller set of datasets than those in MSeg. Therefore, the superiority of the paper over MSeg is not exhibited.\n\n2. Essentially, the class-to-prototype mapping and contrastive learning between pixel and prototypes is just like the commonly used formulation, per-pixel classification, for semantic segmentation. The main difference is that the prototypes is updated by the average pooling of embeddings rather than by gradient. However, the reason behind of this modification is not clear. From this perspective, the story in sec 3.1 is even unnecessary.\n\n3. The cross-dataset mixing is essentially the usage of CutMix and MixUp. Applying them to images of different datasets is not an essential novelty. Such a design is also not supported/verified by the experiments, applying them to all the images regardless their source may also bring (even larger) improvements.\n\n## References\n\n[1] Chen et al., Semi-Supervised Semantic Segmentation with Cross Pseudo Supervision, CVPR2021.\n\n[2] French et al., Semi-supervised semantic segmentation needs strong, high-dimensional perturbations, BMVC 2020.\n\n[3] Zou et al., Pseudoseg: Designing pseudo labels for semantic segmentation, ICLR2021.\n\n[4] Sohn et al., FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence, NeurIPS2020.",
            "summary_of_the_review": "Overall, the paper is not well-prepared for this venue. \nFirst, the three contributions described by the paper are not well supported: the semi-supervised part is unsolid in experiments, the cross-dataset mixing seems to be trivial, the class-to-prototype is not well explained).\nSecond, the meaning of such a task is not well supported by the experiments. Some claims in the paper are also inappropriate without support.\nThird, the paper also needs re-writing.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper presents a pretraining framework to integrate the fragmented annotations from diverse datasets for multi-dataset pretraining on the semantic segmentation tasks. To make use of cross-dataset samples, the authors propose a pixel-to-prototype contrastive learning strategy for pretraining. Moreover, the proposed framework can be extended to unlabeled data and further improve the feature representation. Experimental results show that the proposed method outperforms the pre-trained models over ImageNet on several benchmarks.",
            "main_review": "Strengths:\n1) Semantic segmentation models are usually fine-tuned based on an ImageNet pre-trained model. It is a big challenge unifying multi-datasets for training due to the annotation inconsistency and discrepant label granularity. This work introduces a new solution to unify different datasets for training regardless of the taxonomy labels.\n\n2) The authors extend the contrastive learning to pixel level and propose a pixel-to-prototype contrastive learning strategy to improve the embedding quality by pulling all the pixels in each image close to the same class prototype and pushing them apart to different class prototypes. This new learning mechanism can reduce the storage and GPU memory requirements and alleviate class imbalance, especially for multiple datasets.\n\n3) To better model the inter-class relationship, the authors propose two cross-dataset interaction operations, i.e., cross-image pixel representation mixing and pixel-to-prototype consistency regularization.\n\n4) The proposed method is evaluated on four segmentation benchmarks. The experimental results demonstrate it can consistently outperform the pre-trained models over ImageNet.\n\nWeaknesses:\n\n1) The experiments do not contain the comparisons with SOTA semantic segmentation methods on each benchmark. The reported result is weak as the only baseline is the model pre-trained on ImageNet. There is no need for multi-dataset pretraining if its performance is lower when compared with the SOTA models trained on a single dataset.\n\n2) The ablation studies are only performed on Pascal VOC. The results on the unseen dataset (Cityscapes) should also be reported.\n\n3) It is better to present more per-class performance on both seen and unseen datasets. More importantly, the results of inconsistent classes should be analyzed as it is claimed that the major challenge for multi-dataset training is the label inconsistency. \n\n4) In Section 3.4, it is claimed that the correctness continues to improve with the growth of the prototype. It is better to perform experiments to support this claim.\n\n5) Missing some related works[a, b] that also unify multiple datasets for universal semantic segmentation.\n\nQuestions:\n1) How to understand the claim in the Introduction that the pixel embeddings with the same labels enjoy better intra-class compactness and inter-class separability?\n\n2) In Section 3.1, why the pixel-to-pixel optimization strategy is sensitive to noisy annotations?\n\n3) What if the pixel-to-pixel loss and the pixel-to-prototype loss are fused to train the model?\n\n4) The legend in Fig.3 is too small. ",
            "summary_of_the_review": "The work introduces a new solution to unify multiple semantic segmentation datasets for pretraining. The experimental results show that the proposed pre-trained model is better than the ImageNet pre-trained model. However, the application value is uncertain because of the lack of comparisons with SOTA semantic segmentation methods on each benchmark. Besides, more experiments should be conducted to validate the proposed method and support the claims.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}