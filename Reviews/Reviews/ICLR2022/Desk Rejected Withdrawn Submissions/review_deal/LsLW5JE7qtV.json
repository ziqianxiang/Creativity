{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a sample level weighting approach called Multi-variation Cosine Margin (MvCoM) to address the multi-variation problem, which orthogonally enhances the conventional cosine loss function to incorporate the importance of training samples. Further, a\nheld-out meta learning schedule is used to predict the proposed MvCoM. Extensive experiments on challenging face recognition benchmarks demonstrate the advantages of the proposed method in jointly handling imbalances due to multiple variations.",
            "main_review": "Strengths: The authors have summarized that the long-tailed or imbalanced data distribution comes from multiple factors, such as ethnicity, head pose, occlusion and blur, rather than the simple volume imbalance. To address this issue, a sample level multi-variation cosine margin is proposed to caputure different importances. Moreover, a meta-learning schedule is invloved to estimate the residual terms. The ablation study is solid and sufficient.  \n\nWeaknesses: The idea of sample level weighting is not novel and has been proposed by several missing works [1,2]. In works [1,2], they consider the multi-variation as hard samples and have designed different sample level weighting strategies for face recognition. It seems that there is no essential difference. Moreover, there are many works [3,4] to handle the variation of pose, blur, illumination, expression, etc. For the meta-learning schedule, it is mainly inspired by the works Finn et al. (2017b); Jamal et al. (2020). So the novelty of this paper is not significant. The authors have conducted the experiments on several benchmarks. However, the challenge test set like MegaFace should be invloved. The authors should also compare the proposed method with the missing related works (e.g., sample level weighting methods [1,2] and pose invariant method [3]). The improvement of the proposed method is marginal.\n\n1. Huang, Yuge, et al. \"Curricularface: adaptive curriculum learning loss for deep face recognition.\" proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.\n2. Wang, Xiaobo, et al. \"Mis-classified vector guided softmax loss for face recognition.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 07. 2020.\n3. Zhao, Jian, et al. \"Towards pose invariant face recognition in the wild.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.\n4. Pearline S, Anubha. \"Face recognition under varying blur, illumination and expression in an unconstrained environment.\" arXiv e-prints (2019): arXiv-1902.",
            "summary_of_the_review": "My concern is about the novelty of this paper and the experiments. Please see the strengths and weaknesses.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed a novel face recognition method that is robust to data biases. It solved the problem by introducing a novel multi-variation cosine margin which was the extension of the cosine margin. The proposed margin was learnable variables that were trained using a meta-learning set. It was shown that the adaptive margin based on the class distribution was similar to weighting variables. Experimental results demonstrated the improved performance on the challenging face recognition dataset.",
            "main_review": "The strengths of this paper are as follows.\n+ This paper is the first work that handles multiple imbalance classes training data for face recognition, such as head pose, ethnicity, occlusion, and blur.\n+ The multi-variation cosine margin variable that gives different margin values for each data based on the class distribution. It is able to compensate for the imbalance distribution in multiple classes.\n+ The meta-learning framework that is jointly trained to learn the margin variable based on the meta-learning dataset.\n+ It shows the analysis of how the margin variable represents the weighting value for each data.\n+ Extensive experimental results that show significant performance across multiple challenging testing data.\n\nThe weaknesses of this paper are as follows.\n- Performance comparison was not done in more challenging datasets than IJB-A, such as IJB-S or Tiny Face. Accuracy reported in IJB-S and Tiny Face is 30% lower than IJB-A or LFW. Evaluation of the most challenging dataset is required.\n- It is unclear whether the proposed multi-variation cosine margin can be applied to other margin loss methods, such as ArcFace. If it is possible, the ablation study for various margin loss functions is preferred.",
            "summary_of_the_review": "State-of-the-art face recognition methods are struggling with data bias and challenging datasets, while several works only focus on a single imbalanced class distribution. This paper solves the problem of imbalance distribution on multiple classes by introducing the multi-variation cosine margin which is learned by the meta-learning scheme. It provides the required analysis and experiments that show the contribution of the proposed method. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper addresses biases arising in imbalanced/long tailed distributions due to diverse identity and non-identity based causative factors: \n* ethnicity [African American, Caucasian, East Asian and South Asian], \n* head pose [clustering 30&deg; angles in [-90, 90] range],  \n* occlusion [randomly black out the training images in block sizes of (5, 11, 17, 23, 29)], \n* blur [Gaussian kernel with kernel sizes (3, 7, 11, 15)]. \n\nby proposing a sample level weighting approach in contrast to prior works that focus on number of instances per identity. The authors thus formulate an enhanced canonical identification objective (aka cosine loss) namely Multi-variation Cosine Margin (MvCoM) with (a) meta learning differentiable framework (using hard sample mining) to iteratively learn sample-level importance weights and efficiently regularize recognition loss along with (b) an additive configuration to indicate a sample’s variation importance using the class volume margin as prior, together with other residuals as variations which hence, control the contribution of each instance in the loss function where more weights assigned to long-tail factors leading to overall smaller loss magnitudes.\n\nThe paper’s claims are as follows:\n* Technical- \n    * Multiple factors contributing to distribution imbalance has not been addressed previously within a single framework for face recognition, dealing with which results in a feature space that allows better test-time generalization and considering sample-level variation instead of class-level leads to better training.\n    * Weighted identification loss, which is commonly used in re-weighting methods, is equivalent to a learnable margin built into the cosine loss thereby representing each imbalance factor through a corresponding learnable margin.\n\n* Empirical-\n    * Extensive experiments on challenging recognition benchmarks show that the method can consistently outperform prior state-of-the-art to mitigate data imbalances across minority ethnicities, non-frontal head poses, blurriness and occlusion.\n    * Proposed MvCoM complements various backbones such as CosFace and URFace demonstrating the wide applicability to different face recognition methods.\n",
            "main_review": "**MERITS:**\n* The paper is fairly well written with clearly articulated technical and empirical contributions. The evaluation across various baselines is robust and extensive. \n\n**DEMERITS:**\n* In terms of novelty, the paper offers a limited incremental approach. Some of the approaches and ideas (other than incorporating variations) were discussed in related works in Cao et al, 2019. The idea of sampling and meta learning structure was partially explored in Jamal et al. (2020); Finn et al. (2017a) as discussed in the paper. \n* It’s unclear why the authors have opted for inconsistent mathematical notations between the main text (Section 3.1 onwards), illustrated architecture in Figure 2 and Appendix. For instance, cosine angle $\\theta$ is represented as $\\theta_{yj}$ in equation (2) as $\\theta_{i, yi}$ in equation (14). \n\n**MINOR CORRECTIONS:**\n* In Section 1: Introduction: \"Proposed MvCoM complements various backbones such as CosFace and URFace (see Table 3\" -> proposed MvCoM complements various backbones such as CosFace and URFace (see Table 3)\n* Equation 1 is expected to be: $\\ min_{\\Omega}^{} 1/N \\sum_{j=1}^{N} \\sigma_{y_j} L(f (x_j ; \\Omega), y_j ) $ (Also note, mismatch in brackets)\n* Equation 13 is expected to be: $\\ min_{\\Omega}^{} 1/N \\sum_{i=1}^{N} \\sigma_{y_i} L(f (x_i ; \\Omega), y_i) $ (note mismatch in brackets)\n*  In Section 3.2.1: Estimate the class-volume margin, \"Following Cao et al. (2019), we use the class-wise statistics as the prior for the class-volume magin\" -> Following Cao et al. (2019), we use the class-wise statistics as the prior for the class-volume margin\n*  In Section 3.2.2: Meta-learn the variation aware margin residual, \"As the class-level magin prior $m_{yj}^{cls}$ is unchanged\" -> As the class-level margin prior $m_{yj}^{cls}$ is unchanged\n*  In Section 3.2.2: Meta-learn the variation aware margin residual, “the bias information can only from the training batches not the classifiers.” has not been worded correctly \n*  In Section 7: Reproducibility, \"training schemes, varaition label preparation and the running complexity are carefully analyzed.” -> training schemes, variation label preparation and the running complexity are carefully analyzed.\n* In Appendix, A1: Sample Importance Interpreted as Cosine Loss, \"s empirically can choose\" -> s empirically can be chosen as\n* In Appendix, A3: Implementation Details, \"explained in the main paper method part\" -> explained in the methodology part of the main paper\n* In Section 3.1: Interpreting margin as sampling importance, “Usually, the denominators in Eqn. 3 are similar, that is, all close to $e^{s−m̄} + C − 1^{\\sigma_yi}$ ” -> Usually, the denominators in Eqn. 3 are similar, that is, all close to  $e^{s−m̄} + C − 1^{\\sigma_yj}$. The $y_i$ notation is accurate in the context of the appendix. \n* In Section 3.1: Interpreting margin as sampling importance, “ ${\\\\{x_j, y_j}\\\\}^N$ denotes training set with $x_j$ and $y_j$ as the samples and the label of the $y_jth$ class.” -> ${\\\\{x_j, y_j}\\\\}^N$ denotes training set with $x_j$ and $y_j$ as the samples and the label of the jth class.\n* Optionally, the authors could provide links of the different datasets used in line with [Reproducibility Checklist](https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf).\n\n**References:**\nKaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. \"Learning imbalanced datasets with label-distribution-aware margin loss\" in Advances in Neural Information Processing Systems, pp. 1567–1578, 2019.\n",
            "summary_of_the_review": "In view of the above merits and demerits, the reviewer recommends marginal reject. However, if the concerns below are addressed satisfactorily, the reviewer is open to incrementing current scores. \n\n**Questions:** \n* The reviewer wonders if the behavior of the proposed meta learned margin on average is similar to the distribution aware margins in Cao et al which states that: \"Inspired by the theory, we design a label-distribution-aware loss function that encourages the model to have the optimal trade-off between per-class margins. The proposed loss extends the existing soft margin loss by encouraging the minority classes to have larger margins\" (which is in agreement with the generalisation error bound)? \n* Cao et al states that: \"So far,we generally believe that [label-distribution-aware] approaches that modify the losses are more computationally efficient than meta-learning based approaches.\" In this context, the authors are also requested to justify choice of meta-learning architecture.\n*  The approach appears to be generalisable to domains beyond face recognition considering generic variations like affine, occlusion etc. What are the caveats in this scenario (if any)?\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper addresses the data unbalance from multiple perspectives, including pose, blur, ethnicity, and class volume. \n\nFirst, the weights in the loss function are reformulated into exponential terms (margins) with some approximations. The margins of different factors use additive modeling. Subsequently, a \"learning to learn\" strategy is adopted to obtain the margins of different factors for samples. The learning of margin employs a held-out set of data with labels of the chosen factors.\n\nThe evaluations are performed on IJB-A, CP-LFW, OC_LFW, RFW, etc. Results show improvements.\n\n",
            "main_review": "strengths\n1. Balancing different face variation factors is a novel and important issue to address.\n2. It is interesting to learn adaptive margins with meta-learning.\n3. Comparisons to SOTA show that MvCoM achieves improved performance.\n4. The paper is well-written.\n\n\nweaknesses\n1. The method description is not clear. For example\n- The approximation for the denominator in eqn 3 is not well explained. I check eqn 15 to 17 but I am still confused.\n- How T_k is re-balanced?\n- What's the t1, t2 in algorithm 1.\n- What does it mean by the sentence \"Note that update for the model \\Sigma can be rolled back to the previous iteration t-1 if the current model \\Sigma does not achieve better performance\" and the sentence \"The last two terms in Eqn 16 are O(1)\"?\n- How the attribute classifier is learned?\n\n2. The additive modeling for different margins is not well motivated. Besides, the selection of x_m in eqn 9 is also not convincing. It is possible to provide more explanation or empirical evidence?\n\n3. Many ablations are required.\n- Comparison between traditional reweighting method and the margin-based \"reweighting\"\n- Different strategies to select B_v, like select with arg min operator, randomly selection, etc.\n\n4. MvCoM requires more data with multiple attribute labels in training. So the comparison to CosFace, ArcFace, and URFace may not be fair. Moreover, the improvements are somewhat minor. I would suggest evaluating MvCoM on IJB-B and IJB-C and see there are more significant improvements.\n\n",
            "summary_of_the_review": "The problem and the proposed idea of this paper are new and interesting.\nHowever, the proposed solution is not well motivated or convincing. \nThe results are not very compelling.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes to address the long-tailed intra-class variation issue in face recognition by modeling the per-sample importance margin and employing a held-out meta-learning set to learn these margins. First, the paper shows the weighted identification loss can be transferred to a learnable margin, then proposes to learn these sample-level margins explicitly for 4 intra-class variations, i.e., ethnicity, head pose, blur and occlusions. The paper trains 4 classifiers for these 4 intra-class variations on a held-out set, mines complementary meta-learning batches online and update the recognition model and  margin residuals accordingly. The empirical results demonstrate the proposed method can be readily integrated with the CosFace method and improve on a biased training set.",
            "main_review": "The paper explores an interesting direction to handle imbalanced intra-class variation in face recognition by learning per-sample margins from a held-out meta-learning set. The experiments appear thorough and improve the CosFace method marginally. \n\nFirst of all, face recognition needs to discriminate a large number of different identities and tolerate large intra-class variations due to lighting, pose, occlusion, image quality etc. These are long lasting and well-known challenges to face recognition. Intra-class variations may be imbalanced in the training samples. Deep neural networks are exceptional to handle imbalanced intra-class variations, with the help of triplet metric learning and hard-sample mining. Early on, manifold learning was also studied to handle certain intra-class variation such as pose and lighting. Therefore, the first claim of this paper, “we are the first to address multiple factors that contribute to …, within a single framework face recognition”, is quite questionable and a somewhat overclaim.  \n\nPlease note the submission instruction about the supplemental materials (https://iclr.cc/Conferences/2022/CallForPapers ): “Authors may use as many pages of appendices (after the bibliography) as they wish, but reviewers are NOT required to read these”. Thus, the main paper within 9 pages shall be self-contained and include the necessary technical details for readers to understand the proposed approach. However, this paper provides 4 pages of supplementary material. Some critical technical explanations about “margin residual terms” after Eq.5 in Sec.3.2 and “other variation labels” before Eq.8 in Sec.3.2.2 are only available in the supplementary material. The missing technical details make the paper hard to follow. \n\nWhat are the 4 classifiers for ethnicity, pose, blur, occlusion? What kind of labels required for pose, blur, occlusion (i.e., the variation k’s labels)? What are the assumptions about the sample distributions between the meta-learning set and the training set? For example, what if there are plenty of profile faces in the meta-learning set and very few in the training set, or vice versa. The margin for the volume of samples per identity in Eq.7 is quite ad-hoc, where the hyper-parameter may be sensitive.\n\nSome of the terms are quite vague and not rigorous, e.g., “are towards balanced”, “complementary variations”, “complementary meta-learning batches”, “if the current model does not achieve better performance”, so the training expects the loss of the recognition decrease monotonically for each iteration? “the more favorable margin” means a larger margin?\n",
            "summary_of_the_review": "To tackle the data imbalanced issue for 4 intra-class variations, the paper proposes a per-sample weighting method that is formulated by learning the margin via meta-learning. This is an interesting idea to weigh those samples more with less represented variations. The major concerns are the first claim of this paper is a somewhat overclaim and some key technical details are missing in the main text, especially the justification of Eq.5 and Eq.6, and variation classifiers and other variation labels. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}