{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "\nTo study continual learning in non-stationary environments, the authors\npropose modeling the problem as a block contextual Markov decision\nprocess. The paper proposes a representation learning algorithm that\nuses the lipschitz property to allow generalization to unseen MDPs\n(through latent contexts). The proposed algorithm (ZeUS), is evaluated\non interpolation and extrapolation in various environments. The authors\nfind that ZeUS is better than similar system identification algorithms\nand learns a structured context space.\n\n\n",
            "main_review": "Strengths\n---------\n\n-   Experimental results, especially those in Figure 3, seem strongly in\n    favor of the proposed algorithm (ZeUS). The error bars are\n    surprisingly small, and so there is a large statistically\n    significant difference in the performance increase. It is also good\n    to see that the context space is more structured when including the\n    context loss.\n\n-   The theoretical results seem correct and useful, (but I have\n    detailed some concerns below). Knowing that there is a performance\n    guarantee for sufficiently similar contexts is good, and shows the\n    continuing promise of lipschitz continuity as an enforceable\n    property in RL algorithms.\n\nWeaknesses\n----------\n\n-   The paper is not well positioned relative to other algorithms /\n    baselines. There is a large literature that this paper draws on:\n    contextual MDP, contextual decision processes, continual learning,\n    meta-learning, system identification, etc. Some of these do not seem\n    particularly relevant, such as the inclusion of a contextual\n    observation per state (which i detail below). Taken together, it is\n    difficult to discern the overall significance of the presented work.\n\n-   While the theoretical results in section 4 are comprehensive, there\n    is little insight and connection between the theory and the rest of\n    the paper. For example, the context distance term is approximated in\n    an unclear way. The loss being optimized bears only little\n    resemblance to the task distance in the definition. These issues add\n    up to call into question the contribution and importance of the\n    algorithm's theoretical underpinning.\n\nDetailed Comments\n-----------------\n\n-   Foot note page 2: This does not seem like an advantage without\n    further qualification. Of course there is no forgetting when there\n    is also no learning, but this can be said of any model that does not\n    perform parameter updates. What is important is that the model is\n    able to adapt even without parameter updates. While you state that\n    this is the case, it does not communicate why or how this is\n    achieved.\n\n-   \"Further, our model can be verified after training… We do not\n    perform this type of formal verification, as current verification\n    methods on neural networks only work for very small models, and are\n    expensive to run\"\n\n    I do not understand what is being said here. Can the model be\n    verified or not? Is this an advantage if it cannot be realistically\n    done?\n\n-   Definition 2: What is the difference between the shared observation\n    space $\\mathcal{O}$ and the context observation space\n    $\\mathcal{O}^c$?\n\n-   Section 2: Block MDP is not a relaxation of a contextual markov\n    decision processes (CMDP), which provides a context for an MDP\n    episode. Block MDPs are relaxations of contextual decision processes\n    (CDP), which provide per-state contexts. The naming is confusing,\n    but these are very different ideas.\n\n-   Page 4 robot example: the observation space is the same and\n    specified by the camera that the robot uses.\n\n-   \"Note that Definition 3 is not a limiting assumption because we do\n    not assume access to the context variables c1 and c2, and they can\n    therefore be chosen so that the Lipschitz condition is always\n    satisfied. In this work, we focus on a method for learning a context\n    space that satisfies the above property.\"\n\n    Although the true underlying context is not available, it does not\n    seem correct to say that the c_1 and c_2 can be arbitrarily chosen\n    to satisfy definition 3, as these contexts are also used in the dynamics\n    model. There may be degenerate solutions that satisfy the definition.\n\n-   Section 4, Paragraph 1: Why is it that the results to hold true for\n    any state or observation space? If this is the case, what is the\n    need for the BC-MDP formalism introduced in the previous section?\n\n-   Section 4, Assumption 1: I think there is a third assumption\n    implicit here. The transitions are determined by a policy, and\n    identification can depend on the policy. Hence, shouldn't there be\n    an exploration requirement as well? This is partially addressed by\n    definition 5 for theorem 2, but exploration (or some policy\n    dependence) should still be necessary for identifiability.\n\n-   Section 5, Equation 2: Context 2 only appears in the first term\n    (context-loss). Shouldn't the dynamics and reward terms have\n    additional terms that optimize the reward / dynamics with respect to\n    context 2?\n\n-   Section 5: \"Specifically, we train a transition dynamics model and a\n    reward model (via supervised learning) and use their output to\n    approximate d(c1, c2).\"\n\n    It is not at all clear how this distance is estimated using either\n    definition 3 or 4. Do you use the learned models to approximate the\n    individual distances across contexts and then add them? If so, there\n    are many sources of error: the approximation error of the models,\n    the sampling procedure (which approximates the max), the inner\n    optimization of the wasserstein loss, etc. Definition 3 is also\n    scaled by unknown constants and the bound most hold uniformly over\n    all state-action pairs. It is not clear that such a noisy target\n    would be helpful for learning with Equation 2.\n\n-   Section 5, ZeUS Architecture: I understand that the assumptions of\n    identifiability in Section 4 use just the last k transitions. Since\n    the implementation differs from the assumptions, I wonder why the\n    context encoder still only looks at k transitions? Why not the\n    entire trajectory?\n\n-   Section 6, Figure 3: Are the transition dynamics changing in each\n    plot? For example, is the cheetah torso changing every X number of\n    environment steps? Where does this occur on the x-axis? I am\n    surprised to see relatively monotonic improvement during training,\n    given that there are parameter updates during training on different\n    tasks (and hence, catastrophic forgetting can occur during\n    training).\n\n-   Section 6, Figure 4: Although the reward is bounded, I think that\n    aggregating different reward functions in one return plot is not\n    necessarily correct. One reward function could be harder to learn\n    and hence the return will be overall lower than another \"easier\"\n    reward function.\n\n-   Section 6, Figure 5: From these plots, it does seem that the context\n    loss does lead to more structure. However, it is puzzling that the\n    largest differences aren't between the smallest and largest index,\n    but between interior tasks (such as between 6 and 4 in the left\n    plot). Task index 7 in general has relatively lower pairwise\n    differences compared to other indicies.\n\nMinor Comments\n--------------\n\n-   Page 3: T environment's -\\> The environment's",
            "summary_of_the_review": "The paper proposes a novel representation learning architecture for the\nproblem of learning from a sequence of MDPs, but there are a number\nissues that hold the paper back. The theory, while welcome, is\ndisconnected from the proposed algorithm on a number of levels and does\nprovide much overall insight into the problem of generalization to\nunseen contexts. There are a number of design decisions in the actual\nalgorithm that are unjustified or confusing. I have detailed all these\nin the \"Detailed Comments\" section. My decision, therefore, is to lean\ntowards rejection.\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work shows how Lipschitz conditions can help generalization for block contextual MDP (BC-MDP). The theoretical analysis showed that when the BC-MDP satisfies the Lipschitz conditions as in Def.3, the values in MDPs with similar contexts can be similar (Thm.1&3). An objective function (Eq.(2)) is proposed based on the theory, and experiments on several continuous control tasks show that the proposed method, Zero-shot adaptation to Unknown Systems (ZeUS), can outperform existing alternatives.",
            "main_review": "Strengths\n\n- Clear writing except for some parts of the paper\n- The theoretical analysis brings new insight to generalization across contexts\n\nWeaknesses\n\n- There is a noticeable gap between the theory and the proposed objective function\n- Important implementation details are missing\n\nThis work provides several theoretical insights into continual RL problems based on BC-MDP and Lipschitz conditions. Experiments show that context loss is essential to achieve good performance.\n\n1. The exposition and mostly clear and easy to understand, except for the following parts:\n\n- Theorem 2, what does it mean for an algorithm to be Lipschitz?\n- Theorem 3, the two Q functions are not defined in the main text (although the meanings can be found in Appendix E)\n- What is the \"task metric loss\" mentioned after Eq.(2)?\n\n2. The main concern is that the theoretical analysis and actual implementation are far apart\n\n- There is no incentive in Eq.(2) to ensure that the learned context space with metric is indeed Lipschitz (Def.3).\n- After Eq.(2), d(c_1, c_2) are approximated using reward/dynamic models. However, Def.3 is only an upper bound. Even if the reward and dynamic distances are small, the context metric can still be big. Additionally, it is unclear how the reward/dynamic models are actually used to approximate the context distance, even after browsing through some of the details in the appendix.\n\n3. Experiments\n\n- Just for clarity, is the reward function known or learned for the varying the transition dynamics setting? What happens if the reward function is also learned for the varying reward setting?\n- It is claimed that ZeUS is a zero-shot adaptation, but the figures are showing environmental steps in the x-axis. Can you clarify the experiment protocol?\n- The context distance in Fig.5(left) is not really \"consistently closer\" as claimed in the text. For examples, tasks (0, 7) is somewhat more similar to tasks (0, 6), and tasks (4, 6) versus tasks (3, 6).\n\nMinor comments\n\n- In Def.3, what does the norm mean when the reward is a scaler?\n- Theorem 2: T should be t?",
            "summary_of_the_review": "Overall, the theoretical analysis provides novel insights, but the actual implementation is far from the theory.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors study the problem of continual learning in RL. The paper is concerned with the problem of contextual block MDP. In this setting, an agent interacts with a stationary MDP for a while and the MDP changes once in a while. The authors also assume that the MDP has some rich state structure where a rich state can be mapped to a smaller state.\n\nThe paper proposes a deep learning method that has two main structures, 1) an encoder which given H time step of agent-environment interactions infers the context of the environment 2) and an encoder that maps rich observation to the smaller state. The outcome of the context encoder and observation encoder are then fed to the policy model to generate an action.\n\n",
            "main_review": "The paper is well-motivated but the authors might find it useful to explain their algorithm in more depth in the main text. It is not that clear what is the algorithm. The authors provided a very high-level and abstract explanation but were not sufficient. The authors also need to motivate their assumptions. Also, they need to make a connection between their empirical approach and their theoretical statements.\n\n--- A few points that the authors might be able to directly address during the rebuttal phase: \n1- In definition 4, do the authors prove that d_task is indeed a distance? If not, the authors are encouraged to provide proof. \n2- In Theorem 2, what does it mean that an algorithm to be Lipchitz? the authors are encouraged to provide a definition. \n3- After equation 2, what does \"stopped gradient: mean?\n\n4- Well written\n-- Further concerns and suggestions: \n5- After reading the main part of the paper, it is still not clear how the learning works and how the learned algorithm works. \n5a- It seems, there is an encoder from experiences to context, and an encoder from rich observation to a state. Then a policy network uses these to produce an action. This is very abstract. A more detailed explanation is useful for how all these pieces and steps exactly happen. \n5b- How does the learning happen? what is the training algorithm? It seems there is a model/dynamics learning component that is not discussed in the main text. \n5c- \"transition dynamics model and a reward model (via supervised learning)\" how does this step happen? how the probability of the next state given the current state and action is learned? it is not discussed at all. \n6- It is not clear how valid are some of the assumptions. For example, assumption 1 is quite strong. Please motivate it.\n7- A high-level comment, the authors' don't need to reply to it, but the theory in this paper seems a bit disconnected, it is quite challenging to see how to put the statements together and build a unified understanding. What do the results tell me? \n8- Following this line \"These learned models allow us to construct a new MDP that is εR, εT , εc-close to the original\" how the supervised learning happens not knowing the context? The algorithm seems to need the learned dynamic model to learn the context encoder, but identifying a context seems to be needed to learn the dynamics to begin with. Probably I am missing something. \n9- It seems rho does not show up in theorem 2, it seems a bit odd. Should the result depend on rho? \n10- The experiment study seems to do not advocate outstanding superior performance. \n11- When does the MDP change? how the algorithm infers any change?\n\nI may have been wrong in the above statements.\n",
            "summary_of_the_review": "Disconnect in the theoretical pieces. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper studies continual reinforcement learning setting through the block contextual MDP framework. Contextual MDPs assume a distribution of MDPs such that there is a mapping from a context to the reward and transitions. Block contextual MDPs furtherer assume that the observations are also a function of the context. The theory section gives bounds on the performance of the optimal policy for a new context c2 given a policy that is \\epsilon optimal for context c1 and given that c1 is close to c2. The authors then propose a representation learning algorithm that learns a task representation (learns the context) as a function of history via a model based loss function. The experiments compare this method with some baselines in a continual RL setup. ",
            "main_review": "This papers studies an important problem of non stationarity in RL via the contextual MDP framework. This is a nice idea and there are some nice results in this paper. However, there are many missing parts, questions about some of the assumptions, missing algorithm description and questionable experiments. More work has to be done before it will be ready to published, but there are good results here and I encourage the authors to improve this paper. I hope that my more detailed comments below will help. \n\nI would like to start from the main claim of this paper: to study non stationarity via contextual MDPs. This is. nice idea, but I would like to see some definitions about it: is there a distribution over contexts? is it changing over time? is it changing in a smooth way? are there \"k\" abrupt changes? without giving a formal definition for that, it is hard to understand the connection to contextual MDPs, the algorithm, and the empirical setup. For example, what exactly is the proposed approach? to use the policy from previous tasks on a new task assuming that it didn't change much and assuming that it is close to them from continuity? I really can't find a single (or multiple) story in this paper that explains me how the algorithm is dealing with non stationarity and what the theory is saying on that. This is also why I find the experiments confusing but I will add on this later. \n\nI don't understand Figure 1, and I am missing a discussion about it. What is the difference between the arrows, why is a new context sampled at each time step? if the context is just a state at a different time scale, then maybe the problem that is studied in this paper is more about a factored MDP. \n\nThe literature review section is highly confusing. It is trying to cover too much, too early in the paper, and it cites too many irrelevant papers. It is ok to expand more in the appendix (as the authors have already done) but this section really confuses me. For example, I don't see right away what is the connection between meta RL to contextual MDPs. It might be easier to understand this connection from the experiments that were performed, but at the beginning of the paper I really don't see it. I would much prefer to have the connection explained to me over seeing numerous citations to papers at this point. At the same time, I would like to see much more explanation on the topic of block contextual mdps, which seems to be the topic of this work. In particular, I am missing some literature review on rich observation MDPs (there are many papers on this topic from MSR NYC, e.g., [2],[3],[4]). Finally, I think that a much more detailed discussion on the two papers by Modi. et al would be appropriate, and other papers on contextual MDPs should be at least mentioned (e.g. [5]). \n\nTheory section. \nThe main problem with this section is that it is giving bounds on what would a good context space would give me, while there is no result in this section about how to learn such a context space, which is what the algorithm and the experiments are doing. This was very confusing for me while I was reading the paper. In contrast, in contextual MDPs, the context is usually observed, the agent is learning a mapping from this context to the reward and the transitions, and uses this mapping for zero shot transfer (with PAC or regret results, e.g., in Modi et al.). \n\nAnother high level issue is that all of the results in this section are given for tabular representations, while the paper claims to study block contextual mdps. This is an over claim in this case as the block part extends contextual mdps by introducing a mapping from context to observations, but this mapping (and the observations in general), are never studied. \n\nTheorem 1 should be a corollary, as it is a straight forward application of the simulation lemma [1].  \n\nAssumption 1 seems to be impractical. The authors should try and give an example for a setup where it might hold. In particular, some assumption about the learning algorithm and/or the data distribution in those k steps that the agent takes, and a variable \\delta that measures the probability to identify in k steps. It seems that Defintion 5 and Theorem 3 are going in the direction of defining the data distribution but the discussion was not clear to me. Also note that Theorem 3 is with high probability while Assumption 1 is not. \n\nTheorem 3: bounding the rewards is loosing generality. That is of course a standard class of rewards in RL, so why state that it is without the loss of generality? \n\nTheorem 5 is not really a generalization bound, it is more of a transfer bound. What this theorem says essentially is that for tasks that are close to each other, an approximate policy on one task is also an approximate policy on another task, which is again an application of the simulation Lemma. A generalization bound is usually giving us an upper bound on any future context c from some class, after seeing some n contexts from this class. \n\nSection 5. \nThis section is describing the algorithm but is missing a lot of details. Equation 2 is confusing as \\phi and \\psi were not defined yet. Learning the context from history is not straight forward to me, because the context is usually independent of the experience collected in a task. I would like to see something being said or defined on that, i.e., that such a learning process can actually work in a tabular environment. It is also not clear to me how the policy agent is using the representation. \n\nSection 6. \nIn the experiments section, I am missing a description of the non stationarity setup and how the Zeus algorithm is dealing with non stationarity. If this is the main point in the paper then I would like to understand it exactly. I learn very little from the comparison to the baselines, this seems like an independent contribution. Instead I would like to see some connection to the theory part, and ablations, that explain me what are the important components of the algorithm and how sensitive they are to the non stationarity.  \n\n[1] Kearns, Michael, and Satinder Singh. \"Near-optimal reinforcement learning in polynomial time.\" Machine learning 49.2 (2002): 209-232.\n\n[2] Du, S., Krishnamurthy, A., Jiang, N., Agarwal, A., Dudik, M., & Langford, J. (2019, May). Provably efficient RL with rich observations via latent state decoding. In International Conference on Machine Learning (pp. 1665-1674). PMLR.\n\n[3] Dann, Christoph, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E. Schapire. \"On oracle-efficient PAC RL with rich observations.\" Advances in Neural Information Processing Systems 2018 (2018): 1422-1432.\n\n[4] Jiang, Nan, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E. Schapire. \"Contextual decision processes with low Bellman rank are PAC-learnable.\" In International Conference on Machine Learning, pp. 1704-1713. PMLR, 2017.\n\n[5] Belogolovsky, Stav, Philip Korsunsky, Shie Mannor, Chen Tessler, and Tom Zahavy. \"Inverse reinforcement learning in contextual MDPs.\" Machine Learning (2021): 1-40.",
            "summary_of_the_review": "Nice idea but not ready for publication. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}