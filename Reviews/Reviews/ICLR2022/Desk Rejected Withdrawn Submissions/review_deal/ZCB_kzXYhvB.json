{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes to improve the CFG framework for training the discriminator in GANs by adding a Wasserstein regularization. Their method was motivated by exposing the problems of ICFG, namely high sensitivity to hyperparameters. The paper motivated their method by exposing the problems of ICFG, namely high sensitivity to hyperparameters. Varying these hyperparameters leads to lower image quality as well as mode collapses. The paper ran four GAN methods across a good range of benchmark datasets.",
            "main_review": "The paper structure is clear. In the experiments section, the selection of the benchmark datasets is extensive with a good coverage. \n\nIn Sec. 3.1, the paper provides a motivation for improving the existing ICFG framework by exposing its problem of sensitivity to hyperparameters. The empirical evidence presented in Figure 1 contrasts using lr=0.0025 and 0.000025, of two orders of magnitude in their difference, and using delta= 1 and 0.1. I'm not familiar with GAN training but these hyperparameters seem so different from each other that the argument of over-sensitivity based on this piece of evidence looks very weak to me, especially for a fixed time horizon T.\n\nThere are some technical inaccuracies presented in the paper. For example, about Eq. 3, the paper states that \"d(x,y) stands for the distance between p and q\". How does this involve variables x and y then? What is the distance metric? d is never specified. Additionally, the paper only discussed the L1 Wasserstein distance in the 1-dimensional space R, which is commonly called the earth movers distance, without explicitly stating the setup of Sec. 3.2 and it is by no means a general form of the Wasserstein distance. In Eq. 5, where is f??\n\nIs Sec. 3.3 the intuitive explanation of the effect of using Wasserstein regularization in training the discriminator? How is it supported by either theory or empirical evidence?\n\nThe experiment evaluation metrics using the inception scores and Frechet distances show little difference between ICFG and ICFGW.\n\nWriting and typos:\nThere are many, many typos & grammatical errors in the paper. The paper is poorly written and far from publication standards. Just to mention a few below, \n1. Fig. 1 caption: \"season\", \"weakness ability\".\n2. Missing or multiple citations parentheses throughout the paper.\n3. \"Monge\": missing full name and citation (Sec. 2.2 first line). \n4. comma between two full sentences and typos in \"The formulation of origin Wasserstein distance...\". Captal letter \"J\" in joint, etc (first paragraph of Sec. 3.2).\n5. Notation of the distribution gamma in Eq. 3: what does [p, q] stand for under inf?\n6. \"Kantorovich\" : full name and citation missing.\n7. \"import\" in the second sentence of Sec. 3.4.\netc. etc. ",
            "summary_of_the_review": "The paper is not ready to be published for the reasons listed above in the main review. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "* This paper introduces the Wasserstein regularization to the composite functional gradient (CFG) learning for GAN training.\n* This paper shows the reason why the original CFG struggles on classifying real and fake images.\n* Experiments show superiority of ICFG over the original CFG in MNIST, EMANIST, FashionMNIST, CIFAR10, SVHN and LSUN tower/church.\n",
            "main_review": "### Novelty\n\n(-) Adding Wasserstein regularization to CFG is incremental.\n\n### Soundness\n\n(-) Wrong statement\n- StyleGAN does not use Wasserstein distance.\n\n(-) Missing references on stable architectures and regularizations\n- StyleGAN2, StyleGAN3, ADA, Which GAN training methods do actually converge?\n\n### Impact\n\n(-) The proposed method is hardly beneficial to the research community.\n- It is straightforward combination of existing two methods.\n- Effectiveness of the proposed method is not evaluated on recent benchmarks (256~1024 resolution images, e.g., FFHQ, AFHQ, LSUNs)\n  - LSUNs in this paper look 32x32 (not mentioned though).\n- Wasserstein distance is obsolete in recent GAN literature.\n\n###  Minor\n* Multiple typos and informal abbreviations in the experiments section.\n  * , , in 4.1.\n  * (FID) of. in 4.4.\n  * lr\n* the origin CFG architecture\n  * origin --> original\n  * ~~architecture~~",
            "summary_of_the_review": "This paper needs a fresh literature survey including baseline architectures, losses, and datasets.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper tries to improve the performance of the Composite Functional Gradient (CFG) learning approach in Generative Adversarial Networks (GANs). Although this approach presented its robustness on various datasets, it is hard to say how novel the main approach is. It would be great for the readers if the author could provide a more in-depth reasoning or mathematical background. ",
            "main_review": "First of all, it is easy to see the effectiveness of this method based on experiment results over various datasets. Also, Figures 2-8 helps the readers to expect the effects of this approach. \n\nHowever, regardless of the final result, it is challenging to catch the novelty. For instance, applying spectral normalization over various GAN formulations, like WGAN-GP[1] + InfoGAN[2] setting, is quite common in the computer vision domain, so it is hard to catch the selling point. In this regard, it would be great if the readers could see more rigid mathematical proof or more in-depth reasoning of applying Wasserstein Regularization over CFG. \n\n[1] Gulrajani et al., Improved Training of Wasserstein GANs, in NeurIPS 2017.\n\n[2] Chen et al., InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets, in NeurIPS 2016.\n\n\n----\n\n\nAs a side note, I have some recommendations for this manuscript for better readability:\n- Pg2. 2.2: It would be better to cite what Monge, Kantorovich, and JS-divergence are.\n- Figure 1: It would be better to note 'icfg' in uppercase (e.g., Section 3)\n- For some citations, you may want to consider the '\\citep' option instead of '\\citet' (e.g., the first line of Section 3.1 in Pg 3). You can also add space by using the '~' symbol (it also applies to many citations in Section 4.1, 4.4, and 4.5.1).\n- First equation in Section 3.1: I recommend adding the equation number (it is hard to point this equation on Page 4 as well) while adding the description about f. Also, I think there is no N used in that equation.\n- From the second paragraph of Section 3.1: there are many 'lr' in the paper. Although it is common to the deep learning field, I believe it is better to describe what the 'lr' is, at least for the first time when it appears.\n- Pg 4. 1st paragraph: Joint probability -> joint probability.\n- Pg 4. Section 3.4: 'cfg-eta' I failed to find the description about CFG-eta before it appeared in this section. It also applies on Pg 5, Section 4.2, for both 'cfg-eta' and 'cfg-alpha.'\n- Pg 5. Section 4.1, first line: \"MNIST(LeCun et al., 1998), ,Fashion\" -> \"MNIST (LeCun et al., 1998), Fashion\".    \n- Pg 5. It would be better to use an official name/notation if possible. e.g., the fifth line in 4.1 FashionMNIS -> FashionMNIST; Many names in Section 4.3.\n- Pg. 6. Section 4.4: \"(FID) of.\" -> \"(FID)\", \"net- work;\" -> \"network\".  \n- Pg. 6. Section 4.5.1: \"IS scores\", \"url\" -> \"URL\".\n- Table 4, 7: I still believe that the author should present the result of WGAN and LSGAN for the Inception score. \n",
            "summary_of_the_review": "It might be true that applying the Wasserstein Regularization is the must-do thing to make the original CFG approach works in a general setting. However, I failed to find any solid supports or analysis from this paper. Although the conclusion could be proper, such support needs to be followed, considering recent works that commonly apply spectral normalization to the GANs.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I failed to find any issues.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}