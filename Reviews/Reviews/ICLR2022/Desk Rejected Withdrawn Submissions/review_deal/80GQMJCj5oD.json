{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, the authors consider Bayesian model selection, i.e., the model's marginal likelihood maximization approach, as a way to properly optimize the model's hyperparameters even when sufficient validation data is not available. Since the maximization of the marginal likelihood involves the computation of the Hessian matrix of the log-likelihood function, an efficient way to compute it is essential for models with a very large number of parameters, such as deep models. \nThis paper compares and evaluates four methods for approximating the Hessian matrix (more precisely, its approximation, the Generalized Gauss-Newton matrix or Fisher Information matrix): the diagonal matrix approximation method, the K-FAC approximation method, the Chebyshev polynomial approximation method, and the Stochastic Lanczos Quadrature method.\n\nFirst, experiments are conducted to evaluate the approximation accuracy of the four methods for an exact Hessian matrix or GGN. The authors then evaluate the test error for each of the semi-supervised learning problem and the learning problem in the presence of label noise when the hyperparameters are optimized using a gradient-based optimization method with Hessian matrix approximation.",
            "main_review": "strength \n\n- This paper conducts the comparrison of several possible approximation methods for the Hessian matrix, which can be used as evidence to decide which one should be used in practice.\n\nmajor concerns (weakness)\n\n- The authors' technical contributions are unclear. Could you clarify the contributions other than the part where listed existing approximation methods for Hessian matrices and performed comparison experiments.\n\n- The comparison results of the four approximation methods are unclear. What does the plot of train loss vs. test loss on the left side of Figure 1 make a statement about? For each approximation method, I understand that the right plot evaluates the approximation accuracy of the Hessian matrix itself, but I don't understand what the left plot evaluates. Intuitively, I think it would be appropriate to show a plot comparing the test errors of MLP and LeNet with the hyperparameters optimized using each of the four approximation methods.\n\n- In the semi-supervised learning experiments and the label noise experiments, the authors use the fixed hyperparameter values reported in existing papers as the baseline. However, since there is no guarantee that these hyperparameter values are properly determined, it cannot be conclude that gradient-based optimization is useful based on the result obtained by this comparisons. In this case, I think that the baseline should be the test error when the hyperparameters are determined using the validation data. ",
            "summary_of_the_review": "The technical contributions of the authors are unclear, and I think some of the experimental settings are not appropriate. Therefore, I cannot support the acceptance of this paper.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Authors propose to use Bayesian statistics to calibrate hyperparameters of deep learning models. The main idea is to estimate a second order statistic (which would be too computationally heavily for a neural net). Experiments are proposed to semi-supervised learning and noisy labels. The paper is overall well written, but might lack of experimental comparison.",
            "main_review": "The paper is clear and the idea is very interesting. However I have concerns about the experimental validation of the proposed method.\nI am not an expert of hyperparameter optimization for deep learning, but it seems that some methods have been proposed and should be compared.\n\n1- Is there a specific reason why authors chose not to include criterion such as k-fold cross-validation solved with gradient-based methods (using [1] for instance)? Even if k-fold CV is very slow, or lead to poor performance, it looks like a standard benchmark to have (at least training with a hold-out loss). It would be valuable to compare the generalization performance and time improvement of the proposed method with respect to cross-validation.\n\n2- This could be interesting to compare the performance with other criterion as well, such as proxy of the SURE criterion [2].\n\n3- I know that recent criterion were also introduced to handle limited amount of data, such as [3] or [4], could authors comment on this? I think at least [3] is implementable for deep learning (this might be very slow). I do not know if [4] is implementable for deep learning tasks.\n\n4- Maybe it would interesting to display the resulting algorithm of the proposed method, for clarity and easier reimplementation.\n\n[1] J. Lorraine, P. Vicol, and D. Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. In AISTATS 2020.\n\n[2] C. A. Deledalle, S. Vaiter, J. Fadili and G. Peyré. Stein Unbiased GrAdient estimator of the Risk (SUGAR) for multiple parameter selection. SIAM Journal on Imaging Sciences 2014\n\n[3] K. R. Rad and A. Maleki. A scalable estimate of the out‐of‐sample prediction error via approximate leave‐one‐out cross‐validation. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 2020\n\n[4] K. Lounici, K. Meziani and B. Riu. Optimizing generalization on the train set: a novel gradient-based framework to train parameters and hyperparameters simultaneously. 2020\n",
            "summary_of_the_review": "The idea is interesting, the paper is clear, but the experimental validation might be limited (however I am not an expert in hyperparameter optimization for deep learning). I voted for weak rejection because I would expect at minimum a comparison with hyperparameters selected with cross-validation.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a new approach for hyperparameter optimization (HPO) that aims at comparing different configurations without validation data but instead by resorting to empirical Bayes. Since doing so requires computing a Hessian with respect to the parameters of the optimized model, this can be infeasible for deep neural networks. To circumvent this, the authors study different log-determinant estimators and compare to the ground truth on small networks. Next, they apply their proposal to HPO with limited labels.\n\n",
            "main_review": "**Clarity**\n\nThe paper's writing is generally clear but could be improved. For instance, the abstract tends to make vague claims (e.g., \"especially for limited labeled data, is important but difficult, because obtaining enough validation data in such a case is practically impossible\") and some sentences need to be revisited (e.g., \"Indeed, we can select models, in other words, compare\"). On the other hand, the paper does a good job in Section 2 at giving enough background information for the reader unfamiliar to Bayesian model selection and information criteria.\n\n**Reproducibility**\n\nNot only do the authors carefully provide details on the baselines, benchmarks and experimental setup, but they will also open source their code upon acceptance. While it would have been even better to make this available in anonymous form to the reviewers, overall the work meets a reasonable bar in terms of reproducibility.\n\n**Novelty**\n\nWhile the focus is an application of Bayesian model selection to HPO, which is not common as far as I am aware, novelty is limited. The work is incremental as it mostly employs existing and known tools, such as the Laplace approximation. That said, the problem of performing HPO with limited validation data is meaningful and the paper takes a meaningful approach to it.\n\n**Experiments**\n\nI found the experimental section to be the main weakness of the paper. In particular, I would have liked to see a wider selection of baselines to get more insights into the proposal. For instance, section 5.1 works in the setting of limited labeled data. How would the proposed approach compared to standard random search or Bayesian optimization which uses the available labels, no matter how scarce they are? I believe this should be a key comparison since the main point of the paper is to apply the Empirical Bayes techniques to hyperparameter optimization.\n\nFurther, the experiments in Table 2 and Table 3 only report test error rates but no error bars. The authors should report average performance and error bars, otherwise it is not possible to tell whether the improvements are significant and rising above the level of noise. In section 7.2 it is claimed that 3 random seeds are used. I would encourage the authors to also increase the number of repetitions to draw more robust conclusions. I would have also liked to see experiments on a broader range of problems, both models and datasets.\n\nMinor:\n1. I think \"HPO\" is a more common way to refer to hyperparameter optimization than \"HPO\"\n2. The organization of the paper is slightly unusual. I would just move Section 7 on reproducibility as part of the experimental section and before the conclusion.\n",
            "summary_of_the_review": "Overall, I am (weakly) inclined towards rejection. The paper explores a promising research direction and I encourage the authors to build on it, but at the current stage the paper needs stronger and broader experimental evidence.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work considers a setting where labelled data are limited, thus the HPO cannot rely on the performance of validation set to guide the hyperparameter search. Under this condition, the authors propose to use Bayesian Model Selection (BMS) where the hyperparameters are selected to maximize the marginal likelihood (1st contribution). \n\nTo compute the likelihood, one needs the log determinant of the Hessian matrix which can be infeasible to compute for modern deep neural nets as the number of parameters are very large. Thus, the authors compared 4 approximation methods for the log determinant of the Hessian on 2 small sized networks (2nd contribution). \n\nThe best approximation methods are used in BMS to find 2 hyperparameters of a semi-supervised learning algorithm on 3 common computer vision datasets where only a fraction of the labelled data points are used. The authors show that BMS with approximated log determinant of the Hessian can find better models than the default hyperparameters and the procedure is robust to label noise (3rd contribution). \n",
            "main_review": "Strengths\n\nThe paper addresses an interesting problem: Can we do Bayesian Model Selection for modern deep neural nets (DNN)? As mentioned by the authors, they are useful when the validation set cannot be used. What’s more, I also think this is appealing when the DNN is used as an inner routine of a bigger AI system for simplicity where one can find hyperparameters and parameters of the NN during a single training. \n\nThe work provides a good overview of the methods to approximate the log determinant of the Hessian and the whole paper is clearly written and well structured. The authors provide enough detail for reproducibility. \n\nWeakness\n\nThe empirical evaluations can be improved. Even in the limited data points setting, one can still split the labelled dataset into train and validation sets. How does the proposed BMS method compare to the methods that actually use the validation datasets？Following this, how do these two families of algorithms (based on validation set or BMS) behave when we change the number of labelled data points from small to large? Is there a point that one should switch from BMS to using a validation set?\n\nIf possible, I think it is also useful to know in the large-scale supervised learning setting, how does the BMS based methods compare to using the validation set? How large is the gap if there is any? I am not aware of such results.\n\nAnother perspective is the size of the neural network. Do these approximation methods behave differently for neural nets of different sizes? How good the BMS methods will be for these neural nets with varied sizes?\n\nIn the end, only one SSL task is included in the experiments and it is hard for me to be convinced with just a single example. I suggest the authors test the methods on more tasks.\n",
            "summary_of_the_review": "As the technique contributions are very limited (it is a novel application of existing techniques). I think the work needs much stronger empirical results to be accepted in a venue such as ICLR. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}