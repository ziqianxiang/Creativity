{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, the author has mainly focused on two issues: one estimation of explanation which describes class behavior (faithfulness), and second sensitivity of the explanation (robustness). To solve this, the author proposes an uncertainty-based explanation method (Variational perturbation). The method learns the distribution of feature attribution and classifier outputs for each image.   The author evaluated the proposed model using  MNIST, CUB, and ImageNet datasets.  The author visualizes the proposed techniques for various datasets. The author claims an improvement of qualitative results for the proposed method.",
            "main_review": "Strength:\nThe main reason to accept this paper is empirical results, showing performance on the various methods. \n\nWeaknesses of the paper:\n\na. The paper does not discuss the relationship between faithfulness and variational perturbation. \nHow does the proposed method helps to improve faithfulness?\n\nb. The author can show analysis as mentioned on the robustness of the explanation improves using the proposed method.\n\nc. What is the local perturbation method in this paper? In the literature, many explanation methods discussed perturbing input by adding variants of noises. How is this perturbation different from the previous. \n\nc. The author should discuss the “Top k operator” in detail. \n\nd. How did you select “biggest k% attributions”? Is there any analysis? Can you please mention the list of attributes for an image and provide a few such sample examples? The author also shows the distribution of each feature attribute for each image, as discussed in the abstract. Also, the author shows how this distribution of feature attributes helps to improve faithfulness and robustness.\n\ne. How do you get these “unselected regions” in an image? The author can provide details about this.\n\n\nf. “ The classes with predictive probability to examine the classifier’s behavior itself”.\n How do you know about predictive probability and classifier behavior? It depends on the complete model and type of perturbation\n\ng. The method section is not written carefully. The terms are not discussed in more detail. The loss function and other details about the model are not discussed in the paper. The author should put more focus on structured writing in this section.\n\n\nh. In the abstract, the author uses an uncertainty-aware explanation method, how it is different from Uncertainty Class Activation Map (U-CAM)[4] in terms of robustness and faithfulness of output. \nIn this paper, the author proposed a Bayesian framework to obtain an explanation map, which contradicts the claims in the abstract.\n\ni. The literature survey section in this paper is not so interesting to continue the flow. There should be a detailed literature survey on the perturbation-based methods[1,2,3,4], uncertainty/Bayesian-based methods for explanation schemes as discussed in [4]. Many more famous gradient-based methods, such as Grad-CAM, Grad-CAM++, U-CAM, Guided Grad-CAM, etc., are missing in the papers. \nAs per the understanding,  a blurred image is not a masking technique or a suitable perturbation method.  Can you please elaborate on this work (blurred image)?\n\nj. Petsiuk, et al. proposed a RISE-based method to perturb the image. Can you show attributions using the RISE-based local perturbation method? How good the proposed method is in terms of faithfulness and robustness from the RISE method.\n\nk. The visual feature attribution-based method is not discussed in the paper. How does the proposed method help to find visual attributes?  Please discuss more details on it.\n\nl. The author could motivate the problem statements, that is, how does variational perturbation help improve visual attributes.\n\nm. It is not clear that KL divergence in equation -4 helps to improve visual attribution.\n\nn. The author should compare results with the LIME [1], SHAPE[2], LOO[3], and Occlusion based method for input perturbation, and U-CAM[4] method for logit perturbation.\n\n\no. In table-3 of the experiments section, The author has evaluated both Probability difference vs. logit difference scores. Are both of them required?\n\n\np. Zhang et al. has discussed a Pointing Game [5] method to evaluate the Localization ability of an explanation map as discussed in the SCORE-CAM[6]. \n\nq. The pseudo-code is not so informative to get the proposed algorithm. Few components(soft_topk_approximator, explainer_vp) are not discussed in the pseudo-code. \n\nRef: \n1. Ribeiro, M. T.; Singh, S.; and Guestrin, C. 2016. ” Why should I trust you?” Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining,1135–1144.\n\n2. Lundberg, S. M.; and Lee, S.-I. 2017. A unified approach to interpreting model predictions. In Advances in neural information processing systems, 4765–4774.\n\n3. Li, J.; Monroe, W.; and Jurafsky, D. 2016. Understanding neural networks through representation erasure. arXivpreprint arXiv:1612.08220.\n\n4. Patro, Badri N., Mayank Lunayach, Shivansh Patel, and Vinay P. Namboodiri. \"U-cam: Visual explanation using uncertainty based class activation maps.\" In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7444-7453. 2019.\n\n5. Zhang, Jianming, Sarah Adel Bargal, Zhe Lin, Jonathan Brandt, Xiaohui Shen, and Stan Sclaroff. \"Top-down neural attention by excitation backprop.\" International Journal of Computer Vision 126, no. 10 (2018): 1084-1102.\n\n6. Wang, Haofan, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding, Piotr Mardziel, and Xia Hu. \"Score-CAM: Score-weighted visual explanations for convolutional neural networks.\" In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pp. 24-25. 2020.\n",
            "summary_of_the_review": "The paper has a lot of technical drawbacks. That needs to be addressin in the final version.  ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes an interpretable machine learning method that learns a posterior distribution of feature attribution $s$ given input $x$ and model output $\\hat{y}$, explaining each black-box prediction instance by scoring attribution of every features. The feature attribution is learned by minimizing the trade-off between the reconstruction loss and regularization loss, which is obtained by variational inference of the posterior likelihood. A previously proposed approach (SOFT) is used to enable the gradient backpropagation flow of the model. The authors perform several experiments using three benchmark datasets (MNIST, CUB, ImageNet) and compare their approach with several state-of-the-art methods, showing it outperforms the competing methods in some experiments. The authors claim that it provides a more reliable explanation than the others as it achieves better faithfulness (fidelity) and robustness to input variation and model configurations. \n\n",
            "main_review": "* Novelty and Contribution:\n\nThere are several methods using similar approaches [1,3] that were not mentioned in the paper (Lei 2016, and Bang 2021). All those papers [1-3] learn an explainer (i.e., $q_\\theta$ in this paper) giving feature attribution scores to explain a black-box model. This method should be compared with those previously proposed methods. Without a clear demonstration of the differences, it is hard to evaluate the novelty and contribution of this paper. \n\nAlso, it is not clear how the optimization of the posterior distribution helps to achieve robustness. The authors should provide a clear motivation (rationale) and in-depth discussion. Does the regularization loss help it to be robust? What happens if we don't use the regularization loss and only optimize the reconstruction loss?\n\n* Technical Soundness\n\n1. Faithfulness can also be evaluated by several prediction measures (e.g., AUC) between $\\hat{y}$ and $\\hat{y}_{perturb}$. \n2. Please provide a rationale why it is desired to be stable to input perturbation (section 4.2.2). If the black-box model itself is not stable to input perturbation, shouldn't the attribution be non-stable as well?\n\n* Clarity\n\nOverall, the writing is clear and has no big concern except for several minor typos. Figure 2 is easy to understand. It would be good if the authors clearly specify the final objective function (reconstruction + beta * regularization) even though Equation 5 has almost the final form of the objective function.\n\nPg 6. (SSIM)Wang et al. (2004) --> (SSIM, Wang et al. 2004)\n\n\n* Reference\n\n[1] Lei, Tao, Regina Barzilay, and Tommi Jaakkola. \"Rationalizing Neural Predictions.\" Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. 2016.\n\n[2] Chen, Jianbo, et al. \"Learning to explain: An information-theoretic perspective on model interpretation.\" International Conference on Machine Learning. PMLR, 2018.\n\n[3] Bang, Seojin, et al. \"Explaining a black-box using deep variational information bottleneck approach.\" The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21), 2021.\n",
            "summary_of_the_review": "This paper proposes a method to learn feature attributions to explain a black-box model, and aims to show it is more faithful and robust than the existing approaches. The robustness (stability) of interpretable ML models is an interesting and timely topic, and faithfulness is the commonly used criterion to evaluate explanation quality. However, I decline to accept this paper, mainly due to two major concerns in novelty and contribution. Similar approaches have been proposed before, but there is no comparison between them, and the use of posterior distribution in relation to robustness is not well motived. Further experiments and in-depth discussion are required for acceptance. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors present a perturbation-based attribution method that aims to improve the faithfulness and robustness of the computed attribution, focusing on image input.\nThe attribution is learned specifically for each input, similar to L2X (Chen et al, ICML’18). The learning objective aims to reduce the reconstruction loss as well as a regularization loss. In addition, the top-k selection is improved over L2X by using the differentiable operator proposed by Xie et al. (2020) instead of a chain of Gumbel-softmax operations (Jang et al. ICLR’17). Indeed, in all qualitative examples, the computed masks under the proposed methods (corresponding to the top-k pixels) are more consistent than in L2X.\n\n",
            "main_review": "Overall, the work is incremental and the technical / theoretical contribution is limited.\nI do find it reasonable to focus on image classification, however, I was hoping to learn some important limitations in the state-of-the-art algorithms that the proposed method does overcome, and more importantly, why it does.\nWhile the quantitative evaluation on three datasets does show certain improvements, it remains unclear why the proposed method is superior and whether it generalizes to other datasets and usage scenarios.\n\n[Other comment]\n- I wish the authors considered comparison with LIME, SHAP, and Integrated Gradients. These methods also perform sampling (while not explicitly modeling the distribution of s), and I would be very curious to see how they fair in the evaluated metrics.\n- In the regularization loss, the authors assume independence between the attribution random variable “s” and the input, and accordingly use a standard Gaussian distribution for the prior p(s). This might not always be a realistic assumption, even in the context of image classification.\n- The blurred baseline has also been proposed by Xu et al. in their CVPR’20 article “Attribution in scale and space”. The authors could consider citing their work and comparing with their results.\n\n[Typos]\nDATSETS\nattirbution\nrelavance\nreproducability\ndissect the the\nImagenet => ImageNet\n\n[Singular-plural issues]\n- each samples\n- hyper-parameter setting => settings\n- is one of the methodology => methodologies\n- belongs to perturbation-based method => methods\n- Other ways … is out of our … => are.\n\n[Missing / unneeded articles]\n- [Recurrent issue] of [original | randomized] classifier => of the ..\n- in a real time => in real time\n\n[Unclear phrases]\n- timely efficient\n- related with objects => to?\n- other settings are same with VP => as in?\n\n[Citations]\ngumble-softmax => Gumbel-softmax",
            "summary_of_the_review": "Limited contribution; limited comparative evaluation (esp. in terms of the existing methods); potentially limited applicability (the method require training for every individual input).",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a new saliency map construction method, inspired by variational inference. It assumes a Gaussian prior on the saliency map, and defines the likelihood as the KL-divergence between the original predicted probability vector and the new prediction on the image with various fractions of pixels masked out according to the saliency map. The model is evaluated on faithfulness and stability. It shows good faithfulness according to the prediction difference metric proposed by Samek et al. (2016), and shows good robustness according to the metric proposed by Alvarez-Melis & Jaakkola (2018b). In addition, this method has also been shown to successfully pass the sanity check proposed by Adebayo et al. (2018). ",
            "main_review": "### Strengths\n\nThis paper is generally well-written. The motivations for faithfulness and stability are explained clearly. The technical details about the variational formulation are also sufficient for me to achieve a complete understanding. The proposed method is compared with a large number of existing saliency maps methods, and have shown overall good performance on the metrics that the authors use. \n\n### Weaknesses\n\nDespite the strengths, I do have some concerns about claims and executions in this work. \n\n1. The motivation for the probabilistic variational approach is that the explanation should be stable. However, this seems to be a preconception by humans, and sometimes be at odd with faithfulness. Fundamentally, if the model is using very different \"reasoning patterns\" on highly similar inputs, then I believe that the explanation should faithfully show such instability. For example, in Fig 3, it could be possible that the model works inherently different for different resolutions, which result in the differences in some other saliency maps. Simply because two images look visually similar would not qualify as the reason for why their saliency maps should necessarily be similar as well. \n\n2. The authors mention in the related work that \"However, images perturbed by these methods could lead to out-of-distribution samples, leading to unintended artifacts for explanation\", which I 100% agree. But it looks like the authors did not make an attempt in addressing this issue in the proposed framework, as similar masking operations (blurring, noise, mean) are used which cause image artifacts. \n\n3. Similarly, the evaluation procedure proposed by Samek et al. (2016) also suffer from the same out-of-distribution drawback, especially when a large number of pixels are removed. An attempt at resolving this issue is proposed in the ROAR framework (https://arxiv.org/abs/1806.10758), but its limitations have also been shown (e.g. by https://arxiv.org/abs/2012.00893). Instead, I would recommend including additional evaluations that truly establish the utility of the explanations, for example in uncovering known ground truth feature correlations (https://arxiv.org/abs/2104.14403) or helping people to achieve better performance in a teaming setup (https://arxiv.org/abs/2006.14779). \n\n4. Compared to the original VAE formulation, the proposed generative process seems more \"hacky\", especially in the use of KL divergence as the (log)-likelihood. This seems like an arbitrary choice, as KL divergence can be replaced with other distance metrics such as L1/L2 distance or total variation distance. It seems that other choices may even be better, since KL divergence is not symmetric and can grow unbounded. Could the authors provide some theoretical justification for it? ",
            "summary_of_the_review": "Overall, I recommend reject because I am not convinced that the proposed method is a significant improvement over existing methods. For me to improve my recommendation, I would like to see more comprehensive demonstration of the utility of the proposed method relative to existing baselines and a theoretical justification for the posterior formulation. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}