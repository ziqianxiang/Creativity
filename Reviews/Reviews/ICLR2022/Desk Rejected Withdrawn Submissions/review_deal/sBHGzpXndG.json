{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "Based on the intuition that learning <Subject, Predicate, Object> is important for video captioning,  the authors design the COllaborative three-Stream Transformers (COST) with 3 Transformer-based branches/streams: (1) the Video-Text branch that models the interactions among the global clip appearance tokens and the word-level text tokens, in order to predict video captions, (2) the Detection-Text branch that models the interactions among the object tokens and the text tokens, in order to learn Subjects and Objects, and (3) the Action-Text branch that models the interactions among the action tokens (computed from optical flow-based features) and the text tokens, in order to learn Predicate. \n\nApart from the contribution of designing COST, the key novelty is to enforce the model to generate the accurate Subject, Object, and Predicate predictions while predicting the video captions. A cross-modality attention module is proposed and placed in each Transformer branch to allow for the information flow across the Transformers in the 3 branches and to allow for alignment. The 3 Transformer branches thus collaborate and support each other.  Experiments conducted on the YouCook2 dataset and the ActivityNet Captions dataset demonstrate the proposed method performs favorably against SOTA methods.\n",
            "main_review": "Strengths:  \n[+] The motivation to enhance the learning of Subject, Predicate and Object for video captioning while capturing the interactions among tokens in different granularities and modalities (visual and text) is valid.   \n[+] The ideas are simple and the proposed model is effective to some extent. The qualitative results are interesting.   \n[+] The authors have promised to release the source code.  \n   \nWeaknesses:   \n[-] The proposed method seems to be not better than SOTA methods - the conclusions mentioned in Section 4.3 are not so convincing. Since the proposed method uses additional object features which some of the baselines (e.g. MART) does not use, comparing the MART’s result on YouCook2 val subset in Table 1 and COST-2(v+a) result  in Table 4, the proposed method seems to perform worse than MART when removing the detected object features. This makes me question whether the improvement mostly comes from the extra object features.  \n \n[-] The ablation/comparative experiments are lacking to demonstrate the benefits of proposed method, or to distinguish it from existing works. From the current ablation studies, there is no way to tell the benefits of the proposed Cross-Modality Attention module. What would the results be if we simply replace the Cross-Modality Attention in Figure 1 with Self-Attention over M (M can be H, X or Y) without any cross stream interactions (but still use the exactly same three-stream architecture)? What if instead of using the proposed Cross-Modality Attention, we use the vanilla Cross-Attention (i.e., if we take the Action-Text Transformer as an example, we map Y to the query, and map H and X to keys and values to do the vanilla Cross-Attention)?   \n\n[-] Overlook of existing works and the novelty seems to be limited. There are many existing multimodal Transformer based models and learning paradigms; please see Figure 2 in [1] where the “Share Type”, “Cross Type” and “Joint Type” learning paradigms are mentioned. To my understanding, the proposed method in the paper basically belongs to the \"Share Type\" paradigm in terms of how multi modalities (visual+text) are used, and also belongs to the “Cross Type” paradigm in terms of multi-stream learning.  Since the three-stream COST architecture in Figure 1 is another key novelty of the paper, could you provide evidence to fairly show that the proposed architecture is indeed better than the existing popular paradigms and it is indeed novel? It would also be great to briefly discuss these related work and paradigms.  \n[1] Luo, Huaishao, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Jason Li, Taroon Bharti, and Ming Zhou. \"Univl: A unified video and language pre-training model for multimodal understanding and generation.\"   \n\n[-] The descriptions of the problem statement, method, training, inference and implementation are not clear.\n-   I understand that each video sequence is sampled by 100 frames, but why $N_v$ is 100? In Section 3.1,it is mentioned that $N_v$ is the number of clips, instead of the number of frames in the video sequence.  \n-   Is it correct to say that $N_a$ is the total number of action tokens? It causes confusion because I assumed it is the number of unique predicates (verbs) in the dataset. From descriptions in Section 3.1, I have no idea what “total number of action tokens” means, and it seems that $N_a$ is actually the number of frames sampled from the video sequence according to Section 4.2.  Why should the number of action tokens be equal to the number of frames? \n-   What is $\\Omega$ in Equa. (6)? Just to make sure of reproducibility, I assume the most common verbs means the top $k$ frequent verbs in the clip captions of the video sequence, then what is $k$ that is used for experiments?\n-   Could you elaborate on “we leverage the video-text features in history to obtain the long-term sentence-level recurrence to generate the next sentences”? \n-   To compute the cross entropy loss that penalizes the errors of predicted captions compared to the ground truth, is the $CLS^v$ token used at all? How are captions predicted exactly given the learned representations of tokens in the sequence of the Video-Text branch? The same questions apply to the object category prediction in the Detection-Text branch.\n-   For the multi-label classification for predicting the multiple actions present in the video sequence, learned representations of all action tokens are aggregated as the $CLS^a$ token and then sent to a fully-connected (FC) layer to obtain the confidence score. This  aggregation+FC based processing is only true for the action/predicate prediction, is that right? What happens to the learned representation of the input $CLS^a$ then? Why is using aggregation to obtain a new $CLS^a$ token necessary and why not directly use the learned representation of the original $CLS^a$ token?\n-   Does every token in Equa. (1) have a unique position? The set of objects in the same frame would have different ordered positions, is that so? \n",
            "summary_of_the_review": "The paper is marginally below the acceptance threshold because there is not enough evidence from the paper to support the benefits and novelty of the proposed techniques. The ablation experiments are a bit lacking and more in-depth studies are required. Moreover, several details are not well described. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes an enhanced visual transformer for video captioning. To be specific, the video part integrates three streams -- appearance stream using frame sequence feature, action stream using optical flow feature, object stream using object feature. Meanwhile, the three streams are further fused by cross-attention modules in order to align local-level features. The experimental results on two datasets are good compared to those of previous SOTA methods.\n",
            "main_review": "Strength:\n+ The paper is easy to follow.\n+ The results are good. The ablation study is solid.\n\nWeakness:\n- The novelty is limited. It’s hard to distinguish the idea of this paper from previous ones. ActBert also leverages multiple visual feature streams, including region objects and actions. Global coherence and local alignments are explored by Transformer-based ActBert, COOT. The most distinguished part would be the cross-attention among different visual streams. The previous methods may focus more on the cross-attention between visual and linguistic parts, this paper explores the cross-attention among different visual streams. However, it’s minor.\n- Missing implementation details, so it’s not very clear to see the contribution of each module to the performance gains. To be specific:\n(i) The detection model used in this paper. What is the backbone of the Faster R-CNN model? Does the model pre-trained on COCO before training using Visual Genome? ActBert only uses models trained from the COCO dataset (limited categories compared with Visual Genome), so what is the impact of the strength of the detection model on video captioning performance?\n(ii) How do you collect the supervision for the action stream? One guess is that the supervision comes from verbs detected from texts. But we know that actions are not equal to verbs, so do you have any further processing on the action tokens?\n\nSome other comments\n- The visualization part is not very convincing. In figure 3, the column stands for action and detection tokens. But where is the boundary between action and detection? From the illustration, we can observe a strong imbalance. The left part contains the most highlighted entries while the right part is barely focused.\n",
            "summary_of_the_review": "I appreciate the good performance achieved by this paper. But due to missing implementation details, it's not very clear where does the performance gain comes from. Also considering the limited novelty, I tend to give a borderline reject to this paper and consider accepted if the concerns are addressed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work proposes a three streams transformer based architecture to show the interaction between subjects and their relationships to perform video captioning. To demonstrate the proposed approach, authors show and compare experimental results with SOTA methods on YouCookII and ActivityNet Captions dataset. ",
            "main_review": "This paper describe a reasonable clear history in the introduction section and the story fully support the experimental results. The model descriptions are mostly clear enough for reproducibility. The reviewer thinks this paper is of certain value to the field of video captioning.  \n\n** The strengths of this paper are summarized as follows:\n1. In NLP, a sentence can be divided into three parts: subject, predicate, and object. According to this observation, this paper proposes a separate stream for each part, which seems interesting to me. \n\n2. To combine/interact features from different transformer streams they propose cross-modality attention module which basically compute a affine matrix to maintain visual alignment. \n\n3. Most of the model design is reasonable and with care, which is important for application papers.\n\n4. They conduct reasonable experiments to show the effectiveness of their propose method. \n\n** The weaknesses of the paper are as follows:\n1. In this paper, authors use three separate transformer streams and to interact between those streams, they use affine matrix. Previous transformer based approaches (e.g. [1], [2]) use co-attentional module to guide one modality using other modality. What's the reason for not using the same concept?\n\n2. I think video token and action token are capturing similar information using TSN. According to the paper, video tokens are extracting temporal appearance information from the video clips where action tokens are collecting motion information using optical flow. In that sense, action token collects redundant information. What's the benefit of using these two separate streams?\n\n3. We know that the computational cost of transformer based model is quite high. Here authors use three transformer streams. What's the computational cost of the model? Is it much more than CNN based or single stream model?\n\n4. Do you have any failure cases as qualitative results, where SOTA performs better than the proposed method.\n\n5. It would be great if author includes human evaluation between the proposed method and SOTA method.\n\n6. To align interactions between different modalities authors compute affine matrix. Is this approach is better than other fusion approaches? Author should include ablation for this contribution. \n\n7. Is there any limitation of the proposed method?\n\n8. The Microsoft Video Description (MSVD) dataset and the Microsoft Research Video-to-Text (MSR- VTT) dataset are the most common dataset for video captioning. Author should consider at least one of them. \n\n[1] Yu, Zhou, et al. \"Deep modular co-attention networks for visual question answering.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.\n[2] Li, Guang, et al. \"Entangled transformer for image captioning.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.",
            "summary_of_the_review": "This paper is well written. Author includes their contributions and justification in a great way. But my main concern is the novelty of the paper. This type of transformer based method is not new for video captioning. In this paper, author use three separate transformer streams to extract different level of information from the video and compute interactions between different modalities. Moreover, I think video token and action token are performing similar task. This contribution is not sufficient for ICLR. So I would say, the submission is ok but below acceptance threshold. Author should also focus on the above weakness points.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I do not see any ethical concerns for this research.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes an algorithm for video captioning, which consists of three stream transformers. The three streams are used to capture Video-Text, Detection-Text and Action-Text information. Cross-modality attention is designed to enforce the information exchange across different modalities. The paper presented experimental results on YouCookII and ActivityNet. Compared with the selected methods, the proposed method shows better performance.",
            "main_review": "While the overall idea of the paper seems to be straightforward, many technical details need to be further clarified. The following are my detailed comments about the paper:\n\n1, The paper claims that the affinity matrix is a major contribution. It is unclear to me how effective the affinity matrix is for improving the final results of video captioning. Figure 3 provides some visualizations, but it will be more helpful if the paper can provide an apple-to-apple comparison of with vs. without the affinity matrix on the final performance.\n\n2, What is the difference between video tokens and action tokens? One is using RGB, and the other is using optical flow?\n\n3, From the results in Table 1, with or without COOT seems to make a big difference in the final results. This indicates the importance of using large-scale datasets for pretraining, as COOT is pre-trained with HowTo100M. The paper should also compare results with or without pre-training on the Visual Genome dataset for the detection tokens. It is unclear to me whether the proposed method is working, or simply because the paper uses more large-scale datasets for pre-training. The paper should list what datasets are used for pre-training when comparing different approaches.\n\n4, The cross-modality attention in Figure 1 is a little confusing to me. It doesn’t directly correspond to Eq.(3) and (4). The paper should further improve Figure 1 to avoid any misunderstanding.\n\n5, From Figure 1, word embedding N_{t} is concatenated with N_{a}, N_{v} and N_{t}. But from Eq.(3) and Eq.(4), N_{t} is gone. Where is N_{t} in Eq.(3) and (4)? Is it used for computing the affinity matrix?\n\n6, The loss function in Eq.(5) requires the ground truth for both action and object detection besides video captioning. Do the other papers also use the supervision information from action and object detection to train their video captioning algorithms? Please clarify so that we understand what annotation information is used for training.",
            "summary_of_the_review": "Please find more details in my above comments.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}