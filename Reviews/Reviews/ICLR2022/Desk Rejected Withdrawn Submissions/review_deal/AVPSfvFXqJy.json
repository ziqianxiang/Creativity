{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents the observation that supervised and SSL-trained Vision Transformers (VIT) do not encode the exact same information. Based on this observations, the authors propose to include a distillation loss to guide the training of supervised classifiers. By distilling information from the SSL-trained network, the final classifier obtains better validation performance.",
            "main_review": "The present submission shows that including information from an SSL trained network when training in a supervised fashion on ImageNet helps generalisation. To include this information, the authors propose to use a distillation loss. \nHowever, it has already been shown in previous work that finetuning the SSL-trained network will provide similar results. Actually they are even better than what the authors report. Indeed, while in Table 1 the DeiT-S reaches 81.4, the DINO paper reports 81.5 with finetuning. Moreover, in order to distill and obtain the 81.4 the authors use a DeiT-B teacher which is larger than the network used at initialisation in DINO (of course, also a DeiT-S). \nBecause of this observation, I fail to see what are the contributions of this work. The proposed framework is an overly complicated way of baking the prior from the SSL method into the supervised training loop - and yields worse performance than finetuning. The finetuning should have been reported as a baseline as a bare minimum.\n\nOn a minor note, reporting the validation accuracy along epochs (Fig. 4) is not really informative, does not bring anything, except that it shows some accidents around epoch 50 and 100... The figure lacks a proper caption. This makes the paper look a bit unpolished. ",
            "summary_of_the_review": "Overall, I think that this paper does not show anything really novel. The results obtained are worse that the performance reported using finetuning in previous work (even when using larger capacity as teacher).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work is motivated by the observation that Transformers perform quite differently when they are trained with supervised learning (SL) and self-supervised learning (SSL) respectively. Authors sniffed the potential complementarity of the learned representations of the two learning paradigms and empirically propose to use the model trained with SSL as a teaching assistant to help Transformer learning better. The resulted student Transformer is analyzed w.r.t quantitative performance, shape bias, robustness, and attention area, which verifies the effectiveness of the presented method and is insightful for me.",
            "main_review": "+ The joint use of supervised learning and self-supervised learning is of great importance and had drawn great interest of mine. By using the head level distillation to transmit different knowledge to the student, the method successfully find a conjunction point that SL model and SSL model could complement each other and shows great performance improvement.\n+ This work locates itself well and is the first work that naturally joins the SL model and SSL model in a distillation manner as far as I know. The analysis experiment is inspiring and some of the results are surprising (e.g. SSL teacher assistant help the student pay more attention to the object body).\n+ The paper is well-written and clear. The organization of the work is logical and the overall experimental design (e.g., SL and SSL difference justification, learning curve trend, student model tendency analysis) is technically reasonable.\n+ The proposed method is easy to implement and the improvement is prominent (as illustrated in Tab.2).\n\n- The explanation of Fig.4 (a) is a bit confusing for me. Does the green line and red line represent the performance curve of using SL teacher and SSL teacher alone all through 300 epochs respectively? If so, I suggest the authors add another two lines showing the performance of using SL teacher and SSL teacher from the start to the 100 early stop epoch, which may help the understanding of the usage of the proposed early stop strategy.   \n- Analysis towards prediction preference is only carried on SL Teacher and SSL teacher assistant, I wonder how differently does the learned student model perform compared to its teacher and teacher assistant.\n- I wonder whether the SL teacher and SSL teacher assistant could be applied on pure convolutional neural networks. I understand that the proposed method perform distillation on a head level that cannot be directly applied to CNNs, so I would appreciate it if the authors could share some insight or experimental experience on it.  \n",
            "summary_of_the_review": "Overall I appreciate and enjoy reading this work as the paper progressively demisting the doubts that arose when I read the title of the paper. The combination of the SL model and SSL model is natural and the proposed methods are novel to me. Though I believe the difference between SL and SSL is still not fully discovered, this paper has done a good start and may inspire future research. Thus, I tend to accept this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper adopts a self-supervised pretrained vision transformer as teaching assistant for improving the performance of student via head-level knowledge distillation. To further improve the effectiveness of knowledge distillation, a simple yet effective head selection strategy is proposed to transfer teachers’knowledge to the select critical heads in the student. Extensive experiments show the effectiveness of the proposed method, including on classification, robustness and transferability. ",
            "main_review": "Strengths: \n-1- It is interesting and novel that the SSL teaching assistant with the SL teacher can improve the student’s performance more remarkable, compared to two SL teachers. Experimental validation and the ablation studies are thorough and comprehensive.\n-2- The proposed head-level knowledge distillation method is simple yet effective that it does not require any dimension alignment.\n-3- The paper is well-written and easy to follow. \n\nWeaknesses: \n-1- Although the ablation studies (in table 2) for the proposed head selection strategy have demonstrated the superiority when compared to average head strategy, the comparison of the proposed head selection strategy and random head selection method is missing.\n-2- It is more rational add the abs operation on Eq. (4) to decide the important of the head.\n-3- It is not clear that why head distillation from SSL teacher and SL teacher only choose one head for each other. Please clarify it.",
            "summary_of_the_review": "This paper proposed a novel knowledge distillation method for vision transformer, which is motivated by the observation that the different attention map information of SSL ViTs and SL ViTs. The proposed method is simple yet effective, which can significantly improve the performance of student. As such, I tend to accept the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes a knowledge distillation method for transformers, which enforces the student model to have similar self-attention patterns as the teacher's. The paper also proposes to use both supervised models and self-supervised models for KD.",
            "main_review": "Strength: The proposed method has a good motivation. It makes sense that supervised models and self-supervised models learn different knowledge, and distillation through self-attention seems to be a valid method for transformers.\n\nWeaknesses:\n1. The paper proposes a knowledge distillation method using attentions. However, the paper does not compare with existing methods in knowledge distillation. Therefore, it is hard to justify its contribution. For example, Table 1 shows a 1.8% improvement on DeiT-Ti using DeiT-S as the teacher. What would the performance be if other KD methods are used? Two widely-used distillation methods, namely class-logit distillation and representation distillation, should be compared.\n\n2. In the first column of Table 1, DeiT-S (SL) should be DeiT-S (SSL)? How is the accuracy of a SSL model evaluated? Under the linear probe protocol? \n\n3. How is the 100 epochs for early stopping determined? Is it purely based on heuristics? Since it is a very important hyper-parameter (based on Table 2), it would be better if it can be determined in a more principled manner. It would be good to see more explanations on this matter.\n\n4. The experiments in the paper distill knowledge from a larger teacher to a smaller student. Since self-distillation has shown to be an effective strategy, would the proposed method work for self-distillation? i.e., the teacher and student model have the same size.\n\n5. Some relevant methods in KD should be discussed:\n   - Knowledge distillation meets self-supervision, ECCV 2020.\n   - Contrastive Representation Distillation, ICLR 2020. \n   - SEED: Self-supervised Distillation For Visual Representation, ICLR 2020.\n",
            "summary_of_the_review": "Despite a good motivation, the technical contribution of this paper is not well-justified. Two important weaknesses include: (1) lack of comparison with existing KD methods, and (2) heuristic design of many important hyper-parameters (early stop epoch, distillation layer, etc.). Therefore, I believe that the paper still needs major improvement.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}