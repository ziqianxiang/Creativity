{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes to generate an instance-specific execution path during the inference in MTL. They first generate the task-specific block execution decision and then use gates to control whether a channel in each executed block should be used based on the input instance. They experiment on Census and MIMIC-III datasets.",
            "main_review": "Strength: They propose to decide the block-level execution based on the task id and decide the channel-level execution based on the instance information. The idea is interesting and reasonable. DynaShare extends AdaShare to incorporate the instance embedding to determine the channel sharing pattern (a finer sharing pattern if it works well). On the MIMIC-III dataset, DynaShare also achieves the best performance compared to baselines.\n\nWeakness: They perform very few experiments in the paper. They do not use the common MTL benchmarks such as NYU v2, CityScapes, Taskonomy datasets. Also, they miss comparing with a bunch of SOTA MTL frameworks, such as cross-stitch [A], ASTMT [B], MTAN [C], DEN [D] and etc. Moreover, the authors do not include any analysis or visualizations of instance-specific channel-wise policy in the paper.\n\n[A] Cross-stitch Networks for Multi-task Learning \n[B] Attentive Single-Tasking of Multiple Tasks\n[C] End-to-End Multi-Task Learning with Attention\n[D] Deep Elastic Networks with Model Selection for Multi-Task Learning",
            "summary_of_the_review": "DynaShare extends AdaShare to an instance-specific execution policy. The idea is interesting and reasonable. However, the experiment section is weak. They do not use the common MTL benchmarks nor compare with SOTA MTL methods.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "DynaShare, a deep multi-task learning method, is proposed in this paper.\nWith a hierarchical gating policy consisting of a task-specific policy for coarse layer selection and gating units for individual input instances, which work together to determine the execution path at inference time, the network operates dynamically at the inference time.",
            "main_review": "Pros:\n\n- A network structure can be adapted based on both tasks and individual instances is proposed.\n- The paper is easy to follow.\n\n\nCons:\n\n- This manuscript share similar idea and intuition with existing works, AdaShare, making the technical novelty not quite significant. Both AdaShare and the proposed DynaShare attend to apply dynamic neural networks in the scenario of multi-task learning. The hierarchical gating policy proposed is just based on AdaShare.\n- The introduction of instance-specific gating units may improve the model performance, but it also brings additional computation cost.\n\n",
            "summary_of_the_review": "The novelty and contribution of this work are marginal, which is my main concern. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes DynaShare, a new method for deep multi-task learning that learns from the training data a hierarchical layer gating policy at 2 levels - task and instance. First, the method  learns a task-specific gating policy for each task, similar to previous approaches like [Sun et al., 2020]. Then, inspired by approaches like [Veit & Belongie, 2018], the method further learns an instance-specific gating policy that selects a subset of the network's layers based on the individual input (instance). The main idea of this paper is to combine previous approaches like  [Sun et al., 2020] and [Veit & Belongie, 2018] to jointly learn these policies hierarchically and illustrate its potential for dynamic multi-task learning using experiments on 2 datasets.",
            "main_review": "***Strengths:*** In the context of deep neural networks, a fundamental challenge of MTL is deciding what to share across\nwhich tasks for efficient and accurate learning of multiple tasks. So the idea of dynamic networks that can learn these decisions is certainly appealing. The idea to further extend AdaShare (that learns only a task-specific policy) to learn gating policies based on instance characteristics is interesting, does expand the space for dynamic execution of the network, and could be a promising method for scalable MTL. Experiments on the MIMIC dataset are particularly encouraging of the value of DynaShare for heterogeneous tasks.\n\n***Weaknesses:*** My concerns with the paper are mostly regarding the motivation (or lack thereof in the current presentation) and the experimental results supporting it:\n\n1. **Motivation is unclear.** DynaShare seems to evidently build upon AdaShare [Sun et al., 2020] using a gating policy that is not just task-specific, but also instance-dependent. Although this contribution is clear, the motivation behind introducing an instance-specific gating is unclear, and brings up some questions:\n    - Why is this a limitation (or what is the limitation) of previous approaches like AdaShare? Can the authors comment on this point?\n    - Does adding instance-specific gating reduce the memory/time footprint of the model wrt parameters? Is optimality the motivation, similar \n      to AdaShare?\n\n    In this regard, it would be great to introduce the problem with a subsection that can motivate this work with an example where this kind of policy would help. It should clearly compare as to what it can do better with respect to just a task-specific gating policy.\n\n\n2. **Unclear Experimental Results.**\n\n    1. Table 1 showcases the AUC improvements of DynaShare against multiple baselines (including AdaShare that performs only task-level         gating). From Table 1, although it is clear that DynaShare performance is competitive on the Census task, it can be seen that AdaShare performs even poorer than learning individual models for each task. That is, task-specific gating seems to reduce performance. `This is conflicting when compared to the ablation study in Sec 4.4 and Table 3 on the MIMIC dataset`. Table 3 shows that both task-specific gating and instance-specific gating help MTL and that the task-specific policy provides the biggest gains. Can the authors clarify this?\n\n    2. The results in Table 2 show that DynaShare performs better than the state-of-the-art (MMoEEx) in 2/4 heterogeneous tasks in the MIMIC-III dataset. But, it would be great to see a focused comparison with AdaShare here, which is the actual base approach that DynaShare extends. After combining the results from Table 3 and 2, it seems like DynaShare only improves on AdaShare significantly in 1/4 tasks, i.e. Decompensation prediction. Firstly, I recommend that the authors add AdaShare results to Table 2 as well as Table 3. Secondly, can the authors provide results/comments on `what improvements DynaShare provides specifically over AdaShare for heterogeneous tasks`?\n\n\n2. **Potentially Missing  Experimental Results.** \n\n    1. Hard parameter sharing is restrictive and can result in negative transfer. Soft parameter sharing is better but not scalable with increasing #tasks. Assuming the motivation behind DynaShare is to overcome these scalability issues (as stated in Intro and similar to related work), there is no focused evaluation that shows this. \n> **Suggestion**: An experimental evaluation against baselines of #parameters as no of tasks increases would showcase the effectiveness of this approach. In the past, approaches like [Sun et al., 2020] [Veit & Belongie, 2018] have used reduction in `#params` and `FLOPS` to show the real value of dynamic network executions.'''\n\n    2. Previous results have supported the intuition that similar tasks should have similar execution distribution to share knowledge. Any \nresults on what layers of the network this new hierarchical policy prefers to share among tasks?  \n> **Suggestion**: It would be interesting to visualize the learned inference graphs, by studying the rates at which different layers are executed or the similarity in policies learnt. The results can be a good contribution to reveal future research ideas.\n\n    3. For the method to be easily adapted to various MTL tasks, it is essential that the instance-specific gating is robust to small variations in the instance space.  If not, can it end up reducing knowledge sharing among even related tasks, just because of minor variations in instance characteristics? How much of this minor variation can be controlled by the target rate `t` and the weight `λ` instance  used to control the loss term Linstance ? \n> **Suggestion**: An important aspect to study here can be the effect of `t` on performance and inference time.\n\n\nReferences:\n- [Sun et al., 2020] Adashare: Learning what to share for efficient deep multi-task learning\n- [Veit & Belongie, 2018] Convolutional networks with adaptive inference graphs\n",
            "summary_of_the_review": "The idea of automatically learning dynamic networks that can make knowledge sharing decisions for MTL is quite exciting. Further, taking inspiration from the effectiveness of policy networks to select blocks based on inputs, and exploring it for MTL is certainly novel and looks promising. However, the paper in its current form can benefit from some substantial modifications. First, I would recommend focusing on a concrete motivating goal such as performance or efficiency of MTL networks, explaining clearly what limits existing approaches. Secondly, I would like the paper to clarify some of the conflicting results if possible. Lastly, I would recommend revamping the evaluation section to address some essential components like the effect of target rate t, the consequences on the memory and parameter footprint, and a very focused comparison to the obvious baseline AdaShare.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work introduces a multi-task learning approach, named DynaShare, that dynamically proposes an internal network based on a specific task and an input instance. This is achieved by learning a task-specific policy with instance-specific gating. To learn instance-specific policies, it uses Gumbel-Softmax sampling following the existing practice in the literature. Experiments on several datasets show the competitiveness of the proposed method over its baselines and some other approaches.",
            "main_review": "This work improves the existing baseline, AdaShare, by further realizing an instance-wise gating mechanism, allowing dynamic paths for different inputs at inference time. However, the framework is incremental because it adds an additional gating module for instance-wise prediction on the baseline. The instance-wise gating has been proposed previously as mentioned in the paper. \n\nIn early of page 4, the authors say that it is the first approach to learn instance adaptive gating mechanism, but there is a similar work sharing the same idea; (Ahn et al., Deep Elastic Networks with Model Selection for Multi-Task Learning, ICCV 2019), which was introduced in AdaShare. The work proposes an internal network instance-wise for multi-task learning. Hence, the contribution of this paper is very limited. The authors should rigorously compare the proposal with the above-mentioned work. Additionally, I think BlockDrop and GaterNet also propose a network instance-wise and can be applied to multi-task learning scenarios. The authors also consider comparing with them.\n\nOther than those, there are no notable contents in the paper. Sections 3.1 and 3.2 introduce existing work. Section 3.3 borrows the same thing from the previous work, and Section 3.4 has an almost similar loss function (Eq. (3) and (4)) to AdaShare.\n\nI am not convinced why the authors take the datasets for comparisons. They do not use the datasets that were used in the main competitor, AdaShare. More comparisons using the datasets in AdaShare should be made for a fair comparison.\n\nIn Table 3, there is a marginal improvement when combining task-specific gating with instance-specific gating. The instance-wise gating seems less effective.\n",
            "summary_of_the_review": "Please take a look at the main review that I have made on several concerns and limitations. The proposed method is incremental, and the main contribution is limited because it combines existing techniques to achieve its goal. More importantly, a similar work shares the same motivation and idea, even if the details to realize the goal are slightly different.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work presents a natural and powerful extension to AdaShare by conditioning layer-wise sharing not only to be a function of the task, but also on the representation of the input at that layer. It shifts the paradigm presented in AdaShare from \"which layers should tasks share or not share\" to include \"how should the task itself use or not use the layers allocated to it.\" The authors offer a multi-task learning framework, DynaShare, that leverages this intuition to outperform other multi-task learning baseliens on the Census and MIMIC-III datasets.",
            "main_review": "It is my opinion that this is a very strong submission. The intuition underpinning the method is well-motivated, introduction and related work are incredibly well written, and the description/explanation of the method is clear and concise. It has many of the hallmarks of a \"great paper\", but I believe the experimental analysis is markedly insufficient.\n\n**Strengths:**\n\n1. Extremely well written and polished paper. Abstract, Introduction, Related Work, Method, Experiments, and Conclusions sections are uniformly descriptive, clear, and concise. \n2. The method itself is well-motivated and builds on prior work. The novel insight offered by this work builds on AdaShare's \"we can partition layers among tasks in MTL networks\" to \"we can partition layers among both tasks and the examples within a task in MTL networks.\" Shifting the sharing paradigm to DynaShare would likely result in more flexible modeling and improved performance.\n3. Using gumbel-softmax sampling for learning the instance-specific policy is insightful.\n\n**Weaknesses:**\n\n1. The method itself is complex. The loss consists of 4 separate components: training loss, sharing loss, sparsity loss, and an instance loss, each of which has its own loss weight and sometimes nested hyperparameters as well (i.e. target rate t for the instance loss) which would likely significantly affect the performance of the proposed method. It also uses a two-stage training process (first train AdaShare then retrain with DynaShare) which may limit the method's applicability given its increased complexity.\n2. But the most significant flaw in this work is its lack of comparison with any of the benchmarks used in the original AdaShare paper. Further I have never before seen the benchmark datasets used by this work to validate their method (MIMIC-III and Census).\n3. Comparison with other strong multi-task architecture baselines is also missing (i.e. most notably MTAN although including Cross-Stitch and Sluice would also be very welcome).\n\n",
            "summary_of_the_review": "This submission largely focuses on leveraging intuition and explanation (which is perfectly fine) to motivate DynaShare. As a result, robust empirical analysis on classical multit-task learning benchmarks and comparison with other systems is crucial. I think the paper is excellently written, builds on past work, and would increase modeling flexibility and performance. However, it currently does not present compelling or sufficiently robust empirical analysis. Therefore, I must recommend a reject score for this work in its current state.\n\nAdaShare created an excellent github repository to facilitate comparison and reproducibility (link: https://github.com/sunxm2357/AdaShare). If the authors implement DynaShare within this repository, they won't even need to rerun other multi-task baselines like MTAN, Cross-Stitch, and Sluice and can simply use the values reported by the AdaShare paper. Given the availability of this resource, and presenting DynaShare as a natural extension to AdaShare, I think comparison with the AdaShare benchmarks cannot be understated. \n\nIf the authors can compare against any of Cityscapes, NYUv2, or Tiny-Taskonomy, and find their method leads to significantly improved performance over AdaShare, then I would increase my score to an (8) Accept. An analysis into the Computational Cost of DynaShare -- see **Computational Cost (FLOPs)** Section on page 8 of AdaShare paper -- should also be provided as well as an explanation into how hyperparameters specific to DynaShare are selected. I would also be interested in looking at the rate of instance gating (i.e. the layers assigned to a task, what does the distribution look like for the examples accessing each of those layers?). Do different tasks have different distributions of access? What about different layers of the network (i.e. does the distribution change significantly from earlier to later layers or rather if the layer is shared or not shared among tasks)?\n\n**Nitpick:**\n\nIn the introduction, the authors write: Parameter sharing approaches share a common intuition: *for a single network to perform multiple prediction tasks, the network needs to support multiple specialized execution paths (paths through the network at inference).*\n\nIt is my view this perspective is a bit limiting and not aligned with prior research which views hard-parameter sharing paradigms as general purpose feature extractors that is then consumed by task-specific learners. In this instance, one execution path is suitable for learning a representation which satisfies multiple hypothesis classes/inductive biases. That said, I think the authors' perspective on multiple execution paths as a function of input representation may lead to increased modeling flexibility, but I would recommend positioning it as a new perspective rather than one which invalidates prior approaches.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}