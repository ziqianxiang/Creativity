{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents a test-time adaptation technique that estimates batch-norm statistics during test time to adapt a model to target test samples. This is an empirical work that is shown to work effectively. Compared to existing literature, the paper presents two new settings - one in which the batches are not made uniformly from a single distribution and batch sizes themselves can be arbitrary. In fact, the extreme situation of batch size one is also considered. ",
            "main_review": "Strengths\n1) Simple idea that seems to work well\n2) Extensive experimentation on multiple settings: Test-time adaptation, UDA and zero-shot classification.\n3) The performance across the board is shown to be significantly better particularly in small-batch settings.\n4) The improvement is even better when it comes to mixed batch types.\n\n\nWeaknesses\n1) It's not completely clear to me (may be it is there but at least I am confused after reading the paper) if the proposed algorithm operates in an online manner or the statistics are reset after every test sample (or batch).\n2) Details on how \\alpha and \\beta are updated in learnable weights setting appear to be missing.\n3) Surprising to see that adapting \\alpha and \\beta do no matter.\n4) Scales in Figure 4 varies across plots. Not a great practice in my opinion.\n5) Several minor typos in the paper that must be corrected.\n6) The intro is not very inspiring and could have been written in a much better way.\n7) No insights are provided why a single augmentation provides the best performance. Intuitively, more augmentations should help (or least not harm)\n8) The numbers do not match exactly with the TENT paper. It is probably because this work and the Tent paper are probably measuring different things. If I understand correctly,  this work is reporting the average over corruption types at severity level 5, while TENT is reporting the average over corruption types and levels.\n9) Furthermore, it is not clear if their tuning of TENT is different from the original paper. \n10) The overall intuition/reasoning of how using a single augmentation on a single test sample works so well is also missing. \n \n",
            "summary_of_the_review": "The simplicity of the idea and that it seems to work well is exciting. Having said that, given the empirical nature of the work, there are several things that are left unanswered. The intuition is also somewhat missing. Also it is not clear if the TENT algorithm is tuned and used in an optimal manner. Other queries/clarifications sought are listed in the \"Main Review\" section. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a test time adaptation method to estimate BN statistics with a single sample. Specifically, it estimates the global BN statics in the accumulation manner, and the local BN statics on the test sample at inference time. To compute the local statics, it creates augmented views of the test sample to form a small augmentation batch to calculate $\\mu_{local}$ and $\\sigma_{local} $. Then, it mixes up the global and local statistics to calibrate the BN statistics with a fixed weight $m$.  ",
            "main_review": "Strength:\n\nThe paper presents a new test-time BN adaptation method on a single sample. The method has been well-motivated by pointing out the limitations in SOTA methods, i.e., requests a large batch with the same test distribution. \n\nWeakness:\n- It is unclear how the blending parameter $m$ should be selected. In the paper, $m$ is set to a very small value $m=0.05$ for the local statistics. The authors should provide discussion and experiments on how $m$ is selected, and how it affects the performance.\n\n- The experiment results are not completed. The paper only presents the selected comparison results with CLIP in table 2, and the comparison results with test-time methods only on the digital DA benchmark. Comparison results to Tent and T-BN should be provided for ImageNet-C, CIFAR-100/10-C in Table 2. \n\n- The augmentation of the test sample is important for the proposed method, (based on which the local statistics are calculated). More discussion and experiments should be provided in the paper. It is unclear how different augmentations would affect the performance.",
            "summary_of_the_review": "Overall, the paper presents a new test-time adaptation method on a single sample. However, the paper provides very limited experimental results, it is hard to judge how effectively the proposed method works. The main concern is that the effect of the local statistics is actually very small which $m=0.05$, how it largely improves the performs compared to T-BN (only using the global BN statistics) is unclear or unconvincing. The performance improvement shown in the paper is marginal. The author should show at least one strong case to prove the motivation.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper presents a simple method to fastly adapt a source model to target test samples by appropriately changing batch-norm statistics used in the model. Experimental results show that the proposed method works well especially when the batch size of test samples is small.",
            "main_review": "Strength\n\n- The design of the proposed method is simple, and it should be easy to implement.\n- In the experiments, test-time adaptation for zero-shot classification is an interesting problem setting.\n\nWeakness\n\n- Some concerns on the design of the proposed method.\n\n  - The intuition on the design of the proposed method is not clear. Why do we need to mix local statistics with global statistics?\n\n  - The authors state \"such assumptions might not be practical in the real world as the samples might come from an arbitrary distribution\" in the introduction, but it is not clear how this is solved in the proposed method.\n\n  - The definition of local statistics is inconsistent between Algorithm 1 and equations in the manuscript.\n\n- There are some existing works on how to adaptively correct batch statistics in the inference phase, which should be worth citing to clarify the novelty of this work.\n\n  [R1] \"EvalNorm: Estimating Batch Normalization Statistics for Evaluation,\" ICCV 2019.\n\n  [R2] \"Four Things Everyone Should Know to Improve Batch Normalization,\" ICLR 2020.\n\n- In the evaluation of TENT with smaller batch size, do we need to conduct adaptation process for each batch? Since TENT essentially needs a sufficiently large batch by its design, it might be better to wait for storing enough test data during the test phase to conduct the adaptation process (for example, in the case of the batch size of 1, the adaptation process is to be conducted in every 64/200 iterations).\n\n- The motivation to adopt \"Mixed Distribution\" in the experiments is not clear. Since all corruption types are shuffled and mixed, the target data still stem from single distribution, which would contain more diverse samples, though. It should make the problem hard, but I am wondering whether it is really a practical setting or not. When I read the motivation stated in the introduction, I imagined the situation where each mini-batch can stem from different distributions, though all samples within the same mini-batch follow the same distribution. \n\n- In the experiments with source-free DA setting, why do the authors exclude SHOT from the comparison?",
            "summary_of_the_review": "At the currect manuscript, it is not clear when and why the proposed method works well. In addition, I have several concerns on experimental setting. I vote for \"reject.\"",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}