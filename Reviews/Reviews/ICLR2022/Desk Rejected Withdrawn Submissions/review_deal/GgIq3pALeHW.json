{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a method to adversarially train an uncertainty autoencoder (UAE) with respect to both privacy and utility objectives. The privacy and utility objectives take the form of \"private\" and \"utility\" classifiers which aim to estimate the values of private and utility features respectively. The goal of the UAE is, in addition to minimizing its conventional loss, to promote the success of the private classifier and inhibit the success of the public classifier. Experimental results on various dataset demonstrate that the proposed approach succeeds at suppressing sensitive information while enabling estimation of utility features.",
            "main_review": "## Strengths\n\n(1) The writing and figures are clear.\n\n## Weaknesses\n\n(1) The paper fails to cite the majority of work on learning privacy preserving representations through adversarial optimization. See list below:\n\n* [1] Wu, Z., Wang, H., Wang, Z., Jin, H., & Wang, Z. (2020). Privacy-Preserving Deep Action Recognition: An Adversarial Learning Framework and A New Dataset. IEEE Transactions on Pattern Analysis and Machine Intelligence.\n\n* [2] Zhenyu Wu, Zhangyang Wang, Zhaowen Wang, and Hailin Jin. Towards privacy-preserving visual recognition via adversarial training: A pilot study. In Proceedings of the European Conference on Computer Vision (ECCV), pages 606–624, 2018.\n\n* [3] Francesco Pittaluga, Sanjeev Koppal, and Ayan Chakrabarti. Learning privacy preserving encodings through adversarial training. In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 791–799. IEEE, 2019.\n\n* [4] Nisarg Raval, Ashwin Machanavajjhala, and Jerry Pan. Olympus: sensor privacy through utility aware obfuscation. Proceedings on Privacy Enhancing Technologies, 2019(1):5–25, 2019.\n\n* [5] Morales, A., Fierrez, J., Vera-Rodriguez, R., & Tolosana, R. (2020). SensitiveNets: Learning agnostic representations with application to face images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(6), 2158-2164.\n\n* [6] Taihong Xiao, Yi-Hsuan Tsai, Kihyuk Sohn, Manmohan Chandraker, and Ming-Hsuan Yang. Adversarial learning of privacy-preserving and task-oriented representations. AAAI 2020.\n\n* [7] Seyed Ali Osia, Ali Taheri, Ali Shahin Shamsabadi, Kleomenis Katevas, Hamed Haddadi, and Hamid R Rabiee. Deep private-feature extraction. IEEE Transactions on Knowledge and Data Engineering, 32(1):54–66, 2018.\n\n* [8] Jiawei Chen, Janusz Konrad, and Prakash Ishwar. Vgan-based image representation learning for privacy-preserving facial expression recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 1570–1579, 2018.\n\n* [9] Nisarg Raval, Ashwin Machanavajjhala, and Landon P Cox. Protecting visual secrets using adversarial nets. In 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 1329–1332. IEEE, 2017.\n\n\n\n(2) Many of the paper's key claims exist in prior works. See list below:\n\n* Dynamic vs constant setting has been explored in almost all of the papers listed above. See [1] for an example.\n\n* Evaluating against multiple adversaries trained differently been explored in many of the papers above. See [1] for an example.\n\n* The Utility-Privacy Trade-off (UPT) curve has been explored in many of the papers above. See [1] for an example.\n\n\n\n(3) The experimental evaluations are conducted on toy datasets like MNIST and Fashion MNIST.\n\n\n\n(4) The evaluations lack comparisons to the SOTA methods.\n\n\n\n\n",
            "summary_of_the_review": "The paper fails to cite many important works, many of the paper's claimed contributions have been shown in prior work, the evaluations are conducted on toy datasets, and the evaluations lack comparisons to the SOTA methods.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This work proposes a neural network framework based on UAE to generate privatized representations that protects the private portion and preserves the utility portion. The model is trained under dynamic and constant settings against multiple adversaries. Better privacy and utility guarantees are obtained.",
            "main_review": "Strength:\n1. Technically, this work removes the Gaussian assumption on the latent variables and shows improvement in performance.\n2. This work also proposes a new testing standard, which tests the model against multiple adversary models instead of 1. This provides more robustness in practical applications. \n3. Extensive experiment is done to test the effectiveness of the method.\n\nWeakness:\nMy main concern is that a more thorough discussion of the adversary models might be needed. Here are some specific questions that comes to mind.\n1. Was any of the adversary models used in previous works?\n2. What are the motivations for these adversary models? For example, (1) and (2) seems to address the robustness against different initializations. Maybe briefly explain \n3. How strong are these adversary models compared to those used by previous works? Also, how are they compared among themselves?\n4. How likely are theses adversary models encountered in practical applications?",
            "summary_of_the_review": "This work makes a technical improvement upon previous works and proposes a stronger standard for learning private representations. Extensive experiment demonstrates the effectiveness of the proposed method and improvement over previous works. A more detailed discussion on the new adversary models might be needed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes UAE-PUPET, an Uncertainty AutoEncoder (UAE) based framework for encoding data to protect the ability to predict public attributes while reducing the ability of an attacker to predict private attributes. This approach aims to do this without altering the structure of the raw data (hence not simply releasing predicted public attributes as the label). It also targets a privacy-aware utility provider, which doesn’t know the exact randomness of the scheme. This is then cast as a minimax game. The paper concierge two settings, a dynamic setting where the UAE, attacker and utility provider are trained jointly, and a static setting where the adversary/ utility providers are fixed. \n\nTo compare different privacy mechanisms across the range of possible privacy utility trade offs, they propose the Utility-Privacy Tradeoff Curve (UPT), which plots the competing accuracy on public and private attributes, and finds the convex hull. The paper provides empirical analysis on MNIST, FashionMNIST, UCI-adult and the US census data.\n",
            "main_review": "Strengths:\n- This paper demonstrates promising empericals, demonstrating that this UAE-based framework can effectively hide private attributes against the proposed attackers. \n\nWeaknesses:\n- The importance of the five different (or six) different model pairs wasn’t clear and hard to follow.\n     - Additional analysis showing the performance by these different models would be helpful to clarify if one setting always dominates the others / what they add to the analysis. \n     - Moreover, is this number of adversaries enough? Has the max accuracy converged when considering 5 attackers? What if attackers were generated through another mechanism?\n     - In page 8, paragraph 1, the authors state evaluating against multiple accuracies resulted in worse privacy than reported in the Chen et al paper. Is there a reason to believe this wouldn’t happen again but with >5 adversaries or adversaries trained through different? mechanisms, \n- Comparison to baselines is unclear\n     - It would be helpful to describe the UCI-Adult and MNIST baselines to understand how different models compare. \n     - It is currently hard to understand how this paper differs from prior work without reading the prior papers. \n     - This paper emphasized the use of a UAE, removing the gaussian restriction from VAEs. Could they add a VAE baseline to justify the importance of this design decision or offer some sort of ablation study?\n     - How would this compare to using a traditional AutoEncoder or an AAE?",
            "summary_of_the_review": "This paper offers promising results in leveraging a UAE for minimax based training to remove private attributes from data while maintaining public ones. However, the evaluation methodology and comparison to baselines as unclear. Moreover, the emphasis on leveraging a UAE is not sufficiently studied, and more ablation studies are needed. Unless the authors can address these concerns, I currently recommend rejecting the paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}