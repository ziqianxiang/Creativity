{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper contributes an approach to opponent adaptation in\nadversarial multiagent environments (as opposed to explicit opponent\nmodelling). A second contribution is an approach to generate opponents\nthat is linked to the adaptation approach.\n",
            "main_review": "Strengths:\n\nI like the idea of integrating the opponent strategy generation into the same framework of adaptation.\nI also think the idea of adaptable base policies is interesting (but see below).\n\nWeaknesses:\n\n- I found it difficult to understand how some of the ideas that have been introduced are actually implemented. For example:\n - adaptable base policies: I understand that one would use multiple opponents (in Alg 1), to train a policy. The output of that algorithm is an \"adaptable base policy\". What is this, exactly? Eg if your policy is parameterised by a neural network, would this be a subset of the trained parameters? Or all of them? How are they selected? Wouldn't it be important to know when to stop training the base policy for this to work (eg do you stop training the base policies earlier than usual etc)?\n - adaptation steps: what exactly is the step here (Fig 2)? Is each step a full episode, a complete game, or just a turn in a game? With the return on the y-axis, I'd have to guess this to be an episode or full game.. but how would this be used in a single game against a new opponent?\n - Maybe I missed it but if it is possible to train new and diverse opponents wouldn't you iterate the procedure for some steps to improve the base policy? Was this part of the experiments, and if yes, how many rounds till it converges?\n\n- the soccer environment is a relatively simple version of a soccer game, when, for many years, more challenging environments have been used in this domain, like the RoboCup soccer simulations with 11-vs-11 players in a continuous environment. The 2003 Bowling/Veloso paper \"Simultaneous Adversarial Multi-Robot Learning\", for example, has been applied in the RoboCup domain.\nI think it may be useful to start with a simple environment, but I would not call the 1-vs-1 gridworld soccer particularly challenging. Similarly, the other benchmarks may be useful as apart of an evaluation, but describing them as challenging is overselling the problem a bit.\n",
            "summary_of_the_review": "The paper starts with two\tnice ideas, for an interesting and\nchallenging problem of how to adapt to different opponent play\nstyles. The main issue is that it is difficult to see how some of the\ntechnical problems are being solved, and therefore I would find it\nimpossible to reproduce and implement the ideas. The evaluation does\nnot make it clear to me how well the approach actually works, also for\na lack of detail.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents a policy learning method to improve the adaptation and generalization ability in the multi-agent setting. It introduces four concrete components: \n  * build an implicit model of opponents instead of explicit ones;\n  * use a MAML-like algorithm to learn adaptation in few shots;\n  * generate hard opponents in an adversarial manner to robustify the learned policy; \n  * diversify the generated opponents using MMD metrics.\n\nThe proposed method is validated in three experiments.",
            "main_review": "Strengths\n- The paper tackles an interesting and important problem in policy learning, i.e., adaptation and generalization, which is particularly crucial in the multi-agent setting due to the nature of non-stationarity.\n- The proposed method is highly intuitive. In particular, training against a set of hard and diverse set of opponents looks technically sound. \n- The experimental analysis is rich and convincing. The submitted code contains a clear readme doc as well as pre-trained models.\n\nWeaknesses\n- The technical contribution of the paper seems limited. Most of the proposed components are fairly known. For instance,\n   - implicit modeling of other agetns has been a standard approach in previous multi-agent papers [1-3];\n   - MAML-like training is already famous for fast adaptation;\n   - training against adversarial examples is known to improve the robustenss.\n- It is not very clear why the proposed opponent strategy generation (OSG) should work.\n   - Hard-OSG looks very close to generating unconstrained adversarial examples. There seems no gurantee that generated opponents will remain realistic. If the opponent can be arbitrarily strong, does the learning algorithm for the base policy converge?\n   - Related to above, since the Diverse-OSG push the opponents further away from the existing ones, isn't there a risk of generating numerous unrealistice opponents? \n- It looks that the Readme file in the submitted code only contains commands for the proposed method but no baselines.\n\n[1] Implicit Coordination in Crowded Multi-Agent Navigation, AAAI'16 \\\n[2] Decentralized Non-communicating Multiagent Collision Avoidance with Deep Reinforcement Learning, ICRA'17 \\\n[3] Crowd-robot interaction: Crowd-aware robot navigation with attention-based deep reinforcement learning, ICRA'19",
            "summary_of_the_review": "Overall, the paper presents an intuitive method to tackle an important research problem. Yet, the technical contributions seem limited and debatable. I, therefore, consider it below the threshold. I would be glad to adjust rating based on author feedback and comments from other reviews. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper considers the problem of quickly adapting online to an opponent in a 2-player 0-sum game. Using meta-learning, a base strategy is trained such that it can be updated to counter a new opponent within a few update steps. To ensure that the base strategy is broadly adaptable, the opponent policies used during training are specifically chosen to be 1) robust to the base policy even after an update, and 2) diverse from each other. Experiments in poker, grid soccer, and RoboSumo environments are used to demonstrate the technique’s effectiveness.\n",
            "main_review": "The challenge that this paper explores, fast online adaptation to unforeseen opponents, is a good objective of interest to the community. The approach draws on well known techniques (MAML, LOLA) yet is distinct enough in its training regime (e.g., opponent strategy generation) to stand alone. I found the paper to be clear and well organized, aside from a few smaller issues that I will note below. Most of the issues I have flagged with the paper occur in the empirical results section, but until that point I thought the technique was well motivated and described.\n\nBefore my main comments on the empirical results, I’ll mention a few comments about related work. First, the authors’ framing of implicit versus explicit modelling is underdeveloped (the word ‘implicit’ only appears twice in the paper, and is not defined) but more importantly, is already a framing used in earlier work in poker AI, with quite similar motivation to what the authors are proposing. The authors have a different and novel way of actually adapting to opponents without an explicit opponent model, but they are not the first to use these terms or pursue that goal. I have a larger section of comments on this below, and give references for this highly related work. But specifically, in Section 4.2 the authors state “To the best of our knowledge, L2E is the first attempt which learns to exploit opponents without building explicit opponent models”, and this claim is definitely untrue: the authors really should have been able to find the earlier work on implicit opponent modelling in the same poker domain that they use in the paper. An online search for “implicit opponent model” brings up the papers, with that term in their titles, as the first hits. This claim is just untrue and must be removed from the paper.\n\nSecond, I found that parts of the authors’ technique were quite similar to well-known work, such as MAML and LOLA. This is not a problem: the technique makes further training regime contributions that are (to me) novel, and building on well-known techniques is a good thing. However, it felt like the authors were reluctant to note these connections, when it would have been both easier for the reader, and more scientifically honest, to note the similarities early (e.g., when first describing the technique) and in the detailed description. For example, the base policy update rule (although not the overall training procedure) appears to just be MAML. The authors do note at the bottom of page 2 that L2E can be seen as a case of meta-learning, and then mention MAML as a meta-learning technique, but I found the similarity to be so strong that I feel it would have been appropriate to note and cite MAML in Section 1 when the “base policy training part” is being defined, if not even mentioning MAML in the abstract. Similarly, the Hard-OSG updates in equations 6-9 appear to be LOLA: an inner step where the opponent’s policy is updated, and an outer step where we update our policy such that their update is helpful (LOLA) or minimally harmful (Hard-OSG) to our reward. The authors do cite LOLA in the paper, but only in the introduction when listing off deep learning opponent modelling techniques. It is not cited in the body of the paper, when they present a technique seemingly very similar to LOLA. If the authors agree that Hard-OSG and LOLA are similar, then it would be appropriate to cite LOLA more directly when describing Hard-OSG: both to help the reader who might be familiar with LOLA, or who might otherwise associate it and miss a difference, and also to give credit, if LOLA was a partial inspiration for this step. It would not have harmed the novelty of this paper to give proper attribution to related work, and the reluctance to do so is strange.\n\nMy main issues with the paper are in Section 4 (Experiments) and the Appendix. I am very familiar with the computer poker domain, which is the main domain used in the empirical results, so I will focus my critique there and not on the RoboSumo and Grid Soccer environments. I found the poker empirical results in this section to be quite weak or even exaggerated, and at times either simply untrue or of dubious statistical significance. Some of the experiments require a much clearer explanation than what is given, some claims about L2E are entirely unsupported by the evidence, and the opponent strategies used in some results are extremely weak or are not reproducible from the description given. I will list concrete and detailed examples of all of this in the following text. Overall, I found the Experiments section so questionable that I have to suggest rejecting the paper. With a better results section that is clear, statistically significant, and honest about when the technique works and does not, I can imagine suggesting accepting this paper.\n\nI’ll start with the framing of the experiments, and note the missing details that need to be explained much more clearly. The approach here seems to be the normal offline-train-then-online-test approach:\n 1) Train a good base policy with Algorithm 1 (which involves many update steps, training many opponent policies, etc).\n 2) Test against a new opponent not encountered during training, by playing against them online for a long match of many episodes. Update the base policy through some number of adaptation steps (presumably using batches of data collected while playing against them). Measure the resulting performance of the online adapted policy against the opponent over this match.\n 3) Repeat the process many times (Ideally steps 1+2, showing that the base policy is consistently effective and not just the adaptation from one base policy), with each step 2 result producing a value, such that we can statistically measure the set of results with a mean, standard deviation, etc.\n\nIf that’s the process, then Figure 2 makes sense: we have a base policy, we do N adaptation steps, and measure the performance against the target opponent. But what is happening in, for example, Table 1 or Figure 5? \n - Tables 1a and 1b show the performance of three 10k hand matches. But how, and how often, is the L2E strategy updated during each match? A partial answer is in Table 2 in the Appendix, where it sounds like three gradient steps are done, each with a batch size of 10 in Leduc and 20 in LHE. Does that mean that in Table 1a, that L2E plays 10 hands with the base strategy, then does one update step, then plays another 10 hands with a one-step-improved strategy, and so on, and then plays the final 9970 hands with the three-step-improved strategy? Or does the match start with L2E being given three training batches (of the opponent playing against what policy - the base policy?), performing three updates, and then playing 10k hands with no further adaptation? Or have I misunderstood, and L2E collects data and performs many update steps throughout the 10k hand match? Some of these options would be more meaningful than others (i.e, a technique claiming fast online learning should have to collect its data online and pay the cost for using a suboptimal strategy at the start, instead of getting to start the match with three batches and updates), but in any case, the paper must describe the experiment being performed. Unless I’ve missed it, there is no description of what procedure these results are describing, and important constants (batch sizes, number of updates, etc) are only mentioned in the appendix.\n - Similarly, what is going on in Figure 5? The text describes it as a convergence graph, but the x-axis measures episodes. Is this a training graph, where after every N training steps of Algorithm 1, we do 3 updates of the base policy against a target opponent, to plot its ability to adapt over training? Or does it plot a long match against an opponent, after training, with many update steps? I believe it’s the former, but as with Table 1, this is not spelled out in the text. If it is a training graph, maybe the x-axis should be ‘Training update steps’ instead of ‘Episodes’, which suggests an online process? Further, the text describes Fig 5 as an empirical evaluation of L2E’s convergence properties to Nash equilibrium. But *nothing* about these results measures if L2E is converging to Nash (e.g., minimizing its own exploitability by a best response over training); instead, it shows that L2E is able to exploit a few specific opponents more than a Nash approximation can. The text seems to entirely disagree with what I suspect the figure intends to show.\n\nThere are further issues with Table 1:\n - Am I understanding correctly that each cell represents the average and standard deviation measured over only three runs, each representing 10k hands of poker, for 30k hands total? Three samples seems like far too few runs to compute a meaningful standard deviation over! Particularly when exploring adaptive techniques that may, stochastically, work better in some runs than in others!\n - What is the unit in these results? Presumably big blinds/game, but the caption should say.\n - I believe specific values in Table 1a demonstrates that the results are statistically meaningless. I'll give an example. CFR and NFSP are both static strategies: you train them offline, and then can use them online with no further updates. Both aim to be Nash approximations, which (should be) fairly robust. So if we played CFR against NFSP for a sufficient number of hands, we should get an estimate of the expected value for the first player. And then as a separate experiment, if we played NFSP against CFR for a sufficient number of hands, we should obtain approximately the same value, negated, for the first player: they’re the same strategies, we just changed the perspective from which we measured the result. We’d expect some noise because we only ran 30k hands in Table 1, but within that noise, which we could quantify with a confidence interval, we should get two very similar values, just negated. However: in Table 1a, we see that CFR beats NFSP for 0.144 +/- 0.007, and then NFSP loses to CFR for -0.412 +/- 0.040. That is a drastic difference, especially given the small standard deviation value given by the authors, computed from only three samples! The +/- value listing the standard deviation is a tiny value, attempting to lend credence to the statistical significance of the result, but the magnitude of the values is far, far outside the +/- values given. So I have to ask: is this an *exceedingly* unlikely event, or were the results entered incorrectly, or is the standard deviation computed from only three values misleadingly small in this instance, suggesting significance where none exists? The third seems most likely to me: 3 runs was insufficient for computing a meaningful standard deviation, and 30k hands was insufficient for computing a meaningful mean. And seeing that the result is this far off in a case (CFR vs NFSP) that should be pretty reliable, I cannot trust any of the other values in Table 1a or 1b: I would expect adaptive agents to be even noisier than static agents. In prior work using Leduc and LHE, it is much more common to run millions of hands across hundreds of seeds to get statistically meaningful results. I do not understand why such an insufficient evaluation was performed here.\n - NFSP is described in the text as an approximation to a Nash equilibrium. However, these results demonstrate that it is *far* from Nash since it can be beaten so badly in Table 1a by CFR (which itself is a non-exploitive Nash approximation, that minimizes its own loss and as a consequence may have only a small advantage over opponents). Leduc Poker is a tiny game that can be solved to an extremely tight approximation (less than 0.001 big blind/game) in seconds of computation using CFR, so given NFSP’s horrible results it seems misleading to describe both equally as Nash approximations. Further, Table 1b shows NFSP losing horribly to LA and TA, which an actual Nash approximation (even a lossy approximation) should not do. Given these results, I question whether NFSP is implemented or trained correctly; either that is the case, or it is misleading to describe it as a Nash approximation without further qualification of how close of an approximation it is (e.g., by listing its exploitability).\n - In my ‘Smaller Issues’ notes below, I’ve flagged a potential issue in the state representation that would make the L2E, NFSP, CFR, and all other Leduc and LHE strategies extremely weak, by the standards of prior work in this area. For example, even a CFR strategy in this paper using the described state abstraction, would be extremely far from Nash equilibrium in Leduc or LHE.\n - In Table 2b, I wish that a strong strategy (e.g., a CFR strategy using a reasonable abstraction technique to make the computation tractable) had been included as a column, in addition to appearing as a row. The previous page claims: “L2E can quickly adjust its strategy to avoid being severely exploited by the approximate equilibrium opponents”, and this claim should be supported by evidence. Ideally NFSP would suffice as an approximate equilibrium in Table 1a and 1b for this claim, but as described in the previous point, I do not trust the NFSP implementation used here, as it loses badly against many opponents, and a Nash approximation is intended to guarantee a worst-case draw against any opponent. CFR appears in Table 1a and beats L2E badly (thus not upholding the claim) and no CFR strategy appears in Table 1b.\n\nThe ‘Oracle’ rows in Table 1a and 1b lead me to another issue: the description on many of these figures of an ‘Oracle’ player that gives an upper-bound on how much each opponent can be beaten for, and thus what L2E can potentially achieve. The text describes this as: ‘The Oracle represents each opponent’s approximate best response which are obtained using the DQN algorithm trained separately with each fixed opponent. Oracle’s results represents the upper bound of the performance.’ This appears to be referencing a recently published technique, “Approximate Exploitability: Learning a Best Response” by Timbers et al, which does exactly that, but that paper is not cited. In any case, the statement is untrue: as presented, the Oracle lines *are not* an approximation of the best response or an upper bound on performance! The reason is that this DQN-based approach is lossy, due to the state representation used, and an *actual* best response to each opponent could win more than DQN can, even in the limit.\nAt best, the oracle lines provide an approximation of how much value can be won using the same state representation that L2E, NFSP, CFR, and so on are using; this is only described, in far too little detail in Appendix C1, Environment Representation. The exploitability of a given strategy is the amount that a best response, not an approximation in some abstraction, can win against them, and cannot be less than 0. The worst case for a best response is if they play against a Nash equilibrium (which is itself defined by having 0 exploitability). In Leduc poker, an exact best response can be computed in a fraction of a second, since the game is tiny. The computation in LHE is considerably more expensive (“Accelerating Best Response…” by Johanson et al, IJCAI 2011, timed it at 76 CPU-days) but is at least feasible. \n\nThe problem with the Oracle lines in Table 1, Fig 2, and so on, is that the text misleads the reader by describing them as an upper bound on performance, when the values are far, far lower than the value of a true best response. I can point out a few examples of this difference from the results in this paper. For example, in Table 1a, ‘Oracle’ loses to CFR at a rate of -0.089. However, even if the CFR strategy was exactly a Nash Equilibrium, a best response would achieve a value of 0. And more realistically, a CFR strategy will still have a tiny exploitability of, say, 0.001 big blinds/game after training. So, in Table 1a, the ‘Oracle’ row is significantly under-estimating the upper bound that the other players can achieve: it doesn’t even reach 0, let alone exceed it to capture the CFR agent’s suboptimality. Another example is in Table 2b, in the Oracle versus Random cell. The exact value of a best response against Random has been computed by Johanson et al, and is 4.40089 big blinds/game. The ‘Oracle’ in Table 2b achieves 2.682 big blinds/game, again a significant underestimate. If the authors’ intent is to describe a suboptimal but realistic value that L2E could hope to achieve using the same state representation, then that is fine, but it must be described clearly in that way; calling Oracle an approximation to a best response is simply untrue.\n\nTable 2b also relies heavily on evaluation against only four opponents: Random, plus LA, TA, and LP which were described as “...designed by some skilled Texas hold’em player”, and further described in the Appendix as being based on the PokerStove software. I am very familiar with the computer poker domain, but from the main text and the Appendix description, I cannot figure out what these strategies are or how they were extracted from PokerStove. PokerStove is a software package that lets you compute a hand strength metric, by rolling out possible future cards to measure how often one player’s hand or distribution over hands beats another player’s.\nA metric such as this is only a small part of the challenge of playing poker, however, because it does not, in itself, tell you how to *act*. In earlier work in the poker domain, agents used metrics like this as part of a state representation, to summarize discrete card information down to simpler (but lossy) state features, reducing the full scale poker game down to a simpler, smaller, “abstract game” that can be solved using a technique like CFR or NFSP. The actual challenge of the game is in deciding what actions to take, and that is what CFR does; the hand equity information coming from PokerStove would at best be a small precursor step to picking an action. So how is the policy (e.g., action probabilities) produced? Appendix C.1 describes “The specific winning range boundaries and rules are designed by some skilled Texas hold’em player.”, which sounds like the hand equity information from PokerStove was combined with a hand-designed rule-based system for choosing actions in the game. If that is the case, then 1) this description is insufficient for a research paper, 2) the paper is not reproducible without knowing what rules “some skilled Texas hold’em player” chose, and 3) these opponents are likely to be *extremely* weak, as compared to even the rudimentary CFR-based agents described in prior work in the community. The text describes them as “high performance opponents” but there is no proof to support this claim of the agents’ performance; frankly, given the extensive work on creating strong agents published by competitors in the Annual Computer Poker Competition, the claim is simply untrue: these are not high performance opponents at all. As such, Table 1b really cannot provide any evidence to the success or failure of L2E. At best it shows that L2E can exploit very weak opponents, but it does not show 1) how L2E would fare against a Nash approximation, or 2) if L2E would win more from these very weak opponents than a Nash approximation would. But those are claims being made about L2E in the body of the text, such as on Page 7.\n\nOverall, given the insufficient description of the experimental methods (how the players were updated, training using data collected online or not, what the figures mean, etc), the inaccurate description of benchmarks like Best Response / Oracle, the dubious statistical significance of Table 1a and 1b, and the insufficient description or quality of the opponents and NFSP in Table 1b, I simply cannot accept the empirical evidence presented as sufficient to demonstrate the L2E technique. Each of these issues is addressable:\n 1) Describe in more detail the training and testing procedure used in Table 1 and Fig 5. If the agents are not using online experience as their training batches, justify why. If agents train using 30 hands and then play 9,970 or 10,000 hands with no further updates, justify why.\n 2) Run a *much* larger set of random seeds in Table 1. State confidence intervals, and make sure that you’ve run enough hands for your results to be meaningful.\n 3) Accurately describe the Oracle results. They are not an upper bound on performance. At best, they are a lower bound on the upper bound. Where possible, use actual best response values instead of lossy DQN approximations. This is trivial in Leduc.\n 4) Run Table 1b with one row and column being a strong Nash equilibrium approximation, such as by using CFR with a reasonable state space abstraction so that it can be tractably solved. For all other column players (LA, TA, LP, etc), describe them in sufficient detail to be reproduced (e.g., no appealing to ‘designed by some skilled Texas hold’em player’).\n 5) Address my comments below about the extremely lossy state representation used in Leduc and LHE. All of the agents that use that state representation (I believe L2E, NFSP, CFR, and probably DQN, MAML, etc) are far underperforming what is possible given what is published in the computer poker community for even small and crude LHE agents. Leduc is a tiny testbed game, and there is no reason to use a state representation that is lossy.\n\nAs is, I do find L2E to be a well motivated and potentially useful algorithm. I have to suggest rejection, but by addressing the issues noted above, the connection with prior Implicit Opponent Modelling work in this space, and some of the smaller issues I’ve noted below, I can imagine recommending acceptance at a future venue.\n\nNotes on Implicit Opponent Modelling, and prior work:\n - This paper only uses the term ‘implicit’ twice: once in the abstract and once in the introduction, and in neither instance is the term defined, or an example given of what an ‘implicit opponent model’ is. The authors should define what an “implicit opponent model” is in this work, as clearly as they define explicit opponent models.\n - Implicit versus explicit opponent modeling has been proposed and explored in recent earlier work, in the poker domain, for fast online adaptation to unforeseen opponents. These earlier works are similar in objective to the authors’ paper, and the authors should cite those works and describe how their paper differs. There *is* a difference between how this paper and the earlier work explore the idea, and this approach retains its novelty. It just loses the novelty of framing implicit versus explicit modelling, which is already established by earlier work. Consider the following two papers in the poker domain, both of which predate the authors claim of L2E being the first to exploit opponents without building explicit opponent models:\n   - ‘Implicit Opponent Modelling via Dynamic Case-Base Selection’, Rubin and Watson, 2011.\n   - ‘Online Implicit Agent Modelling’, Bard et al, AAMAS 2013. \n\nSmaller issues:\n - Introduction. You note two issues with prior explicit opponent modelling work (1: requires a lot of data, 2: depend on a similar opponent used for testing). You’ve missed an important third issue that L2E also avoids: incomplete information. In games like poker, but I’m guessing also RoboSumo, our agent only receives their own state observations, and not those of the opponent. For example, if our opponent folds in poker, we never observe their cards, and thus cannot easily create an explicit opponent model that requires us to know what cards they fold with. Implicit opponent models avoid this problem.\n - You refer to ‘opponents’ and 0-sum games throughout. But is this restriction actually necessary? It seems like L2E would work just as well for cooperative settings (like LOLA) or non-zero-sum environments where each agent is incentivized to maximize their own reward and not necessarily to minimize their opponent’s.\n - Phrasing like ‘hard-to-exploit’ when motivating OSG. ‘exploit’ has a widely used meaning in the computer poker literature, where ‘exploitability’ means ‘loss to a best response’, and ‘hard to exploit’ thus means ‘near Nash equilibrium’.  What you mean here is ‘hard-to-exploit-by-the-base-policy’, and you actually do want opponent strategies that are exploitable in general. If your OSG method produced strategies that were actually hard-to-exploit by the conventional meaning, thus near Nash equilibrium, then I doubt they would be useful opponents for updating the base policy: there would be no weakness there for the base policy to adapt to and exploit. Be aware that phrasing like ‘hard-to-exploit’ could thus be misunderstood by readers familiar with the computer poker domain that you’re using.\n - Algorithm 1. We call the function “P = Diverse-OSG(B, O, N)”. What is N? It’s not an input, output, or otherwise used in the algorithm.\n - Section 4: there’s a reference to the poker state representations in Appendix B.2, but that seems like the wrong ref (B.2 is Diverse-OSG). Do you mean C.1?\n - Sec 4.1: L2E is described as “The base policy can quickly approximate the best response strategies”. Do you mean that it approximates the *strategy* or that it quickly converges towards the *value* of the strategy? What you demonstrate is achieving a similar value, but present no results on the strategy itself converging.\n - Sec 4.1: As I noted above, you need to describe concepts like the test-time batch size, number of updates, whether updates are done online or before the match starts, and so on, at this point in the paper. You cannot leave it undescribed, or with constants (batch size, # updates, etc) only mentioned in the appendix.\n - Fig 2: As with your later results, these plots should show a CFR line in orange. Show how much better your exploitive approach can do than a non-exploitive but simple to compute Nash approximation. Also, before this point, you have to say how large a training batch is. It’s not fast adaptation if a training batch was thousands of hands. And the caption cannot describe oracle as an approximate best response, without quantifying how close the approximation is; like I described above, these ‘oracle’ values are a very lossy approximation.\n - Sec 4.2: The test says “We have redesigned and reimplemented the MAML algorithm for the two-player competitive environment.” How was it changed? This sounds like an exaggeration.\n - Sec 4.2, CFR and NFSP. If these are described as being approximate Nash equilibria, you should justify how close to Nash they are. That value is computable for Leduc in a fraction of a second, and is tractable in LHE. Claiming that it’s an approximation, without justifying how exploitable it is, can be viewed as an exaggeration of your results.\n - Fig 3a: This figure is unclear. Which Flop decision points are these plots showing us the action probabilities for? If it’s one particular flop state (i.e., preflop betting sequence to reach the flop, and flop betting sequence thus far), then say which sequence it’s for. If it’s an average over all Flop betting sequences, then say that, but also - is it a weighted average using the probability that the players reach the sequence? In either case, the top plot shows all four strategies only raising and never calling or folding, and the bottom plot shows strategy 1 folding the best possible, guaranteed to win, hands. Is that actually possible or desirable?\n - Sec 4.4. How are strategies generated without Hard-OSG? Generating them without Diverse-OSG makes sense (just drop the diversity term in the loss), but you should describe in more detail how strategies are generated without Hard-OSG.\n - Fig 4: This looks like the only graph, at least in the main text, that uses the grid soccer environment. Since most of your results are in the poker domains, this graph would be much more meaningful to the reader if we saw how the *poker* agents’ performance dropped in the ablation study. Can you present poker results for Fig 4 instead, and perhaps use the grid soccer environment in addition to an existing other figure?\n - Appendix C.1: In the rules description, note that in poker, the highest card (Leduc) or highest hand (hold’em) *does not* win the game. It wins a *showdown*, if a showdown is reached. The point of bluffing is to make an opponent fold the stronger hand, so that a showdown is not reached.\n - Appendix C.1: Hold’em deals 5 public cards, not three. Call and Check are the same action, and are only available in disjoint states (e.g., a check is just a call when you are not facing a bet). \n - Appendix C.1: From the state representation description, I now understand why many of your poker results are so weak compared to the existing literature. For example, your text describes Leduc using a vector with 6 dimensions to encode the cards, and LHE with 52 dimensions to represent the cards. Are these N-hot vectors, each representing one card in the deck, where you set the dimension to 1 if a particular card is visible, and 0 otherwise? If so, then that is a terrible and extremely lossy representation. That would lose the ability to represent whether a particular card is held in our hand or is a public card. In LHE, we would represent the cases (hand=AsAd, board=Ac2h3h) and (hand=2h3h, board=AsAdAc) the same way. But in the first case we have a very strong hand, and in the second case we have a very weak hand! Similarly, in Leduc, the difference between us holding K with a J on the board, or us holding J with a K on the board: representing a likely win identically with a likely loss. In Leduc, this can be addressed as easily as adding another 6 entries to the vector, and using one set for the private card and the other for the public card. For LHE, please look into the existing literature on state representations in poker. The betting representation in LHE also seems extremely lossy: there are 7x9x9x8 possible betting sequences on the river, which you are representing with only 20 dimensions. If you want to use a lossy betting representation, you must describe what those 20 dimensions mean. But my main concern is the card representation. If you want comparable results, you should look into how the existing literature has done card abstraction. Your results will look much better, and by that I mean all of your L2E, NFSP, CFR, MAML, and so on results, if you use a meaningful state representation as opposed to what I believe is this needlessly lossy one. In any case, this section needs more detail explaining what exactly is being done.\n",
            "summary_of_the_review": "L2E is a well motivated and potentially useful meta-learning approach to online opponent modelling. However, the empirical results are insufficient to support the work, are of unclear statistical significance, and some claims made by the authors are either unsupported or are  false. As described, the experiments are not reproducible. There is also a missing connection to similar prior work in the same environment. I suggest rejecting the paper.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}