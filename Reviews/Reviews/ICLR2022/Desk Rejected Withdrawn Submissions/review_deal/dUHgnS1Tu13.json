{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors proposed a new scheme to balance the trade-off for large and small patches, allowing for both coarse global interactions and fine-grained local interactions simultaneously.  In the local attention layer, attention was applied to each patch and its local shifts and then these patches are fed into the global attention layers. The global attention layers apply the pyramid of attention with decreasing resolution inputs. As opposed to the static positioning in other visual transformers, the authors demonstrated improved results on CIFAR dataset.\n",
            "main_review": "Strengths: the authors bring out a very important question in the patch design part of Transformer and proposed a novel scheme to balance the issue of the number of parameters/computational cost and model effectiveness. The Local-Global Shifting is designed to allow both coarse global interactions and fine-grained local interactions.\n\nWeakness: compared with Swin-Trans, CVT and Nest, the improvement is limited, as different Trans training techniques can vary a lot in these different architectures, it would great if such techniques are applied and further investigated.",
            "summary_of_the_review": "It is a very good exploration in designing the novel local-global attention layers in Transformer as opposed to the static positioning in VIT. Compared with Swin-Trans, it would be great if the benefit of this design can be further illustrated and justified.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents local-global shifting vision transformers to learn vision representation hierarchically. The proposed local attention layer applies local attention to each patch and its local shifts to capture the local structures. The global attention layers with a pyramid architecture are also designed to further learn the features. Experiments are conducted on CIFAR and ImageNet.",
            "main_review": "This paper is well organized and easy to follow. Although the idea of using local shift and local attention to capture local relationships is somewhat new, I still have quite a few concerns about this paper:\n\n- The proposed local attention method is very close to conventional 2D convolution. I think this operation can be easily replaced by many previous methods like Swin and convolutional patch embedding in [r1].\n\n- The idea of using global and local attention to improve vision transformers is not new. Twins [r2], PVT, and many others have already thoroughly studied this problem. I think the proposed method doesn't have any significant advantages compared to these previous methods.\n\n- The improvement over previous work is not very significant. The smaller model (Our-T) is significantly worse than many previous works like PVT.\n\n[r1] All Tokens Matter: Token Labeling for Training Better Vision Transformers\n[r2] Twins: Revisiting the Design of Spatial Attention in Vision Transformers",
            "summary_of_the_review": "Overall, I think the novelty of the proposed method is very limited. The authors fail to show the advantages of the proposed model compared with the previous ones. Therefore, I would like to recommend rejecting this paper.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors proposed a new model called local-global shifting vision transformer for image recognition. In this model, images are first split into patches and then each patch attends locally to its surrounding patches through self-attention. Afterwards, global attention is then applied to the feature maps globally at multiple scales. Combining the local and global self-attention mechanisms, the proposed model achieved good performance on both low-resolution data CIFAR-10 and CIFAR-100 and regular resolution images of ImageNet-1K. The ablation studies on CIFAR-10 show that paying attention to surrounding patches locally through either local shifting or convolution operation.",
            "main_review": "Pros:\n\n1. The authors designed a new vision transformer architecture that first performs local self-attention on high-resolution early feature maps and then global self-attention on lower-resolution later feature maps. \n\n2. The performance is reported on CIFAR-10/CIFAR-100/ImageNet-1k, and compared with prior arts such as PVT, Swin Transformer, etc. \n\n3. The authors conducted ablation studies on CIFAR-10 to show that local self-attention with separate positional embeddings for different neighbors is important to achieve good performance.\n\nCons:\n\n1. The novelty of the proposed method is limited. First, modeling local visual dependencies at the earlier stage is not new. We have seen previous works that either perform convolution locally (e.g., [a][b]) or local self-attention at the earlier stages (e.g., [c][d]). At a high level, the proposed method is very similar to the convolutional operation. On the other hand, applying global self-attention at the later stages which have reduced feature maps is also a typical way in various vision transformer architectures. \n\n[a]. Early Convolutions Help Transformers See Better. Xiao et al.\n[b]. Container: Context Aggregation Network. Gao et al.\n[c]. Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding. Zhang et al.\n[d]. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. Liu et al.\n\n2. On CIFAR-10 and CIFAR-100, the proposed method achieves better performance and comparable throughputs than previous works. However, on ImageNet-1k, the proposed method achieves comparable performance but much lower throughputs. For example, with a small model, the top-1 accuracy is 82.2 but the throughput is as low as 298.9 imgs/s. Based on these overall results, it is difficult to prove the proposed method is a good choice of vision transformer architecture. Moreover, when further increasing the input image resolution for object detection tasks, the proposed method will face difficulty if applying global self-attention to all later stages, knowing that the sizes of feature maps at later stages are still hundreds by hundreds. Without more experiments on object detection tasks, the effectiveness of the proposed method can not be properly justified.\n\n3. The authors missed a lot of related works. Besides those listed in the paper, there are a number of works that combine local and global self-attention either simultaneously or alternatively, such as [a][b][c][d]. All of these works are designing some techniques to efficiently model both short- and long-range interactions across different visual tokens. Particularly, the proposed local-shift attention in this paper is very related to the one proposed in Focal Self-attention, i.e., finding the surrounding fine-grained tokens to compute the keys and values for each query at the center. \n\n[a], Long-Short Transformer: Efficient Transformers for Language and Vision. Zhu et al.\n[b]. Focal Self-attention for Local-Global Interactions in Vision Transformers. Yang et al.\n[c]. RegionViT: Regional-to-Local Attention for Vision Transformers. Chen et al.\n[d]. Local-to-Global Self-Attention in Vision Transformers. Li et al.\n\n4. In Table 1, the compared complexity and region for some methods are incorrect. For example, the complexity is not exactly O(B^2) for PVT since it reduces the size of key and value maps. NesT is not purely a local attention method because it also performs global aggregations in the middles. For Swin, the complexity is not O(B^2)  with B=7. The number of windows should be also considered. The authors should be more careful about these comparisons across different methods.\n\n5. Typos and unclear points. In Sec 3.2, the O_1^i above Eq(1) should be I_i^1 based on the definition at the end of Sec 3.1. In table 5, it is not clear what the convolution variations mean. According to Sec 3.1, the convolutional operation is only applied to aggregate each patch into a single embedding. However, here, it reads as the authors used convolutional operation for local modeling instead of using self-attention to shifted patches.",
            "summary_of_the_review": "This paper presents a local-global shifting vision transformer architecture for image recognition. Though the authors showed some promising results on CIFAR-10/CIFAR-100/ImageNet-1k, there are still a number of drawbacks in this submission, as listed above. As such, I think this submission is not qualified to this venue yet. More experimental justifications are needed to verify the effectiveness of the proposed method across different tasks, including high-resolution ones such as object detection. In the revision, the authors should also include more related works for a more comprehensive literature review.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}