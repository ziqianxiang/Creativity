{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper introduces a new CAM-based method called Poly-CAM, which aims to generate high-resolution maps.  In particular, it proposes to utilize activation maps (in higher resolution) from earlier layers via a local normalization (LNorm) operator, and contribute a way to compute the weight in CAMs. The author claims this method achieve the state-of-art performance on corresponding metrics for explaining network prediction.",
            "main_review": "Pros: \n+ Explainability of neural networks is an important research topic and highly relevant to ICLR audience.\n+ This work aims to solve an inherited problem (low resolution) existed in previous CAMs.\n+ For the definition of weight, this work build on the top of CIC from Score-CAM and introduce two extensions, CDC and CVC.\n+ It achieves a better performance on insertion and deletion metrics.\n\nCons: \n- The paper is not well written and hard to follow. For example, the notations are messy (in section 3.1). The notation is not consistent (use L in 3.2, while use l in 3.3). For the core equation (4), what is the definition of P^c_l? It only shows the recursive step without the final result like equation (3).\n- The reason for using LNorm is not clear enough. Why it works? Or is it necessary? If I understand correctly, it measures the distortion of pooling and upsampling, but what kind of role does it play in (4)? More explanations are needed before I consider increasing my score.\n- The insertion-deletion difference metric cannot be regarded as a new metric, it is just a subtraction without any information gain.\n",
            "summary_of_the_review": "Overall, I vote for rejecting, with lots of concerns unsolved. \n\nI like the recursive approach that backpropagate the weight from final layer to earlier layers so that the generated map is in high resolution. My major concern is about the clarity of the paper and additional explanations for key operations. I can increase my score if the authors can address my concern in the rebuttal period. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The authors proposed a new XAI method, named POLY-CAM, to extend classic Grad-CAM to generate high resolution class activation maps, without relying on gradient backpropagation. Briefly speaking, POLY-CAM is achieved by aggregating the activation maps from the early layers to the last layers instead of sticking to the last layer only. POLY-CAM shows the state-of-the-art performance in terms of faithfulness metric, and unsurprisingly passes the sanity check. \n",
            "main_review": "1.\tThe largest claimed contribution of the paper is using activation maps from multiple layers instead of the last layer. It’s not surprising that the earlier layer activation map you use, the higher resolution you can possibly get, and thus the subtle relationship you can better obtain. An immediate question naturally arise is the good performance due to the usage of the first layer or it’s due to the mixture of different layers. The author should at least list the activation map w.r.t each single layer as baselines, using the 3 weights mentioned in the paper. \n2.\tAccording to my understanding, the XAI method can only achieve a tradeoff between resolution and robustness. For example, Grad-CAM is low resolution but robust whereas saliency methods are the opposite. Since POLY-CAM resembles more to the saliency methods, the author should at least choose some classic saliency methods such as SmoothGrad and Integrated Gradient as baselines too. Additionally, a sensitivity analysis is needed to demonstrate to what extent POLY-CAM is robust to the adversarial attack.\n3.\tThe insertion and deletion faithfulness metric has an intrinsic problem in the sense that the inserted/deleted image is most likely out-of-distribution. That being said, the change of class softmax output can be ill-behaved and not really trustworthy. How to justify this?\n4.\tHow is P_L^c is defined in Eq. (4)? \n5.\tHow is  s_l set? The closer to the input the larger? If so, it implies that maybe the activation map from the first layer can suffice. ",
            "summary_of_the_review": "\nOverall, the paper aims to tackle the interesting XAI problem. It’s well-written, easy to follow, and the literature review is quite comprehensive. However, the questions need to be addressed to my concern of the novelty.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "\nThe paper is about visual explanations and builds on top of gradCam. The paper makes the correct observation that gradCam is low resolution. The paper proposes to add intermediate featuremap activations to retain the high-resolution information. Experimental results and visualizations demonstrate that the method can keep more information than considered baselines.",
            "main_review": "Strengths:\n\n- Easy to read and understand\n- Intuitive idea\n- Results look promising\n\nWeaknesses:\n\n- Very similar work on high resolution class activation maps is missed, see missing ref [a] below.\n- Bit sloppy in equations, see details below.\n\nMissing citation:\n\n[a] Shi et al \"Zoom-CAM: Generating Fine-grained Pixel Annotations from Image Labels\" In International Conference on Pattern Recognition (ICPR), 2020\n\nThe missing citation [a] uses exactly the same idea of exploiting the activations of intermediate layers to generate high-resolution visualizations. Thus, the \"novelty\" claims in this ICLR submission seem not so strong.\n\nSure, the exact approach is slightly different than [a], but I would expect:\n- a conceptual comparison of how this ICLR submission is different in related work\n- an experimental comparison to demonstrate the empirical difference \n\nSome details:\n\n- Eq 1: what is the min or max of a matrix? Is it a scalar? Then you subtract a scalar from a matrix (which is technically not possible, as only matrices of the same size can be subtracted), so I assume it is all done element wise? (ie: the scalar is 'broadcasted' to be of the same size as the matrix? It would be good to make this explicit.\n\n- Eq 4: the term \\bf{w} is not defined. I assume these are the learnable parameters in each layer l? I think these are called \\Theta, earlier, which is confusing. It would be good to make this explicit.\n",
            "summary_of_the_review": "The idea is sound but there is a very similar unreferenced published paper [a]. Without a conceptual and empirical comparison to [a] I cannot accept this paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I do not think that the missing citation is done on purpose. So, I have no ethical concerns.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new method, Poly-CAM, to produce high resolution class activation maps without gradient backpropagation. The proposed method gradually upsamples the activation maps from the deeper layers and uses them to weigh the activation maps in the shallower layers. The proposed method also studies three different weighting methods.",
            "main_review": "+The proposed method is motivated well.\n+The experiments seem convincing and the performance is impressive.\n+The analysis in this paper is good.\n+The paper is easy to read and follow.\n\n-I have to say that I am not the expert in this area. While I find the proposed method is effective, I feel that the main idea of fusing multi-layer activation maps is not very exciting. The intuition behind the ``Channel-wise Decrease of Confidence'' is also straightforward.",
            "summary_of_the_review": "I think it is a good submssion, as the proposed method seems effective and the analysis is good. However, I feel that the idea is somewhat incremental. Fusion is not an exciting idea to me. But I am not the expert in this area, so I am open to hear from other reviewers and the authors.\n\n###############################################################\nI thank the authors for their replies and additional experiments. In the initial stage, I found the proposed idea of fusing multi-layer activation maps not significant. As other reviewers pointed out, this idea has been explored before. I did not find a high-level discussion that explains the differences between the proposed idea and the idea in the published paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}