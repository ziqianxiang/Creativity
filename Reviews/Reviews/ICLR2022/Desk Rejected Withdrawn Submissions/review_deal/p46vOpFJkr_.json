{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This study is built upon the existing work of supervised contrastive learning (Khosla et al. (2020)), where a data augmentation is added to generate more training samples. The augmentation is conducted by applying Grad-Cam to mask the images in order to preserve the class-related semantic information. The experiments on two benchmarks show the performance-boosting over the baseline.\n",
            "main_review": "The pros and cons of this work are described below.\n\n**Pros**:\n\n+The proposed approaches ExCon and ExConB sound technically correct. And the idea is easy to follow.\n\n+The ExCon improves image classification accuracy on the Tiny ImageNet benchmark, while the improvement in Cifar-100 is subtle.\n\n+The t-SNE map shows an interesting point: the proposed approach results in a more separable distribution of feature embedding among different classes.\n\n**Cons**:\n\n-The novelty is limitted.\n\n1) The novelty is weakened by the existing work of Khosla et al. (2020), where the proposed approach follows the training pipeline proposed by Khosla et al. (2020).\n\n2) The main difference lies in that besides augmentation used in Khosla et al. (2020), the proposed ExCon/ExConB incorporates more augmented samples. From my point of view, increasing the augmented data makes the comparison unfair, where the performance with mask augmentation only needs to be reported. Additionally, it is supposed to provide the details about the number/ratio of augmented data v.s the original. \n\n3) I doubt the claim \"proposed framework is the first to explore explanation-driven augmentations in the contrastive learning setup\" as several works incorporate the explainability (class-wise activation map) into training, resulting in performance-boosting, e.g.\n\n[1] Li, Kunpeng, et al. \"Tell me where to look: Guided attention inference network.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.\n\n[2] Fukui, Hiroshi, et al. \"Attention branch network: Learning of attention mechanism for visual explanation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.\n\n[3] Wang, Lezi, et al. \"Sharpen focus: Learning with attention separability and consistency.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.\n\n[4] Yan Luo, Ming Jiang, Qi Zhao; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2019, pp. 0-0\n\n\n-The experiments are limited to support the claimed contributions and some details are missing for reproducibility.\n\n1) Explainability is not thoroughly examined. Though the two types of scores are provided to evaluate the explanation quality, it lacks the comparison between the class-wise activation map and the segmentation ground-truth, showing the attention highlights the \"semantic\" information related to the target classes.\n\n2) How much extra augmented (masked) data are involved in training? Does the variation in the ratio of mask augmentation v.s original (random) augmentation impact the performance?\n\n3) Authors claim that ExConB is an improved approach compared to ExCon because ExConB incorporates more negative training samples. While it seems not agreed by the results as the ExCon shows better accuracy on Cifar-100.\n\n4) The selected dataset is not interesting. Both are not large-scale, and multi-class image classification is missing. And It is supposed to have an apple to apple comparison with the baseline, e.g., the experiment on ImageNet is missing. How did the authors get the performance for the previous work? I notice that the accuracy on Cifar-100 is different from that reported in the paper by Khosla et al. (2020).\n\n5) The proposed approach involves some engineering tricks to tune the model and hyper-parameters choice, and the discussion is missing. \n\n6) I was curious about the choice of activation maps, e.g., the impact of different activation maps on the performance. Additionally, it is supposed to discuss converting the attention maps to binary masks, e.g., the choice of threshold. \n\n-The presentation needs improvement.\n\n1) Some notations lack description when they are mentioned the first time, e.g., ExConB, hyper-parameter \\tau in Equation 2, etc. \n\n2) In Figure 1, it is confused to see that masked image samples in the left part where the original work, Khosla et al. (2020), does not have such augmentation. \n\n3) I highly suggest authors avoid subjective description/claims, for example: \"Our training pipeline is trivial to implement.\" \n",
            "summary_of_the_review": "The novelty is weakened by the existing work SupCon. The experiments are limited to support the contributions of the proposed works. Additionally, presentation/writing needs improvement. Overall, the paper does not reach the acceptance bar of ICLR. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper uses GradCam to construct augmentations that only remove the background information in an image. The paper uses the augmentations to create new views for supervised contrastive learning. Additionally, the authors change the supervised contrastive loss function to keep some augmentations of images as negatives. The paper shows small lift on CIFAR100 and tiny ImageNet.",
            "main_review": "This paper introduces an interesting idea, but I believe it requires more work to validate the paper's main claims. The writing and presentation can also be improved in some aspects. I believe with a few more iterations, this paper could be really compelling and interesting.\n\nThe performance lift over SupCon is very small for CIFAR100 (some of the bolded numbers are within error bars of some of baselines). The lift on tiny ImageNet is larger, but I'm worried that the batch size is too small for the supervised contrastive learning setting. With a batch size of 128, you expect each class to have very few positives in each batch. This makes SupCon almost reduce to SimCLR! Increasing the batch size to get more positives for each anchor would be helpful.\n\nThe qualitative evaluation of the embeddings could also use more iterations. The visual examples in Figure 1 are helpful, but they are only spot instances -- are there quantitative metrics about how close augmented points are to their original anchors? E.g., what is the average similarity between images and their augmentations across the entire dataset?\n\nThe t-SNE visualization is nice, but t-SNE can be misleading (https://distill.pub/2016/misread-tsne/). That makes the quantitative evaluation even more important!\n\nThe drop and increase scores/fidelity scores in section 4.3 are critical to the evaluation, but they are not very clearly explained. It would be useful to have some formal notation for clarity. It is also hard to tell whether the reported values actually lead to better explanations. It would be useful to show gradcam visualizations in the paper to compare.",
            "summary_of_the_review": "In summary, the method is interesting, but the empirical results of accuracy are not very compelling, and the evaluation of embedding quality needs more support. As a result, I think this paper is below threshold in its current form.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes using masking operation to augment the original images to facilitate the supervised contrastive learning. The mask is generated dynamically by GradCAM during training. The masked images are treated as either positive or negative examples based on the classifier outputs. Experimental results on CIFAR-100 and tiny-ImageNet show both the accuracy and the explanation of the model learned by proposed method are better than the original supervised contrastive learning baseline.  ",
            "main_review": "Pros:\n1) The proposed method is simple and easy to follow. \n2) The results show that proposed method can improve both the explanation ability and accuracy of trained model. \n\nCons:\n1) The paper claims that the proposed method can yield more stable training. But from the results presented in sec. 4.1, I cannot make the same conclusion. What do the authors mean by the term \"stable\"? Can the authors provide more convincing evidence to support this claim? \n\n2) At the early stages of training, the feature or the classifier is not well trained. So the incorrect prediction for the masked image can be attributed to the imperfect encoder or classifier. That means, it is possible that the masked image can be underlying positive example but with incorrect prediction. But in ExConB, these masked images are simply treated as negatives, which may introduce noise into the contrastive learning. I wonder if ExConB is more sensitive to some training hyper-parameters or the characters of datasets. \n\n3) For the experiments on Tiny-ImageNet, the authors did not separate test and validation set. But the authors still demonstrated the best validation results, which means the authors select the models directly from the test set. It may have the risk of over-fitting. \n\n4) For the results on CIFAR-100, I notice that the results for SupConOri are much lower than those reported in the original paper. Is it because the authors use a smaller batch size? If so, I wonder if the proposed method can also achieve the same performance gain when using a larger batch size. As the trends for CIFAR-100 and Tiny-ImageNet are not exactly the same, I would like to see experimental results on more datasets, e.g. full ImageNet.\n\n5) For the sec. 4.3.1, the discussions about the results are missing. For the t-SNE visualization, are all the shown masked images positive ones? \n\n6) Can the authors provide some salient region visualizations for both proposed method and baseline? \n\n7) The paper writing can be improved. ",
            "summary_of_the_review": "Overall, the proposed method is simple and easy to implement. But I have some concerns relating to the technical solutions and some experiment results. Please see the Main Review part for more details. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes qualitative improvements, of an already existing work from Kosla et. al 2020 on Supervised Contrastive Learning for images, using feature importance scores provided by explanation algorithms such as GradCam. More specifically, the paper suggests that we can obtain better performance for supervised contrastive learning if instead of random augmentation of input image data, which for instance involves random cropping or rotation, we use masked anchor images to augment the data. The masks are generated using feature importance scores of anchor images and a user provided threshold. The authors of ExCon paper suggest that high feature importance scores indicate semantically meaningful parts of the image that are strongly correlated with the model prediction, hence augmenting data with those segments of image, while masking out the rest, will help to generate images that are closer to anchor semantically in the embedding representation, as opposed to random transformations. The main contributions of the paper include: defining a loss function that takes into account masked anchor images, qualitatively and empirically showing that the new loss leads to better accuracy on the downstream classification tasks and increases the explanation quality.",
            "main_review": "**Strengths**\n\nThe work on explanation-driven supervised contrastive learning is novel. I didn’t find similar papers in the past.\nThe authors showcase clearly that explanation-based data augmentation improves\nthe accuracy score of contrastive learning models for images using CIFAR, Imagenet datasets and different batch sizes\nthe quality of augmented data based on the T-SNE plots.\nThe samples from the same class are closer to each other in the embedding representation space and farther apart from other classes compared to the baseline approach Khosla et. al. 2020.\n\n*  The authors took into account the quality of the explanations by looking into the explanation infidelity score and the prediction scores of explanation-based masked inputs.\n*  In my opinion the ideas are clearly explained in the paper. Also  the authors compared their approach with a state of the art contrastive supervised learning approach (Khosla et.al. 2020).\n\n\n**Weaknesses**\n\n**Major Weaknesses**\n1. The authors of the paper mention that we can use any explanation method to generate the masks, however they run the experiments only with GradCam. GradCam represents the product of a layer activation and gradients of output w.r.t. the same layer activations. The authors mention that they used the last convolution layer to compute GradCam score but there is no description that they probably interpolated GradCam heatmaps to the size of input to obtain the masks.\n   - It would have been great if the authors described it in more detail. \n   - Apart from that, there are other explanation methods such as Integrated Gradients, LIME, KernelSHAP, etc that  are more commonly used in the practice and it would have been great to benchmark those explanation techniques as well.\n   - Moreover it would be also interesting to measure the effects of different threshold values used for explanation masks on the quality of contrastive supervised learning.\n2. The algorithm description in the Algorithm 1 listing, Section 3.4 seems very high level.\n    - In the paper the authors mentioned that for some cases they started computing the explanations after K epochs because the explanations in the early epochs of the training aren’t reliable but this is not captured in the algorithm description. \n   - Also, \\Epsilon <- Train ( \\Epsilon, X_tilda, Y_tilda) and \\C <- Train(\\Epsilon,  \\C, X_tilda, Y_tilda) do not tell us what parameters we are learning here. Mathematically, there should be dependency on dependent variables, weight matrix Theta or W weight parameters that we are solving this optimization problem for.\n   - Apart from that, it looks like they are freezing the weights of the encoder before training the linear classifier according to the paper description. That, however, doesn’t seem to be captured in the algorithm description. The algorithm description looks a bit incomplete.\n3. Currently loss function formulation looks very similar to Khosla et. al. 2020. (cross-entropy loss) except that we use differently augmented (explanation-based)  dataset. I wonder if we could define the loss in a more generic way, for example,\n propose to use any similarity metric between e_i and e_p instead of sticking to dot product. And also compare with other works that referred to Khosla et.al. in their experiments such as Weakly Supervised Contrastive Learning \n4. The experiments, which are done for Imagenet and CIFAR dataset, are interesting but I   feel that they aren’t very well explained. For example there are many plots without explanations. E.g.:  `Drop and Increase Scores` plots and the explanation section are very short and barely explain the experiments. Apart from that, it would have been interesting to see experimental results when explanations are plugged in for different stages of the training and using different values of `tau` temperature. Apart from that, including results for different  applications such as NLP will make the case more convincing. \n\n**Minor Weaknesses**\n\n1. Reproducibility of the paper:  I didn’t see any references to the github page or a software implementation. Nonetheless since it is similar to the Khosla et. al. 2020, it shouldn’t be difficult to implement given that Khosla et. al. 2020 released the code on github.\n2. Figure 2, Tables 3, 4, 5, 6  are not referenced anywhere in the script / text.\n3. Figure 4 in the appendix: For the third example we have an incorrect mask however we still use explanation driven masked images. According to the description it should have used random modification of the image.\n4. In the Figures 4 and 5: Are randomly generated examples 1a, 2a and 3a refer to random transformation of the anchor image and 1b, 2b and 3b to the background image ? It is not very clear from the description and it would have been great to describe it.\n5. In Section 3.3 the authors say that they are creating two correlated views of the original data point but it is unclear if one is background(negative sample) and another one anchor (positive) sample or if both are positive. \n6. In ExCon algorithm, when we choose a random transformation in case an explanation mask gives incorrect prediction, do we check if the random transformation is ‘better’ than the datapoint generated by the explanation mask. It can be that the mask generated sample is still better than random transformation in some cases ?\n7. Equation 2 is very similar to Equation 3 and it takes too much space in the paper. I think only keeping equation 3 and referring to Khosla et.al. paper for equation 2 will save some space. \n8. Equation 2 and 3 do not describe the `tau` variable. I had to read Khosla et.al. paper in order to understand that it is the temperature. It would have been great to run some experiments and test a range of values for `tau`.\n9. Section 4.3.1 looks a bit incomplete. The description:\n“Good explanations should lead to a large increase in the softmax activation score or a small decrease.” sounds pretty vague. Why explicitly, large increase or small decrease ? What about a small increase ? 10. Also, it looks like this is meant to be for the second optimization(the classifier) task. It would have been great to describe it further. \n4.3.1 describes `drop and increase`  for the explanation based approaches, however, in the tables 3 and 4 we are also comparing SupConOri and SupCon. This is a bit confusing given that there is no description for drop and increase tables and SupConOri and SupCon aren’t using explanations at all.\n11. In section 4.3.2 when computing infidelity it is unclear how the input was perturbed. According to https://arxiv.org/pdf/1901.09392.pdf ,input image needs to be perturbed for example by adding white noise. It is unclear what amount of noise was added to the input image when computing the explanation infidelity. \n",
            "summary_of_the_review": "The idea of using explanations to improve the quality of supervised contrastive learning is intriguing and novel. The authors show on CIFAR and Resenet datasets how explanation-based data augmentation can help to improve the quality of contrastive supervised models using accuracy scores and T-SNE  plots.\nNonetheless, I think that the amount of the contributions might not be sufficient to be accepted to the conference. I’d recommend the authors to conduct deeper analysis and improve the algorithm description. As suggested in the major weaknesses section, the authors can improve algorithm description and make it more consistent with the described experiments, improve experimental section and describe the experiments more clearly, make loss function more generic and refer to generic contrastive loss function instead of copying Khosla et. al. 2020 and  take  into account other state of the art explanation techniques. \n\nMinor and Major weaknesses sections describe potential action steps that can be taken to refine the paper.\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}