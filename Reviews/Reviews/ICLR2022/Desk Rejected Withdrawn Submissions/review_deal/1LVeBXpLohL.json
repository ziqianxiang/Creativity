{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors propose Confidence based Weight Scaling (CWS) as a recalibration algorithm for machine learning models. CWS interpolates between the adaptively-binned softmax probability vector output by the model and uniform distribution; the optimal interpolation value is found via grid search on the calibration validation set.  The motivation for CWS comes from temperature scaling, which can also have the effect of smoothing the softmax probabilities towards a uniform distribution. The authors include empirical comparisons between various recalibration algorithms on standard vision benchmarks.",
            "main_review": "Strengths:\n-Weight Scaling calibration is easy to understand and implement, and could have significant impact for practitioners as a better alternative for temperature scaling, the current most popular recalibration algorithm. \n\nWeaknesses:\n\n-Table 1 shows that the ECE score after Weight Scaling (WS) calibration was lower than the ECE after Temperature Scaling (TS) in only half of all cases. So, WS calibration doesn't seem to give a major advantage performance wise? The authors claim that the advantage is in WS calibration simplicity, but it's not clear how WS calibration is simpler than TS.\n\n-Tables 1-3 reports ECE (%) numbers. However it's known that using adaECE/equal-mass-binning-ECE gives less biased measurements of true calibration error [1, 2].  Also, there are debiasing techniques that the author could use to even further reduce the bias of the ECE measurement such as the Debiased Estimator [3] or the Monotonic Sweep Estimator [2].\n\n-\"Weight Scaling\" is a confusing name to me. I thought we would be scaling the weights of the model, but in fact the method works by interpolating the softmax probability distribution output from the model with the uniform distribution.\n\n-The authors first bin the confidence scores before finding the optimal scaling parameters for each bin. I'm suspicious that binning is only able to outperform methods that don't bin because our heuristics for estimating true calibration error use binning. \n\n\n[1] Nixon et al. Measuring Calibration in Deep Learning. https://arxiv.org/abs/1904.01685\n[2] Roelofs et al. Mitigating Bias in Calibration Error Estimation. https://arxiv.org/abs/2012.08668\n[3] Kumar et al. Verified Uncertainty Calibration. https://arxiv.org/abs/1909.10155\n",
            "summary_of_the_review": "Overall, the method is simple, but I'm not convinced it gives a significant performance advantage empirically over temperature scaling.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper introduces a new method for post-hoc calibration of neural networks. Similar to temperature scaling, the proposed approach (“weight scaling”) performs a convex combination of the neural network output and an uninformed output. However, while temperature scaling performs this combination on the logits, weight scaling performs this combination directly on the output probabilities. The authors introduce this approach, as well as variants where a different scaling parameter is chosen for different probability bins.",
            "main_review": "The proposed method is simple, with a few slight advantages over temperature scaling - closed form optimal solution, preserves the confidence ordering of the data. The authors also demonstrate that weight scaling (like temperature scaling) satisfies basic desirable properties - it does not affect accuracy, it has no hyperparameters, it maintains the order of probabilities, etc. However, I would argue that this paper does not make an adequate case for why weight scaling should be preferred over temperature scaling (see below). I do believe that it is possible to make a case, but I believe this would require more theoretical and empirical analyses. Beyond that, I believe that there are a few technical issues, and a lack of experiments (see below).\n\n**Does not present an adequate case for why weight scaling is better than temperature scaling:** The authors point out two differences between weight scaling and temperature scaling. First, the authors claim that weight scaling is simpler because the optimal weights can be computed in closed form. While this is true, I would argue that this is not as significant as the authors claim it is. (Finding the optimal temperature takes a few iterations of gradient descent, or a grid search. This is not a significant overhead.) Second, the authors note that weight scaling - unlike temperature scaling - preserves a ranking of the data, in the sense higher confidence pre-scaling corresponds to higher confidence post-scaling. Again, this is a **property** of weight scaling, but I do not think that it can be claimed as an **advantage** unless the authors explicitly show why this is in fact a desirable property. Indeed, for a task like selective prediction, a rank-preserving calibration algorithm is essentially a no-op (defining a cutoff threshold in pre-scaled probabilities would result in the same selected data as a cutoff threshold in post-scaled probabilities).\n\nThe authors need to make the case that these properties are actually beneficial or desirable, at least in some scenarios. I don’t think that there is a big case to be made regarding “simplicity” - a closed form solution is nice, but optimization is arguably not a pain point of temperature scaling. Instead, I believe the paper should demonstrate a case where rank-preserving calibration is actually beneficial to some downstream task.\n\n**Technical Issues**:\n\n- I believe this paper requires more experiments. The authors claim in the abstract to have “extensive experiments” and that their method achieves “state-of-the-art calibration.” The results, for the most part, seem inconclusive as to whether CTS is better than CWS. The experiments should report error bars to determine whether or not the differences between CTS/CWS are significant. It would be interesting to see how CTS/CWS perform on a fine-tuned model (e.g. a model trained on Imagenet, and fine-tuned on Birds or a similar dataset).\n- Section 3: Regarding the iterative procedure that alternates adaECE optimization and recomputing bins - is there any guarantee that this converges? If not, then this should not be a recommended approach.\n- Section 3: “This is due to the fact that… L1 is more robust to this diversity.” What does this mean? I believe there is an opportunity to make a mathematically precise statement here, and terms like “robust” and “diversity” are vague and do not help your argument.\n\nSmall notes:\n\n- Section 1, 2nd paragraph: Kumar et al. 2019 does not perform calibration during training\n- Table 1 caption: “W \\approx 100” - what is W? As far as I can tell it is not defined.\n- Figure 2, bottom row: why is there no orange line for CTS?\n",
            "summary_of_the_review": "I do not believe that the authors have made a sufficient case for weight scaling. While they demonstrate that weight scaling and temperature scaling have some slightly different properties, they do not demonstrate (theoretically or empirically) why these differences are desirable or necessary. (I would suggest having an experiment with a downstream task that demonstrates why e.g. the rank-preserving nature of weight scaling is a useful property). This is especially necessary because the empirical results are not strong enough to demonstrate why weight scaling should be preferred to temperature scaling. Error bars on the experiments are necessary. While I would welcome a revision from the authors, at the moment I believe that such a revision would require enough work to merit submission to a different conference.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new method to rescale the predictions of classification models to improve their uncertainty calibration. Models are said to be well-calibrated if the predicted probability (confidence) matches the actual accuracy of the model. A simple but effective method to improve model calibration is to rescale the model logits (before softmax-normalization) by a scalar factor that is chosen to optimize the model likelihood on a held-out validation set (temperature scaling; TS). \n\nThe paper proposes an alternative method that instead finds a weight alpha that interpolates between the predicted class distribution and the uniform distribution (weight scaling; WS). Like temperature scaling, the proposed weight scaling does not change the accuracy of the model, but can improve its calibration.\n\nThe proposed method is evaluated on a range of convolutional image classification networks and four datasets: Cifar-10, Cifar-100, Tiny-ImageNet, and ImageNet. It is compared to temperature scaling and found to lead to lower expected calibration error in some cases.",
            "main_review": "### Strengths\n1. The exposition of the calibration problem, and the description of the proposed method, are clear.\n2. The description of the “smoothing trajectory” (Figure 1) of TS makes it clear that the transformation applied by temperature scaling (i.e. linear scaling of the logits) is somewhat arbitrary, and illustrates that other scaling approaches may be better, even if they have the same number of parameters as temperature scaling. This provides good motivation to find alternatives to temperature scaling.\n\n### Weaknesses\n1. *Theoretical motivation:* Unfortunately, the smoothing trajectory of WS is similarly arbitrary. The paper provides no theoretical justification why the WS trajectory (straight line in Figure 1) is a more principled choice than the TS trajectory.\n2. *Performance of proposed method:* Tables 1 and 2 show that the proposed method does not consistently outperform other methods. In particular, it does not always outperform temperature scaling, which is an extremely simple and robust method. Outperforming temperature scaling is a necessary requirement to justify switching to a new rescaling method.\n3. *Simplicity of proposed method:* The paper claims that simplicity is an advantage of WS over TS, but WS does not seem to be simpler than TS. For example, the claim that WS does not require tuning of any hyper-parameters is wrong: Since the method uses ECE as an optimization target, it requires the choice of a binning scheme and number of bins to estimate ECE. This is not the case for TS with likelihood optimization. More generally it is well known that ECE is difficult to estimate due to strong and hard-to-characterize estimator bias, and due to the existence of many estimator variants (some of which are mentioned in the paper). Therefore, the reliance on ECE as a target increases the complexity of the method significantly compared to optimizing likelihood, as is usually done in TS.\n4. *Outdated model and dataset testbed:* The paper uses a similar set of models and datasets as [Guo et al.](https://arxiv.org/abs/1706.04599) used in 2017. Since then, new architectures have become the state of the art, e.g. ViT and MLP-Mixer. Many checkpoints are available online and could have been evaluated for this study. Further, more recent models have been shown to be much better calibrated, even before re-scaling (https://arxiv.org/abs/2106.07998), so results based on the set of models used here may not generalize. Finally, the study does not consider out-of-distribution performance, e.g. on ImageNet-C, -R, or -A.\n5. Some of the cases requiring special treatment in WS, e.g. underconfident predictions, are not as rare as claimed in the paper. For example, models trained with label smoothing tend to be underconfident across all confidence values, and even overall-overconfident models can be underconfident for part of the confidence space.\n",
            "summary_of_the_review": "While the exposition and motivation provided in the paper are clear and interesting, the proposed method is neither better nor simpler than the most basic baseline (temperature scaling). Further, the experiments used out-dated models.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper applies post-hoc calibration by finding scales that combine the uncalibrated confidence with uniform distribution via convex combination. For each confidence bin, the scale is found by minimizing the L1 discrepancy of confidence and accuracy in the respective bin. Several experiments using Computer Vision models/datasets have been conducted to show the effectiveness of the method.\n",
            "main_review": "## Strengths:\n\n1. The method is simple, and the paper is easy to follow\n2. The implementation is also simple\n3. The experiments show that the method can be better than traditional calibration methods such as temperature scaling\n\n## Weaknesses:\n1. Method \n\n    1.1. I could not find any motivation/reason in the paper why this method should have a better calibration than other methods.\n\n    1.2. The novelty of the method is somewhat limited. Overall, the method is ad-hoc as it combines existing tricks/ideas without any theoretical grounds. This paper falls into an emerging genre of deep learning papers that treat calibration with an undue level of mysteriousness.\n\n    1.3. The method may not work in rare cases (e.g., when the model is under-confident). Although it is good for a method to have good calibration performance on average, it is also necessary to avoid catastrophic consequences in rare cases for safety-critical applications as it is the purpose of confidence calibration. \n\n2. Writing & Related Works\n\n    2.1. The flow of the paper is boring in some cases as it repeats some statements multiple times (e.g., about accuracy preserving) or includes some unnecessary parts (such as the inclusion of random variable z and coin flipping). I believe the paper can be compressed to a much shorter version.\n\n    2.2. Regarding accuracy preserving, several calibration methods discuss these types of functions [a,b,c]. It would be good if the authors included a discussion and, if possible, comparisons to these methods. In addition, I think the accuracy may not be preserved in this method after applying different scales to different bins. Please correct me if I am wrong.\n\t\n3. Experiments\n\n    3.1. What is the purpose of Table 1? WS is usually better than other methods when the number of classes is less than 100 (except for label smoothing, which is reverse). What is the number of bins in Table 1?\n\t\t\n    3.2. The method's sensitivity to the number of bins and validation set size is not explored. Note that using a small number of bins may impose bias in the results, as mentioned by [d,e]. Some previous works have used M=100 bins to attenuate the bias ([e,f]).\n\t\t\n    3.3. Only the top-class ECE is evaluated in the paper. In many applications, the confidence scores of other classes might also be important. Multiclass calibration errors (like marginal calibration error[d]) are also important for evaluating calibration methods. In addition, evaluating other calibration metrics like Brier, NLL (which are proper scoring rules), and KS-score (Gupta et al.) can improve the paper's experimental evaluations.\n\t\t\n    3.4. The experiments in the paper only consider in-distribution data. However, calibration is crucial under distribution shift and out-of-distribution (OOD) data. I think these experiments are necessary to have a complete assessment of the method discussed in the paper.\n\n----------------------\n### Minor:\n- What does CTS stand for?\n- In Algorithm 1, 3rd line, $\\arg\\max$ should be $\\max$.\n- I do not think if the advantage of WS over TS is its simplicity.\n- page 9. paragraph 3, 5th line!, under-confidence samples should be under confident samples.\n-----------------------\n### References\n[a] Zhang et al., \"Mix-n-match: Ensemble and compositional methods for uncertainty calibration in deep learning,\" ICML 2020.\n\n[b] Rahimi et al., \"Intra order-preserving functions for calibration of multiclass neural networks,\" NeurIPS 2020.\n\n[c] Tomani et al., \"Parameterized temperature scaling for boosting the expressive power in post-hoc uncertainty calibration,\" arXiv preprint arXiv:2102.12182.\n\n[d] Kumar et al., \"Verified uncertainty calibration,\" NeurIPS 2019.\n\n[e] Wenger et al., \"Non-parametric calibration for classification,\" AISTAT 2020.\n\n[f] Patel et al., \"Multiclass uncertainty calibration via mutual Information maximization-based binning,\" ICLR 2021.\n\n",
            "summary_of_the_review": "Due to the limited novelty and scope of experiments and lack of theoretical foundation, I believe the paper in its current format is not ready for publication in a top conference such as ICLR.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}