{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors consider deep, narrow neural networks for approximating nonlinear operators. The main result is to use finite decimal representations of the input and encode them into a single neuron for approximating operators by neural networks of depth 5 (or 6). \nThe universality is proved for some cases without rates of approximation. ",
            "main_review": "The paper consists of three main results. \n\nThe first one extends that of Eldan-Shamir to the operator case. The method is borrowed from the paper of Telgarsky and is well expected.\n\nThe second main result follows the exactly same construction of Chen-Chen for approximating nonlinear operators and derive rates of approximation based on an assumption of approximating a product function. However, the assumption is made for the approximation of the product function by a neural network of depth L to \"up to any uniform error\". This does not make sense, to this reviewer's opinion. Moreover, there is no discussion on how the approximation depends on the activation function and the error accuracy. \n\nThe last main result is given in the forms of Theorems 3-5 and Corollary 6 by using finite decimal representations of the input and encoding them into a single neuron for approximating operators by neural networks of depth 5 (or 6). The construction contains some interesting ideas though the construction leads to large sizes of the constructed neurons. This drawback is reflected in the fact that all the theorems and corollary do not hint any quantitative estimates for the approximation.\n \nA key condition made in Theorems 3-5 and Corollary (not explicitly stated in Theorem 4 and Corollary 6, but one can check from the proof that the condition imposed in Theorem 3 is needed) is that the activation function is \"continuously differentiable at one or more points with nonzero derivative\". This is not only confusing (the condition is stronger for \"more points\", so there is no need to add this term) but also wrong mathematically because this condition is not used explicitly at all in the proofs. The standard condition in the literature is that the derivatives of all orders at a fixed point is nonzero. It seems that the authors do not understand this condition for the classical sigmoid type activation functions. \n",
            "summary_of_the_review": "\n\nThe first result is well expected.\nThe assumption in the second result does not make sense. \nThe last main result does not hint any quantitative estimates for the approximation. \n \nA key condition made in Theorems 3-5 and Corollary is that the activation function is \"continuously differentiable at one or more points with nonzero derivative\". This is not only confusing but also wrong mathematically. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper studies universal approximation theorems (UAT) for operator neural networks, and explores several aspects relating benefits of depth vs width. The important keyword here is \"operator\" NNs which goes beyond simple representation capabilities of NNs wrt to continuous function approximation like the work by Cybenko, Hornik and so on.\n\nPrior works from 1995 onwards have been studying UATs for operator NNs of arbitrary width and bounded depth. This paper kind of reverses the setting by allowing arbitrary depth but bounded width, and show how such NNs can be universal approximators of continuous nonlinear operators.\n\nThe main result is in Section 3, where they prove that an operator NN of arbitrary depth and constant width is a universal approximator of nonlinear continuous operators if the activation functions is continuously differentiable at a point of nonzero derivative. To accomplish this the authors use several tricks for how to encode and decode the input, in order to reduce the width needed to a constant. Roughly, one has to truncate the inputs to some number of digits and then concatenate truncated values into one value. Later layers should decode and extract from this \"compressed\" encoding the truncated input values that will be needed for the operator approximation. The latter stage was shown how to perform in Kidger & Lyons (2019) by the use of arbitrarily deep nets.\n\nOther results included in the paper concern the benefits of depth in neural networks following the similar approach as in Telgarsky (2016). In section 2, the authors show depth-width tradeoffs analogous to the ones of Telgarsky stating that shallow nets may require exponential width in order to perform a task that is easily solved by a deep, yet narrow NN.\n\n\n\n",
            "main_review": "The reviewer thinks studying operator NNs is a very exciting direction that could be useful in practical applications. The reviewer thinks that the paper contains some nice tricks that are mainly related to the encoding/decoding part of their construction and perhaps this is the most interesting part in the paper. However the reviewer finds the results presented in the paper quite limited in their scope, and in the novelty of their techniques.\n\nMore specifically, Section 2 that contains the operator NNs depth-width tradeoffs ala Telgarsky, is just a restatement of Telgarsky's results but in the language of operators. The reviewer thinks the novelty of this part of the submission is quite limited. The proof is also quite straightforward.\n\nFor Section 3, the reviewer finds interesting the way that the authors essentially perform \"memorization\" of inputs so that they can decrease the width substantially. The main conceptual idea of reconnecting the netowk from the 1995 paper Chen & Chen is interesting in order to propagate inputs and reduce width. However, given the work by Kidger & Lyons (2019), the reviewer thinks this is a relatively incremental contribution. \n\nThe reviewer appreciates the author's clear comparison with the previous works, and the fact that they stated the 2 important shortcomings following the proof of Theorem 2 on page 5.\n\nQ: Based on works by Hanin and Sellke (Approximating continuous functions by ReLU nets of minimal width) and by Park et al. (Minimum Width for Universal Approximation), if we study function approximation properties of NNs where their width is restricted to be less than the input dimension, then we cannot expect to get universal approximation theorems for NNs. Can the authors clarify for what types of activations/NN architectures this work bypasses those earlier results? Even though this  is not contradicting the results in this paper since you modify the networks, can the authors be more explicit for where the difference lies exactly?\n\nQ: In the proof of Sec. 2, why is it that G_k defined to be a constant operator, yet the theorem statement has G_k be a nonlinear continuous operator? Also the last phrase after Theorem 1, \"This suggests that UATs for deep operator NNs comprise an important contribution to\nour understanding of the limitations of deep learning and expressibility of nonlinear operators.\" should be a bit lessened given that Telgarsky basically had the same construction/tradeoffs.",
            "summary_of_the_review": "Interesting direction of work, however novelty of main results is quite limited given the prior works. First part of the paper that talks about tradeoffs is an appropriate restatement of Telgarsky's result (2016), while the results in Section 3 heavily rely on existing machinery with a few new and nice tricks. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "-",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper gives a new universal approximation theorem (UAT) for the class of operator neural networks (each layer is an operator rather than a finite weighted sum). Previous work on UATs for operator neural networks established their universality but for bounded depth networks using large width. This paper shows the benefits of depth and how small width, sufficiently deep networks can approximate arbitrary continuous functions.",
            "main_review": "+The paper addresses a concrete problem\n+ the solution is fairly complete\n+ the construction and proof and proof mathematically nice\n\n- there is no surprise in this paper\n- UATs have been established many times over in the past few years. Does this one provide an additional insight?\n- UATs are \"good to know\" but tell us little about why NNs are so effective in practice.",
            "summary_of_the_review": "I believe the result here is worthy of a short journal publication, but perhaps not of a competitive conference publication.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The main contributions of this paper are the following: \n\n1. The authors demonstrate a universal approximation theorem for narrow (width $\\Theta(1)$) and deep neural networks. \n2. The authors demonstrate a separation between depth $2k^3 + 8$, constant width neural networks and depth $k$, and subexponential width ReLU neural networks. \n3. The authors demonstrate that after truncating inputs, deep neural networks of width 5 can be used to uniformly approximate continuous real-valued functions on a compact set regardless of the dimension. \n\nPrior work has either focused on universal approximation theorems (UATs) in the context of wide and shallow neural networks. One closely related paper [1] proves a UAT for width n+m+2 neural networks with n inputs, m outputs. This paper reduces the width to being a constant. Their construction relies on being able to truncate and append numbers, and later decode the final result. \n\n\n\n[1] Kidger & Lyons (2019) ",
            "main_review": "Strength: \n\n1. This work improves on the previous state of the art in terms of the width of the neural network for getting a universal approximation theorem. \n\n",
            "summary_of_the_review": "I think this work makes an improvement on the previous state of the art for UATs and vote to accept it. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper considers the approximation ability of operator neural networks. They construct an operator NN with width 5 that can approximate any nonlinear continuous operator with width d+5 where d is the input dimension. They then prove NNs with truncated inputs only need width 5 to have UAT. They also construct an operator NN with polynomial depth which can't be approximated by any shallow NN unless it has exponentially more neurons.",
            "main_review": "Significance: what's the importance of operator NN and its essential differences between classical NNs? They authors even failed to provide a concrete definition of operator NN. It seems to me that results in the operator setting can be easily derived from parallel results in the classical setting. The results are only incremental as well.\n\nNovelty: this paper is basicly rephrasing existing results in the operator setting, especially Theorem 1. The construction of Theorem 2 is very similar to that of Lu et al. 17, which isn't mentioned in this paper. Since Lu et al. 17 is the first paper considering UAT of width bounded NNs, this paper definitely lacks comparison with existing results. Theorem 3 uses a truncation of inputs which not only requires width no less than input dimension, but also uses a very special activation. This 'illegal' truncation significantly dispels the difficulty of UAT with bounded width, therefore the width-5 UA isn't of much importance (and the encoding&decoding trick is well-known in the NN approximation literature). Theorem 4 didn't even make assumptions on \\sigma. \n\nWriting: many important notions aren't defined rigorously, including the central object 'operator NN' discussed in thia paper. The statements of theorems lack precision: in Theorem 2 G(u) isn't defined, in Theorem 4 \\sigma isn't defined. The proofs aren't written in mathematical linguistics. The overall organization is weird, too. For example, important parts like 'settings', 'related works' are missing, and Section 2 is abrupt.",
            "summary_of_the_review": "The novelty and writing of this paper are far below the acceptance threshold, thus I recommend reject.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}