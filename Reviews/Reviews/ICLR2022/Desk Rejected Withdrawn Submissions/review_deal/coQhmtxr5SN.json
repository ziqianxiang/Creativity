{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper generalizes standard Bayesian optimization techniques for optimizing an unknown target function to a greedy strategy called H-entropy search for minimizing a decision-loss associated with the unknown target function.\n",
            "main_review": "This paper considers optimizing a decision loss for an unkown target function by querying values of the function in a Bayesian framework. A greedy strategy (called H-entropy search) is proposed - it queries the example leading to maximum reduction in the posterior expected Bayes risk. It is shown to be a generalization of the standard Bayesian techniques for optimizing an unknown target function. It is shown that when the loss function associated with the posterior Bayes risk satisfies certain mild conditions, the greedy strategy can be conveniently implemented via gradient-based optimization. The greedy strategy is applied to several optimization problems by designing problem-specific losses: top-k estimation with diversity, generalized level set estimation and multi-value sequence search. For these problems, the greedy strategy outperforms several baseline approach, which is unsurprising as those baseline approaches are not designed to minimize the problem-specific loss functions. \n\nThe paper is well-written and easy to read. The experiments demonstrates that the greedy approach outperforms several baselines which do not take the decision loss into account during information gathering. The idea is natural and simple, but it is interesting to see that several standard Bayesian optimization techniques can be seen as special cases of the greedy approach (though proofs are straigtforward), and this general framework may find broad applications.\n\nTo strenghthen the paper, it would be helpful to address the following questions for the experiments so as to provide a more complete picture of the performance of the proposed algortihm:\n- During test time, is the predicted action the posterior Bayes action? In particular, how is a point classified for level set estimation?\n- Are GP priors used in all the experiments? If yes, how are the kernels and hyperparameters chosen?\n- For tok-k estimation, how do the plots of the sum of the selected inputs against iteration look like?\n- Is vanilla stochastic gradient descent used to for computing the acquisition function in the greedy algorithm? If yes, how is the learning rate chosen? In addition, what is the sample size used in computing gradient estimates?\n- How much time does each iteration take for each method?\n\nMinor comments\n- Proposition 2: $P(\\Theta)$ need to be replaced by all possible distributions on ${\\cal X}$.\n- What do PES and MES stand for? Are they just alternative names for ES (entropy search)?\n- pp. 5: optimiztion -> optimization\n- pp. 7: \"for this task the Vaccination function\" -> for the Vaccination function\n",
            "summary_of_the_review": "A simple and natural idea that is clearly explained and supported by experiments. The idea potentially has broad applications for minimizing a decision-loss associated with an unknown target function.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces H-entropy search, a family of acquisition functions (AFs) for Bayesian optimization (BO) built on the concept of H-entropy, a generalization of the classical Shannon entropy. An H-entropy is determined by a loss function, $\\ell$, and a set of actions, $\\mathcal{A}$. The choice of  $\\ell$ and $\\mathcal{A}$ can be tailored to an arbitrary quantity of interest to minimize. Thus, this approach does not only provide a flexible family of AFs for standard BO but also for other related problems such as level set estimation. Moreover, it is shown that many widely used AFs in standard BO can be derived as special cases of this family of acquisition functions under suitable choices of $\\ell$ and $\\mathcal{A}$. It is also shown that, under suitable conditions, this type of AF is differentiable and its gradients can be computed using an infinitesimal perturbation analysis, thus allowing the use of gradient-based methods for a more efficient AF optimization. Finally, several numerical experiments under various choices of $\\ell$ and $\\mathcal{A}$ are conducted, showing favorable results compared to other existing methods.",
            "main_review": "Strengths\n\n1. This work introduces an interesting unified view of various BO AFs through the lenses of H-entropy measures.\n2. The realistic test problems considered in this work are interesting and, to my knowledge, novel within the BO literature.\n\nWeaknesses\n\nThis paper has multiple weaknesses that I describe in detail below.\n\n1. While the H-entropy search family of AFs is interesting in that it encompasses many of the existing acquisition functions for BO, its usefulness is not clearly demonstrated (and is not evident to me). This family of AFs is so broad that deriving general theoretical results or efficient computational methods for it seems very challenging. As an example of this, the conditions under which the differentiability of these AFs is proved are not satisfied by the knowledge gradient or entropy search AFs.\n\n2. The formulations provided for the top-k estimation and multi-value sequence search problems seem rather arbitrary; i.e., the loss functions used in each of these problem setups seem arbitrary. Thus, while these are valid examples of how the framework could be used to derive AFs for non-standard choices of $\\ell$ and $\\mathcal{A}$, their practical relevance is not evident. The authors should clarify whether these problem setups (and their corresponding choices of $\\ell$) have been considered in the literature before.\n\n3. The loss function used for the level set estimation problem is odd. A more appropriate measure of performance for this problem setup is the F1-score (see, e.g., Gotovos et al (2013). Active learning for level set estimation).\n\n4. The numerical experiments that consider level set estimation problems should include AFs tailored for that particular task. One example is the LSE acquisition function from Gotovos et al (2013).\n\n5. It is a bit disappointing that a novel H-entropy search AF for the standard BO setting is not proposed. It would be interesting to explore other choices of $\\ell$ and $\\mathcal{A}$ in this setting.",
            "summary_of_the_review": "This work introduces an interesting class of AFs which is shown to contain many of the existing AFs in standard BO as special cases. While this unified view is interesting, its usefulness is not clearly demonstrated. In particular, the practical relevance of the special cases of $\\ell$ and $\\mathcal{A}$ considered in this paper is not evident. Moreover, the empirical evaluation does not consider existing acquisition functions tailored for the level set estimation problem, and thus does not demonstrate competitiveness with respect to the current state-of-the-art. In general, there is a substantial amount of work to do to prove that such a unified view could be useful.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors showed that existing BO methods are special cases of different loss functions in H-entropy / information gain. Using H-entropy/information gain, the authors were able to derive a series of new acquisition functions and their gradient based optimization methods for problems including subset selection and level set estimation. ",
            "main_review": "I really liked this paper when I read the abstract, and I think it indeed proposed interesting ideas on H-entropy to connect different concepts related to Bayesian optimization. However, I found it a bit disappointing that the writing is not clear and some basic definitions are somewhat confusing.\n\n1. There are a couple of places where writing can be much improved in the introduction.\n1.1 It was not clear to me whether the focus is on introducing H entropy to address more tasks (including level set estimation etc) or to make connections among different acquisition functions. It was not until the end of the intro where I start to realize that both are proposed. It just doesn't feel crystal clear what exactly the authors are trying to propose. For example, 2nd paragraph is all talking about different tasks other than BO, but the title is about BO. And then 4th paragraph started talking about BO again but just 1 sentence, then it's back to other tasks. Reading the intro makes me feel like the author who wrote the intro did not have a clear mind.\n1.2 The choices of words make it very confusing. I would not call level set estimation an optimization task. As far as I know, BO and Bayesian level set estimation are usually considered \"siblings\" rather than \"parent-children\". The same for top k with diversity: that is subset selection.\n1.3 Contributions can be made clearer: the last two points seem to be one. Also what is a strong empirical performance? Can you be more concrete?\n\n2. Related to Setup and Experiments: though I understand that the loss term is defined to facilitate the definitions of H entropy and information gain, I don't think it's necessary to introduce loss as l=-f(x*) and then in experiments evaluate -l as the metric, which is basically f(x*). To me, it is very confusing. Can you define H entropy instead for a maximizing objective such as f(x*)?\n\n3. Under the definition of H entropy, can you include some discussions on how it is related to regular definitions of entropy, e.g. Shannon, Rényi etc? Proposition 2 has the related proof but it happens much later in the paper. I think it's also good to explain in detail which definition/theorem in which paper introduced this Definition 1 and add its citation properly at Definition 1. I briefly looked through the cited papers but didn't find anything similar to what the authors defined. I do wonder whether Definition 1 should be called entropy, which evaluates how \"chaotic\" a distribution is. If Definition 1 is novel, I think the definition of a newly proposed entropy is beyond the expertise of this conference and should be reviewed by the mathematics and probability community.\n\n4. I would suggest having more intuitions on what H entropy/information gain implies and how choosing a loss function could impact H entropy. In particular, for entropy search, what exactly does distribution q mean?\n\n5. While these acquisition functions can be interpreted as H information gain with different losses, how does this understanding help us choosing a better loss function?\n\n6. Is multi-value sequence search a new problem? If not, please add citations for it.\n\n7. What is the runtime or complexity of the proposed method? It seems to me that the gradient can be expensive to compute given the optimization steps for each gradient.\n\n8. There are a lot of tasks discussed in this work but there is not enough space for experiments that demonstrate and analyze the performance of methods for different benchmarks in each task. The baseline methods are also very limited. For level set estimation, there are existing simple methods that work well https://proceedings.neurips.cc/paper/2005/hash/8e930496927757aac0dbd2438cb3f4f6-Abstract.html. For subset selection (or what the authors call top k estimation with diversity), at the very least one can compare to classic work such as https://jmlr.org/papers/volume9/krause08a/krause08a.pdf. I'm also confused how KG, as a BO method, is selected to solve LSE and subset selection problems.\n\n9. Is H-entropy/information gain submodular?\n\n\n\n",
            "summary_of_the_review": "The overall ideas on H-entropy and its interpretations for BO methods are very interesting. However the writing is not very clear and experiments are lacking. I hope the authors can address the issues to make the paper more readable and convincing.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a generalized acquisition function for Bayesian optimization (BO) called h-entropy search. The acquisition function is defined as expected reduction of the loss function for which connection with the entropy is shown by existing studies. The authors discuss that h-entropy search includes several well-known acquisition functions as a special case. For three application scenarios, examples of the loss function definitions are shown, for which empirical evaluations are also provided.  ",
            "main_review": "The paper is easy to follow and well-organized. The application examples would be useful. However, technical novelty of the paper is limited. Although the authors emphasize generality of the proposed framework, I do not see significance for the generality because it does not provide novel acquisition function for the usual BO and no properties that hold commonly for each one of special cases are revealed. It is seemingly no more that that equations can be written by a common representation. The three application examples are practically interesting, the proposed approach is a simple expected risk reduction, by which novelty for the design of acquisition function is somewhat marginal. Detailed comments are as follows.\n\nAs the authors mentioned, h-entropy can be seen as a generalized formula of a various criteria such as EI, ES, and KG. However, I do not think that just deriving a generalized formulation is significant. If there exist some 'shared' statistical property in the context of BO such as convergence rate, it is worth generalizing different criteria. The paper does not reveal any shared property of each one of special cases. In this sense, I'd have to say that the connection is superficial. Although there exist several existing studies for connection between h-entropy and information, their context is quite general and implications for BO and GP are not clarified. \n\nWhy can (2) be 'entropy' (i.e., amount of information)? This is essential, but nothing is discussed. The authors just refer the existing studies, and do not justify the interpretation that it is still an effective way to measure the information gain in the context of BO.\n\nRelated to that, although the h-entropy is proposed as an uncertainty measure, can it always be seen as an uncertainty measure regardless of the definition of the loss \\ell? For example, KG only evaluates the change of the posterior mean. This indicate that even when there still exists a large amount of uncertainty, KG can have better value if the posterior mean is improved, compared with other options that reduce uncertainty a lot though the posterior mean is not improved. This is another reason that I thought the connection is only superficial.\n\nParticularly in three application examples, the proposed framework simply evaluate a difference of the expected performance between the current and the one step ahead models. In my understanding, it is a kind of the classical expected error reduction (EER) in active learning. The EER is a reasonable strategy, but I do not think that designing EER for specific tasks has particular technical significance.",
            "summary_of_the_review": "Overall, the paper is well-organized with good application scenarios, technical novelty of the proposed framework is not satisfactory.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}