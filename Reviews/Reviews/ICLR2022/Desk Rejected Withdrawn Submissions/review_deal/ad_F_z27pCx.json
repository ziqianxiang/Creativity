{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper claim to identify a fundamental problem of dimension reduction & manifold learning algorithms, as the mapping functions induced by these methods violate the basic settings of manifolds, and hence they are not learning manifold in the mathematical sense. To overcome this issue, this work provide an algorithm called fixed points Laplacian mapping (FPLM), that has the geometric guarantee to find a valid manifold representation (up to a homeomorphism). Comparison between FPLM and some classical manifold learning algorithms are included, and it seems FPLM can achieve good empirical results on these relative simple synthetic & toy data. ",
            "main_review": "### Strengths & Originality: \n\nThe idea of \"simplex preserving \" (integrity, connectivity and neighboring relations) under the concept of manifold learning (nonlinear dimension reduction) is interesting and seems reasonable, or at least is one kind of alternative objective function we can considered. From here, the two-round optimization based FPLM is proposed with a reasonable clean computational framework, \n\nExperimental results (synthetic data) are clear examples to support the effectiveness of proposed FPLM compared with classical manifold learning algorithms, e.g., swiss roll as the manifold with boundary, or 2-sphere as the case without boundary, and FPLM get good results for both cases. \n\nBesides empirical results, analysis of FPLM and some geometric guarantees are provided in section 4 and Appendix for 2-manifold and 3-manifold under certain conditions. \n\n### Weakness: \n\nOverall it seems FPLM is relative clean for implementation but seems lack of complexity analysis in the paper. This is probably clear but still quite helpful to be stated clearly, for 2-manifold, 3-maniold and higher (intrinsic) dimensionality cases. \n\nFrom mathematical point of view, this idea of \"simplex preserving\" seems interesting but there is no discussion from practical point such as why this is necessary, or what kind of advantages by doing so for downstream applications. As often, dimension reduction & manifold learning is used one intermediate step, e.g., used as a pre-process step before classification, and it will be greatly helpful to see such discussion for FPLM. As for now, this work seems only focus on how to do it but not why. Perhaps, in the domain of computational graphics, there are motivations for \"simplex preserving\"? \n\n2-manifold in section 5, different steps of FPLM are illustrated for this famous swiss roll data, this is good but seems the same kind of illustration on a more clear running example will be helpful to show the insights of this proposed FPLM, e.g., 2-sphere is a clean example *(compare to swiss roll) but given no-boundary then only 1 round of FPLM is applied, not 2 steps. \n\n3-manifold is included in section 5.1, given this is possibly related to works in the domain of computational graphics, it seems better to have more results and discussion for this part & SHARK tetrahedral is briefly mentioned in Appendix B.2, but more is good to see. ",
            "summary_of_the_review": "This is an interesting paper and given manifold learning is a field has 20 years plus history, this work still show an relative novel way to think this problem and provided some reasonable results by FPLM on toy data, and hence I recommend for accept but given the weakness my current score is \"marginally above the acceptance threshold\", and happy to change it later with discussion from other reviews & rebuttal. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper claims that most manifold learning algorithms do not generate valid results, because manifold assumptions in the strict mathematical sense are violated. They provide an algorithm that does not violate the mathematical manifold assumptions. They also provide sufficient conditions that should be satisfied so that the result of the algorithm is a bijection to a manifold. The main contribution of the paper is the use of a simplex decomposition (I would have preferred interpolation), that constructs a manifold given a set of discrete points. This step replaces the traditional k-nearest neighbor step that most manifold learning algorithms create. The paper presents results on small size 2 and 3-dimensional datasets. ",
            "main_review": "The paper addresses a well-known general problem of how to apply continuous mathematical concepts on discrete high dimensional datasets. A discrete set of points is always zero-dimensional in the strict mathematical definition. We always make the assumption that it is a sample of a continuous manifold, but most of the algorithms rarely come with guarantees of whether the sample is representative of the continuous manifold. This paper takes a step forward and uses simplex decomposition in order to represent the manifold. I would have preferred the term interpolation. The basic problem that is not emphasized enough on the paper is that the simplex decomposition is very expensive and it is hard to compute beyond3 or 4 dimensions. \n\nOn page 2 \"Observations and Motivation\" the authors are trying to contrast their technique against knn, but the plots are rather confusing. I think according to their thesis if we want the dimensionality reduction results to be a bijection there should be no crossings between the edges on the lower-dimensional space. They are trying to show that their graph formation in the 3-d leads to a 2-dimensional representation where there are no crossings. This is not visible in figure 1.  I think there is also something else wrong with the motivation section. The majority of manifold learning methods mentioned in this paper have the flat manifold assumption. The examples given here are not flat manifolds, they have non-zero curvature. \n\nIn page 3 there is the second contribution is controversial in terms of its validity. \n\n** \"By using this method, we identify a fundamental problem in some prominent methods: the\nmapping function induced by these methods is not bijective, and in turn, violates the basic\nsettings of manifolds.\" **\n\nWhy is the bijectivity so important? It might violate the manifold assumption of bijectivity. Meanwhile the proposed transformation maintains the manifold up to a homeomorphism (The definition of a homomorphism should be defined and examples should be given. This is an ML venue and no background in the area should be assumed). What we need to motivate is why ensuring a bijection is more important than a homeomorphism.  In many ML problems a homeomorphic transformation might be catastrophic.\n\nThe experimental results are preliminary and in low dimension, but they validate the theory. Plots 3  and 4 look wrong though. I have extensive experience implementing and using the Manifold learning methods and for the swiss roll, you can actually unfold it in two dimensions perfectly.  Same thing holds for 5 and 6 for the sphere. By definition sphere is not a locally flat manifold so there is no way they can be embedded in 2 dimensions.",
            "summary_of_the_review": "The paper introduces an interesting approach to manifold learning. Instead of using a knn graph, they propose a simplex decomposition. That guarantees a bijection of the latent space to the original data but it introduces homeomorphic distortion. The paper focuses a lot on the theory and it is rigorous. There are a few fundamental things missing for this idea to be useful in the area of Machine Learning which is the central focus of ICLR:\n\n* Experiments on real data\n* Real-world applications of the proposed transform. Clearly, it is not suitable for visualization. Other tasks like classification should be considered.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper identifies a fundamental problem in existing manifold learning/dimension reduction methods - that is the learned map from the original high-dimensional space to the low-dimensional space is not bijective, and therefore, the map does not adhere to one of the main properties of manifold chart maps. As a remedy, a new manifold learning algorithm, termed fixed points Laplacian mapping (FPLM), is proposed. Some theoretical results are presented. The new algorithm is demonstrated on a few 2 and 3-dimensional shapes, showing that, unlike other manifold learning methods, FPLM gives rise to a representation without edge crosses.",
            "main_review": "The presented algorithm (FPLM) is new and could be a solid contribution to the field of manifold learning. However, in its current form, I don't believe the paper is ready for publication due to the following reasons.\n- The work is not well motivated. The authors repeatedly mention that current manifold learning methods do not generate a bijective map. \n(i) it should be clear exactly which manifold learning methods are referenced. (ii) it seems that this statement only relies on empirical observations. (iii) it is not clear whether this problem is restricted to 2- or 3-dimensional shapes and representations. (iv) perhaps the most important issue, it is claimed that bijectivity is important for subsequent learning, but it is not explained why. Currently, the exposition focuses on the mathematical aspect, but besides being mathematically incomplete, it is not clear why current methods pose problems for subsequent learning.\n- The experimental part is weak and not convincing. It includes only a couple of low dimensional shapes, and it is unclear why the results of FPLM are better than the results of the other methods.\n- The writing and English should be improved throughout.\n\nDetailed comments:\n- The refs. for PCA and Isomap are incorrect.\n- \"Although ManL and DR methods are important pre-processing in machine learning and widely used, unfortunately, the\nunderstanding of what results they produce is largely missing.\" I believe this statement ignores many existing theoretical results and needs to be more specific.\n- \"For example, the results of these methods on 2-manifolds usually contain a large number of edge crosses, indicating that the map is not one-to-one.\" Too general. It would be better to focus only on the methods that were actually tested.\n- \"it is important to have a geometrically correct algorithm which guarantees bijectivity\". It should be explained why it is important for subsequent learning/analysis.\n- \"By generalizing the previous embedding theorem\" which theorem?\n- \"By using the validity checking method mentioned above, e.g. the example in Section 1 and many\nmore in experiment section, we realised that those mostly used DR/ManL methods we tested are not\nbijective, and hence do not really learn a manifold. The question is then, is it possible to design such\nan algorithm which has bijectivity guarantee, at least for some manifolds? The answer is positive.\" (i) English. (ii) The statements need to be more specific. I am not sure that testing 3 or 4 methods could be regarded as \"mostly used DR/ManL methods\". Also, the presented empirical study consists of only one 2-dimensional shape and one 3-dimensional shape, so I am not sure that such a general statement is appropriate here.\n- In eq (1) - in comparison to Laplacian Eigenmaps - the orthogonality constraint is missing. Why? Isn't orthogonality important?\n- \"If $\\mathcal{G}_\\mathcal{S}$ is strongly connected, the fixed points in the first round, collected in $C_1$, are the images of the vertices from a randomly selected d-simplex after reducing its dimensionality.\" - the initial choice of fixed points is unclear.\n- Figs 3 and 4: It is unclear why the lack of crosses is important for learning. For me, the result of Isomap could be considered the best as it unfolded the swiss roll quite accurately.\n  \nTypos:\n- \"higher enough\"\n- \"the main contributions of this paper is listed\"",
            "summary_of_the_review": "New manifold learning algorithm, but not well-motivated, weak experimental study, and poor presentation and writing",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}