{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a new network, called PaTNet, to generate adversarial examples in the real world. The network takes the original image and an adversarial patch as input and applies several transformations to the adversarial patch before inserting it into the original image. The transformations involved include geometric transformation, printer color transformation, and illumination adaptation to ensure the adversarial patch is robust to physical transformations. The paper also proposed a two-stage training scheme for PaTNet and shared a large-scale adversarial T-shirt dataset for evaluation purposes. ",
            "main_review": "Pros:\n1. The paper is well organized and easy to follow.\n2. The paper tackles an important and challenging task, which is how to generate adversarial examples that are robust to physical transformations.\n3. The paper’s proposed PaTNet is novel and shows convincing results.\n4. The adversarial T-shirt dataset is useful.\n\nCons:\n1. It is not clear to me why adopting the three transformations within PaTNet, namely geometric transformation, illumination adaptation, and printer color transformation. The physical-world conditions are complex and the rationale of picking the three transformations, and structuring them in such order is not fully described.\n2. Although PaTNet is novel itself, it is not entirely clear the difference and contribution over its predecessor work, which also aims to learn physical transformations automatically from data (e.g. Jan et al. 2019). Compared with GAN-based methods, is injecting a strong prior of using three pre-defined transformation networks strictly advantageous and if so, why?",
            "summary_of_the_review": "Although the paper proposed a new network to generate physical adversarial examples by simulating real-world transformations, the network's structure is quite empirical and the motivation of learning three specific transformations in the network is not clear. The advantage of learning a fixed set of transformations to simulate physical transformations is also not discussed in depth. The novelty and contribution of the overall method are therefore limited.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposed a pipeline to craft physical adversarial examples that fool deep learning-based object detectors. Existing methods such as Expectation over Transformations (EoT) optimize adversarial perturbations that still work after applying random transformations. However, the resulting adversarial examples often look unrealistic because the space of all transformations is too large. In contrast, this work learns a prior over which transformations to apply with training data and thus generate more realistic adversarial examples.\n",
            "main_review": "**Originality**: I think the paper lacks the novelty of the main conference paper. Results-wise, it’s not the first time an adversarial T-shirt has been made. Method-wise, it’s less surprising that learning an application-specific space of transformations to run EoT may improve the results.\n\n**Quality**: Overall, I think the arguments are supported by the experiments. However, one concern I had is in section 4 “Comparison with Other Approaches” where authors mentioned:\n\n> we obtained the adversarial patches designed by these approaches from their arxived papers and printed the patterns out on T-shirts.\n\nDid authors make sure other approaches’ patches are not subject to image compression when the figures are made? \n\n**Clarity**: \n\nIn section 3 Proposed Approach: \n\n- I think the connection between the proposed approach and image inpainting is pretty confusing. Did authors mean they are both image-to-image translation tasks?\n\n- The notation is confusing. In equation 1, $x_i$ stands for a normal image, whike $x’_i$ stands for the resulting adversarial example. However, in equation 4, $x_i$ is a ground-truth adversarial patch from a T-shirt while $x’_i$ is the final output of PatNet.\n\n- Additionally, in equation 4, where does $x_i$ (a ground-truth adversarial patch from a T-shirt) come from? If authors can already generate it, why do we need PatNet?\n\n**Significance**: Although I appreciate authors’ efforts to collect diverse training data and actually craft physical adversarial T-shirts, I think this paper may be more significant to the security community instead of the machine learning community.\n",
            "summary_of_the_review": "Based on the Originality, Quality, Clarity, and Significance I evaluated above, I recommend rejecting the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "- Draft proposes a multi-stage framework for crafting physical world adversaries. The framework consists of applying transformations such as geometric transformation, printer color transformation, and illumination adaptation learned in two-stage training process. Experimental results show that the generated adversarial patches result in better ASR than other approaches. Ablations to investigate the environmental conditions and robustness against distance are performed.\n\n- Also, authors composed a dataset known as the 'adversarial T-shirts dataset' which is made available to public.",
            "main_review": "- Draft discusses an important aspect of the adversarial vulnerability of the DNNs in the physical world.\n\n- Are the detectors under investigation (Yolo-v2, etc.) adversarially learned? Some of the recent detectors (e.g. Yolo-v5) are adversarially trained. It would be interesting to know the ASR of the proposed framework in case of adversarially trained detectors or in the presence of other defense strategies.\n\n-  Given that components of the proposed framework are significantly inspired from multiple existing works (e.g. STN, CCC for IA), it scores relatively less on the conceptual contributions, and comprehensive experimental validation would make the draft stronger.\n\n- Experimental evaluation of the proposed framework could be improved by including recent and robust detectors such as Yolo-v5.\n\n- Transferability of the proposed physical attack across different detectors should be discussed thoroughly.\n\n- Effectiveness of the framework across different adversaries (integrating with different fooling objectives, i.e. different attacks) needs to be explored.\n\n- Given that the framework looks complex, some comparisons to understand the complexity relative to the existing approaches can help.",
            "summary_of_the_review": "- Since the contribution of the draft is majorly bringing together existing modules/concepts towards crafting real-world adversaries, reader expects thorough validation to understand the merit. However, draft falls slightly short on that front.\n\n- Some more experimentation (as mentioned in the Main Review) and analysis is needed to validate the effectiveness of the proposed framework.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a new approach called \"Patch Transformer Network\" (PaTNet) for generating adversarial patches on T-shirts worn by moving people that are able to fool person detectors. The main contribution of PaTNet is that the mapping from digital patch to its appearance in a video frame (including printing, deformable transformation by non-rigid shirt, and acquisition by a camera) is represented by a system consisting of three components that is learned from data. Experimental results indicate that the proposed approach is effective in generating adversarial t-shirts. The annotated dataset from which the transformation is learned will be released.",
            "main_review": "## Novelty ##\nPaTNet is presented as an alternative to Expectation over Transformation (EoT). However, PaTNet seems to be rather a specific instantiation of EoT where the transformation is deterministic (no expectation) and learned from data. Moreover, EoT (and prior work using EoT) does not assume that transformations are manually designed - specifically, color mapping by printers was already learned form data in prior work. As such, PaTNet does not qualify as an alternative to EoT. The novelty of PaTNet is thus more the design of transformation, consisting of three components: geometric transformation, printer color transformation, and illumination adaptation. While this is a novel design, it is also heavily based on prior work such as Spatial Transformer Networks (Jaderberg et al., 2015) , FC4  (Hu et al., 2017), and the approach from (Xu et al.,2020). Overall, novelty is relatively small for an ICLR paper.\n\n## Quality ## \n\nThe authors provide ablation studies in the results that show the impact of the different components of PatNet. They carefully disentangle effects of digital vs. physical world attacks, targeted detector, environment conditions and patch distance. They also compare to relevant baseline physical t-shirt attacks.\n\nOverall, the empirical evaluation is good for a paper dealing with physical world attacks.\n\nThe paper could be further strengthened by showing the effectiveness of PaTNet on other threat models than the adversarial t-shirt threat.\n\n## Clarity ##\n\nOverall, the paper clearly motivates and summarizes the design of the transformation and of the two-stage training of PaTNet. Some design decisions are not clearly motivated or detailed such as\n * why is an $\\ell_1$ distance used in Equation 4?\n * why is STN not fine-tuned in the second stage of training?\n * what is the value of $\\nu$ in Equation 5?\n * In Equation 2, the order of the transformations is not clearly motivated. In principal, a patch would first be printed on the t-shirt, than transformed, and lastly illuminated. Why is the order of transformations different here?\n\nMoreover, I find it misleading to state that training of PaTNet is \"mostly done in an unsupervised way and the only information required\nis the patch location.\" In which sense is this unsupervised? The patch location is a fully supervised annotation.\n\n## Significance ##\n\nThe proposed procedure provides good results on a challenging real-world adversarial attack (adversarial t-shirt). It outperforms related work, albeit a direct comparison is a bit tricky since the threat models are not identical as the authors also note. \nThe release of the dataset recorded by the authors is an independent contribution on its own, albeit with some ethical concerns (see below).\n\n## Ethical concerns ##\nThe main contributions of the paper consist of a method and a dataset for making physical-world adversarial attacks with patches easier and more effective. This is a double-edged sword since there are clearly harmful applications that would benefit from this contribution, e.g. misleading perception components in driver assistance systems via patch attacks on T-shirts. Unfortunately, these ethical dimensions are not discussed in the paper.",
            "summary_of_the_review": "The paper's main strength are an effective combination of existing methods and providing and using a novel dataset tailored for supporting adversarial patch attacks. The resulting patches show strong performance, indicating that the contributions actually result in qualitatively stronger attacks.\n\nNovelty of methods is relatively small for an ICLR paper. More severely, ethical dimensions of this work are not discussed even though there are some potential harmful applications of this work in terms of reducing safety. \n\nOverall, I see the paper in its current form marginally below the acceptance threshold. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety",
                "Yes, Potentially harmful insights, methodologies and applications"
            ],
            "details_of_ethics_concerns": "The main contributions of the paper consist of a method and a dataset for making physical-world adversarial attacks with patches easier and more effective. This is a double-edged sword since there are clearly harmful applications that would benefit from this contribution, e.g. misleading perception components in driver assistance systems via patch attacks on T-shirts. Unfortunately, these ethical dimensions are not discussed in the paper.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}