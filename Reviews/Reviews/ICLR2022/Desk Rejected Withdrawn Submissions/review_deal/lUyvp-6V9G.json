{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This manuscript proposes to use multiple vectors to represent a node in a graph to prevent underfitting issues. The proposed method called HIME learns $k$ different vectors for each node based on the root vector.",
            "main_review": "Even though this manuscript tries to solve an interesting problem, I reject this manuscript due to the following concerns.\n\n1. How the label taxonomy is reflected on the embeddings is not clear. I believe the 'hierarchical structure' is the factor the model should take into account. But, according to (5), only the information about whether two labels are connected or not is used, i.e., $s_{ij}$ simply represents whether $i$ and $j$ are connected or not. I believe, the model should be able to capture the hierarchy between nodes to correctly reflect the label taxonomy.\n\n2. In many cases, a node can have multiple labels, e.g., a researcher can belong to multiple research communities. I think we should distinguish cases: (i) a node belongs to only one or a few communities or (ii) a node belongs to many communities. That is, the model should be able to adaptively choose the number of branch vectors. Currently, the authors use a fixed number $k$ for this.\n\n3. In experiments, the dimension of embedding vectors is too small. In many conventional settings, the dimension is at least more than 100 or something. Currently, the authors use the dimension of 32. In Figure 4, the performance gap between the baseline methods and the proposed method is not significant when the dimension is 32. I wonder how the results will change if we increase the dimensionality of the embedding vectors.\n\n4. It would be better if the authors add a more concrete theoretical analysis about how Poincare ball resolves the underfitting issues. Also, the authors should present some analysis about how the results are changed if they replace the distance function with the Euclidean distance.",
            "summary_of_the_review": "I reject this paper because the model has major limitations in reflecting the desired property, realistic scenarios, and empirical/theoretical merits compared with the existing methods.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the author studied the problem of network embeddings for taxonomy networks. Specifically, the authors proposed a hierarchical multi-vector embedding method, which generates multiple vectors for a node to fit different set of labels. The proposed method was tested on several dataset to prove its effectiveness.",
            "main_review": "Reasons to accept:\n\n1. The paper is well written.\n2. The problem of network embedding is interesting and this topic would be interested to many researchers.\n\nReasons to reject:\n\n1. The experiment setting was not very clear. If we compare K-vector HIME model with other baselines, shouldn't we also increase the embedding size (dimensions) for the baseline models by K times? Otherwise, the gain may just come from the increase of embedding dimensions. Think an scenario where you concatenate the final K size-N vectors to be a big size-(K*N) vector representation. Can we get this vector representation by just train the model from scratch?\n2. The experiment size seems to be very small, the biggest embedding dimension is 32. I believe the typical size should be at least 256. Could the small dimension size lead to underfitting? If we increase it to 256, maybe baseline methods also perform well and don't have underfitting.",
            "summary_of_the_review": "In general, I am not inclined to accept this paper, mainly because I am not convinced by the experiment settings of this paper. I don't think the motivation of having multiple vectors to represent one node is proved by the experiments in this paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "When a graph is presented with node labels in a hierarchical taxonomy, the paper proposes to learn a single vector for each label and multiple vectors for each node. Specifically, it leans every node a single ’root vector’  and several ’branch vectors’ to fit its labels. ",
            "main_review": "There is one question about optimizing the several branch vectors for one node. Nodes may have different number of labels. However, a fix number of k branch vectors are learned for each node. The vector replacement policy tries to solve unbalanced loads (some branch vectors are active, while others are inactive because they are not associated with any labels).  It still cannot handle the issue that some nodes have q > k and others have q>k, where q is the number of labels for a node. \n\nAnother question is that labels  are treated independently. However, labels are hierarchically dependent, based on the taxonomy.\n\nThe root node representation R is learned by optimizing Eq (4). However, it is not participating the learning of B and Q. It is also not clear how R is used for classification. This separated learning of R doesn’t seem useful.  \n\nSome writing errors to correct, such as:\nIn this part, We \nFinally We will \nminimizing Lossnl, The negative \nan ’inactive’ branch vectors means that \n",
            "summary_of_the_review": "The designed method needs justification on the usage of a fixed number of branch vectors, and the usefulness of root vectors.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper targets the under-fitting problem of taxonomy embeddings, which occurs when the vector of a node is obliged to fit all its labels and neighbor nodes. To solve the problem, this paper proposes HIerarchical Multi-vector Embedding (HIME). Specifically, each node has multi-vector embeddings to fit different sets of its labels, and the nodes and labels are embedded in the Poincare ́ ball model of hyperbolic space. Experiments demonstrate the advantages of HIME in preserving node-node and node-label relationships in low-dimensional embedding space.",
            "main_review": "Strengths:\n1. The proposed approach is well-motivated to resolve the under-fitting problem.\n2. Experiments demonstrate the advantages of HIME in preserving node-node and node-label relationships in low-dimensional embedding space.\n\nWeaknesses:\n1. In the taxonomy, it is common that some nodes have multiple labels while other nodes have only a few labels. Making all nodes have the same number of embedding vectors may have negative effects for some nodes. Considering $k$ as a learnable parameter may be better.\n2. The LRU algorithm is unclearly introduced and misses many necessary details. For example, in Section 4.2.2, it said that 'as long as the LRU policy continues working, a correct vector replacement can be finally achieved'. The statement lacks proof. The paper also mentioned that 'we replace one of its inactive vectors with a new vector initialized close to the active branch vector with the highest hit value', but the expression is too vague. I would suggest presenting the algorithm in detail, e.g., illustrating specifically how to initialize the new vector instead of just using 'close to'. \n3. The experiments do not give the label-label statistics of the datasets or evaluate the label-label relationships. I think the experiments would be more complete if the label-label relationships can also be evaluated since it can show if HIME has compromises in preserving the taxonomy hierarchy.\n4. Some important baselines are missing in the experiments. For example, the Poincare ́ ball model is not the sota hyperbolic embedding method. It was improved by the Lorentz model. JOIE is very related to this paper and it also deals with the entity typing task (i.e., node-label relationships).\n\nQuestions and comments:\n1. In Section 4.3 about Label-Label Relationships, it said that 'we view edges in $S$ as un-directed'. However, the loss function (Eq. 5) takes the same formula as Eq. 3 and 4. So actually the node-label edges and node-node edges are also considered as un-directed?\n2. The experimental results only cover low-dimensional cases. What about the performances of HIME and other baselines in high-dimensional embedding spaces? Does the under-fitting problem of the baselines get alleviated in high-dimension?\n3. In node-node relationships experiments, the number of branch vectors $k$ is not given. Does that mean $k$ does not influence the node-node relationship performance?\n4. In Appendix A.2.4, it said that 'by using the taxonomy information $T$ alone, the machine are not likely to generate a correct tree structure in the Poincare ́ ball, with the root label embedded close to the origin, while the leaf labels embedded near the border'. The statement contradicts the previous works in hyperbolic embeddings. I still suggest evaluating the label-label relationships to see the performances in preserving the hierarchical structure of the taxonomy.",
            "summary_of_the_review": "The proposed method is well-motivated and interesting, but some parts of the algorithm are not clearly presented. The experiments are not solid enough. The label-label relationship task, some important baselines (e.g., Lorentz embeddings and JOIE), and performances in high dimensions are missing. Overall I think the paper is not ready for acceptance yet and needs major revisions.\n\n*Post-rebuttal*: \n\nAfter reading the author response, I think the explanation about constant branch vector number is reasonable. The new experimental results in the high embedding dimension are convincing. Therefore, I would like to raise my score to 5.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}