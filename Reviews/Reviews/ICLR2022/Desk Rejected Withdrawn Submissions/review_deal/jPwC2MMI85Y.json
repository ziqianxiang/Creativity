{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a message passing-based model for predicting molecular properties. The proposed model takes advantage of prior knowledge of functional groups to extract subgraph structures and build hypergraph message passing operations. Besides, the input hypergraph can be adjusted during training with learned embeddings. The proposed model is evaluated on eight benchmark datasets and exhibits better performance than the baselines.",
            "main_review": "Strengths:\n- The paper is well-written and the proposed model is clearly demonstrated.\n- The idea of adopting prior knowledge of functional groups is interesting for the representation learning of molecules and the prediction of molecular properties.\n\nWeaknesses:\n- The power of the proposed model is not strong enough as compared to baselines as shown in Table 1. Even using additional information of functional groups, the proposed model can only beat the other models on some datasets. \n- The rules to extract functional groups in this work cannot get all functional groups defined in chemistry. Thus the resulting hypergraph might be not suitable to completely represent certain molecules.\n- The modified hypergraph during training is not analyzed in detail. How would the structures change via the modifications?\n- Although the proposed model outperforms the baselines with only one message passing layer as shown in Figure 4, it is not clear that why using one layer here is an advantage. The authors should address this with more detailed explanations. For example, will using one layer in the proposed model be more efficient in time/space than the other baselines with multiple layers? Or will using only one layer in the proposed model alleviate the over smoothing issue?",
            "summary_of_the_review": "By summarizing the strengths and weaknesses of the paper, I would suggest the current version of the paper be rejected.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper leverages functional group information to predict the molecular properties. \nIt uses some rule to extract functional groups, models them as hypergraphs, and uses hypergrapm message passing layers to learn on the hypergraphs. The information flows from atom to functional groups only. \nIn addition, the paper uses graph structure learning to adjust the members of the functional groups. \n",
            "main_review": "Pros:\n\n+ Learning to model functional groups is interesting, which brings in prior knowledge into molecular representation learning.\n+ Figure 2 is nicely plotted, which clearly shows the complete pipeline.\n+ The paper is clearly written.\n\nCons:\n\n- The paper fails to convince the readers to believe the performance gain is due to model design. In Table 1, the paper compares the proposed MolHMPN with other methods which do not use functional groups. On some datasets, MolHMPN beats the others. However, is this  performance grain due to more prior knowledge or model design? For example, the authors can add a baseline which pools over atoms to obtain functional group representation instead of using Hypergraph message passing layers. \n\n- The graph encoders (AtomGC & FuncGC) are different from compared baselines (MPNN encoders). Can you compare AtomGC with GAT? The adjustment of attention mechanism in AtomGC can be discussed in more detail. \n\n- The paper lacks a true ablation study. What is presented now is more like sensitivity analysis. Can you separately show the contribution of functional groups, AtomGC, FuncGC and membership adjustment? Where is the results showing \"We also experimentally confirmed that this design shows better prediction results\" on page 6?\n\n- How the hyper edges changes during learning? Can you visually plot the functional groups used before and after adjustment? Is this adjustment really make sense?\n\n- The rules to exact functional groups are not complete. They cannot cover all functional groups. \n\n- Notation issues: Using consistent forms to represent functions can be better. For example, change $G_\\theta$ in (14) to $g_\\theta$.\n\n\n",
            "summary_of_the_review": "I like the motivation of the paper, but the current solution is not well validated. As written in the main review, several concerns exist. \nTherefore, I vote for rejection but may increase my score if my concerns are addressed. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a model that uses the known domain knowledge to predict the properties of the molecules. Specifically, the model extracts subgraphs, which are similar to known functional groups. A hyper-message passing network is proposed to use the extracted subgraph structures. An additional algorithm is employed to learn and adjust the extracted substructure in an end-to-end fashion.  The proposed model is comparable with or performs better than known methods on 8 real-world datasets.",
            "main_review": "- Pros\n    - The paper introduces a specific domain knowledge (functional group) into graph prediction problems.\n\n- Cons\n    - Lack of detailed analysis on the extracted and learned structures.\n    - Lack of proper ablation studies.\n\n- Questions\n    - Can hyper-edge extension induce disconnected subgraph structure? What substructures are founded from the hyper-edge extension and membership adjustment? and are there any chemical properties related to the findings?\n    - What functional groups are related to which tasks?\n        - What are the final substructures obtained from hyper-edge extension and membership adjustment?\n        - Is there any task-specific adjustment?\n    - Are functional groups only interacting with the other functional groups?\n        - As shown in Equations 7 and 8, the functional groups only interact with their neighbor functional groups.\n    - How are the features of subgraph, z_k, constructed?\n    - What happens if the substructure is fragmented by the membership adjustment or the hyper-edge extension?\n    - Given the current ablation study, it is difficult to understand where the performance gain comes from. For example, what is the impact of the weighted loss function as written in the appendix?\n        - What is the impact of membership adjustment?",
            "summary_of_the_review": "This paper proposes an interesting approach based on the functional groups to tackle the graph prediction problems. The approach seems valid and novel, however, it is difficult to understand where the performance gain comes from with the given experiments. Proper ablation studies and analysis will greatly improve the contribution of this paper.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a molecular hyper-message passing network (MolHMPN) to integrate pair-wise and higher-order connectivities for molecular properties prediction using domain knowledge-guided learned substructures. The proposed model provides a way to utilize prior knowledge in chemistry for molecular properties prediction tasks, and balances between the usage of prior knowledge and data-driven learning adaptively. The extensive experimental results show that the proposed method is able to outperform the other baseline methods for most of the dataset, and show that using domain knowledge-guided data-learning is effective.",
            "main_review": "##########################################################################\n\nPros:\n\n- The paper provided an interesting direction for the molecular properties prediction field. The core idea is on the adaptation of functional groups using prior knowledge and the utilization of the prior knowledge selectively when conducting the molecular prediction tasks, balancing between prior knowledge and data-driven learning. That's a great idea and the problem itself is important and practical. \n- The author conducted extensive comprehensive experiments, including 8 benchmarks datasets, quantitative results and ablation study, to show the effectiveness of the proposed model. In particular, the experiments show that MolHMPN is able to achieve outstanding results with only one HyperMP layer. In the ablation study, the experiments show that using domain knowledge-guided learned substructure improves the performance of the benchmark tasks. The authors also compare the usage of different types of substructures using the same model architecture and show the efficacy of employing chemically meaningful and valid substructures. Overall, the comparison of benchmark methods is also interesting to read.\n- The paper is well-written and the design decisions are clearly explained, especially the METHODOLOGY section is very detailed and easy to understand.\n\n##########################################################################\n\nCons:\n\n- Unfortunately, from Table 1, it seems that compared with the baseline methods (PAIR and SUB), MolHMPN has no obvious advantages, and the results on the 4 datasets (Tox21, SIDER, BBBP, and ESOL) are not as good as the baseline methods. It is recommended that the author make a more detailed explanation or try more other baseline methods for comparison.\n- From Table 2, comparison between the different K used, the author stated the extended learning strategy has mostly improved the performance of MolHMPN. This is the result of inference from experiments. Is there a more theoretical or mathematical explanation?\n- From Table 3, comparison between different types of subgraphs. Why is the result of 2-hop ngh. better on BACE and ESOL? Why does Ring & C. Bond have better results on Lipophilicity? I advise the author can give a more detailed explanation.",
            "summary_of_the_review": "Considering the above pros and cons, my recommendation of the paper is marginally above the acceptance threshold.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}