{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a grasp learning approach which exploits equivariance properties of grasp location and orientation in a specifically constrained robot-picking setup. The method proposes a contextual bandit approach which subdivides 2D grasp location in the horizontal plane and vertical grasp orientation. For both action components Q functions are learned separately based on binary grasp success as reward. The Q functions model rotational equivariance through equivariant convolutional filters. Experiments demonstrate that grasps can be learned more efficiently than if not exploiting equivariance properties. Also ablations are provided, demonstrating the importance of equivariant learning.\n\n  \n",
            "main_review": "Strengths:\n* The idea of applying equivariant convolutional layers to grasp learning seems novel and effective for the specific picking setup considered in this work.\n* The paper is mostly well written and the main concepts can be understood well.\n* The paper provides experiments in simulation and on a real robot which demonstrate the approach. Also an ablation study is provided which shows the benefits of design choices.\n\nWeaknesses:\n* The paper is not upfront in the abstract, introduction, conclusion about the limited robot-picking setup considered in this work. The setup assumes that the camera is placed from an above view of a planar horizontal surface on which objects are placed. The image plane must be parallel to the object support plane. The robot is equipped with a  parallel gripper and vertical top-down grasps can be executed with variable position along the object support plane, vertical height above the plane, and orientation of the gripper along the vertical axis. This grasping scenario is quite limited and might only work for a limited range of objects, scenarios, and applications (like picking and placing in a arbitrary orientation at some other location). The limitations should be clearly stated, also early in the paper. \n* The first sentence of Sec 7 is inappropriate, because the equivariance property does not apply generally to the visual grasp detection problem.\n* The assumption that the grasps are location invariant is an approximation due to the fixed camera location and the pinhole camera projection. This should be discussed.\n* for the \"equivariant q_1\" the regular representation is used for intermediate conv layers, motivated by encoding \"more comprehensive information\". This requires more elaborate discussion. Please clarify what you mean with comprehensive information and why using equivariance in q_1 is beneficial.\n* Please explain why D_4 including reflections is chosen/performs better for q_1 over C_n or C_n/c_2 like for q_2. Why choosing a different rotation granularity/representation in q_1/q_2 ?\n* For \"equivariant q_2\", the output dimension of F_out (n) seems to mismatch with the dimensionality of the used group C_n/C_2 (n/2). Please clarify.\n* The \"Loss Function\" section uses different notation: what is I and \\gamma ?\n* The paper mentions equivariant convolutional kernels which are not properly defined for the used equivariance groups like D_4 or C_16/C_2. They should be defined at least in the supplementary material for reference.\n* The experiments compare with two data augmentation baselines which do not fairly exploit the equivariance property. Significantly more training data could be injected using data augmentation techniques by sampling for each example a large amount of shifted and rotated training images with target actions augmented accordingly. The proposed baselines only seem to use 1 or 2 augmented examples per run. Even more surprisingly, in the proposed method 8 orientation-augmented samples are added per run to overcome the small discretization in D_4 for q_1. How would a pure data augmentation method perform which samples significantly more augmented examples and trains in mini batches over these augmentations (instead of only one minibatch of 1 augmented sample per run like RAD)? How would such a method perform in the limit case which shifts by every pixel and discrete orientation? How would those methods compare to the proposed equivariant approach.\n* The real robot experiments should also compare with a solid data augmentation method as explained above. Can a proper and fairly set data augmentation method achieve similar performance than the proposed method? What would be the advantages/disadvantages of the proposed method in that case?\n* Why is the number of discrete rotations increased from 8 to 16 in the 1000 grasp experiment on the real robot ?\n* Why not include the z coordinate in the learned action ? How would the approach perform in terms of success rate and sample efficiency ?\n* How is success measured in the real robot experiments ?\n\nFurther comments:\n* please define dimensionality m in sec 3.2. what is used in the experiments?\n* The title mentions \"real time\" but there is no proper definition what this means? Would any robot experiment not be executed in real time ? Do you mean in just a few hundred trials ? Or do you mean learning online ? Or both ?\n* The font size of the graphs in fig 3 are too small. they should be min 0.7 caption font size\n\n",
            "summary_of_the_review": "The paper proposes an interesting approach to grasp learning which exploits equivariance properties in a specific picking setup with a special placement of the camera, robot and objects. The paper is not quite upfront with the limitations. The experiments seem to validate that the proposed method can learn fast, but it does not well carve out if it is clearly superior to alternative data augmentation strategies that exploit the equivariance property better than the ones considered. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes the use of SE(2)-equivariant networks for learning grasping points in objects, when using contextual bandits' formulation of the problem. The authors discuss the exploration strategy in their bandit setting, and provide simulated and real-world results for the bin-picking problem. ",
            "main_review": "The paper focuses on the problem of SE(2) grasping from a bin. This is a well-studied problem in robot learning, in particular with data-driven methods. However, I find the contribution of this paper very limited. In my opinion, the paper's main contribution is the use of the equivariant network, which is a line of works that the authors integrate into their problem. What the network ends up doing is a regression over poses in 2D and bins of orientations. This is a well-known strategy for tackling the grasping problem, e.g., it was similarly proposed but for supervised learning in [1], to disentangle the problem of multiple orientations. If this is not the case, the authors should put effort into better explaining their method, as all approaches separating pose detection from orientation prediction, use the argmax of the grasp pose prediction to infer the orientation (as depicted in Fig. 1, and discussed in Sec. 3.2), and is commonly done in [1], [2]. Moreover, the authors present the methodological aspect of the contextual bandit problem in Sec. 5, without giving credit to the original paper that framed the problem as such, that is DexNet v1 [3], therefore it sounds like it is the authors' contribution. \n\nRegarding the empirical results, I mainly have concerns for the chosen baselines, as the comparison is doomed to point to the bandit method as the better method, given that the other methods are supervised learning methods that would probably not converge on a small grasping dataset. That being said, I am not convinced by the increasing advantage of the proposed approach for achieving the reported results, as I have not found videos regarding the simulated and real-world performance, and the codebase does not include the implementation of the baseline methods. \n\nFinally, I am concerned about the appropriateness of the conference for this paper. The paper does not propose a fundamentally new approach for robot learning or a new representation but rather integrates a different network architecture into a well-known method for learning to grasp. Comparing to bandit-based approaches would make it a more fair empirical study. I would recommend to the authors, apart from considering the reviews and concerns about their contributions, to consider submitting a revised version of their work to a robotics conference. \n\n[1] Chalvatzaki, G., Gkanatsios, N., Maragos, P. and Peters, J., 2020. Orientation Attentive Robotic Grasp Synthesis with Augmented Grasp Map Representation. arXiv preprint arXiv:2006.05123.\n\n[2]  D. Morrison, P. Corke, and J. Leitner, “Learning robust, real-time, reactive robotic grasping,” IJRR, vol. 39, no. 2-3, 2020.\n\n[3] Mahler, Jeffrey, et al. \"Dex-net 1.0: A cloud-based network of 3d objects for robust grasp planning using a multi-armed bandit model with correlated rewards.\" 2016 IEEE international conference on robotics and automation (ICRA). IEEE, 2016.",
            "summary_of_the_review": "Given the major concerns regarding the contribution, the differentiation of the method compared to the state of the art, and the concerns about the empirical results, I proposed rejection of the paper. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper identifies the grasp pose detection as a SE(2)-equivariant problem and designs SO(2)-equivariant methods for grasp detection. The authors decouple the grasp pose detection with two networks and adopt equivariant convolutional layers for each network. Abundant experiments are conducted and they demonstrate the good performance of the method. The proposed method achieves good efficiency during training.",
            "main_review": "===>Strengths:\n1. The authors observe that the grasp pose detection is SE(2)-equivariant and decouple this problem into two stages. Each stage is computed with SO(2)-equivariant model. Such formulation and the introduction of equivariant learning are new in this area.\n2. Abundant technical optimizations are explored.\n3. Many experiments are conducted and the authors compare their method with many counterparts in simulation and real-world. \n4. Technical details and codes are provided. The community is easy to reproduce the results.\n\n===>Weakness:\n1. The problem setting, planar grasp pose detection, has been studied for decades and recent methods have achieved very good performances. There are already mature datasets and methods that can perform well. Thus, although the proposed method is interesting, whether it is a good area to apply equivariant learning is doubtful.\n2. There are a lot of existing datasets like [A][B][C], the authors should at least compare the proposed method on one of these benchmarks. This can facilitate fair comparisons. The authors might argue that these datasets are not for RL purposes. But as far as I can see, the proposed networks can be trained easily in a supervised manner.\n3. In the literature review, recent 6D based grasp pose detection methods like [D][E] are not discussed. The authors are encouraged to discuss the differences and potential application of the proposed equivariant learning method.\n\n[A] Deep Learning for Detecting Robotic Grasps\n\n[B] Jacquard: A Large Scale Dataset for Robotic Grasp Detection\n\n[C] Real-world Multi-object, Multi-grasp Detection\n\n[D] GraspNet-1Billion: A Large-Scale Benchmark for General Object Grasping\n\n[E] 6-DOF GraspNet: Variational Grasp Generation for Object Manipulation",
            "summary_of_the_review": "The authors introduce novel equivariant learning for the problem of planar grasp detection. Real world experiments show good efficiency of the method. However, important comparisons with existing methods on widely used datasets like Cornell and Multi-Object Multi-Grasp are missing. And the necessity of applying the proposed method to a well-solved problem is not fully illustrated. The reviewer hopes that the authors can address the concern in the main review.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a novel neural-network based method which employ NN architecture with consideration of SE(2) equivariance, \nthe mathematical group theory. The author mentions that very high success rate of object grasping task for robot gripper is established by much fewer number of training trials than previous methods.\nIn the formulation, SE(2) group is factored into R^2 x SO(2) (2D translation and 1D rotation) and then for each of them the Q function map which represents worth of the translation and rotation candidates is implemented by employing CNN, U-net and ASR(augmented state representation). For the NN representation, R^2 is quantized into pixel grids and SO(2) is quantized into discrete rotation group Cn(2),  circular subgroup of SO(2).\nThe implemented Q-maps are trained by rewards gained when grasping trials are succeeded. \nThe experimental results show that the network outputs the worth of each gripper pose under SE(2) and then the gripper can successfully grasp objects in any 2-D pose, through the simulational and real robot environment.",
            "main_review": "The employment of the network embedding the group SE(2)  for proposal of robot grasping action is an interesting approach. \n\nIn sec 5.1, \"We empirically achieve the best performance when defining q1 in the Dihedral group D4\" -> the reason for the best performance by D4 (rot by 90deg unit) should be mentioned if you have an explanation.\nD4 seems too rough to represent any rotational pose of the target object and the gripper.\n\nIn Eq.12, it is slightly unclear why the sampling of x from Boltzmann distribution is needed. \n\nThe experimental results doesnot include any proposed grasping gripper pose example for an target object. The readers do want to see how appropriate gripper pose is proposed by this architecture.\nIn Fig.2, there are multiple objects  the input depth image in (a). It doesnot seems that any segmentation or target selection is done. So it is a bit curious how to select the target to grasp. For training step, does the grasping trial action randomly sampled?\n\nAblation studies are well shown. \n\nIn Sec 5.1, \"Since the parallel jaw gripper is symmetric when rotated by pi/2\" -> pi/2 is a mistake of pi ? ",
            "summary_of_the_review": "The strength of the paper is to try to apply SE(2) equivariant group theory to gripper pose generation for grasping the target object.\nOne more strength is to achieve the very efficiently small number of NN training steps.\nThe weaknesses is that any examples of proposed grasping gripper poses are not shown.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}