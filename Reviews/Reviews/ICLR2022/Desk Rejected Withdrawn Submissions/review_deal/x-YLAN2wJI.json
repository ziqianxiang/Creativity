{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presented a new approach to reduce the computation cost of InfoNCE-based contrastive learning approaches. The main idea is to use fast kernel approximation methods to speed up the computation in the correlation between the negative samples and the anchor in the InfoNCE loss. By utilizing off-the-shelf kernel approximation methods, this paper managed to reduce the memory and computation cost of InfoNCE-based approaches such as SimCLR and Graph contrastive learning (GRACE) by a notable margin. Additionally, the introduced kernel methods are motivated by the mutual information bottleneck principle.",
            "main_review": "## Strength\n\n- This paper presents new methods to reduce the memory and computation cost of contrastive learning. This is highly relevant to applications of contrastive learning, given the fact that its cost of training is notably high because of the large number of training epochs.\n\n- The proposed approach is motivated by principles of maximizing mutual information. Section 3 offers a connection between the InfoNCE loss and kernel functions. This connection is quite interesting and has good novelty.\n    - To utilize this connection, this paper suggested using a fast kernel approximation method—the structured orthogonal random feature—to speed up the computation related to the negative samples.\n\n**Other comments**\n\nPaper is well-written and easy to follow. Related works are thoroughly discussed.\n\n## Weakness\n\n- In the experiments, the application of the above kernel approximation is achieved by applying off-the-shelf kernel approximation methods. While the results are certainly interesting and worth publishing, it remains unclear if the techniques are applicable to other kinds of contrastive learning methods. For example, I would be curious to hear authors’ thoughts regarding whether the proposed approaches extend well to hard negative sampling (and other follow-up results to SimCLR), which has been shown to achieve more accurate predictions than SimCLR (Robinson et al. (ICLR 2021)).\n\n- In section 4.3, the author have already acknowledged that methods such as BYOL and SimSiam which do not use negative sampling can get away with the computation problem that this problem is trying to tackle. In particular, both BYOL and SimSiam have shown strong prediction performance on the ImageNet dataset. Thus, the advantage of the proposed approach over these (by now already widely known) approaches remains unclear at best. I think this section should be more clearly stated. In addition, the experiments could at least be expanded to include the corresponding comparison on several datasets.\n\n\n**References**\n\n- Contrastive Learning with Hard Negative Samples. Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, Stefanie Jegelka. ICLR 2021.",
            "summary_of_the_review": "This paper presented an approach to boost the performance of InfoNCE-based contrastive learning methods. While the idea of using kernel approximation to speed up the computation of the InfoNCE loss is interesting and the results compared to SimCLR are good, the technical novelty is marginal. In addition, similar results have been shown in related topics (e.g., speeding up self-attention in transformer models).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Considering the present drawbacks of InfoNCE-based contrastive learning methods: (1) possible exploitation of superfluous information useless for downstream prediction tasks; (2) quadratic time complexity and memory cost, the authors propose effective and scalable contrasstive learning (ESCo) framework for multi-view contrastive learning. In the proposed framework, the target of Multi-view Information Bottleneck (MIB) is optimized to keeps maximal task-relevant information and discard as much task-irrelevant information as possible. \nTo further reduce the space-time complexity, Random Fourier Features (RFF) and Structured Orthogonal Random Features (SORF) are employed where more negative samples and much larger batch sized could be used consequently. Finally the effectiveness of the proposed framework is verified on the tasks of image and node classification by being compared with InfoNCE-based methods.",
            "main_review": "Strength\n- The authors take a different perspective from MIB and use Lagrangian relaxation to achieve the optimization target of MIB which does not depend on additional self-supervised learning tasks as [1].\n- The space-time complexity is effectively reduced to linear from quadratic by employing RFF and SORF.\n- Extensive experimental studies on image and node representation learning show the superior performance of proposed methods compared to the InfoNCE-based baselines.\nReference\n[1] Self-supervised Learning from a Multi-view Perspective. ICLR 2021\n[2] Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere. ICML 2020\n\nWeakness\n- It is unclear what is the advantage of the proposed ESCo framework over the previous works [1] and [2]. \n   The authors argue that ESCo needs no additional self-supervised learning tasks, while the proposed optimization target in Equation (6) is similar to the additional inverse predictive learning task used in [1]. \n- More hyper-parameters are introduced in ESCo framework (i.e., lambda and tau) which need careful tuning in the real-scernario.\n- The experimental study of this work is not convincing, it does not exhibit the comparison results between the proposed methods and previous works [1] [2] which are most related to this work.\nReference\n[1] Self-supervised Learning from a Multi-view Perspective. ICLR 2021\n[2] Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere. ICML 2020\n",
            "summary_of_the_review": " It is unclear what is the advantage of the proposed ESCo framework over the previous works [1] and [2]. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work proposes an Effective and Scalable Contrastive (ESCo) framework which targets a new objective that seeks to maximize the similarity between the representations of positive pairs and minimize the pair-wise kernel potential of negative pairs. Moreover, to reduce quadratic time complexity and memory cost, this work leverage the Random Features to achieve accurate approximation with linear scalability.  ",
            "main_review": "Strengths:\n(1)\tThe proposed ESCo is a general contrastive learning method and will become infoNCE-alike loss. \n\nWeaknesses: \n(1) My biggest concern for this work is its experiments. The authors only investigate theirs on toy data and some small and not-widely-used datasets. Moreover, they also compare SimCLR method and directly ignore many other existing SOTA contrastive learning methods.  SimCLR usually requires a large batch for negative samples and may achieve poor performance when the dataset is small. In this way, the experiments cannot verify the performance of the proposed one. \n\n(2) The authors claim their method is efficient. But they do not test on large-scale datasets. \n\n(3) This work explores the Information Bottleneck principle for self-supervised learning settings. A similar idea actually is already explored in “What Makes for Good Views for Contrastive Learning?” The authors should discuss the differences. \n",
            "summary_of_the_review": "Overall, this work does not provide very new methods, and experiments do not verify its advantages.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This papers proposes to accelerate the InfoNCE-based contrastive learning models by leveraging the Random Features to approximate the kernel computation. It is shown that the original vanilla InfoNCE objective is a specification of the proposed method, and experiments on both synthetic and real-world datasets show its superior performance over the InfoNCE-based baselines in (unsupervised) representation learning tasks for images and graphs.\n",
            "main_review": "The proposed method is a further development of the InfoNCE based contrastive learning methods, which focuses on scaling up these methods by leveraging the Random Features to approximate the kernel computation. There are several concerns on the proposed method:\n\nFirst of all, I find the paper especially the title and abstract to be a bit overclaiming. For example, it is said the proposed method is  provably effective method. I don't really know what this means, as I do not see how and where the proposed method is *provably effective*. The proposed method, of course, is based on the principled random feature parameterization of kernel functions. This, however, does not come with provable guarantees. From my understand, the proposed method is theoretically motivated, but does not come with provably effective.\n\nSecond, I am not sure about the analysis in Section 4.2. It is said that the original InfoNCE based methods are O(N^2) complexity in the objective. And the proposed method is O(NDd) in calculating the kernel approximation. Combing this in the objective, the objective complexity is O(N^2Dd). I don't see why this is better than the original complexity, as they are both O(N^2). A more detailed comparison and analysis should be conducted.\n\nThird, it is not clear from the main text whether a minibatch is used in learning. For the ease of computation, I assume it is. Then the next question is how large the minibatch size is. From my experience, a more important aspect of the scalability of the InfoNCE based methods is on whether the minibatch size has a great impact in the performance. This has been studied in the Barlow Twins method by Zbontar et al. In other words, investigating the effectiveness w.r.t. the minibatch sizes seems to be an important problem, but was not studied in the paper. Also, I think the Barlow Twins should be a baseline to compared with as it is also an effective method for contrastive learning.\n\nMinor comments: \n1. x_j^B in the numerator of eq.1: I believe it is x_i^B?\n2. Below eq.3: it says *\\beta is a Lagrange...\", but there is no \\beta in eq.3.",
            "summary_of_the_review": "Overall, the technique of this paper is good and the description is reasonably well. However, the claiming the paper is not well described, which I think needs to be addressed significantly.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}