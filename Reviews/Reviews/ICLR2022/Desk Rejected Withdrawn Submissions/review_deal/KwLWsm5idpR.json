{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper starts off by remarking that ML models can have criteria, e.g., fairness, in addition to the usual accuracy metric. The paper then proposes to address such situations using multiobjective optimization, specifically by tuning hyperparameters of the ML models. Next, the paper describes a hyperparameter optimization framework that takes into account multiple objectives to explore the pareto front of the multiple objectives. Experiments are provided on real world datasets to show the working of the algorithm.",
            "main_review": "The paper addresses an important problem. However, it can be improved significantly along two dimensions before it is ready for publication 1) exposition and 2) empirical analysis. Please see the comment below for details.\n\nFirst, the writing of the paper seems to need some major work. Algorithm 1, which is the most important component of the paper, is disconnected from the surrounding text. It is not clear immediately what population size $n_p$ is. What is the sufficient decrease $\\alpha$ and how is it picked? What is pattern search success and failure? Also, from Section 3.3, it is not entirely clear how the basic building blocks work (e.g., what is reference cache tree, what is the Hausdorff distance, what is its variation used here) and how the different existing components are put together. What was the motivation for combining these components and what other choices were ignored (e.g., what other options are available instead of using LHS)? Terminology like local and global search is also not precisely defined and the reader is left to infer it on their own. With all of this confusion, it is not clear where the contributions of the previous methods end and those of the current paper start.\n\nThe experiments also need quite some work. First, it is not clear why Section 4.1 - 4.3 work on different datasets? Why not provide the results for all the datasets? Also, given that the level of unfairness is very small in German data (0.027, meaning the acceptance rates of the two groups differ by 2%), is it even worth trying to remove it? How do the results look if we average them over different random splits of the data? It should also be noted that the \"importance of multi objective tuning\" has been noted in prior studies. Many prior papers (e.g., Agarwal et al) can reduce unfairness w.r.t. several fairness definitions. The comparison with Agarwal et al. in Section 4.3 is also not explained very well. What are the regression hyperparameters of Agarwal et al. and how are these tuned? The technique of Agarwal et al. can operate with multiple fairness thresholds (c in Eq. 2 in their paper). Which of these threshold was picked when computing the pink triangles? What was the runtime difference between the two approaches in Figure 4? Finally, why was the runtime not reported in Section 4.2? Also, given that there have been several studies by now looking at fairness and model hyperparameters (the paper mentions several, in addition also see the [Perrone et al.](https://arxiv.org/pdf/2006.05109.pdf) from AIES'21 and [Schmucker et al.](https://meta-learn.github.io/2020/papers/24_paper.pdf) from NeurIPS'20 Metal learning workshop), it is not clear why a head-to-head comparison with at least one of these schemes was not included. Overall, the empirical evaluation is not able to make a case of *why* and *when* the proposed scheme is better than what is already out there.\n\nIn the text under Theorem 1, I think the statement regarding Exponentiated Gradients might be a bit unfair to Aggarwal et al. After all, their Lagrangian includes a fairness term which can be modified to change the level of unfairness that can be tolerated. So in that sense, their objective really consists of two different terms, one for unfairness and one for accuracy. A comment from the authors on this would be great.\n\nThe writing is somewhat overly-exuberant at times and it would help to provide more clarification/evidence for some of the statements made in the paper. For instance:\n *  Section 3.1: \"Autotune's ability to optimize over multiple objectives becomes extremely important\". There are several existing fairness methods that can optimize for multiple fairness objectives.\n * Section 3.3: \"The combination of global and local searched drastically improves the robustness\". How is this established? Did the paper conduct an ablation study to test this?\n * Section 3.4: \"Autotune's parallel design is extremely powerful and capable of efficiently using compute grids of any size\". Powerful w.r.t. which methods? Can't fairness enhancing methods like Agarwal et al. be similarly parallelized (e.g., for different values of c)? Or is it that the proposed hyperparameter optimization method is more parallelizeable than the base optimization methods that it builds upon?\n * Section 4.4: \"More than 20% of true positive rate ...\" Is this observation specific to this paper only? What is so special about the multi-objective setting that enables this? Aggarwal et al. (Figure 1 in their paper) also show large reductions in unfairness while incurring only a small hit in accuracy.\n",
            "summary_of_the_review": "Interesting problem, but 1) key contribution not clear 2) experiments not convincing and 3) writing can use significant work.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper describes how to use an AutoML framework, Autotune, to perform multi-objective optimization in order to trace out a multi-dimensional accuracy-fairness Pareto front. The paper summarizes Autotune's framework, search strategy, and parallelizability. An empirical section performs experiments on several datasets, including UCI Adult and German credit and COMPAS, to empirically demonstrate the benefits of multi-objective optimization over single objective optimization and the importance of fairness hyperparameter tuning in conjunction with multi-objective optimization. The authors also demonstrate an example of a five-dimensional Pareto on the UCI adult dataset that shows the tradeoff between accuracy, demographic parity, equalized odds, true positive parity, and false positive parity (Figure 5). ",
            "main_review": "+ The paper does a nice job motivating the relevance and importance of performing fairness assessment through multi-objective optimization.\n+ The paper has a useful related work section, describing work in fairness and in multi-objective optimization for fairness. It may additionally be relevant to cite Martinez, N., et al. Minimax Pareto Fairness: A Multi Objective Perspective.\n\n- The contributions of the paper are not clear. The paper suggests there is a methodological contribution in adapting autoML for the fairness multi-objective setting, but I could not find where details on this where given. Instead the methodological details provided (e.g. Algorithm 1) summarize the existing Autotune algorithm.  \n- The results should describe the test set size and provide estimates of error bars. \n- It would be useful to consider alternative visualizations for Figure 5 that would make it easier to compare the tradeoffs between fairness metrics. \n- It would be informative to provide an analytical explanation for why fairness hyperparameter tuning is important for recovering the full Pareto front.",
            "summary_of_the_review": "This paper addresses an important problem but the contributions of the paper relative to existing literature on Autotune are not clear. The empirical results are illustrative but would be more informative if they were accompanied by analysis for e.g. why fairness hyperparameter tuning is important.The empirical results should include error bars.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper introduces an AutoML framework (Autotune) that is able to build ML models accounting for both model accuracy and model fairness. Autotune allows for hybrid search strategies to be defined by the user by leveraging a host of algorithms that are predominantly single objective in nature.  Technically, the introduced approach optimizes the bias mitigation hyperparameters and model hyperparameters simultaneously by generating higher-dimensional Pareto fronts through a single optimization process to achieve an approximation of the global front that depicts the trade-off among multiple model fairness and model accuracy\nmeasures. Four numerical results on three commonly investigated real-world case studies show that solving the fullspace optimization problem is more efficient than focusing on two selected dimensional subspace objectives, and it delivers more insights into the accuracy and fairness trade-offs.",
            "main_review": "\nThere are several things to like about this paper:\n\n1. The problem studied in this paper is well-motivated. Multi-objective (MO) problems in the context of fairness and accuracies are pervasive. However, this area of MO-fairness has received relatively little attention from the research community.\n2.This paper is well-organized and clear written. The authors start from stating the overall picture of for Autotune framework. Then, the authors present Hybrid Search Strategy in Autotune\n3. The claims are well supported by four multi-objective tasks on three public datasets. \n\nHowever, I found that there are several shortcomings:\n\n1. Mathematical framework (objectives) studied in the paper are not well defined before using. After checking Appendix, I found that the authors define the composite AutoML and fairness multi-objective optimization problem in A.1. It is better to move it to the main context, for example Section 3.2 rather than directly jumping into the algorithm itself.\n2. Given that there are extensive experiment results reported, it is critical to ensure the reproducibility of the paper. I did not find that the developed algorithms will be open-sourced or link to the code.\n\n\nMinor issues: \n- in general: better to use full-space rather than full space\n- in general: better to use non-discriminative than nondiscriminative\n- page 9, figure 5: exeuctions -> executions\n- page 9: importantance should be importance",
            "summary_of_the_review": "In this paper, authors introduce a novel AutoML framework that naturally supports multi-objective optimization for fairness and hyperparameter tuning. Although I like the topics investigated in this paper, it would be better if the paper can make it more clear about the mathematical framework  about the problem studied in the main context. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}