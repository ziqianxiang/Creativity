{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes an enhanced sign method (SM) for adversarial attacks. Specifically, instead of using one-stair sign, the proposed method involves multiple staircases for each side, which named S2M. For each stair, a proper weight would be added to adjust the gradient. The main motivation of this paper is to make use of not only the sign but also the values for generating attacking samples. Overall, this basic idea is straightforward and make sense. Experimental evaluations are implemented to show the effectiveness of the proposed S2M.",
            "main_review": "Strength:\n+ The motivation is good. This paper has pointed out the limitations of traditional sign method (SM), which is remarkable. Traditional SM only consider binary decisions, while ignore the complex and dynamic gradient distributions changing from different samples and networks. The propsoed S2M is able to upate gradient with mroe accurate scale factors.\n+ The proposed method is straightforward and seems effective. The proposed stair-based sign method could provide more accurate estimation of gradient updating actions. Through dividing the activation into K groups, the proposed S2M could assign a weighting parameter for each stare.\n+ The experimental analysis is sufficient and reasonable.\n+ The paper is well written and organized.\n\nWeakness:\n- The effects of the stair partition and the weighting strategy are somewhat reduplicative. As has stated in Page 6 ‘S2M is equivalent to applying adaptive weight for each pixel in sign noise.’ It seems that the stair partition just serves for the definition or adjustment of the gradient weight. However, there would be a much more straightforward manner to adjust the weight with learning-based method, such as introducing the attention mechanism. Therefore, the essence of the proposed method should not be the ‘staircase’ sign method, but should be the assignment strategy of weighted gradient with suitable values.\n- In the ablation study of evaluating K, an important baseline is missing. When K = 1, is it possible to calculate a weighting value for only one stair? Such a model S2M(K=1) is different from SM(K=1), and is more proper to be as the baseline.\n- Minor typos, e.g., Page 7, The target label for each iamge in this dataset ... -> The target label for each image in this dataset...\n",
            "summary_of_the_review": "Overall, this is a good paper with well-motivated idea and effective method for adversarial attacks. The proposed stair-wise dynamic weighting pipeline (S2M) provides a reasonable way for enhancing previous widely adopted sign method (SM). Considering above strengths and weaknesses, the quality of this paper is above the acceptance threshold. Currently, the reviewer would like to slightly recommend to accept this paper, and encourage the authors to address proposed concerns.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a new white-box adversarial attack method named staircase sign method.\nThe existing attacks only uses the sign of the gradient but neglect the magnitude.\nThe proposed staircase sign method can be seen as a weighted sign method for differentiating the model's sensitivity along different dimensions of the input.\nExperiments demonstrated the effectivenss of the proposed method for transferrability.",
            "main_review": "Strengths\n1. The proposed method is well-motivated, simple, and clearly written. It is an intuitive modification to the existing FGSM method and I-FGSM method.\n\nWeaknesses\n1. Lack of enough experimental evaluation to demonstrate the advantage of the proposed method compared to its baselines. The proposed method is an intuitive modification to FGSM/I-FGSM/PGD. However, the baseline works are oriented for white-box targeted or non-targeted attack instead of transferrability. To fully demonstrate the effectiveness of the proposed method, it should be evaluated on the original task as well. This is important because it is highly possible to be adopted for evaluations in the original task by follow up works. Absence of such result hampers follow-up works. Besides, it is also encouraged to compare the proposed method with some recent new methods, such as auto-attack \"\"Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks\", ICML20.\n\nMinor Issues\n1. Please avoid abbreviating \"I-FGSM\" into \"I\" in the table without any explanation. Not every reader is able to understand the single ambiguous upper case letter \"I\" as \"I-FGSM\" without struggling.",
            "summary_of_the_review": "The proposed method is intuitive, simple and easy to understand. It is also effective as demonstrated by the experimental results.\nHowever, the proposed method is evaluated on a different task than the original task of the baseline method. Important experimental evaluation on the original task may be missing, which hampers follow-up works for adopting the proposed method.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety"
            ],
            "details_of_ethics_concerns": "The proposed method boosts adversarial attack. But I think it is fine at this point.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper aims to craft adversarial examples for the transfer-based attack. More specifically, the paper proposes a Staircase Sign Method that divides the gradient into several segments according to its value, and then assigns each segment with a staircase weight to generate adversarial perturbations. The proposed approach is evaluated on the ImageNet dataset with the family of FGSM methods.\n",
            "main_review": "Strengths: \nThe paper is clearly written and easy to follow. It uses the staircase sign function to improve the sign method used in FGSM-based attacks. The proposed method shows performance improvement especially for those methods integrated with diversity input patterns and the AoE case. Efficiency is another advantage of the proposed approach. \n\nWeakness: \n1) The contributions and novelty of this paper seem limited. The general idea is to change the sign function used in the gradient generation with a staircase function.\n2) The experiments only show the performance compared with the family of FGSM methods. How well does the proposed approach perform compared to other attack generation methods such as Universal Adversarial Perturbations (UAP) that also generalize well across neural networks? As for the “Hold-out” case, what about the performance of the proposed method compared with other transferable targeted attacks such as FDA [1] and TTP [2] as pointed out in the related work?\n3) The method shows improvements over FGSM-based methods integrated with diversity input patterns and the AoE case, but the improvements in the ensemble case is not very clear. Besides, in the hold-out case that hopes to show the transferability of the proposed method, the performance gain is not very obvious as most cases have a very low attack success rate. For instance, the proposed method only increases the success rate from 0.0% to 0.4% for FGSM-Po. \nA further question is about the transferability experiment setting. The experiment uses four models: Inc-v3, Inc-v4, Res-152, and IncRes-v2. Some of these models such as Inc-v3 and Inc-v4 are structurally similar. The results would be more convincing if the evaluation could include some other distinct architectures such as VGG19 and DenseNet used by FDA and TTP. Also, the experiments in this paper mainly focus on ensemble models. Showing transferability from one single model to another would be helpful.\n\n",
            "summary_of_the_review": "This paper proposes to use the staircase sign method instead of the sign method to boost the transferability of the family of FGSM attacks. In general, the contributions of the paper are not significant. The experiments do not adequately support the main claims and some important baselines are missing. The proposed approach is evaluted with the family of FGSM methods only  and not compared with other transferable targeted attacks.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a Staircase Sign Method (S^2M), which replaces the vanilla Sign Method to alleviate the problem of ignoring the gradient value difference. S^2M heuristically divides the gradient sign into several segments according to the values of the gradient units, and then assigns each segment with a staircase weight for better crafting adversarial perturbation. Experiments on ImageNet demonstrate the effectiveness of author proposed method.",
            "main_review": "##########################################################################\n\nPros: \n \t\n1. The paper is the first to point out the poor gradient estimation limitation of Sign Method in transfer-based attacks.\n\n2. The proposed S^2M alleviates the problem of deviating from the global optimal region. Also, the method is simple and effective and can be integrated into existing transfer-based attacks. However, it lacks experiments on stronger defense models.\n\n3. Experiments on the ImageNet dataset demonstrate the effectiveness of S^2M both on white-box and black-box attacks. \n\n##########################################################################\n\nCons: \n\n1. In Fig. 2, why the author assumes that the gradient of a black-box model ($g_B$) is close to staircase sign ($g_A$) than $g_A$? Actually, the $g_B$, as a black-box model, may be closer to any one, whether is staircase sign($g_A$) or $g_A$. In Fig. 3, the author only conducts experiment to show that the staircase sign($g_A$) is closes to $g_A$ than sign($g_A$). I suppose that the author should also conduct experiment to show that $g_B$ is closer to staircase sign($g_A$) than $g_A$. Otherwise, why not we directly use $g_A$ to generate the adversarial examples?\n\n2. In Proposition 1, the author utilizes the i.i.d. assumption of $G_t$. But the author didn’t claim what the distribution of it clearly. From the equations below, I guess that the author assumes that $G_t$ is subject to a uniform distribution. However, no matter from the perspective of time or space, this assumption does not hold.\nFrom the perspective of time, we all know that when the model is trained, the gradient will become flat when it is close to convergence. From the perspective of space, from my personal previous experimental experience, the gradient of different pixels on the whole image shows a Gaussian distribution as a whole, that is, pixels with small gradient amplitude account for the majority, and pixels with sharp gradient changes are fewer.\n\n3. The notations about $W$, $W_t$, $W^{i,j}_t$ are in mess. The author only clearly defines $W_t$ in Equ. 4. But there is no clear definition of $W$ and $W^{i,j}_t$. From the context, I infer that i, j represent different positions in the image. Based on this, I think that the $W_t$ in Equ. 4 should be $W^{i,j}_t$. \n\nThe $W$ in the line below Equ. 4 should also be $W^{i,j}_t$. \n\nThe $\\| \\sum_{t=0}^{T-1} \\alpha \\cdot sign(G) \\odot W \\|$ in the proof of Proposition 1 should be $\\|\\sum_{t=0}^{T-1} \\alpha \\cdot sign(G_t) \\odot W_t\\|$.\n\n4. The lack of experiments on stronger defense models. The three defense models used in the experiments in the paper is adversarial trained classifier, which is provided in year of 2018. However, in recent year, abundant stronger defense methods have been proposed. As most papers about adversarial attack have done in recent years, more experiments on the stronger defense model need to be conducted.\n\n5. In addition to proving the effectiveness of proposed method through experiments, more theoretical analysis about the reason why S^2M can improve the transferability of generated adversarial examples are desirable.\n\n#########################################################################\n\nSome typos: \n\n(1) Section 1, two lines above the summary of contribution, a iterative attack baseline -> an iterative attack baseline \n\n(2) In Equ. 2, the meaning of annotation of T didn’t explain\n\n(3) All Algorithm 14 -> Algorithm 1\n\n(4) Duplicate entries about the same publication of “Nesterov accelerated gradient and scale invariance for adversarial attacks” in References\n\n\n",
            "summary_of_the_review": "Overall, I vote for reject. The idea of S^2M is simple and effective. But I have doubts about the two assumptions proposed in the paper, more explanation and experimentation are needed. Also, the effectiveness of the method has not been fully proven, more experiments on the strong defense model need to be conducted. And I hope more theoretical analysis about the effectiveness of proposed method is desirable, not just through intuition and experimentation. Hopefully the authors can address my concern in the rebuttal period.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}