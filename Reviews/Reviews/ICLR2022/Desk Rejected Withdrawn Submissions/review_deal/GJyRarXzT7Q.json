{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper performs experiments over a range of language models (14-15 in total) to understand the impact of  factors like model size, training size, random seed, etc and their fairness-performance relationship. They train a text-sequence classifier on Jigsaw and HateXplain dataset and evaluate their fairness using Equalized Odds and plot it against their Balanced accuracy. The paper also investigates the impact of post-processing for bias mitigation.",
            "main_review": "* The paper is interesting and explores various factors wrt models. The results provide valuable information on what hurts the fairness and what can lead to an improvement majorly wrt model architecture, training size etc.  However I have some concerns on the evaluation of \"fairness\":\n1. The authors evaluate the fairness using only EO as the metric. While they have justified the use, I believe this doesn't provide us a complete picture. For e.g. I'm not sure if it's safe to say that size of the model doesn't matter for fairness based on results from Section 5.1. There have been studies explaining the peculiar behaviors that LMs demonstrate spurious correlations based on the dataset that they've been trained on and the variation in the performance on these two datasets can be associated to that. Maybe a more granularized evaluation of the kind of text that's leading to the scores and how that's changing wrt models and various other factors. Also, in the cases where there's an improvement in th EO, details on what improved and if there's a certain kind of text, protected group etc that contribute to these improvements would provide more insights\n2. The authors evaluate a text sequence classifier. I am wondering if the same pattern is followed in other tasks. For e.g. how does a LM perform in the base task of text generation? Authors can evaluate various models on datasets like Real Toxicity Prompt[1] and understand if a similar pattern follows,\n* Results in fig.3 are really interesting. Especially for Jigsaw dataset where the accuracy is almost stable across all sizes but the balanced accuracy does show some variation with equalized odds varying alongside. This presents a  strong case to the argument that evaluating merely accuracy isn't enough (especially for larger datasets). However from the figure it seems that there is indeed some relation (and inversely proportional kind) between the balanced accuracy and EO in the case of Jigsaw for DistilBERT and ELECTRA-large. Do the authors have experiments to investigate this?\n* There are minor concerns wrt presentation:\n1. The authors explain the metrics like EO, balanced accuracy etc in the appendix. Having it in the main text would improve readability, Information like \"lower EO is better\" (Section 5.1) can also be provided in the captions of the figures.\n2. Section 5: \"We report on our study of the performance and fairness characteristics \" can be rephrased to \"In our study, we report the performance...\"\n3. The use a \"more than a dozen LMs\" feels redundant. The authors can instead specify the number of models (14-15) and that about their variations of size to let the reader know what they should expect.\n4. Section 4: \"While this method is effective in enforcing EO, one limitation is that it does not offer a trade-off between minimizing the deviation from EO and reducing the loss in accuracy.\" -> I'm not sure I understand. Do the authors mean that minimizing EO hampers the accuracy?\n\nIn summary, I find the paper interesting but believe that the authors can do a detailed study to provide more insightful results and information that can help this research area.\n\nEdit: adding the reference to RTP dataset\n[1]: Samuel Gehman and Suchin Gururangan and Maarten Sap and Yejin Choi and Noah A. Smith, \"RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models\"\n",
            "summary_of_the_review": "Interesting but needs more detailed study",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors focus on analyzing the impact of pre-trained language models on fairness measures for toxic text classification. The authors experiment with existing methods and models for the above task and provide some recommendations.",
            "main_review": "The authors focus on analyzing the impact of pre-trained language models on fairness measures. While most works are geared for improving downstream tasks on standard metrics like accuracy, the societal impact of such large models has been relatively understudied. In this work, the authors focus on a specific task, namely, toxic text classification. \n\nThe authors first focus on characterization of various factors impacting fairness evaluation including the impact of model size, random seeds and data size. Overall, the authors do not find any explicit correlation of model size and architecture with fairness. They attribute the performance variation to be largely an artefact of the task and model combination. Interestingly, they observe that increasing the amount of training data can improve the fairness measure even though the standard accuracy measure gets saturated at some point. Finally, the authors use some existing post-processing techniques for mitigation with minimal degradation in task-specific performance.\n\nOverall, the paper is well-written and easy-to-follow. The authors leverage existing methods and models for an evaluation study with no methodological contribution. The primary finding of the work is that there is less variation on task-specific model performance with different configurations, while there is wider variation in fairness measures. However, some of these observations are dataset-specific. The authors do experiment with only two datasets where they find the impact of data size on improving fairness to be inconsistent across the two settings. Therefore, a recommendation is to extend this study to more datasets to draw some convincing conclusions.\n\nFrom the mitigation perspective, the authors experiment with existing methods and show that the techniques work and should be used, which is hardly surprising.\n\nThe authors also question the validity of the ground-truth in their study as they highlight the noise in the annotations. While the authors claim \"As a consequence, while we expect the trends shown in this paper to hold, the actual absolute numbers may vary with datasets.\", there is no justification of this claim without quantifying the degree of noise in the data.\n",
            "summary_of_the_review": "The authors present an evaluation study experimenting with existing methods and models. The work does not present any methodological contribution. The authors do not find any strong trends on the impact of various factors on fairness evaluation. The experimental observations are somewhat inconsistent across the two datasets. The authors experiment with existing mitigation techniques to show that they work and recommend the community to use them which is hardly surprising.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Authors present an empirical study of how different pre-trained language models perform in terms of fairness on two toxic text classification tasks. For this study, authors study models of various sizes (distilled as well as non-distilled models) with fairness measured at coarse-grained level (religion, race, and gender). Based on the empirical results, authors argue that model size does not have a strong correlation with fairness, and hence, it's difficult to make a generic statement about model size and fairness on a given task. In my knowledge, it’s one of the large scale studies to understand the relation between model size and fairness on toxicity classification tasks. ",
            "main_review": "\n\n### Strengths:\n- While the paper does not introduce any new technique to measure or mitigate bias, the paper presents multiple insights on the relation between fairness and model size, which might be useful to the NLP community. \n- Given that pre-training is an expansive and time-consuming process, authors explore the effectiveness of post-processing bias mitigation techniques, and empirically show that it’s an relatively easy solution to enhance fairness of fine-tuned LMs.\n- Paper is very well written. a) All experiments, implementation, and results are described in great detail. b) Paper cites relevant research papers. c) All charts are well documented and are easy to read.      \n\n### Weaknesses:\n\n- **What potential harm(s) are you trying to measure?:** I might be missing something obvious here but I don’t understand the rationale behind creating `protected` vs `unprotected` groups at coarse-grained level. *Equalizing the odds means matching the true positive rates and false positive rates for different values of the protected attribute $A \\in {0,1}$. This means that we are enforcing equality among individuals ($A=0, A=1$) who reach similar outcomes*. According to your paper, $A$ can be defined as \"gender info\" for protected group and \"No gender info\" for unprotected group. In jigsaw dataset, your `protected` group for gender and sexual orientation consists of (bisexual, female, male, heterosexual, homosexual gay or lesbian, transgender, other gender, other sexual orientation) and `unprotected` group consists of all remaining examples where gender is missing/unknown. \n   - By this definition, the protected group consists of every human being so what does the unprotected group represent here? Could you explain how these two groups can be compared? Why should we enforce equality among these two groups? \n   - What does fairness comparison mean in this experiment ? Can you define what kind of potential harm(s) are you trying to measure here? \n   -  Further, given that your protected group consists of text with gender information for all possible genders including *other gender, other sexual orientation*, how are you ensuring that these two groups are not overlapping?\n\n- **The usefulness of this work:** One of the main motivation to study bias in toxicity classifiers is to ensure that such classifiers don’t disproportionately predict sentences associated with a protected group (say LGBTQ group) as toxic, which may further silence discussion around their issues. Since all experiments are done at a coarse-grained level, I have reservations about the usefulness of this study. If you repeat the same experiments on fine-trained categories (for example on bisexual, female, male, heterosexual), do you reach the same conclusion that model size has a poor correlation with fairness?  \n\n- **Unsubstantiated claims:** Authors say that existing literature claims that fairness variation can be explained by model size/compression. I don’t think authors are substantiating this by citing relevant NLP literature. Authors cite Bender et al., 2021 that warn against pre-training large capacity models on a large amount of unfiltered data from the internet, but they do not claim that increasing the model size always leads to increased bias. To substantiate that existing literature claim something, authors should cite relevant papers. In this case, they should cite papers studying the relation between the model size and fairness on NLP tasks.\n",
            "summary_of_the_review": "I have concerns related to the internal validity and thus usefulness of the presented work. As outlined in my weakness section, I don’t think that the authors’ definition of “protected” vs “unprotected” groups at a coarse-grained level provides a meaningful setup to study fairness. That’s why I am recommending a rejection. If authors provide evidence that their conclusions hold at a fine-grained level, I am willing to reconsider my decision. \n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper motivates the importance of studying group fairness measures, alongside task-specific accuracy measures by evaluating the performance of PTLMs on the task of toxic text classification using 2 datasets of different domains - Jigsaw & HateXplain. The authors report the performance of 14 PTLMs of various sizes on the task. The performance of these models are measured across balanced accuracy for the task and equalized odds for group fairness. Additionally, the authors study the influence of varying random seeds for model initialization, amount of training data, and model sizes for this problem. Further, they apply two existing post-processing approaches for bias mitigation (HPS, FST) on this task and report their observations. ",
            "main_review": "The paper is well-written with a clear explanation of the motivation behind the problem studied and the experiments conducted. The experimentation is thorough and verifies the findings reported. The accuracy-fairness relationship for toxic text classification from the POV of model sizes, amount of training data and random initialization is interesting, and can help inform the selection of PTLMs to use for this task. Selecting labeled datasets of varying domains is also beneficial since it demonstrates model behavior across these domains. \n\nHowever, the paper focuses on a very specific task for this study, and conducts experiments on 2 datasets alone. The generalizability of the paper could be improved by expanding on other tasks & datasets, even within the text classification setting. Further, the post-processing bias mitigation approaches employed are direct applications of prior work (which the authors state & cite), and more in-depth analyses & explanation of why these approaches behave differently across both datasets chosen would make this work stronger.\n\nHere are a few suggestions for improving the strength of the paper.\n1) Section 5.2: What is the intuition behind variation in random seed affecting fairness to a larger extent, when effects on balanced accuracy are not pronounced? Some insights would be helpful here.\n2) Section 5.4: In addition to Fig 6, could the authors include a comparison of FST, HPS and TPP across religion, race and gender for HateXplain (similar to Fig 5). That would provide a clearer picture of the efficacy of these bias mitigation techniques across both datasets.\n3) Section 5.4: The authors report on the difference in behavior of bias mitigation techniques across the 2 datasets, e.g. \"Unlike Jigsaw, the result of the bias mitigation techniques follow different trends....\". Here, the community would benefit from additional analyses/ablation studies to demonstrate why these techniques affect each of the datasets differently. Are these differences owing to data sizes, label distributions, type/structure of the underlying text, domain differences, etc.? \n4) Could the authors add a few (qualitative) examples of predictions from select models (perhaps 1 of each size category) for each dataset, both before & after applying bias mitigation? It would be interesting to observe how these manifest across both datasets for the same end task.\n5) Could this study (or a portion of the study) be expanded to other tasks, such as sentiment analysis (using the Equity Evaluation Corpus) as shown in https://saifmohammad.com/WebDocs/EEC/ethics-StarSem-final_with_appendix.pdf?\n6) Do the authors have insights on cases/tasks in which the bias mitigation techniques used in the paper would not perform well? \n7) Missed reference: Lohia et. al. also study post-processing for bias mitigation in the context of both individual and group fairness: https://arxiv.org/pdf/1812.06135.pdf \n\nFor the authors' benefit, linking a recent work (NeurIPS 2021) that has emerged on this topic: https://arxiv.org/pdf/2110.13796.pdf, for future  reference. (This is not to be mistaken as a missed citation by authors since this is a very recent publication.)",
            "summary_of_the_review": "The paper presents a detailed study of the behavior of PTLMs across task-specific accuracy as well as fairness measures on 2 datasets for toxic text classification. Further, the effect of existing bias mitigation techniques on this problem have been demonstrated. The problem is well-motivated and explained, and the experimentation is thorough.\n\nThe problem is interesting and important to look at, however, the significance and novelty of the contributions is in question. Bias mitigation in the form of post-processing has been studied earlier under the purview of both individual and group fairness: https://arxiv.org/pdf/1812.06135.pdf. The empirical study on model performances across accuracy and fairness needs more explanation and analyses, which would help future researchers working on the same task. Further, the generalizability of the study could be improved by expanding to other tasks and datasets (e.g. sentiment analysis: https://saifmohammad.com/WebDocs/EEC/ethics-StarSem-final_with_appendix.pdf, gendered pronoun resolution: https://www.cs.cmu.edu/~ytsvetko/papers/bias_in_bert.pdf). \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies the fairness/accuracy tradeoff for the task of Toxicity Classification on two common Toxicity datasets (HateXplain and Jigsaw Toxicity).  The authors emphasize the need for evaluation of fairness in downstream tasks along with measuring accuracy. To this end, they analyze 12 pre-trained models ranging from 12M parameters to 400M parameters. The analysis tries to answer the following questions with respect to fairness/accuracy.\n\n- What is the effect of the number of parameters/model size?\n- What is the effect of different random initializations?\n- What is the effect of dataset size?\n- How well do simple post-processing bias mitigation techniques work? (eg Fair Score Transformer (FST)\n\nThe main findings are\n\n- In some cases, fairness can improve even when downstream fine-tuning training accuracy plateaus\n- The variance in fairness is higher than the variance in model accuracy for different initialisations\n- Simple post-processing bias mitigation technique can be a robust approach to improve fairness\n\nAuthors also went on to implement a key improvement in the FST(Fair Score Transformer) technique and made it numerically stable and fast. \n",
            "main_review": "### Review\nThe paper presents a cautionary tale against just measuring accuracy. The effect of size of dataset and random seeds seems convincing. However it is hard to get crisp takeaways from the paper. The claims presented in the paper seem very limited and might only apply to the selected datasets. Even for the two datasets there is sometimes inconsistency in results between the two datasets which I think is not adequately addressed in the paper. The study on evaluating fairness of pre-trained language models(PLMs) by fine-tuning on the toxicity task doesn’t seem to account for the differences in fairness of these PLMs apriori (before fine-tuning). Different models might have different starting points for fairness. The technical contribution of the paper seems limited. My overall recommendation at this point is a “reject”. Though, I think the paper can benefit from further deep-diving into the reasons that might be contributing to the differences in fairness performance.\n   \nOverall, the paper is self-contained, easy to follow and well-written. Details for various choices of hyperparams, supporting methods and metrics have been described in the appendix. The findings are somewhat anticipated. The related work section provides a good comprehensive background.\n",
            "summary_of_the_review": "## Summary\n\n### Comparing final (accuracy, fairness) isn't a very convincing setup\nOver the course of the fine-tuning, the accuracy of the model would generally increase for the downstream task. It would be interesting to see how the fairness of the model trends over the course of the fine-tuning. In the current setup the authors report the final (accuracy, fairness) values for different models. However, different models used here have different architectures and different pre-training strategies in some cases. The number of epochs during fine-tuning might vary across the models as models might take different numbers of epochs to converge. What is the justification for choosing 1-3 epochs for all models?\n \nAdditionally, the early stopping might also be set on accuracy. Overall, the setup doesn't present a very convincing picture just looking at the final (fairness, accuracy) value. Furthermore, different models are trained on different training corpus, have different inductive biases and might have different starting points for fairness. There is no evidence presented in the paper that starting points for fairness across the board are more or less the same. One way to provide more insight here could be to plot fairness over the course of the training.\n \n### Are the differences an artifact of just the small size of the dataset?\nHateXplain is a smaller dataset as authors mention and some of the findings on Jigsaw aren't consistent with the findings on HateXplain. More commentary about how this difference affects the results could add to the weight of the paper.\n \n### Post-processing techniques for tabular data seem to work well - why?\nFor the bias mitigation techniques, the authors have used two post-processing techniques. The authors mention that both these techniques have shown success in mitigating bias on tabular data. What is your insight on why this might be the case? Would any generic post-processing mitigation technique work equally well?\n\n\n\n## Misc\nPage 17 A.4\nrages => averages ?\n \nPage 9 Figure 6\nxseveral => several\n \nPlease re-consider removing the term \"model compression\" from the abstract as the paper does not focus on any aspects related to compression, pruning etc.\n \nGood job on improving the existing implementation. In the spirit of reproducibility, have you tried reaching out to the authors of FST to see if they can make their implementation available publicly?\n \nFor the calibration technique, you use a logistic regression on the softmax output. Did you try some generic post-processing calibration technique which does not require training? How would you justify your choice?\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}