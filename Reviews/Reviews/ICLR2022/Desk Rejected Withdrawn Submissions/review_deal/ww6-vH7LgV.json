{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper provides a benchmarking suite for assessing uncertainty quantification methods in machine learning, focusing in particular on image-to-image translation (i.e. image generation) tasks applied to pairwise cell-imaging datasets. It also presents an ensemble method, FastEnsemble, which aims to allow for faster training in comparison with other ensemble methods. An empirical comparison of FastEnsemble and other uncertainty quantification methods are shown on the benchmarking suite.",
            "main_review": "This paper provides a new benchmarking suite for image-to-image prediction, consisting of three out-of-distribution/distribution-shift datasets, performs a comparison of uncertainty quantification methods on these datasets, and presents FastEnsemble, a new ensemble method for uncertainty quantification.\n\nI think a primary strength of this paper is that it provides concrete datasets and metrics for uncertainty quantification in the image-to-image prediction setting. However, my main concerns about this paper are the following:\n- I feel that this paper at places lacks clarity (e.g. in details/descriptions of the benchmark datasets), and that the writing seems somewhat sloppy or rushed (see last comment below).\n- The methods presented in the paper have minimal technical novelty, and I am not convinced of their performance benefits over baselines.\n- Towards the end, the paper seems to shift to an experimental comparison of ensembles (only) on image classification on standard image datasets; this both seems quite similar to previous work, and also is an odd shift given the motivation and earlier goals of the paper.\n\nI give some specific comments and questions below:\n\n**Clarity in description of the out-of-distribution benchmark:**\nI feel that the description of the out-of-distribution benchmark is lacking some clarity.\n\nFirst, note that the initial training dataset (MSC-Clean) is described as a set of image pairs (an image for input and an image for output). In the MSC-LNCaP dataset (the second out-of-distribution benchmark dataset), the authors state that this “dataset is artificially created by mixing the images of MSC cells … with LNCaP cells”. However, the details here are not clear. Namely, were patches added directly to images (i.e. superimposed on top of images)? Were these patches applied to both the input and output images, or only the input images?\n\nIn the MSC-Impurities dataset (the first out-of-distribution benchmark dataset) the authors describe that they “included three different types of image artifacts”— however it is not clear to me exactly how this is done. Are additional image *pairs* added (i.e. both the input and the output)? Are the image artifacts present in both the input and output images?\n\n**U-net model:**\nIt appears that all image-to-image translation results are shown for a single model, which in this case is a U-net model. This is briefly mentioned in Section 3.1 during a description of the datasets, but the model is not called out or described. However, I feel that this is an important detail to make clear, as the uncertainty quantification results may be sensitive to which model class is being used.\n\n**Ensemble performance:**\nIn describing Figure 2, the authors state: “Our experiment further indicates that existing fast ensemble methods (BatchEnsemble, Snapshot Ensemble) cannot close the gap concerning OOD robustness”. However, from Figure 2 it appears that BatchEnsemble and Snapshot do nearly as well as Ensemble. Furthermore, when comparing against FastEnsemble (proposed by the authors), it seems that FastEnsemble also does not perform as well as the naive ensemble, and thus also “cannot close the gap concerning OOD robustness”. So it would be great if this could be clarified or explained. Namely, is the gap in performance between the BatchEnsemble, FastEnsemble, and naive ensemble meaningful?\n\n**Writing quality:**\nThere are a number of places in this paper that seem to have rushed or sloppy writing. I have copied some examples here from throughout the paper:\n- Intro: “​​usually a new class that is not appeared in training”\n- Intro: “nor systematical studies”\n- Intro: “patchs of the image”\n- Intro: “mesenchyaml stromal cells Imboden et al. (2021)”\n- Intro: “Ideally, such mechanism needs to: ... independent of modeling details, can work even for black-box models.”\n- Intro: “Predictive uncertainty also alarms the human-in-the-loop (HITL) machine learning paradigm”\n- Related work: “are naturally becoming a Bayesian neural network”\n- Related work: “This method approximate the posterior by”\n- Section 3: “mainly used the same dataset that was tested and published in our previous study (Imboden etal)”\n- Section 3: “To benchmark the out-of-distortion detection...”\n- Section 3: “is the one to train the U-Net model to be experimented later”\n- Section 3: “read patch boxes in Figure 3”\n- Section 4: “...despite the Bayesian methods being more theoretically principled. include a problem statement We hypothesize that the…”\n- Section 4: “by the recent findings mode connectivity of local minimum Garipov et al. (2018)”\n- Section 4: “our FastEnsemblesurpasses”\n- Conclusion: “this work presents a comprehensive benchmarking results”\n- Abstract: “... given the variety of learning objectives, data modalities, types of data corruption.”",
            "summary_of_the_review": "I think that creation of a benchmarking suite, and accompanying datasets, for uncertainty quantification in the image-to-image prediction setting is a useful goal. However, I feel that this paper contains rushed writing, methodology with limited technical novelty, and that some of the experimental setups have significant overlap with previous work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes assessments of confidence and uncertainty estimation approaches under distribution shifts with particular attention to applications in biomedical imaging. An ensemble of uncertainty estimators is subsequently proposed to potentially improve the inference speed vs quality trade-offs\n\n",
            "main_review": "There is no doubt that the gamut of uncertainty estimation literature has often focussed on well-curated datasets and different issues that crop up in datasets representative of different applied settings have not been adequately accounted for. This paper introduces the question of expanding such analyses to data from a practical use case of biomedical image analysis, which is highly appreciated. While the problem domain has a high premium on uncertainty quantification and ensemble methods for fast inference can add value to applications in biomedical image analysis, the methodological novelty of the paper seems to rest on the supposed ensemble approach, which I find insufficient. Additionally, the motivation behind the various datasets used is unclear, as is the rationale behind choosing these particular datasets to represent the biomedical image analysis domain over other commonly used open source datasets from venues such as the MICCAI Grand Challenges etc. the sparsity introduction methods also need to be explained in greater detail, and overall the paper needs to be improved in terms of its clarity and style of presentation. While the problem being studied is no doubt important, I think there is some need to clarify and contextualise the novelty of the proposed approaches and improve the presentation of the content in a future version. ",
            "summary_of_the_review": "There is no doubt that the gamut of uncertainty estimation literature has often focussed on well-curated datasets and different issues that crop up in datasets representative of different applied settings have not been adequately accounted for. This paper introduces the question of expanding such analyses to data from a practical use case of biomedical image analysis, which is highly appreciated. While the problem domain has a high premium on uncertainty quantification and ensemble methods for fast inference can add value to applications in biomedical image analysis, the methodological novelty of the paper seems to rest on the supposed ensemble approach, which I find insufficient. Additionally, the motivation behind the various datasets used is unclear, as is the rationale behind choosing these particular datasets to represent the biomedical image analysis domain over other commonly used open source datasets from venues such as the MICCAI Grand Challenges etc. the sparsity introduction methods also need to be explained in greater detail, and overall the paper needs to be improved in terms of its clarity and style of presentation. While the problem being studied is no doubt important, I think there is some need to clarify and contextualise the novelty of the proposed approaches and improve the presentation of the content in a future version. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a method to quickly train ensemble models to improve prediction uncertainty. This first requires a seed model, and subsequent models are trained greedily to minimize loss and maximize distance with existing models. This outperforms most discussed baselines such as SGLD, SVI and MC-dropout, with a training budget not much higher than training a single model. A benchmark for image-to-image translation task is also introduced. ",
            "main_review": "Possible leak in identifying information about the authors: \"This work mainly used the same dataset that was tested and published in **our** previous study (Imboden etal)\".\n\n\n**Strengths**:\n\n- The method is simple and straightforward, and performs better than some sophisticated methods such as SVI, SGLD, and MC-dropout.\n- The method is efficient to compute and is only marginally slower than training a single model.\n- A benchmark on bio-medical applications is introduced. \n\n**Weaknesses**\n\n- Compared with the naive method the performance is still a bit low. The proposed method is difficult to integrate with hardware that is massively parallel, unlike the naive method.\n- Some relevant comparisons are missing; such as Fast Geometric Ensembling from 3 years ago. In CIFAR-10, the proposed method seems to perform less competitively compared with FGE when the budget is around 200 epochs. \n- I am not sure why this is called image-to-image translation or image generation; this could mean many things that are outside the scope of what the method discusses such as cases where there are no paired images or labels (CycleGAN), or there is no prediction task to be defined (unconditional image generation). Calling it \"semantic segmentation\" might be more appropriate.\n- I don't see the ROC curve for the proposed method, only the AUROC for FPR <= 0.2 on the benchmark. I am not sure if this is done deliberately or not.\n\n**Questions**:\n1. Are new ensemble models initialized at w_0 or randomly? I suspect this to be the former, but it is not explicitly stated in the algorithm.\n\n2. Why $L_1$ distance in particular?\n\n3. This method reminds me of Stein's variational gradient descent, while SVGD applies distance repelling for samples simultaneously, here it is applied in a greedy fashion.\n\n4. I am not sure if I get what the image-to-image translation in the benchmark is between.",
            "summary_of_the_review": "The paper introduces a simple and competitive ensembling method for uncertainty estimation, and a benchmark for semantic segmentation on biomedical data. There are some concerns regarding whether the method compares with baselines that are recent and competitive enough. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a benchmark study that compares six SOTA methods for estimating uncertainties in image-to-image translation tasks with a focus on cell imaging datasets. The paper introduces a fast ensemble method to speed up training time for ensemble-based methods for uncertainty quantification. This method relies on generating various independent models given an initial model with a minimal computational overhead compared to ensemble-based methods.",
            "main_review": "Strengths:\n\n- The paper focuses on the biomedical imaging domain where overconfident deterministic estimates can not be trusted blindly, out-of-distribution sample and domain shifts are the norm rather than an exception, and uncertainties can be nonuniform across the image. \n- Both out-of-distribution (local/image-patches perturbation) and distribution shifts (global perturbations) are considered.\n- Compared to existing evaluation frameworks (e.g. [1]), this paper provides a comparative study of 6 representative methods.\n- The proposed fast ensemble method provides a computationally efficient method for uncertainty estimate that balances training speed and estimation quality.  The method is inspired by the connectively of local minima for deep networks. It is simple to be incorporated in any model. \n\n\nWeaknesses:\n\n- The paper ignores recent evaluation frameworks for uncertainty quantification  (e.g. [1]).\n- The paper focuses on estimating epistemic uncertainties (this has to be stated early on in the paper). Aleatoric uncertainties are also relevant in biomedical imaging applications. \n- A single image-to-image translation model is considered (UNet). This study could be strengthened by considering multiple families of models/architectures.\n- Some experimental aspects are not clear. For instance, what do the bounding boxes represent? how were they determined? \n- The rationale behind the choice of L1-norm for the biased term is not articulated.\n\nTypos:\n- Page 5, \"include a problem statement\" in the last line should be removed.\n\n[1] Gustafsson, Fredrik K., Martin Danelljan, and Thomas B. Schon. \"Evaluating scalable bayesian deep learning methods for robust computer vision.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. 2020.",
            "summary_of_the_review": "This paper combines a comparative study of 6 SOTA methods and a new efficient ensemble-based method for uncertainty estimation in a relatively under-explored learning task; image-to-image translation. The paper considers both out-of-distribution and domain shift assessments. However, the paper does not provide a complete picture of uncertainties by only focusing on epistemic uncertainties. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}