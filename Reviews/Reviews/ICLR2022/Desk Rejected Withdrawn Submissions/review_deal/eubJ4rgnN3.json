{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper focusses on the problem of identifying OOD examples at test time. Building on previous work which shows that softmax/temperature-scaled prediction probabilities are lower for OOD than in-distribution samples, this paper proposes a model which is trained on both in-distrubution and OOD images with a loss function designed to suppress the probability of OOD samples. Thus, what has been empirically observed in past work is being explicitly ensured during training in this approach. For this, the paper proposes a modification of the SENet block, and a loss function. The paper reports results using MNIST, F-MNIST as in-dist, and EMNIST, KMNIST, not-MNIST, omniglot and a few synthetic datasets as OOD samples.",
            "main_review": "Strengths:\n\n1. The problem of identifying OOD samples is extremely important: every algorithm has its limits, and knowing when a model will fail lies at the heart of robust, deployable ML models.  \n\n2. Strong literature review: the related work and introduction covers the recent literature on OOD classification well. It is also well summarized and would help even non-expert reviewers understand the content.\n\nWeaknesses:\n\n1. One glaring, big problem with the setup is the use of OOD data during training. At the heart, the problem of OOD detection is hard because we do not have access to that data during training. All past works cited in this paper as well do not use OOD data at test time to the best of my knowledge. For instance, if we consider MNIST as in-distribution and KMNIST as OOD, then showing them both during training to discriminate whether a data point belongs to MNIST or to KMNIST is not a hard problem. The bigger problem is that KMNIST cannot be used during training, which is not true in this paper. Thus, the OOD samples are no longer OOD as they are used during training.\n\n2. Writing: The paper in its current form is not fit for publication as it has several grammatical errors which make it quite hard to understand and follow. \n\n3. Ambiguity: Several statements in the paper are ambiguous or claims that have not been supported by citations/discussion. For instance:\n\n\t- in the abstract the paper mentions \"not acceptable for AGI\", which is quite ambiguous---what does not acceptable mean here? why is it unacceptable to predict OOD samples from only in-dist sample labels?\n\t- \"Humans can inherently identify what is unknown\": What does unknown refer to here?\n\t- \"reliability and accuracy of detecting OOD class samples were saturated\": Is there reason to believe that accuracy has saturated using approaches mentioned there and that they should not be pursued further? If so, what is the reason for this claim?\n\n4. Figures: The figure legends and captions are too small to be legible in both Figures 1,3,5,6 and 7.\n",
            "summary_of_the_review": "The question is interesting, but there is a glaring flaw in the experiment design: use of OOD samples during training, which means they are no longer OOD at all. Furthermore, the paper writing and figures are not mature enough for publication.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper aims to solve the problem of out of distribution (OOD) classification. Based on the observation that CNN filters can not learn spatial relations between channels, they propose a modified SEnet block to learn it. To focus more on in-class samples, they uniformly normalize the loss of OOD samples. Some visualization experiments are conducted to show the features of the samples. The main contributions of this paper are the SENet and the new loss that normalize the loss of OOD samples.\n",
            "main_review": "1. This paper tries to learn some channel-wise relationships to improve the OOD classification performance, which is interesting.\n\n2. By doing experiments on datasets like MNIST, this paper shows it is useful on these datasets.\n\n3. As the main contributions are the modified SEnet and a new loss function, this paper lacks innovation to a certain extent. As modified SEnet is a form of attention, maybe investigating different attention mechanisms can get a better framework. To suppress the influence of OOD samples, this paper uses a special loss function. There exist some technologies like anomaly detection that can deal with this situation. A new strategy to distinguish between IN-class and OOD class samples.\n\n4. This paper needs more experiments. First, it is better to conduct more experiments on much larger datasets like ImageNet to make the method more convincing. Second, some ablation studies should also be performed to illustrate the effectiveness of the method, especially the modified SEnet and the loss function with normalization.\n\n5. There are some problems with writing. For example, in the abstract, the word features are used twice in the sentence ‘Here in this paper … SEnet block’. The authors had better check the paper carefully to avoid grammar mistakes. \n",
            "summary_of_the_review": "This paper tries to solve the OOD classification problem using a modified SEnet block and a normalized loss function to suppress the influence of OOD samples. The novelty is quite limited. And more experiments on larger datasets need to be conducted to prove the effectiveness and generalization abilities.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a new neural network-based method for Out-of-Distribution (OOD) Detection. Based on the observation that the spatial features from the CNN filters cannot learn the global spatial relationship between channels, the authors propose to use a modified SEnet block that learns the low-level contextual features as well as the high-level spatial features at each layer. The experimental results show that the proposed method outperforms other existing methods on simple datasets. ",
            "main_review": "The proposed method assumes that both the in-distribution data and the OOD samples are available for training while the competing methods for OOD detection typically use only in-distribution data during training. In this case, the problem setting and complexity become substantially different. Please clarify this, and explain under what kind of real-world scenarios the used problem setting will be useful. \n\nAs the authors admit in section 3.5, the proposed approach is validated only on simple datasets having a fixed background. To show its superiority to state-of-the-art methods, more experimental validation on diverse datasets should be provided. \n\nThe authors claim that two modified convolution layers can capture local and global features simultaneously. However, the ablation studies are missing, and the experimental analysis and discussion on the specific modeling choices are not provided. This also applies to the choice of the loss function proposed in this study. More experimental validation and discussions are recommended overall. \n\nMinor comments: \n\n- In section 2, no reference is provided for \"information-theoretic techniques\".\n\n- The meaning of 'adaptive' (e.g., in the title of sec. 3) is not clearly explained \n\n- The text font sizes in Figures 1, 3, and 5 are too small. And in Figure 1, it is recommended to highlight which parts have been modified in the given architecture.\n\n- Equations (1) and (2) represent the same thing, but the notations are not well explained and actually confusing. And isn't it cross-entropy, not entropy, in the case of OOD samples in (1)? \n\n- This paper contains many typos and grammatical errors. Thorough proofreading would be required. Some examples are as follows: \n\n(Introduction) insection 3 → in section 3\n\n(section 1, last paragraph) insection 3 -> in section 3\n\n(section 2, Probabilistic approaches) line 2, approach.They -> approach. They \n\n(section 2, Probabilistic approaches) line 3, OOD samples Here → OOD samples. Here\n\n(section 2, Probabilistic approaches) line 4, recent work → Recent work\n\n(section 2, Reconstruction-oriented approaches) line 2, technique Here → technique. Here\n\n(section 2, Reconstruction-oriented approaches) line 3,model compress → model compresses\n\n(section 2, Reconstruction-oriented approaches) line 3-4, and Compare it → and compare it\n\n(section 3) line 1, we perform will perform → we perform\n\n(section 3.3) line 1, two loss function → two loss functions\n\n(Table 1) the best best performance → the best performance\n\n",
            "summary_of_the_review": "The proposed OOD detection method, which is based on the combination of existing (or slightly modified) models, is limited to the scehnario where OOD samples are available during training, and its performance is validated only on very simple datasets. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors proposed a method for detecting OOD samples, the method requires both OOD and IND samples in order to train.\nThe core of the method is learning the relationship between the channels, and it is done with a modified SEnet block.",
            "main_review": "The authors proposed a method which attempt to learn to global structural information, in addition to the already learned information, \nsuch as spatial, color and texture information.\n\nWhile the general notion of the paper is good, there are several areas in which the paper should improve.\n\nFirst of all - It is very limited, while the authors clearly mention that their method is limited to datasets which a constant background,\none may wonder to what domain can it be applied? as the work is clearly vision-oriented, and not general, this is a very severe limitation.\nWhere the authors could have applied it to several different datasets, such as one (or more) of the sketch datasets, cliparts, and the such,\nthe authors have demonstrated their method only on MNIST and FMNIST as IND.\n\nIn addition, the comparison is biased against the other methods, ODIN uses other OOD set for calibration of its hyperparams,\nMahalanobis utilize part of the OOD set (post-training) for calibrating a linear regressor, I am not sure about `confidence',\nas a cite is missing, and it can refer to multiple methods. However, the proposed method uses OOD samples as part of the training,\nmore relevant comparisons would be OE or Energy (Energy-based Out-of-distribution Detection, Liu et al, 2020).\n\nUnrelated the above, the figures in the paper need to be improved - \n6b for example, while I realized (after carefully examining the plot) that the blue AUC line is just under the top and left borders of the plot, this does not improve the readability of the paper.\nA different dataset where the AUC line is lower should be used - as the experiments in the paper are all on relatively easy datasets, this might require additional experiments.\n\nfig 7a has a redundant line above it and very small text.",
            "summary_of_the_review": "Where the idea behind the paper is good, further experiments are needed in order to justify it, in addition, the paper readability requires some minor improvements.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}