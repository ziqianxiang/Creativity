{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a representation learning framework by constructing a hypothetical causal graph corresponding to the (latent) components of data $\\mathbf{X}$ and the prediction variable $\\mathbf{Y}$. The main aim is to learn a representation such that this representation isolates the causal parents of $\\mathbf{Y}$ as per the proposed graph. An information theoretic minimax approach is taken to learn this representation and finite sample complexity bound of generalization ability is obtained.",
            "main_review": "The approach assumes two different hypothetical causal graphs, one with confounders and the other without. It should be discussed if its reasonable to expect that the data $\\mathbf{X}$ contains all the confounders, and there are no unmeasured confounders. If there are unmeasured confounders (which most likely to be without further knowledge of the data), then the provided analysis is not really applicable. At the very least, it should be mentioned that the analysis assumes causal sufficiency. It also needs to be mentioned if the considered datasets are causally sufficient.\n\nWhen confounder case is considered, the authors employ a form of mutual information decomposition in Eq. 5. How is the inequality obtained? A short explanation makes it easier to read.\n\nOne of the main concerns I have with the presented approach is that its not clear if the causal parents of the variable $\\mathbf{Y}$ is identifiable from observational data alone. There is no reference to this result nor theoretical result which discusses it. I am not really sure if the obtained representations are anywhere close to the true causal parents. For example, do you see the generalisation bound close to the ideal case (Eq. 15)? There are no experiments on simple synthetic dataset to verify this. In this regard, most of the analysis is only valid if the causal parents are identifiable.\n\nThere are many clarifications and typos which need to be fixed. \nHere are some of them:\n\n  -  What is $\\beta$ is Eq. 15?  \n\n  -  In Eq. 7. Isn’t $H(pa_{Y})$ a Lagrangian term with a different Lagrangian parameter? In that case, in the next equation where inequality is obtained, the entropy term does not necessarily cancel out. \n\n  -  In proof of Lemma 1.2, what is Eq. (23)? Why is $I(\\mathbf{X},Y)$ mentioned? In Eq. (22), are $\\mathbf{pa_y}$ and $\\mathbf{pa_y^{*}}$ different?Also Eq. (20) seems to concern with limit of the Kolmogorov Complexity but Eq. (23) invokes it with mutual information. \n\n  -  In addition, for Lemma 4, does $H(\\mathbf{X})$ corresponds to Shannon entropy but in the analysis with representation learning, are the authors still dealing with Shannon entropy or differential entropy? In the case of latter, are the results still valid?\n\n  - In equation 9, what does $\\mathbf{Y}(\\mathbf{Z}=\\mathbf{z})= y$ refer to? Until that equation, the text assumes $\\mathbf{Y}$ as a random variable.\n\n  -  It seems that the final proposed algorithm (or objective function which is computable) is only different from r-CVAE upto a single term. The empirical results seems to be similar too. In this light, r-CVAE needs to be discussed in greater detail (its a non-causal approach) and analyse why a causal view gives rise to this term and how does it influence the results. \n\n  -  Many of the inequalities obtained should be explained briefly and currently the explanations are missing.\n\n  -  There are many typos throughout the paper.\n\nThe authors consider confounded version of the hypothetical graph. But backdoor criterion is not used anywhere to adjust for these confounder variable in their theoretical analysis. In addition, it would be nice to clarify if the causal version of mutual information as given in (Nihat and Polani, 2008) is more suitable for the entire presented analysis instead of just mutual information. Note that confounders can be handled within the definition of (Nihat and Polani, 2008) assuming causal sufficiency and backdoor adjustment formula.\n\nThe exposition of the entire algorithm was a bit convoluted. A lot of machinery was introduced and explained just to arrive at equation (5) which is basically the Information bottleneck (IB). While it is interesting on its own that it gives an alternate derivation of IB apart from Rate Distortion theory, it does not require any causal formalism. One could assume it to be just a Bayesian network and still arrive at the same result. \n\nThe proposed algorithm is hence only to disentangle parents of $\\mathbf{Y}$ and descendants of $\\mathbf{Y}$. PNS is introduced and approximated. But some terms (like $I(\\mathbf{Z},Y)$) are dropped to make the objective simpler? How does dropping it affect the proposed final algorithm?\n\nReferences:\n\nAy, Nihat, and Daniel Polani. \"Information flows in causal networks.\" Advances in complex systems, 2008.",
            "summary_of_the_review": "While the proposed approach presents a lot of analysis on using Mutual information and hypothetical causal graph to propose a representation learning approach, the entire exposition is a bit convoluted and ridden with typos and major concerns (see above). In addition, it is not clear if the proposed contribution of finding causal parents is actually achieved. In this regard, I feel the paper is not ready for publication yet.  ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a novel quantification of the causal effect of the representation on the downstream task and learns robust causal representation from data from the perspective of mutual information. Besides, they depict a quantitative link between the amount of causal information and the sample complexity on downstream tasks. Comprehensive experiments demonstrate that the models trained on causal representations learned is robust under adversarial attacks and distribution shift. ",
            "main_review": "Reasons to accept:\n1.\tThe paper proposes a new way of learning robust representation from observational data, that identifies the parents' node pa_Y under the assumption that the observed variables consist of four latent representations, i.e., X = \\{pa_Y,dc_Y,nd_Y,nc\\}.\n\n2.\tLemma 1 and the objective function (Eq. 8) coincide with deep Information Bottleneck (IB). The author generalizes the IB setting into causal space. \n\n3.\tThe paper proposes a Counterfactual Vulnerability (CV) term to identify a mapping function that learns the representation most likely to satisfy the PNS condition. It reduces the search space of the model and makes it more likely to converge to the optimal solution. \n\n\nReasons to reject: \n1.\tTypos in Fig. 1: Decsendants → Descendants; Unify the symbols, pa_y,dc_y,nd_y → pa_Y,dc_Y,nd_Y. \n\n2.\tImportant details are missing or confusing, which hurts readability. What pa_Y,dc_Y,nd_Y refer to in the real-world scenarios, and what kind of scene will have such data? \n\n3.\tThe paper focuses on learning a mapping function that maps \\phi the X to be pa_Y, i.e., \\phi(X)=pa_Y and defines pa_Y as the minimal sufficient statistics of Y. One confusing aspect is that dc_Y are colliders in the causal effect estimation of \\phi(X)=pa_Y on Y, but the author has been trying to convince readers: pa_Y are confounders and confound the causal relationship between Y and dc_Y. This is true, but it seems even more disturbing for the article. \n\n4.\tAlthough dc_Y and pa_Y are both closely related to Y, the information Bottleneck theory is sufficient to identify the representation of \\phi(X)=pa_Y. The baselines are all before 2017. Comparative experiments with some more compact mutual information estimators and recent IB baselines may be necessary [Belghazi 2018] [Cheng 2020]. \n\n[Belghazi 2018] Belghazi, M. I., Baratin, A., Rajeshwar, S., Ozair, S., Bengio, Y., Hjelm, D., and Courville, A. Mutual information neural estimation. In ICML, 2018.\n\n[Cheng 2020] Cheng P, Hao W, Dai S, et al. Club: A contrastive log-ratio upper bound of mutual information[C]//International Conference on Machine Learning. PMLR, 2020: 1779-1788. \n",
            "summary_of_the_review": "The paper proposes a new way of learning robust representation from observational data, but some baselines are missing.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper tackles the problem of representation learning using only causal covariates for more robust performance. More specific, it designs a new loss function that is the lower bound for the Lagrangian form of a certain optimization problem whose solution is the causal covariates. Finite sample analysis characterizing the gain of using causal covariates for prediction rather than the whole set of covariates is provided,  offering theoretical justification for the good performance of the proposed method. Further, promising empirical results are shown.",
            "main_review": "Strengths: \n1. I think this paper tackles an interesting and important question, and providing a practical and sound solution. \n2. The empirical and theoretical justification seems convincing to me.\n\nWeaknesses:\nFrom my side, the biggest weakness lies in the quality of writing. It seems the paper is written in a hectic and there are many typos. And the maths part is a bit sloppy, some of them affects me understanding the method. For example, in definition 3, what does x \\in X mean? In Equation (6): Z is used as a dummy variable in first line, yet it seems to have a concrete definition Z=phi(X) according to the first line in Theorem 1 and the second line in Eq(6). It makes me confused. In Eq(13), seems we have Z, Z', \\bar Z', but the information in subscript of min seems incorrect. My suggestion is to carefully read through the paper, especially the math part and avoid any typos/vagueness. Besides, I think it would be useful to discuss a bit how the final loss function is used (e.g., what network structure is used? How the minimax loss is approximated/optimized?). Further, in derivation for the final loss, there are many intermediate inequalities. It would be curious for readers to know how tight each bound is: for example, the authors may consider adding discussions on when the equality is achieved, etc..",
            "summary_of_the_review": "I think this paper has its merits of tackling an important question and providing a practical solution. Yet I believe it is not fully ready for publication at ICLR for now mainly due to the writing quality.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors tackle learning causal representations for robust/generalizable deep learning. Using an assumed causal DAG, and the data processing inequality, they arrive at an objective to optimize that relies on information theoretic quantities. The tractable version of this objective is similar to that of information bottleneck models. Such an objective leads to a learned representation that might conflate causes (i.e., parents) of Y with non-descendant variables of Y. To address this issue, the authors design a minimax approach that also ensures that the learned representation (likely) satisfies the probability of necessary and sufficient cause of Y.",
            "main_review": "Strengths: \n- The authors tackle an important topic that is currently receiving a lot of attention in the ML literature. Learning causal/robust representations would be of interest to a wide audience.  \n- The authors provide rigorous finite sample analysis of their approach \n- The authors present compelling empirical results that show that their approach outperforms baselines. \n\nWeaknesses: \n\n1 - The clarity of the paper is lacking at best. Logical jumps made it a really slow read. Consistent typos make it hard to understand what the authors intend to say. Below is a list of all the typos I was able to catch. This is the biggest weakness of the paper, but it does not factor that much into my score. I trust that if the paper is accepted for publication, the authors would make a non-hasty pass through the writing. \n\n2- Other issues relating to clarity: \n\n2a - After theorem 1, the authors say \"when the underlying causal information is unavailable from observational data we use eq5 to identify causal information\". Can the authors clarify what they mean by that? By causal information, do you mean causal structure? And how do they use equation 5 to identify the causal structure?\n\n2b - what does the dashed line in figure 1(b) between ndy and dcy mean?\n\n3 - None of the results (tables 1, 2 and figure 2) report standard deviations/standard errors, which makes it really difficult to assess if CaRR outperforms the baselines. Would the authors be able to provide these? \n\n4- Can the authors explain how they arrived at a value of 0.8/-0.8 for b in their experiments section?\n\n5- Can the authors explain why their model seems to give better accuracy under high adversarial attacks as measured by the inf norm? It seems counterintuitive or odd.\n\nAddressing comments 2-5 would improve my score. \n\n---------- \nEditorial comments/typos:\n\n- 2nd paragraph in introduction, fourth line: taking use --> making use of\n- Preliminaries, counterfactual estimation: we should inference the posterior --> we should infer the posterior\n- method, section 4.1 fourth line: ascendent nodes --> parent nodes\n- method, section 4.1, 4 lines under equation 5: let Z = \\phi(X) denotes --> let Z = \\phi(X) denote\n- method, section 4.1, the line directly below equation 8: the different is --> the difference is\n- method, section 4.1 third line below equation 8: indtead --> instead\n- method, section 4.2, 2nd and 6th line, and in definition 4: necessity and sufficient --> necessary and sufficient\n- method, section 4.2 3rd line from the top of the page: if intervene representation z--> if we intervene on the representation z\n- experiments, implementation of our method: the first positive part eq 13 (1) is evaluate --> the first positive part eq 13 (1) is evaluated\n- experiments, implementation of our method: while b is a hyperparameter --> where b is a hyperparameter\n- experiments, metrics: calsulat --> ? I am not sure what this word is.\n- Experiments, result analysis: Alghough --> although\n\n",
            "summary_of_the_review": "The authors present an interesting method to learn the (causal) parents of a variable Y. Issues relating to clarity (point 2 in weaknesses), as well as some of the empirical results/choices (points 3-5 in weaknesses) make me vote for a borderline rejection. If the authors address these weaknesses, I am happy to adjust my score. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}