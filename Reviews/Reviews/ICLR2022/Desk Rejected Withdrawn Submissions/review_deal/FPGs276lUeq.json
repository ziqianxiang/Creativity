{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors present how to utilize conditional diffusion models to image-to-image translation tasks. The visual and quantitative results are impressive for the given tasks. They also demonstrate the generalization of diffusion models to different image-to-image translation tasks.",
            "main_review": "Strengths:\n+ A good engineering process of choosing different losses and network structures.\n+ A reasonable report of the generalization ability of conditional diffusion models.\n+ Impressive visual results of different image translation tasks.\n\nWeaknesses:\n- The paper and the methods are not novel. The authors just applied the existing models and loss functions to different tasks and obtain better results. Though impressive on multiple tasks, the paper lacks strong scientific motivations for further pushing the performance. \n- It's better to present domain translation tasks like (semantic maps to images etc.) to claim the advantages of diffusion models on image translation tasks. The current tasks are mostly image restoration tasks like the denoising diffusion model presented.\n- Some comparisons are not sufficient. Like for image inpainting, CoModGAN (ICLR'21) at least should be compared. Multiple SoTA methods are ignored by the authors. \n- The discussion of sample diversity is limited. The paper lacks the necessary analysis of the advantages of the chosen losses. ",
            "summary_of_the_review": "The overall paper looks like a nice engineering report, with multiple tricks and a selection of existing models. The reviewer cannot see the novelty or any newly-proposed ideas which make the method intuitively better than the other, and the discussion related to the diffusion model is also not sufficient enough. Some evaluations of the paper are not sufficient for some specific tasks, so we are not sure about the fairness of the model when the authors did not control the parameter sizes or other details. \n\nI sit on rejecting this paper and encourage the authors to resubmit it to another related conference that focuses more on the engineering process or applications. The current form may not be quite suitable for ICLR which enjoys the beauty of learning methods and theory. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper applies denoising diffusion models to various challenging image restoration tasks, with excellent results. There is nothing particularly surprising in the approach, but major improvements to the SOTA are by definition a big deal. The authors furthermore attempt to introduce a standardized quantitative evaluation procedure to replace task-specific metrics. Let's hope the community will buy into that.",
            "main_review": "The paper specifically addresses coloring, de-JPEG, inpainting, and \"outpainting\" (aka uncropping, aka image extension). These are challenging image restoration tasks that require global understanding, with the possible exception of de-JPEG, which is similarly difficult only in the extreme low-quality regime. \n\nDenoising diffusion models are essentially image-to-image translators to begin with, and the steps needed to coax them into these restoration tasks are rather small. This speaks to the strength and generality of these models. As such the paper is easy to follow, and the quantitative results are exceptionally strong across the board. Getting close to a chance level in human evaluation is rather unexpected. The qualitative results are convincing, for example the diversity in Fig.C5 is remarkable. With results like these I think the paper must be accepted, and I rate it very high, even though I wasn't particularly surprised by the technical contribution.\n\nAs for evaluation, the authors have a point. Image restoration has been very task specific in the past. And it's even worse than the paper says. For example, some subfields (super-resolution?) even use their own, non-standard PSNR computation. So, yeah, I'm strongly in favor of 1) standardized quantitative metrics 2) emphasizing quantitative over qualitative (because the latter is guaranteed to suffer from a variation in cherry picking effort). That said, I'm not a fan of ImageNet as it suffers from various quality issues, such as aliasing, low quality JPEGs, etc. Not sure if Larsson2016 somehow skated around that? In any case, we as a field must play with what we have and currently ImageNet is likely the best bet.\n\nInteresting to see that the more or less standard attention mechanism fails. With your global attention the results really as globally consistent. Fig.C2 penultimate row made me smile. \"Palette\" created a consistent 2-colored dress!\n\nMinor comments:\n- Sec 2: I didn't like this section. It's an unnecessary detour between intro and the main course. If this was my paper, I would try to cite most of these papers in context (intro, later sections) and remove all overlap from here. Ideally the entire section could be deleted.\n- Sec 3: satandard\n- Sec 5.1: \"... Coltran by more than 10%\". Actually, more than that, you mean 10 percentage points.\n- Table 1: Are the comparison methods from 2017 relevant? Seems like ancient history when it comes to GANs.\n- Sec 5.7: You say that several tasks are learned simultaneously, but it's not made explicit whether each training image is corrupted by exactly one randomly chosen task, or whether grayscaling, JPEG, inpainting can all be applied to the same image. \n- Why is the method called \"Palette\"? Because the same thing applies to multiple tasks? I'm not fond of then name, which has very specific meanings in computer graphics, but I guess it's too late to change it now.\n\n",
            "summary_of_the_review": "Denoising diffusion models give excellent results in various image restoration tasks.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper mainly studies the applicability of image-to-image diffusion models (image-to-image translation using conditional diffusion models) on four particular tasks: colorization, inpainting, uncropping (extrapolation/outpainting), and JPEG decompression, with an evaluation protocol based on ImageNet and Places2 for these four tasks in terms of several quantitative measures and human evaluation. The proposed Palette is built upon previous image-to-image diffusion models as well as architectures, with the investigation and analysis on the loss function (L1 vs. L2) and neural network (w/ and w/o self-attention layers). The main contribution of this work is the investigation of image-to-image diffusion models on four tasks in terms of an evaluation on ImageNet and Places2 datasets.",
            "main_review": "Strengths:\n - This study is a good application of conditional diffusion models on four particular image-to-image translation tasks: colorization, inpainting, uncropping (extrapolation/outpainting), and JPEG decompression, although these four tasks are actually not that typical in the study of image-to-image translation.\n - The paper is easy to follow.\n\nWeaknesses:\n - The key concern about this work is the limited novelty on both method and investigation. First, the proposed Palette has nothing new compared to the recent conditional diffusion models and their applications on computer vision tasks like super-resolution (refer to Section 3). Second, the investigation on loss functions (L1 vs. L2) and self-attention actually doesn't provide more insightful views with limited experiments and engineering tuning. Thus, the contribution of this work is limited.\n - The proposed evaluation protocol is another concern with limited novelty. Despite the evaluation combining several quantitative measures and human evaluation on ImageNet/Places2, the proposed protocol is incremental towards some particular tasks (e.g., it might not be suitable for some tasks considered in the classical work of image-to-image translation Pix2PIx and CycleGAN).\n - The compared methods are limited. For example, about inpainting and outpainting, the work like \"Rethinking image inpainting via a mutual encoder-decoder with feature equalizations, ECCV2020\", \" Wide-context semantic image extrapolation, CVPR2019\" and \"Spiral generative network for image extrapolation, ECCV2020\" should be considered.\n - For sample diversity, there are also some diverse image inpainting studies recently for discussion and comparison, such as, \"UCTGAN: Diverse Image Inpainting Based on Unsupervised Cross-Space Translation, CVPR2020\", \"FiNet: Compatible and Diverse Fashion Image Inpainting, ICCV2019\", etc.",
            "summary_of_the_review": "This paper is a good study on conditional diffusion models for image-to-image translation, however, this work doesn't provide more deeper insights on both conditional diffusion models and image-to-image translation, thus I think that the paper still needs more significant revision for the acceptance of ICLR.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper claims they give a general image-to-image translation framework using conditional diffusion models, validated in four tasks with the state-of-the-art result. The input of the given model contains both degraded image and its corresponding ground truth. It studies how l1 and l2 affect the generation diversity in the learning objective, along with the role of self-attention in diffusion models.",
            "main_review": "# Strengths\n* Fruitful experimental results including qualitative and quantitative evaluations. Visual results are quite impressive.\n\n# Weaknesses\n* The given diffusion model seems to address image editing tasks instead of image-to-image translation ones, e.g., change image contents from a clean one. For image-to-image translation, it transforms images from one domain to another one. It means images in the target domain are not exploitable for the model input. However, the proposed model does use the ground truth (or reference in the paper) in its input (L8P4). From this perspective, this paper makes wrong claims about model performance with unfair comparisons, since it employs the ground truth as additional inputs. If authors insist on conducting image-to-image translation, it would better to change the used reference to other clean images (different from the ground truth), and claim the task is reference-based image translation.\n* This paper is short of new designs, explorations, and motivations. The used model architecture is from existing works and it only changes its input by concatenating the ground truth. ",
            "summary_of_the_review": "I tend to reject this paper. I believe this paper misuses the task setting, leading to unconvincing experimental results. It needs a thorough rewriting of the target task and how to solve the mentioned task in the diffusion model framework with its own contributions.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO.",
                "Yes, Discrimination / bias / fairness concerns"
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}