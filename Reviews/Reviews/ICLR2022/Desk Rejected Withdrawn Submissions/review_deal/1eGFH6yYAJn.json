{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper identifies a major problem in current multi-module training procedures, modality laziness. It states that multi-module model learns easy-to-learn features from each modality and ignores hard-to-learn features, whereby learns insufficient representations of each modality. It demonstrates this problem comparing linear evaluation performance on the frozen encoders from uni-modality models and multi-modality models. The paper also theoretically formulates the problem of modality laziness. According to the above observation and formation, it proposes a multi-module model named UMT that incorporates knowledge distillation where the uni-module models act as teachers and force the multi-module model to learn the distilled features. The distillation losses and multi-module loss are trained together with different weights.  The paper also shows strong performance improvement using UMT compared with other multi-model training methods across a variety of tasks.",
            "main_review": "1. The related work of this paper is brief. A more detailed review on some multi-module training methods that also utilizes knowledge distillation can be good way to emphasize the novelty of the method of UMT. \n2. The same linear evaluation can be applied to multi-module models trained with UMT to show that the model laziness is alleviated. If the multi-module features trained with UMT for a particular modality has better linear evaluation performance than multi-module features trained with UMT for the same modality, it is a more direct evidence for the solving of modality laziness, instead of doing self-distillation. \n3. I am a bit confused about how training priority is implied in all of the theoretical proofs. It seems that it is assumed that a model learns feature $f_1$ before another feature $f_2$ if the predictive probability of $f_1$ is larger than that of $f_2$, ie. $p(f_1) > p(f_2)$. Even though the paper mentioned other researches reach the same conclusion, but no sure if the conclusion can be theoretical base to be used in the proof.\n4. There are a few typos in the submitted paper. Like in the proof of Theorem 1, the second line, the second \"trained in $x^{m_1}$\" should be $x^{m_2}$.",
            "summary_of_the_review": "The problem of module laziness has been a key problem of multi-module learning problems. The paper provides a general evaluation procedure to empirically demonstrate this problem. Moreover, it also provides theoretical formation on the problem. Later, a new method is proposed to solve the problem of module laziness. More ablation studies can better help us understand the power of UMT.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors identify a phenomenon called Modality Laziness in existing multi-modal training approaches. That is, a multi-modal model learns fewer features than uni-modal models. To resolve this problem, the authors propose Uni-Modal Teacher (UMT), which distills the pre-trained uni-modal features to the corresponding parts in multi-modal models while performing multi-modal joint training, as a pushing force to tackle the laziness problem. The proposed method UMT is evaluated on two tasks: audio-visual classification on the VGGSound dataset and RGB-Depth semantic segmentation on the NYU depth V2. Experimental results show advantages over an existing multi-modal fusion method Gradient Blending, and several simple baselines.\n\n",
            "main_review": "*Pros*\n- The proposed model is built upon an interesting empirical observation of Modality Laziness, where the modality with more hard-to-learn features, is significantly under-trained, even when the training of the multi-modal model has already converged.\n- The proposed model is tested widely on different types of multi-modal data such as audio, video data, and RGB and depth data. \n- The author proposed some theoretical insights to motivate the proposed multi-modal training approach. \n\n*Cons* \n- One main concern is that the observation of Modality Laziness is built upon a superficial late-fusion scheme on multi-modal data. It is unclear if such an observation will still hold in a more advanced model that adopts other training schemes such as mid-fusion, multi-layer fusion, or iterative fusion [a]. Therefore, more discussions are required on other training schemes.\n- Another concern is the insufficient comparison. The model is only compared to one related method (i.e. Gradient Blending) and a few simple baselines. It is therefore hard to convince the readers about its strength in comparison to recent more advanced multimodal learning models that are built upon (1) more advanced network architectures such as [a] and [b], or (2) more advanced multimodal learning objectives such as [c] and [d]. It is therefore suggested to add comparisons and discussions wrt to these methods. \n\n[a] Perceiver: General Perception with Iterative Attention, ICML2021\n\n[b] Attention bottlenecks for multimodal fusion, NeuRIPS 2021\n\n[c] Contrastive Multiview Coding, ECCV2020\n\n[d] Distilling Audio-Visual Knowledge by Compositional Contrastive Learning, CVPR 2020",
            "summary_of_the_review": "The paper discusses a new multi-modal learning problem named Modality Laziness and presents some interesting theoretical insights. The major concerns mainly lie in the fact that the proposed multi-modal learner is not sufficiently discussed and compared with more advanced approaches with better model architectures and objectives.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Although the paper did not provide any discussion regarding ethics, the overall idea and evaluation do not violate the ethics concerns as stated above.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper describes a method for training multi-modal classification models. Using data from multiple modalities  (e.g. sound and video) intuitively should improve classification performance, compared to using the data from a single modality. The main premise of the paper is that training feature extractors for each modality jointly with learning how to fuse these modalities for classification results in features that are easy to learn, but do not generalize (Figure 3). They propose to “distill” the features learned by uni-modal classifiers, training multi-modal classifier that keeps modality features close to the features learned by uni-modal classifiers (Figure 6, right). Assuming simplified generative model of the data the paper derives theorems that show 1\\ why uni-modal approaches outperform multi-modal approaches that learn features and classifiers jointly 2\\ why the “distillation” they propose outperforms uni-modal approaches. The paper evaluates the proposed method on two multi-modal classification (audio + video, RGB video + optical flow) and one semantic segmentation dataset (RGB + depth) where it outperforms the baselines and the recent paper (Gradient Blending, Wang eat al. 2020). \n",
            "main_review": "**Strengths**\n\n- The topic of multi-modal fusion and multi-modal classification is important and practically relevant and the paper proposes the method that demonstrates improvement over baseline approaches as well as recently published method (Gradient Blending, Wang et al. 2020). \n\n**Weaknesses**\n\n- A large portion of the paper discusses theoretical analysis that assumes simplified generative model for the data, which is divorced from the complexity of the problems presented in experimental section. However, although the simplified generative model is acknowledged (“We simplify the data generation process since the multi-modal learning process can be highly complex and hard to characterize“) the paper assumes that the results in experiments follow theory (“The theory meets the experimental results in Section 3.1 perfectly well, indicating that the assumptions and the models used in Theorem 1 indeed characterize the reality“). That wouldn’t be a problem if significant portion of the paper (3 pages) is devoted to theoretical analysis. \n\n- The paper considers late fusion for classification approaches, but mid-level feature fusion for segmentation. It is not clear what is the motivation for this choice. \n\n- However, the main room for improvement is in the clarity and the exposition of the approach. Specific points regarding clarity:\n\n  - The paper doesn’t mention that its goal is classification, but in the introduction mentions “we follow a standard protocol in self-supervised learning”. In first couple of pages it’s difficult to figure out what is the problem the paper attempts to address. \n  - Figure 6. in the supplementary material greatly improves understanding of the proposed approach, I would suggest to move it to 1st page of the paper.\n  - The paper introduces the notion of self-standing and paired features. It defines paired features as features that can only be trained in multi-modal setting (last paragraph of 1st page). However, the method proposed in the paper first trains uni-modal models and then distills the features learned by these models. The hypothesis is that the resulting model uses paired features to improve performance over uni-modal models. That is confusing, since paired features are derived from uni-modal features, albeit in multi-modal distillation scenario. Later (in the 1st paragraph of 5th page of the paper) it is mentioned that “uni-modal approaches can accidentally learn paired features”, which contradicts the definition of paired feature previously in the paper. I would suggest to clarify that part of the paper. \n  - The difference in performance between uni-modal and multi-modal approaches is attributed solely to paired features, and yet it is not clear how one can detect that paired features exist in the dataset, except by comparing the performance of uni-modal and multi-modal approach and concluding that the difference is due to existence of paired features. Can paired features be visualized, e.g. via Grad-CAM?\n  - The Figure 1 is not clear. The caption seems to explain that multi-modal training ignores “important” features that are “hard-to-learn”, but it does not explain what “important” means. I assume that important features are good predictors of class label. Therefore multi-modal training selects individual features that are good predictors of class label, but ignores pairs of features from different modalities that jointly predict better class label than each individually. The reasons for such behavior are hinted in Figure 3. It would be instructive to relate this phenomenon to “explaining away” or “shortcut reasoning”.\n  - The paragraph “While sometimes naively training a multi-modal model even cannot outperform the uni-modal model because of different overfitting rates, Gradient Blending (Wang et al., 2020) introduced adaptive loss weighting to overcome this problem. However, our experimental evidence shows Gradient Blending still suffers from Modality Laziness”, seems to suggest that the only reason why proposed method outperforms Gradient Blending is because modality laziness exists and it is not addressed (sufficiently) by gradient blending. From Figure 6. it’s clear that the proposed method also uses loss weighting, so it’s not clear what is conceptual difference with respect to Gradient Blending.\n  - Algorithm 1 is not clear: $f_m$ is mentioned in the “Input” part, but it’s not used anywhere in the algorithm. It is not clear which loss ($\\mathcal{L}_{\\text{distill}}$) is used to measure discrepancy between $f^{\\text{target}}_1$ and $f_1$, $f^{\\text{target}}_2$ and $f_2$ and which loss ($\\mathcal{L}_\\text{task}$) is used to measure the discrepancy between $y_\\text{hat}$ and $y$, nor that total loss is, I presume, $\\lambda_\\text{task} \\mathcal{L}_\\text{task}(y_\\text{hat}, y) + \\lambda^1_\\text{distill} \\mathcal{L}_\\text{distill}(f^{\\text{target}}_1, f_1) + \\lambda^2_\\text{distill} \\mathcal{L}_\\text{distill}(f^{\\text{target}}_2, f_2) $, nor it it clear how are hyper-parameters ($\\lambda_\\text{task}$, $\\lambda^1_\\text{distill}$ and $\\lambda^2_\\text{distill}$) estimated. Only in supplementary material (A.5) their values are mentioned, but not how they are obtained.\n  - “We call this phenomenon Modality Laziness, where the modality with more hard-to-learn features, is significantly under-trained, even when the training of the multi-modal model has already converged”. Without knowing which loss is used it’s difficult to understand what “converged” means in this context.\n  - “However, the model is not fully trained at point A, and the zero-training-error region (blue) stops us from further training. As a comparison, uni-modal pre-training approaches can break the barrier and achieve point B, which outperforms point A concerning the test error”. It is not clear what “fully trained” means? For example, in the case of hinge loss that can mean 0 training loss, but it’s not clear if hinge loss is used here. \n  - It would be good to clarify that $x^{m_1}$ and $x^{m_2}$ are views of the same object ($x$) represented by modalities $m_1$ and $m_2$, and that $f_i(x^{m_1})$ refers to $i$-th feature that represents modality $m_1$ of object $x$. If that is the case, then axes in Figure 3 are not clear. \n  - Training procedure is unclear: what does “the model learns features in descending order of predicting probability”? The number of features is pre-determined by the dimensionality of the feature vector, so I suppose that it means that the features that are best predictor for the class are the first to move from 0 (to which they are initialized).\n  - In Theorem 1(a) it’s not clear what “feature number” means, since the number of features is fixed. \n  - Unclear paragraph: “Firstly, distillation changes the learning priority since models prefer to learn the distilled features. During the analysis, we formulate such changes as a boosting on the surrogate predicting probability, which only changes the training priority but does not change the actual predicting ability.” Related to that, it’s not clear what “training priority” referred in Theorem 2. refers to.\n  - In Section 4.1 it is not clear what linear, MLP head and attention head refer to: there is no description not schematic overview of the function these heads perform.\n  - Unclear sentence “Auxiliary-CEloss means adding extra linear heads to receive the corresponding uni-modal features and then generating additional cross entropy losses.” Schematic overview of different options for multi-modal fusion would improve this section greatly. ",
            "summary_of_the_review": "Although the paper addresses an important problem and outperforms the baselines it is very difficult to read because of numerous unclear issues and design choices. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper theoretically analyzes one of the key issue in multimodal learning, called modality laziness, and proposes a framework based on distillation from unimodal models to tackle the laziness problem. Authors verify that the proposed idea can achieve better performance on various multi-modal datasets.",
            "main_review": "Strengths:\n- The paper is well written and easy to follow.\n- The basic idea of modality laziness and paired features in multimodal learning are interesting.\n- Competitive performance on multiple datasets and tasks. \n\nWeaknesses: While the idea of modality laziness/paired features have not been explored much in the literature, multimodal learning and how to improve its performance over standard late fusion baselines have been well studied (e.g., Gradient Blending and AdaMML [ICCV 2021]. There are several important weaknesses in the paper that need to be throughly addressed to improve the quality of the work (mainly missing experiments and comparisons).\n\n- The major weakness of the paper is on experimental validation. Why is only linear evaluation used for experiments? What is the effect of full finetuning? Does the claims still hold on finetuning? \n\n- What happens if we consider more than two modalities? How are the results of the proposed method comparable to gradient blending or other multimodal fusion baselines with more than two modalities, e.g., RGB+Audio+Flow?\n\n- How is the proposed method comparable to the recent work AdaMML: AdaMML: Adaptive Multi-Modal Learning for Efficient Video Recognition, ICCV 2021? How is the proposed method comparable to learnable weighted fusion that learns weights while combining the predictions of two modalities?\n\n- Why the proposed method is superior to Gradient Blending (GB)? GB shows much better performance than the naive joint training in their paper across many datasets and task combinations. However, the current paper shows worse performance of GB on two relatively small scale datasets. Did authors implement GB by themselves? Instead of comparing GB on these two small scale datasets, comparisons should be done on the benchmark datasets like Kinetics, Epic-Kitchens and AudioSet such that fair comparisons with GB and other methods can be done to justify the effectiveness of the proposed method. \n\n- What is the effect of optimizer in the modality laziness? Does the claims made in this paper are agnostic to the optimizer used to train the multimodal network? Experiments and discussions should be performed to verify this.\n\n- How is this method comparable to a context gating baseline (similar to collaborative experts where each modality is treated as an expert in Liu et al. Use what you have: Video retrieval using representations from collaborative experts, BMVC, 2019)? \n\n- Did authors initialize the multimodal model by the unimodal models before training? How is the simple initialization strategy comparable to the distillation approach? What is the effect of initialization using unimodality weights on modality laziness?\n\n- How is the proposed method comparable to a simple baseline based on DropPathWay? See the paper titled \"Audiovisual SlowFast Networks for Video Recognition\" for more details on such comparisons.\n\n- What is the effect of backbone on modality laziness and the proposed method? Experiments should be done using other network architectures besides ResNet used in the current experiments.",
            "summary_of_the_review": "The paper needs significant changes including new experiments on benchmark datasets (Kinetics, Audio Set, Epic-Kitchens) with more than two modality combinations (e.g., RGB+Audio+Flow), fair comparisons with state-of-the-art methods like GB, and discussions before being accepted to ICLR. The experiments are very limited and not convincing in the current version of the paper.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}