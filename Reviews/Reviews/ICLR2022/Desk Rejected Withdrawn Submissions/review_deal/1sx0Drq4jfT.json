{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This work introduces \"Meta-Surrogate Model\" to improve the transferability of adversarial examples, which optimize a Meta-Surrogate Model(MSM) by bi-level optimization procedure, w.r.t generate adversarial examples by MSM first, feed them to other source models(single or multiple), then optimize the weight of MSM to maximize the losses of source models. Then attack the victim model with adversarial examples generated by MSM.",
            "main_review": "The method is new to the best of my knowledge, and the paper is easy to follow. My concerns are as follows:\n\n1. It's interesting that when using a single source model, it could also outperform baselines by a large margin (Table 2). The optimization objective of MSM is to maximize cross-entropy loss of the source model. But when it comes to using a single source model, directly backpropagating the gradient through the source model seems a better choice to maximize the CE loss, like the I-FGSM does. Why does MSM could provide more transferable adversarial examples? I recommend explaining this phenomenon to make the method more solid. And it would be better to provide success rates of white-box attacks in Table 2.\n2. \"The 8 source models obtain 90.0%, 91.8%, 92.6%, 85.6%, 88.3%, 90.5%, 82.0%, and 81.8% accuracies on the test set, and the 8 target models obtain 80.0%, 82.5%, 76.4%, 86.4%, 86.9%, 88.9%, 90.5%, and 87.5% accuracies.\". It is a little strange that using better performance source models to attack worse target models.",
            "summary_of_the_review": "Overall, the paper needs more discussion about the concerns in the main review.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper further delves deeper into the question of how to improve the transferability of black-box adversarial examples. It optimizes parameters of a meta-surrogate model using a bi-level optimization problem such that the adversarial examples generated from it have maximum transferability to target models. ",
            "main_review": "*Strengths*\n\nProposed approach achieves a significantly better transfer rate than all baselines approaches. It also brings the work on bilevel optimization to adversarial example generation, which is an important contribution. \n\n*Weaknesses*\n\nI believe that the analyses of the proposed attack success can be further improved by accounting a more realistic set of target models . \n\nFirst the paper takes a traditional approach and measures the success of black-box attacks predominantly on non-robust networks. Since early work on adversarial attacks, a lot of research has been conducted in making robust training faster for commonly used datasets. Thus it's likely that the targeted model (at least on small datasets) will be robustly trained. True that authors do include some robust models (on both cifar10 and Imagenet), but there is a larger difference between the perturbation used to train the model (3/255) vs budget used to attack the models (16/255). \n\nAs the authors note, black-box attacks are much faster than query based attacks, thus most suitable for attacking large scale systems. However, the paper lacks experiment working with large scale real-world black-box systems, such cloud prediction APIs offered by common machine learning as a service systems. I encourage authors to test transferability of their attacks on these systems. \n\nI understand that sometimes we have to follow a common setup from previous works to do a fair comparison. However, in doing so we keep inheriting the same biases from earlier works. For example, one might get the wrong idea that black-box attacks are 97% effective (table-1) even on robustly trained models (which is untrue since attack accuracy on cifar-10 at epsilon 8/255 is ~50% [1]). This claim is fundamentally arising from the issue that there is a mismatch between training and test perturbation budgets.\n\nIt will be helpful if each table also includes results on white-box attacks. Since black-box attacks are strictly sub-optimal than white-box attacks, this upper bound will help to calibrate the results. \n\nIt is also unclear how the meta-surrogate model is used practically? Is it a pre-trained network, trained using natural/vanilla training? Are the parameters of the MSM optimized individually for every single image (equation 3), which, if true, is a big efficiency drawback. \n\n1. https://robustbench.github.io\n\n",
            "summary_of_the_review": "I find that the paper can be improved by accounting a better set of targeted models in demonstrating the success of the proposed approach. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "A complete black box adversarial attack thus far has not performed well as research shows. By complete black box, it means there is no information available about the target model, not even query based probing. Most SOTA uses a surrogate model to generate adversarial examples, where the surrogate model may, every likely, not in any way resemble the target model in practice. This paper claims that by adopting a meta training scheme, more effective black box attacks are achievable when compared to the SOTA.",
            "main_review": "Strengths:\n- As far as I know, the proposed meta training scheme is novel.\n- Writing is decent, but i think it can be simplified a bit more or some parts explain a bit better. More in \"weaknesses\".\n- I also appreciate that the authors provide larger dataset results, aka on imagenet, which is quite expensive to run for adversarial attack work.\n\nWeaknesses:\n- First off, iiuc, the MSM is trained by assuming that the dataset used by the target model is known (mentioned in first paragraph under \"Methodology\"). If this paper's prime motivation is that it is a truly black box attack, I find this assumption compromising. In practice, for black box attack, the dataset used by the target model is likely unknown, and in many cases, updated continuously.\n- On page 4, it says \"Section 3.3 will show more explanations ...\" which I don't find that to be the case. Section 3.3 in fact assume Eq. 4. Did I misunderstand the writing?\n- Can we properly define success rate in experiments? Even if it is well defined in literature or it is straightforward, I think we should define what it is.\n- Can we provide clarity how is MTA and IR combine? Unless I missed it, this was quickly gleaned over in one sentence. It helps me to understand the impact of IR In MTA.\n- The empirical results are ok but some concerns remain for me. On imagenet, MTA and its variants are not always the best performing, sometimes by a somewhat significant margin. Also, it is quite befuddling that the single source model results seem to be better than multiple source models, which is a bit counter-intuitive.",
            "summary_of_the_review": "Overall, I felt the paper has merits as the idea of meta learning to achieve generalized AE may be novel. Unless, I did not understand the paper fully, the design choices in Eq. 4 may be explained more clearly -- intuitively I can see why they help but a more formal ex-position may make the paper better. \n\nMy biggest issue is that the paper assume that the dataset used by the target model is known. This takes away a lot of the wow factor for me as I definitely appreciate how difficult it is to achieve real-world black box adversarial attacks. This paper could have been better it experiments have been conducted in the case where the target model's dataset is unknown or even continuously updating.\n\nThe next issue I have is that MTA and its variants do not always show to be the overwhelming winner in the benchmarks, especially on the imagenet results. I think that is ok, as I do appreciate the difficulty running adversarial attacks on imagenet but more insights on why will make the paper better.\n\nIf these concerns can be somewhat mitigated this time round, I will change my rating. However, even if not, this paper is still a good paper for the next conference once things are further beefed up.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the authors proposed a novel method of Meta-Surrogate Model (MSM) which can generate more transferable adversarial examples in the black-box adversarial attacks. The problem is formulated as a bi-level-like optimization problem which achieved via a differentiable attacker. Extensive experiments demonstrate its effectiveness in generating the transferable adversarial examples.",
            "main_review": " In general, the paper is clearly written and easy to follow but I still have several concerns:\n1.\tThe authors are expected to clarify their technical contributions, or what are the main advantages of the Meta-Surrogate Model compared with the ensemble-based adversarial attacks. \n2.\tThe authors are expected to make more comprehensive analysis on the computational cost of the algorithms especially when the number of the source models are increasing. Is it fair to compare the performance with the alternative methods since the authors actually use multiple source models and more gradient information? \n3.\tThe authors are expected to demonstrate the transferability especially when the network structures are significantly, e.g., the vision transformers. \n4.\tIn some cases, one may expect a better transferability by averaging or smoothing the gradient such as the DIM or TI-based methods. I think the proposed method in this paper is actually averaging the gradient on the source models which is similar the to previous works. \n",
            "summary_of_the_review": "I think the paper is clearly written but the authors are expected to clarify their technical contributions. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}