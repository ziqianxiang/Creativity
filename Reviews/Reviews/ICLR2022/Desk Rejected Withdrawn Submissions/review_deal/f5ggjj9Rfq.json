{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper, proposes a new adaptive optimization method called AALIG (adaptive ALI-G) for solving non-interpolated optimization problems. The proposed algorithm can be seen as an extension of two recent methods: (i) the Adaptive learning rates for Interpolation with Gradients (ALI-G) (Berrada et al., 2020), and Stochastic Polyak Step (SPS) (Loizou et al., 2021). In particular, the paper provides a variant of the ALI-G algorithm that does not require the optimal loss values a priori.\n\nThe paper focuses on the experimental evaluation of the new algorithm and it does not provide convergence guarantees for the proposed method.",
            "main_review": "The paper is well written and the idea is easy to follow. However, there are several issues with the presentation of the paper and the lack of deep understanding of the proposed approach (both in terms of theory and experimental evaluation).\n\n1)  The two closely related algorithms to the proposed approach are the ALI-G by Berrada et al., 2020, and the Stochastic Polyak Step (SPS) by Loizou et al., 2021. The paper mentions that both of these algorithms require interpolation to guarantee convergence and the knowledge of $\\ell_z(w^*)$. This is wrong, the SPS optimizer uses the $\\ell_z^*=\\inf \\ell_z$ in the update rule and as explained in Loizou et al., 2021 these quantities can be easily and efficiently pre-computed even in the non-interpolated case for several ML loss functions. Note also that the theoretical results of Loizou et al., 2021 do not require interpolation to guarantee convergence of the SPS. However when interpolation is satisfied the method convergences to the exact solution. \n\n2) Misleading statements:\n\nIn section 3, the authors mention ``ALI-G removes the need for a learning rate and performs comparably on many benchmarks (Berrada et al., 2020).\" The authors refer to ALI-G as the algorithm in equation (3) while in the experiment of Berrada et al., 2020 a momentum term used on top of the the update rule. The momentum variant is the algorithm that Berrada et al., 2020 show that has good performance on many benchmarks (without the momentum is not clear how this update rule behaves).\n\nIn Section 4, the authors mention: ``$\\ell_{z_t} (w_t)$ is already lower than its AOV $\\tilde{\\ell}_z^k$ we have achieved the approximate optimal value for this sample and no more progress is needed until the AOV is updated\". Why this is natural in the stochastic setting? Note that in the stochastic setting finding an approximate optimal value for a sample does not mean that we are close to the optimal solution of the general objective.\n\nFigure 1 is a nice illustration of what might happen in practical scenarios, but I cannot see why having such a figure in the main paper is helpful for the reader or why it is informative. It is not conclusive of what is happening in practice; it simply shows that all scenarios are possible. Even the authors mentioned that: ``Of course not every example in the training set satisfies one of these characterizations, such as those shown in the right half of figure 1.\"\n\n3) Missing theory: The main drawback of the paper is that it comes without any convergence guarantees. I can understand having an optimization algorithm that comes without convergence rates (showing how fast is the method) but the proposed algorithm come without convergence guarantees at all. Without any theoretical results we do not know if the algorithm converges even in simple scenarios. For example it is no clear if the method converges to a neighborhood or to the exact solution. Finally what is the quantity that converges to zero (is it the function suboptimality, the iterates, a different Lyapunov function, etc.)?\n\n4) On experiments: As the paper focuses only on experimental evaluation of the algorithm, one expects that a detailed comparison with the two closely related methods ALI-G and SPS will be the main focus. However the authors do not compare with these methods at all (even in the interpolated setting). What is exactly the advantage of the new AALIG compare to these algorithms in practical scenarios? \nAALIG does not require the knowledge of the optimal values a priori, but is this the only benefit? This is of course important but since there is no theory to support the new method one should be able to have strong experimental comparison in practical scenarios. \nTo this end, i expect that the proposed AALIG will not have any benefit in terms of performance (speed/generalization) compare to ALI-G and SPS (where one uses  $\\ell_z(w^*)=0$). Also as i mentioned above SPS is able to guarantee convergence to a neighborhood of the solution even in the non-interpolated setting which means that one should be able to have a comparison of the two methods even on some simple non-interpolated problems.",
            "summary_of_the_review": "I believe that the proposal of novel adaptive methods is necessary for modern machine learning applications and the extensions of ALI-G and SPS is extremely interesting for the optimization and machine learning communities. However, this paper have several issues that do not allow me to suggest acceptance. There are no theoretical guarantees of the convergence of the proposed algorithm even in simple settings and the numerical comparison is not adequate as there is no practical benefit in terms of performance of using this method compared to ALI-G or SPS.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The authors proposed a new single parameter optimization method AALI-G. In particular, the authors introduced a simple heuristic for the ALI-G algorithm to generalize it for problems that do not satisfy the interpolation property.",
            "main_review": "Pros:\n\n--The proposed idea is simple in implementation.\n\n--The authors provided an extensive evaluation on different datasets.\n\n--The paper is well written and easy to follow.\n\n\nCons:\n\n--No theoretical guarantees for proposed optimization method. Theoretical results could improve the overall value of the algorithm.\n\n--Based on experimental results (Table 1), the proposed method does not work well with data augmentation. In particular, the gap in accuracy between SGD and AALI-G increases when data augmentation is added. Therefore, the practical application of the proposed method is limited.\n\n--The authors use SGD with a step learning rate schedule (SGD-step) as a baseline. One can consider SGD with an exponentially decaying learning rate schedule (SGD-exp). This optimization approach has only two parameters: initial learning rate and decay. In practice, given the gap between SGD-step and AALI-G, SGD-exp can achieve similar performance to SGD-step having only one more hyper-parameter than AALI-G.\n\nQuestions and proposals:\n\n--The authors of ALI-G emphasize that their approach works well for a realistic setup where interpolation property is satisfied up to some constant. It would be interesting to examine ALI-G performance versus AALI-G on the benchmarks provided by the authors of the given paper.\n\n--The authors emphasize (section 4, the first paragraph) that AALI-G method is designed for problems where the interpolation property is not satisfied. While the authors covered a big range of different datasets, the number of tested architectures is limited. It would be interesting to see the performance of the AALI-G method for embedded devices.\n\n--Could you please explain why in algorithm 2 l^k_z = max(l_z^{k+1} - l^{k-1}_z, B) ?\n\n\nAdditional notes:\n\n--While the second hyper-parameter K is fixed for all experiments, in real scenarios it can be necessary to tune this parameter. \n\n--In [1] (Table 1) the gap between ADAM and SGD is much smaller than in the given paper. Despite the different settings, I have a strong suspicion that ADAM in this paper is not properly tuned.\n\n[1] Berrada, Leonard, Andrew Zisserman, and M. Pawan Kumar. \"Training neural networks for and by interpolation.\" International Conference on Machine Learning. PMLR, 2020.",
            "summary_of_the_review": "Overall, I recommend rejecting the paper. This is an empirical paper and the experimental results do not look convincing.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper \"Faking Interpolation Until You Make It\" tries to extend the success of recent optimization algorithms that utilize the knowledge of the minimal value of the loss function (at which interpolation of the training data is achieved) to the non-interpolating setting. It merely assumes that some lower bound for the loss (e.g. 0) is known, but not that the minimizer attains this loss value. Subsequently, an algorithm is developed that extends the recently proposed Interpolation with Gradients (ALI-G) technique to a setting where the lowest attainable loss value for each example is estimated in an online fashion. Numerical results demonstrate advantages of the proposed algorithm over competing line search algorithms for image classification and natural language processing tasks. ",
            "main_review": "The paper is well-written and easy to follow. The idea to extend ALI-G to a non-interpolating setting is interesting and the proposed heuristic for the online updates of the approximate optimal values is both, very simple and reasonable. In particular, it is promising to see how the proposed technique has some advantages over competitors like PAL and SLS for image classification and NLP tasks. In particular, I like that this work seems to paint a fair picture of the proposed algorithm including comparisons in Section 5.1 where the proposed approach does not outperform the others. \n\nOn the side of the weaknesses of this paper, the approach is a very simple extension of ALI-G, of which the authors discuss in section 6, that it can perform arbitrarily poorly on very simple (convex) problems. Thus, a major question for me is, if - for a technically simple algorithm with no theoretical analysis / guarantees - the numerical results are sufficiently convincing to merit a publication. Analyzing the numerical results in detail, I would argue that \n- PAL and partially also SLS are superior to the proposed technique in Figures 2 and 3.\n- The following experiments (image classification and NLP) show the generalization capabilities of the networks trained with different algorithms only, and do not discuss the quick minimization of the objective anymore. In the test/validation accuracy metrics, the proposed AALIG method is better than competing line search or adaptive gradient methods. At least for image classification, it is, however, outperformed by SGD, that finetuned two hyperparameters, the step size and regularization parameter, and thus is a little more cumbersome. Yet, at least the 4% difference on ImageNet using a ResNet-18 is significant, such that I do not believe that many researchers would trade the hyperparameter search for a possible reduction in accuracy. \n- In NLP, where Adam is known to work very well, it is encouraging to see that AALIG has a slight advantage in the conducted tests. \nYet, overall, it feels like a strong use case is partially missing. On the matrix factorization example (where techniques like adam perform poorly), the competing methods are better. In image classification, SGD is better and - given that a standard hyperparameter schedule from He et al. could be used - I am not sure how much the search for good hyperparameters really is a problem. In particular, it would have been good to turn to more exotic data sets where the standard parameters do not work well. NLP is the most promising field in terms of the experiments conducted in this work, but given that Adam still had a slight advantage in one out of two exemple data sets, I consider the numerical evidence for the advantages of AALIG too small.\n\n",
            "summary_of_the_review": "Due to the relatively simple extension of the existing ALI-G method and a lack of theoretical convergence analysis / guarantees, I have high expectations in the numerical results. As the latter do not fully convince me, or at least I do not see the perfect field of application for AALIG yet, I see this paper slightly on the negative side. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes an optimization method that is inspired by interpolation-based methods in an attempt to address problems where interpolation might not hold.  Since the optimal loss value is difficult to identify for many optimization problems, the goal of this work is to estimate a running \"optimal\" loss so that it can leverage the characteristic of ALI-G and SPS when that value is not provided. The method is Adaptive ALI-G which extends ALI-G by introducing approximate optimal values (AOV) to estimate the lower bound for each example in the dataset. Experiments were shown for matrix factorization, NLP and vision datasets like CIFAR and ImageNet.",
            "main_review": "Pros\n- The paper is easy to follow as the algorithm is simple and is explained almost entirely in Algorithm 2.\n- The problem is of high interest since training methods faster and generalizing better is extremely impactful for machine learning.\n- The results are promising on standard optimization datasets.\n\nCons\n\n- There is no theory as to whether the proposed method converges which is a big minus. Since this work is an experimental paper it should be testing the method in a lot more domains like segmentation and architectures like transformers.\n\n- No run-time is provided, I could imagine it would take a long time to update the approximate optimal loss for each example. \n\n- The memory requirement could be challenging since for each example we need to keep a scalar. For large datasets like ImageNet, it would be over 1 million. Wouldn't this be a concern? Further, if we used this method for domains such as semantic segmentation where the loss is computed for each pixel, don't we need to keep track for each pixel loss for each image in the dataset? that seems to be prohibitively costly.\n\n- It is not clear whether the proposed method is robust against different values of \"K\". That's an important ablation study to have.\n\n- Since the authors are using momentum for their method, shouldn't they compare against SPS with momentum for a fair comparison?\n\n- The authors are not reporting multiple runs with a standard deviation which is important for identifying how sensitive the method is with random initializations. \n\n- The authors only show the \"final\" validation accuracy in Table 2. It would be nice if we could see how the validation accuracy is changing across time and to see how fast it converges. \n\n- Is there a reason the authors didn't compare against [1]  which is an adaptive variation of SLS?\n\n[1] Vaswani et al. Adaptive Gradient Methods Converge Faster with Over-Parameterization",
            "summary_of_the_review": "I recommend a weak reject because there is no convergence theory and therefore more rigorous experiments are required to validate the efficacy of this optimization method. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}