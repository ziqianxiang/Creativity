{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper is a novel clustered FL method that is to use VAE for model clustering in server side. Specifically, the proposed method is motivated by the idea of data-agnostic distribution fusion across clients with non-IID data. The proposed FedDAF represents heterogeneous data distributions on different clients through the fusion of several virtual distribution components and develops a variational autoencoder method to derive the optimal parameters. Empirical studies show the proposed method has better convergence and training efficiency.",
            "main_review": "Strengths:\n1. It is a novel idea to integrate clustering and personalised FL to solve the non-IID problems.  Results in Fig.4 provides an intuitive illustration to validate FedDAF’s capability approximating the original data distribution.\n2. The FedDAF is data-agnostic which means it can overcome statistical heterogeneity of data from different clients while keeping the data private.\n3. Experiments on benchmark datasets show the proposed FedDAF achieves better convergence and training efficiency.\n\nWeaknesses:\n\n1. The problem formulation is unclear. The proposed method is practical but the theoretical support needs to be greatly improved.\n2. The proposed method has shown that the parameters of Batch Normalization (BN) layers of a local model will maintain critical information of the client’s local distribution. Several methods motivated by this observation have been proposed, such as methods in (https://arxiv.org/abs/2102.07623). As FedDAF’s property of data-agnostic also results from the variational inference over the parameters of BN layers, it should be compared with those BN layer-based methods?\n\n\nQuestions and suggestions:\n\n1. The formulation of Eq.4 cannot be directly inferred from Eq.2 and Eq.3. Please give a more detailed and straightforward explanation to support the formulation of Eq.4. For example, Eq.2 is to optimise w while Eq.4 is not to optimise w. Eq.3 is to calculate distribution D while Eq.4 is to use the same formulation to calculate w. The aggr(.) in E.q 2 is not an arbitrarily defined strategy whose definition is generated from an optimisation procedure, thus replacing the aggr(.) requiring a comprehensive analysis of its optimisation procedure.\n2. In line 4 of Algorithm 1, it will be better to explicitly separate the d_k from the w_k. The current statement looks like that line 3 and line 4 are irrelevant to each other. Btw, the comment in line 4 should be a typo: statical --> statistical\n3. In the first chapter of Page 2, “However, clustered federated learning may suffer from privacy leakage with shared data to cluster clients, and its performance relied on the cluster number which is a hyperparameter needed to be manually adjusted from task to task.” First, the paper’s proposed method still needs to upload the client’s model parameters or gradients, thus, the proposed method has no improvement on the privacy leakage issue. Second, the proposed model needs to decide the dimensionality of hidden vector z which is equivalent to the hyperparameter of cluster number in clustered FL method. In summary, the proposed method cannot solve the mentioned problems.",
            "summary_of_the_review": "The authors propose a novel distribution fusion model to handle the non-iid problem in federated learning and develop a VAE method to optimize the proposed model. However, a few statements are unclear or inconsistent.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a federated training model for tackling the non-IID problem. The paper first recomposes the latent overall dataset as the weighted sum of a number of unknown virtual data segments, where the local datasets can be fitted with certain probabilities. The authors argue that if these weights and probabilities can be found, then they can be used in the parameter aggregation for offering better training results. The authors propose to utilize the VAE and EM to match the distribution of reported means and standard deviations of the feature maps and their shifted variants, instead of finding the target variables. In each iteration, once these weights has been found, the master server will aggregate the local parameters with these weights.",
            "main_review": "Strength\n\n1. A data-agnostic distribution fusion approach is developed to tackle the data heterogeneity issue in federated learning.\n\n2. A VAE model is utilized to learn the optimal parameters in the distribution fusion.\n\n3. Extensive evaluation on a number of real datasets demonstrate the superior performance of the proposed approach for tackling the data heterogeneity issue.\n\nWeakness\n\n1. The additional VAE model has to be trained until convergence in each aggregation round of federated learning. This causes non-trivial computational overhead to the overall FL training process. The paper fails to provide the complexity analysis from either empirical or theoretical prosepectives.\n\n2. The paper assumes that the unknown variables follow a certain prior distribution without sufficient discussion to justify the choices. For instance, the prior of \\lambda_k is set to be a Beta-distribution, which is known to be computationally hard to compute. Thus, the authors rely on the Kumaraswamy's method to derive an approximate solution. In addition, an ablation study about the choice of priors may help understand the choice.\n\n3. The paper essentially replaces the distribution of local datasets with the distribution of means and standard deviations of the feature maps and their shifted variants while citing existing works to support the feasibility of such process [1, 2, 3, 4]. In this work, two distributions must be correlated closely with the mapping between them ideally being bijective. However, [1, 2, 3, 4] use normalization techniques to accelerate the training of neural networks and have no discussion about the relationship between two distributions. The authors should provide more detailed discussion about the relationship and error bound between the distance of the data distributions and the distance of the corresponding feature map distributions of the datasets. Namely, verify whether the close feature map distributions imply or guarantee the close data distributions.\n\n4. The authors fail to provide the convergence analysis for federated learning.\n\nReferences\n[1] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. \"Layer Normalization\". In: CoRR abs/1607.06450 (2016).\n[2] Sergey Ioffe and Christian Szegedy. \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\". In: Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015. Ed. by Francis R. Bach and David M. Blei. Vol. 37. JMLR Workshop and Conference Proceedings. JMLR.org, 2015, pp. 448–456.\n[3] Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky. \"Instance Normalization: The Missing Ingredient for Fast Stylization\". In: CoRR abs/1607.08022 (2016).\n[4] Yuxin Wu and Kaiming He. \"Group Normalization\". In: Int. J. Comput. Vis. 128.3 (2020), pp. 742–755.",
            "summary_of_the_review": "Overall, the well designed structure makes the workﬂow clear and easy to follow, but the further analysis and discussion are expected to clarify the contributions in the techniques as well as in the evaluation section.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a federated learning approach to address the non-IID case by learning to weight each local parameter differently in the averaging, using statistical information extracted from the model parameters. The approach is empirically shown to outperform several existing algorithms on a number of benchmark datasets.",
            "main_review": "I find the idea of extracting the statistical information from the parameters themselves interesting, even though this idea is limited to the cases where such information is available (e.g. from normalization layers). This I would consider the key contribution of the paper.\n\nI do have several concerns. Firstly, the algorithm still ends up performing weighted averaging of the updated local model parameters in each round. For such simple purpose, the proposed variational approach seems overly complicated, involving three separate weighting parameters (\\pi, b and c). I suppose this is because the authors insist on a mixture model that involves 0-1 assignment to the \"clusters\" (through c). I wonder whether a simplification of the model can lead to a simpler, easier-to-compute weighting scheme.\n\nOne simple sanity check of the weighting scheme is in the case of IID distribution but with different local dataset sizes. In this case one can argue for a weighted averaging scheme where the weight is proportional to the size of the local dataset. Since for privacy reason the size is unknown but can the proposed algorithm result in sensible weights in this basic case? \n\nTheoretically, it is unclear whether the proposed algorithm solves Eq.(2).\n\nIn terms of the experiments, I wonder whether the same qualitative trend is observed for model-dataset combinations other than the four shown in Figure 2 & 3. Also, are the hyperparameters of each compared model tuned properly?\n\nFor a method targeted at the non-IID scenarios, one would imagine that the test set should also be non-IID. In particular, the test set should be split using the same distribution as the training set for each separate local party. This is especially relevant when comparing the results to personalized models where each model is tailored toward the individual local distributions and not the global distributions. Also, I don't think learning a single global model is always the best approach when it comes to non-IID scenarios.\n\nThis is minor, but the label of the proposed model is FedDMM in all the figures but FedDAF in the main text.\n",
            "summary_of_the_review": "While the authors propose an interesting idea, the proposed approach does not seem like the right way to exercise this idea. I cannot recommend acceptance due to the many concerns stated above.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In the paper, the authors tackle the non-IID data sampling problem in the context of Federated Learning. This problem has some empirical detrimental effects on the convergence rate of the Federated Learning family of algorithms. They attempt to mitigate this detrimental effect via a novel aggregation strategy that strives to better take into account the global distribution of data across all the clients taking care not to send sensitive information from the clients to the server. In order to do that, they model the global distribution as a mixture of M virtual components, where each local distribution can be allocated to several different virtual components, and they use statistical information reported with the local models' parameters coupled with variational inference in order to learn the proper aggregation.\n",
            "main_review": "The solution proposed by the authors is able to effectively mitigate the detrimental effects of heterogeneity in the distributions associated with the participating clients beating several FL algorithms.\n\nCritical points: \nwhy do you use a gaussian prior to model b_k (the allocation weight of the k-th client)? Shouldn't be better suited a beta distribution since b_{km} is in (0,1)?\nWhy do you compare with clustered FL algorithm only in table 1 the comparison should have been done in the other experiments as well to see the learning curves which represent a thorough evaluation metric.\nThe proposed solution is always referred to as FedDAF, but the pictures do not report that name, I assumed is FedDMM instead, there is confusion here.",
            "summary_of_the_review": "The solution proposed by the authors is able to effectively mitigate the detrimental effects of heterogeneity in the distributions associated with the participating clients beating several FL algorithms.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}