{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper investigates the problem of node classification in graph-structured data and studies the effect of output consistency between nodes in graphs when only feedforward network is used to transform the node features (without the typical message passing in inference time). There are three consistency constraints studied, but only the first one (neighborhood consistency) is shaped by the graph topology. Experiments on Cora, Citeseer, Actor, Coauthor-CS, and Coauthor-Phy show that the three consistency constraints are useful, and indeed can be competitive against message passing neural networks, while it is faster during inference.\n",
            "main_review": "The result of the study is useful for practitioners in the area of collective classification in networked data. That is, in certain applications (e.g., where the nodes are homogeneous, and the neighborhood indeed is indicative of the label smoothness), it might not require full message passing to achieve good performance, provided that the loss functions are carefully designed to reflect the graph topology. However, this is where the conclusion should end.\n\nThe idea of output consistency in networked classification has a long history, started in the early 2000s with conditional random fields and relational Markov networks, well before the popularity of graph neural networks (see, for example refs in [1]). Here the consistency is captured in the label compatibility potential functions. Also in the 2000s, locality preserving techniques and manifold regularisation techniques were popular, many of which leverage the topological consistency in the output of the data points (which are nodes in the graph-structured data). Similar ideas have been widely investigated in the area of recommender systems (e.g., users who are similar in profile and tastes should rate the same movie similarly).\n\nMessage passing neural networks (MPNN) aim to gradually refine the node feature transformation using features of other nodes. This is  general idea, orthogonal to output consistency, and hence there is no need to \"replace\" and go \"beyond\" it as the authors want to claim. Also it is application to both node classification, graph classification and many other situations where interaction between nodes (or processes) must be accounted for. On the other hand, output consistency is typically limited to node classification (although we can make it work for graph classification as well, the topic the current paper doesn't investigate). The current celebrated Transformer is indeed a MPNN. I would have a trouble seeing how it can be replaced by just output consistency constraints!\n\nAlso please don't equate GCN with the entire class of MPNN.\n\nAs for Theorem 1, I don't see how Eq (6) can be approximation of Eq (7). With this \"approximation\", the entire system is a linear projection, hence has depth of 1, breaking the idea of deep neural network as a sequence of non-linear transformations.\n\nA small point, but could be important: GAT wasn't invented by (Kipf and Welling, 2016).\n\nReferences\n[1] Pham, Trang, et al. \"Column networks for collective classification.\" Thirty-first AAAI conference on artificial intelligence. 2017.",
            "summary_of_the_review": "While I would like to see empirical results of this kind of work because they are useful for practitioners, I would not go this far to claim to go \"beyond\" message passing neural networks, or even hinting for a new \"paradigm\", because the consistency ideas given structure are well-studied in the past 20 years.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors propose graph consistency learning (GCL) to train models on graphs using consistency measures. The aim is to train an MLP that can perform as well as or better than an GNN with message passing. They provide theoretical justification for the consistency constraints that they impose. Experiments on multiple datasets demonstrate the advantages of the proposed method. \n",
            "main_review": "The authors consider the semi supervised node classification framework in this paper. The GCL framework (eq 1) is an MLP that essentially looks like a GCN layer, but without message passing. They consider learning an attention weight for each pair of connected nodes. There are separate heads to ensure connected nodes have similar representations, and to predict the node label. If the labels of 2 nodes are the same, then the representations from the 2 heads are also encouraged to be similar.  They sample the positive edges to encourage mini batch training. The experimental results are also convincing. \n\nThe main contributions of the paper come from the consistency constraints.  Using the neighborhood to learn an MLP has been studied in graphMLP as well. The label consistency is new, and probably adds to the performance. \n\n",
            "summary_of_the_review": "I'm not fully convinced about the novelty of the approach. I'd like to see what happens when these additional consistency constraints are added in linkDIST and graphMLP. I'm also not sure why these methods have not been added in Tables 2 and beyond. Only Table 1 shows that the performance improves over these baselines. The theorem provided is rather straightforward, and follows from how the loss(es) are constructed by the authors. \n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a method to learn over graph-structured data that foregoes the message-passing scheme that typifies modern graph neural networks. In terms of architecture, the method uses only MLPs. Instead, the authors incorporate the knowledge of the graph structure through auxiliary loss functions, e.g., encouraging the learned representations to encode positive correlations between adjacent nodes. These loss functions are in addition to typical supervision, e.g., cross-entropy as a classification loss. The authors claim that GNNs suffer with limited labels, lack robustness, and are burdened in computation and memory. In contrast, they empirically claim their method alleviates these three issues. Finally, under restrictive assumptions, they show one of their loss functions is lower bounded by the distance between an embedding and the average of its neighbors.",
            "main_review": "### Strengths (+) & Weaknesses (–)\n\n(+) The paper is pleasant to read and is well-organized.\n\n(+) The authors tackle an important question: should we rely on the inductive bias of the message passing paradigm to correlate neighboring features, or are there performant alternatives?\n\n(–) The comparisons are flawed. In particular the 'label consistency' and 'class-center consistency' losses are disjoint with the GNN methodology, and a fairer comparison would be with GNNs that also use these two losses. Some questions are: do GNNs see a similar performance improvement (as in Figure 2) to the proposed method when using these two loss functions? Do GNNs, when equipped with these additional loss functions, also see improvements in results in Section 4.2? In particular, does robustness and generalization improve? \n\n(–) One of the claims is that GNNs suffer from high computation and memory burden due to keeping the full graph in a single batch (as opposed to mini-batching on sampled nodes). However, no results are shown for a dataset where using a GNN would result in an out-of-memory error. Furthermore, although inference is admittedly faster, I would expect that training time may be slow due the neighborhood consistency loss. How does training time of this method differ from training GNNs?\n\n(–) Table 1 is quite misleading in that the last 3 rows do have the adjacency matrix as available data, but differ in that they are only used in defining the loss functions. Perhaps \"Used in Forward Pass\" would be a more appropriate term, instead of \"Available Data\".\n\n(–) The paper is heavily lacking in referencing previous work, and in some sense not particularly novel. For example, the idea of contrastive learning has been prevalent long before its use in the graph representation learning community, e.g., see Chopra et al., 2005 (http://yann.lecun.com/exdb/publis/pdf/chopra-05.pdf). Further, the neighborhood consistency loss can be seen as a modification of the triplet loss, which is very well-established in the computer vision community, (e.g., see Hermans et al., 2017 https://arxiv.org/abs/1703.07737) and I believe no reference is made to it.\n\n(–) Even further, the motivation of Laplacian regularization (indeed, regularization is an 'auxiliary loss function') is quite similar to the neighborhood consistency loss, and has existed long before in the semi-supervised learning literature, e.g., used in the famous label spreading algorithm by Zhou et al., 2004 (https://proceedings.neurips.cc/paper/2003/file/87682805257e619d49b8e0dfdc14affa-Paper.pdf) Again, no reference or empirical comparison is made to Laplacian regularization. In fact, the GCN (Kipf & Welling, 2016 https://arxiv.org/pdf/1609.02907.pdf) was proposed as an alternative to Laplacian regularization. This is all to say that the field of work of incorporating constraints from the graph, in a means other than message-passing, has also existed but was sparsely referenced. \n\nLess important nitpicks:\n- In the abstract, \"explicitly involve the forward\" → \"explicitly involve the forward pass\" I believe?\n- Figure 2 is difficult to parse due to the inconsistent scales. \n",
            "summary_of_the_review": "The paper tackles an important question, namely whether or not we should rely on the inductive bias of the message passing paradigm to correlate neighboring features. However, the proposed auxiliary losses are already known, and are not compared fairly: how does the performance of GNNs change when using those loss functions that do not incorporate graph structure? Even further and equally importantly, little to no reference is made to existing work in the field.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a simple yet effective MLP-based method for semi-supervised node classification. The paper argus that three weaknesses of existing methods are 1) Weak generalization with severely limited labeled data, 2) Poor robustness to label noise and structure perturbation, and 3) High computation and memory burden for keeping the entire graph. To tackle each of the above limitations, the paper trains node embeddings with three consistency constraints: 1) neighborhood consistency, 2) label consistency, and 3) class-center consistency. Moreover, the paper proposes an efficient sample strategy for training efficiency. Finally, the paper makes connections of the proposed method and traditional message passing scheme of GNN.",
            "main_review": "Pros\n- The paper is well-written and easy to follow.\n- Thoretical analysis on connections between message passing and neighborhood consistence is interesting to see.\n\nCons\n\n- The paper lacks justifications on why GCL performs well with severely limited labeled data and noisy labels. It would be better to see ablation studies to show the contribution of each component among the three constraints.\n\n- What is the benefit from \"implicitly\" utilizing the adjacency information rather than explicitly using it? \n\n- Although [1] is still explicitly using the adjacency information, it is the most recent work in this line, and thus should be mentioned.\n\n[1] COMBINING LABEL PROPAGATION AND SIMPLE MODELS OUT-PERFORMS GRAPH NEURAL NETWORKS, ICLR21\n\n- It would have been better if various experiments such as those in Table 2,3 and 4 were performed on larger datasets, as Cora and Citeseer are small datasets. The robustness could be better demonstrated on larger graphs.\n\n\nMinor comments\n\t- \"explicitly involve the forward\" -> \"explicitly involve the forward propagation\"",
            "summary_of_the_review": "Overall, I think this paper would have its own merit for a publication, if the above comments are addressed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}