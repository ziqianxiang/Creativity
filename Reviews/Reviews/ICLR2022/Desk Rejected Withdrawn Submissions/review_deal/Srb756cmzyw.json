{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors proposed a framework to leverage descriptions for images for the WSOD task. The model are pre-trained with (image, caption) pairs, then it are trained on the COCO and VOC dataset, on which the labels are generated from captions. Experiments are conducted on MSCOCO and VOC dataset.",
            "main_review": "Strengths: The targeting problem is interesting and it is useful in practice.\n\n\nWeaknesses: Contributions are limited.\n\n",
            "summary_of_the_review": "To improve the quality of this paper, please consider the following concerns:\n\n1. The contributions are limited. There are no improvements on the backbone. And all the losses are not novel. It seems that the only novel contribution is the instance weighting step.\n\n2. How label are generated from captions should be described clearly. And how to handle the effect of synonyms.\n\n3. It seems that the paper is not finished. No conclusion section.\n\n4. Is there any overlap between pre-training and testing set?\n\n5. Explain the meaning of W_{cls,a}, W_{det,a} in Eq. (10).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper explored vision language pre-training for the task of weakly supervised object detection (WSOD).  The authors presented a method that learns visual and textual representations from image-text pairs by aligning region and token features. The learned representations were further transferred to initialize a model further trained for WSOD --- a variant of the widely adopted WSDDN model. The results are improved performance for WSOD on both COCO and PASCAL VOC datasets. \n\nThe key innovation of the model lies in the vision language pre-training for WSOD. Despite the recent success of vision language pre-training for image classification tasks, the benefits of these pre-training methods for region-based reasoning remains unclear.  Further, the authors also made minor contributions to the existing pre-training method (PixelBERT) and WSOD model (WSDNN). \n",
            "main_review": "Strength:\n* The idea of using vision language pre-training for WSOD is interesting.\n \n* Paper is reasonably well-written and technical details are clearly described.\n\n* Results on COCO and PASCAL VOC datasets are promising.\n\nWeakness:\n\n* Technical novelty is unfortunately limited. \n\nUsing vision language pre-training for object detection was previously discussed in Zareian et al. 2021. While I understand the modeling is totally different, this prior work diminishes the novelty of the key idea. \n\nThe pre-training largely follows Pixel BERT. The main difference seems to be the region-token alignment using contrastive loss. However, such an idea (with the same loss function) was previously explored in the context of weakly supervised visual grounding by Gupta et al. (see below). I am not convinced that part is novel.\n\nGupta, T., Vahdat, A., Chechik, G., Yang, X., Kautz, J., & Hoiem, D. Contrastive learning for weakly supervised phrase grounding. ECCV 2020.\n\nThe modifications to WSDNN (cross-modal discrimination and instance weighting) look interesting. Yet these are again a combination of existing techniques and are highly tailored for the setting of learning from captions for WSOD. The ablation studies suggested mixed signals for them. For example, with cross-model discrimination, the instance weighting does not seem to help. I am again not convinced that there is sufficient technical novelty here. \n\n* The text has a feeling of rushed writing. The conclusion section is missing from the paper. \n\n* The results are fine yet all methods look quite similar on COCO. The gap between the proposed method and CAP2DET (EM) is within 0.1% on COCO. It is not clear if the proposed method is significantly better than previous methods on a more challenging dataset (COCO). \n\n* Additional comments: an important baseline of recent vision-language pre-training methods. \nI feel obligated to point out that the very recent vision-language pre-training methods, such as CLIP, support zero-shot inference on a downstream recognition task. For example, one can take the CLIP model and the same set of region proposals used in the paper, extract visual features, and match them to the text embedding of target categories (with some thresholding for background regions). Such a model will be a strong baseline to compare against. \n\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... & Sutskever, I. Learning transferable visual models from natural language supervision. ICML 2021.\n",
            "summary_of_the_review": "This paper has an interesting idea. Yet the technical novelty is limited and the results are unsatisfactory. The writing could use some work (e.g., a proper conclusion session). I don't think the paper is quite ready in its current form and thus recommend to reject this paper. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies the problem of weakly supervised object detection (WSOD) using natural language supervision. The proposed model follows \"pretrain+finetune\" pipeline and investigates the ways of learning better visual representations for the downstream object detection task. To effectively and properly leverage the noisy image captions, detailed improvements are proposed, including learning explicit alignment between image regions and text tokens, cross-modal instance discrimination to avoid forgetting region-token alignment, novel weighting mechanisms for reducing the side effect of those captions with weak visual alignment. Emprical studies suggest that the proposed method has its unique advantage especially for transfer learning. ",
            "main_review": "Strengths:\n\n1. Proper techinical insight and solution. The proposed method pinpoints several detailed issues for WSOD such as the different coherences between the image and the caption for each image-caption instance. \n2. Reasonable and clear technical presentation and satisfactory paper writing.\n\nWeaknesses:\n\n1. Novelty concern: The problem setting is interesting. However, the proposed model looks incremental compared to existing algorithms. The cross-moal instance discrimination is actually a region-token alignment module, the same as Figure 1 (A). The instance detection is adopted as WSDDN. The instance weighting is new, but requires heavy engineering tuning (not so elegant).\n2. There are nonnegligible gap between the implemented baseline results and the reported ones for WSDDN. The original WSDDN paper reports a result of over 30% mAP (IoU>=0.5) in VOC2007 while the implemented one is only 17.7%. Could the authors give more explanation on what causes such a performance disparity?\n3. In Table 3 and Table 4, there are inconsistent model behavior for different model variants and different datasets (COCO vs VOC). It could be better if the authors provide further explanation.\n4. Regarding the instance weighting technique, Beta(CLUE) uses extra dataset (CLUE) to train a binary cross-modal coherence classifier while Beta(HESSEL) uses extra Wikipedia and British Library dataset to train a cocreteness scoring model. So, I may think that it is unfair to compare the proposed instance weighting technique with existing methods.\n\nTypos:\n1. Equation typo in Eq. (12).\n",
            "summary_of_the_review": "Overall, the novelty and the experimental quality of the submited manuscript might NOT reach the standard of ICLR. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper applies natural language supervision to lean visual representations specifically for the downstream tasks of weakly supervised object detection (WSOD). During pre-training authors propose to learn alignment between image regions and text captions as a dot product similarity score (called ALIGN) and contrastive learning between positive caption-image pairs vs. negative (called xID). Each idea of pre-training is found to improve the model's ability to learn downstream WSOD tasks.\nAuthors apply this idea on top of WSDDN(Bilen & Vedaldi, 2016) to show gains in performance when pre-trained on COCO and fine-tuned on VOC2007.",
            "main_review": "__Strengths__\n\n* [S1] This paper tackles a relevant and open problem of WSOD by raising relevant issues and is well-positioned in the current research.\n* [S2] Motivation is clear. Applying the idea of representation learning from caption-image pairs to WSOD seems like a reasonable idea.\n* [S3] Paper is well written.\n\n__Weaknesses__\n* [W1] Sparse baselines: no direct comparison to SOTA WSOD results from other papers.\n* [W2] The overall results (e.g. AP0.5 for VOC2007) seem much lower (e.g. ~23) than what I have seen in other WSOD papers (e.g. ~40 for COCO-pre-trained models in cap2det which this paper cites). Why such a big gap? Perhaps adding more comparable baselines (W1) can help.\n* [W3] Novelty is an incremental addition (of ideas from prior transformers and contrastive learning work) to a particular problem (WSOD).",
            "summary_of_the_review": "This is a well-written paper with solid motivation and reasonable ideas. The main issue is that the experiments do not support the claims well due to the lack of baselines and comparison to existing work.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}