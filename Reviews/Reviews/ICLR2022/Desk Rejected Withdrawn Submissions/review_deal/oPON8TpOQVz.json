{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper focuses on the task of online continual learning with noisy labels and argues that both diversity and purity of examples in the episodic memory is important in rehearsal-based continual learning methods. For resolving these problems, this paper proposes Chameleon Sampling, which consists of two main components: (1) the construction of episodic memory; (2) robust learning with episodic memory.\n\nFor the former, this paper provides a formal definition of diversity and purity. Based on this, this paper proposes a score measure for selecting samples to construct the memory. For the latter, this paper leverages the small-loss trick to divide the memory into clean and noisy subsets according to the posterior probability of GMMs. It further split the noisy subset into two subsets (re-labeling/unsupervised learning) according to predictive uncertainty measure.\n\nExperiments are conducted on CIFAR-10/100, Clothing 1M and WebVision datasets, in which continual learning methods including RSV and GBS, robust learning methods including SELFIE, Co-teaching and DivideMix are compared.",
            "main_review": "Strengths: \n1. Considering label noise takes a step further compared to the setup of blurry continual learning. \n2. Experiments on symmetric/asymmetric noise cases verify the effectiveness of Chameleon sampling compared to direct combination of existing continual learning and robust learning methods.\n\nWeakness:\n1. Some parts of this paper is hard to follow.\n    1.1 In Definition 1, the notations $f(x) \\in R^n$ and $e \\in f(x)$ seem weird, is f(x) a vector or a set? It seems that $e$ is a vector consisting of several entries of $f(x)$?\n    1.2 In Definition 1, the definition of $f_rel(x;j)$ is also confusing. Is $e$ a vector of entries of $f(x)$ or a vector of indexes of $f(x)$?\n    1.3 In Section 3.3, the meaning of the notation $g$ in Eq. (3) and $u$ in Eq. (6) needs demonstrated.\n2. Connections of the two components of Chameleon sampling are weak. It seems that Chameleon sampling just combines robust memory construction and robust learning method directly, in which both parts are developed independently. Either theoretical analysis or experimental ablation study on these two components may be needed to emphasize the novelty and non-triviality of the two components in Chameleon sampling.\n3. Experiments do not support the claim well.\n    3.1 As this paper follows the blurry continual learning setting in Bang et al., 2021, why the experiments (at least for the noise ratio=0% case) do not follow their settings and why the proposed method in Bang et al., 2021 is not considered as baseline? \n    3.2 In the experiment of CIFAR-10/100, results with noise ratio=0% are needed to give a thorough comparison.\n    3.3 All of the baseline methods in experiments on the Clothing1M and WebVision dataset are combination of continual learning and robust learning methods (e.g., RSV+SEFILE), what's the performance of only continual learning methods (e.g., RSV, GBS and that in Bang et al., 2021)?\n",
            "summary_of_the_review": "This paper investigates an interesting problem and presents some insightful ideas for resolving the problem, but I still have some concerns w.r.t. the paper and hope that authors' rebuttal can address them.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studied the online and blurry continual learning with noisy labels problem. To deal with this practical but difficult situation, the authors present a way for balancing diversity and purity in episodic memory management, as well as a robust learning approach to deal with noisy labels. Specifically, the authors construct a score function that takes into account both training loss (purity) and feature similarity (diversity) at the same time. Furthermore, a data- and noise-agnostic strategy for automatically adjusting the balance coefficient during training is proposed. Finally, a robust learning strategy is described to deal with a number of mislabelled instances.",
            "main_review": "Strengths:\n1.\tThe proposed problem is practical and challenging.\n2.\tIt is interesting to see that the proposed method can also help select more informative examples when updating the memory.\n\nConcerns:\n1.\tThere is no blank between the caption in Figure 1 and the main text, which is hard to read.\n2.\tClean examples usually have small loss but they can also have large loss, which are called hard-example: examples that are close to the decision boundary [1]. To increase the diversity, the memory should include those hard examples instead of the mislabelled examples.\n3.\tThe setup is not clearly described. The noise type is not introduced.\n4.\tThe overall algorithm should be elaborated. \n4.1\tIn Section 3.2, the score function is derived but how it works? I can only find that the score function is for sorting. If I missed something, please let me know.\n4.2\tThe memory is divided into three parts but I can only find the loss function for the unlabeled set.\n5.\tIn Section 3.3, the authors claimed that the noisy examples are with high diversity and contribute to better generalization. However, from my understanding of the label noise community, the noisy examples generally degenerate the generalization ability of the model. Is there anything special here?\n6.\tThere are many typos (e.g., data- nad noise-agnostic) and inconsistent representations (e.g., mislabeled vs. false labeled).\n\n[1] Bai Y, Liu T. Me-Momentum: Extracting Hard Confident Examples From Noisily Labeled Data[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021: 9312-9321.",
            "summary_of_the_review": "Overall, the writing should be improved, and the core philosophy of this method should be justified.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a series of techniques to learn a classifier in the blurry continual learning setting in the presence of noisy labels.  Their approach keeps a set of \"memory\" instances to be used periodically during training, which has been established in previous work for continual learning.  The novelty in their approach lies in the techniques to ensure the purity of these instances.  They do this by scoring newly observed instances by a combination of their purity and diversity.  Those with high scores are kept in memory, while those with low scores are either issues soft-labels or as used for regularization, depending on if the classifier is confident in their labels or not.  Their empirical results show that their set of techniques, called \"Chameleon Sampling\" (ChamS), results in classifiers that are more accurate at the end of the continual learning tasks when those learned by combinations of techniques used to do continual learning or handle noisy labels.",
            "main_review": "Major Points:\n\n1) Many times the authors state the trade-off between diversity and purity, and how it is inherent in continual learning problems.  However, these claims are made with little formal or intuitive justification (I think it is telling that Section 2.3 seems dedicated to this, but contains no citations and no formalisms).  Some of this becomes a little clearer when looking at how ChamS handles these, but the concepts of diversity, purity, their relationship, and why they are especially important in continual learning are never fully explored.  I believe the paper requires formal definitions of these concepts and a more detailed, formal reasoning on why they represent a trade-off in continual learning.\n\n2) The paper relies too heavily on prior work throughout the write-up to be self-contained.  In many cases, the authors simply cite prior work as justification of fairly high-level, but key points instead of providing a sufficient level of details themselves.  Especially noteworthy cases:\n2a) Section 1: Practical, real-world motivation for continual learning\n2b) Section 1: Stability-plasticity dilemma\n2c) Section 3.1:What is the motivation behind the “blurry” continual learning setting?  What does it mean for a class to be “major” and for one to be “minor” both in practice and formally to a learner?\n2d) Section 3.3: What is the small-loss trick and how does it relate to GMMs, and in turn, how does it estimate the probability of being clean.\n\nI will go into more specific details below, but this persists throughout the paper and greatly inhibits sufficient understanding of the proposed methods.\n\n3) The proposed method seems to be a combination of prior work and intuition.  There is very little formal or intuitive reasoning given as to why these particular set of techniques are necessary for this problem.  At the very least, there needs to be explicit wording somewhere in the paper that clearly states the main intuitive and principled different between the proposed approach and the baselines.  Said simply: \"What is makes ChamS fundamentally different than prior work\".  This is especially important because the paper emphasizes that diversity and purity are important to their method but make no attempt at relating the two in a formal way.\n\n4) Section 3, the methodology section, begins by discussing the properties of the data in the blurry continual learning task, but then goes directly into how ChamS manages memory.  There is little formal definition of the learning task or it's properties.  How are different tasks presented to the learner?  What is the objective goal of the task?  Are there assumptions on the classifier and/or learning algorithm?\n\n5) I think Definition 1 needs more notation and intuition before it is stated and discussed.  f seems to be both a representation learned by a classifier and the classifier itself.  The description of w is confusing and seemingly dependent on the the structure and parameterization of f, which is not discussed. \n\n6) Equation 2 also needs better discussion.  What is the function “l” in the purity term (Note: \"l\" is discussed a couple paragraphs after it is introduced here)? How do you have access to the true label “y”, and if you did, then why do you need to worry about noisy labels at all? \n Further, there is little to no discussion on why the first term represents purity and the second term diversity.  There needs to be much stronger justification for this score function to understand its intent and properties.\n\n7) The formal definition of the episodic memory is not introduced by the time it is used in Equation 2.  Through context and prior work, I assume it is a cache of previously observed instances, but it is impossible to know for sure since it is never defined.\n\n8)  Quoting section 3.2: “Accordingly, the coefficient value increases gradually to 1 since the training loss is monotonically decreasing via optimization. Therefore, ChamS gives a higher weight to the purity at the early stage, while to the diversity at the late stage.”  This does not seem possible.  (1/2)*min(1/x,1) has a maximum value of 1/2 over x \\in (-infinity,infinity).  How can this value be 1?  I’ll chalk this up to a miscommunication or typo.  However, even to make such a claim, it is necessary to discuss the learning algorithm, the model class of the classifier, and the properties of the loss function.\n\n9) Equation 3 also needs further specification.  “g\", “p(g)”, and “p(l(x,\\bar{y};\\Theta)” all need more definition.\n\n10) Why is 0.5 an acceptable threshold for determining clean versus dirty samples, and relabeling and unusupervised learning sets.\n\n11) Quoting the paper: “Because the examples with high uncertainty are difficult to re-label, they are treated as the unlabeled set without relying on their possibly unreliable labels”. How can this be assumed?  Model uncertainty is both a reflection of the instance as well as the model.  For instance, in early training a model may be uncertain in an instance’s label simply because there are not a wealth of similar instances in the training set.  This is independent of the difficulty to label that instance.  Further, without a specific noise model for labels, it’s difficult to determine if an instance is “difficult” to label.  As a result, it’s difficult to reason about the difference between noisy labels with high confidence versus those with low confidence.  There needs to be stronger discussion on the assumptions made that lead to these design decisions in the algorithm. \n\n12) What is the practical effect on the model for soft-relabeling in 7?  By pushing labels to the model’s predicted class, you are seemingly reducing the error for noisy samples during learning.  Considering these are high-confidence instances already, it would seemingly suppress the influence of these instances in the optimization by explicitly reducing their error.  Is this the desired effect?  Why is this important?\n\n13) I think an algorithm sketch tying all the components of ChamS would be greatly useful.  As they are written now, it is difficult to understand how exactly they work in the context of a continual learning setting where data is continuously streaming in.\n\n14) Experiments:\n14a) What is the continual learning tasks?  The data sets used don’t have natural continual learning tasks associated with them.\n14b) In many other continual learning works, they not only report the final accuracy, but intermediate accuracy measures as tasks are introduced. It's not clear why such results are not reported here. This could provide some much needed clarity on not only the behavior of ChamS, but also the specifics of how the experiments were run.\n14c) I think even simpler baselines could be used to further highlight the difficulty of these tasks.  First, one could naively train a standard neural network architecture on the data as it streams in.  This could provide a naive baseline.  Similarly, one could train one or more of the baselines on data with no noise to get a practical high-water mark for accuracy on these tasks.\n14d) Error bars representing some variance in results over a number of trials here is important.  Because the data sets are not naturally continual learning tasks, and for many of them, label noise is synthetically generated, there is the potential for a lot of variation.  Without multiple trials and some notion of variance between trials, it's hard to say whether these results are statistically significant or if there is a sensitivity to experimental configurations that affect performance.\n14e) While 14c and 14d are considerable issues, the results do seem promising otherwise.\n\nMinor Points:\n1) The related work seems to cite a number of important works, but it stops short of directly comparing many of the works discussed with ChamS.  I think it is important to not only reference related work, but fit the proposed methodology within it.\n2) “splitted” should be “split” in section 3.1",
            "summary_of_the_review": "While the proposed sampling approach achieved impressive empirical results at face value, the technical specification and motivation for the particular problem setting are severely lacking.  Further, ChamS leverages a considerable amount of prior work while combining them in a way that lacks both intuitive and principled justification.  Finally, the empirical evaluation does not fully articulate the evaluation procedure and it does not appear that multiple trials were run despite the potential high variability from one trial to another.  I believe that these issues are severe enough for rejection, but I do not think the paper is hopeless.  If future drafts of this work took more care in clearly articulating key  technical details, describing what principled reasoning or even intuition was used as a basis for ChamS, and presenting empirical results with more clarity and statistical significance, I think the quality of the paper would be greatly increased.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new method termed Chameleon Sampling for continual learning (CL) with noisy labels. Specifically, the diversity and purity of examples are considered during the sampling process. ",
            "main_review": "Pros:\n1. The studied problem is useful. The consideration of label noise in CL is a new breakthrough.\n2. The writing of this paper is satisfactory.\n3. The proposed model is easy to understand.\n\nCons:\n1. The strategy in handling noisy labels is actually not new.\n2. Some experimental results are far from perfect.\n\nThe detailed explanations on the cons of this paper can be found in the Section of \"Summary Of The Review\".\n",
            "summary_of_the_review": "My concerns and problems are listed below:\n1. This paper considers noisy labels in CL, which is good. However, I'm a bit disappointed with the method adopted by the authors. In fact, the techniques used in this paper, such as selecting \"small-loss\" data, re-labeling, data augmentation and consistency, have been widely appeared in the prior works on label noise learning or semi-supervised learning. Therefore, I think the novelty on this aspect is not that sufficient.\n2. For diversity, in fact, many prior works on active learning and curriculum learning (also self-paced learning) also consider this issue. So, why not directly deploying the prior techniques here for achieving diversity?\n3. I feel that the proposed model is a bit empirical. It contains many empirical choices and heuristic operations. It would be better if some theoretical analyses can be conducted.\n4. In the experiment, I found that the accuracies on many cases are very low (see Tables. 1,2,3), where some of them even below 30%. In fact, even existing weakly-supervised learning works on label noise learning and semi-supervised learning can achieve much higher performance than the records here. Therefore, I feel that the proposed method can hardly be used for practical problems.\n5. For the literature review on label noise, another important trend based on loss correction is missing. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I do not have ethics concerns on this paper.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}