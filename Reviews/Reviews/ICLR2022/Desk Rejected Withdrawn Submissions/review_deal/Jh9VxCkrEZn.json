{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper combines the continuity of ODEs with GNNs/TCNs for spatiotemporal learning. The major contribution contains: 1) a spatial ODE to capture long-range spatial dependencies; 2) a temporal ODE to consider long-range temporal dependencies; 3) coupling 1 and 2 for multivariate time series prediction and spatio-temporal forecasting.",
            "main_review": "Strengths:\nS1. Good presentation skills.\nS2. Experiments are extensive and comprehensive, e.g., performed on 5 datasets.\nS3. Spatiotemporal modeling is important to many real-world applications, such as traffic prediction.\n\n\nWeakness:\n\nW1. The major concern is the novelty. Improving GNNs/TCNs to possess continuous hidden dynamics is not new to the AI community, especially we already have [1, 2, 3]. By the way, these models should be considered as related works and even baselines for spatiotemporal modeling.\n\nW2. Some statements are not well supported or incorrect, e.g., how does your model address the issues mentioned in Sec 1? \n- Why ODEs can address long-range spatial/temporal relations? No detailed explanation here. As mentioned in Sec 1, it is achieved by the infinite receptive field of ODEs. However, the ODE integration interval is very large in this paper (e.g., 0.25 or 0.5 -> 4 or 2 layers), which is inconsistent with the infinite layers (i.e., continuous hidden state). \n- The claim of \"continuity in this paper seems incorrect. According to the implementation details, the proposed model is a stacking of residual layers since the ODE integration interval is very large. For example, using an interval like 0.25, 0.5 (in page 5) is very close to using two or four residual layers, in which the only difference is the sharing manner of parameters. This paper didn't provide a comparison between DyG-ODE and a model with an equal number of non-shared residual layers.\n- Why layer-wise parameterization is hard to converge? Any theoretical proof or experimental results to demonstrate? Moreover, why using neural ODEs is easier to converge? This point is not convincing to the audience.\n\nW3. Generally speaking, running neural ODEs requires O(1) space and is instead very time-consuming due to the inefficiency of the ODE solver. However, this paper didn't provide any discussion on the time complexity. It would not be reasonable if the model achieves marginal improvement while consuming much training/inference time, especially in a real-world deployment. By the way, how many ST blocks were used in each dataset? \n\nAlthough this paper is well-written and the experimental evaluation is good, it still has large space to improve. If I have made mistakes in the above comments, please feel free to correct them. \n\nReference:\n[1] Poli et al., Graph Neural Ordinary Differential Equations, arxiv, 2019.\n[2] Fang et al., Spatial-Temporal Graph ODE Networks for Traffic Flow Forecasting, KDD, 2021.\n[3] Fuketa et al., Neural ODE with Temporal Convolution and Time Delay Neural Networks for Small-Footprint Keyword Spotting, arxiv, 2020.\n\n",
            "summary_of_the_review": "Although this paper is well-written and the evaluation is good, it still has large space to improve.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concerns.",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper presents a dynamic graph ODE for spatiotemporal representation learning on time series data. The goal is to model the continuous dynamics of spatiotemporal data with two ODEs and learn the graph structure for data without predefined graphs. They conduct experiments on three conventional time-series datasets without predefined graphs and two traffic networks with predefined graph structures. The method is evaluated on both single-step and multi-step forecasting problems and achieves equally or better performance compared to the SOTA methods. ",
            "main_review": "Originality/Significance: The proposed idea is not completely novel. Compared to two important related works, this paper adds ODEs to model spatiotemporal dynamics over [1] and this paper adds graph structure learning for modeling spatiotemporal data without predefined graphs in [2]. However, the combination of the two looks smooth and reasonable.\n \nClarity: This paper is well-written. The language of the paper is clear. However, there are some confusing parts. In Eq. 2, it is not clear how f wraps two ODE functions. The motivation claims that the proposed method avoids redundant trainable parameters and dedicated designs, such as residual and skip connections. This is not well-elaborated and residual blocks are still used in the framework claimed in Eq. 8.\n \nQuality: This paper addresses an important research problem (spatiotemporal representation learning and downstream forecasting tasks). Overall, this work is well-motivated, and the proposed method is reasonable. The experiments on single-step and multi-step forecasting suggest the model achieves SOTA performance. For the three conventional time-series dataset, they outperform the baseline by a large margin, but they only achieve close performance to the best baseline in the two traffic datasets. I am also not convinced about why the extra graph structure learning makes sense in a fixed traffic network, even though the framework seems to be general. Finally, I’m concerned about the efficiency of the proposed method in real-world applications.\n \nPros:\n* Important task to model multivariate time series with spatiotemporal representation learning\n* Clear motivation to introduce ODE and graph structure learning\n* Consider five time series benchmark datasets which consist of traditional ones without graph structures and two traffic ones with fixed graph structures\n* Experiments suggest the model achieve SOTA performance in all five datasets and the importance of each proposed component\n \nCons:\n* The method looks like the combination of two modules proposed in previous work, ODE for dynamic modeling [2] and structure learning for graph construction [1].\n* It is not clear how graph structure learning helps in the fixed graph structure scenario. \n* The time complexity of the proposed method is not analyzed. The way how two ODEs are designed seems very complicated. It would also be good to show the practical training time.\n\n[1] Wu, Z., Pan, S., Long, G., Jiang, J., Chang, X., & Zhang, C. (2020, August). Connecting the dots: Multivariate time series forecasting with graph neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (pp. 753-763).\n \n[2] Fang, Z., Long, Q., Song, G., & Xie, K. (2021, August). Spatial-temporal graph ode networks for traffic flow forecasting. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining (pp. 364-373).\n \n\n",
            "summary_of_the_review": "Overall, I tend to accept this paper given its good motivation and general framework to handle both time series with or without predefined graphs, but the authors are encouraged to add more insights, discussions, and clarifications.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a fully continuous model called Dynamic Graph ODE (Dyg-ODE) to capture both long-range spatial and temporal dependencies. The dynamic graph ODE approach couples the spatial and temporal ODEs. This enables the model to be more expressive. A wide variety of experiments are performed and show the potential utility of the proposed approach. Many other results are also provided in the appendix, which are very useful. The main contribution appears to be the graph ODE formulation for the spatial-temporal time-series. There are some closely related papers mentioned below that are missing and should be appropriately discussed. The writing also needs significant work. There are some unsupported claims throughout the paper that need to be fixed. Many of the key ideas and discussion points can also be improved and clarified. See below for other important feedback.",
            "main_review": "A fully continuous model called Dynamic Graph ODE (Dyg-ODE) is proposed to capture both long-range spatial and temporal dependencies. The approach couples the spatial and temporal ODEs, enabling it to be more expressive. This work is an extension of STGODE with dynamic graphs. Nevertheless, the same idea can be achieved with any GNN by restricting the number of steps for each layer. This should be discussed, especially in Section 3.4.\n\nIn the abstract, it is unclear why “discrete neural architectures or graph priors would hinder their effectiveness and applications in the real-world”, this sentence needs to be clarified as to why these hinder effectiveness and their ability to be used for applications.\n\nThere are a few important works missing that need to be discussed appropriately. The most related work seems to be Dynamic PageRank, which is also an ODE and dynamical systems approach. There is also CTDNE, which is a continuous-time dynamic network approach that has since been extended in various ways using GNNs. \n\nIn Figure 1, it is unclear how the spatial information is modeled. This should be clarified. Proposition 1 seems very similar to Dynamic PageRank paper where they recover static PageRank when going to infinity.\n\nTypo in Section 4.1 datasets, “… and the rest two are traffic …”\nAlso, Page 9, “we further demonstrate that DyG-ODE is also more computational efficient than discrete methods.” \nFurther, this sentence should be clarified as well, e.g., is this in terms of time, space, etc? \n\nFor the baselines that assume an input graph, why not just use the graph constructed from your method as input? This would allow for the same set of state-of-the-art baselines to be used in both cases, and provide a more solid evaluation.\n\nHow did you select the integration intervals? They seem quite arbitrary, since they all differ for each dataset. What is the implication of using different intervals, and ODE-specific hyperparameters? \n\nHave other ODE solvers been tried, besides the simple Euler? For instance, Runge-Kutta, and so on. This would be useful. \n\nMany of the claims are not substantiated. For instance, “Firstly, learning the long-term temporal dynamics by combining a sequence of temporal convolutions results in the discontinuous trajectories of latent representations, thus making the model prone to numerical errors.” What is meant by numerical error here? The proposed approach seems to be more prone to numerical errors, and I recall a trick was used to “fix” it, which is similar to the one used by other work. The second point, which is reiterated many times throughout the paper, is also unclear. Why are long-range dependencies of such importance, especially when the graph is constructed. Obviously, if the graph is constructed appropriately, then such long-range dependencies would lead to worse performance. I also didn’t notice an experiment that confirms this case. It can easily be tested though using the same graph in both methods.\n",
            "summary_of_the_review": "The writing needs significant work. The proposed approach is interesting and appears to be useful in practice. Evaluation is extensive. There are many unsupported claims throughout the paper that need to be fixed. Many of the key ideas and discussion points can also be improved and clarified. There are additional experiments and results that should also be included in the evaluation, as discussed above. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper focuses on learning spatio-temporal representations of multivariate time series. Inspired by Neural ODEs, the author propose to learn the continuous spatiotemporal dynamics by combining 2 ODEs: one for continuous graph propagation (spatial ODE) and another for the temporal aggregation process (temporal ODE). A key advantage of the proposed approach is that it alleviates the need for static graph priors. Additionally, the authors claim that this approach is able to capture long-range spatial dependencies, which the discrete counterpart is shown to be unable to. Empirically, the proposed approach is shown to outperform several baselines from the state of the art across 5 different datasets. ",
            "main_review": "Strengths:\n- This paper addresses a very relevant problem and of broad impact on the community.\n- The degree of novelty is good, and it is clear how this work differs from previous contributions.\n- The submission is technically sound.\n- This is a well-written paper and generally well structured, making good use of appendices.\n\nWeaknesses:\n- There are a few aspects of the empirical evaluation that could be improved. It is unclear why the authors choose to report relative squared error and correlation coefficients for some datasets, while for other MAE and MAPE are used. \n- Additionally, the statistical significance of the obtained improvements over the baselines looks questionable. Especially when compared to MTGNN, the improvements look very small. The authors should consider including standard deviations for the provided results over the 5 runs, or ideally, perform statistical significance tests to demonstrate that these results are significant. \n- I think the authors should have investigated empirically and in-depth the ability of the proposed approach to capture long-range spatial dependencies, since this is claimed several times as a strength of the proposed approach. Also, I don't think that the datasets considered are the best to showcase that. For example, 3 out of the 5 datasets considered are traffic datasets and, in traffic, most spatial dependencies tend to be quite local. Did you inspect the learned spatial dependencies for any of the datasets considered?\n- The ablation study could be improved by, for example, looking at the impact of learning the graph together with the entire model vs. relying on a prior. ",
            "summary_of_the_review": "In summary, this is a clear and well-written paper that tackles a very relevant problem that can impact and be of interest to a broad audience. The proposed approach is interesting, novel and technically sound. However, I have key concerns regarding the empirical evaluation that prevent me from recommending its acceptance at this stage. Particularly, the empirical results show very marginal improvements over state-of-the-art methods, and no statistical significance analyses are provided. Kindly see my comments above regarding my concerns about the empirical evaluation. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work presents a new model in learning dynamics on graphs using nested neural ODEs. The authors provide reasons for why they structure the two models and combine them together; the authors also provide theoretical justification of the advantages of continuous-time models over the discrete counterpart. The authors also perform experiments to show the empirical advantage of the proposed models.",
            "main_review": "***Limitations of this work***\n\nIn terms of computational cost, nested neural ODEs might take too much number of function evaluation to compute, making the model incapable to apply to longer time series. In terms of novelty, the model might be seen as a simple combination of neural ODE and a special case of continuous-time graph neural networks, e.g., graph neural ODEs and graph neural diffusion.\n\nThe authors stated that \"For simplicity, we optimize model parameters via the standard reverse-mode differentiation, which can be replaced by the adjoint sensitivity method as suggested by Chen et al. (2018) for a better memory efficiency.\" In fact, the adjoint method might not be applicable for all these models. To use the adjoint method, you will need the value of the forward equation at the ending position. When you work on the adjoint method for the outer ODE and proceed to some time $t$ that might not have occurred in your forward loop, there is no reasonable way to get the ending position of the inner ODE.\n\n\n***Correctness***\n\nI think the authors provide a fair comparison to different baseline methods by repeating with 5 different random seeds and averaged.\n\n\n***Clarity and reproducibility***\n\nThe writing is clear and easy to follow. Also, the authors provide details of the model structure, so there should be no problem in reproducibility.\n",
            "summary_of_the_review": "***Summary of the paper***\n\n\nThis work presents a new model in learning dynamics on graphs using nested neural ODEs. The authors provide reasons for why they structure the two models and combine them together; the authors also provide theoretical justification of the advantages of continuous-time models over the discrete counterpart. The authors also perform experiments to show the empirical advantage of the proposed models.\n\n***Limitations of this work***\n\nIn terms of computational cost, nested neural ODEs might take too much number of function evaluation to compute, making the model incapable to apply to longer time series. In terms of novelty, the model might be seen as a simple combination of neural ODE and a special case of continuous-time graph neural networks, e.g., graph neural ODEs and graph neural diffusion.\n\nThe authors stated that \"For simplicity, we optimize model parameters via the standard reverse-mode differentiation, which can be replaced by the adjoint sensitivity method as suggested by Chen et al. (2018) for a better memory efficiency.\" In fact, the adjoint method might not be applicable for all these models. To use the adjoint method, you will need the value of the forward equation at the ending position. When you work on the adjoint method for the outer ODE and proceed to some time $t$ that might not have occurred in your forward loop, there is no reasonable way to get the ending position of the inner ODE.\n\n\n***Correctness***\n\nI think the authors provide a fair comparison to different baseline methods by repeating with 5 different random seeds and averaged.\n\n\n***Clarity and reproducibility***\n\nThe writing is clear and easy to follow. Also, the authors provide details of the model structure, so there should be no problem in reproducibility.\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}