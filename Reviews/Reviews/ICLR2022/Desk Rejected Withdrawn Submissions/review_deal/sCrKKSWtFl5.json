{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presented a self-supervised calibrated photometric stereo algorithm. The inverse rendering algorithm was proposed where the surface normal and BRDF parameters were represented by continuous functions that map 2-d coordinates to each attribute. The proposed method introduced some physically plausible designs. First, the BRDF of an object is represented as a combination of  base materials. Second, the shadowed pixels were detected based on the local visibility of rays and removed in optimization. The performance of the method was evaluated on DiLiGenT benchmark and some other real scenes and achieved lower reconstruction errors than other state-of-the-art algorithms. ",
            "main_review": "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nPros.\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n- In similar with NeRF, optimizing surface attributes as continuous MLPs in photometric stereo is an interesting direction.\n- The proposed self-supervised algorithm outperformed other inverse rendering algorithms and some supervised methods on DiLiGenT objects. Especially, the prediction error on Reading is impressive.\n- The algorithm can recover not only surface normals but also surface BRDF params in addition to depth and shadow as an added benefit.\n- It's nice to see some useful ablation studies.\n- The paper is well written and I couldn't find obvious cosmetic errors.\n\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nCons.\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n- The numbers presented may seem good, but upon closer examination of the paper, it appears to be that the contribution of the proposed method has not been properly validated. The paper showed only normal estimation errors for a very limited number of objects, without sufficient theoretical discussion or analysis about two ideas presented in the paper; One is to formulate the problem in the framework of inverse rendering and use a continuous function representation similar to Nerf for simultaneous estimation of physical properties, and the other is using depth information to properly exclude shadow regions in sampling during training.\n- As for the first idea, I am very suspicious about the contribution of explicitly considering physical properties to the accuracy of normal recovery. First of all, I disagree that the lack of real training data is problem in the supervised approach. Photometric stereo would be one of the few problems in computer vision where the network can be trained by completely synthetic data alone. The important thing could be to prepare appropriate training data rather than taking physical characteristics into account. In reality, PX-Net[LogothetisICCV2021] have already shown that the purely pixelwise CNN-PS algorithm trained on better synthetic samples improved its DiLiGenT score from 7.2 to 6.4 degrees (6.5 degrees by this work) and their PX-Net achieved even better performance (6.17 degrees) (Please don't misunderstand that I'm not saying it's not good because the proposed method was less accurate than this method, but I'm pointing out that the superiority of self-supervised approach against supervised one has not been shown.).\n- The advantage of using a continuous function representation with coordinates as input is also not clear. In table 3, the proposed framework without shadow treatment achieved comparable performance against [Taniai2018] (which one is better depends on the object), which indicates that the difference in architecture of inverse rendering does not make a big difference at least in the prediction accuracy, rather the shadow treatment is more important. More theoretical and analytical discussion should have been provided to compare the neural networks that take coordinates vs ones that take images as input.  I also believe the proposed method should have been compared against Kaya2020 as the latest algorithm based on the inverse rendering. Even though Kaya2020 was propsoed in uncalibrated photometric stereo scenario, it can directly be compared against the proposed method by using the calibrated lighting.\n- As for the second idea, I was slightly disappointed that the loss doesn't propagate gradients from step function and the depth was simply computed from the predicted normal map which means it just contributed to detect shadowed pixels and not to be used as an additional cue for surface recovery.  Actually, it is widely known that excluding shadow regions when calculating the reconstruction error improves performance, and there is nothing new to be found in the improved accuracy of this procedure. The question is whether the proposed method of shadow extraction, i.e., calculating the visibility of rays based on the depth image directly obtained from the normal, is superior to the existing method of shadow extraction based on a statistical framework from observations. As for this point, the ablation study showed that while the proposed shadow rendering performed best in average, a simple statistical shadow removal often worked better on some specific objects than proposed method. The paper should have discussed what condition the proposed shadow rendering has its advantage over simple method. The results would have been more complete if the shadow masking is introduced in other inverse-rendering framework such as [Taniai2018] or [Kaya2020].\n- I agree that the inverse rendering has its advantage that it can decompose images into some physical attributes.  However, the results of the material, depth and shadow recovery or novel-light rendering were demonstrated qualitatively, and not compared with other methods quantitatively.\n- This paper stated that one of the advantages of the proposed method is that it converges quickly especially ten times faster than [Taniai2018] and [Kaya2020], but the reason for this was not clearly described.\n- The discussions about the valid number of input images and valid image resolution are completely missing. Especially, minimum requirement should be provided.\n \n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nQuestion\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n- Since the view direction is fixed in this setup, it seems that there is no different to model brdf as a function of between (n, l) and (n, h) because it just gives a constant shift in lz.  Is it true?\n- What is the effect of using different position encoding strategy or just simply feeding the raw coordinates?\n- Though the paper only compared nonparametric representations of BRDF basis, what about the parametric BRDFs such as Cook-Trrance, Ward or Diseney's Principled BSDF as in conventional works?\n- Because the depth map is only constrained by the surface normal map, how to resolve the ambiguity of the depth offset?\n- \"Cow\" in DiLiGenT is not  actually a metallic object but a metallic-painted object (physical properties vary greatly depending on the percentage of flakes). I just wondered if the proposed method can actually work for a purely metallic object?\n- The choice of number of BRDF basis should be depending on the object. Is there any strategy to find the optimal one? Why there is a big gap of errors between 9 and 12 in Table2?\n \n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nSuggestion\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n- I encourage the authors to discuss the differences between methods from a more precise perspective, rather than just making broad comparisons with benchmarks. For example, to take one example, it seems that the proposed method was doing very well against Reading (8.75 is best score that I have ever seen). Interestingly, [Taniai2017] also worked very well for this object. Looking at the results, the unique properties of self-supervised method seemed to be working well.  So, what makes Reading different from other objects? It cannot simply be explained by its material and concavity because the proposed method didn't work for Harvest, more concave, metallic-painted object.\n- I also encourage the author to discuss more about the downside of the proposed method . Using physical properties sounds good, conversely, it means that the system loses its robustness when the assumption was violated, for example, in the presence of inter-reflections. Actually that was why the recent purely regression-based algorithms were on the rise. In addition, the proposed method is purely pointwise and cannot account the global image context at all unlike image-based algorithms such as [Taniai2018] or [Chen2018]. Standing on those disadvantages, the advantages of the proposed framework over others will help clarify the position of this work.\n- Since the proposed method is an extension of the optimization-based inverse rendering framework using non-Lambert parametric BRDFs (e.g., Shi2014, Ikehata2014) rather than Lambertian robust approach, I recommend authors discuss more about them in related works.",
            "summary_of_the_review": "Overall, I vote for rejecting. The writing is well, and there are some interesting ideas, but I have major concern about the validation of the method. In order to demonstrate the superiority of the proposed method, it seems necessary to discuss quantitatively and qualitatively whether each contribution is really worthwhile, rather than simply showing the benchmark scores.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes a machine learning framework for joint estimation of surface normal and reflectance via an inverse differentiable rendering process with known lighting direction and intensities (parallel lighting setting). The major difference of the proposed framework when compared with previous work is the depth map branch, which provides depth information for shadow detection. The image formation model (eq.1) models shadow as binary term. Surface BRDF is represented as the combination of diffuse term and specular term. The specular term is further simplified as a linear combination of specular basis derived from surface normal and lighting directions. The whole framework is designed so it can be trained in a self-supervised manner.",
            "main_review": "Bringing depth information in to calculate for shadow is an interesting idea. Discarding shadow areas in each input image can help improve the normal estimation accuracy as we can see from the experimental results. Also, the depth branch is designed to be optimized with the normal output from other branch using geometry constraint, making the whole pipeline self-supervised.\n\nThere are two weaknesses from my point of view: 1, Joint estimation of so many different variables tend to be unstable as the author also admitted in Sec.6. it requires a proper initialization (In this paper, the author mentioned shadow guidance in Sec. 4) to be able to converge to a plausible result. 2. Though the paper takes shadow into consideration, it does not process the shadow area according to eq.1. Pixels in shadow area are discarded.  If the input image shares similar shadow regions, the proposed framework may fail since the network would have no information about the overlapping shadow region shared by all input images.\n",
            "summary_of_the_review": "I think the paper did a decent job to improve PS estimation by designing and implementing a more realistic rendering model framework. Thus, I lean towards acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "In this paper the authors present an MLP-based approach to solve the photometric stereo problem for non-Lambertian objects. The main component of this piece of research is the neural parametrization of surface reflectances, as well as the surface geometry, factorizing the image appearance into normals, diffuse and specular components, and shadows. The approach is self-supervised, but it assumes some strong priors as the light direction and the number of specular basis to encode the BRDF model. In comparison with some competing approaches, the method obtains a good trade-off between accuracy and computational cost. Unfortunately, other uncalibrated approaches are not included in the discussion nor in the experimental comparison. ",
            "main_review": "In general, the paper is well-written and it is easy to be followed. The problem is interesting and the applicability high. However, I still have some issues with the current version. \n\nThe well-known photometric stereo problem, i.e., the joint estimation of reflectance and shape (via normals) from a set of images with frame-varying illumination is handled by means of neural radiance fields. The work is an extension of NeRF [Mildenhall etal. ECCV’20] to the photometric-stereo domain instead of using motion cues as input. The proposal is very interesting in this way. \n\nThe method achieves a good trade-off between accuracy and computational cost, as the solution outperforms the provided by **some** competing techniques while it is faster. \n\nThe specular BRDF lies in a non-negative linear combination of atoms of specular basis. Particularly, the authors use k basis, being this number known in advance. I feel this is a very strong prior in comparison with previous approaches in the topic, where this prior is normally relaxed. As it can be seen in table 2, a good selection is mandatory to obtain the best results. \n\nIn the same manner, the method assumes as input the light direction, a very strong prior in photometric stereo, as very recently this problem was solved in an uncalibrated form. \n\nIn general, all the constraints and losses were previously introduced in competing methods. \n\nAblation study in table 3 is very illustrative. \n\nOther minor comments: \nAn extra “)” is included in Eq. (7). ",
            "summary_of_the_review": "On balance, I think the paper is very interesting but some strong priors are assumed. In this sense, I do not believe the results are better than the provided by competing techniques, where neither the number of atoms nor the light directions are assumed to be known as input. Unfortunately, that was not explained in the paper, and as a consequence, some claims were not justified properly. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}