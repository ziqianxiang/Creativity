{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors propose an analysis on the neural networks pruning technique, and in particular, they theoretically prove that there exists a bound in the number of SGD pre-training iterations beyond which pruning via greedy forward selection yields a subnetwork that achieves good training error. The theoretical discussion is limited to a very specific model (a two-layer, fully-connected network), while the empirical evaluation involves also other architectures.",
            "main_review": "Even if the proposed theoretical analysis is interesting in my opinion, the novelty proposed by the authors in this manuscript is below the average for an ICLR paper, in particular, because the authors do not propose a novel solution based on the theoretical observation that they proposed. Moreover, the most interesting part of the analysis is limited to a very specific model. In general, the paper is well written and pleasant to read, but at some points, the discussion should be extended and clarified.\n\nIn section 2 the discussed 2 layers network (eq (1)) has a very particular architecture since it presents a single layer of neurons whose outputs are summed and divided by the number of neurons.  It seems more similar to a single layer model since no linear transformation that involves trainable parameters is applied on the output of the first layer of neurons. Please clarify which parameters are exploited by each layer of the architecture.\n\nFor what concerns the structure of the paper, in my opinion,  section 5 (\"Related Work\") should be moved in before section two. Moreover, section 5 should be extended explaining in more detail some of the considered work, in particular (Ye et al., 2020) and  (Oymak & Soltanolkotabi, 2019) that are useful to understand the proof of Lemma 1 and how Theorem 1 has been derived.\n\nThe authors state that the number of SGD iterations above which a pruned subnet achieves a good training loss is dependent on the size of the dataset. What is not clear to me is how the variance between the samples contained in the dataset (e.i. the amount of information)  influences the number of iterations. Let's consider two datasets of the same size, and one contains many samples that are very similar, while the other is composed of samples that are very different and sparse in the input space. How much does the bound of required SGD iterations differ in these two cases? How much the amount of information contained in the datasets and/or the complexity of the considered task influence the amount of dense network pre-training needed for a pruned network to perform well?\n\nThe empirical evaluation turns out to be questionable in some aspects. For instance, the authors, for what concerns the 2 layers network, state “Namely, we adopt a ReLU hidden activation and apply a sigmoid output transformation to enable training with binary cross-entropy loss. Experiments are conducted with several different hidden dimensions (i.e., N \\in {5K,10K,20K}). We pre-train the two-layer network with an SGD optimizer, the momentum of 0.9, no weight decay, and a batch size of 128, which we found to perform well in multiple different experimental settings. “, therefore the tests are performed using a very specific hyper-parameters setting, and to me, it is not clear how the authors can exclude that this very particular setting did not influence the measured performance.\nIn appendix C.1 the authors discuss the relationship between the size of the model and optimal learning rate, but the discussion on the selection of the other hyper-parameters (momentum, weights decay, batch size, etc) is missing.\nEven the method used to determine the optimality of the lr, and also to assess the model performance is not clear to me. Did the authors use a validation /test set? A similar methodology was used also for the other considered models.\n\nAs a final remark, I would like to highlight the fact that the paper is 9 pages long while the appendix is 14 pages long.  In my opinion, a paper that requires more pages for the appendix (that contains relevant information to understand the proposed analysis) than the length of the main paper, is maybe more suitable for a journal than for a conference, which imposes a limit on the number of pages. Since that is just a personal opinion, this last point did not influence my evaluation of the manuscript.\n\n",
            "summary_of_the_review": "Even if the proposed theoretical analysis is interesting in my opinion, the novelty proposed by the authors in this manuscript is below the average for an ICLR paper, in particular, because the authors do not propose a novel solution based on the theoretical observation that they proposed. Moreover, the most interesting part of the analysis is limited to a very specific model.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper study the minimum number of pre-training iterations to make pruning successful. A theoretical justification is provided based on the analysis for a two-layer network. The authors empirically verify their theoretical findings on MNIST, CIFAR-10, and ImageNet with MobileNet and ResNet. ",
            "main_review": "* In Figure 2, 'thousands' --> '1x10^3'\n* In Figure 2, the accuracy is visualized by the color of each cell. Since accuracy is one of the most important metrics, it should be visualized with lines. \n* A conclusion reached by this paper is 'number of pretraining iterations depends on dataset size instead of model size.' However, when dataset size changed, it is more appropriate to measure number of epochs instead of iterations.  During training, with the same batch size, if the dataset size is smaller, one should be able to achieve the accuracy saturation with fewer iteration. So, ''smaller dataset ---> fewer iteration'' is somehow trivial. \n* Another interesting and **related** question is how large dataset we should use. According to the authors' conclusion, you can do fewer pre-training iterations to find the sparse net for a smaller dataset. From a different perspective, suppose we do enough pre-training iterations, what is the best pruning accuracy we are able to achieve with a smaller dataset, is this accuracy is much lower than using the full-dataset. If so, fewer pre-training iterations is a result of lower performance in purpose. \n* Figure 2 is crucial to understand the proposed method. Instead of using three sub-plots, the author can make a plot, where x-axis is the size of model and the y-axis pruned accuracy given a fixed number of pre-training iterations.  \n* In table 2, the most aggressive pruning ratio is ~(100-65). I am interested in seeing whether the conclusion is more pronounced for more aggressive pruning.  ",
            "summary_of_the_review": "The discussed problem is important and interesting. Some presentation and analysis need to be further improved. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper conducts a theoretical analysis on the relationship between training sample number and pre-training iteration number for network pruning. The result shows that for the two-layer network pruning problem, the SGD pre-training iteration number is logarithmically dependent on the training sample number.",
            "main_review": "In Theorem 1, there is an assumption that Nd > m^2, which is hard to be satisfied in real network training problems. Considering the major result of Theorem 2 is highly correlated with Theorem 1, it is questionable that the established analysis is effective.\n\nThe experiment result cannot well support the established theoretical analysis. The logarithmically dependent on the training sample number cannot be observed from Figure 2. The experiment results on deeper networks conclude that training a larger model needs more iterations. This is an obvious intuition, which is too weak without sufficient detail to support the theoretical result. \n",
            "summary_of_the_review": "My major concern includes (1) over-ideal assumptions in the Theorems and (2) the experiment lacks sufficient details to support the main theoretical results.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper theoretically analyzes the subnetwork performance with respect to the number of SGD pre-training iterations based on greedy forward selection (Ye2020). The discovered findings make a good connection with the existing empirical study, such as early-bird tickets.",
            "main_review": "##########################################################################\n\nPros: \n\n \n\n1. The paper is well written and easy to follow. Assumption 1 is quite common in practice, i.e., Relu. \n\n \n2. The theoretical insights provided by the paper are in line with the empirical findings in the existing works.\n\n3. The findings in Figure2 are quite interesting.\n\n\n##########################################################################\n\nCons: \n\n1. I expect that there are some terms in the bound that can represent the sparsity of the subnetworks, since the sparsity of the subnetwork is highly correlated with the performance.  However, I am somehow missing this in the bound. Can the authors please clarify this?\n\n2. It is not clear to me that the experimental results are averaged with multiple runs or just once. As shown in Table 1, the difference between some settings is very small. It is not clear to me if this accuracy difference is caused by randomness or the theoretical bound provided. \n\n3. It is unclear to me why this paper focuses on \"greedy forward selection'' for pruning. Methods like iterative magnitude pruning (IMP) are more commonly used in practice. Theoretical study on IMP is definitely more valuable for the community. \n\n",
            "summary_of_the_review": "This work aims to analyze the correlation between the pre-training time of the dense network and the performance of the subnetwork. Although the study on this problem is important and the bound seems to be correct, the story and experimental results need a bit more work.\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}