{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "Paper explores the topic of weakly supervised representation learning using videos.",
            "main_review": "Pros:\n- Overall the topic of weakly supervised representation learning is interesting, relevant and it has significant practical implications\n- Zero-shot pose estimation task and re-identification and recognition task seem interesting\n\nCons:\n- The technical contribution of the paper is shallow, relying on known architectures and known training methodology\n- In my view, the empirical framework is missing the results comparing the representations learned with the proposed methodology to other abundant weakly supervised representation learning approaches based on static image learning. In my mind, the promise of the paper (based on its title) is that motion informed learning opens up new opportunities compared to learning on static images and synthetic image augmentation. Authors partially support this intuition showing promising results on re-identification and pose estimation tasks. However, these tasks are significantly linked to motion and it is arguable whether the representation obtained with the proposed approach is simply an overfit to motion based perception. It is unclear if the representations learned this way acquire semantic and discriminative features, which could be tested e.g. on classification tasks after fine tuning the final softmax layer on top of the proposed representation.\n- Another promising implication of the paper could have been the use of large-scale video datasets to learn powerful and semantically meaningful representations, which would require training on such datasets and augmenting the benchmarks with a semantic benchmark (such as testing on imagenet classification or few-shot image classification, for example).\n- The weakly supervised learning based on videos is a topic that has considerable history. For example, there exist works showing that videos can be used to learn depth perception in a weakly supervised way. Thus the results of the current paper are not totally surprising, reducing the overall significance of its empirical contribution.\n- This paper proposes a benchmark to measure the quality of visual representations. However, the benchmark seems to be very specific to video processing and it is unclear if learned representations are useable outside of this domain.\n- Disambiguation of temporal attributes has not been demonstrated in the paper, only weak anecdotal evidence based on TSNEE plots.\n- Disambiguation of view has not been demonstrated in the paper, no evidence.\n- In my opinion, the contribution statement \"we introduce a novel methodology for Siamese self-supervised training on video frame pairs without the need for architecture or loss function adaptation;\" is a gross overstatement, given that this is really based on SimSiam. What is the new methodology you propose, to skip frames between two views fed into SimSiam? Isn't it too trivial and obvious to be a scientific contribution?\n",
            "summary_of_the_review": "I believe the paper is not good enough, because the technical contribution is not sufficiently novel and significant.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper propose to use the neighbouring frames in video clips to unsupervisedly trained a backbone model. The proposed approach is based on existing self-supervised learning framework, demonstrate by utilise different set of training samples can lead to more efficient learning. The trained model is applied to three different down stream tasks and empirically shows good improvement over the compared baselines. ",
            "main_review": "Overall, the main idea of this work is simple yet effective on various downstream tasks. Fundamentally, it argue that instead of using strong data augmentation to generate training samples, it is more effective to use neighbouring frames (with small amount of transformation) to train with video samples. The proposed sampling method is also benefit from existing work that do not required negative samples in training. In addition, the model trained with proposed framework demonstrates good performance (or comparable) on multiple down stream tasks. This demonstrates that the trained model is less bias toward a particular attribute. I believe the finding is able to make wider impact in the research community. \n\nThere are several concerns in the current submission that I would like the authors to clarify:\n- Why is the learned representation \"motion-informed\"? During the learning process, each pair of training sample are effectively two images that are very similar in nature yet differ slightly on pose, view angle, lighting conditions, blur, and so on. It is not clear to me if \"motion\" is somehow used to guide the learning. The paper do empirically demonstrate the training is effective. However, I would like the paper to properly discuss (or proved) such training mechanism can achieve good properties (e.g., 'disentangle view, pose, or temporal attributes').\n\n- In related work, it states that \"We argue that training on video frame pairs leads to the implicit construction of an embedding space that is sensitive to additional object attributes\". What are the additional object attributes here and how will that encoded during the learning process.\n\n- In eqn (1), h() is a MLP head that \"take one vector to get a prediction of the other\". Please elaborate more on this. What is the function h aim to produce. Consider h is to predict another vector, is this neighbour prediction? Consider some form of light transformation is done on the sampled images, it is not obvious to me what is the objective of h in this equation. \n\n- In the Zero-shot pose estimation, a method is described to fit the bounding box of the associated nearest neighbour in training set back to the query images. This create an uncertainty that if the performance of Table 1 is due to this fitting process or due to \"good reference\" obtained by the trained backbone. For instant, is it possible to provide addition result, where instead of the nearest neighbour, can we use the next best reference (or even the 10th nearest sample) and check if the 3D mAP @ 0.5 IoU remain similar. In other words, could it be that the transpose process is so effective that even if the retrieve sample is not ideal it still give good results.\n\n- Fig 4, I don't see any yellow line overlays that stated in the caption.\n\n- Is \"Objectron Baseline (2-stage)\" an existing work? if so please provide the related reference. \n\n- Figure 7 (supplementary), what is the different between green and red bounding box.\n\n- If possible, results on Table 5 (supplementary) should be discussed in the main paper, as it is an important aspect of the proposed approach. In addition, I am curious if there is a range for the \"nearby\" frames, e.g., 1 < t < \"a threshold\". In many scenarios, neighbouring frames could be identical (where camera and observed objects are static), this is effectively similar to \"Same\" in Table 5.\n\n- Samples in Fig 2 is limited to properly justify the quality of the embedding. If possible, please provide more data points in Supplementary.\n\n\nMinor issues on references:\n- [27] venue missing\n- [20] page number is incorrect\n- Multiple of the arXiv citations are published, e.g., [2] --> CVPR 2021 and [3] --> ICLR 2020",
            "summary_of_the_review": "Overall, the paper provide a simple idea that is very easy to implement and the results are good. In term of technical novelty, this work is indeed limited. I believe this work can benefit the community, especially when large amount of focus is now on video data and representation learning. I am willing to adjust my recommendation if authors can address the concerns. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "This work proposes an alternate use of the type of data that is already available in the research community. There are no ethics concerns based on the novel approach in this submission. ",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a simple data augmentation method (learning from nearby frames from the same video sequence) to learn pose-  and motion- aware representations via self-supervised learning.\n",
            "main_review": "Pros: \nThe quantitative evaluation and qualitative analysis demonstrate the effectiveness of the proposed approach in several tasks such as, post estimation, re-identification and action recognition. \n\nCons: \n1. The technical contributions are minor. The whole architecture is based on the SimSiam framework without any modification or improvement. The only difference is using videos rather than classical image transformations as data augmentation.   \n\n2. The comparison to SOTA could be improved. For instance, in page 9, the authors claimed that ‘our proposed motion-informed embeddings surpass the classic embeddings ...’. However, Table 3 can not support such a claim since we observed that the proposed method achieved lower accuracy compared to SOTA. We expect authors to provide more convincing results to support this point.    \n\n3. After reading the whole paper, I am a little bit confused about what the authors want to achieve in this paper. The objective of the self-supervised learning scheme seems to be contradictory to the results shown in experimental sections. If the inputs of SimSiam are two nearby frames from the same video sequence (containing a single object), by maximizing the similarity between them, the network should learn pose- and motion- invariant embeddings since the invariance is appearance and variance is motion/pose. However, the claimed objective of the proposed method is to learn pose- and motion- aware embeddings, which is totally opposite. I would like more explanation from authors on this point, as well as more explanation on experimental results since it is contradictory to the learning objective.   \n\n4. The used datasets are too simple (too specific) and the experiments are not convincing. Does the proposed method work only with a single object in the middle of the frames such as Objectron or sport videos with strong motion, such as UCF101? Can it work on Kinetics ? It is not clear what the experiment on Figure 2 is illustrating. Several frames outside the red region correspond to a chair with roughly the same pose as the ones inside.  Object ReID should be compared with classical benchmarks on car or person ReID, such as Mars.\n",
            "summary_of_the_review": "The contribution is limited and not very clear.\nMoreover the experiments are too simple and not convincing.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes to extend image-level self-supervised learning to video level by using nearby consecutive frames as different views. Specifically, it applies SimSiam on video sequences and conducts some experiments on pose estimation and re-identification.",
            "main_review": "The idea of using consecutive video frames as different views for self-supervised learning in this paper is too intuitive and straightforward.  I can't find anything new in the Methodology part. Also, the performance is not impressive either. On the action recognition task, its result is even far from SOTA.\n",
            "summary_of_the_review": "Overall, the idea of this paper is not novel and the performance is not impressive. This paper is not qualified to be an academic paper, instead, I would say it is somewhat like a course project.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}