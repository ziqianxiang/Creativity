{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a new method called LiST based on self-training for pre-trained language models (PLMs) in few-shot learning settings. \nThe main techniques claimed by this paper is  1) self-training to leverage large amounts of unlabeled data for prompt tuning to significantly boost the model performance in few-shot settings 2）light-weight fine-tuning where we only update a small number of the model parameters.\nThe experiments show the proposed method is superior to other methods in several benchmarks.\n",
            "main_review": "The main method of this paper is easy to follow, and the experiments show the proposed method is effective in some benchmarks.\nI have worried about the novelty and contribution of this paper is not sufficient. The main two technique (see the summary of the paper) of this paper has been investigated the Semi-supervised learning for few-shot learning community. e.p: Learning to Self-Train for Semi-Supervised Few-Shot Classification. Thus, i think the novelty of this paper is not enough and suggest the author do some comparison for some work in semi-supervised few-shot classification.\n",
            "summary_of_the_review": "Given the novelty and contribution of this work, I give weakly reject in this stage and look forward to the author response.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper provides a new method for lightweight tuning of language models in few-shot learning setting. More specifically, the authors propose to tune adapter parameters instead of large-scale language model’s parameter for parameter efficiency. The self-training with prompt finetuning is used to improve the data-efficiency aspects. The experiments are conducted on six tasks, showing decent improvements. ",
            "main_review": "This paper studies how to conduct lightweight fine-tuning of large-scale language models in few-shot learning settings. Both lightweight tuning and few-shot are important and timely topics to study. Existing works usually study these two topics separately and this paper integrates two problems together to form a more challenging problem: few-shot learning with lightweight constraints. I think this paper opens a good direction on how to better leverage large-scale models for downstream tasks and proposes a solution with clear motivation and novelty. This paper shows the feasibility of lightweight tuning in limited data setting. \nStrengths: \n1.\tThe studied problem is novel, important and challenging. \n2.\tThe motivation of model design is clear and the design itself is novel. The paper shows that lightweight mechanism is able to achieve similar performance with full model tuning in few-shot setting. This finding is important for future works. Moreover, the idea of lite self-training and data selection based on adapter is interesting and novel. \n3.\tThe experiments are comprehensive. The authors show the effects of model choices, varying shots and the performance of different lightweight methods in few-shot settings. Many interesting findings are presented especially lite self-training shows decent improvement and what the key receipt of designing lightweight methods is in few-shot learning. \nWeakness:\n1.\tIn table 5, two bottleneck dimensions are used for few-shot learning setting. It would be interesting to see a discussion about how the performance changes with respect to bottleneck dimensions in few-shot setting.\n2.\tThis is not necessarily a weakness point but I wonder what most critical differences are to consider when lightweight tuning methods are designed for few-shot setting versus full supervision setting. \n",
            "summary_of_the_review": "I am glad to see that this paper is able to tackle few-shot learning and lightweight tuning simultaneously and shows several important findings. The model design and the problems are both novel and very interesting. Thus, I vote for acceptance",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper describes an approach for fine-tuning pre-trained language models with few tunable parameters for few shot learning. This is done by introducing the adapter layers within the building blocks of Transformer-based pre-trained models. Because these adapter modules are fine-tuned on the downstream tasks, the total number of tunable parameters are less as compared to network parameters. Experiments are conducted on natural language understanding tasks and performance improvements were observed.",
            "main_review": "The direction of this work is nice and the problem tackled by this paper is indeed important. The obtained results are nice and there is indeed some potential value in this work. However, I have a few concerns with the paper:\n\ni) In Page 2, it is mentioned that if a pre-trained model with M parameters is applied to T tasks, then it will require M x T parameters. In practice, pre-trained models are not applied to downstream tasks in this manner. As an example, Mask-RCNN adds a classification, detection, and mask head to the pre-trained backbone. So, the demonstrating example on Page 2 of the paper is a bit misleading. \n\nii) Adapter modules are inserted inside the transformer layers and only these modules are fine-tuned. What happens when you freeze the pre-trained language model and add task-specific heads? In my understanding, the number of tunable parameters will also be fewer as compared to the pre-trained backbone. However, one obvious advantage of this approach is that pre-trained model can be used without modifying its internal structure. More experiments and discussion with respect to fine-tuning approaches are required to understand the benefits of adapter-based models for few shot learning.\n\niii) Label refinery, a knowledge distillation method, also uses an iterative approach using noisy labels for improving the performance on computer vision tasks. The LIST method seems to be an extension of this approach. However, I did not find any discussion about this or similar approaches. Can authors comment on that?\n\niv) In Table 3, RoBerta models deliver significantly better than BERT models. Architecturally, both of these models are the same and one would expect a similar performance when fine-tuned with adapter modules. It is not clear why there is so much performance difference between these two models. If it is because these models are pre-trained on different corpora, then I would expect much more gains with T5, which has more parameters and trained on larger corpora. But that doesn't seem to be the case as T5-Large models performance is comparable to RoBerta. \n\nClarifications:\ni) On page 4, it is mentioned that adapter parameters are initialized with \"zero mean small Gaussian noise\". What about the variance of Gaussian distribution?\n\n\nReferences:\n\n[R1] Label refinery: https://arxiv.org/abs/1805.02641\n\n[R2] Mask RCNN: https://arxiv.org/abs/1703.06870",
            "summary_of_the_review": "This paper extends the existing methods (Adapters) for fine-tuning pre-trained language models for few-shot learning. Because Adapter networks have few tunable parameters, the technical novelty of the proposed method is limited. However, I think the detailed experimental analysis in this paper would be useful for the community.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a method for few-shot learning with parameter-efficient finetuning. They algorithm is based on prompt-based finetuning due to its superiority in few-shot performance than classic finetuning. It employs self-training, a method which generated pseudo-labels for unlabeled examples to improve few-shot performance. In order to reduce number of trainable parameters during finetuning, they used adapter layer finetuning, which employs small fully-connected layer. The evaluation results indicate better performance compared to previous semi-supervised algorithms based on using Roberta-large as backbone encoder.",
            "main_review": "Questions:\n1- Figure 1: why number of trainable parameters for prompt tuning is same as classic finetuning. prompt tuning uses the embedding parameters only during finetuning.\n\n2- I think the author has mixed the definition of prompt tuning and prompt-based finetuning. in prompt tuning, discrete or continous, only the embedding layer parameters are trained. in prompt-based finetuning, all model parameters are tuned. \n\n3- why student model is not initialized by teacher model rather than RoBerta base? isn't this a better starting point for student model?\n\n4- Section 3.1: it is mentioned that no dev set is used for few-shot experiment, and the self-training is used for M=6 loops. for teacher and student training, what is the criteria for training? is it fixed number of epochs or training steps?\n\n5- Section 3.2: the author mentioned this is the first time that adapter layers are used for few-shot setting. However, there are several works that have studied the role of adapter layer in few shot settings, below are few of them:\n- AdapterHub Playground: Simple and Flexible Few-Shot Learning with Adapters\n- UserAdapter: Few-Shot User Learning in Sentiment Analysis\n\n6- the notation in equation 6 is confusing. the derivative is with respect to \\epsilon_{i} to compute the weights of sample x_{i}. however, the loss in the nominator is the sum of loss values over x_{i} samples from D^{t}. Is D^{t} the meta validation split?\n\n7- equation 6: gradient of validation loss for \\epsilon_{i} are computed at \\epsilon_{i}=0. why the gradient is computed as weight of unlabeled sample i considered to be zero?\n\n8- section 3.3.2: it is mentioned that to prevent label leakage to student, at each self-training iteration, the student model is re-initialized. however, for weighting the pseudo-label examples, the labeled data sed D^{t} is used as meta-validation set which cause the label information to be leaked to student model during self training! Moreover, the KD warmup also cause label information to leak to student model. what is the goal of preventing the leakage?\n\n9- Two baselines which use prompt-based finetuning with self-training, with and without re-weighting, but tuning all model parameters are missing in evaluation. In other words LiST algorithms by tuning all model parameters.\n\n10- Table 2: in calculating number of tuning parameters, unlabeled examples weights (\\epsilon_{i}) are not considered as tunable parameters! PromptST has less tunable parameters due to no re-weighting, but it is shown it has equal number of parameters with LiST. number of weights is smaller than number of parameters by1 or 2 order of magnitude, but it should still be accounted.\n\n11- since the proposed method is based on using self-training and adapters, it would be another ablation to try it on ClassicFN approach. This way, the benefits if self-training along with adapter can be better understood in different finetuning settings. UST and MetaST are the self-supervised variant for classical finetuning, but their approach in using re-weighting and adapter are different\n\n12- Table 2: all algorithms are compared using Roberta-large as backbone only. It is necessary that choice of different backbones, such as Bert-base, Bert-large and Roberta-base to be evaluated to better understand the performance gap between algorithms\n\n13- Figure 5: LiST is using Roberta-large as backbone, and it is compared with Bert and Roberta models. LiST with different backbones, including Bert and Roberta for base and large should be reported to better understand the benefits of the proposed algorithms with respect to backbone size.\n\n14- Figure 5(c): the author compared LiST with Bert and Roberta in terms of number of tunable parameters. However, the performance of LiST is dependent to its backbone. Therefor, this is not a fair comparison.\n\n15- Section 4.3: the author mentioned that since Roberta-large outperforms other models in classical finetuning, therefor they choose this model as LiST backbone, for all their comparison with other baselines. However, the performance gap of these algorithms should be compared with different backbone sizes and models.\n\n16- Figure 5: what is the reported numbers? are they mean accuracy? it would be beneficial to include standard deviation in this figure too\n\n17- Table 5: Other lightweight methods are compared with LiST, while they are not using any self-training, which makes this comparison unfair. \n\n18- Table 6: what is the performance when student is initialized with teacher?\n\n19- Table 4,5,6: all the ablation studies are presented for K=30 few-shot setting only. However, it is unclear about the performance on other few-shot cases, i.e. 10 and 20 \n\n20- when referring to K=30 labeled examples, is it number of examples per class? or total number of labeled examples?\n\n21- section 5: the author mentioned that a contemporary work used adapter in full shot setting to show its usefulness in few-shot settings. however, the design of LiST adapter was based on K=30 few-shot setting only (Table 4). Does the choice of adapter has same performance in K=10 and 20 settings?",
            "summary_of_the_review": "The proposed algorithm is the combination of adapter layer tuning with self-training. The proposed algorithm is tested on prompt-based finetuning due to its better few-shot performance to classic finetuning. The performance comparison of LiST with other self-supervised learning approaches are based on Roberta-large backbone only.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}