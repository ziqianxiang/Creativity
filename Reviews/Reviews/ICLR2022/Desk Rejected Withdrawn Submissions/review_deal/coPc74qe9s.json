{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "Overview: This work proposes a unified neural architecture representation that enables implementing on different neural architecture search spaces. A generalizable neural predictor is presented by integrated GNN, self-attention as well as contrastive learning. The experiment results show that the proposed method performs better than other transferable zero-cost proxy baselines on NasBench101, NasBench201, and NasBench301 in terms of Spearman correlation coefficient. EA algorithms has a better search performance with the proposed method than Synflow on NasBench101, NasBench201, and NasBench301. \n\n\n",
            "main_review": "Pros:\n+ The results from predictor is promising compared to zero-cost proxy alternatives in terms of Spearman correlation coefficient on popular NAS benchmarks (NasBench101, NasBench201, and NasBench301). \n\n+ Unify the neural architectures’ encoding approach among different NAS space. \n\n+ A novelty way to learn a neural predictor. \n\n+ Paper is well written. \n\nCons: \n- This work only reports cifar10 results while popular NAS benchmarks (e.g. NasBench101, and NasBench201) are available to cifar10, cifar100, as well as ImageNet. \n\n- This work only did experiments on NAS benchmarks but not a real NAS problem. \n\n- Based on the last comment, I would challenge that if predictor-based methods indeed work on large-scale NAS space (i.e. 10^20 architectures in total). While the size of NasBench301 is large enough but their accuracies are based on a surrogate model as well. \nOne-shot NAS methods (e.g. https://arxiv.org/abs/2004.13431, https://arxiv.org/abs/1904.00420, https://arxiv.org/abs/2006.06863 )  is very cheap but effective to predict the architectures in the search space. It only trains one or few super-nets as a proxy model to estimate the sampled architectures rather than using 10~ or 100~ architectures as the training data (as author also mentioned in the paper, “we train several vector representations over a small set of hyper-parameters, this is, in\nfact, the bulk of the running time.”).  I would suggest the authors to compare the proposed methods with one-shot NAS methods in real NAS problems with different metrics (e.g. accuracy and search time). \n\n- Code is not available.\n\nOther suggestions: \n\n1. To compare the ranking correlations on Nasbench101 and Nasbench201, I would like to suggest the authors to use the entire dataset instead of sampling part of architectures as the represents, which may cause some biases over the comparisons.\n \n2. To test the search performance with EA algorithms, why only compare to Synflow but other baselines mentioned in Table.1 (e.g. Jacov and Grasp)?  ",
            "summary_of_the_review": "please see above",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a GNN-based performance predictor for NAS. It first constructs a general-purpose computation graph with primitive operators, and then uses contrastive learning to train the computation graph. Experiments are done on NAS-Bench-101/201/301, and results show it has better generality than previous methods.",
            "main_review": "Overall, the approach proposed in this paper is intuitive and interesting, but it is unclear whether such approach can be generalized to much larger search space and networks.\n\n— Strengths —\n\n1. Good idea to apply GNN for performance predictor. In particular, by maintaining primitive operations, instead of group operations, it be can be potentially generalized to more networks.\n2. The constructive learning procedure is interesting. As constructive learning is mostly used for image domain, it is not obvious how to expand it to GNN. This paper proposes some practical techniques to make it work for GNN.\n\n— Weaknesses —\n\nMy main concern is that it is not clear whether such approach can be applied to larger search spaces and networks. All experiments in this paper is based on NAS-Bench-101/201/301, which are all from the same family of cell-based NASNet. Recently, many state-of-the-art methods, such as OFA/FBNet/MobileNetV3/EfficientNet, tend to much larger and complex search space with very different operations than NASBench. I would be interested in whether the proposed approach can be also applied to those search spaces and networks. It is particularly important here because the main claim of this paper is its “general-purpose” predictor.\n\nA few minor questions:\n1. The second paragraph in section 5.1 is quite confusing. Could you clarify how many labelled networks (known accuracy) are needed to train your predictor?  It says “source families are fully labeled”, do you mean you make use of all models in NASBench? That would be too expensive in practice?\n2. I generally have doubts about performance predictor. For NASBench, surely you can have all the ground  accuracy truth for tons of sampled networks, but for other search spaces and unknown, it is not clear why model accuracy can be easily predicted. Could you provide some intuitions?\n",
            "summary_of_the_review": "1. Interesting approach to build performance predictor for NAS.\n2. It is not clear whether such approach can be generalized to larger search spaces and networks.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose a general-purpose neural predictor for NAS, which uses a computation graph to represent any given candidate networks. What's more, in order to train the universal embeddings of computation graphs, the authors exploit GCN, self-attention and contrastive learning methods to conduct a semi-supervised graph representation learning. Experiments show the effectiveness and exceptional transferability of the proposed predictor.",
            "main_review": "Strengths:\n1.\tThe authors propose a transferable predictor based on computation graphs, which incorporates GCN, Self-Attention and Contrastive Learning. And then they verify its effectiveness through experiments.\n2.\tSpearman’s rank correlation coefficient results show the effectiveness of fine-tuning, which achieves promising transferability to the target search space.\n\n\nWeaknesses:\n1.\tThere is an assumption “graphs are topological close should have also comparable performance”. Nevertheless, it may not hold for architectures. For example, by only modifying one node/edge (add or remove skip connection), the architecture may incur significant performance drop. Thus, it is questionable to use spectral distance to evaluate the similarity.\n2.\tIn Table 3, besides the number of queries, it would be better to compare the real search cost (e.g. in terms of GPU days).\n3.\tThis paper only considers small search spaces, e.g. NASBench. Can the proposed method be used in DARTS and MobileNet search spaces? It would be better to report the results on these spaces.\n4.\tIn Section 4.2, the authors claimed that “our model selection approach is very stable”. However, there seem no empirical results to support it.\n5.\tSince this paper focuses on learning predictors, several recent related work [a,b] should be discussed and/or compared.\n\nReference:\n[a] ReNAS: Relativistic evaluation of neural architecture search. CVPR 2021.\n[b] Contrastive Neural Architecture Search with Neural Architecture Comparators. CVPR 2021.\n\n",
            "summary_of_the_review": "It is interesting to develop a general performance predictor. Nevertheless, the assumption behind the similarity metric is questionable. Moreover, It is unknown whether this method is able to yield good results in large/real search spaces.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes to use GNNs to learn structures of convolutional networks in nas101/201/301 search space in a unified way using SimCLR then fine-tune with EA.",
            "main_review": "Strengths: \n\n1). Well-written. It is easy to follow the idea.\n\n2). Encode more features such as input/output channels, input/output height/width sizes, convolution kernel size, bias in Nodes and extra operators like multiplication and concatenation are novel. \n\n3). Ranking and Downstream search results are promising. \n\n\nWeakness:\n\n1). DARTS (NAS301) can be simply represented in the same way as NAS101/201 by additionally adding a summation/multiplication/concatenation node to make nodes to represent operations and edges to represent data flow. If this work is used to learn unified vector representations of NAS101/201/301, if it is a little bit exaggerated to call it a 'general purpose' predictor?\n\n2). One important factor in computational graph is its single-directional flow property (e.g. directed, acyclic, single-in-single-out), see https://arxiv.org/pdf/1904.11088.pdf, https://arxiv.org/pdf/2102.07108.pdf, https://arxiv.org/pdf/2007.06559.pdf for a take. I didn't see author[s] explicitly model this property in Section 3, instead, it sounds more like a structure-aware encoding method. From this perspective, why call it a 'unified architecture representation for computation graph'?\n\n3). In Table 1, it seems unfair to compare with zero-cost methods, since there are no pretraining time for it. Additionally, in this work author[s] choose to perform a few-shot tuning to boost ranking performance, which further increases the upstream time. In addition to report zero-cost proxies, author[s] should consider adding pretraining time and ranking results with different pretrained embedding methods as well.\n\n4). Why only a single downstream search method (EA) is provided? It would be better to add BO, LS, etc.",
            "summary_of_the_review": "This paper has a good motivation, well written. However, there are flaws between claims and method design. I also suggest author[s] evaluating more downstream search methods to evaluate the proposed embedding method.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}