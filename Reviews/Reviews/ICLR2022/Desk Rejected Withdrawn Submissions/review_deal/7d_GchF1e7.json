{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper introduces the structured pruning method for Transformer-based language models, such as BERT. The key idea of the method is to utilize the variance of the output neuron of each linear block and prune out the neurons with a small variance. This row-wise pruning is especially beneficial for modern hardware accelerators and shows a considerable speed-up of about 30% in GPU and little to no performance loss. In addition, the authors suggest using this pruning technique during the pre-training phase. Experimental results show that their pruned model achieves comparable performance on downstream tasks with 50% sparsity.",
            "main_review": "(1)\tI think the (logical/theoretical) support on why the variance is a good metric is somewhat unclear. It is intuitively acceptable, but can you provide some connections between the variance and importance? Not directly related, maybe “Variational Dropout Sparsifies Deep Neural Networks” (2017) paper can give some idea.\n\n(2)\tThe important aspect of the paper, that the pruning during pre-training is better than pruning during fine-tuning, seems to be not supported by experimental results. Pre-training requires much more resources than fine-tuning from the already trained baseline, so it should be clearly explained that pre-training-based pruning has some advantages.\n\n(3)\tOnly 50% pruning sparsity results are presented, which is OK but not sufficient. The trade-off of the sparsity and speed/performance would be helpful. I believe the performance comparison needs extra training (which I don't want to force), but speed comparison can be somewhat easier.\n",
            "summary_of_the_review": "This paper presents a clear idea, and the results are interesting. However, I think the support for two statements (variance-based pruning/pruning during pre-training) is not solid.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a novel iterative row-pruning method in the BERT models pre-training. Such a method selects rows in weight matrices to be pruned based on the corresponding output neuron’s variance information. Compared with conventional pruning methods, pruning by rows produces regular and dense architecture that are easy for hardware optimization. ",
            "main_review": "Strength\n1) The paper is clearly written and easy to follow. \n2) The studied topic is of great significance — 1) Pruning in pre-training stage can produces a sparse model readily for all downstream tasks. Also, the weights changes in the pre-training stage may provide more meaningful signals to guide pruning.  2) Pruning a “regular-shape” dense model is of great practical importance. \n3) The idea of pruning in unit of row based on the neurons’ responses is novel. The variance-based measure seems to be simple and effective.\n\nWeakness\n1) The amount of experiments seems to me a bit insufficient. Maybe the authors can present results on different scales of models, wider range of pruning ratios, and more downstream tasks. \n2) Additionally, it would be nice to have comparison with some \"distillation during pre-training\" baselines (e.g., MiniLM, DistilBERT), as they also conduct compression at the same ratio in the pre-training stage. It would also be great to see comparison with \"pruning during fine-tuning\" baselines, which may better demonstrate the advantages of pruning during pre-training than during fine-tuning.\n\nQuestions\n1) The motivation to use variance-based approach is a bit unclear to me. Why using variance is suitable for pruning in the pre-training stage? Does it also work for fine-tuning stage as well? Have the authors tried existing types of measure to determine the row importance (e.g., first-order or second-order gradient information)?\n2) Section 3.2, how do the authors determine the pruning percentages for different types of matrices? Are they set to be the same or different depending on their “roles”? I am asking because an important motivation of row-pruning is to produce a “regular” architecture. \n3) Section 3.4 experiments, as the pruning is conducted on the fixed model, do the authors use the exact variance (directly summing over all batches in Eq. (3)) instead of the moving average?\n4) Section 4, it seems that the authors conduct pruning in the very early stage of pre-training, have the authors observed any advantage for this early schedule over other schedules in pre-training?\n\n",
            "summary_of_the_review": "The topic of how to better conduct pruning during pre-training is of great significance, and the row-pruning idea is novel. My main concern is about the limited experiment results. I would be happy to raise my rating if the author can provide more baselines and address my question 1).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a structured pruning regime, where they prune entire rows that correspond to output neurons with the lowest variance. The intuition is that low variance outputs stay the same across batches and can be replaced by their mean value. The authors apply this regime in BERT models and demonstrate the usefulness of this method achieving competitive scores in GLUE and SQUAD tasks.",
            "main_review": "Pros:\n\n+ A practically viable pruning scheme that can be used to speed up model training/inference\n+ A paper is clear, the intuition is sound and the sparsity analysis supports the intuition \n\nCons:\n\n- Activation-based pruning is not a novel idea. For example, it is explored in Polyak & Wolf, 2015, “Channel-Level Acceleration of Deep Face Representations” and is mentioned in Li et al., 2017, “Pruning filters for efficient convnets”. The authors cite the latter but do not compare to it. The abovementioned papers also explore using variance for pruning. As the authors mention, these papers apply pruning to convolutions and not the dense models but dense model application is not by itself a sufficiently novel contribution.\n- The authors do not experimentally compare their method with other pruning techniques. For example, the authors mention the magnitude-based approaches but never compare the performance of their method with magnitude-based ones. Another good comparison candidate is network trimming described in Hu et al., 2016, “Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures”. The network trimming regime prunes the outputs that are consistently zero which means that network trimming will tend to prune the same outputs as variance pruning. This may make network trimming and variance pruning identical in practice.\n- The experiment design is also lacking - the 50% sparse model outperforms the full model in some cases but it’s hard to attribute this to the pruning regime as there’s no comparison with the 50% smaller model.\n\nSuggestions:\n1. The paper lacks a thorough literature review on other pruning techniques that are based on activations. The claim to novelty needs to be based on top of these papers.\n2. The paper will benefit from explicitly enumerating the main contributions. As of now, it’s unclear what authors consider their main contribution.\n3. (Table 1) There are no experimental results on a 50% smaller model that was trained from scratch without pruning. This makes it impossible to judge how good the pruning results are.\n4. (Table 1) There are no experimental results comparing the proposed pruning technique with previously described pruning techniques.   \n",
            "summary_of_the_review": "Although pruning based on activations’ variance sounds practically viable, the novelty of this work is uncertain. The proposed technique is similar to what is already described in the literature. The paper will benefit if updated with the focus on novelty.  \n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a structured pruning algorithm that can be used to accelerate BERT on any hardware. They argue that existing structured pruning techniques use importance criteria extracted from the weights themselves. They argue that this method of importance criterion have several limitation, for example, it does not account for marginalization of large weights when the input to them is very small or that the output of a neuron with large weight may not change between inputs. Further some other structured pruning techniques while effective, create networks that are easy to accelerate on HW Accelerators but not using simple BLAS operations that are more widely accelerated. In order to solve this issue, the authors propose a variance pruning algorithm, that prunes an entire row of a matrix. The pruning algorithm looks for variance of the output neuron across batches. If an output neuron does not change, then it is deemed unimportant and pruned away. The resulting matrix can now be compressed to a smaller dimension and can use the same BLAS algorithm as before for execution, albeit using a smaller dimension. Further, they propose using this technique during the pretraining phase of BERT training rather than finetuning. Results indicate that the model can be pruned by 50% without any loss in accuracy on downstream tasks. The smaller bert large model also leads to better throughput and performance.",
            "main_review": "Strengths:\n1. The work targets pretraining phase of BERT, an important problem to accelerate.\n2. Results suggest no impact on accuracy at 50% sparsity\n\nWeaknesses:\n1. If the goal of the paper was to exploit structured sparsity using BLAS operations, then a simple approach would be to use group lasso. Group lasso has shown significant effectiveness for LSTMs (https://arxiv.org/abs/1709.05027) and should function as a baseline to compare against.\n2. The authors argue that magnitude pruning is a poor criterion for pruning and yet provide no statistics or comparison to justify their claim. \n3. The authors say that some existing structured pruning techniques are only useful on specialized HW accelerators. However the authors dont compare against a body of existing work achieving similar goals as this paper (See point 1 above and points 2 and 3 in Questions)\n4. The justification of when to prune also seemed adhoc. They use Guo et al. (2021) and Wu et al. (2020)  as examples of prior work on which they base this decision. However, explanations of why both these papers indicate pruning during pretraining are hard to follow. For example, the authors claim that \"According to their results [Guo et al. (2021)], the fact that particular weights that have changed more than others during fine-tuning still does not indicate their importance, as their significant adjustment was done in the earlier pre-training phase..\". However the premise of Guo et al. (2021) is very different and more about encouraging parameter sharing across tasks once a model is pretrained. Additionally, I was not able to find the claim author laid out in that paper. Similarly, using Wu et al. (2020) as a justification to decide when to prune also seemed like a stretch. \n\nQuestions:\n1. Pruning criteria is very similar to ones used to prune convolutions in https://arxiv.org/abs/1611.06440 (Section 2.2, Activations). Can the authors comment on that?\n2. Structured pruning that leads to reduction in matrix multiplication size and performance improvement on any hardware is also proposed here - https://aclanthology.org/2021.acl-long.171.pdf. This technique is also applied during pre-training phase. Can the authors compare their results with them?\n3. At 50% compression, low-rank matrix factorization can also be an effective method for fast pretraining, did the authors compare with this alternative?\n4. How does pruned neurons change for the same init but different shuffle of the dataset? If the variance criterion is indeed pointing to irrelevant features, then a different shuffle should pick the same neuron\n5.What is the impact of pruning schedule on accuracy? Why was the current pruning schedule of pruning every 500 steps for the first 2500 steps of pretraining chosen?\n6. I had trouble following Figure 3, if it is pointing to sparsity level in each FFN and MHA, then each block should only have one color, with color representing the sparsity in that block. If the figure represents active/ non active neuron, then a block should be a mix of only two colors. Can the authors explain Figure 3 a bit more?\n7. Can you augment Figure 2 to include L1 and L2 as a criterion also and compare that with variance pruning?\n\n\nSuggestions for improvement:\nAddressing the weaknesses above will help a lot in strengthening the paper. Especially, \n1. Comparison against more simpler baseline (Weaknesses, point 1 )\n2. A more detailed justification of limitation of existing structured pruning techniques. Maybe adding a table of prior work and the importance criterion used by them will be helpful for readability\n3. More ablations that provide more insight into the technique or justify its usage (eg - Questions, point 4) \n4. Figure 3 is hard to follow, some clarification and more explanation around it would be helpful for the reader.\n\n",
            "summary_of_the_review": "I think the main premise of the paper is on a weak foundation. Their claims about limitations of existing structured pruning techniques being useful on only a small subset of HW is hard to accept. Further, their argument to use variance pruning as opposed to other possible way of doing structured pruning is also weakly justified. The paper further does not evaluate against relevant existing work or simpler possible alternatives. Given these issues, I recommend a weak reject rating for the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}