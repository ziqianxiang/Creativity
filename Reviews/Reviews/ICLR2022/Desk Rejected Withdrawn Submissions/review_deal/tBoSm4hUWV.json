{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes WaveMix, a method to mix spatial information for computer vision models. At its core, this method applies 2D Discrete Wavelet Transform (DWT) on the convolution-extracted feature map. It applies several levels of DWT in parallel to generate a multi-resolution representation, and use convolution to process them and then concatenate them together. ",
            "main_review": "Strength: This paper explores a meaningful idea to aggregate spatial information for computer vision models. Spatial information aggregation is an important component, and existing solutions including convolution, self-attention, Shift operation, MLP mixing, etc. This paper, however, proposes a meaningful alternative to existing ideas. \n\nWeakness: The experimental section of this paper is too weak. \n1/ The main experiment is conducted on the CIFAR-10 dataset, which is a extremely small dataset, that recent works, especially transformer-based, only use it as a fine-tuning evaluation or zero-shot evaluation set. \n2/ Results achieved on CIFAR-10 datasets are too low to conduct any meaningful comparison. As a reference, Convolutional models such as ResNet20 could easily achieve 92% top-1 accuracy on CIFAR-10 [1] back in 2016. Results reported in this paper are far below that range. Also, CIFAR-10 has only 10 classes, it is not very meaningful to report top-5 accuracy, as in Table 1. \n3/ The ViT model's performance reported in this paper is too low. Obviously, this paper didn't use the best setting to experiment on ViT models. Because of this, the conclusion of this paper is not convincing. \n\n[1] https://arxiv.org/pdf/1512.03385.pdf",
            "summary_of_the_review": "A meaningful alternative method for spatial information mixing for CV, but weak experiments. Therefore, clear rejection. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper presents a new basic operation for vision recognition based on multi-level Wavelet transform. The model uses the two-dimensional discrete wavelet transform to mix tokens and aggregate multi-resolution information over long distances. Experiments are conducted on CIFAR-10 and Tiny ImageNet to verify the effectiveness of the method. ",
            "main_review": "The idea of using multi-level Wavelet transform to mix spatial information and using the operation to design a backbone model is new. However, I still have quite a few concerns about this paper:\n\n- The motivation of using multi-level Wavelet transform is not very clear. According to the Introduction, the authors want to design a new operation to address the quadratic complexity of self-attention. However, there is no clear analysis of the complexity of the new operation. I think the GPU consumption provided in Table 1 and 2 can only reflect a certain aspect of the complexity. The authors also claim that the proposed model has \"the right inductive bias to utilize the 2-D structure of an image\". I think one of the key contributions of vision transformers is that it provides a new way to learn visual representation with fewer handcraft designs and inductive biases. If the proposed method is an alternative to ViTs, I think it would be better to use fewer inductive biases. If the method want to better to use inductive biases, I think the true baseline of this method should be CNNs or other models instead of ViTs.\n\n- Many important details of the proposed model and the baseline models are missing. For example, the detailed architectures of the WaveMix models are not provided (e.g., the number of layers, the dimension of each layer, etc.). It is mentioned that a feedforward layer is used in the model, but I cannot find the layer in Figure 1.\n\n- The experiments are not convincing. Since the authors use modified baseline models in the experiments, I think it is necessary to provide the training details and model configurations of all models. It is confusing why the MLP-mixer model has 16x parameters compared to ViT. Since the proposed model also uses 2D Conv and Transposed Conv, I think it is better to compare the results with conventional CNNs. Since many details of the experiment settings and baseline models are missing, it is hard to judge the effectiveness of the model.\n\n- There are many confusing descriptions. For example, “We used linear and convolutional feed-forward layers with 1x1 kernel in our experiments, as the number of parameters of linear layer and 1\u00021 convolutional layer was almost the same”. I think the linear layers used in ViT is equivalent to 1x1 convolution. \"WaveMix performed 236% better than a 4-layer CNN which was trained on this dataset\". But I cannot find the results of the 4-layer CNN in Table 1.",
            "summary_of_the_review": "Although I think the basic idea is somewhat new, there are quite a few missing details of the method, which makes it impossible to judge the actual contribution of this work. The experiments look unconvincing.  The motivation is not clear. I think this paper clearly is not ready for publication. Therefore, I would like to recommend rejecting this paper.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a new architecture for deep neural networks applied to computer vision. The idea is to use a multi-scale wavelet transform on convolutional feature maps to allow for information to be integrated over large image areas. \n\nThis has potential advantages over existing architectures: In CNNs, similarly large receptive fields require either very deep networks, or very large (and parameter-heavy) convolutional filters. In Vision Transformers, the receptive field spans the whole image by design, but relies on costly all-to-all self attention.\n\nExperimentally, the paper compares the proposed architecture to recent alternatives in the regime of very small (4-layer) networks and small datasets (CIFAR-10 and Tiny-ImageNet). In this regime, the proposed architecture is shown to perform better than the others, while requiring fewer parameters and less GPU memory. The dependence of accuracy on model depth and dropout are also analyzed.",
            "main_review": "This paper provides a well-reasoned motivation for the use of wavelet transforms in deep neural networks, as well as promising proof-of-concept experiments. However, the scale of the experiments is too small to draw general conclusions about the usefulness of the approach. Further, some of the comparisons to alternative architectures are flawed. Therefore, this study is incomplete and much more work is needed before general conclusions can be drawn.\n\n### Specific concerns:\n\n1. **Scale of experiments:** The experiments compare very small network architectures (4 layers only) on very small image classification datasets (CIFAR-10 and Tiny ImageNet).\n    1. If the goal is to make a general computer vision contribution, it is unclear why the experiments are restricted to a regime that is so far away from current standard baseline models, not to mention the state of the art. For example, according to the [official GitHub page](https://github.com/google-research/vision_transformer#expected-vit-results) , a ViT-B/16 model pretrained on ImageNet-21k (for which checkpoints are publicly available) can be fine-tuned to a CIFAR-10 accuracy of 98.59% in 17 minutes on A100 GPUs. This should be achievable in most research settings (e.g. using freely available Google Colab resources). It is unclear if the results obtained on tiny network variants, without pretraining, generalize to realistically useful sizes.\n    2. If the goal is to provide a specialized architecture for extremely resource-constrained (e.g. embedded) environments, a more detailed characterization of the performance of the proposed architecture is necessary. What are the FLOPS and wall-clock runtime? What is the inference latency? How does performance scale on different architectures (CPU, GPU)? \n2. **Choice of baselines:** Given the small scale of the experiments, the choice of some of the baselines is flawed. Specifically, Vision Transformers and MLP-Mixers were developed and tuned to provide state-of-the-art results when trained and evaluated on large datasets, with a focus on scalability to huge model sizes. Simply reducing the size of these models to four layers, and training them from scratch on a small dataset, makes no sense. These results likely provide no insight into the relative performance of these architectures in the regime they were designed for.\n3. **Generality of claims:** Given the small scale of experiments, the claims made in the abstract and paper are overly general. For example:\n    1. The abstract claims that the proposed method can “significantly reduce computational costs and memory footprint without compromising on image classification accuracy.” To warrant such a general claim, state-of-the-art results on larger-scale datasets (e.g. ImageNet) would be necessary.\n    2. The introduction claims that “in low data regime, transformer models are shown to perform poorly compared to convolutional models as they lack the proper inductive bias, and hence require more data to model the 2D image features.” Recent work shows that this is not true, and that ViTs and MLP-Mixers can be trained from scratch to better performance than ResNets (https://arxiv.org/pdf/2106.01548.pdf).\n",
            "summary_of_the_review": "This paper provides a well-reasoned motivation for the use of wavelet transforms in deep neural networks, as well as promising proof-of-concept experiments. However, the scale of the experiments is too small to draw general conclusions about the usefulness of the approach. Further, some of the comparisons to alternative architectures are flawed. Therefore, this study is incomplete and much more work is needed before general conclusions can be drawn.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The authors introduce non-learnable, discrete wavelet transform blocks in order to mix information at different stages of a neural, image classification model. They show that it works reasonably well when compared on CIFAR10 and when compared to models like ViT, FNet, MLP Mixer and Hybrid ViN, while keeping a low memory footprint.",
            "main_review": "\nPros:\n- Exploring wavelet transforms within neural architectures for vision is a very interesting direction that should be explored more.\n\nCons:\n- Baselines: It is important to understand that ViT was never intended to work well on small data and it was even shown in the original paper that it severely suffers when reducing data. So using ViT as a baseline here is not very useful at all. There is a plethora of ViT derivatives now that might even work well on small scale data such as CIFAR10 although most of these are also not intended to work well on those without pretraining.\n- Results: Accuracies of not even 80% on CIFAR10 are quite underwhelming. Judging from papers-with-code, top architectures achieve well over 95% on CIFAR10, even without pretraining.\n- I am not convinced that the methods would scale well to reasonably sized images and datasets. Training on at least imagenet sized data is required.",
            "summary_of_the_review": "This paper is unfortunately not ready for publication so I kept the review brief. The initial studies conducted should be considered preliminary at best. The baselines used for comparison are not at all appropriate for the datasets used. The image sizes and datasets are extremely small, with imagenet tiny (64x64 pixels) representing the \"larger\" dataset. Hence it is not possible to draw any conclusions from the paper wrt to the introduced architectural changes at this point.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}