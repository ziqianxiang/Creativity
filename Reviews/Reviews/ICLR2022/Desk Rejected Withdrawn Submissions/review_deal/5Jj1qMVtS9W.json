{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The author proposed a new method for creating rank-consistent predictions in ordinal regression (OR) problems that can easily be plugged into any neural network architecture. The architecture is based on CORAL (Cao et.al, 2020), with the difference that it estimates conditional probability of p(y^{(k)}=1|x, y^{(k-1)}=1) rather than directly estimates p(y^{(k)}=1|x). In this way, the output is always guaranteed to be rank-consistent by the rule of chain probability, whereas CORAL only guarantees the output of the model to be rank-consistent in optimality. The proposed model optimizes a sum of binary cross-entropy loss, which compares if the label is greater than something and the output of the probability from the model. The author then demonstrated the effectiveness of the model in a range of real-world data.",
            "main_review": "Strengths:\n- The proposed model is interesting since the output of the model is always rank-consistent even before training the model, an improvement to the CORAL model.\n- The author provides a detailed analysis of the rank-consistency claim, as well as analytical comparisons with the CORAL algorithm.\n- The author shows the performance improvements over the baselines models in several datasets.\n\nWeakness:\nI have a few concerns and questions regarding the paper:\n1) The paper is based on the CORAL algorithm with the modification in the probability estimation. However, the author's assessment of the CORAL algorithm is a bit unfair, in my opinion.\n   - Rank consistency. \n\n      In the discussion, the author mentioned that the CORAL algorithm might produce rank consistency since it is not expected to reach global optimum and requires a post hoc check. It is true that the claim in the CORAL paper is only in the case where the parameter of the model is the optimum solution of the optimized objective. However, for neural networks training, local optimum solutions are oftentimes good enough. Furthermore, the CORAL's authors showed that in practice, the CORAL model produces 0 rank-inconsistent solution in their experiments.\n   - Expressiveness of the model. \n\n      The author claims that the CONDOR model is more expressive than the CORAL model since the weight parameter is shared among all classes in the CORAL model. This assessment may be true for the case of the linear model. But for the case of neural network learning, the expressiveness power of the model is done in the previous layers of the networks, so that intermediate latent variables before the loss layer should be easily separated in a linear fashion. Therefore, when talking about the expressiveness of the neural networks model, we need to take into account the full model architecture, not only the loss layer.\n2) For the evaluation metric, the author claims that the sum of binary cross-entropy loss (BCE) is the most appropriate metric to evaluate ordinal regression (in comparison with the MAE and EMD losses) since both MAE and EMD assume uniform spaces between rank whereas BCE does not. Furthermore, the author claims that the BCE is the only metric that directly evaluates ordinal performance, particularly since the uniform spacing is not compatible with most of the real-world ordinal regression problems, as the author said.\n\n   This assessment is not totally accurate either. It is true that the MAE assumes uniform spacing in the metric calculation, but the BCE is not free from the assumption either. The BCE metric that the author proposes (Eq. 4) is a weighted sum of binary cross-entropy loss over the OR's binary predictions. When the weight \\lamda_k is set uniformly (e.g. \\lambda_k = 1) as in the paper (the end of page 3), then it assumes the equal contribution of the binary loss (log loss / ce loss) to the total metric. This uniform contribution assumption is also backed into the BCE metric. In fact, the MAE loss can also be constructed as in Eq 4, where we perform a uniformly weighted sum over the binary zero-one loss (1 - accuracy) of the binary classification sub-tasks.\n3) In the experiments, the author mainly compares the proposed models and the baselines against the BCE metrics. This may give a bit unfair advantages to the CONDOR algorithm since it directly optimizes the metric whereas other models do not. I suggest the author use MAE as the main metric for comparison as it is the popular metric to use in ordinal regression problems.\n4) The naming of the loss and the metric as binary cross-entropy (BCE) is a bit confusing since it usually refers to the binary classification setting. The loss that the author is using is not exactly a BCE loss but rather a sum of BCE losses. I suggest the author rename it to avoid confusion.\n5) There are a few ordinal regression papers that are missing from the reference:\n    - Jason D. M. Rennie and Nathan Srebro. Loss functions for preference levels: Regression with discrete ordered labels. In Proceedings of the IJCAI Multidisciplinary Workshop on Advances in Preference Handling, pages 180–186, 2005.\n    - Wei Chu and Zoubin Ghahramani. Gaussian processes for ordinal regression. Journal of Machine Learning Research, 6(Jul):1019–1041, 2005.\n    - Mark J Mathieson. Ordinal models for neural networks. Neural networks in financial engineering, pages 523–536, 19\n    - Rizal Fathony, Mohammad Ali Bashiri, and Brian D. Ziebart. Adversarial Surrogate Losses for Ordinal Regression. In NIPS, pp. 563-573. 2017.\n    - Yanzhu Liu, Adams Wai Kin Kong, and Chi Keong Goh. \"A constrained deep neural network for ordinal regression.\" In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 831-839. 2018.",
            "summary_of_the_review": "The paper proposes an interesting approach in the rank-consistent ordinal regression problems, which provides an improvement over the CORAL model. However, some of the claims made by the author are not fully validated. Therefore, I recommend a weak reject.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work describes a neural network architecture rank-consistent predictions for ordinal outcomes. This architectures is shown to satisfy a certain universality property. The network was shown to perform well relative to two alternative approaches in numerical experiments based on both synthetic and real data.",
            "main_review": "This work describes an approach rank-consistent predictions for ordinal outcomes. This approach entails the use of a particular output layer in a neural network and a particular loss function. When paired with a universal architecture, using this layer is shown to satisfy a certain universality property. The network was shown to perform well relative to two alternative approaches in numerical experiments based on both synthetic and real data.\n\n\nI like that the proposed approach is simple and general enough to be used in a wide variety of settings (e.g., images, time series).\n\nHere are a few comments:\n\n1.  Your proposal is a nonparametric variant of the extended continuation ratio model (which allows for the coefficients to depend on the level of the outcome) -- e.g., see Section 13.4.6 in\n\nHarrell, Frank E. \"Regression modeling strategies.\" Bios 330.2018 (2017): 14.\n\nA nonparametric variant of the (non-extended) continuation ratio model was previously presented in\n\nKauermann, Göran, and Gerhard Tutz. \"Semi-and nonparametric modeling of ordinal data.\" Journal of Computational and Graphical Statistics 12.1 (2003): 176-196.\n\nIt's also quite possible that a nonparametric version of the continuation ratio model has been presented previously, though I'm not familiar enough with this literature to be sure.\n\nFrom the perspective of these existing references, your Eq. 3 is equivalent to the hazard formulation used in survival analysis applications. In particular, the ordinal outcome setting that you are considering a special case of a survival analysis setting wherein the survival time is a discrete random variable taking on finitely many values and there is no right censoring.\n\nYour work differs from existing work in extended continuation ratio models in two ways. First, you parameterize the prediction function as a neural network. As in other settings, this added flexibility can allow you to deal with high-dimensional, structured data that traditional parametric or nonparametric approaches cannot handle. Second, the extended continuation ratio model uses a log-log activation function, whereas you use a sigmoid activation. Of these two distinctions, I think the first is far more important. Nevertheless, given that the key ideas described in your work have been in the literature for quite some time, I also think that it is important that you put your work in context by citing these (and other) relevant works.\n\n2. The theory in this paper seems to be fairly limited. Lemma 2.1 is very straightforward, and Theorem 2.2 essentially follows by a Taylor expansion. I wonder whether there is anything else that you can say about your proposal, e.g. whether, in some idealized setting, you can argue that the optimization landscape is favorable relative to that of alternative approaches.\n\n3. As someone with training and experience in analyzing medical data, I find some of the statements made about current standard practice to be inaccurate. For example, in the Discussion,\n\n>> In addition to the theoretical strengths and performance improvements of the CONDOR method, we note that most applied papers simply use categorical classification in their problem settings rather than consider current state-of-the-art methods for ordinal regression.\n\nI specifically link this statement to medical data because, in the very next paragraph, medical applications are referred to as a \"prototypical applied problem domain\". To see that the above quote is inaccurate (at least, when stated so broadly), it suffices to refer to any standard statistics/biostatistics reference on dealing with ordinal outcomes (e.g., the above referenced Harrell reference). In these references, readers are encouraged to use approaches designed specifically to account for the ordinal nature of the data (e.g., proportional odds or continuation ratio models). With that said, many of the approaches proposed in these references are parametric in nature and will not perform well for complex, structured data. Therefore, I would recommend that you adapt the above statement (and a similar one in the second paragraph of the introduction) to better reflect common practice. You can then, of course, also point out other deficiencies in common practice that you are aiming to address.",
            "summary_of_the_review": "My current view is that the contributions of this work are too modest for acceptance to this conference.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies the problem of ordinal regression. This paper proposes a novel method that keeps the rank consistency by simply producing a heterogeneous Markov chain representation of the marginal probabilities (i.e., the product of multiple marginal probabilities). For model training, the used loss is the weighted binary cross-entropies of all the subtasks. Experiments are conducted to validate the effectiveness of the proposed method.",
            "main_review": "Pros:\n1. An interesting method is proposed to keep the rank consistency for ordinary regression.\n2. Both theoretical analyses and experimental results support the proposed idea to some degree.\n\nCons:\n1. I feel that the proposed method is too simple so that the contribution seems not enough. I would agree that a simple yet effective method is better than a complex and effective method. However, I think that this paper only provides a very minor modification. The experimental results also cannot fully support the effectiveness of the proposed method. So I am not sure whether the contribution is enough or not.\n2. As I mentioned, the experimental results cannot fully support the effectiveness of the proposed method because there is a lack of the compared methods and the used benchmark datasets. I think more compared methods should be involved and more large-scale benchmark datasets should be used. Without extensive experiments, the effectiveness of the proposed simple method cannot be demonstrated.\n3. The writing of this paper could be further improved and some expressions are ambiguous. Figure 1 is too big, which occupies too much space. Besides, I feel that this paper looks like an unfinished version because the length of this paper does not fulfill 9 pages.",
            "summary_of_the_review": "An interesting and simple method is proposed and both theoretical analyses and experimental results support the proposed method to some degree. However, the contribution may not be enough and the experiments cannot fully support the proposed method. The writing of this paper could be further improved.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose modifications to a (deep) neural network architecture to enforce rank consistent probability distributions for ordinal regression problems.",
            "main_review": "The paper is well-written, easy to read and technically sound.\nHowever, I have 3 main concerns:\n\n-Novelty: enforcing rank consistency has been present in the literature in many flavours and the authors seem to have missed important work in the literature. In particular, the work [A] in IJCNN seems to explore already all the potential novel ideas of the paper. The authors should clarify the differences to this previous work in the literature.\n\n-The experimental comparison also lacks the comparison with more state-of-the-art methods. For instance, [B, C] promote rank consistency by penalizing deviations in the loss functions. It would be important to extend the comparison to other methods in the literature.\n\n-The adopted datasets/problems for the experimental also raise concerns. In the synthetic dataset, all regions intersect in the origin. Is this an ordinal dataset/problem? Are all problems ordinal as long as you adopt an ‘ordinal loss’? The same concern applies to the MNIST dataset.\n\nReferences\n\n[A] Ordinal Image Segmentation using Deep Neural Networks. Kelwin Fernandes, Jaime S. Cardoso.  IJCNN 2018\n\n[B] Non-parametric Uni-modality Constraints for Deep Ordinal Classification. Soufiane Belharbi, Ismail Ben Ayed, Luke McCaffrey, Eric Granger. 2019\nhttps://arxiv.org/abs/1911.10720\n\n[C] Ordinal losses for classification of cervical cancer risk. Tome Albuquerque, Ricardo Cruz, Jaime S. Cardoso. PeerJ 2021\n",
            "summary_of_the_review": "Although well-written and technically sound, the paper lacks novelty and a strong experimental evaluation.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}