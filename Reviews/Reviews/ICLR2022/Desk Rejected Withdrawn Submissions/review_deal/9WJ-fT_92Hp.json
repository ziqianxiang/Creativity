{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents a reinforcement learning library written in PyTorch designed to efficient and scalabe. The framework is evaluated via the implementation of multiple RL algorithms, multiple distribution methods, and multiple environments.\n\nFinally, the proposed framework is used to achieve state of the art results on the Obstacle tower Unity3D channel.",
            "main_review": "### Strengths\nThe proposed library will likely be useful as it supports many different RL algorithms and distribution methods\nPerformance is impoved on the obstacle tower challenge\n\n### Weaknesses\n\n-\tNo comparisons with other async RL frameworks, i.e. RLLib, ACME, SampleFactory, SEED RL, rlpyt. etc. Even if the proposed library has a better API design, if it is signicantly slower than the alternatives it may not be impactful.\n-\tSuitability for ICLR. While ICLR does have a history of publishing systems papers, those papers typically have technicaly novelty (i.e. centralized inference in SEED RL, ICLR 2020, batched rendering in BPS, ICLR 2021) or enable the community to do something easily they couldn't before (BackPACK, ICLR 2020). As far as I can tell the proposed library doesn't have introduce a novel method nor does it have significantly more algorithms or environments than RLLib or rlpyt. Perhaps their is novel in how the obstacle tower challenge results were achieved, but this isn't clear from the paper\n\n\n\nCitations\nRLLib: https://docs.ray.io/en/latest/rllib.html\nACME: https://github.com/deepmind/acme\nSampleFactory: https://github.com/alex-petrenko/sample-factory (ICML 2020)\nSEED RL: https://github.com/google-research/seed_rl (ICLR 2020)\nrlpyt: https://github.com/astooke/rlpyt\n\nBPS: https://arxiv.org/abs/2103.07013 (ICLR 2021)\nBackPACK: https://openreview.net/forum?id=BJlrF24twB (ICLR 2020)",
            "summary_of_the_review": "I am concerned about strength of contribution of the paper and its suitability ICLR and cannot recommend it for publication in its current form.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces a new RL library, using ray and PyTorch. This library is then used to train high-performing agents in Atari environments, PyBullet environments, and the Unity Obstacle Tower environment.",
            "main_review": "This paper has a number of issues, including the clarity of the exposition and the overall contributions.\n\n- There are many typographical errors throughout.\n- Stylistically, it would be better to report many items (such as hyperparameters) in tabular format, rather than in the exposition, unless it is going to be discussed further.\n- Why is PyTorch being used specifically?\n- Section 2 needs to be improved. In particular, how are the workers described here different from ray workers? The comparison to RLLib in particular is not clear here; it would be much better to do a side-by-side comparison in terms of implementing a specific function/algorithm and show how this library compares.\n- It is not clear how the CPU/GPU coordination is handled, since this is a very difficult problem that is environment-dependent (and so dictates which aspects can be easily coordinated and where bottlenecks may occur).\n- The discussion in Section 3.1 is too abstract without seeing the code.\n- Sections 3.3/4.2: how are policy lag and gradient asynchrony being computed here? Are they actually \"bad\" or a feature of these algorithms?\n- Section 4.1: where are the claimed speed increases coming from? Are the Atari and PyBullet experiments designed to show that this library allows for reasonable performance?\n- Section 4.2: what does \"could be executed in an arbitrarily specified device\" mean?\n- The approach to solving Obstacle Tower appears to rely on a mix of reward shaping and heuristics - how does the library being introduced actually enable this? How is the performance of the agent actually related to the particular library/framework introduced here?\n- Section 5 refers to releasing the code, but there is no code included with this paper.\n- Section 6: code can be submitted anonymously, and it is not clear why the library name was removed from the supplementary material.",
            "summary_of_the_review": "This paper has a number of issues, including the clarity of the exposition and the overall contributions; please see the main review for more details. The main suggestions for improvement would be to really clarify the contributions here - if the main contribution is the library presented here, it should be explained how this allows for easier/better training compared to other libraries.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a system for efficient development of deep RL agents enabled through modular design. The design separates aspects of agent implementation to support several configurations of policy architectures. The authors demonstrate the system through a series of case studies in three environments (Atari, PyBullet, Obstacle Tower). The case studies demonstrate improvements in resource efficiency during training, and a custom model implemented in the system achieves performance comparable to a top competition score in the complex Obstacle Tower environment.",
            "main_review": "Strengths\n-------------\n* The Scheme module system is a nice contribution, and the case studies demonstrate the utility of the system to support research in RL. \n* The discussion of system design is well-placed in the literature. The technical consequences of the design decisions are clearly introduced. \n* The case studies raise interesting research questions, and the experimental design supporting the studies is sound. \n* The paper is clear and well-written.\n\n\nWeaknesses\n-----------------\n* The presentation of the systems contribution is shallow in some respects.\n* Reported performance measures do not provide a clear picture of policy performance.\n\n\nComments for author\n----------------------------\n* System description: I am glad to see more work with systems contributions at machine learning venues. However, I feel this paper could do more to communicate the contributions of the system.  \n(1) I would expect a description of the core system design which provides enough detail to support development of new Scheme-inspired modules. The system is well-presented in the context of the current literature, and each of the system components are described in sufficient detail, but a view of the operation of the components together is missing. A paragraph or figure to describe the modular interoperation of the components (Learner, Scheme, Algorithm, Storage, Env, and Worker) would be nice, especially as an overview at the start or end of Section 3.  \n(2) In addition, tying the design choices explicitly to the presentation of results will further clarify the systems contribution of the work. The organization of results in Sections 4.1, 4.2 does not readily support the case study analyses as they relate to features of the system. One suggestion is to use a table to introduce the agents used in the case studies with their original citations, the Scheme specifications for these agents, and a dedicated Scheme-oriented label used to refer to them in the paper and figures (e.g., RPA-GPS for RAPID). Or otherwise direct readers to Table 4 in the supplementary materials.  \n(3) Finally, claims about code re-use and reduced development time are not supported, though the case studies do make use of a variety of system components. It would be nice if these claims were justified with details from the case studies.\n\n* Full picture of system performance: The paper is framed around improvements to resource efficiency for training and modifying RL agents, and the authors use FPS of frames processed as the primary performance measure to capture the speed of training. The authors note that the resource efficiency gains in the system come at the cost of some asynchronicity in the policy updates and describe the theoretical implications of this change on the problem formulation. Ultimately, the RL task is to learn policies which maximize reward. The paper does include learning curves for some of the case studies, but effects of the implementation modularity on policy performance measured with respect to reward are not considered. Specifically, including learning curves, test scores, or other reward-based descriptions of performance for the models trained in the scaling performance experiments (4.2, Figure 3) would provide evidence that the distributed architectures do not negatively impact policy performance. \n\n* Few baselines: The experiments are well-designed to isolate the effects of interest. However, the case studies consider internal model implementations only, which limits the external validity of the results. If the model implementations available in this library are introduced as a contribution, the paper should provide empirical evidence that the model implementations achieve results comparable to alternate implementations in other libraries. Neither external model implementations or random baselines are considered, though some performance values from prior work are reported separately in the text (in 4.1, the original PPO learning curves from Schulman et al. 2017 are qualitatively compared to the learning curves in Figure 2; in 4.3, the tower-floor performance of a basic PPO agent is mentioned). If these values are included as baselines, they should at least appear in the captions, or displayed together in a figure or table with performance measures of the trained agents presented in the work. \n\n* Clarity of results: The current presentation of results requires a lot of work from the reader to digest what is shown. The FPS measurements discussed in 4.1/Figure 2a only appear as annotations in the figure where they are difficult to compare directly. These values should appear in a table which makes the agent comparisons explicit and provides the percentage change calculations inline. In addition, some of the models used in the case study are not fully introduced and missing citations (i.e., APPO, RAPID).\n\n* Results reporting practices: The maximum performance measure reported for Obstacle Tower Challenge may be an artifact of the randomness in the environment. Comparing items using a maximum score (instead of e.g., the mean) defines a multiple comparisons procedure (MCP) [1] which may introduce errors in any downstream analysis. In the case of the results reported in 4.3, the agent achieves comparable performance with the competition agent, but the distribution of test scores at the end of training still appear centered near the challenge winner score. It is not clear whether the reported test result is an artifact of the MCP or evidence of improvement over the competition winner, particularly as the comparison is against a single sample for the competition agent. It would not harm the contributions of the paper to more clearly characterize the results of the Obstacle Tower agent by reporting the mean or median performance over all test attempts. \n\nMinor comments and typos:\n* Presentation errors in Figure 1b, G and R labels\n* model consists on breaking down -> model consists of\n* Figure 3 caption: Gradient Asynchroy \n* more complex than the previously tested -> than those previously tested\n* withing -> within\n\n[1] Jensen, D. D. and Cohen, P. R. Multiple comparisons in induction algorithms. Machine Learning (2000).\n\n\nQuestions\n-------------\n* What are the theoretical implications of this modular system design? Some configurations map to existing agent architectures. Are there known degenerate configurations, and if not, are there consequences to freely varying the agent components outside of policy lag and gradient asynchrony?\n* How were parameters selected, particularly the training step parameter for Obstacle Tower? The supplementary materials report 600M target steps, but the results in Figure 4 end at ~530M. \n\n\nRequested Changes\n---------------------------\n* Provide an overall system description which lists all components and their interactions over the course of training.\n* Include learning curves or other reward-based performance measures of the distributed model scaling experiments in 4.2/Figure 3.\n* Improved clarity in presentation of results as described in “System description”, “Clarity of results”. \n* Include baseline results where possible, displayed clearly with model results. \n",
            "summary_of_the_review": "The strongest contributions in this work are in the systems aspects (i.e., the Scheme module), and a systems read of the paper is further supported by the resource-focused performance measures used in the case studies (e.g., FPS, training-hours). The presentation of the system is somewhat shallow in terms of the description of Scheme, and as an RL contribution, it does not provide a complete view of the impact of design choices on policy performance. Still, the paper generally contains the information needed to understand and assess the work, the system enables novel analyses of policy training and architecture designs, and the case studies provide evidence of increased resource efficiency using the system. If the system description can be sharpened, I recommend to accept.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper addresses the subject of efficient reinforcement learning in an asynchronous setting, with the goal of utilizing a cluster of CPU cores and GPUs as efficiently as possible. In this regard, the paper proposes a new modular code-base that implements several utilities for defining and training asynchronous and synchronous reinforcement learning agents by composing modules together. Notably, the code-base is able to support many standard asynchronous reinforcement learning algorithms, such as A3C, IMPALA, RAPID, and APPO, which is an important inclusion for this sort of contribution to the field. Additionally, the design of the code-base appears to be such that modules can be combined in ways that go beyond existing asynchronous reinforcement learning algorithms, and can be used to explore novel variants of components referred to in the paper as Learners (such as SAC or PPO), storage algorithms (such as a HER replay buffer), and data collection mechanisms (for example, one can terminate each worker early if others have finished collecting rollouts).\n\nThe code-base is the primary contribution of the work according to the authors. As a secondary contribution, the authors demonstrate the use of their code-base to scale up existing asynchronous reinforcement learning algorithms to solve the Obstacle Tower Unity3D challenge environment. The authors achieve what is to my knowledge state-of-the-art in this environment. Additionally, the ability to ablate the specific settings of asynchrony appears to be an important potential contribution of the work; however, there are limited experimental results in this regard, and I would not consider this particular opportunity to be realized as a contribution of the work in its present form.\n",
            "main_review": "I will organize my review as follows. I will begin with a discussion of my take on the paper. I will then discuss strengths and weaknesses, and suggest several ways in which the paper may be improved. Overall, while the presented experimental result that achieves state of the art on the Obstacle Tower Unity3D challenge environment is impressive, the significance of this result, combined with the contributions of the paper I listed in my summary in the previous section are on the weak side in its present form, and can be improved. Additionally, the paper is positioned in a space where alternatives exist (consider RLLib and ACME, which now appears to be open source), and the marginal benefits of the proposed code-base over these alternatives is not yet well argued, but can also be improved, as I will describe later.\n\nStrengths:\n\n* The major strength of the proposed code-base is the support of more combinations of components than prior work. This means that methods like HER can be combined with algorithms like SAC in a distributed training pipeline, which is novel from a framework perspective. The paper would be much stronger if this contribution of the code-base was focused on, and more experimental results were included ablating combinations of Learners, storage algorithms, and actors that have not been tried before.\n\n* A second strength of this paper is the quality of the results on the Obstacle Tower Unity3D challenge environment. This paper presents state-of-the-art performance in this environment, which is a compelling reason to start using their code-base as opposed to alternatives. The paper can be made even stronger if the authors are able to ablate their performance on the environment, and identify with high confidence what makes this new result possible. If the answer is only possible using a modular code-base such as theirs, this would further encourage adoption of their reinforcement learning system in the broader community. \n\nWeaknesses:\n\n* One major weakness of the paper is that the ideas presented are weakly novel. The paper has the potential to provide novelty through the two strengths as described above, but in both cases, the paper in its current form can develop the experimental results much further. See my comments in the strengths section for specific suggestions for improvement to this end. The main contribution of the paper is a new code-base for reinforcement learning research, but after reading the paper, it is unclear how this code-base will enable new frontiers for reinforcement learning research going forward. That being said, the lack of modularity in existing frameworks like RLLib and ACME may indeed be a critical limitation that needs to be addressed in order to enable such new frontiers. If the authors intend to strengthen this argument, experiments should be included that demonstrate specifically what new combinations of modules are needed, and how not having these modules is a limitation to asynchronous reinforcement learning. If experiments of this form were included in the paper, and supported a need for modularity like that provided by the proposed code-base, the paper would be considerably stronger.\n\n* A second weakness of the paper is that not many reinforcement learning environments are included. It is especially important for a framework to demonstrate its ability to reproduce the results of existing work on as many previous benchmarks as possible. Doing this can require a lot of computation, and is not typically necessary for other types of papers that may build on existing code-bases, but for a reinforcement learning framework, this is an important step.\n",
            "summary_of_the_review": "My recommendation is a rejection of the paper (4/10), on the grounds that the contributions made by the paper are not yet of high significance to the broader research community in distributed reinforcement learning. However, with improvements that include better justification of the modularity provided by the code-base via experiments, and an ablation of their results in the Obstacle Tower Unity3D challenge environment, I am willing to increase my rating. I encourage the authors to discuss with me how the paper can be improved.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethical concerns.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this work, the authors propose a modular RL framework for build and training Deep RL agents. The authors build their work on the Ray framework and claim to delineate themselves from the rllib RL framework due to their modular and flexible approach with less abstraction.\nthe authors implement several on-policy (A2C, PPO) and off policy (DDQN, SAC, DDPG, TD3, MPO) RL algorithms and appear to include support for recurrent agents, although I am not sure which algorithms support LSTM/ GRU as part of their network architecture.\n\nThe authors then achieve the highest to date peroformance on the Unity3D obstacle tower challenge by using their RL implementation combined with some additional reward engineering.",
            "main_review": "I am all in favour of good quality RL implementations that are flexible enough to allow for researcher to try out more exotic ideas and approaches.\n\nStengths:\n1. The authors provide a good selection of recent RL algorithms both on-policy and off-policy \n2. They show results of these algorithms on a number benchmarks (Atari, Unity3d obstacle tower)\n\nWeaknesses:\n1. At no point do the authors compare against other implementations of their algorithms, I am surprised they do not include a comparison of either the wall clock training speed or the performance of a policy learning with the same hyper-parameters. The plots in Figure 2 are known benchmarks, they comparable results are publicly available. \n2. As the authors base their implementation on the Ray framework, there should be a comparison of performance with that of Ray RLlib\n3. The authors are missing implementation of: Rainbow DQN.\n4. It is not clear to me that the reason for improved performance in the obstacle tower environment is related to the new RL framework, my impression is that this was achieved through reward shaping in the hard levels. What is the contribution, the framework or the shaping? How good is the performance without the reward shaping?\n5. It is not clear to me if all algorithms can use a recurrent network, or is this just the on-policy algorithms?\n\n",
            "summary_of_the_review": "All in all this is a useful contribution for the RL community, but it is missing some key comparisons. I am also concerned that this may just be an engineering exercise that lacks novelty. A further concern is that what does this implementation have over Ray RLlib, I understand there is much less abstraction and perhaps more flexibility, but considering how well documented the alternatives are, is this implementation really neccessary?\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}