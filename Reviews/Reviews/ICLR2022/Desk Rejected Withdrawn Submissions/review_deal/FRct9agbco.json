{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors are interested in learning visual representations in the presence of knowledge that could come in the form of labels for all or some of the data, or from self-supervised leaarning (SSL) on other modalities in a multimodal setting.  They extend a recent SSL method that is based on mean shift and constrain the nearest neighbor part by using the additional information. They compare to other ways of supervised pretraining and show better transfer performance for ImageNet while further experiment with using intra-modality constraints for SSL on videos.",
            "main_review": "### Strengths\n\n* The authors work on an interesting topic: generalization for supervised models\n* The paper is well written and easy to follow\n* There is a (lacking but still) large number of experiments on interesting tasks like semi-supervised, noisy and multimodal learning\n* The method shows strong gains for transfer learning for image classification, outperforming all SSL methods tested on average\n\n### Weaknesses\n\nW1: The technical novelty of the paper is weak. \nThere not a lot of novelty in constraining mean shift with additional info (eg see [Tuzel et al ICCV 2009, Anand et al PAMI 2013]) and this paper is porting similar intuitions over novel methods and applications. The approach of this paper can be summarized as taking (the recent) MSF [Koohpayegani et al., 2021] and constraining the mean-shift to other points with the additional knowledge (eg same label).\n\nAs the authors also note, their method bares a lot of relations to SupCon and the top-all version of their method is practically SupCon with a different loss (mean-shift vs contrastive). From the experiments we see that top-all and top-10 perform exactly the same on average for transfer learning. There is really little point trying to set the top-k hyperparameter and restricting the neighbors within a class, and the proposed method becomes even closer to SupCon. In terms of results, it is worth noting that although the authors show on average gains over SupCon for transfer learning and image classification, their method performs worse than SupCon on IN-1k (which is the main contribution of SupCon)\n\n* [Tuzel et al ICCV 2009] Kernel Methods for Weakly Supervised Mean Shift Clustering\n* [Anand et al PAMI 2013]  Semi-Supervised Kernel Mean Shift Clustering\n\nW2: The weak technical novelty would not be an issue on its own if the paper offered a wide and complete set of experiments, accompanied with interesting insights for the community. This is not really the case beyond their main experiments on transfer learning for image classification. Specifically, the authors touch very interesting applications where further novelty could have been added, but they seem to do the bare minimum in both and have \"proof-of-concept\" experiments:\n* The semi-supervised case is only evaluated in ablations, ie without any comparissons to the many recent works out there (eg SimCLR-v2 or others).\n* For the multimodal case they follow the insights from CoCLR and use the one modality to get nearest neighbors for the other. It is unclear if any real contribution or insights can be derived from these experiments.\n* For the case of label noise that is also mentioned in the abstract experiments, experiments are on the smaller im100 dataset and comparissons very basic again.\n\n### Questions\n\nQ1: Would this approach work when the additional info is \"coarse labels\"? \nQ2: Why do you think that SupCon is better than CMSF on In-1k?\nQ3: In the intro, the authors claim that \"Moreover, our method is more general and can use other sources of knowledge for constraining the NN search.\" It is unclear to me how they mean that and which methods they are \"more general\" from.\n\n### Notes\n* The most important/related citation, MSF, is missing its venue. It is ICCV 2021 \n* The FrozenPrototype baseline is really interesting. [Hoffer et al ICLR 2018] is a missing citation for this.\n\n[Hoffer et al ICLR 2018] \"Fix your classifier: the marginal value of training the last weight layer\"\n\n### Concurrent works\n\nThere are technically concurrent (ICCV 2021 was in October) works to this that are essentially using similar intuitions and the only difference is the losses used and the nature of the additional information (coarse labels); these should still be cited:\n* [Xu et al ICCV 2021] Weakly Supervised Representation Learning with Coarse Labels. \n* [Touvron et al ICCV 2021] Grafit: Learning fine-grained image representations with coarse labels.\n",
            "summary_of_the_review": "Overall this paper is an incremental version of MSF where the mean-shift algorithm is constrained with the labels, similar to what supCon did with contrastive SSL works. The method shows interesting gains for transfer learning for image classification, outperforming all SSL methods tested, however the experimental results on other cases tested (semi-, noisy, multimodal) is still lacking. With minor technical novelty and an incomplete set of experiments, I think that although interesting, the paper is not yet ready for acceptance.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper investigates pretraining of visual representation. Building on BYOL, it proposes to mine the teacher target using a nearest neighbor search selecting top-k targets out of a large queue of examples, similar to MSF. Additionally, it proposes to constraint the search using additional semantic information (only consider neighbors sharing the same class) or using a different modality (RGB/Flow) to perform the search.\n\nAuthors validate their methods by pretraining on supervised Imagenet and transferring the learned representation on various tasks. They also evaluate the performance on ImageNet using the linear protocol and on the UCF101 dataset showing that the approach works in cross modal scenario. Finally they show that the approach is robust to noisy label.\n",
            "main_review": "The approach proposes a simple approach to integrate label or cross-modal information to pretraining.\n\nStrength:\nAuthors provide an extensive empirical evaluation of their approach in various setting (ImageNet linear eval, transfer learning, noisy label, video classification). They compare with various baselines, both supervised (NTXent, SupConv) and unsupervised (MoCo, BYOL, MSF) and demonstrate good performance. Authors also perform various ablation to justify the different design choice.\n\nWeakness:\nThe technical novelty over BYOL or MSF is limited.  The main contribution of the paper is to constraint the NN search based on label or multi-modal information. While there is an empirical gain when transferring the representation, it is not clear if the gain justify the extra constraint, i.e. requiring label during training.  The empirical section could also be strengthened by comparing with other recent SSL approaches such as SwAV or DINO.\t\n\nOn a minor note, I think the author should also discuss their relation to PAWS (https://arxiv.org/abs/2104.13963)  another recent approach that investigates integrating label information using an approach based on nearest neighbors. \n",
            "summary_of_the_review": "Overall, the paper appears borderline to me. While I appreciate the extensive empirical evaluation and the simplicity of the idea, the technical contribution and empirical gain relative to BYOL appears to be small. Additionally, the paper could be strengthened by comparing with additional baselines such as SwAV or DINO.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work extends ideas from SSL literature: consistency regularization between two views and mean shift with nearest neighbors. The authors proposes non-contrastive representation learning framework which obtains more positive pairs by using additional knowledge and learns representations by pulling them to be close. This paper validates getting more but purer positive pairs helps to learn good representations which transfers better to downstream tasks than standard supervised learning or unsupervised learning approaches, and authors show that the learned representations are robust to label noises.",
            "main_review": "### Lack of novelty\n\nSeveral works [1, 2] have already shown that representation performance gains can be obtained when using additional information (i.e. labels) for unsupervised learning methods on a supervised dataset. The proposed method is nothing more than obtaining an additional positive samples by using additional supervision in a self-supervised learning method.\n\n### Performance gain is marginal\n\nWhile the assumption that there is supervision is a very strong assumption compared to other unsupervised methods, the performance gain is marginal. I think the authors should report the results of training the CMSF for more epochs (i.e. 1000 epochs).\n\nIn addition, it seems a natural result that the proposed method shows better performance for using top-k positive samples rather than the whole ones when there are label noises, which does not support the claim that the proposed method is robust to label noises.\nIt should also be compared with other contrastive learning based methods which use additional supervised loss to show robust to label noise.\nBesides, when the noise level increases, the performance of the presented method drops dramatically compared to the unsupervised method. This does not appear to be better than the unsupervised method, despite the strong assumption of supervision information.\n\n### Need more analysis\n\nThere are no ablation studies or analyzes of why the CMSF learns better transferable representations for contrastive learning using different supervised-learning approaches. For instance, it seems that the superiority of CMSF can be further expressed by comparing the results of using the SSL method used in BYOL and standard cross-entropy loss function together.\n\nReference\n\n[1] P. Wang et al., Contrastive Learning based Hybrid Networks for Long-Tailed Image Classification, CVPR 2021\n[2] A. Xiao et al., Contrastive Semi-Supervised Learning for ASR, ICASSP, 2021\n",
            "summary_of_the_review": "- The paper is written with clear motivation and easy to follow.\n But it needs proper mathematical/empirical evidence to support few claims of the proposed method.\n- The novelty of the proposed method is limited since it simply extends the existing self-supervised learning approaches to have more positive pairs by using additional annotations which is strong assumption and not scalable compared to existing unsupervised methods.\n- Experimental results on transfer learning and linear evaluation show marginal gain compared to unsupervised learning methods, and there should be more proper baselines in the experiments in the presence of label noises to validate superiority of the proposed the method.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper addresses representation learning from labeled and/or unlabeled data. Inspired by the success of self-supervised learning, this paper proposes representation learning method that can utilize additional knowledge related to the data. This knowledge may come from annotated labels in the supervised setting or in unsupervised learning, this knowledge can come from another modality or another pre-trained network. This paper particularly works with the previously proposed mean-shift algorithm and proposes to constrain the search space of nearest neighbors. Although consideration of nearest neighbors in the mean-shift (MSF) is quite adhoc, the findings on cross-modal and/or cross-network constraining of nearest neighbor search space in self-supervised scenario is quite interesting. These findings are also robustly evaluated through various experiments. The paper is well written and easy to read. However, I think the novelty of this paper is quite limited, as we can see many similar works [1-7] that already known to us.\n\nReferences\n[1] Mine Your Own vieW: Self-Supervised Learning Through Across-Sample Prediction (https://arxiv.org/abs/2102.10106)\n[2] Integrating Auxiliary Information in Self-supervised Learning (https://arxiv.org/abs/2106.02869)\n[3] Unsupervised Feature Learning by Cross-Level Instance-Group Discrimination (https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Unsupervised_Feature_Learning_by_Cross-Level_Instance-Group_Discrimination_CVPR_2021_paper.pdf)\n[4] With a Little Help from My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations (https://openaccess.thecvf.com/content/ICCV2021/papers/Dwibedi_With_a_Little_Help_From_My_Friends_Nearest-Neighbor_Contrastive_Learning_ICCV_2021_paper.pdf)\n[5] Mean Shift for Self-Supervised Learning (https://openaccess.thecvf.com/content/ICCV2021/papers/Koohpayegani_Mean_Shift_for_Self-Supervised_Learning_ICCV_2021_paper.pdf)\n[6] Supervised Contrastive Learning (https://proceedings.neurips.cc/paper/2020/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf)\n[7] Learning Robust Representations via Multi-View Information Bottleneck (https://openreview.net/pdf?id=B1xwcyHFDr)",
            "main_review": "I have the following comments on the paper:\n\n1) My main concern is that the method is very similar to some of the already known works [1-7]. Among them, nearest neighbors and variants are already explored in [1,3,4,5] for unsupervised representation learning, constraining representation learning with auxiliary and/or label information is already explored in [2,6,7]. Learning representation considering constraint from multiple modalities is also explored in [2,7], among them [7] literally learnt representation from very different modalities, such as text and image (using MIR-Flickr dataset) and sketch and image (using Sketchy dataset). Considering those existing works, I think the work presented in this paper only has a very minor difference.\n\n2) Constraining the self-supervised learning with a model trained on different modalities adds further computational overhead. Some other methods learn those representations via a joint optimization technique. For example, CoCLR jointly learns the representation for video and flow views via the optimization technique. MIB [7] also learns representation for sketch, text and image jointly. Here also the novelty part is not very clear to me.\n\n3) I think the aim of ablation study reported in section 2.1.5 and table 2 is not so clear at the moment. Is it possible to represent those information in table 2 in the form of plot so that the variation of different hyperparameters is visible? The main message from section 2.1.5 is not clear.\n\n4) In table 3, CMSF outperforms all the baselines including CoCLR in Flow, but in RGB CMSF 3.6 points less than CoCLR. The reason is not that clear.\n\n5) In noisy supervised setting, it is not clear how the labels are corrupted?\n\n6) In table 3 caption, \"conatraint\" should be \"constraint\"\n\nReferences\n[1] Mine Your Own vieW: Self-Supervised Learning Through Across-Sample Prediction (https://arxiv.org/abs/2102.10106)\n[2] Integrating Auxiliary Information in Self-supervised Learning (https://arxiv.org/abs/2106.02869)\n[3] Unsupervised Feature Learning by Cross-Level Instance-Group Discrimination (https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Unsupervised_Feature_Learning_by_Cross-Level_Instance-Group_Discrimination_CVPR_2021_paper.pdf)\n[4] With a Little Help from My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations (https://openaccess.thecvf.com/content/ICCV2021/papers/Dwibedi_With_a_Little_Help_From_My_Friends_Nearest-Neighbor_Contrastive_Learning_ICCV_2021_paper.pdf)\n[5] Mean Shift for Self-Supervised Learning (https://openaccess.thecvf.com/content/ICCV2021/papers/Koohpayegani_Mean_Shift_for_Self-Supervised_Learning_ICCV_2021_paper.pdf)\n[6] Supervised Contrastive Learning (https://proceedings.neurips.cc/paper/2020/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf)\n[7] Learning Robust Representations via Multi-View Information Bottleneck (https://openreview.net/pdf?id=B1xwcyHFDr)",
            "summary_of_the_review": "As written in my main review, this paper has components/ideas that have already been explored before. Therefore, considering those works this work is only contributing very very marginally to our knowledge. Hence the recommendation.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}