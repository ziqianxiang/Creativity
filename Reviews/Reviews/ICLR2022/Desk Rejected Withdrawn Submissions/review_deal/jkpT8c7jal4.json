{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper looks into the problem of adjusting calibration and refinement of probabilistic classifiers. The authors propose a view to linking the ECE measure with AUC, which can then be used to balance calibration loss and refinement loss in various model training processes. Some experiments are conducted to demonstrate the effects of regularisation methods on the calibration-refinement trade-offs.",
            "main_review": "At the top level, this paper is on an interesting topic about the calibration-refinement trade-offs among modern deep NN classifiers. While there has been much recent work on calibration, refinement loss is somehow overlooked. It is therefore encouraging to see a paper on this particular problem.\n\nThat being said, I have two major concerns after checking the entire paper.\n(1) The derived relationship between ECE and AUC (eq.9) is problematic.\nAs indicated by the authors, one of the key contributions of this paper is the discovery of a relationship between ECE and AUC. However, the derivation includes a misusage of previous results by (Hernandez-Orallo et al., 2012).\nAs defined by eq.6, the expected model accuracy $\\mathbb{E}[A]$ is just the standard definition of accuracy (i.e. probability of making a correct classification).  Given a fixed model and decision rule (as the setting specified by the authors), this value is a constant.\nHowever, the relationship between accuracy and AUC (Hernandez-Orallo et al., 2012) is derived from a different setting (e.g. when we evaluate the accuracy with varying thresholds of decision).  The relationship can only be derived by \"averaged over all possible predicted positive rate.\" (not \"true positive rate\" as quoted by the authors). (Hernandez-Orallo et al., 2012) considers both (Accuracy) and (AUC) as functions of (TPr) and (FPr), and the right-hand part of eq.8 can only be obtained by computing an integration between (Accuracy) and (Predicted Positive Rate).\nAs a result, eq.8 (hence eq.9) is not correct as the $\\mathbb{E}[A]$ term is not the averaged term as proposed in (Hernandez-Orallo et al., 2012), but merely the standard accuracy. The critical integral part is missing.\n\n** $P$ and $N$ in eq.8 and eq.9 are not defined and require guessing for the moment.\n\n(2) Important related work is missing.\nGiven this paper is on the relationship between calibration and refinement loss, it should be aware that the topic is also considered by (Kull M, Flach P. Novel decompositions of proper scoring rules for classification: Score adjustment as precursor to calibration, ECML-PKDD 2015) under the setting of proper scoring rules.\nIn that work, the authors propose that the refinement loss can be further decomposed to (Irreducible Loss) and (Grouping Loss). (Irreducible Loss) is fixed by the source distribution and cannot be reduced by the models. Therefore, any change of (Refinement Loss) is due to the change of (Grouping Loss).\nTherefore, to have a better understanding of how we can balance (Refinement Loss) and (Calibration Loss), it would make more sense to link the regularisation problem in this paper to the (Grouping Loss) and illustrate how the loss is affected by applying regularization.\nWhile the authors suggested that the standard ECE is not a Proper Scoring Rule, it is more reasonable to consider the refinement loss problem under PSR where it was initially proposed. Also, the PSR framework can work with a true multi-class setting instead of a binary confidence setting as in the current paper.",
            "summary_of_the_review": "This paper discusses the interesting problem of balancing calibration loss and refinement loss. However, the main contribution on the relationship between ECE and AUC seems to misuse a previous result and is not convincing at the moment. To further include some results from previous work might also bring more insight to this paper.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper brings up an interesting point --- whether refinement is hurt by popular calibration approaches and argues that the answer is negative. To do this, they use empirical results from a few recent algorithms including FL, ERL, LS and MX.",
            "main_review": "In this section, I am listing some specific points and then I describe the main reasons for my review in the next section.\n\n\n* What is $P$ and $N$ in Eq 8 and 9 ?\n\n* I also find the assumption $C_m > A_m$  for all $m$ rather strong. \n\n* As the authors claim that many existing approaches like Entropy regularisation and Focal Loss are actually label smoothing, could the authors please provide a reference or further description to show that these two approaches are indeed label smoothing ?\n* The equation of Focal loss provided in Eq. 15 is not the one used in Mukhoti et. al. 2020. In particular, they seem to use an exponential on the weighted term of the entropy. The experimental section does use $\\gamma=3$ but the main text completes omits that point.\n\nThe authors argue that in these approaches \"regularisation added only helps to tone down the winning class confidence and\nincrease the losing confidences\". However, this is something Mukhoti et. al. infact argue against. They make this point with Table H.1 that FL does more than just increase the entropy of the predictions by preserving the fractions of points correctly classified with high confidence.\n\nThe authors state \"We suspect since the network’s overfit to varying degree on different datasets. This results in varied improvement in calibration and hence the impact on refinement also varies.\" But I don't see the evidence of this. The AUROC, which indicates refinement, is quite similar for STL, CUB, and Imagenet in Table 2 even though the accuracies differ ( similarly in Table 3).\n\nAdded to that, Figure 1, does not seem to have any imagenet results.\n\n",
            "summary_of_the_review": "Overall, I do not find the experiments corroborating the hypothesis that there refinement is hurt by modern calibration approaches. The drop in refinement (proxied by AUROC) is often very small and in some cases there is an increase (by the ERL regulariser, which should be the one decreasing refinement the most as it does nothing other than increasing entropy).\n\nThe mathematical section has a rather strong assumption that $C_m > A_m$. The authors mention some works that show where it holds for most bins but provide no analysis for the error incurred when it does not hold.\n\nThe assumptions about focal loss seem rather misargued. The focal loss paper did differ on some of the points made here (see the main review above). The experimental section of this paper differs from the mathematical section on this front as well.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper tries to point out the refinement is also an important metric in evaluating the predictive uncertainty besides calibration. A intuitive graph shows the representative cases and difference between accuracy and calibration. Then the relationship of calibration and refinement, as well as that of regularization and calibration, are discussed a bit. ",
            "main_review": "In general, the paper has a good introduction and intuitive explanation of the calibration and refinement. But is hard to follow when it goes to detailed discussion as it's not well organized and written. It does not really provide useful guidance or methodology for improving the refinement.\n\nThe detailed questions are listed as follows:\n1. In the major equation 8 and 9, P and N seem not defined. I could assert P might be the true-positive rate. However, without any definitions and explanation, it not easy to strictly understand the relationship of ECE and refinement.\n2. Is seems a strong assumption that for every bin, the predictions are over-confident. How does this assumption function?\n3. What is the major objective of section 2.2? Does it mean regularization have a negative effect on refinement? How could we fix it if we want a good refinement without discarding regularization?\n4. Do AUROC and AUPR play the same role in representing refinement?\n5. There are some typos in the paper like the second sentence in the abstract. Please polish it carefully.",
            "summary_of_the_review": "As the paper is not well written and the major claims are not clear, I recommend to reject the paper unless the author could fix all the problems in the rebuttal.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors highlight the connection between ECE and AUROC when AUROC is evaluated in terms of refinement. The authors break down ECE in a way which shows existing algorithms can work by only reducing the average confidence. The authors, show empirically that existing calibration approaches improve calibration at the expense of refinement.",
            "main_review": "# Pros \n\n- The authors give a nice breakdown of refinement and how it breaks down in the ECE metric. \n- Refinement is something that many other works do not mention and it looks to have many historical mentions, which means it may be widely overlooked in the recent calibration literature. \n- Intuitively, the confident and unconfident predictions should be well separable if we are to avoid overconfident + wrong predictions which hurt calibration error. \n- The experiments seem to cover common datasets and network architectures. \n\n# Cons\n\n- The label smoothing loss $\\mathcal{L}_{LS}$ appears to have the wrong sign. Equation 11 says that it is negative, which would maximize the KL between the uniform and output distributions. \n- Table 1 is missing baselines for temperature scaling, which is a relevant baseline. Temperature Scaling is defined as TS in the appendix and it is said that TS uses a temperature of 1.5, but there is no mention os TS in any of the tables or results as far as I can see. \n- NLL is not included in the metrics which are evaluated. NLL is a proper scoring rule and one which is widely reported in many works. If the authors want to highlight the usefulness of measuring refinement over existing approaches, then NLL should be compared against.  \n\n\n# Minor\n\n- Definition 2.2: the last line says “switching the rank of an incorrect prediction with the correct one decreases r.” Shouldn’t the ‘correct’ and ‘incorrect’ be swapped since there is no rank of an incorrect prediction?\n- Referenced equations all say “Equation equation x”\n- Table 1 text is too small and very hard to read. \n- Section 5.1 similar observation $\\rightarrow$ a similar observation\n- Section 5.2.1 employee $\\rightarrow$ employ\n- Section 5.2.1 ood $\\rightarrow$ OOD (ood is used elsewhere in the text as well)",
            "summary_of_the_review": "The authors did a thorough job of introducing refinement and highlighting a tradeoff between calibration and refinement which often happen at the expense of each other. However my intuition is that NLL would be a metric which would also perform well, which is mentioned as a proper scoring rule in the paper and yet left out of the experimental results. \n\nIf the intent of the authors id to introduce a new metric which should be evaluated by other authors, then it should be one which outperforms all other metrics and provides additional insight into the performance of the algorithm. I am not convinced that refinement would make a better measure than the already established and widely used NLL, so therefore I base my score on this fact. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}