{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper introduces a memory-driven semi-parametric approach to text-to-image generation. A memory bank of image features is constructed from a training set of images. Then the retrieved image features are provided to the generator to produce realistic synthetic results.\n\nThe novelty of this paper comes form two folds: i) memory construction and retrieval; ii) two new architectures of the generator and discriminator to exploit the memory.",
            "main_review": "1. XMC-GAN is a strong baseline and current state-of the art results. Why not do a comparison with XMC-GAN. Even if the FID score is not good, what about the human evaluation?\n2. It is good to see improvement on the memory bank, and I wonder the similarity score effects. Could you add an ablation study on its effects on retrieval.\n3. The proposed methods lack novelty. The architecture of generator adopts another existing work. The memory bank construction and retrieval is simple heuristics. \n4. Human evaluation is not extensive, and more methods should be evaluated. Some other metrics such as SOA-C, SOA-I, FID 0-1-2-4-8 should also be reported.\n5. Equation 1-5 show the similarity scores, and which one is used for memory back retrieval. Besides, the memory bank size seems a balance between quality and efficiency.\n",
            "summary_of_the_review": "This paper needs further improvement to present in the ICLR conference. Please refer to the weakness part, and improve the paper accordingly.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a semi-parametric method for text-to-image generation. The main idea to directly use the training data to extract image features useful to synthesize novel images from novel text prompts. The proposed method first performs a retrieval step given an input text prompt where the most compatible image features are selected from the training set. Next, the selected image features are given to a generator network as global and disentangled more localized features in order to synthesize the image containing the semantics provided by the text input. The generator network is trained using a content aware discriminator network that uses skip connections to prevent information loss when judging real and fake images. In experiments, the authors show the proposed method outperforms the baselines in FID, R-psr, and a user study testing for realism and text alignment.",
            "main_review": "Strengths:\n+ Clever direct use of the training data for image generation.\n+ Outperforms the baselines in both realism and alignment with text condition.\n+ Well written paper.\n\nQuestions / weaknesses:\n\n- Time and memory requirements:\nFrom the results, we can see that the proposed method in fact generates diverse images that better represent the input text. However, having to search over the entire training data (non-parametric methods) comes at a price. To give a complete picture of the proposed method and to potentially spark followup research, I would suggest the authors to provide time and memory requirements in comparison to the baselines. I believe this is something that the authors need to make sure to mention in the final manuscript.\n\n- Effectiveness of the disentanglement:\nThe authors check whether the fully connected layers meant to result in disentangled features are doing their job by feeding a mismatched text and image pairs into the network, and conclude that because the R-psr score drops significantly the disentangled features are necessary. I am not sure how this experiment shows this since, as far as I understand, one of the main reasons to use disentangled features is to prevent copy / pasting being done by the generator. If we want to check whether the network is simply learning the copy / paste operation from the non-disentangled features, I would suggest the authors perform a diversity test by providing different noise inputs and checking whether the outputs change. If the network is simply copying the training data as a result of the non-disentangled features, then the outputs should not be diverse.\n\n\n- How is the word-image matching exactly done?\nThe authors mention that the final method uses the word-image matching. However, I am not sure how the authors made sure that the image embedding and text embeddings can be compared. The image embedding is extracted with the pre-trained VGG network from Simonyan & Zisserman, 2014. This embedding space was not trained to be aligned with any textual data, and so, I am not sure how the authors are able to make these alignments. Is there some training step where the text embeddings are learned such that they can be aligned with the image features? If so, when is this done? I would appreciate it if the authors can clarify this in the rebuttal or let me know if I am missing something.\n\n\n- Section 4.2.3 (Words-Words Matching):\nThis section assumes that the test text input has the same number of words as the text in the training set (w \\in R^{NxD} and w_i' \\in R^{NxD}). Is this an assumption in this method? I would assume that the text text input is free to be any number of words and the same goes to the training set text. If so, Equation 3 is invalid. Can the authors clarify this in the rebuttal? \n\n\n- Fig6 comparisons:\nIn Fig6, we can clearly see the advantage of this method against the baselines. Nevertheless, I think the authors should also provide the training images that were used to synthesize the images highlighted in this Figure. The do show evidence of diversity in the generation in Fig7. However, I feel showing the training images in Fig6 is necessary to show a complete picture of how close these images are to the ones retrieved from the training set.\n\n\n- Missing related work:\nA very influential text-to-image work from Reed et al., 2016 is missing from the related work:\nGenerative Adversarial Text-to-Image Synthesis\n(https://arxiv.org/pdf/1605.05396v2.pdf)\n\n\n- Typo:\nPage 5, last paragraph, line 5, second word: dataest -> dataset\n\n\n======================================================================\n\nSuggestions / potentially interesting things to try:\nOne very interesting characteristic of this method is that it models global and disentangled / more localized features.  What if you provide mismatching global and disentangled features? Will it make the model, say, generate a tiger with zebra colors? If so, this could open up cool applications for people into art to try.",
            "summary_of_the_review": "All-in-all, I think this is a very interesting paper with clearly improvement over previous arts. However, I still have some questions to get a complete picture of the proposed method (see review). I am willing to increase my score depending on the author's response.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a new approach to text-to-image generation. In this approach firstly, the image features are retrieved from the memory bank based on the text. Then based on these features and the text the neural network generates a new image. The memory bank is constructed by extracting features from the training set of real images. The method achieves the state-of-the-art in terms of FID and R-precision on COCO and CUB bird datasets.\n\nIn addition to this, the paper introduces a few improvements in generator and discriminator architectures and provides an analysis of the quality and diversity of generated images.\n",
            "main_review": "--Strengths\n1) The authors obtain state-of-the-art results in terms of FID and R-precision. FID improvement is significant on the CUB bird dataset, from 14.81 to 10.49. They conduct a human evaluation that demonstrates the superiority of their approach as well. \n2) The diversity analysis seems legit and I am convinced by the paper that the model does not simply copy-paste retrieved images. \n3) The manuscript provides a meaningful ablation study.\n4) The manuscript compares different retrieval matching strategies.\n\n--Weaknesses\n1) The method requires storing a memory bank to generate images. The size of this memory bank can be huge depending on the number of samples one needs to store. \n2) The method combines architecture[3], the training pipeline[3], and an idea of the memory bank[5] from prior works with reasonable modifications. I believe this contribution is important but limited and a further investigation of the proposed pipeline is needed to strengthen the manuscript. \n\nI think that answers to the following questions will strengthen the paper:\n1) How much memory is required for the proposed method compared to other baselines?\n2) What happens if the memory bank gets reduced? For example, how does performance degrade if only 25, 50, or 75% of the training set is used in the memory bank?\n3) Is it important to have a good retrieval system? The addition of retrieval metrics for the train and test set to Table 4 in Supplementary can answer this question.\n4) In my opinion, it is hard for the proposed method to handle multi-object scenes if object combinations are not present in one image in the memory bank. It would be great to have this intuition confirmed or disproved. \n5) The authors use the intermediate output of VGG16 from the memory bank as input to the generator network. Can one improve generation quality by using more contemporary pretrained weights instead of VGG16, e.g. BYOL[6], VQGAN[7], or VIT[8]?\n\n\n--Additional remarks \n1) I believe there are two relevant papers, that the authors did not cite: DALL-E[1]  generates images based on COCO captions in zero-shot regime, and there is a well-known popular technique on the Internet that combines CLIP [2] with a pure image generation pipeline to generate images consistent with the text (e.g. [5]). \n2) It is better to clarify to which dimension the softmax function is applied in section 4.2. \n\n[1] Zero-Shot Text-to-Image Generation, Ramesh et al.\n\n[2] Learning Transferable Visual Models From Natural Language Supervision, Radford et al.\n\n[3] ManiGAN: Text-Guided Image Manipulation, Li et al.\n\n[4] PasteGAN: A semiparametric method to generate image from scene graph, Li et al.\n\n[5] https://colab.research.google.com/drive/12a_Wrfi2_gwwAuN3VvMTwVMz9TfqctNj?usp=sharing\n\n[6] Bootstrap your own latent: A new approach to self-supervised Learning, Grill et al.\n\n[7] Taming Transformers for High-Resolution Image Synthesis, Esser et al\n\n[8] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, Dosovitskiy et al\n",
            "summary_of_the_review": "The method achieves SOTA in the text-to-image generation task, but the method has its drawbacks like the need to store a potentially expensive memory bank to enable generation. The novelty seems limited as the method is much like prior works mentioned in the paper. I recommend performing an additional analysis and experiments to strengthen the paper. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper explores the novel idea of text based image synthesis images using a combination of parametric knowledge (encoded via training a GAN) and non-parametric knowledge (encoded by some images relating to the target of the synthesis).  Essentially the architecture is a text-driven GAN with additional conditioning from one or more semantic image features.  Conveniently the images can come from a text based image retrieval process.  The additional guidance of the synthesis by is named ‘memory driven’ synthesis by the authors.",
            "main_review": "My view of this paper is on the borderline, tending towards rejection. The paper seems an early idea in need of deeper exploration – specifically, whilst the idea of combining retrieved image features into GAN is interesting the benefits of the image feature conditioning are not well demonstrated or evaluated.  \n\nThe use of additional conditioning and guidance in the generator for text2image synthesis has been explored previously for example in SAGAN/Attentional Generative Network (AttnGAN as cited) where the text encoding is fed both to the generator / upsampling input and to secondary generator blocks.  In terms of architecture the proposed is similar, but instead of feeding in the text-derived semantic features, they are instead semantic features coming from retrieved images from the memory bank.\n\nThe FID scores show clear improvement – i.e. the memory is providing a second source of information that is helpful – but the paper is silent / does not explore the nature of this information?  Is it the image layout that is useful, since this is poorly expressed via the text prompt?  Or the textures present in the image are better captured via the memory (non-parametric) that the trained GAN (parametric) model?  The paper shows quantitatively an improvement in FID score but there is little analysis as to what mutual or complementary information might exist in these inputs.  If the aforementioned hypotheses are the case, then why would a late stage fc layer (VGG conv5_3) i.e. semantic feature provide additionality?  If not, then what exactly is being gained from the image features? Can some visualization be produced showing which information is coming from which source?  \n\nDigging deeper into the question would also help prove the value of the approach beyond simple an FID.  The results included show that the synthesised image resembles very closely the retrieved image.  For example in figure 5, the appearance of the bird or the vehicle are nearly identical between the retrieved image and the synthesised image.   It would be useful to show multiple synthesis results for a given text prompt but different retrieved images (rather than the current ‘diversity’ experiment showing different synthesis runs with different noise seeds).   \n\nMy concern with the current setup, is that the model may be learning largely identity function mapping the retrieved image to the synthesised result and largely ignoring the text prompt.  Showing diverse synthetic output for fixed noise and text but different memory images would help allay that concern.\n\nThroughout the paper, images are referred to in the plural but the results appear to show just a single retrieved image – how would multiple images be combined?  If average pooled, does that make sense if multiple modes might exist in retrieved image data?  Currently the paper discussed average vs. max pooling for image regions but it was unclear to me how multiple images could be used.\nIn summary I think the idea is interesting but beyond superficial exploration of FID (and related effect of ablation on FID) there is no real exploration of the information sources and the impact.  I have some doubts as to the effectiveness of the fusion between image and text features as a result, and whether the paper is really learning from both.  Given the current result set, it could simply be that the improved FID in synthesis is due to over-reliance on the image feature prompt from the retrieval.\n",
            "summary_of_the_review": "The paper seems an early idea not yet ready for publication.  The core novel idea is interesting but not convincingly demonstrated and there is no exploration of the mutual / complementary information provided by the two modalities.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}