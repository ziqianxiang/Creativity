{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes an unsupervised layer image decomposition method for object-centric scene decomposition. The proposed method learns a set of pixel-space prototypes and infers the decomposition by greedily fitting prototypes onto a canvas to reproduce the image. The method is interpretable and relies on only a small number of parameters. Experiments show decent decomposition results on several multi-object datasets.",
            "main_review": "Strength:\n\n+ The proposed method is simple to understand, runs fast, and can be applied to scenes with many (e.g., >30) objects.\n+ The proposed method is interpretable.\n+ It is novel to apply frequency analysis to unsupervised scene decomposition.\n\nWeakness:\n\n- The image formation model is flawed. Eq. (5) assumes that the ordering of the layers can be determined by a greedy reconstruction process, i.e., the first layer selected by minimizing Eq. (4) is treated as the nearest one to camera. However this can be false -- imagine a scene with only two boxes. A big box is partially occluded by a small box. The big box will contribute more to reconstruction, so that the algorithm will select big box as the nearest one, which is wrong.\n\n- The object representation is overly limited. The learned prototypes are in the form of images and the transformations are only translations and single coloring. Thus, the model cannot deal with any other transformations such as rotation and scaling; intra-category shape variations such as slight deforming, 3D-2D projective transformations; or any appearance variations beyond single coloring.\n\n- Insufficient experimental validation to support the claims. The claimed contributions include disentangling object appearance, position, and color, but the experiments do not demonstrate disentangling appearance. There is no comparisons to existing methods on datasets other than Tetrominoes to demonstrate higher segmentation accuracy or reconstruction quality. For exmaple, it is possible to do comparison on CLEVR-6 dataset?",
            "summary_of_the_review": "Considering the technical flaw and the limitations that are not well discussed/addressed, as well as the insufficient experiments, I recommend revision and resubmission.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a method to decompose scenes into object components, which falls into the field of unsupervised object-centric representation learning. The proposed method decomposes an image into object components via a set of learned object prototypes. And the decomposition is performed with a phase-correlation cell, which estimates the transformation between an object prototype and its transformed version in the image using a frequency domain representation. To verify the effectiveness of the phase-correlation cell as well as the whole pipeline for unsupervised object discovery, experiments on simple datasets are carried out with comparable performance achieved with less learnable parameters.",
            "main_review": "The use of phase correlation for locating prototype objects in an image can be efficient, and the whole framework is also conceptually simple, i.e., given a set of object templates, the method estimates the transformations to explain the input image. However, there are two major concerns regarding the proposed framework for unsupervised object discovery.\nThe first concern is how well the phase correlation helps with estimating transformations that go beyond translational shifts? For example, rotation, scale change, and even viewpoint and illumination change? It seems that the datasets used for the experiments contain solely translational shifts of the templates, so it is hard to judge the effectiveness of the method.\nThe second concern is how well the whole pipeline deals with more complex data, for example, objects with complex appearances and cluttered backgrounds? The only quantitative comparison is performed on the Tetrominoes dataset, which comes with constantly colored blocks and black background, much simpler than the typical datasets used for evaluating unsupervised object discovery, for example, objects room and the clevr dataset. Thus the comparison falls short. Moreover, even on the Tetrominoes dataset, the proposed method does not outperform the other state-of-the-art method in terms of the ARI metric. The NGSIM dataset seems to be a better one for evaluation, however, there are only two qualitative figures, and there is no evidence that the proposed method can compete with the other methods on this dataset.\nAdditionally, as described in section 3.4, the training is not robust due to the uneven selection of object prototypes, but why does adding noise help? What kind of noise is added with how much probability?\n\n",
            "summary_of_the_review": "Besides the technical novelty issues, there are two major concerns on the proposed technique on unsupervised object discovery. The first one is the effectiveness of the phase correlation on more complex transformations. The second one is the effectiveness of the proposed method on complex objects. Currently, the comparison regarding other methods is not convincing as it is performed on a dataset that is too simple and shows no improvement in terms of ARI.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a method for unsupervised scene decomposition into individual objects and identities. The method learns a collection of templates with masks that are matched at all spatial locations to an input image and the top k matches are kept. A color module colorizes the originally monochrome templates and the image is then assembled via a greedy selection of the best fitting templates.\nThe method is evaluated on 3 datasets with varying complexity. A Tetris dataset (Tetrominoes), Atari Space Invaders, and a bird’s eye perspective of a traffic camera observing a road (NGSim).\nOn these datasets the paper demonstrates equal or better performance than previous methods while training a significantly smaller model.",
            "main_review": "Overall, the idea of using traditional methods such as template matching (either using FFT or convolutions) to reduce the problem complexity is a good idea that in this case simplifies the learning task dramatically. This can be seen in the experiments for example on Tetrominoes where the model performs on par with much larger models.\n\nThere are two main weaknesses:\n\n### W1 - Scalability\nFFT is a fast way to perform template matching across the image, but it becomes slower with template size and image size. The inference speed is only reported on the smallest dataset (35x35 images). It would be interesting to show the dependency on the size of the image and the template. I suspect that at a certain template size, a convolution might become faster than FFT. This issue is maybe part of the reason why the batchsize on Tetrominoes can be 64 while on the two other datasets (which have larger images and templates) the batchsize is 4. \n\nThe second scalability issue is that, the space of transformations is reduced to only translations. The current model for example learns one template for each of the rotations of the 7 tetrominoes in the dataset, which requires 19 templates.\nIncluding other affine transformations such as rotations or scale changes, would lead to an explosion of the search space in the matching step.\nThe other option to combat the problem is of course to simply learn more templates to cover rotations and other object transformation. However, this option will also become intractable quickly, when the complexity of the scenes increases. Other methods, that depend on more complex reconstruction mechanism (such as Slot Attention, IODINE etc.) show also good performance on some 3D datasets (e.g. CLEVR) where objects translate, scale and rotate in 3D. \n\nThe issue here is that while the method shows good performance and efficiency on simple data, it trades this efficiency with its ability to work on more complex data. With the ultimate goal of object centric learning being the ability to learn from real images in the wild, this is a major shortcoming of the proposed method. \n\n### W2 - Insights\nWhile the model itself is of simple elegance, the paper would benefit from additional analysis to allow for a deeper understanding of the method.\nThis includes for example an analysis of the effect of the number of templates and the number of instances $O_{max}$ per object. These values are selected per dataset in the evaluation, but in a real use-case (also in the NGSim dataset), the true number is unknown. It would be very valuable to understand the sensitivity of the approach to these hyperparameters.\nSimilarly, the color module could be ablated by training a model without it. This model would learn colored template variations similar to the rotations. Does this improve the results? If not, the color module resolves one factor of complexity without drawbacks which would be nice to show.\n\n### W3 - minor issues\nIn template matching, instead of simply using the top k predicted locations, a non-maximum suppression step is performed before the selection. This often creates more meaningful localizations of different instances, since it suppresses occurrences very close to each other. On perfect synthetic data this will not make a difference, but on the car dataset this might help.\n\nOn SpaceInvaders and NGSim the performance of the model could be reported and compared to other template-based methods such as ULID in terms of reconstruction error. This would allow for a quantitative evaluation outside of the very saturated Tetrominoes dataset.\n",
            "summary_of_the_review": "While the proposed method is quite elegant through its simplicity and can show good results on the used datasets, there are two main weaknesses. W1: the model likely trades efficiency with the ability to work on more complex data. This is an important shortcoming, since the grand goal of object centric learning is to eventually work on real data, while this seems to be a regression in complexity compared to other methods. W2: the evaluation could benefit from some additional experiments that shed light on the sensitivity of hyper-parameters and the significance of its individual components/modules. \nFor these reasons, my current rating is 3.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I have no ethical concerns.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed an unsupervised image decomposition method with phase-correlation networks. In comparison to previous related works, the method proposed in this paper uses a small number of learnable parameters and achieves comparable performance. The main contribution of the paper is the design of PCDNet that represents the object components of an image by the transformed versions of a set of learned object prototypes. Moreover, the frequency-domain representation of images allows the proposed method more interpretable.",
            "main_review": "+Strengths\n1) The paper proposed a PCDNet to represent the object components of an image. The idea of an image can be represented as the transformed version of a set of learned object prototypes is interesting.\n2) The use of frequency-domain representation provides insight for image decomposition. \n3) The paper is well-written and easy to understand.\n\n-Weaknesses\n1) As claimed in the paper, the proposed method is fully interpretable. However, after going through the paper, the reviewer cannot find an in-depth discussion on the interpretation of the proposed method qualitatively and quantitatively.\n2) The phase-correlation method is widely used in computer vision tasks. What are the differences between the phase-correlation method used in the paper and the original phase-correlation method (Alab et al. 2012)?\n3) The proposed method is very similar to the method proposed by Monnoier et al. (2021). Although the authors mentioned this work in the paper, the differences or modifications made by the paper are not clear. From the experiments, the proposed method indeed uses less parameters than the method proposed by Monnoier et al. (2021). I believe there must be other advantages of the proposed method over previous methods. Please clarify the advantages. \n4) The experiment is not much sufficient. It would be better if more datasets and quantitative results could be provided to demonstrate the performance of the proposed method.\n",
            "summary_of_the_review": "My recommendation is mainly based on the weaknesses of the paper such as the limited novelty and insufficient experiment.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "The paper does not involve ethical issues. ",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}