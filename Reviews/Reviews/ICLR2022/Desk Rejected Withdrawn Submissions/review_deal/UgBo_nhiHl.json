{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The reviewed work proposes a framework for training neural networks sequentially based on the concept of gradient boosting. The authors describe a simple way to adapt their framework to three major tasks in supervised learning, namely classification, regression, and ranking. Obtained results suggest an improvement over a few alternative approaches on several datasets, while ablation studies support the key architectural choices.",
            "main_review": "The authors of the reviewed paper present an effort towards a challenging and noble goal of discovering a novel general-purpose method for machine learning. Overall, after reading the manuscript there are hardly any doubts that this goal has been reached, although a few questions remain to be clarified related to the utility and effectiveness of the proposed approach.\n\nMain comments:\n\n- The paper does not seem to theoretically justify the approach. Namely, it remains unclear why the output of each new penultimate layer added to the input features of the next learner should improve the expressiveness of resulting activations and hence help to solve the problem more efficiently? In one of the final sections (i.e. 6.5. GrowNet versus DNN) the authors attempt to address the above question, by showing that the performance of the best DNN with 10 hidden layers is inferior to the one of GrowNet. Yet no good reasons for this have been presented. \n\n- The claim about efficacy is not completely supported by the evidence. The authors used 5 datasets in total to test their approach, 1 for classification, 2 for regression, and 2 for the ranking tasks. With the performance on some of the datasets (Slice Localz., Music Year Pred.) being very close to the only presented competitors XGBoost and AdaNet. Please, mention if GrowNet reaches or surpasses a SOTA performance for all these five datasets. Neither the number of datasets nor the number of alternative solutions seems to be reasonably large to make strong claims about the efficiency of the proposed solution. \n\n- Related to the previous point, the claim about efficiency could have been made stronger if GrowNet would have been also compared to other boosting-based architectures such as XBNet, AdaBoostCNN, and XGBF-DNN. Methods such as LightGBM and CatBoost are mentioned in the experiments, but not discussed in the results section. Also, adding more classical ensemble-based baselines such as bagging ensemble of decision trees, adaptive ensemble of the decision trees, random forest, and finally extremely randomized trees might have helped to provide further evidence for the value of the proposed approach.\n\n- Other boosting neural network methods such as XBNet (Jun 2021), AdaBoostCNN (2020), XGBF-DNN (January 2021), which seem to be relevant for this publication, seems to be missing from the section dedicated to Boosted Neural Nets.\n\n- More detailed information on the execution time of the GrowNet as well as on alternative approaches would provide an additional dimension to the overall discussion on the utility of the method.\n\nA minor comment:\n\n- The abstract certainly requires editing, e.g. changing \"... rendering outperforming results against state-of-the-art boosting ....\" to \"superior results\" or to \"outperforming the state-of-the-art boosting ...\". Also, spelling out \"all three tasks\" as classification, regression, and rank learning. Also, adding a sentence or two on motivation and application of the new approach.",
            "summary_of_the_review": "It seems that although the majority of the claims are supported by the paper, the central claim i.e. superior performance to SOTA boosting-based methods and DNN lacks sufficient evidence. The performance of the proposed method is only measured against a handful of alternative solutions on very few datasets. Other, seemingly available SOTA techniques from the same field are missing from the analysis. \n\nTechnical novelty. Although some aspects certainly exist in prior work (i.e. boosting mechanisms and training neural networks), the presented approach seems to be new. While contributions appear significant.\n\nAt present, the empirical evidence seems weak and inconclusive.\n\n\n\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethical issues were identified. ",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Authors present a variant of Gradient Boosting with shallow neural networks used as base learners. The base learners are not completely independent (unlike GBDTs), but the hidden layer of previous boosting stage augments the next stage learner.\n\nThe approach bears resemblance to layer-wise construction and (re-)training of a ResNet when the \"corrective step\" is employed.",
            "main_review": "Strengths:\n- Idea is a simple and straight forward.\n- Builds upon several existing works.\n\nWeaknesses:\n- Writing and presentation can be improved across the paper.\n- Evaluation is on too few datasets. Tabular data is extremely heterogenous, and a new algorithm cannot be completely characterized by its performance on a few datasets.\n- Notation in equations (2) and (3) is confusing. The overall loss function to minimize, and the objective for the weak learner are different, and shouldn't be using the same symbol for both.\n- Section 3.1 claims the base learners are independent, but Fig 1 shows that they are not.\n- Details of how \\alpha_t is learnt is not clear. Is it a free parameter? With the same learning rate as the rest of the network? What if it flips sign during gradient upgrade?",
            "summary_of_the_review": "Justification of recommendation is based on the strengths and weaknesses highlighted above.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "A gradient boosted machines (GBM) based model is proposed were shallow neural networks are used as weak learners. Training is done as in the traditional gradient boosting: one learner at a time with shrinkage applied at each step. The major difference in the model is that penultimate layers of the previous learners is stacked into an input and passed to the next learner to allow representation learning.\n",
            "main_review": "My main concern with the proposed approach is that why should we use the greedy additive training (gradient boosting in this case) when we can optimize the given model properly? If we consider the objective function at the top of the page 4 (let us call it eq. 0), then the model in Fig. 1 can be optimized jointly. For this matter, we can consider such a model as another neural network which is fully differentiable and can be trained using SGD. In fact, ResNet can be considered as an example of such methodology. The main purpose of using greedy additive models for trees is that the joint optimization for the collection of trees is non-trivial (it is NP-hard even for a single binary tree). Therefore, using some approximation makes sense. But here, directly using SGD (or any other gradient based optimization) not only applicable but also fast (i.e., no need to train each weak learner sequentially followed by the corrective step). Experiments section does not include this obvious baseline. \n\nAlso, the technical novelty sounds  insignificant to me. Boosted neural nets have been studied before (also mentioned by authors). Similarly, applying hessian information is a dominant method for training gradient boosting machines since introduction of XGBoost (but I believe it was introduced before). Combining features of neural net (possibly with input) is commonly accepted technique in ML literature (e.g. ResNet).\n\nExperimental results show better or comparable performance w.r.t. XGBoost on relatively large datasets. However, since the final model is effectively a neural net, I guess the right baseline would be another neural net of the same (or similar) complexity. Also, authors emphasize that the particular advantageous of their model (compared to DTs) is that the correlation of features (like in images) will be taken into account. However, the benchmarks used in the paper are so-called tabular datasets and there is no image or textual datasets where the true power of representation learning can be seen.",
            "summary_of_the_review": "The proposed method has some merits: good literature review, clean and concise explanation of the method, experimental evaluation on several important tasks, etc. However, as of now, I'm leaning towards rejection of this paper due to concerns in methodology, novelty and experiments (as I have mentioned in the main review).",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes using shallow neural networks as weak learners in gradient boosting. The differences with the classic boosting approach are: 1) at each boosting step, the original features are augmented with the output from the penultimate layer of the previous iteration, 2) the authors use a corrective step to optimize all the parameters (to improve initial greedy approximation).",
            "main_review": "Strengths:\n* The proposed approach is simple, and the authors provide empirical analysis to illustrate its properties\n* The paper is well written and easy to follow\n\nWeaknesses:\n* For an empirical paper, the number of datasets and baselines is very limited\n* The technical novelty is limited\n* The paper misses related papers that appeared in the last several years \n\nThe main idea of the paper is to use neural networks instead of regression trees as base learners in gradient boosting algorithms. Thus, from a technical perspective, the idea is relatively straightforward: it is known that gradient boosting can be combined with any base models, and the fully-corrective step was also used in previous literature, similarly to second-order methods.\n\nThus, the main contribution of this paper is the empirical analysis of a particular model. For an empirical paper, the number of datasets and baselines is limited: two datasets for regression, one for classification, and two for learning-to-rank. Among the baselines, there are only XGBoost and AdaNet. \n\nThe lack of unified benchmarks for tabular data research field leads to results that cannot be compared since they are evaluated on different data. This problem is discussed, e.g., in [1], where the authors evaluate several recently proposed deep learning models in a unified setup. They use GrowNet as one of their baselines and note that MLP is often on par or even better than GrowNet. This leads to questions regarding practical usefulness of the approach on various data.\n\nAnother concern is that the paper does not cite recent related papers: the field of learning on tabular data is quite popular now, and there are many papers on applying neural networks to tabular data. \n\nSmall comments:\n- n+ and n- are not defined in Algorithm 1\n- In  Chen & Guestrin (2016), I don't see NDCG@5 that is claimed to be taken from this paper; and the value for NDCG@10 differs\n- For XGBoost tuning, it is strange that the number of trees is a hyperparameter chosen among 5 options: usually, the maximum number is limited and the best number of trees is chosen on a validation set\n\n[1] Gorishniy Y. et al. Revisiting Deep Learning Models for Tabular Data //arXiv preprint arXiv:2106.11959, 2021.",
            "summary_of_the_review": "It is a well-written empirical paper that uses a limited number of datasets and baselines for the experiments and does not discuss recent related work.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}