{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, the authors proposed a unified approach to deal with context-free issue in visual dialog. Specifically, they first tried to disentangle information within questions, which was learned in a pretrained process. Then a novel variational attention mechanism was developed and applied to visual dialog task, aiming to address context-free visual dialogs. ",
            "main_review": "++ The authors applied information-theoretic latent disentanglement to alleviate the issue of language prior in vqa and visual dialogs. \n\n++ The authors proposed a novel discrete variational attention mechanism to visual dialog task. \n\n++ The description is in relative detailed.\n\n-- The paper should include stronger baselines.\n\n-- In table 1, on VQA v2, adding regularization (claimed as one contribution) got worse performance? But on VQA-CP v2, got better performance? Shouldn't the trend be the same?\n\n-- From the experimental results, I'm not very convinced by what the z^s (style) learned. And how that contributed to language prior. \n\n",
            "summary_of_the_review": "The paper tried to solve vqa, visual dialog with the help of information theory, especially on alleviating language prior. But the results are kind of contradicting, and the results cannot fully demonstrate the author's claims. In addition, the baselines are not strong enough. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a method of context-free visual dialogs. Deterministic soft attention mechanism has been proved significant for context-relevant questions in visual dialogs, but it may receive too much irrelevant information for context-free questions. Hence the authors propose a unified approach which incorporates a pre-trained VQA model and a variation attention mechanism to produce context-free and condition relevant information separately.",
            "main_review": "In summary, their main works can be summarized as followed:\n(1) The authors introduce VQA task into visual dialogs for responding context-free questions.\n(2) The authors propose a regularization scheme for VQA model to alleviate language priors and to strengthen the dependence of vision and language.\n(3) The authors present a discrete variational attention mechanism to identify context-relevant conversations for visual dialogs.\n\nOverall, this manuscript is easy to follow. The technical contribution of this method is significant. The experiments conducted on VQA v2, VQA-CP v2 demonstrate the effectiveness of the proposed regularization scheme for VQA task. The experimental results conducted on VisDial v1.0 also show the superior performance over some state-of-the-art methods.\n\nThe reviewer suggests weak accept. The weakness of this paper is as follows.\n\n(1) The authors should provide the detailed improvement of visual dialogs performance under different pre-trained VQA models, i.e., would the more powerful VQA model like [1] and [2] improve the visual dialog’s performance?\n(2) The formula (1) has some confusion that the content representation is learned from questions but extracts style representation from answers. \n(3) Visualization for attention is needed for visually proving the proposed variational attention. \n(4) Some typos need to be double checked, e.g., “variational attention modules the attentive distribution…”.\n\nReference:\n[1] Greedy Gradient Ensemble for Robust Visual Question Answering.\n[2] Removing Bias in Multi-modal Classifiers: Regularization by Maximizing Functional Entropies.\n",
            "summary_of_the_review": "I am a researcher interested in visual dialogue and published many papers on top conference related to this topic.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No such concerns.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposed to address the context-free issue in the visual dialog task. This paper proposed (1) a disentanglement network that is pretrained on the VQA task to address this issue, and (2) a variational attention mechanism to reserve the context-relevant information.",
            "main_review": "**Strengths**\n\nThe idea of achieving the disentanglement of question feature for VQA and visual dialog is novel and interesting.\n\nThe idea of using variational Bayesian framework in visual dialog is novel.\n\nThis paper evaluated the proposed method on two VQA datasets and one Visual Dialog dataset.\n\n**Weaknesses & Questions**\n\nIn Section 1, this paper states that a challenge for visual dialog is that \"parts of visual dialog are not consistent and continuous\". Is there a more detailed and comprehensive explanation of this motivation?\n\nThis paper proposed a hard attention mechanism to exclude irrelevant dialog histories. It seems that previous visual dialog methods can be transferred to a discrete variant by setting a threshold. For example, in Figure 4, it is easy to set the threshold to transfer the soft attention to the hard attention. Can we do this in a such simple way? If so, what is the advantage of the proposed variational attention? If not, what disables this? This concern is highly related to the novelty and contribution of this paper.\n\nThe definition of \"context-free\" is not clear. In Visual Dialog, there are two types of context, visual context (image) and language context (dialog history). It would be better if a clear definition of \"context-free\" can be given. \n\nThe idea of applying a variational Bayesian framework has been applied in (Zhang et al., 2018) (Niu et al., 2021) for visual grounding (a.k.a., referring expression comprehension), another vision and language task. As visual question answering and visual dialog are also typical vision-and-language tasks, it would be better and more complete to include the discussion and comparison of this work in the aspect of variational Bayesian framework and attention mechanism.\n\nThe improvement of discrete attention is marginal. As shown in Table 2, the discrete variant achieves comparable performance on most metrics. Although the proposed methods outperform other baselines on R@1 by more than 1.5, these results cannot demonstrate the effectiveness and contribution of the discrete attention.\n\nTable 1 shows how the hyper-parameter $\\gamma$ affects the VQA model. However, how $\\gamma$ affects the visual dialog model is lacking.\n\nQuestion: which dataset is finally used for VQA pre-training?\n\n**Typos**\n\nSection 3, line 3, \"treat ... is a pre-trained model\" -> \"treat ... as a pre-trained model\"\n\n**References**\n\nZhang et al., Grounding Referring Expressions in Images by Variational Context. CVPR 2018.\n\nNiu et al., Variational Context: Exploiting Visual and Textual Context for Grounding Referring Expressions. TPAMI, 2021.",
            "summary_of_the_review": "Considering both strengths and weaknesses listed in \"Main Review\", I vote for rejection at this stage.\n\nThe novelty of this paper is considering the disentanglement of question feature and using variational Bayesian framework.\n\nMy concerns are (1) the contribution and effectiveness of the proposed discrete attention are limited; (2) some implementation details are not clear. This paper may need a major revision.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}