{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper tackle the problem of federated distillation and proposes 2 complementary methods to help improve the performance of the global model by using logits from the k local/edge models. The 2 methods are:\n\n1. The paper hypothesizes that the prior relationship of the label space can be very useful in enabling the global model to learn from the local models. So, the authors propose to use Optimal Transport to encode the inter-label relationships\n2. The authors also propose a weighting mechanism while distilling from the local models that uses L2 Distance based measure. This is done to mitigate the bias arising from the potentially skewed dataset used in the local models\n\nThe loss used is w_1*D_1+w_2*D_2+ .... Where w_i are weights coming from (2) and D_i are the distances / losses coming from (1)\n\nTo compute D_i they use the sinkhorn distance between the global model label distribution and the local model label distribution on a distillation set. \nTo compute w_i they propose to use the L2 distance between the predicted probability and the mean predicted probability for each label. \n\nResults:\nTo compare the results the authors propose a distance measure -> Semantic Distance which tries to incorporate the relationship between the labels while assigning a score.\nAll the experiments are performed on tasks that have an ordering on the labels like sentiment analysis (strongly positive, positive, .. negative, etc.), Emotion Recognition (anger, happy, etc.) and NLI (entailment, contradiction, neutral)\n\nThey use different weighting mechanisms: unweighted (A), dataset size based weighting (D), L2 distance from the uniform distribution (U), (proposed) L2 distance from the mean probability prediction (E). \nFor the distance measure, they use entropy based distance (KL divergence) as a baseline. \nThe cross product of the above two leads to the final results tables.\n\nWe need to look at F1 score and the semantic distance across the 3 tasks \n\nLets look at the F1 score :\nSentiment Analysis:\n1. When the L2 distance from the mean (E-weighting) is used the performance (F1) between Sinkhorn and Entropy based distances is same or very close by\n2. With no weighting (A-weighting), Sinkhorn is slightly better than Entropy based distances in terms of F1\n\nEmotion Recognition:\n1. When using E-weighting, the results (F1) are quite mixed as sinkhorn wins sometimes and Entopy distance wins other times\n2. With no weighting (A-weighting), Sinkhorn always performs better in terms of F1\n\nNLI:\n1. When using E-weighting, the results are almost identical (F1)\n2. When using A-weighting, the entropy based method is identical or better (for the global task) in terms of F1\n\nAcross the board the Semantic Distance is smaller for the Sinkhorn distance case compared to the entropy distance case.",
            "main_review": "Strengths\n1. The paper is well written with good references and explanation to the problem.\n2. The authors provide a strong hypothesis the the prior label relationship and novel weighting mechanisms should help Federated learning algorithms\n3. They build quite strong baselines and test their hypothesis across different axes\n4. For the cases where we need better semantic relationships among the predicted labels the paper shows that OT/sinkhorn distance provides a good inductive bias to the model.\n\n\nWeakness\n1. I don't find it surprising that the semantic distance is smaller for Sinkhorn based methods compared to the  Entropy based distances. It was just a matter of applying a loss function that can incorporate the label relationships. What would have been really interesting is to see a huge jump in F1 performance by virtue of encoding the label relationships. But we see quite mixed signals in the table 2-4. It seems like the gains are very task dependent\n2. For the local datasets, it would have been good to provide the performance of the local models to see how much headroom there was for us to get a better contextualize the differences across the different global models. Right now it is not very clear if a change in 1% in F1 is significant or not.\n3. The paper doesn't do a very good job of checking if the gains that we see in some tasks are by virtue of using OT/sinkhorn in training the model in general or if the gain is truly coming from sinkhorn + federated distillation. i.e If we had simply trained the model with OT (optimal transport) irrespective of the federated distillation would we still have got the same gains? In other words, is OT simply improving the model performance and not the distillation efficiency?\n4. The paper doesn't really justify why the L2 distance with the mean is a good measure. In fact this assumption doesn't always seem to hold good. When we look at NLI task the data-set based weighting beats everything else by a huge margin of 8% F1. This just feels like a heuristic that worked out. In general it would have been good to analyze when a certain weighting mechanism worked based on the composition, overlap, task characteristics, etc. of the different datasets used.\n5. The authors argue that the model confidence in itself is a bad idea. So, they use the L2 distance from the mean. Essentially, weighting mechanism is doing a form of mean normalization. It would have been interesting to see mean + variance normalization as another heuristic.\n6. Section 5.4, Figure 4. While we see some small difference in the graph between sinkhorn and entropy, it doesn't seem drastically different. Given that we are noticing task specific gains, it would have been good to get the same table for other tasks. \n\nMinor things:\n1. The paper discusses a few things that don't really add to the discussion of the paper. ex: Proposition 5.1 \n2. When the mean bias terms are calculated, the paper mentions they take it over a distribution of noise. What does that mean? \n- I presume it is just taking mean over all input data.\n3. Minor typos / missing definitions: \n    (a) In Section 1, 4th line. \"enhances\" -> \"enhanced\"\n    (b) In Section 1, above \"Contribution:2\". \"general\" -> \"generalize\"\n    (c) In Section 3.1, it would be good to define Y_s and Y_t\n4. The bolding on the tables are quite misleading. There are many entries with same value but only one of them is cherry picked and bolded.\n\n",
            "summary_of_the_review": "The paper proposes 2 complementary methods to improve Federated distillation. (1) Optimal transport (OT) to encode prior knowledge of labels (2) A mean normalized confidence heuristic. \nOverall the paper does a good analysis on the 3 selected tasks where there is a relationship to the labels. It paves the way for leveraging OT to be used in FD. We clearly see a gain in semantic distance (SD) across the tasks. And this can be a good thing for model stability.\n\nThat said, the paper leaves some questions unanswered and lacks rigor in subtle areas that would help us truly attribute the gains to the proposed techniques and be able to extend them to other areas of NLP. The F1 results show gain in some areas but lack in certain tasks, making the F1 gain quite ambivalent. It is not quite clear if this can be extrapolated to other tasks.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper describes approach to performing Federated Distillation using Sinkhorn distance for the problems where natural similarity metric between the labels applies (such as NLU). The minimization of the optimal transport cost of the global model’s predictions from the confidence sum of soft-targets assigned by local models gives the global model. To compute the confidence score of a model the L2 distance of a model’s prediction from its probability bias is used. The method claims improvements of the global model’s performance over the baseline designed on three NLU tasks with intrinsic label space semantics: fine-grained sentiment analysis, emotion recognition in conversation, and natural language inference using experimental analysis results comparing cross entropy with the sinkhorn based approach and showing the F1 score versus the similarity metric (called Semantic Distance). ",
            "main_review": "I like how the paper began by describing the problem with the distributed federated learning and the issue with the imbalanced data sets and the bias it introduces in the label distribution and the local models trained. The proposition of the the sinkhorn distance for normalizing the weights from the local models, before combining them globally sounds like an interesting proposition. In addition, the authors formulation of the Semantic Distance as a means of judging the classifier performance (rather than the label) for taking into account the underlying bias is also interesting.\n\nHowever, there are number of important hings the paper fell short on\n1) The claims are not supported by the experimental results. The proposed method is shown as giving better performance in certain instances and not the other\n2) the  explanation of why sometimes cross entropy or a weighted combination gives better performance is unclear\n3) how SD metric is achieving a useful (and consistent measure) that supports the claims is also unclear\n4) the setup uses similar architecture for local and global models and the authors claim (intuitively) that the proposed solution is architecture independent, this makes the evaluation weaker\n5) There are English mistakes, which should be corrected but moreover in some instances the important details are not well defined (such as [method]-E under section 6 is defined as an e.g.\n6) why the global set was one with more e.g. throughout (this is given as justification for number of puzzling figures in tables 1-4)\n7) Plot 4 and accompanying explanation is unclear, if we are increasing the variance can we claim the said benefit for SD? Also, where is the statistical sig. diff. (as opposed to hand waving the difference based on absolute value) for the box and whisker plot\n8) Transfer dataset details are missing (except mention what it is), how did the perf. looked like on these?\n9) tuning is mentioned but details not covered\n10) comparative perf. against other approaches on FD missing",
            "summary_of_the_review": "I like how the paper began by describing the problem with the distributed federated learning and the issue with the imbalanced data sets and the bias it introduces in the label distribution and the local models trained. The proposition of the the sinkhorn distance for normalizing the weights from the local models, before combining them globally sounds like an interesting proposition. In addition, the authors formulation of the Semantic Distance as a means of judging the classifier performance (rather than the label) for taking into account the underlying bias is also interesting.\n\nHowever, there are number of important hings the paper fell short on that have to do with the experimental setup and subsequent analysis of the evaluation performed and observed statistics. It left a lot to be desired in terms of being able to support the claims that the said method improves the performance over cross entropy or balancing approach and efficacy of the SD metric is also unclear. A comparison against other state of the art FD approaches is also missing.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the author studied the problem of federate learning for NLU tasks. Specifically, the authors proposed to learn the global model with the objective to minimize the optimal transport cost of the global model's predictions from the confident sum of soft-targets assigned by local models. Experiment results show that the proposed model outperforms three baselines",
            "main_review": "Reasons to accept\n1. The problem of federate learning a an important problem and will have a lot of applications in the industry. It also helps mitigate some of the ethnic concerns of machine learning algorithms and protect user privacy. \n2. The methodology seems to be novel and non-trivial, which may inspire following researchers.\n\nReasons to reject\n\nThe paper is not well-written, and the authors are welcome to do thoroughly proof-reading. For example: \n\n1. Page 2, Contribution 2: \"As shown in fig. 1,... contribution 1,... contribution 2\". This is really confusing and I don't know what are the contributions that you are trying to highlight.\n2. Page 2, you used Italic for Lipschitz, then please be consistent and also use Italic for Rademacher. \n\nThere are many more typos or errors in the paper and they really affect reading and understanding the paper.\n\nOn the other hand, the experiment lack the comparison to other baseline algorithms. It seems that the authors only compare the different variety of the proposed model. What about the \"real\" baselines?",
            "summary_of_the_review": "In general, I am not incline to accept this paper in its current form. I would encourage the authors to improve the writing of the paper and to add more experiments, or discussions on why other baselines are not appropriate. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The submission targets a federated learning scenario where one can upload the local model's prediction but not the local data or the local parameters.\nIt suggests using the Sinkhorn distance to measure the distance between the global model's prediction and the local model's prediction, which replaces the KL loss commonly used by the existing methods for knowledge distillation.\nIt further suggests weighting the samples produced by a local model based on the local model's confidence, where the \"confidence\" is defined as the L2 distance of the model’s prediction from its bias.\nThe performance is on par with the baselines in terms of traditional metrics such as F1 and accuracy, but better in terms of a new metric proposed by the submission.",
            "main_review": "Pros:\n- The observation that \"there exists natural similarity metric between the labels in many natural language understanding (NLU) tasks\" is interesting.\n- The writing is easy to follow.\n\nCons:\n- The experiment results are very weak. Almost no improvement on the major metrics such as F1 and accuracy. The ablation studies also suggest an insignificant or unreliable improvement.\n- The custom metric proposed by the submission might be problematic. It seems possible that a model that yields much worse accuracy can have a better score in terms of this new metric. Traditional metrics such as the normalized discounted cumulative gain (nDCG) may be a better option for measuring the \"closeness of the output distribution against the ground truth\".\n- It is debatable whether the proposed \"confidence\" score is indeed confidence.  Traditionally, confidence is linked to the variance of the logits (assuming a Bayesian model), rather than the value of the logit itself.\nWith the definition proposed by this submission, the global model may be corrupted by a local model that randomly makes its prediction by picking a random vertice of the simplex shown in Fig 2.\n- The OT loss does not seem a straightforward solution for exploiting the fact that \"there exists a natural similarity metric between the labels\".\nOT can help the case where model-1 predicts \"strong negative\" while model-2 predicts \"negative\" if there are *enough* samples for OT to realize that model-1 tends to give a much lower score than model-2.\nYet OT is not sample-efficient when it comes to learning the fact that \"distance(neutral, positive) < distance(strong negative, positive)\", compared to methods that simply leverage the labels' text embeddings.",
            "summary_of_the_review": "Weak results, and many technique choices are not well justified.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}