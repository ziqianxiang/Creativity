{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, the authors propose to replace several first cells of a child network with pre-trained layers and let NAS search for the remaining cells. This method is based on the observation that the low-level feature maps between the networks from NAS-Bench-201 search space and a a hand-crafted one are very similar. By freezing the first few layers, the memory footprint and convergence time are both reduced. The analysis and the experiments are done only on CIFAR-10/100 and ImageNet-16-120, and the networks from NAS-Bench-201 and ResNet-32, which is quite limited.",
            "main_review": "1. The biggest problem with this paper is that the authors only studied networks from NAS-Bench-201 and ResNet-32 on datasets CIFAR-10/100 and ImageNet-16-120. It is questionable that the same observation and conclusion can be made on other models and datasets:\n\nThe observation low-level feature maps generated by NAS-Bench-201 architectures share similar characteristics to those from ResNet-32 are made on CIFAR-10. As we know, CIFAR-10 is a relatively easy datasets; there may be not much low level features or patterns for a network to capture and hence the low-level feature maps from different networks are similar. Would the same observation hold on more challenging dataset?\n\nIn the paper, the authors only used ResNet-32 as the pre-trained network. This network isn't particularly popular in practice. Have the author tried other networks? What about networks for other tasks like segmentation, detection?\n\nWith different pre-trained models and different datasets, is it still possible to replace and freeze as many as 5 layers without performance drop? If not, then the memory and computation gain may not be as big as shown in the paper.\n\n2. The ablation studies are off the topic. I don't think for this paper, readers are interested in how learning rate warmup and cutout augmentation are effective.\n\n",
            "summary_of_the_review": "This paper brings up an interesting topic but unfortunately the analysis and experiments are limited to particular benchmarks and models. I suggest the authors submit the paper to other conference with more solid analysis and more comprehensive experiments.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This work uses pre-trianed ResNet block replace the shallow trainable blocks in DARTS-based neural architecture search frameworks and frozen those parameters for saving the memory consumption during the training process. \n",
            "main_review": "Advantages:\n\nThe authors attempt to explore the relations between automatic designed and hand-crafted neural networks, which is a useful try for the NAS community. \n\nThe key observations are interesting and the experiments are convincing and show multiple views of observations. \n\nThe paper is well-written and easy to read.\n\nWeakness:\n\nThe authors claim that the low-level representations of NAS-bench-101 architectures and ResNet-201 are similarly based on the CKA similarity metric. However, in Figure 1, the representations of the layers in the same positions of the NAS-bench-101 and ResNet-201 show high CKA similarity including low-level, middle-level, and high-level. It seems the motivations of replacing the shallow block of NAS with pre-trained ResNet blocks are not sufficient enough.\n\nThe method of using some hand-crafted architectures to replace part of the NAS framework may require extra time and manual-design works (e.g., calculating the similarity of representations in this paper, manually designing a proper block structure). This probably will increase the cost and difference with the automatically designed intention of NAS.\n\nThe related work survey is not sufficient. The recent work of the memory-efficient NAS should be fully explored and analyzed.  \ne.g.,    \nMCUNetV2: Memory-Efficient Patch-based Inference for Tiny Deep Learning, NeurIPS 2021    \nMemNAS: Memory-Efficient Neural Architecture Search with Grow-Trim Learning, CVPR 2020    \nMemory-Efficient Hierarchical Neural Architecture Search for Image Denoising, CVPR 2020    \n\n\nBesides fro-zing parts of parameters during the training process, should any other contributions be noticed?\n",
            "summary_of_the_review": "The paper introduce interesting observations and provide sufficient experiments about their method. My concerns mainly focus on the novelty. The parameter or layer frozen is a popular strategy and largely depend on objective tasks. So the further effectivenesses of the method for different types of tasks are unclear. Meanwhile, if the auto learning layers are not memory-efficient compared with manually-designed layers. Is there any way to make it better? Replacing them back to manually-designed layers will also meet the time-consuming cost. How to deal with the issue?\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper proposes to improve sample efficiency (searching time) and memory efficiency of a search by fixing the first few layers in a CNN to come from a pretrained network (e.g., Resnet), thus reducing search space size.\nThis practice is motivated by empirical observations that output features generated by those early layers are often similar (as measured using Centered Kernel alignment).\nResults include evaluation on NAS-Bench-201 and DARTS CNN space, comparing to Ragularized Evolution (RE) and DARTS-PT.",
            "main_review": "The paper raises some interesting questions regarding search space design and its effects on searching efficiency - although the authors seem to be focused mostly on presenting their work as a searching methodology (incorrectly?). \nIn general, even though the paper takes an interesting angle on NAS, unfortunately motivation and evaluation both lack critical pieces for the work to be accepted, in my opinion.\nPlease find the detailed commentary in points below.\n\n**Strengths:**\n\n1. Incorporating blocks from pretrained networks directly within searched networks seems to me like an interesting case-study that hasn't received much attention so far (although I might be missing some most recent works in that regard, the closest I'm aware of would be NAS based on blockwise distillation, but lack of distillation make the submitted paper somewhat different).\n2. The authors show that their method can improve correlation between validation and test accuracy of models, which is an often overlooked property of NAS that can highly affect outcome.\n3. The paper is generally well-written and easy to follow.\n4. Most (all?) of the experiments report average performance.\n\n**Weaknesses:**\n\n1. As already hinted, the proposed searching methodology is not really a searching (NAS) algorithm but rather an empirical study of designing a search space by utilizing pretrained models.\nAlthough a difference might not seem critical at first, I find it especially important since search space design constitutes a somewhat independent research direction within NAS [1-4].\nHowever, the authors are either unaware of this line of work or chose not to position their work against it - in either way, I find this to me a major downside.\nIn general, search space design works focus on slightly different objectives and should follow a different evaluation paradigm compared to works that focus purely on improving searching methodology.\nFor example, I would expect the paper to address issues like: how does the best achievable model change after we swap some block in the NB2 search space? How does average accuracy of models in the new, reduced search space compare to the original search space?\nHow does performance of different NAS algorithms change as we keep reducing the size by including more pretrained blocks?\nInstead, the paper tries to emphasize the fact that reducing the search space size can help reduce memory requirements behind running NAS.\nIt's a correct statement but to present a full picture it would be necessary to comment on implications related to the search space.\nTo put it briefly - the method achieves better memory efficiency due to smaller search space size (so, e.g., a supernet has to be smaller) but at the same time this comes at a price of limited discoverability of new models (we have fewer of them in the search space) - the second part is left completely unanswered.\n\n2. Following on evaluation, even disregarding missing parts related to search space design, what currently is presented seems a bit unusual to me and I'm not sure if the results can be considered conclusive because of that.\nSpecifically, the authors seem to have trained most of the networks from NB2 for only 12 epochs whereas full training takes 200 epochs.\nWhat is more, the pretrained ResNet was actually trained for 200 epochs (per Appendix A1).\nSince training for 12 epochs might easily be non representative of full training, an obvious question that arises is how do findings presented in this paper change as we train all models until convergence?\nAlso, it seems that there is a discrepancy between training scheme in section 3.1 (weights were taken from NB2, I'm guessing those were weights of fully-trained models?) and experimental results that was using proxy training, making the current narrative less convincing.\n\n3. Another thing related to evaluation is the choice to report correlation between validation and test as the main (only?) evaluation criteria.\nAlthough I can see why this aspect could be important for NAS, the authors actually never explain why this criteria was chosen.\nWhat is more, even if the explanation was there, in the end what we care about in NAS is performance of the obtained architectures - in that regard we can see that, e.g., for query-based NAS the proposed method does not improve in neither final accuracy nor sample efficiency (Figure 4) which raises a question: what is the point of presenting improvements in test-validation correlation in Table 1? In general, the entire section 3.1 - even though it touches upon a (potentially) relevant problem in NAS - seems to be be a wrong premise in the context of the presented work.\nHow does better test-validation correlation imply that \"the proposed method is well-suited for NAS\"?\nHigh correlation is usually desired if we're using proxy metric A to optimize for the metric of interest B - this could be the case in the submitted paper, e.g., if the correlation was measured between accuracy of a network with and without a pretrained ResNet block.\nHowever, this does not seem to be the case.\n\n4. The paper is missing comparison to some relevant existing work (even disregarding those mentioned in point 1).\nThere is actually quite a few papers that try to either: 1) reduce memory cost of NAS, especially differentiable NAS [5-7]; or 2) use, in some way, pretrained blocks when searching for more efficient networks [8-10].\nI suggest the authors try to position their paper in the context of those works, if not taking the road of search space design.\n\n5. Some purely technical aspects of the evaluation are also not very convincing in my opinion.\nFor example, the authors present performance of DARTS-PT on the DARTS CNN space as 2.78% error on average, however according to the original paper it is 2.61% - this is a big different in the context of DARTS and would demand some explanation from the authors.\nAlso, the results achieved by the proposed method (2.81%) are actually quite bad and do not help convincing that the proposed method can robustly deliver good results.\nOn another note, I don't understand why the authors say that \"It is noted that the warmup phase does not introduce any extra cost\"... the cost might not be high but as long as it require some additional compute the authors should not make claims like that... I would even go a step further and say that the cost of pretraining a ResNet should be included in the searching cost.\n\n6. Some minor issues:\n - the authors never report number of parameters or FLOPs of the new models - are blocks taken from ResNet smaller/larger than those that get replaced?\n - I would really rethink saying things like \"(...) NAS-Bench-201 that have high diversity in both topology and operations\". NAS-Bench-201 is the simplest NAS search space which can be found in NAS research that I'm aware of. Claiming that it has high diversity of topology and operations is bit of a stretch (especially when it comes to operations, I guess topology-wise it could be disputed when compared to some classical hand-crafted networks).\n\n\n**References:**\n\n[1] I. Radosavovic et al. \"On Network Design Spaces for Visual Recognition\". 2019\n\n[2] I. Radosavovic et al. \"Designing Network Design Spaces\". 2020\n\n[3] Y. Hu et al. \"Improving One-Shot NAS with Shrinking-and-Expanding Supernet\". 2021\n\n[4] Y. Ci et al. \"Evolving Search Space for Neural Architecture Search\". 2021\n\n[5] X. Chen et al. \"Progressive Differentiable Architecture Search: Bridging the Depth Gap between Search and Evaluation\". 2019\n\n[6] H. Cai et al. \"ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware\". 2019\n\n[7] Z. Guo el al. \"Single Path One-Shot Neural Architecture Search with Uniform Sampling\". 2020\n\n[8] C. Li et al. \"Blockwisely Supervised Neural Architecture Search with Knowledge Distillation\". 2020\n\n[9] B. Moons et al. \"Distilling Optimal Neural Networks: Rapid Search in Diverse Spaces\". 2021\n\n[10] P. Molchanov et al. \"HANT: Hardware-Aware Network Transformation\". 2021",
            "summary_of_the_review": "An interesting take on NAS but unfortunately poorly positioned in the current literature and with quite a few places where decisions behind evaluation methodology are not very convincing, and in general evaluation is not very comprehensive. Definitely more work is needed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}