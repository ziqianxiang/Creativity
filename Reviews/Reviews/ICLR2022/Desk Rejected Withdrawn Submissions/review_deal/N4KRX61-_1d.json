{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper proposes a framework of Symbolic Reward Machine (SRM), an extension of Reward Machine (RM), for specifying interpretable and explainable reward functions. Then, a Bayesian IRL method is proposed to concretize an SRM using expert demonstrations. Experimental results demonstrate the effectiveness of SRMs in terms of training efficiency and generalization. The reviewers acknowledged that the problem of inferring symbolic rewards is important and that the proposed SRM framework is an important step in this direction. However, the reviewers pointed out several weaknesses in the paper and shared concerns, including (a) limited comparison with alternate approaches to tackle the problem and positioning w.r.t. the existing literature; (b) the domains seem rather simple since they require only a few demonstrations (implying that the holes being inferred might be quite small); (c) the novelty and theory around the SRM representation is not fully clear. I want to thank the authors for their detailed responses. Based on the reviewers’ concerns and follow-up discussions, there was a consensus that the work is not ready for publication. The reviewers have provided detailed feedback to the authors. We hope that the authors can incorporate this feedback when preparing future revisions of the paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents Symbolic Reward Machines (SRM) which helps in achieving interpretable and explainable reward functions. The paper also proposes a hierarchical Bayesian framework to concretize Symbolic Reward Machines using expert demonstrations. The author evaluates the proposed method on a Mini-grid environment with varying complexity and compares against state-of-the-art methods.",
            "main_review": "- The paper is well written with good details on SRM and proposed hierarchical Bayesian learning framework.\n- The author presents good motivating example for SRM.\n- The paper presents good literature survey and situates the proposed work well with current literature.\n- The paper evaluates the proposed method on a grid-world which discrete problem. It would be good to see some experiments on continuous control tasks (e.x. Mujoco). At least the authors can comment on how the proposed method on continuous control tasks.\n- The authors are also encouraged to comment on how these method can be extended on a real-robotics tasks.\n- What is the time and space complexity of Algorithm 1?\n",
            "summary_of_the_review": "Already explained above.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper deals with the problem of learning non-Markov task specifications (or rewards) encoded as a reward machine by inferring the real-value parameters of a reward machine (typically the edge transition rewards in an RM). The authors adopt a Bayesian deep learning approach to simultaneously learn a reward approximator, a distribution over the parameters of the reward machine, and the policy to optimize the most-likely reward instance. \n\nThe authors then go on to demonstrate in grid world environments that benefit from non-Markov rewards and policy representations that the inferred RM outperforms standard reward definition using standard RL approaches. And they further demonstrate that the inferred reward machines are beneficial  in larger instances within the same domain.",
            "main_review": "**Major comments**:\n\n*Differences between Reward machines and SRMs*: The authors propose SRMs as a novel framework, but I wasn't able to grasp how SRMs differ from regular reward machines with unknown edge transitions reward. Perhaps it would be a good idea for the authors to reframe their presentation to be contrastive in terms of the different representation in regular reward machines and symbolic reward machines. \n\n*Comparisons with previous work*: The primary comparisons provided by the authors seem to be with approaches that do not model the non-Markov structure of the reward function in learning from demonstration. Thus these approaches are bound to demonstrate poorer performance when confronted with tasks where the reward function and therefore the policy is dependent on the history. Reward machines are a way to succinctly summarize the history, and thus make the synchronous product Markov, thus allowing the use of MDP based RL algorithms. A true baseline for this task would include reward specifications learnt in non-Markov forms. Toro-Icarte et al. [1] learn reward machines directly (but as opposed to this paper, they learn both the structure and the reward parameters, whereas this work only focuses on the reward parameters, and assumes the strucuture to be given). In the same vein, Shah et al. [2], and Vazquez-Chanlatte et al. [3] infer LTL formulas that can be translated into sparse reward machines. In fact Toro-Icarte et al. [4] have also proposed reward shaping algorithms that transform sparse LTL-RM rewards into denser reward machines that are easier to learn. But the comparisons in this paper merely show that inferring structural reward information is helpful. That said, the proposed approach is different from prior published approaches, but a direct comparision is helpful towards a more complete study\n\n*Clarity concerns:* This work appears to build significantly on Bayesian GAIL and the work by Fu et al. 2018. Certain concepts and notations from those works are built upon without adequate motivation or explanation in my opinion. It certainly helps the paper to be self-contained. Further the first three paragraphs of Section 5 will be served well with a major rewrite such that the mathematical reasoning flows better in the paper. \n\n*Results by varying number of examples:* I had quite a bit of trouble in intuiting how the change in number of training example would impact the solution quality. Are those curves essentially flat, or is it indicated in some examples that fewer examples is actually more benefecial?\n\n[1] - Toro Icarte, Rodrigo, et al. \"Learning reward machines for partially observable reinforcement learning.\" NeurIPS (2019): 15523-15534.\n\n[2] - Shah, Ankit Jayesh, et al. \"Bayesian inference of temporal task specifications from demonstrations.\" NeurIPS (2018).\n\n[3] - Vazquez-Chanlatte, Marcell, et al. \"Learning task specifications from demonstrations.\" NeurIPS (2018).\n\n[4] - Icarte, Rodrigo Toro, et al. \"Reward Machines: Exploiting Reward Function Structure in Reinforcement Learning.\" arXiv preprint arXiv:2010.03950 (2020).\n",
            "summary_of_the_review": "I believe learning symbolic reward machines is an interesting problem framing, and the paper is enroute to making an important contribution. My primary concerns remain the positioning of the paper with respect to prior work and the difference of the formalism vis-a-vis the original definition of reward machines. I am inclined towards this paper but not in its present form, and I can be persuaded to upgrade my score if the presentation of the technical approach is improved, and the method is contrasted qualitatively or empirically with some of the prior works in learning non-Markov task specifications",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper presents a new IRL algorithm built on top of a reward machine (FSA-like) representation of a reward function.  The machine is lifted to relational predicates that may have free variables that need to be inferred.  The IRL procedure takes in a set of expert trajectories that are used to ground (or “concretize”) these variables and an RL algorithm (such as PPO) that can be used to learn policies based on reward functions.  A GAN approach is then used until the policy converges to behavior that matches the experts.  Empirical results are shown in 3 mini-grid scenarios.",
            "main_review": "UPDATE:\nI thank the authors for their revisions and clarifications on several of the comments. The authors make a good point that mini-grid domains can be difficult. I do, however, remain concerned that the approach may not be scalable (for instance the inability to learn more than just a few parameters holes).  The authors’ comment about only needing 10 expert trajectories to learn a reward function hints that the domains may not be sufficiently complex to judge an IRL algorithm and multiple reviewers pointed out that not much changed from using 1 demonstration to 10.\n\nI also remain concerned about the confusing comparisons of paradigms here.  I would really liked to have seem comparisons against more IRl algorithms rather than the cross-paradigm comparisons to RL and exploration techniques.\n\n----------------------------------------\n\nI am very happy to see work being done on relational representations for reward learning, which is an important topic for the field and a key for expanding domains where RL/IRL is applicable.  However, the empirical results in this paper on 3 mini-grid worlds are too small-scale and also compare with unreasonable baselines.  In addition, the current paper requires much more focus on specific problems and a more formal description and theoretical analysis of the proposed representation before it is ready for a top-tier publication.  I expanded on these issues below.\n\nOn the empirical side, mini-grid is a good domain for showing proof-of-concepts, but a work like this that purports to lift IRL to more realistic domains needs to at least show the approach is applicable in a more realistic setting.  More importantly, the baselines for comparison here are inappropriate.  The authors compare their IRL approach, which has the benefit of expert trajectories and an extremely expressive reward function that is only missing a few parameters, to RL algorithms like PPO that have to learn from scratch.  IRL and RL are fundamentally different problems, and comparing an algorithm that has access to trajectories and a powerful reward representation to one without such data and knowledge is bound to come out as the current results show.  The authors should instead focus on comparing strictly to other IRL algorithms, including linear IRL learners with relational features, or other IRL approaches that learn FSMs or relational representations (for instance see the paper “Inverse Reinforcement Learning in Relational Domains”)\n\nThe paper also lacks a clear and formal problem definition and focus on a specific tractable problem.  The paper is not clear up front about what parts of the representation need to be learned (it turns out just a few parameter holes need to be filled in?) and whether the learning agent is aware of the Reward Machine representation (how is the reward Markovian if the agent does not know about the FSM state?).  Many assumptions seem to also spring up out of necessity rather than being part of the problem constraints- for instance the assumptions about conjunctions and linear arithmetic on Page 6.  \n\nThe paper also contains many references to related work that tis not actually related, such as the discussion of exploration algorithms in RL, which is not relevant for an IRL algorithm.  There is also a significant amount of notation that is developed but never used.  This includes the GAIL terminology on Page 3, which could be left as a black box and the concatenation operators on page 4, neither of which seem to be used in the paper.\n\nFinally, I was disappointed that the paper did not rigorously investigate the expressivity of the new representation.  If the main contribution here is a complex relational reward representation, what kinds of things can it represent?  What can it not represent?  The authors introduce operators like the # (counting) operator that seem to lift the expressivity of the reward machine beyond an FSA.  If so, is reasoning with this representation tractable?  Is it possible to learn any more than a handful of parameters?  How is all of this learning done with just 10 expert trajectories (unless the parameter space is very constrained)?  The paper should provide some theoretical investigation of the representation itself since that is the main contribution here. \n",
            "summary_of_the_review": "The paper has a very interesting reward function representation but seems to confuse the IRL and RL cases and compares the two empirically.  The examples are also too small scale and the theoretical analysis of the new reward function is under-developed.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents a method for inverse reinforcement learning for learning the non-Markovian (trajectory-based) rewards for symbolic transitions of a pre-speciefied (up to free variables that need to be learned) automaton. For this purpose, Symbolic Reward Machines (SRM) are introduced, which define symbolic states, transitions and rewards, whereby MDP-trajectories can be unambiguously mapped to SRM-trajectories. The method uses adversarial imitation learning, where the rewards that are computed by the SRM (for a given grounding of the free variables) are used by a discriminator to classify expert- and agent-trajectories. Following the framework of Bayesian GAIL, a distribution over groundings/rewards is learned. However, whereas Bayesian GAIL learns a particle approximation to the posterior, the submission learns a variational Gaussian distribution. The method is evaluted on (rather challenging) gridworlds, where it is able to learn rewards for concepts such as \"picking up a key after finding the goal\".\n",
            "main_review": "\nMerits:\n- Inverse reinforcement learning with structured, symbolic rewards is a very promising topic as it can help to learn more generalizable and explainable reward functions by leveraging expert domain knowledge.\n\n- The approach is original and has contributions both in the formalization of the SRM and the procedure for training them.\n\nWeaknesses:\n- I found the paper rather difficult to follow due to a bad presentation. I have some open questions, and also several suggestions regarding the structure / presentations, which are outlined below.\n\n- I have several concerns regarding the technical soundness:\n  * The method replaces the term J_con by a binary cross entropy loss that penalizes the constraint-violations of the mean hole-assignment. This objective is quite different from J_con as it does not take into account the covariance of the variational distribution, and does not reward its entropy. This simplification is also not well-motivated: Couldn't we use samples instead of the mean and reward the entropy of the Gaussian to get an objective much closer to J_con that would still be tractable?\n  * The method changes the weighting between J_con and J_soft, which is somewhat hidden by the step-size scaling $\\eta$ which is actually quite large (1e8).\n  * It is not clear why the argmax of the variational distribution is used (line 3 in Algorithm 1) rather than computing the average reward based on samples as Bayesian GAIL.\n\nQuestions:\n* I can not make sense of the definition of the dummy transition (last line on page 3, term over the arrow). I guess it should read as \"For every outgoing transition, the guard is False\", but I can not see how to parse it that way. What does the and-operator apply to? Should the $\\neg{\\phi}//0$-part be lowered (right now it looks like a superscript of $\\delta$)?\n* First line at page 4: Why is it \"at any state $q \\in \\mathcal{Q}$\" rather than $p$. The expression seems to state that for any L-state $q$, there is only one incoming transition. However, I think it should state that for any L-state $p$ there is only one outgoing transition. For example consider 3 L-states (0, 1, 2). Assume, the initial L-state is $0$ and there are two transitions (with $\\phi$ = True), ending in state 1 and 2 respectively. This SRM does not seem to violate the definition of \"deterministic\", still any MDP-trajectory could be mapped to the L-trajectory 0->1 and 0->2.\n* Page 8, Gridworlds: Fig. 3b) I assume the green key is for the green door. What is the difference between doors of different color and gray doors? For exampe, there is a red door, but no red key. Fig. 3c) The agent does not seem to be able to reach the blue target (both doors have obstacles in front).\n* Why don't we see a difference in performance between 1 demonstration and 10 demonstrations (Fig. 3g-i). Is the task so simple that 1 demonstration is sufficient?\n* I can't make sense out of the transitions in Appendix A2.1. For example in the sentence beginning with \"The transitions are designed mostly based on high level human insights [...]\" the transition seems to parse as \"If the goal was reached, the agent must have unlocked a door after picking up a key\". However, in the diagram in Fig. 8, the agent can reach the goal directly from the \"Before Unlocking\" state.\n* In the experiments, it is not always clear what settings (RL vs IRL) were used. For example, Fig 3d): Is PPO(CNN) and AGAC(CNN) using a sparse MDP reward function (not using any demonstrations) and \"Algo1+X\" using the demonstrations (and not the sparse reward functions)?\n* Is the LSTM only used for the policy or also for the value function? Since the learned reward is not Markovian in the MDP-states, I think the states for the RL algorithm should be given by the history of all states, and thus, the LSTM should be applied both for value function and policy, right?\n* The free variables mainly show up as rewards. However, they also appear within guards, e.g. in the self transition to the \"After Unlocking\" state in Fig. 1b). Can the method also learn the dynamics of the abstract transitions? The self-transition is kind of a special case, because it only affects the reward in the end (If the rule is not applicable, the dummy transition is used which is also a self-transition). But in principle it should also be possible to transition to different states depending on the free variable. Did any of the SRMs make use of the possibility to use free variables to parameterize the \"control flow\" of the abstract machine?\n* How does the approach compare to using standard GAIL/AIRL for matching expert-specified trajectory features. It would be possible to compute features $\\phi(\\tau_{1:t})$ for a partial trajectory $\\tau_{1:t}$ of the expert/agent-trajectories and provide these features as input to the discriminator in order to learn a feature-based reward function $r_t = r(\\tau_{1:t})$ for matching these features. Similar to the current work, the RL agent would learn in an MDP where the state is given by the complete history of state-actions. The features could be as expressive as the SRM-trajectories, e.g. a feature could indicate how often a door was closed and whether a key was dropped after unlocking a door. What are the main benefits of the proposed approach?\n\nSuggestions:\n* Section 3: Equations 1,2 should be better explained to make the paper more self-contained (the section makes little sense if you do not read the Bayesian GAIL paper). One or two sentence, that highlight that BGAIL uses a different discriminator objective compared to GAIL would already be very helpful.\n* The fact that the learned reward is non-Markovian should be discussed. What implication does this have on the generator update (RL-part)?\n* The guard of self transition to the \"After Unlocking\" state in Fig 1b) needs to be better explained (I saw that it is explained in the appendix, but when reading the paper it was quite confusing).\n* Last paragraph of page 4: I suggest to move most of it to the caption of Fig 1b, to make the Figure more self-contained.\n* The Gaussian noise $\\epsilon$ needs to be motivated.\n* The structure of the presentation of the optimization needs to be improved. I would mention the final objective (ELBO maximization) earlier, by mentioning that a variational approximation to the posterior is learned. It should be made more clear, that this objective is used for learning the reward function and variational distribution.\n* The \"binary cross-entropy loss\" for replace $J_\\text{con}$ needs to be better explained. The main paper (not the appendix) should state clearly that J_con is actually not used, and how the cross-entropy loss is defined. This part needs to be better motivated. If using samples from $J_\\text{con}$ and and entropy objective (as I suggested earlier) did not work, this should be mentioned.\n* Algorithm 1: It is not clear from the pseudo code how line 5 and 6 have any effect. Maybe remove them, or assign them to variables that are provided to J_soft. It needs to be better motivated why line 3 uses the argmax, rather than samples from the Gaussian variational approximation.\n* The stepsize $\\eta$ needs to be discussed\n* The paper uses the discriminator structure of AIRL, which seems quite crucial, because for the GAIL-discriminator the learned reward would converge towards a constant function that would not generalize. I think this detail needs to be discussed better.\n* Some of the questions I mentioned above should also be answered by the paper.\n\n__Update__\nI read the other reviews and the authors' rebuttal. I decreased the score from \"accept\" to \"slightly above borderline\", mainly for two reasons:\n* The rebuttal did not properly address my concern with $J_\\text{con}$. I was not talking about the sign in front of the objective, but I was criticizing that the actual objective was replaced without proper justification. While it is reassuring that at least an additional entropy objective was used (which was not mentioned in the initial submission, and also now only seems to be mentioned in the appendix), the objective is still significantly different than the original objective.\n\n* I think, I was too generous in my initial review. The paper has several clear flaws, in particular with the weaknesses of empirical evaluation (other non-Markovian method, that include similar amount of prior knowledge on the reward structure should perform similarly well), and hence the paper should be assessed as borderline. However, I don't see a major problem in the current submission, so I still think that the paper should be accepted.",
            "summary_of_the_review": "The paper proposes an original approach to the interesting topic of inverse reinforcement learning of structured rewards. I have some concerns regarding the technical soundness, in particular related to the objective $J_\\text{con}$. The presentation of the work needs to be improved.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}