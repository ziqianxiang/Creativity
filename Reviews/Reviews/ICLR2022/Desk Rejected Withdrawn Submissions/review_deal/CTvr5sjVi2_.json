{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper present an empirical finding (with theoretical motivation) on the issue of over-estimating Q function in adversarially-trained Q network. The theoretical result is demonstrated on a simple linear Q network example, and the experiments are on DQN. \n",
            "main_review": "**strength** \n1. This paper presents an interesting finding\n\n**weakness**\n1. there are some places not clear (see below questions) and require clarifications\n2. the reference format needs to be revised, and there are some related works not cited\ne.g. \non the adversarial attack in RL:\n\n[1] Y.-C. Lin, Z.-W. Hong, Y.-H. Liao, M.-L. Shih, M.-Y. Liu, and M. Sun, “Tactics of adversarial attack on deep reinforcement learning agents,” IJCAI 2017\n\n[2] T.-W. Weng, K. D. Dvijotham, J. Uesato, K. Xiao, S. Gowal, R. Stanforth*, and P. Kohli, “Toward evaluating robustness of deep reinforcement learning with continuous control,” ICLR 2020\n\n[3] I. Ilahi, M. Usama, J. Qadir, M. U. Janjua, A. Al-Fuqaha, D. T. Hoang, and D. Niyato, “Challenges and countermeasures for adversarial attacks on deep reinforcement learning,” IEEE Trans. on Artificial Intelligence, Sep 2021\n\n**Questions**\n1. In the motivating example 3.1, it assumes the optimal theta_i are orthonormal, but what if this assumption does not hold? how would the following derivation change? Also, how realistic is this assumption?\n\n2. There's also a confusion that I hope the author can clarify: in the 2nd last paragraph of Sec 3, the authors stated \"changing theta to decrease the regularizer R(theta) can lead to both over-estimation of the first ranked action, and re-ordering of the ranking of the suboptimal actions. But if all the actions are over-estimated and as long as the best action doesn't change, then it shouldn't be an issue? Can the authors clarify?  \n\n3. can the authors explain more details on the Fig 1 and Fig 2? \n* Are they in fair comparison? e.g. in terms of the real accumulative reward? \n* Is the result in Fig 2 expected and can be explained more intuitively? \n* What if Fig 1 is replaced by a_3 or a_4? ",
            "summary_of_the_review": "This paper presents an interesting finding on the over-estimation of the Q values in adversarially-trained DQN. There are a few places require author's clarifications and a few places requires revision including formatting, clarity, etc. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper studies the impact of adversarial training-based regularization on the correctness of value functions. In particular, the paper analyzes the adversarial training objective devised by Huan et al. 2020 on a simple example with linear function approximation and shows that this distorts the value function and prevents it from finding the optimal action, by changing the rankings and leading to overestimated values. Some experiments are performed to confirm if this is the case.    ",
            "main_review": "The paper is interesting in that it studies the efficacy of adversarial training in value functions, however, the paper feels incomplete to some extent. I will list down the main limitations I found in this paper and would encourage the authors to address these in the rebuttal period.\n\n1. it is well known that objectives such as Huan et al. 2020 will distort the value function. Such objectives are used in offline RL and will lead to the value function behaving differently from the original value function. But again, if the regularization hyperparameter is small, this can work ok (as is also shown in the theoretical analysis), so why is this such a big problem? Does Huan et al. lead to poor performance compared to DQN (or naive RL) in practice? if the answer is no, then I think this somewhat diminishes the contribution of this paper. Like it is unclear, why adversarial training shown previously still helps if there are still these problems. Without this perspective, something seems missing in our understanding. \n\n2. I don't think the analysis in the linear function case is super convincing. Note that in the linear function setting, the objective in Definition 1 can be minimized to $-\\infty$ by simply increasing the norm of the weight vector. And as a result, while this is a case where you can show the badness, it is hard to believe that this is what happens in practice. I see the motivation, but for an ICLR paper, claiming to do a theoretical analysis of adversarial training, this is not enough in my opinion. What if you do this analysis in the overparameterized linear setting, and not tied to a given example? Can we derive the conditions under which adversarial training is much worse than regular training? Or can this be shown in a non-linear setting?\n\n3. The performance drop metric of measuring the performance of counterfactual actions is a reasonable approach to measure the robustness of the adversarially trained agents, but again it is shown on only three atari games, and it is unclear what this metric tell us about the performance of the algorithm compared to the actual performance of the method.\n\n4. Section 6.4 appears speculation to me -- \"In particular,\nthe consistent Bellman operator corresponds to a special case of a certain reparameterization of\nKullback-Leibler regularization for value iteration Vieillard et al. (2020). Thus, it may be the case\nthat the decrease in overestimation of Q-values and improvement in performance is due to a type\nof implicit regularization rather than to an increase of the action gap. Hence, our results show that\nincreasing the action gap alone may coincide with an increase in overestimation of Q-values.\" -- This part must be explained, else this is just super unclear what is being meant here.\n\n5. It is unclear what future takeaways one must have. Sure, these phenomena make sense, but I could control for these via state-of-the-art methods in preventing overestimation (e.g., clipped double Q-learning) or by making sure the coefficient on the adversarial training loss is not too high. Are there some deeper questions researchers should aim to answer? The main contribution of an analysis paper comes from providing solid, concrete takeaways for researchers to build upon and this is not clearly provided.     ",
            "summary_of_the_review": "I think the paper is tackling an interesting question, but as it currently stands, the paper is not at the level of an ICLR paper, particularly because of the points 1-5 above. I think the theoretical example is very preliminary, and the underlying reasons for how overestimation affects performance are not studied properly. It is unclear how to connect this in the global scheme of designing better algorithms, and the analysis is limited to only three (non-standard) Atari games on the empirical side. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper investigates state-action value estimation in SoTA adversarially trained deep neural policy and regular RL policies. They found that adversarially trained policies are more vulnerable to suboptimal sa pairs.",
            "main_review": "This paper systematically studies value estimation in adversarially trained NNs. They provide theoretical motivations for the behavior of these types of neural policies. The finding itself is interesting that adversarially trained agents, although with similar asymptotic performance, suffer a lot when extra conditions are on the state-action pairs. \n\nI think the desired part of this paper is that how can practitioners use these findings to guide their design and future training. There are too many observations in RL are just \"yet another detail\". A clear demonstration of the use of the observation in the paper would significantly strengthen this paper.\n\nThe experiments are mainly done in the atari suite, which is fine. However, more visualizations might be helpful for presenting the content better. ",
            "summary_of_the_review": "The paper shows interesting findings about adversarially trained neural policies. However, after the main finding, more insight is needed for the whole community. Are you saying we should avoid these policies? Or we should do something to prevent this? ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}