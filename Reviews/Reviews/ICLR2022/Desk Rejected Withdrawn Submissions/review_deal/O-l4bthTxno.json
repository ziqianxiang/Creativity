{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a region-aware knowledge distillation in an unsupervised manner. Specifically, feature attention is calculated based on feature activations. This feature attention is then used to identity the top-K regions that need to be distilled. A region-wise contrastive learning framework is then adopted for knowledge distillation. For better performance, the authors also use perceptual distillation in their proposed method. Experiments on Horse < - > Zebra and Edges2Shoes show the effectiveness of the proposed method. \nThe main contributions of the paper lie on 1) unsupervised region-aware knowledge distillation and 2) perceptual distillation.",
            "main_review": "Pros:\n1. With region-aware knowledge distillation, the image-to-image GAN can be largely shrunk with a little performance drop.\n2. The regions of interest/regions to be distilled are selected in an unsupervised manner, which does not require extra mask labels.\n3. The perceptual distillation improves the distillation performance.\n\nCons:\n1. The paper can be viewed as an extension of [1], which also proposes to only distill interested regions rather than the whole images. The main improvement is that the proposed method does not require extra mask labels. Instead, it automatically calculates the feature attention based on the activations. However, I am concerned about its flexibility for other image-to-image tasks. Specifically, the paper only demonstrates two easy image-to-image translation tasks. For other tasks in CycleGAN, such as winter-summer, Monet to photos (or image style translation), I am afraid that the proposed method cannot detect such regions.\n2. The authors should valid their proposed methods on more datasets, including on the face-oriented tasks used in [1] ([1] can be viewed as an upper bound, as it requires extra face parsing map)\n3. The perceptual distillation is not novel, as it has already been used in [1] (Sec 3.2.2 Image-Level Distillation in [1])\n4. In Sec 3.2, the found crucial regions may be irregular, how to convert to rectangular regions in Fig 2. Is it possible to use irregular regions? Will such conversion (from irregular to rectangular regions) affect performance?\n\n[1] Yuchen Liu, Zhixin Shu, Yijun Li, Zhe Lin, Federico Perazzi, and Sun-Yuan Kung. Content-aware gan compression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12156–12166, 2021.",
            "summary_of_the_review": "1. The main idea of \"Not all regions are worthy to be distilled\" has been proposed in  \"Content-aware gan compression\"[1]\n2. The perceptual distillation has also been proposed in [1]\n3. So, the main contribution is the unsupervised way of finding regions of interests.\n4. The proposed way of finding such regions is limited to a small category of image-to-image translation and is not applicable to winter-summer translation, or image style translation.\n5. The paper only validates the effectiveness of the proposed methods on two small sub-tasks. The authors should conduct experiments on more sub-tasks. \n\n[1] Yuchen Liu, Zhixin Shu, Yijun Li, Zhe Lin, Federico Perazzi, and Sun-Yuan Kung. Content-aware gan compression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12156–12166, 2021.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper target to alleviate the intolerant memory and computation consumption of current GAN-based image translation networks and thus achieve the deployment of these algorithms on edge devices. The knowledge distillation is proposed to transfer the knowledge learned by the teacher model to an efficient student model. The authors argue that not all regions are worthy to be distilled, which is intuitive. This novelty is incremental by combining a) localizing the crucial regions in images with attention mechanism b) performing knowledge distillation with region-wise contrastive learning, and c) distilling knowledge in the generated images with perceptual distillation. These training techniques have been adopted in previous work.",
            "main_review": "There are several issues that need to be explained by the author:\n\nAt my first glance at this paper, it seems the combination of the attention mechanism and the knowledge distillation. Some example images (e.g., Fig.2) are very simple and do not have diverse backgrounds, and it is not surprising that the model could obtain reasonable attention output. Could the proposed method work on some challenging datasets with diverse objects? How does the author define the crucial region? According to which criterion? \n\nThe perceptual distillation is just to adopt the perceptual loss adopted in [1,2]. The novelty is very limited.\n\nReferring to Table.3, the improvement is very limited. I guess this region-aware contrastive distillation would not work very well for some challenging image-to-image translation tasks (e.g., the label-to-photo image translation on some natural scene datasets such as Cityscapes, ADE20K). Also, the author only provides the experimental results based on Pix2pix, could the authors provide the experimental results of using Cycle-GAN methods. I concern the improvement or performance would not be very good.\n\nAt the end of the article, it is mentioned that adding knowledge distillation will make the network training more stable. It can be seen that the improvement is very obvious in Fig.6. Is there any auxiliary experiment that can further prove what causes the training stability? Any theoretical guarantee? Is it the reduction in the number of parameters caused by the change in the number of channels or the adoption of the attention mechanism?\n\nSome minor issues: \nThe region size is fixed? \nThe ablation study about the hyper-parameter K? \nThe experimental part is are not comprehensive. The experiment section is telling the same thing. More experimental results on other vision image-to-image translation tasks should be included.\n\n[1] Johnson, Justin, Alexandre Alahi, and Li Fei-Fei. \"Perceptual losses for real-time style transfer and super-resolution.\" European conference on computer vision. Springer, Cham, 2016.\n[2] Photographic Image Synthesis with Cascaded Refinement Networks\n",
            "summary_of_the_review": "1) The contribution of this paper is incremental\n2) The experiment part lacks detailed ablation studies. Some valuable experiments are not conducted",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper introduces a distillation method for image-t-image translation models based on an attention-based region selection and contrastive distillation. The main idea behind the proposed method is to use generator attention to determine the most important regions of the image and apply a recently proposed contrastive image translation loss [1]  between the random patches within the important regions of the student and teacher latent embeddings. In addition to that, the method uses a perceptual similarity loss between the translation results of the student and teacher. The proposed method is compared against the state-of-the-art distillation approaches for generation and image translation in terms of the translation quality (FID score and semantic segmentation quality).\n\n\n\n[1] Park, Taesung, et al. \"Contrastive learning for unpaired image-to-image translation.\" European Conference on Computer Vision. Springer, Cham, 2020.",
            "main_review": "Strengths:\n\n1) The main idea to maximize the mutual information between the corresponding patches of a student and teacher in the most important regions and ignore the background is intuitively valid and sufficiently novel. Even though each of the proposed components, namely contrastive loss for image translation [1], attention-guided translation [2, 3], perceptual similarity loss for image translation [5, 6], and student-teacher distillation [4], have been widely used before, in the proposed method, authors combined them in a reasonable and efficient way.\n \n2) Authors provided an extensive set of experimental results that illustrate that the method efficiently distills the translation knowledge and outperforms other distillation baselines, as well as the ablation study that shows that each component is needed to achieve the best performance. \n\nWeaknesses:\n\n1) There are quite a few writing errors in the paper, which makes it difficult to read. \n\n2) The related work section does not include the papers [2, 3] which are extremely relevant as they also use attention to guide image-to-image translation. In fact, [2] should be included as one of the baselines to compare against the proposed method, as to exclude the possibility that a higher FID score is caused by the attention-guided translation.\n\n3) From the experiments in the paper, it is not clear whether the use of important regions actually yields more efficient distillation.  Since one of the main contributions of this paper is the distillation of I2I generator, this paper would greatly benefit from an experiment that shows how the level of distillation (e.g., the student model size) affects the translation quality. Ideally, the paper should have a plot of # parameters in the student model versus the FID score of at least a few baselines and the proposed method with and without the use of crucial regions.\n\n\nMinor comment: the results discussed in the paragraph \"Ablations on Distilling the Crucial Regions\" with the strategy (c) must be added to Table 2 as a column \"crucial region -, contrastive distillation +, perceptual distillation + (i.e., when patches from non-crucial regions may be used as positive/negatives, and a perceptual similarity loss). \n\n\n\n\n[1] Park, Taesung, et al. \"Contrastive learning for unpaired image-to-image translation.\" European Conference on Computer Vision. Springer, Cham, 2020.\n\n[2] Alami Mejjati, Youssef, et al. \"Unsupervised Attention-guided Image-to-Image Translation.\" Advances in Neural Information Processing Systems 31 (2018): 3693-3703.\n\n[3] Tang, Hao, et al. \"AttentionGAN: Unpaired image-to-image translation using attention-guided generative adversarial networks.\" IEEE Transactions on Neural Networks and Learning Systems (2021).\n\n[4] \"Distilling the Knowledge in a Neural Network\", G. Hinton et al. NIPS Deep Learning and Representation Learning Workshop (2015)\n\n[5] Huang, Xun, et al. \"Multimodal unsupervised image-to-image translation.\" Proceedings of the European conference on computer vision (ECCV). 2018.\n\n[6] Sungatullina, Diana, et al. \"Image manipulation with perceptual discriminators.\" Proceedings of the European Conference on Computer Vision (ECCV). 2018.",
            "summary_of_the_review": "The paper introduces a novel distillation technique for image-to-image translation and provides a set of experimental results that show that the proposed method preserves the translation quality well compared to the baseline methods. An additional baseline should be included in the experimental results, and an additional experiment that shows the distillation efficiency with and without the use of important regions only should be conducted to fully illustrate the main claim of the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}