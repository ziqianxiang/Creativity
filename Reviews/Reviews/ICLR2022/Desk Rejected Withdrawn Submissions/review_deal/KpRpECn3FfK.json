{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a graph based memory model to solve POMDPs. Specifically, the idea is to construct a graph between new observation and previous observations at any time step within an episode and then use graph convolutional network to compute belief state (which is input to the policy network) from this graph. The edges between observations are realized using some criteria specified by human or learned automatically from the data. Different criteria serve as different type of topological prior information. The authors provide a list of hand-designed priors and a technique to learn it if not specified. Empirical evaluation is performed on three settings that include POMDP version of Cartpole, non-sequential concentration card game and navigation task. The authors compare their method with existing memory based models used in POMDP literature and illustrate superior performance in terms of rewards, which is well supported with ablation studies and comparison across priors. Better memory complexity in terms of number of parameters compared to other models is shown to support the main claim of the paper. Authors further analyze the memory graphs to discern which observations affect the belief state in an interpretable manner.\n",
            "main_review": "Strengths:\n\n- The insight to build a graph structure between observations to serve as a memory module is simple and useful to buld a flexible prior for computing belief state. \n- It also allows the use of Graph Convolutional Network based architecture that facilitates lower memory footprint which is clearly demonstrated in the experiments.\n- The empirical evaluation is done on diverse tasks and settings which benefit with different types of prior, thus allowing to assess the efficacy of the approach for different priors. \n- The ablation studies and analysis of memory graphs is very useful and the paper is well written and easy to follow.\n\nWeaknesses:\n\nThe overall contribution of the paper is severely limited due to success of major claims being realizable only in simple settings that benefit with hand-designed prior. \n- The use of graph structure and GNNs for providing state encoding in reinforcement learning problems is not novel [1]. In addition, if human knowledge is necessary for specifying a prior, that leads to lot of trial and error testing requirements to overcome prior misspecification. This makes the inference of graph structure, a key challenge and most important component of this approach. For example, many domains would require both spatial and temporal information to be preserved and it may be hard to hand-design a prior for both together.\n- Also, it is not clear how this hand-designed prior based memory graphs would perform in large scale applications where the number of edges may explode. Plus, many domains have visual observations, can the authors elaborate on how they handle visual observations? (I assume navigation has visual obs?)\n- The current techniques related to GCN architecture are standard which is fine. But the procedure for learning the topological prior is also ot novel and it is not clear how they fare against existing state-of-art techniques (e.g. [2]) \n- Empirically, in terms of memory complexity, the learned version GCML already shows higher footprint than GCM with hand designed prior Fig 4(a). Further, the performance of GCML is not as good as GCM on any tasks presented in the paper even when they are very simple. It is very likely that a learnable graph memory model may require increased parameters to perform well in complex domains. This may lead to closing the gap in advantage between GCML and Transformer  in terms of memory footprint which is key advantage currently.\n- Clarity: It is not clear if the entire graph is regenerated at each time step or edges are only generated for incoming observation and already existing nodes? It appears to be the latter case but would like a clarification on that.\n\n[1] Deep Reinforcement Learning with Relational Inductive Biases, ICLR 2019\n\n[2] Neural Relational Inference for Interacting Systems, ICML 2018",
            "summary_of_the_review": "The approach to establish graph structure between observations to compute the belief state in POMDP is simple and useful and the results show clear advantage both in terms of memory complexity and performance when the technique is used with hand-designed prior. However, this is a very limited outcome and the approach and overall paper will be stronger if the performance of current technique to learn topological priors is improved, leveraging the literature related to [2] and or new technique is devised and experiments are performed on both these simple and other complex settings to show the efficacy of learned prior. Give the current state of the work, it is not yet ready for publication.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper studies a new type of structured memory representation to be used in RL to solve POMDP. The authors proposed graph convolution memory. GCM uses either human-defined or data-driven topological priors to form graph neighborhoods. The proposed topological memory can achieve higher performance on control, memorization and navigation tasks while using fewer parameters than other memory modules.",
            "main_review": "### Strength:\n\n- The authors propose a general way of learning structured memory representation. It has the flexibility to be hand engineered or learned. \n- Empirically the hand engineering one has good “parameter-efficiency” compared with prior work, and achieves good performance in a variety of domains (control, memorization, visual navigation).\n- The proposed memory architecture performs well where long-range memory is needed. For example, the concentration game is a task that requires long-range memory.\n- The paper is generally well written and easy to follow. \n\n\n### Weakness:\n- In almost all experiments, GCML doesn’t perform very well, this makes the contribution of this paper weaker. The authors mentioned in the discussion “For more complex problems where human intuition falls short, GCML can learn a prior.” This claim seems not grounded, and cannot be proven by experiments in the paper. I guess my biggest concern is why GCML cannot converge to hand-designed prior? For example, for the cartpole task, a human would use $t-1$, $t-2$, and GCML uses $t-9$, $t-10$. GCML not being able to learn a good topological prior is one major limitation in my opinion. \n- The gain of GCML over transformer models is not clear to me. In the experiments they perform similarly. I am not comparing GCM with transformers because GCM has human-engineered prior knowledge. The authors must show the GCM has a clear advantage over transformers (or a fully connected graph) to claim the contribution of the proposed memory module.  \n- Figure 4 suggests the model has better scaling performance in terms of number of learnable parameters. However, the choice of $|z|$ is arbitrary and maybe not comparable across models. This is not very scientific. I recommend the authors remove this figure, and instead, they can show performance w.r.t number of parameters with experiments.  \n- The authors motivate this work by clustering objects into rooms, which is great. However, in the appendix, the analysis doesn’t seem to show that. Figure 11 seems a bit cryptic and doesn't show the semantic meanings of nodes. \n- The related works are referred to at the end of the paper. This is not ideal since the paper mentioned the baselines in the experimental section first with the abbreviations.  \n- In the navigation task, the authors should mention which dateset is used, how many scenes, etc. Instead of just saying habitat challenge. They should also consider comparing with other memory models, like scene memory transformers (SMT, Fang et al)..\n\n",
            "summary_of_the_review": "It is an interesting paper tackling a very important problem in RL -- leveraging memories to solve challenging partially observable MDPs. However, some of the claims are not grounded (see the weakness sections), and the gain over transformer models is not clear to me. I am not sure how this model can be used without a human-engineered graph structure. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to build a topological graph on the agent's experience data and use a graph convolutional network to extract the belief state from the graph, which can then be used to predict action or value. Using memory in reinforcement learning is an important research problem. The paper shows another way to use intra-episode experience data.",
            "main_review": "Strengths:\n\n* The paper studies an important problem in reinforcement learning: how to make use of experience data (memory) to speed up policy learning.\n* The proposed method (GCM) is a clean and simple algorithm.\n* GCM shows better performance when combined with manually defined topological prior compared to the selected baselines in the paper.\n\nWeaknesses:\n\n* Why are only the observations taken into account when computing the belief state from the graph? There are prior works ([1], [2]) that show using (state/observation, action, reward) tuples as the input can help improve the policy performance.  [1] uses an RNN network to process the experience data, [2] uses a causal attention mechanism to process the memory. Do authors claim that using observations is sufficient as the components of the memory?\n* From Figure 6,7,9, GCM with learned topological prior performs no better than LSTM baseline. It only performs better with manually well-tuned topological prior. This poses a potential limitation of the method: one would need to use domain knowledge and try different topological priors to find the best prior for a new task. GCM with learned topological prior tend to perform worse than LSTM baselines. I would also believe LSTM baseline should have much faster inference speed as it does not need to compute on the entire graph (memory) at each time step. Therefore, it is unclear why GCML would be preferable.\n* Figure 8 shows that GCM with learned topological priors performs similarly to GCM with dense topological connections (connect all nodes). Therefore, it remains unclear whether the proposed method for learning the topological prior is effective.\n* Why choose two-layer GNN? Does a 3-layer GNN give better performance?\n\n[1]. Duan, Yan, et al. \"Rl $^ 2$: Fast reinforcement learning via slow reinforcement learning.\" arXiv preprint arXiv:1611.02779 (2016).\n\n[2]. Mishra, Nikhil, et al. \"A simple neural attentive meta-learner.\" arXiv preprint arXiv:1707.03141 (2017).",
            "summary_of_the_review": "The paper needs to justify why only observations are considered when memory data is used. The proposed method for learning the topological prior is not effective.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}