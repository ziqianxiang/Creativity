{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper introduces the gradient manipulation techniques from multi-objective optimization for multi-task learning to constrained RL. It gives solid mathematical materials to help the reader understand the rationale. Some experiments are conducted to validate the effectiveness of the proposed methods.\n\nThe major contribution is the establishment of connections between the gradient manipulation techniques and  constrained RL.",
            "main_review": "Strengths:\n1. This paper captures the recent trend in developing gradient surgery methods to optimize the training of multi-task deep learning and tries to bring it to CRL.\n2. This paper put much efforts in presenting the background knowledge in related areas. As far as I know, the concerning areas are all math heavy areas.\n\nWeakness:\n1. This paper lacks a more deep understanding of CRL, missing some important related work.\n2.  The experiments are a little bit weak. It does not compare the proposed methods with some related work. Moreover, it does not report the weakness of the proposed methods, i.e., they will severely increase the computation time.\n",
            "summary_of_the_review": "This paper is interesting in general. But at present, it has some weaknesses and thus it is not mature enough for acceptance in ICLR.\n\nSome missing related work. \n\n1. Reward is enough for convex MDPs.\n\nTypos:\n\nwith two radient recalibration techniques -> gradient\n\nI strongly recommend the authors to make more deep understanding with related areas such as multi-objective RL and conduct more convincing experiments.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper addresses the problem of reinforcement learning in constrained MDPs with a single constraint. The authors are based on previous works that try to find Pareto Optimal solutions moving in a direction that is the linear combination between the gradients of the two objectives. The main contribution is that the authors here propose a reweighting / clipping of the coefficients of this linear combination depending on how the cost and reward of the new policy are with respect to the previous one. The authors also show a property that the constraint at each policy update does not decrease (therefore it moved in a feasible direction) and compare with state of the art methods based on Lagrangian relaxation, constrained TRPO and Pareto Optimal algorithms without weight clipping.  \n",
            "main_review": "1) Strenghts: \n    - The idea of using a method of weighting the gradients of the cost and constraint, initially used  for Pareto optimization in reinforcement learning, in the context of constrained MDPs is interesting. Also the clipping of the weights of the constraint and/or objective seems novel. \n    - Extensive experiments for comparison with existing works and ablation studies.  \n\n- Weaknesses: \n   - The paper is not very well written and hard to follow; a major editing is needed. \n   - The theoretical analysis of the proposed method, as well as its relevance to the practical algorithm implementation are not clear. More specifically, the main theoretical result is Theorem 4.3, which gives a bound for the improvement of the constraint. First, the authors mention that a backtracking search must be performed for $\\eta(w)$ and grid search for $t$, however these methods do not seem to be used in the algorithm. In addition, the fact that there is a lower bound in the cost does not, per se, imply that the algorithm converges to a local optimum: it implies that the policy tends to move towards a feasible one (with respect to the constraint) or that it moves towards a policy which has a larger slack in satisfying the constraint if the policy is already feasible; for reward optimization the latter behavior is not desired. It is mentioned that a similar guarantee holds for the reward as well, however it is not clear if good parameters can be found for both. Algorithm 1 tries to address this by adjusting the clipping threshold $t$, but it seems quite ad hoc and I cannot  see how it can lead to any guarantees. \n   -  The shortcomings of Lagrangian - based policy optimization and (especially) CPO are not very well justified ( even in Appendix C) , so it is hard to get an intuition why the proposed method would perform better.  \n\n- Other comments: \n   - What is a \"trivial Pareto-optimal policy\"?\n   - It is not clear how the framework of PPO is used; equations (8) seem to imply that an estimate of the gradient of the rewards and constraints with respect to the parameters of the policy is needed, however PPO does not do explicitly gradient descent. Please clarify.  \n   - $t'$ in Algorithm 1 does not seem to be defined anywhere, nor its effect on the performance of the algorithm is discussed.  \n   - When assigning new values in Algorithm 1, the arrows are usually written in the opposite direction. \n   - Could  this framework be applied for more than one constraints? ",
            "summary_of_the_review": "This paper presents an interesting idea, however it is difficult to read and it is not very clear from the theoretical analysis why the proposed algorithm offers performance guarantees or overcomes  the shortcomings of the state of the art. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors argue for the use of Pareto-Optimal Search methods for solving Constrained MDPs and present CONTROL, a novel algorithm from this family. The method builds on previous work on pareto-optimal search and augments them with two key ideas: gradient re-balancing, which normalizes the gradient components of the reward and the constraint objectives before searching for the pareto-optimal update direction in order to correct for the imbalanced development issue,  and gradient perturbation, which clips the reward pareto weight to smaller values when the policy is not feasible to allow to sacrifice performance in favor of updating the policy towards constraint satisfaction. The authors provide theoretical contributions (detailed in appendix) to motivate the use of pareto-optimal search methods to solve Constrained MDPs and to motivate the soundness of their proposed alterations (gradient re-balancing and perturbation).\n\nThey present results for experiments with 6 tasks of the Safety Gym, a standard benchmark environment for Constrained RL research, and compare against commonly used constrained RL algorithms as well as ablations of the presented method.",
            "main_review": "### Strengths\n\n- Overall I found the idea of using Pareto-Optimality search methods in the context of Constrained RL to be an interesting avenue.\n- While I could not check all derivations in detail, the mathematical presentation of the proposed approach seems sound.\n- The experiments seem sufficient to me in terms of volume and the results for the proposed method are favorable.\n\n### Questions and weaknesses\n\n1. The authors refer to the set of baselines as state-of-the-art (Page 2, Paragraph 3, last sentence). I believe this is a debatable statement. While I don’t have any strong issue with the chosen baselines (it is very common for Constrained RL algorithms to be comparing with PPO/TRPO-Lagrangian and CPO and I believe it is still reasonable and informative to do so), I also must take account that many new algorithms which also claim to beat these baselines have since then been published (for example [1]) and thus qualifying these baselines as state-of-the-art may be an overstatement. Moreover, only 6 tasks of the Safety RL benchmark environment are considered in this work. I believe that the authors could either keep the same set of experiments and simply remove the claim for state-of-the-art performance. If they do wish to keep this claim, then I think it needs to be better supported with comparisons to more recent baselines and on a larger set of tasks.\n2. The authors mention that they “let $J_C(\\pi)$ negative for readability”. I found this choice to actually be more confusing as it contrasts with the definition given in previous work. In my opinion, it would be clearer to state the CMDP problem as it is usually defined: $max_\\pi J_R(\\pi) s.t. J_C(\\pi) \\leq \\zeta$ for Equation 3 and $\\pi > \\pi’ i.i.f. J_R(\\pi) \\geq J_R(\\pi’), J_C(\\pi) \\leq J_C(\\pi’)$ for Definition 2.1.\n3. In section 3.2, when enumerating advantages of Pareto-optimal search in CRL, it is not clear what exactly the authors are comparing this method to. For example, they state “knowledge-free” as one advantage of pareto approaches. However, competing methods like Lagrangian approaches and Trust-region methods are also knowledge free, so why is this property mentioned at all in this context?\n4. I found the gradient perturbation somewhat reminiscent of the automatic adjustment of the lagrange multipliers in the Lagrangian approaches to CMDPs. The general idea for gradient perturbation is to reduce the relative weight of the gradient w.r.t. the reward-objective when the constraint is violated. However, this is also what the second player does dynamically in the Lagrangian methods. Can you elaborate on this comparison and maybe give some intuition about how these two approaches differ? In general, as stated above, I believe that the high-level discussion that is proposed by Figure 2 on the different behaviors of (1) Lagrangian methods (2) Trust-region methods and (3) Pareto-Optimal methods is an interesting and important reflexion to have and that the presented work would gain in elaborating even more on this topic.\n5. Overall, the writing could be improved; many sentences contain errors and/or are unclear (see Typos below for some examples).\n6. I have not found the code used for the experiments. There doesn’t seem to be a link in the manuscript. \n\n### Typos\n\n- Replace “while” by “on the other hand” or merge the two sentences (page 2, paragraph 2, last sentence)\n- The authors often refer to the “concept of Pareto-optimal”. Shouldn’t this be refered to as the concept of “Pareto-optimality” instead?\n- Definition 2.1, is strictly holds -> strictly holds\n- Definition 2.2, an policy -> a policy. Also, “We call a policy is global Pareto-optimal” is not grammatically correct.\n- Definition 2.3, an vector -> a vector.\n- Pareto-optimal Searching -> Pareto-optimal Search ? (page 3)\n- in each updating -> at each update (page 3, over equation 4)\n- should be a linear combination of gradients of J_R(\\pi), J_C**(\\pi) (page 3, above equation 4)\n- Figure 2 caption for b), c) and d): multiple typos to correct and reformulation needed\n- Section 3.1: To be consequential**\n\n[1]: Zhang, Y., Vuong, Q., & Ross, K. W. (2020). First order constrained optimization in policy space. arXiv preprint arXiv:2002.06506.",
            "summary_of_the_review": "I believe that the idea of applying pareto methods to Constrained MDPs is interesting and would benefit the community. The mathematical presentation of the proposed method seems sound and the results are encouraging. For these reasons I would tend to support the acceptance of this work. On the other hand, the quality of the writing could be improved, some aspects of discussion could be further elaborated and the claim of superiority of the presented algorithm is not well supported enough in my opinion. I believe that these concerns need to be adressed before publication of this work. I would be open to increase my score after the corrections are made.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes an approach for constrained RL that treats the objective and cost/constraint as two separate objectives, and computes a gradient direction to move in based on the two separate gradients. The paper proposes a new approach for combining the gradients, that has two key differences from existing work: it balances the contribution of the two gradients via normalization, and it clips the gradient of the task reward when appropriate (to ensure that a constraint-satisfying solution is found). The empirical experiments demonstrate this approach works well for the point mass tasks from Safety Gym.",
            "main_review": "The key strengths of this approach are that it is simple and flexible, and can be combined with any RL algorithm based on policy gradients. The paper clearly motivates the problem and explains the proposed approach, and how it differs from previous approaches. There is also an intuitive explanation of why previous approaches fail, that its backed up by theoretical justification and proofs. Figures 1 and 2 do a nice job of motivating and visualizing existing approaches, compared to the proposed approach. Another strength is that the experiments are on an established benchmark for constrained RL, Safety Gym, and the results show improvement over existing approaches. \n\nHowever, this paper has a few key weaknesses. The first is that it takes a narrow view of multi-objective approaches for RL, by only focusing on gradient-based approaches. Thus the statement that \"existing Pareto approaches are not practical at CRL\" and that they combine gradients in an imbalanced way is too broad, since this only applies to a small subset of the multi-objective literature. In fact, there is a substantial amount of recent work on multi-objective RL that goes beyond these gradient-based approaches. Within the MORL literature, there are algorithms that find the entire Pareto front at once, like envelope Q-learning [1] and the evolution-based approach in Xu et al. [2]. There are also methods that outperform linear scalarization in continuous control domains, like MO-MPO [3]. I would like to see more of a discussion of existing MORL approaches and whether they could be applied to constrained RL, since this paper is focused on the connection between constrained RL and multi-objective approaches.\n\nAnother weakness is that the evaluation is only on point mass tasks from Safety Gym, which are quite simple. I would be curious to see how this performs on the Car and Dog tasks in Safety Gym, compared to prior work.\n\nAdditional comments / questions:\n- The statement that linear scalarization will find points on the Pareto front is false. It is well known that linear scalarization cannot find solutions on concave portions of a Pareto front [4].\n- There are multiple typos and grammatical errors throughout the paper.\n- I would appreciate more clarification regarding the statement (at the top of page 6) that says the improvement may be imbalanced even if the improvement ratio is 1, if the scales of $J_R(\\pi)$ and $J_C(\\pi)$ are quite different.\n- The results in Tables 1 and 2 should report standard deviation across the five seeds.\n\n----\n\n[1] Yang et al. A Generalized Algorithm for Multi-Objective Reinforcement Learning and Policy Adaptation. NeurIPS 2019.\n\n[2] Xu et al. Prediction-Guided Multi-Objective Reinforcement Learning for Continuous Robot Control. ICML 2020.\n\n[3] Abdolmaleki et al. A Distributional View on Multi-Objective Policy Optimization. ICML 2020.\n\n[4] Das and Dennis. A closer look at drawbacks of minimizing weighted sums of objectives for pareto set generation in multicriteria optimization problems. Structural Optimization 1997.",
            "summary_of_the_review": "Overall, this paper is written clearly, with theoretical grounding and empirical support. It proposes two tricks for improving the gradient-based multi-objective optimization approach, to apply it effectively to constrained RL. However, the paper overlooks most existing work in multi-objective RL, and the experiments are on relatively simple tasks.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}