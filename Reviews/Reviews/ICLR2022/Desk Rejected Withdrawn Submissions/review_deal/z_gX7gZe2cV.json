{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies a black-box membership inference scenario where the attacker has a transformed version of a potentially member sample and interested in knowing whether this sample was a member. They propose two attacks, one based on setting the inference threshold based on the model's losses on the transformed input and two based on reversing the transformation either through bruteforce or adversarial perturbations. They apply these attacks against image classification and binary classification over tabular data and show minor improvements over some prior attacks applied on their setting.",
            "main_review": "The paper aims to evaluate a realistic MIA scenario where the attackers do not have the original samples but they have transformed samples. However, they consider only simple transformations that can be easily recognized and removed from the samples (rotations/filters). It would be valuable to consider more realistic transformations would be cropping (the adversary only has a portion of the image, e.g, a face),  different viewpoints or environmental conditions (rain/illumination etc.). \n\nFurther, the attacks by Yu et al. and Choo et al. (both cited in the paper) should have been evaluated as prior work as they share similar intuitions as the proposed attack. Yu et al. assumes that the model is trained with data augmentations but given the assumption in this paper (the attacker knows how the image is transformed), the same attack can be applied in this scenario too. Choo et al.'s attack also judges the membership based on the distance to the decision boundary or by observing the model's losses on transformed samples. It is entirely possible to apply Choo et al.'s attack outside the label-only setting as well, which would only make the attack better. These attacks would be better baselines for the scenario in this paper. I recommend authors to evaluate these attacks as well. \n\nMoreover, judging by the results in table 1 and table 2, the proposed attacks only lead to minor improvements over prior work (1-2%). Most improvements are on adversarial perturbation and the rotation transformations. Given that the attack basically does a bruteforce search to revert the rotation, it is not surprising and I believe a the MIA would fail on a different adversarial attack that involves some randomization. Given that the proposed attacks are costlier (e.g., more queries to the model), I'm not sure if the improvements are significant enough. We also don't know how better baseline MIAs in the previous paragraph would perform.\n\nThere are also the unanswered questions: what if the attacker does not know the transformations, what if there are multiple transformations, what if the models are trained with data transformations or with adversarial training, how many queries does the attack need for the reverse-transformation attack?\n\n\n\n",
            "summary_of_the_review": "+ Realistic threat model.\n\n- Only considers basic and predictable transformations.\n- There are more better MIAs as baselines in the literature for this threat model that are not evaluated in the paper.\n- The improvements seem to be minor over other baselines.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper considers a special scenario of membership inference attacks (MIA), where the data samples obtained by the adversary are transformed version of original samples in the datasets. It further designs two MIA methods: the first method is to extend the existing threshold-based attack by setting threshold values after data transformation; the second method is to compute the minimum perturbation needed to make the sample’s loss is small enough and compare the perturbation norm with a threshold value. The paper evaluates the effectiveness of proposed two MIA methods on CIFAR-10 and Purchase-100 datasets.",
            "main_review": "[strengths]\n1. The threat model of considering MIA under data transformation is novel and very interesting.\n2. Extensive experimental results on two datasets (CIFAR-10 and Purchase-100) with multiple common data transformations.\n\n[weaknesses]\n1. The first MIA method is a very simple and straightforward extension of existing methods, nothing new here. The second method is somewhat novel, however, the current implementation is not fully automatic: in Table 2, the authors manually try three $\\epsilon$ values and report the best attack performance. What about fixing the $\\epsilon$ value also based on shadow models, i.e., choosing $\\epsilon$ to maximize MIA attack success on shadow models?\n2. The advantage of proposed MIA methods is very limited: Table 2 shows that in most cases (8 out of 12), the increase of attack success over existing methods is smaller than 1.5%. Sometimes, the proposed methods are even worse. I was expecting the new MIA methods, especially the second one, can achieve similar success as there is no data transformation (similar to Choo et al., 2020 where they show that label-only attacks achieve comparable success as black-box attacks). However, it is far away from that.\n3. Given that your second attack method shares a similar intuition as Choo et al., 2020, what about simply using the label-only attack by Choo et al., 2020 on the transformed input samples? Can you run additional experiments to compare it with your methods?\n4. From Definition 3.2, my first impression is that the paper aims to search an almost perfect “reverse transformation”. However, the optimization below indicates that it only needs to make sure the prediction loss small enough. I suggest the authors editing Definition 3.2 to make it consistent (e.g., including $\\epsilon$).\n\n",
            "summary_of_the_review": "Despite the novelty of data transformation scenario, the advantage of proposed attack methods is very limited (weakness 2). Also, the second proposed method is not fully automatic (weakness 1) and fails to compare with existing label-only attack approach (weakness 3). Therefore, I suggest rejecting this paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies a weaker membership inference attacker. The paper assumes an adversary who only has access to the transformed version of a sample and wants to infer its membership. To this end, the paper presents a technique that approximately reconstructs the sample before the transformation is applied. In evaluation, the paper shows that their attacks can marginally improve the success rate of five membership inference attacks with the transformed inputs.",
            "main_review": "**Strengths:**\n1. Consideration of a somewhat realistic adversary.\n2. The paper is well-written; easy to follow.\n\n**Weaknesses:**\n1. Not sure the paper advances our knowledge of MI attacks.\n2. The technical contribution is on the input reconstruction side.\n3. The attack only improves the success rate marginally.\n4. The evaluation setup seems not that realistic.\n\n\n**Detailed comments:**\n\n[Weak contributions]\n\nOne of the major concerns is that I am not sure if this paper advances our understanding of the membership inference attacks.\n\nFirst and foremost, I want to make crystal clear that MI attacks are designed to infer the membership of an exact sample, not the transformed samples. I don't think it's a problem if existing MI attacks fail with the transformed samples.\n\nSecond, if we want to claim that existing MI attacks should work with the transformed samples, we should clarify those two.\n\n(1) Where do you want to draw a boundary? Suppose that you have two samples, one is a member, and the other is a non-member. But, you can obtain the member by transforming the non-member. Are you claiming the second sample is the member?\n\n(2) In which scenario is this adversary practical? and why does the evaluation represent the scenario? Recent deep learning models are trained with data augmentations---like SimCLR or CLIP. However, in experiments, the victim's models are trained w/o data transformations.\n\nNone of the two points is clarified in the paper. It makes me concerned that this paper starts from an incorrect threat model and studies an artificial adversary. Also, as shown in (1), the paper didn't provide the definition of the \"transformed samples\" clearly. In (2), I am also not sure the evaluation can back the hypothesis the paper makes.\n\nThis paper makes some connections to the label-only membership inference attacks. However, I think the two are completely different. The main idea of the label-only membership inference is to show the false sense of security in the prior work---if we do hide logits from the adversary, we can defeat the membership attacks. Nevertheless, this paper finds a setting where the prior work didn't do and studies without clear motivations.\n\n[Weak technical contributions]\n\nAnother concern is that the attack techniques they propose are not related to the membership inference attacks. But, these techniques are more aligned with identifying (or fingerprinting) what transformation(s) are used to a sample. So, I don't think the contributions are on the MI attack side.\n\nIn addition, as I mentioned in (1) above, there could be a failure case. What if the transformation of a member can result in a non-member. In this case, how do we measure success or failure? Indeed, the evaluations show that the proposed MI attacks do not provide a significant advantage to the attacker, which may reflect such failure cases. It seems that further inspections are required.\n\nThe paper also makes several hypotheses about the impact of various transformations on the characteristics of a model. But, many of these intuitions are already provided in the prior work, so the contributions are weak. For example, in Sec 3.2, the paper shares an intuition that if a sample is a member, the loss is less likely to be changed when we apply some perturbations (larger e-area than non-members).\n\nIt's not a significant concern but worth mentioning. In Definition 3.2, the paper shouldn't use the inverse. It could make readers mislead that the paper can inverse some non-inverse operations. Even if the paper warns that they abuse the notation, it shouldn't justify using an incorrect notation.\n\n[Weak evaluations]\n\nThis paper claims to study a realistic attack scenario, but I don't understand the practicality of many experimental setups.\n\n(1) As I mentioned before, why is it okay to assume the victim does not use any data augmentation for training models? I would assume the other way around; they will train with data augmentation.\n\n(2) In missing features, why do we assume that some features are flipped? Isn't it more realistic if the attacker ignores these features unless they do a property inference?\n\n(3) In Sec 4.1.4, what is the accuracy? I assume it's not the classification accuracy but a membership inference attack accuracy. \n\n(4) In the same section, why do we use the coverage difference rate instead of the membership advantage? I would like to understand this choice; otherwise, it looks the authors chose a metric for no reason.\n\n(5) In the same section, why don't we care about non-members?\n\n\n[Minor comments]\n\nIn Related Work, I recommend citing Yigitcan et al.'s paper \"When Does Data Augmentation Help With MI Attacks?\" with Yu et al. The former studies the impact of various transformations on the MI attacks, but the latter studies their impact on more general privacy.",
            "summary_of_the_review": "I am afraid that the five major concerns below can be addressed by the rebuttals; thus, I am leaning towards rejecting this paper. But, I am willing to adjust my score if my evaluation comes from a misunderstanding of the paper.\n\n(1) Its contributions to our understanding of MI attacks are unclear.\n(2) The threat model is unclear.\n(3) The technical contributions seem irrelevant to the MI attacks, and some of them are known in the prior work.\n(4) Despite a lot of effort, this attack seems like improving the existing attacks marginally.\n(5) There are many questions about the experimental setups.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Paper considers Membership Inference attacks (MIA) on machine learning models when only transformed data point is available to the attacker. Note, that the setting considered assumes that the attacker has knowledge that a transformation is being used. Authors motivate their study by that MIA makes an assumption that the attacker can access a copy of the original data point, which is not very realistic. Paper considers datasets in both image and non-image domains. For images, authors use gaussian noise, adv noise, JPEG compression, scaling, photo filtering and rotation. Authors evaluate existent attacks from the literature and devise their own that improve performance over existent literature. ",
            "main_review": "Strengths:\n+ Interesting reaslistic setting\n\nWeaknesses:\n- Problems with interpretation of results\n- Not possible to compare to existent literature\n- Missing adequate baselines\n\nFirst and foremost, I want to note that I find the setting considered in this paper extremely interesting. There is still significant confusion over when MI attacks work and this paper considers MI attacks in a much more realistic setting. Having said that, I find current arguments in the paper hard to interpret. In particular, the results seem to be missing comparison to non-transformed setting and are confusing in places. \n\nConnection to Choo et al. is interesting, yet I am not too sure I understand it too well here. What is the fundamental difference between semantically meaningful transformations and label-only attacks; more importantly Choo et al.? In my understanding, Choo et al. also attacked using data augmentations, but they also used the original sample as input. Choo et al. evaluated what happens when the attacker uses data augmentation in training, finding improvement in performance, yet it is still below that of just clean samples. It would be great to see a more direct comparison in both the setting and the results. \n\nTalking about comparison, I find the choice of models and training setup confusing, since its incompatible with the rest of the literature. Evaluation over multiple models and corresponding standard deviations would also make the paper a lot better. The models are severely overfitted, which is fine since otherwise MI doesn’t work, but it would be great to see results for other cases. Ideally, it would be great to see a variant of Fig 6 from Choo et al.\n\nFor CIFAR10, I am confused why Original CC performance in the table is 65, since text says base accuracy is 69. I am also confused about the description of classification correctness performance. Why is adversarial noise increasing the overall method performance, shouldn’t the opposite happen according to the definition in Appendix C and base CC performance?\n\nIs there any reason why Algo 3 uses sign of the sign of the estimated gradient and not the gradient directly? \n\nHow can Table 2 report results better than LTT? This is especially for Rotation, where the true sample can genuinely be found. Where is additional leakage coming from? Why is baseline performance not reported for the clean sample? \n\nTo improve readability, the paper would benefit from careful restructuring.\n\nTypo:\n+ Algo 3 should also say g^-1. \n\n\n\n\n\n\n",
            "summary_of_the_review": "I find current arguments in the paper hard to interpret. In particular, the results seem to be missing comparison to non-transformed setting and have some inconsistencies across the paper. I am slightly concerned that the setting a bit too close to Choo et al., yet evaluates only over a single model.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}