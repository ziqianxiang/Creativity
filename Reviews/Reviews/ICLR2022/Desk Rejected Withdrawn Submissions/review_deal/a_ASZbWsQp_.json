{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies the robustness of vertical federated learning. They use intuitions from robust feature subspace recovery for low rank matrices to design a defense framework for vertical federated learning. They empirically show the effectiveness of their framework for some settings. They also try to provide theoretical evidence for the robustness of their scheme, but I do not find their theoretical studies illuminating. ",
            "main_review": "The robustness of vertical of federated learning seems like a fundamentally different problem that has not been studied before. In VFL, there are n parties and one server and they want to collaboratively train a model, similar to the horizontal fl. However, in VFL, the parties each own a subset of features for the same dataset. The labels are kept by the server and the parties cannot change them. \n\nThis paper tries to use robust sub-space recovery as a motivation to design a defense against poisoning in VFL. The main idea is that, if data has enough redundancy across different features, then we should be able to *decode* the true features even in presence of a few corrupted columns. To this end, they develop an auto encoder that first encodes the data into a low dimensional subspace and then decodes it back to the original space. By putting some assumptions on this auto encoder and the data, they can prove desirable properties for VFL, but these assumptions are very strong and it is not clear to me why these assumptions are natural, in any setting.\n\nThey also empirically measure the effectiveness of their method against certain attacks and show that their defense can outperform some of previous defenses, against that particular attack.\n\n\n",
            "summary_of_the_review": "I like the problem that the paper is studying. VFL is an important setting and it's robustness is an understudies research problem. I also like the direction of using the geometry of data to undo some of the adversarial changes. However, I think this paper still needs more work for the following reasons:\n\n- The theoretical study of the paper is not very clear. There are many assumptions in the theory and it is not clear if these assumptions could be satisficed in *any* natural setting. I would suggest the authors to take a simple setting (e.g. gaussian) and show how their algorithm can lead to provable guarantees for that setting.\n\n- There is no connection between theory and the designed algorithm. In the Theory, additional to assumption about the data, authors also have strict assumptions for the auto encoder that needs to be satisfied. However, these assumptions clearly do not hold for their designed autoencoder. Also, the assumptions about the data do not seem to hold for the datasets studies in the paper, e.g. cifar.\n\n- The authors need to study adaptive attacks in their empirical analysis. \n\n- The tone of the paper seem to be suggesting that their defense is a *provable* defense by saying that their assumptions are *mild*. However, that does not seem to be the case. \n\n\n\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies robust vertical federated learning framework against backdoor attacks. The authors utilize robust autoencoders with purified training to remove the backdoor features and obtain clean global model. The authors also proved that, with some assumptions, the exact uncorrupted features can be recovered.",
            "main_review": "Overall, I don't think the paper presents an interesting or practical defense against backdoor attacks and the quality of the paper is below the bar. Below are the detailed comments:\n\n$\\textbf{Writing needs improvement}$: there are abused usage of notations and the presentation of the paper needs to be improved. $\\beta$ is used to both denote the fraction of backdoor training instances and the regularization parameter in Eq. (9) in the paper. The input to the encoder is also not consistent in the paper, sometimes it is $l$, sometimes it is $h$. Regardless of the attack scenario, I think the input to the autoencoder should always be $l$. It is also unclear to me why we need feature purifying step (stage 3) in the paper. My guess after reading the whole paper is, the features extracted from stage 2 is not the original clean features, and we need stage 3 to extract the original uncorrupted features. If this is the case, the authors should state it clearly in the paper. Then, it also raises another question: what does the \"robust feature subspace\" mean in stage 2? If the recovered feature subspace are still malicious, then the process should not be called \"robust\". When describing the different attacks in section 4.1, it will be better to explicitly state that, $\\{h_B,h_{benign}\\}$ denotes the feature of any backdoored test instance. The evasion attack (FGSM) strangely points to Biggio et al., (2013), which is not related to FGSM.\n \n$\\textbf{Unrealistic assumption}$: there are some unrealistic assumptions made in the paper and greatly hinders its practicability. First, the authors assumed the fraction of backdoor instances $\\beta$ is small for backdoor attacks. However, this not realistic because one can still design backdoor attacks with larger fraction without impacting the main task accuracy much. Note that, backdoor instances are mainly to draw a connection between the backdoor trigger and the target label, and it does not have to interfere with the main task. Therefore, even when $\\beta$ is high (e.g., 0.5), the backdoor can still be meaningful. The malicious agent does not care about the fraction of backdoor instances to use as it has complete control over its training data. Because $\\beta$ can be large, stage 2 in the paper does not hold ($\\mathbf{E}$ is not column-sparse) and the whole algorithm breaks from here. Second, in Theorem 2, the weight metrics of the autoencoder is assumed to be orthnormal, but I believe this assumption can hardly be satisfied in practice. Third, the authors assumed the fraction of malicious clients are small and then derived the optimization problem in stage 3. Although the assumption is valid, I did not see why it can translate to the optimization problem of finding the minimum number of malicious agents. Having small fraction of malicious agents does not mean always having minimum number of malicious agents.\n\n$\\textbf{Weak attacks}$: the attacks considered in the paper are weak. For the backdoor attacks, as mentioned above, the backdoor fraction can be large and therefore, the authors can try backdoor attacks with larger fractions. For the test time attacks, the authors used FGSM attack, which is not designed as a strong baseline to generate adversarial examples. At least the stronger PGD attack should be used. The authors stated that untargeted attack is easier and therefore they focused on targeted attack in the paper. If the \"untargeted\" here means indiscriminate poisoning attack, then such a statement is not justified. But I guess the authors are still talking about backdoor attacks and adversarial examples and \"untargeted\" means, we no longer have the target label. If this is an attack paper, ignoring untargeted attack completely makes sense as this is easier to achieve. However, this paper is a defense paper and we cannot ignore an easier (and stronger) attack, especially when generating adversarial examples. In fact, all defenses against adversarial examples are defending the untargeted attacks. The solution to this is either to limit the scope of the paper to targeted backdoor attacks only or also evaluate against untargeted attacks.  \n\n$\\textbf{Other questions}$: In the experiments, the number clients is too small (maximum of 4 clients) and I am not sure if such experiments are meaningful in practice. I understand the authors mainly follow from the previous work (Liu et al., 2020), but the practicability issue remains. What happened to the clean CIFAR10 accuracy? It is too low (<60%) and again raises practicability issues. Also, Liu et al., (2020) adopted MNIST in their experiments, not CIFAR10.  ",
            "summary_of_the_review": "I recommend for rejection mainly because: 1) unrealistic assumption on the design of the defense and the theoretical guarantees, 2) weak attacks as the baselines.  ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper tackles the problem of adversarial attacks against federated learning. The main proposal is RVFR, an original method that allows to protect against backdoor attacks targeting the specific “vertical federated learning” setting. The proposal is provided with a theoretical analysis of its effectiveness, and is then evaluated on two well-known datasets, showing better results than prior works in adversarial scenarios.\n\nOverall, the presentation of the paper should be improved.\nThe quality of the English text is appropriate.\nFigures and Tables should be improved.\nThe topic addressed by the manuscript is relevant and in-line with ICLR.\nThe references must be expanded with security-related works.\nThe contribution is potentially significant.\n\nSTRENGTHS:\n+ Relevant and trendy subject\n+ Evaluation on two datasets\n+ The theoretical analysis is appreciable\n\nWEAKNESSES\n- Very poor Introduction, Abstract and Background (Related Work)\n- Unclear threat model\n- No Tradeoff",
            "main_review": "The paper is interesting and tackles a relevant problem that will likely affect future systems. I appreciate the theoretical analysis (although my expertise cannot fully guarantee its complete correctness), which is an aspect often neglected in “novel” ML solutions---as they are mostly empirically validated. Moreover, the encouraging results on two well-known datasets are promising.\n\nHowever, the paper requires more detailed explanations and clarifications on many aspects before being ready for ICLR. Let me elaborate on the abovementioned weaknesses, starting from the most critical ones.\n\n\n**Unclear Threat Model**. The threat model is provided in Section 4.1, and the authors (implicitly) use the general notation of goal, capabilities, knowledge (the strategy is explained later). However, the capabilities of the attacker are not well defined. Specifically, the paper says that the attacker can control a fraction ($\\alpha$) of the M clients. Hence, I ask: *what can the attacker do with such clients*? Can they perform *any* manipulation? Are some of the features not modifiable? Is such attacker subject to realistic constraints or feature dependencies? Is the perturbation magnitude bounded? I am referring to the well-known issue of ‘problem vs feature space’ attacks [B], because real attackers are subject to many real world constraints (and especially in networked systems [C, D, E]) and not all adversarial perturbations may be physically realizable ([F]). The authors should elucidate this issue, because it could differentiate between “fictional” and “practical” attacks, therefore defining whether the proposed method is applicable to solve real world problems.\nMoreover, the following is unclear:\n\n•\t“There are $\\alpha$ fraction of agents that are malicious and total  $\\beta$ fraction of instances with backdoor trigger across all malicious agents.” I am unable to understand the relationship between $\\alpha$ and $\\beta$. Is $\\beta$ a subset of $\\alpha$? \n\n•\tFrom Equation 3 onwards, is h^{j} meant to denote $h^{j}$? Is there a need for the braces, or is it a typo?\n\n•\tPlease differentiate from $h_{benign}$ and $h_B$. The current notation is very confusing\n\n**Tradeoff**. \nA common problem in adversarial ML countermeasures is that they may degrade baseline performance [G, H]. Hence, I am interested in knowing how the proposed method responds when there are no “malicious” clients (or when such clients behave legitimately). In this paper, I am unable to determine what is the baseline performance of the models in “non-adversarial” settings. Does such performance degrade after RVFR is applied? How does RVFR compare to previous works in these circumstances? Even if the baseline performance does not decrease, what is the overhead of the proposed RVFR with respect to past defenses? Figure 3 and 2 only show results for adversarial scenarios. In real circumstances, a defense should have some practical utility. Note that I would not reject the paper even if RVFR does have a significant “cost”. However, such cost must be known.\n\n\n**Very poor Introduction and Abstract**. The Introduction fails to provide a concrete justification and enough context for the considered problem. Let me list all the issues I encountered while reading the introductory part of the paper:\n\n•\tThis statement in the abstract is vague: “However, unlike the standard horizontal federated learning, improving the robustness of robust VFL remains challenging.”. Specifically, why is it challenging? Just name a few reasons.\n\n•\tThis statement in the abstract is unclear: “ensure that with a low-rank feature subspace, a small number of attacked samples, and other mild assumptions”. What does this mean? The abstract should be more high-level. Such technicalities are not necessary.\n\n•\tPlease be consistent. In the Introduction: “In FL, a central server coordinates with multiple agents/clients”. Either use “client” or “agent”.\n\n•\tI suggest mentioning [A] for a practical, recent and useful application of FL in a real world problem.\n\n•\tThis example is unclear: “In VFL, different agents hold different parts of features for the same set of training data. For example, in VFL for credit score prediction, Agent 1 may have the banking data of a user and Agent 2 may have the credit history of the same user, while the server holds corresponding labels.”. To me, it appears that Agent 1 and Agent 2 have different data, and hence represents a HFL problem (and I still do not understand the necessity of the last statement involving the ‘label’). Perhaps the authors should provide a visual example that better explains the difference between HFL and VFL.\n\n•\tThe Introduction still suffers of the same problem as the abstract. “However, it is challenging to defend against malicious attacks in VFL.”. Why? It is very annoying to read the abstract, not get a response to such question; and then re-reading the same concept in the Introduction, and not finding an answer even there. The impression is that the authors are trying to make the problem more difficult than what it currently is: if it is challenging, it should be clear.\n\n•\tIn the Introduction: “The fraction of malicious agents is relatively small”. Can you define such fraction? Does it have an upper boundary?\n\n•\tThe term “backdoor attacks” is never mentioned in the Introduction until the “contribution” paragraph. Such term should be better contextualized: not all poisoning attacks are backdoor attacks.\n\n•\tI had to reach the Related Work section to understand why poisoning attacks in VFL are “challenging”. However, the motivation provided by the authors is confusing—to say the least. According to the paper, the challenge is “Backdoor attack against VFL is challenging since in the default setting the agent does not have the label information.” First, what is challenging exactly: the attack, or the defense? Second, what is such “default setting”? Third, why does the agent not have the label information? I believe the latter is due to the (poor) explanation provided in the early example in the introduction.\n\nMinor issues:\n\n•\tPlease use the term “stage” (or “phase”) and not “time” to differentiate between training and testing/inference.\n\n•\tPlease add some text between Section 6 and 6.1\n\n•\tIn Section 6.1, the authors state “We study the classification task on two datasets: NUS-WIDE and CIFAR-10. Following (Liu et al., \n2020), which proposed the backdoor attack against VFL, we use NUS-WIDE dataset to evaluate our defense.”. Does it mean that the defense is only evaluated on NUS-WIDE?\n\n•\tIn Section 6.2, the authors state “The noise variance is 0:05 for NUS-WIDE and 0:0001 for CIFAR-10 to preserve reasonable utility of the model.”. Please define what “utility” means in this statement.\n\n•\tFigure 2: please maintain the same range for the y-axis. It does not only varies among the rows, but also among the columns. \n\n•\tFigure 3 and Figure 2: the range of the y-axis should be the same for both figures.\n\n•\tUse the same term to refer to figures. If the figures are named “Figures”, then use “Figures” in the text, and not “Fig.”\n\n•\tWhat is the difference between “Backdoor Accuracy” and “Clean Accuracy” in Figures 2 and 3?\n\n\n\nEXTERNAL REFERENCES\n\n[A]: \"Federated learning for predicting clinical outcomes in patients with COVID-19.\" Nature medicine (2021): 1-9.\n\n[B]: \"Intriguing properties of adversarial ml attacks in the problem space.\" 2020 IEEE Symposium on Security and Privacy (SP). IEEE, 2020.\n\n[C]: \"Modeling Realistic Adversarial Attacks against Network Intrusion Detection Systems.\" ACM Digital Threats: Research and Practice. 2021.\n\n[D]: \"Constrained concealment attacks against reconstruction-based anomaly detectors in industrial control systems.\" ACM Annual Computer Security Applications Conference. 2020.\n\n[E]: \"Resilient networked AC microgrids under unbounded cyber attacks.\" IEEE Transactions on Smart Grid 11.5 (2020): 3785-3794.\n\n[F]: \"Improving robustness of ML classifiers against realizable evasion attacks using conserved features.\" 28th {USENIX} Security Symposium ({USENIX} Security 19). 2019.\n\n[G]: \"Adversarial example defense: Ensembles of weak defenses are not strong.\" 11th {USENIX} workshop on offensive technologies ({WOOT} 17). 2017.\n\n[H]: \"Deep reinforcement adversarial learning against botnet evasion attacks.\" IEEE Transactions on Network and Service Management 17.4 (2020): 1975-1987.\n\n\n\n",
            "summary_of_the_review": "The paper tackles a relevant security problem and the contribution is (potentially) significant. However, many concerns still affect the paper: some crucial points are not clear, and there is an overall lack of \"pragmatism\" in the paper. All such issues are relevant considered the strong relationship with security.\n\nI believe that addressing the highlighted issues can be done without excessive effort, as it mostly requires providing some more transparency.\n\nUnfortunately I cannot fully gauge the correctness of Section 5, representing the theoretical analysis.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a robust vertical federated learning framework called RVFR against backdoor attacks. In the framework, the agents train the local feature extractors and send the embedded features to the server. Then, the server trains a robust auto-encoder to recover and purify the features. Last, the purified features are used to train the global model. The paper provides theoretical analyses of the feature recovery under different threat models. The experiments show that RVFR is more effective when defending the backdoor attacks compared with the other baselines.",
            "main_review": "Strengths:\n1. The paper is well written and easy to understand.\n2. The studied problem is interesting.\n3. The paper provides a solid theoretical analysis of the proposed approach.\n\nWeaknesses:\n1. In the studied setting, there exists a server that holds the label. However, in practice, the labels are usually held by one of the agents (e.g., [1]). It seems that the analysis in the paper is based on that the server is a third party. Is the existing analysis still applicable when the server is one of the clients?\n2. In the threat model, the authors assume that the attacker knows the feature information held by every agent and the label information on the server. The assumption is quite strong. Only the server can assess all the feature information. In the experiments, the authors conduct the backdoor attack in [1]. However, [1] does not seem to have the above assumption. There is a gap between the threat model and the experiments.\n3. In step 3, one assumption is that the fraction of the malicious agents is small. Is there any theoretical analysis on the upper bound of the fraction? Moreover, in the experiments, there are only two or three agents, where the fraction of the malicious agents is not small.\n4. The experiments only use two datasets, which are not solid enough. Also, it is strange to partition an image to multiple agents to simulate the vertical federated learning setting.\n5. In Figure 2 and 3, can the authors add the local training as a baseline for the comparison on the clean accuracy?\n6. In the experiments, the authors may show the distance between the recovered feature and the original feature to demonstrate the effectiveness of feature recovery.\n\n[1] Backdoor attacks and defenses in feature-partitioned collaborative learning\n",
            "summary_of_the_review": "The paper provides an interesting solution for robust vertical federated learning. However, I have some concerns about the vertical FL setting, the threat model setting, and the experiments. Please refer to the main review for details. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}