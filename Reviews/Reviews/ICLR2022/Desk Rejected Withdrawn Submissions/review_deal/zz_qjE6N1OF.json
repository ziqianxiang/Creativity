{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "Summary:\n- Authors propose applying predictive processing, a popular framework in neuroscience to minimize surprise, in DeepRL, specifically to RNNs.\n- P4O is evaluated on several environments from Atari-57 and can significantly  outperform SOTA algorithms on SeaQuest.",
            "main_review": "Strengths:\n- Simple modification to RNN architecture\n- Strong results in Section 3.4; shows P40 can continuously learn, unlike other agents that plateau early on\n\nWeaknesses:\n- Introduction; Talks about sample-efficiency issue in DeepRL but PPO isn't a very sample efficient algorithm. Can this stack with other more sample efficient algorithms such as SAC? How does this modification affect long term performance (as shown in Section 3.4)?\n- Novelty; There is a lot of prior work (such as R2D2 from Deepmind, GTrXL) that play with the gating mechanisms and don't seem to be mentioned. It would benefit this paper a lot to introduce the prior work and evaluate the best gating mechanism.\n- Section 3.4; Great to see if P4O has better long-term performance on more envs to prove generalization\n- Section 3.3; Needs to better explain why P4O exhibits great performance in the long run (s.a isolating each component of P4O and identifying the performance deltas)\n\n\nSmall Nits:\n- How many seeds were run for Figure 2 and 3?\n- Black and white plot for Figure 3\n- Based on my understanding, this intuition behind predictive encoding is similar to Curiosity, in that P4O also captures the variance of input data directly influenced by the agent. Good to mention this.\n\nQuestions:\n- In Table 1, why Dreamer, Rainbow, IQN reach 200 M Atari frames in 10 days? ",
            "summary_of_the_review": "Overall:\nThis paper presents a simple modification to the RNN architecture to significantly improve long-term performance for an RL agent. I'm willing to raise this paper to weak accept as long as the authors address the Weaknesses above and can isolate the benefits of P4O.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Ethics Concerns:\nNone",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors optimize a PPO agent augmented with a surprise-minimization loss. The addition of this loss is motivated by the predictive coding theory of sensory information processing, which is proposed by neuroscientists. They compare their surprise-augmented P4O against vanilla PPO agents on Atari games.",
            "main_review": "**Pros**\n- The introduced method is simple and effective when benchmarked against vanilla PPO.\n\n**Cons**\n- The authors don't compare against any surprise minimization/prediction-error based approaches. These are also omitted from the related work. Some notable works to compare against are listed under Missing References below.\n\n**Missing References**\n\nSurprise minimization: SMIRL: Surprise Minimizing RL in Dynamic Environments. Glen Berseth, Daniel Geng, Coline Devin, Chelsea Finn, Dinesh Jayaraman, Sergey Levine. January 2021.\n\nSurprise maximization and connections to neuroscience: Active World Model Learning with Progress Curiosity. Kuno Kim, Megumi Sano, Julian De Freitas, Nick Haber, Daniel Yamins Proceedings of the 37th International Conference on Machine Learning, PMLR 119:5306-5315, 2020.\n\n",
            "summary_of_the_review": "I vote to reject this submission because it does not cite or compare against highly-relevant prior work.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose P4O, an on-policy actor-critic RL algorithm that augments proximal policy optimization (PPO) with an additional objective term to predict future encoded observations via recurrent network layers. The authors assess P4O on 6 of the Atari-2600 games, comparing favorably against ablations on 5 / 6 games in terms of sample complexity, and demonstrating that the higher throughput compared to other single-GPU agents allows them to outperform such agents in wall-clock efficiency.",
            "main_review": "Strengths\n- The proposed model is clearly explained.\n\nWeaknesses\n- This paper is missing references to a myriad of predictive coding/latent dynamics learning works from raw perceptual inputs in reinforcement learning and optimal control. In particular, it is common to impose structure in the model (e.g. linearity in dynamics) to facilitate use of the model in planning [A, B, C]. In contrast, the dynamical model in P4O is a recurrent neural network, and the model is used only in terms of its representation as input to a feedforward control policy. The authors must seek to address the question of whether there is any merit to using their \"more black-box\" approach to learnable control embeddings.\n- The idea of learning models specifically in the context of Atari has also been explored by [D, E]. What is the novelty in P4O's approach to modeling with respect to these works?\n- The experimental evaluation is critically weak. The authors choose to evaluate on Atari games, but only run experiments on six, and only compare to ablations on their method for these six. Most works that evaluate on ALE/Atari-2600 present results aggregated from at least 40 tasks and compare to other strong methods using similar assumptions/constraints. The choice of focusing comparisons to other methods on just SeaQuest requires much more justification (e.g. Montezuma's Revenge: hard exploration). It is completely unclear to me that wall-clock time is a more critical measure of efficiency than sample complexity. Besides, the original PPO paper also evaluated on continuous control tasks. If computational constraints were really an issue, the authors should have at least run experiments on continuous control. \n\nReferences\n- [A] Watter and Springenberg et al., Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images, 2015, https://arxiv.org/pdf/1506.07365.pdf\n- [B] Shu and Nguyen et al., Predictive Coding for Locally-Linear Control, 2020, http://proceedings.mlr.press/v119/shu20a/shu20a.pdf\n- [C] Zhang and Vikram et al., SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning, 2019, http://proceedings.mlr.press/v97/zhang19m/zhang19m.pdf\n- [D] Chiappa et al., Recurrent Environment Simulators, 2017, https://arxiv.org/abs/1704.02254.\n- [E] Oh et al., Action-Conditional Video Prediction using Deep Networks in Atari Games, 2015, https://arxiv.org/abs/1507.08750.\n ",
            "summary_of_the_review": "This paper proposes one algorithm and seeks to validate the algorithm via experiments but uses a critically weak empirical evaluation. In its current form, it also ignores much relevant prior work that arguably subsumes its proposed method in technical merit.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors compares an encoder-prediction LSTM-action-value head architecture versus a classical encoder-LSTM-action-value head architecture. To train properly the prediction LSTM they add, in addition to the classical action, value and entropy losses, a predictive processing loss that minimizes the errors e_{t+H} between the encoder state x_{t+H} and the belief prediction p_{t+H-1}. They provide results on 6 games of the Atari benchmark.",
            "main_review": "The paper is well written and the idea is clearly presented. The authors present  an encoder-prediction LSTM-action-value head architecture that is shaped by a predictive processing loss in addition to the classical RL and entropy losses. They also provide interesting details on how to minimize the predictive processing loss for horizon H greater than 1 which makes the method more general. However, my main concern with the paper is that the method is not contextualized properly with respect to the existing literature. Indeed a lot of work has been done recently in terms of Representation Learning for Deep RL algorithms. Like the presented method, in those works, an additional loss is used to better shape the RL network in order to gain data efficiency. Those methods rely also on predictions of future quantities and can be contrastive, predictive or generative. Here are a non exhaustive list of such methods:\n- SPR (https://arxiv.org/pdf/2007.05929.pdf)\n- CURL (https://arxiv.org/pdf/2004.04136.pdf)\n- PBL (https://arxiv.org/pdf/2004.14646.pdf)\n- Pixel Control, Reward Prediction (https://arxiv.org/pdf/1611.05397.pdf)\nThe discussion and the experiments should be centred around those methods because they share the same goal as the method presented in the paper. The baselines chosen are interesting but they are either ablations of the proposed method or pure RL baselines.\n\nThe experiments could be improved by adding a sweep over the horizon of prediction H. I was not able to find the value of H in the paper. In addition, more games could be added in the experiments. In the SPR paper they run the algorithms on only 100K images, a similar setting could be done here in order to compare SPR and P4O.\n\nOverall, the paper could be greatly improved if better contextualized and experiments proving that the method is indeed data-efficient against proper baselines. I am inclined to increase my scores if the authors take into account my remarks.",
            "summary_of_the_review": "The paper is clearly written, however it is not properly contextualised and there is a lack of comparison between competitive data-efficient baselines.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed an intriguing augmentation to PPO based on the intuition from predictive processing. The augmentation marges a belief LSTM and a prediction LSTM to build a world model for the RL agent. An augmented predictive processing loss is added to the traditional PPO objective for updating the world model. Experiment results show P4O outperforms its ablated model design in 6 selected Atari games.",
            "main_review": "This paper proposed an intuitive structure by following the predictive processing. The idea is simple but effective in terms of boosting SOTA performance. The paper is generally well-written. Some corrections and clarification should be made, but it is generally acceptable. However, it seems the paper is composed in a rush. Some definitions and results are incomplete. I list some issues and potential solutions. If the authors have concerns over my suggestions, I am open to discussions.\n\n1. The “prediction LSTM” can not properly act like a world model since it does not use a_{t-1} as input. For evidence, Check figure 4 in [1]. In other words, the world model is essentially a dynamic model that predicted the future belief of states. Without knowing the current action, the prediction can not be accurate. Just image the transition model in RL. I encourage the author(s) to feed the selected action to the “prediction LSTM”.\n\n[1] David Ha, Jürgen Schmidhuber. \"Recurrent World Models Facilitate Policy Evolution.\"\" NeurIPS 2018: 2455-2467 (*This paper has already been published in Neurips, please cite the published version.*)\n\n2.  \"A second LSTM layer representing the agent’s belief states.\", but I do not find where you model the belief of the states. Check the definition of belief state in [2]. The belief states model the agent's belief about future states. Since the predictions are beliefs, the uncertainty of the predictions must be modeled. It is why the \"world model\" applies a VAE to model the distribution of latent variables. I encourage the author to build a stochastic world model.\n\n[2] Rodrıguez, Andrés, Ronald Parr, and Daphne Koller. \"Reinforcement learning using approximate belief states.\" Advances in Neural Information Processing Systems 12 (1999): 1036-1042.\n\n3. \"This encoded game state is subtracted from a prediction p generated by an LSTM layer.\" When we substract some predicted values from other values, we are not computing their distance (Neither MAE or MSE ) and I do not think a valid error or loss to play with. I encourage the authors to provide a clear definition of errors.\n\n4. Please combine section 2.3 with section 3 and retitle them as empirical results or experiments. Otherwise, please explain the motivation for inserting the experiment setup (Section 2.3) into the methodology section (Section 2).\n\n5. Please apply the full Atari 2600 benchmark otherwise people might think results are \"cherry-picking\". It is a tradition that RL researchers will report the full results and let the reader see where they outperformed or underperformed other baselines.\n\n6. The baseline models should be compared in all games instead of a single game \"Seaquest\". Please report the complete results.\n\nQuestions:\n1. I have a question regarding the motivation of the encoder design. Why do you use a structure like \"conv+pool+residual*2+conv*2\"? I do not think it is a common design for image-based RL or residual networks. Any particular reason or just an empirically well-performed choice?\n2. The author defines \"a policy \\pi (a_t|h_t)\", so where the p_t goes? Aren't they combined?\n3. What happens to Figure 3. Why the images are in the black-and-white format.\n4. Why not include MuZero and Agent57 as baselines. I think the author believed they are the SOTA agents.",
            "summary_of_the_review": "In summary, the idea of this paper is intriguing. Despite the advancement over PPO is not significant, but the boost of empirical performance is great. Since the advantage over PPO is mainly empirical, I do believe the complete results are necessary here, and more importantly, I capture some potential issues in the model design or definition. I urge the author to consider my suggestions or explain why they do not follow them. In short, I do believe this paper can be benefited from a resubmission.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}