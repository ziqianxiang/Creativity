{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a measurement of diversity in datasets. using random network distillation. The proposed method is general and can be applied for both NLP and CV datasets. Via manually controlling the diversity of the dataset, they show the effectiveness of the proposed method.  ",
            "main_review": "Strengths:\n1. The proposed method is general and can be applied for both NLP and CV datasets.\n2. Via manually controlling the diversity of the dataset, they show the effectiveness of the proposed method. \n3. The intuition of using the random feature is interesting, where it tries to decouple diversity from fidelity.\n\nWeakness/Questions:\n1. In section 3.1, the authors try to motivate RND by the use case in RL. However, 1) I don't understand why random noise can serve as an exploration bonus; 2) I don't see a clear connection between RL and the diversity of datasets.  \n2. The point of RND is to evaluate the generalization gap of features given by a random network. How about we just estimate the entropy of the random features?\n3. Why does RND take the average over predictor networks of different epochs Eq(3)? Why not just take the last predictor? \n4. My biggest concern lies in the experiment part: 1. no experiments show that RND is better than FID; 2. no experiments show the benefit of the random network over the pretrained network (e.g. the one used in FID or IS); 3. no experiments show the necessity of several designs: the normalization and the averaging over different checkpoints. ",
            "summary_of_the_review": "Overall, this paper presents an interesting idea but the experimental analysis is rather weak. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "[ Contributions ]\n* This paper introduces a new flexible diversity metric.\n  * Existing metrics do not always capture diversity.\n* The proposed metric employs random network distillation.\n  * Split the data into train and val sets.\n  * Train a predictor network to predict the feature vectors output by a randomly initialized target network. (Like BYOL but the target network is never trained.)\n  * Compute the train and val error at the end of every epoch.\n  * The proposed score = the average (normalized) generalization gap over all epochs.\n  * A predictor network trained on training data from a very diverse data distribution should have a harder time predicting the output of the target network on validation data.\n* The proposed metric can be applied to any type of data.\n  * Existing metrics work on specific benchmark tasks.\n\n[ Claims ]\n* The authors validate the proposed metric on images and texts.\n  * Especially in few-show image generation.\n",
            "main_review": "[ Novelty ]\n+ Considering generalization of networks for measuring diversity is novel.\n\n\n[ Soundness ]\n+ Desired properties of a diversity metric are clearly described.\n  + Including more samples.\n  + Robustness to noise.\n  + Disentanglement between quality and diversity.\n  + Agnosticity to training data.\n  + Applicability to many kinds of data.\n\n- Not-always-true argument: Intuitively, when data is diverse, we expect training data to differ significantly from validation data, while when data is not diverse, we expect training to be similar to validation data. \n  - Above argument depends on the sampling methods, which are not described in the paper. Even the number of samples is not provided.\n  - This is the fundamental ground for the metric but it is not supported by any evidence or theory.\n\n- Missing reference: Diversity metric from below paper shows more desirable properties compared to improved recall.\n  - Reliable Fidelity and Diversity Metrics for Generative Models, ICML 2020\n\n[ Clarity ]\n- Many important details on the experiment setups are missing in the paper.\n\n\n[ Evaluation ]\n\n- Missing analysis on the desired properties.\n  - RND score against varying inclusion of samples.\n  - RND score against varying amounts of noise.\n  - RND score against varying quality.\n\n- The proposed metric is not evaluated on few-shot generative models even though they claim.\n  - Few-shot Image Generation via Cross-domain Correspondence, CVPR 2021\n  - Few-shot image generation with elastic weight consolidation, NeurIPS 2020\n  - Few-Shot Unsupervised Image-to-Image Translation, ICCV 2019\n\n- Advantages over existing diversity metrics are not provided.\n  - Why do the research community need the proposed metric instead of existing ones?\n",
            "summary_of_the_review": "I buy the novelty of the proposed metric but it is not rigorously grounded by theory or evaluations.\n\nIt is not persuasive enough to use the proposed metric while there are existing ones.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a new metric for measuring the diversity of datasets based on random network distillation (RND). The metric, called RND score, is based on the idea that datasets which are more difficult to distill should also be more diverse. RND score is data agnostic and does not require a reference dataset. Several tests are conducted on real and synthetic image and text data, where RND score is shown to be capable of detecting mode collapse in GANs and good for evaluating single-image GANs which lack sufficient sufficient training sets for standard GAN metrics.\n",
            "main_review": "**Strengths**\n- Well written and easy to understand.  \n- Useful for measuring how well single-image GANs can produce new variety without replicating the original image.  \n\n**Weaknesses**\n- RND score is slow to compute. Requires training multiple networks for each diversity evaluation.  \n- I am not convinced that RND score is significantly better at measuring diversity than simpler methods such as measuring the average likelihood of the dataset in an embedding space as done in [1]. The likelihood method has the benefit of being much faster to compute since it doesn't require training a new network for each diversity evaluation.  \n- For most standard generative modelling tasks we do have access to a reference set, so standard metrics such as FID and Recall would work fine.  \n\n[1] Kupferschmidt, Kristina Lynn, Eu Wern Teh, and Graham W. Taylor. \"Strength in Diversity: Understanding the impacts of diverse training sets in self-supervised pre-training for histology images.\" (2021).\n\n**Other Comments/Questions:**  \n- Acronym RND is used before it is defined (in first contribution bullet point).  \n- In the second bullet in the list of contributions it is stated that \"This observation calls into question the usefulness of such popular metrics as FID scores for measuring diversity\" and in the FID related work section it is stated that \"FID assumes that the training data is 'sufficient' and does not reward producing more diversity than the training data\". FID does not necessarily measure diversity, but rather the fit to the target distribution. In the case of generative modelling having less diversity than the target distribution is not desirable, but having more diversity is also not good, so FID is correct to not reward extra diversity. In the case of single image generation where the goal is to produce diverse variants then these statements are more correct, but in that case it should be clarified that it is for the single-image GAN case.  \n- One of the motivations for using a random network embedding is to make the metric agnostic to the training data. Why not use a model that has been pretrained in a self-supervised manner, or use the early layers of a pretrained ImageNet classifier? These should still be generic enough as to work with any form of image data.  \n- Is RND score sensitive to the size of the datasets? E.g., if we take the RND score of 100 images from CelebA versus 10000 images from CelebA, would we expect to get the same score? ",
            "summary_of_the_review": "The main limitation of this paper is that it lacks a strong use case for the proposed metric. I do not doubt that RND score can measure diversity, and in fact I think it is a nice, if expensive, way of measuring it. However, I am not convinced of the usefulness of RND score for evaluating conventional generative models, which is the main use case presented in the paper. In most cases there already exist several metrics which are sufficient at evaluating how well generated samples fit the target distribution. The most compelling use case for the diversity metric that was presented was for evaluating single-image GANs, since most pre-existing metrics do not work on this problem, and in this setting we specifically want increased diversity and not a fit to a target distribution. I think the paper could be made stronger if it were rewritten to focus on more compelling use cases than conventional multi-image GANs.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a new scoring method that aims to evaluate the diversity of  generative models.  The score method is inspired by existing research in reinforcement learning literature. In a nutshell, it splits the generated data in training and validation sets and aims to score  diversity based on how well a  predictor network, trained on the train set, can  estimate the feature vector of the same but randomly initialized network architecture.",
            "main_review": "Strength:\n\n- The paper is generally well written and the idea is interesting.\n- The authors show experiments in both images and text generation.\n\nWeakness:\n\n- Major Concern #1. My major concern about this paper is the lack of comparison and discussion of the experimental results vs  recent existing metrics. Experimentally, it only compares to FID and to the recall from [1] (2019) and the comparison/discussion is very shallow. This is  done in table 1 and in my opinion this table may be a bit hard to parse.  What is the reader supposed to get from this table wrt to the proposed method? Where does the proposed method correlates with existing baselines and where it doesnt?  Also why  k=5 is used to compute the Recall?.  [1] showed that k=3 was a  more robust choice that avoids saturation. Moreover, newer frameworks have  been recently proposed improving  the ideas from [1,2]. How does the current work compare to them both technically and experimentally? For instance, how does it compare to  [3,4]? I consider this discussion/analysis is missing.\n\n- Major Concern #2. The proposed metric dependens on the network initialization,  network architecture, hyper-parameters of the optimizer and number of datapoints of the random splits (just to name a few). Figure 11 and 12 show that the score changes with the number of datapoints used for training and validation (expected) , and the  network architecture (also expected). I consider these experiments should be in the main work and more thoroughly  discussed . While I agree that the relative order is preserved, the relative difference is significant in some cases (figure 12. particularly for 0.5 class 200 and 600).  I imagine  they will also change if  different seeds are used for the network intialization and/or optimizer parameters. I believe this is a limitation of the proposed method, and the authors should propose ways (even if they are a bit heuristic) to standardize all these hyper-parameters.\n\n- Equation (3) what is the intuition behind averaging the scores? Is n_0=1?\n\n\n[1] Improved Precision and Recall Metric for Assessing Generative Models, Neurips 2019\n\n[2] Assessing Generative Models via Precision and Recall, Neurips 2018\n\n[3] Reliable Fidelity and Diversity Metrics for Generative Models, ICML 2020\n\n[4]  How Faithful is your Synthetic Data? Sample-level Metrics for Evaluating and Auditing Generative Models, ICML 2021",
            "summary_of_the_review": "Overall, I consider the authors should conduct a more throughout experimental comparison wrt to existing metrics showing the weakness and strength of the proposed approach and experimentally  showing why the need of the new metric. I also consider that in order for the proposed metric to be significantly useful the authors should  proposed a way to standardize, even if based in heuristic, the (not small) number of hyper-params the metric introduces.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}