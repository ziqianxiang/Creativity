{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a token slimming module to insert into each Transformer layer and combine with the layer-by-layer distillation training, achieving an obvious speedup and good accuracy. ",
            "main_review": "Strengths:\nGood speedup and accuracy balance helped from the soft token slimming module. \nGood analysis of different aspects of the proposed methods. \n\nWeakness:\nThe ablation studies about pruning and token slimming parts are weak. \nFor pruning, there are several SOTA methods but only compared with dynamicVIT.  For example: 1. SVITE: Chasing Sparsity in Vision Transformers: An End-to-End Exploration.  2. AutoFormer: Searching Transformers for Visual Recognition.  3. DVT: Not All Images are Worth 16x16 Words: Dynamic Transformers for Efficient Image Recognition. \nFor token slimming, I did not see a direct comparison with Patchslimming and other methods on the same benchmark model? ",
            "summary_of_the_review": "I have concerns about some weak ablation studies to fully demonstrate the usefulness of the proposed TSM module.  I temporarily think this paper is marginally below the acceptance threshold. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents the self-slimming vision transformer that dynamically aggregates tokens to reduce the spatial redundancy of vision transformers. Two new modules named TSM and RTSM are designed to aggregate tokens and recover the aggregated tokens. A layer-wise distillation loss is also proposed to improve the training. Extensive experiments are conducted on ImageNet.",
            "main_review": "The method is clearly motivated and well explained. Compared to previous work that drops the entire tokens to reduce the redundancy. The soft slimming method can better preserve the information of the original feature map. The results look impressive. The proposed method can largely accelerate vision transformers and outperform previous token pruning methods. However, I still have several concerns about this work:\n\n- As mentioned in the paper, the idea of transformer sliming is not new. The proposed dense knowledge distillation has been widely used in previous works on CNN knowledge distillation.  The proposed method is largely based on the previous dynamic token pruning method like DynamicViT. I agree that the exploration in this paper is valuable but it indeed limits the novelty of this work.\n\n- Some key designs are not clearly validated. For example, how is the number of blocks in each stage (Table 2) decided? It seems the numbers are critical for the final performance. The default reduction ratio $\\hat{N}/N$ is set to 0.5. Will the ratio significantly affect the performance?\n\n- Hierarchical transformers like Swin are widely used in the vision community. Is the proposed method compatible with Swin and PVT?\n\n- This paper can be stronger if the results on dense prediction tasks are provided.",
            "summary_of_the_review": "Overall, I think this is a good paper with some technical innovations and strong results. This paper tackles a practical and important problem in vision transformer and achieves impressive results. Extensive experiments and analyses clearly demonstrate the effectiveness of the method. Although I still have some concerns about the novelty and some technical details, I think the strengths outweigh the weaknesses. Therefore, I would like to rate this paper as marginally above the acceptance threshold.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a token pruning module to reduce the number of tokens. It can accelerate the inference process of vision transformers. The authors also present a knowledge distillation strategy to further improve the performance.",
            "main_review": "Strength:\n- This paper investigates an interesting direction to reduce the computational cost of vision transformers. Reducing the number of tokens can  significantly accelerates the inference of vision transformers, which is orthogonal to the structure pruning.\n\nWeakness:\n\n-The layer-to-layer knowledge distillation has been widely used in multiple previous works, such as [1][2], The main contribution,token slimming module, is based on the reconstruction strategy, which is similar to the strategy in [2].\n\n-Though the authors claim SOTA  performance in Table 3, the knowledge distillation is adopted. While the competing method does not use distillation strategy, and thus it seems unfair. It is better to compare the proposed method with more distillation strategies.\n\n[1] FitNets: Hints for Thin Deep Nets, arxiv 1412.\n[2] Few Sample Knowledge Distillation for Efficient Network Compression.",
            "summary_of_the_review": "This paper investigates an interesting direction to reduce the computational cost of vision transformers. But the comparison is a little unfair.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose to prune vision transformer from token dimension. Instead of hard token pruning, the authors propose a Token Slimming Module (TSM) to prune the token in a soft manner by aggregating redundant tokens into fewer informative ones. To reduce the performance degradation after pruning, the authors further propose Dense Knowledge Distillation (DKD) to transfer the knowledge from the unpruned network to the pruned one. Experiments on various vision transformer architectures (e.g., DeiT, T2T-ViT, and LV-ViT) demonstrate the effectiveness of the proposed method. ",
            "main_review": "**Contribution**: \n\n1.\tThe authors propose a Token Slimming Module (TSM) to prune the token in a soft manner. Compared with hard token pruning, the proposed method is able to identify the meaningful tokens dynamically. As a result, the performance drop is smaller.\n\n2.\tThe authors propose Dense Knowledge Distillation (DKD) to transfer the knowledge from the unpruned network to the pruned one, which is able to reduce the performance degradation.\n\n3.\tExperiments on various vision transformer architectures show the promising performance of the proposed method.\n\n\n**Questions and points needed to be improved**:\n1.\tThe novelty of the proposed method is limited. The idea of using knowledge distillation to improve the performance of the compressed models has been proposed in many methods, such as [1][2]. Moreover, the parameter transfer (Using teacher as a pre-trained model) is also a common choice in network pruning.\n\n2.\tExisting vision transformer aggregates multiple tokens into new ones in the self-attention layer. Therefore, the existing hard token pruning methods also consider token aggregation. What is the difference between the proposed soft token pruning and the existing hard token pruning method [3][4][5]? \n\n3.\tHow to determine the sparsity of tokens? Can the proposed method automatically determine the sparsity of token according to the image content?\n\n4.\tSome notations should be more rigorous. For example, in Section 3.2, the notation “C” is not mentioned in the paper. More explanations are required.\n\n5.\tIn Table 4a, to demonstrate the effectiveness of the proposed soft token pruning, it would be better for the authors to show the accuracy-FPS curve of different types of pruning, which would strengthen the paper.\n\n6.\tIn the experiments, what does T-Linear denote? More explanations are required.\n\n\n7.\tIt is possible to combine token pruning and structure pruning together to obtain a more compact model with better performance?\n\n\n**Reference**:\n\n[1] Combining Weight Pruning and Knowledge Distillation for CNN Compression. CVPR 2021.\n\n[2] Model compression via distillation and quantization. ICLR 2018.\n\n[3] Patch Slimming for Efficient Vision Transformers. arXiv 2021.\n\n[4] DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification. arXiv 2021.\n\n[5] Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer. arXiv 2021.\n",
            "summary_of_the_review": "The results are promising. However, the novelty of the proposed method is limited. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}