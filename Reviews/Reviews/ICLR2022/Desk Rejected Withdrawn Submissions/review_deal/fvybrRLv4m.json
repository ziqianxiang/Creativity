{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a dictionary analogue to the compressed sensing result in CSGM (Bora et al). In CSGM, given measurements of the form $y = A x $+ noise, and when the measurement matrix $A$ is _known_, a generative model $G$ can be used to recover $x$ upto representation error of the $G$. \n\nIn this work, $A$ is _unknown_, and the authors propose an alternating minimization technique that alternatively improves their estimate of $A$ and $x$. The authors prove results about the recovery guarantees of their algorithm, under the assumption of a decoding oracle as in CSGM. Furthermore, they show that under the Weight Distribution Condition and Range Restricted Isometry Condition of Hand & Voroninski, such a decoding oracle exists.",
            "main_review": "Strengths:\n1. The paper largely borrows a lot of analysis techniques from Bora et al, Hand & Voroninski, but I think there is novel enough contribution in the analysis. Specifically, the authors show that a pre-conditioning matrix can be used to implement and prove the convergence of alternating minimization.\n\n2. The stated problem has not been considered before.\n\nWeaknesses:\n\n1. The problem is a little contrived. In typical dictionary learning, the dictionary (let's call this $D$) is assumed to be some dataset specific basis that is better than hand-crafted prior like Wavelets. In this setting, $Dx$ is a good image for $x$ drawn from a distribution that has sparse structure, and hence we care about recovering the dictionary $D$ given samples $Dx_i \\;  i=1,...,N$. In the CSGM framework, $G$ would be the replacement for such a dictionary, as $G$ is learned such that $G(z)$ is a good image for random Gaussian $z$. In this work, the authors assume that $A$ is a dictionary that satisfies S-REC and $G$ is a good prior that can combine the columns of $A$. This doesn't really make much sense to me -- why bother training a generative model only to keep the dictionary fixed?\n\n1. Assumption 3, which assumes that they have access to a good initialization for $A$, is very strong. The authors say that Arora et al 2015 make a similar assumption, but I find this comparison unfair -- Arora et al are able to propose an algorithm that can find such an initialization.\n\n1. Given that this is a primarily theoretical work, I do not think it is OK to assume the existence of the pre-conditioning matrix $C^{-1}$ used in the algorithm. If I were to estimate this using the empirical estimator, what goes wrong? Is $\\nu$ too large?\n\n1. The results are _extremely_ hard to parse because of inter-dependencies between the parameters, and it's very difficult to understand the tightness of the bounds. Specifically, the terms $\\nu, \\gamma, \\delta$ are all interlinked and depend on the ambient dimension and latent dimension in different ways, and it's difficult to understand how good the results actually are.\n\n1. The assumptions on the dictionary are stronger than traditional work. Typical dictionary learning only requires incoherence of the dictionary elements. This work assumes S-REC, which is significantly stronger.\n\n1. The authors assume that the image lies in the range of their generative model, whereas CSGM applies to any signal close to the range. I did not find a discussion about this in the paper.\n\n1. I did not understand anything in the experiments section : the experimental setup, hypothesis, and validation do not make sense to me. If the measurements are generated by an image in the range of the generative model, then of course you will get better reconstructions than k-SVD and other sparsity based methods. Also, I did not understand the motivation or conclusion about the bit complexity of $z$.",
            "summary_of_the_review": "I find the problem a little contrived and the assumptions are quite strong. The experimental results are not particularly convincing.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": " In this work, the authors analyze a dictionary learning framework for compressing data by introducing generative model-based priors for coefficients, as opposed to the traditional sparsity-based framework. To recover a dictionary with generative coefficient priors, a natural alternating minimization algorithm is proposed, where estimates for the dictionary and generative coefficients are updated in an alternating fashion. Algorithmic recovery guarantees are provided, which establish that a ground-truth dictionary can be recovered with a sufficiently good initialization and an optimization oracle for the non-convex generative prior-based update. When an optimization oracle is unavailable, guarantees based on a random ReLU generator are provided, which show that the empirical risk minimization problem has a descent direction outside of certain neighborhoods. Empirical results in simple scenarios suggest generative coefficient priors can lead to better compression.",
            "main_review": "**Quality:**\n\n- Strengths: - The theory presented is able to combine previous analysis for dictionary learning and generative priors in a meaningful way, showcasing convergence guarantees which are challenging in this area. - Compression seems to experimentally improve under this generative coefficient prior in simple settings.\n\n- Weaknesses: - The experiments are on very small datasets and in toy settings. - Some parts of the theory are insufficiently explored. For example, under what scenarios can we expect invertibility of $\\mathbb{E}_z [G(z) G(z)^T]$? Perhaps this could be shown to hold in simple settings, e.g., $G(z) := \\text{ReLU}(Wz)$ with the WDC assumption. Tools from NTK theory could potentially be helpful here, since the entries are of the form $\\mathbb{E}_z[\\text{ReLU}(\\langle w_i,z\\rangle)\\text{ReLU}(\\langle w_j,z\\rangle)]$. - I have some concerns about some of the claimed relevance of this approach to transfer learning. In particular, I was a bit confused by the experimental setup of the generative prior derived from MNIST VAE. What is the broader claim about the relationship between this framework for dictionary learning with generative coefficient priors and transfer learning, and how does this experiment comment on this relationship? Is the claim that dictionary learning with generative priors can be phrased as learning the last linear layer of a generative model?\n\n**Clarity:** Overall, the paper was fairly well-written and easy to follow in most parts. Here are some typos that I found:\n- Top of page 2: “atoms, simultaneously” -> “atoms, while simultaneously”\n- Bottom of page 5: a transpose on $\\mathbb{E}[G(z)G(z)]$ is missing and a parenthesis on $\\mathbb{E}[A^s G(z) - A^* G(z^*)G(z^*)^T]$ as well.\n- In the appendix, it may be best to use notation that shows $\\Pi_{i=d}^1 W_{i,+,z}$’s dependence on $z$, e.g. $W_z := \\Pi_{i=d}^1 W_{i,+,z}$\n- In the update rule in Algorithm1, should the projection operator be applied to $A^s$ or to $A^s - \\eta \\hat{g}^s$?\n\n**Novelty and significance:** To the reviewer’s knowledge, this work is the first to incorporate generative neural network priors in the dictionary learning setting. In terms of analysis, the theory is a combination of previous work on dictionary learning from Arora et al [1], along with theory from Bora et al [2] and Hand and Voroninski [3]. While new theoretical tools aren’t provided, the combination of these ideas is novel.\n\n**Minor comments:**\n\n- For convergence guarantees of optimization over $z$ in compressive sensing or denoising, one may want to cite Huang et al [4] and Heckel et al [5]. \n- Recent work has shown that the logarithmic factor in the WDC can be relaxed to a constant factor [6].\n\n**References:**\n\n[1] Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Motiga. Simple, effiicent, and neural algorithms for sparse coding. JMLR\n\n[2] Ashish Bora, Ail Jalal, Eric Price, and Alexandros G. Dimakis. Compressed sensing using generative models. ICML\n\n[3] Paul Hand and Vladislav Voroninski. Global guarantees for enforcing deep generative priors by empirical risk. COLT\n\n[4] Wen Huang, Reinhard Heckel, Paul Hand, Vladislav Voroninski. A provably convergent scheme for compressive sensing under random generative priors. Journal of Fourier Analysis and Applications\n\n[5] Reinhard Heckel, Wen Huang, Paul Hand, Vladislav Voroninski. Rate-optimal denoising with deep neural networks. Information and Inference\n\n[6] Constantinos Daskalakis, Dhruv Rohatgi, Manolis Zampetakis. Constant-expansion suffices for compressed sensing with generative priors. NeurIPS",
            "summary_of_the_review": "Overall, this work provides a solid technical contribution in terms of its analysis, but the true applicability of this framework is more difficult to see and the experiments are on the weaker side. Based on this, I am leaning towards a score of 6.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new model for dictionary learning that can capture complex cooccurrence relationships between dictionary elements. Furthermore, a new version of dictionary learning is presented to incorporate non-trivial dependencies in the coefficients via a generative prior. Specifically, \n1) an intuitive alternating minimization algorithm is designed for recovering the unknown dictionary; \n2) the provable convergence guarantees of the algorithm are validated in a particular idealized setting; \n3) the effectiveness of the proposed dictionary learning algorithm is proved on denoising tasks.\n",
            "main_review": "Strengths: \n1) The new version of dictionary learning algorithm process (Section 4) is relatively clear, and the contribution points are clear;\n2) The theoretical proof of the algorithm is relatively complete, especially considering several settings, without noise and when the generative model is an expansive Gaussian ReLU network. The derivation of each formula also has good theoretical support, such as the tools from Hand & Voroninski (2018);\n\nWeaknesses\n1) In Section 1.1. “Thereby bringing the deep learning revolution to dictionary learning.” This conclusion needs to be revised. The combination of deep learning and dictionary learning began to develop in 2016 or earlier (such as deep dictionary learning). This paper should add a condition like a generative prior;\n2) In Section 2. This part talks about dictionary learning, compressed sensing with additional structure, generative priors for inverse problems, and learning distributional transformations, which seems a bit long-winded. This paper focuses on two key technologies, including dictionary learning and generative priors. It is recommended to elaborate on the related work of these two technologies;\n3) In Section 4. Theorem 5.5 should be Lemma 5;\n4) In section 5. Theorem 5.2 and Theorem 5.3 is almost the same as Hand & Voroninski (2018). Thus, it is not recommended to use a large amount of space in the text to explain these two theoretical derivations;\n5) In section 5.1. “Putting all of these facts, together, one can obtain as desired that”. Please explain in detail.\n6) In Section 6. The experiment lacks an analysis of the optimization method including the ablation study of the proposed algorithm.\n7) In Section 6. For the transfer learning experiments (in the \"Generative prior derived from an MNIST VAE\") on denoising, it is recommended to conduct comparative analysis of different degrees, that is, the similarity of the source domain and the target domain;\n8) Remark 2 in Section 6 can be removed from the text. “because the sample complexity at which our method succeeds is too low for these methods.” \n",
            "summary_of_the_review": "In this paper, a new version of dictionary learning is proposed to incorporate non-trivial dependencies in the coefficients via a generative prior. The theoretical proof of the paper is substantial.  However, the method in this paper lacks novelty. In my opinion, it is a derivation of MOD algorithms with a generative prior. In addition, this paper also needs some revision in the layout and experiment part.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors study the dictionary learning problem under a known generative prior to the coefficients. Under various model assumptions, the authors provide a provable algorithm for recovering the unknown dictionary given a suitable initialization. ",
            "main_review": "Strengths:\n\nThe framework of combining dictionary learning and generative priors is novel up to my knowledge. I appreciate the authors' effort to propose such a framework. In addition, although this framework seems to be somewhat unnatural and impractical at the first glance, the authors try their best to provide some practical motivations and justifications.\n\nWeaknesses:\n\n1. Although the authors have tried to provide justifications and a thought experiment. I am still not convinced that such a framework is really practically meaningful. The authors claimed in the abstract that \"it may be viewed as transfer learning for generative models, in which we learn a new linear layer (the dictionary) to fine-tune a pre-trained generative model (the coefficient prior) on a new dataset\". But I don't think the current experiments on synthetic compression and denoising tasks can support this claim. For example, I cannot see clearly the advantage of training a generative prior (suppose that there are $d$ layers) first and then training a new linear layer (the dictionary), instead of training a ($d+1$)-layer generative model directly. \n\n2. Theorems 5.1 and 5.2 seem to be simple extensions of the results in Bora et al. (2017), and Theorem 5.3 seems to be a simple extension of the results in Hand & Voroninski (2018). \n\n3. The notation is quite messy and there are assumptions that seem to be rather restrictive or even unrealistic in theory. For example, \n\n- Assumption 3: Usually, a dictionary can only be recovered up the ambiguity of sign, scale, and permutation, how can the authors make an assumption to get rid of all these ambiguities directly?\n\n- Assumption 5: What is the typical scaling of $\\nu$, is it $\\Theta(1)$ or it vanishes as $m,n \\to \\infty$? In addition, the authors use the notation $C^{-1}$ but mention that  $C^{-1}$ does not need to satisfy any properties including invertibility. It is weird, and $C^{-1}$ is a really bad and misleading notation.\n\n- Assumption 6: I guess that the required technical condition $(\\nu \\||C^\\*\\|| + 10 n \\||C^{-1}\\|| \\frac{R^2}{\\gamma}) \\le \\frac{1}{4}$ is very stringent and unrealistic. On the one hand, since both $n$ and $R$ should be considered large values (and $\\gamma$ is typically a fixed constant, e.g., $\\gamma = \\frac{1}{2}$), it requires $\\||(C^\\*)^{-1}\\||$ to be very small (e.g., $\\||(C^\\*)^{-1}\\|| = 1/n^{6}$) if we think of $\\||(C^\\*)^{-1}\\|| \\approx \\||C^{-1}\\||$, and it thus requires $\\||C^\\*\\||$ to be rather large (e.g., $\\||C^\\*\\|| \\ge n^6$). However, on the other hand, from $\\nu \\||C^\\*\\|| \\le \\frac{1}{4} $, $\\||C^\\*\\||$ cannot be large (unless that $\\nu$ is assumed to be rather small, which is also restrictive).  \n\n- In Lemmas 5.4 and 5.5, as well as Theorem 5.2, there is a term $R$ (or $R^2$) in the upper bounds. For instance, in Lemma 5.5, if we consider the scaling that $R = n^{c}$ (for some positive constant $c$), $\\||\\Sigma\\||=\\||A - A^\\*\\||$ needs to be very small (e.g., $\\le 1/n^{c}$) to make the upper bound for $\\||G(z)-G(z^*)\\||$ meaningful, which is again very restrictive.\n\n- I hope that the authors can discuss the typical scalings of most of the important parameters clearly. Otherwise, some of the above-mentioned assumptions and theoretical guarantees are not meaningful to me. \n\n- In Algorithm 1, the authors use fresh samples during each iteration. However, e.g., as mentioned in the paper:\nCandes, Emmanuel J., Xiaodong Li, and Mahdi Soltanolkotabi. \"Phase retrieval via Wirtinger flow: Theory and algorithms.\" IEEE Transactions on Information Theory 61.4 (2015): 1985-2007;\nit is not realistic to imagine (1) that we will divide the samples into distinct blocks (how many blocks should we form a priori? of which sizes?) and (2) that we will use measured data only once. That is, the use of fresh samples during each iteration seems to be rather restrictive in practice.",
            "summary_of_the_review": "In spite of the novel framework of combining dictionary learning and generative priors, due to the insufficient practical motivation and major technical flaws (e.g., perhaps unrealistic assumptions and meaningless error bounds), my overall impression is that this submission is not ready for publication at the current version. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}