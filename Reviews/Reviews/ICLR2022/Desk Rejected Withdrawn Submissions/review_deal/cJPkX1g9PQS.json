{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposed to use contrastive learning to generate coherent text, where the hard negatives are mined using local negative sample ranking (similar to ANCE but less computation). The proposed method is evaluated on numerous benchmarks, and improved over previous state-of-the-art UNC model. ",
            "main_review": "Pros:\n1. The paper is simple and easy to understand, and the problem in this paper is also interesting\n2. All results come with standard deviation, which is much easier for reader to spot the significance of proposed method (component)\n\n\nQuestions / Cons: \nHowever, I do have some questions/concerns, and think there might be some way to improve the paper.\n\n(1) In section 3.1., based on my understanding, [CLS] token is appended in every sentence in XLNet, rather than the document D. Could you help me clarify this part? Also the notation in this part is not clear, for example, where does $t$ in $v_t$ come from? Does $b$ consider as parameter that needs to be trained (since it does not appear in $\\theta$)?\n\n(2) In table 1, it seems that UNC model is trained only on WSJ. Without proper pre-training, the generalization ability of the model is typically not good since the domain of the text could be significantly different. Instead the proposed method is modified from XLNet, a model trained on a very large-scale dataset, so it is not really a fair comparison. \n\n(3) Also for the results in table 1, \"Our- Pairwise\" model already outperforms UNC baseline by a large margin, though it only use the pairwise ranking loss, and it hasn't incorporated any components introduced in the paper yet. After introducing contrastive loss by using more negatives, the model's improvement seems really marginal compare with simple pairwise baseline. Same with the full model, seems all other components does not help much (momentum encoder and hard negative mining). \n\n(4) The number of negative examples (5) seems quite small, have you tried larger number?\n\n(5) Table 2  has the same problem with table 1 (weak results from new components). In addition, it uses \"contrastive\" as model name, which model does it refer to, \"full\" or \"contrastive\" as stated in table 1.\n\n(6) The model was tested ONLY on coherent dataset, so we are not sure if the proposed objective could be transfer to other downstream tasks.",
            "summary_of_the_review": "The paper tried to use contrastive loss to tackle the discourse problem, and proved it works better compare to SOTA UNC model on various dataset. However, the experiments could be improved to prove the effectiveness of the proposed method.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper discusses the problem of coherence modeling. It points out that although previous works have conducted wide research on coherence modeling, they can only perform well on the permuted document task while cannot generalize well to downstream tasks. \n\nIn this work, the authors propose a coherence model based on contrastive learning that can generalize well and can be used off-the-shelf for other downstream applications. Based on a pretrained XLNet, the authors adopt pairwise ranking, contrastive learning and momentum encoder to enhance the performance. The authors also propose to mine harder negative samples to boost the effect of contrastive learning. Several evaluation datasets are collected from downstream tasks and the authors compare their proposed methods with the current state-of-the-art methods. Experimental results demonstrate that their methods are superior to the current SOTA models. Further analyses demonstrate that the proposed methods can generalize well to downstream applications in different settings.\n",
            "main_review": "Pros:\n\n1. The problem of modeling coherence for downstream applications is important and can have an extensive impact on further research and evaluations.\n\n2. It is a natural thought to adopt contrastive learning to perform self-supervised learning on the task of coherence modeling. The proposed methods can outperform the current SOTA methods, and also they demonstrate better generalization ability on several downstream evaluation datasets.\n\n3. The \"local negative sample ranking\" method is effective since mining harder negative samples is proved to be useful. The experimental results in the paper proves that the proposed \"local ranking method\" can reduce the search space of candidate negative samples. \nThe paper is well-written and easy to understand.\n\nCons:\n\n1. The SOTA baseline used in the paper, UNC, is a non-pretrained model, while the proposed methods in this paper are based on XLNet. Considering the large pretraining corpus, it is imaginable that XLNet-based methods can perform better and generalize better to downstream tasks. It is also shown in [1] that a pretrained model can overperform UNC and many other strong baselines. Given the results in Table 1, the performance gain seems to be mainly brought by incorporating XLNet as the base network while the contribution of contrastive learning seems marginal. \n    \n    (a). One question I want to raise here is that I wonder how the performance will be if XLNet is evaluated on these datasets before finetuning. Such a comparison can show how much domain knowledge is contained in a un-finetuned XLNet.\n    \n    (b). Another alternative experiment is to apply the proposed contrastive learning objective to the SOTA baseline, UNC, and see if contrastive learning can bring improvements to it.\n\n2. Some experimental results lack interpretation. For example, \n    \n    (a). In Figure 2a, why will harder negative samples cause instability to the basic contrastive model? Is such a phenomenon observed in previous research or on other tasks? \n    \n    (b). Since Table 1 and Figure 2b only compares the influence of h, how will be the results if no hard sample mining method is adopted here?\n\n3. I am also concerned how much contribution such methods can bring to the evaluations of downstream applications. This paper mainly discusses generalizale coherence modeling. However, as shown in Table 1 and Table 2, when trained on INSTED-WIKI dataset, the performance on INSTED-WIKI is 82.01. But if trained on WSJ, the performance on INSTED-WIKI will degrade to 72.04. It seems to be a large gap. So I am concerned about whether we can regard it as a good generalization if such a gap exists. Also when we really use a model with this generalization performance to evaluate downstream tasks, how confident will the predictions be?\n    \n    (a). I would like to see more comparisons between the generalized results and results trained on the dataset in the same domain. This can help with understanding of how confident the model generalization ability will be. \n\n[1] Abhishek et al., Transformer Models for Text Coherence Assessment, Arxiv preprint\n",
            "summary_of_the_review": "This work studies a fundamental task of modeling coherence for downstream applications. The experiments can well support the claims of the advantages for the proposed methods. However, I am concerned if the improvements are mostly brought by the pretrained models. As stated in Cons 1, more experiments on a un-finetuned XLNet or adapting the proposed contrastive objective to the current SOTA model (UNC) will help to clarify this. Also, more discussions on the experimental results and more experiments on evaluating the generalization ability will help to make this work stronger.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors point out that, though many claims have been made about improved coherence, studying coherence rigorously and in a fine-grained manned has been difficult. Yet, the saturation of reference-based metrics that do not consider coherence has made the need for such evaluations imperative. The authors review the history of coherence modeling and then proceed to describe the datasets used, the XLNet pretrained model, and two simplified versions of their model, which use margin-based and contrastive losses, respectively. Next, the motivation for better negative mining is described, followed by the \"momentum encoder\" that the authors propose to solve the problem of positive and negative examples becoming stale as training proceeds. Length invariance and hard negative mining are added on top of this for the final model, with results showing slightly better average results than the first two models described, but with standard deviation ranges that generally contain the mean points of the simpler models. For the analysis, the authors show the differences across certain hyperparameter values. Finally the authors test using different negative types (sentence intrusion and the permuted document task), as well as using a recently created manual linguistic probing dataset to identify what categories of coherence and incoherence their model is capable of finding, albeit using a different training set than was used for the majority of the paper.",
            "main_review": "The authors do a good job motivating the ide athat coherence models are deeply necessary for understanding machine generative text and of contextualizing their work in connection to previous literature. However, given that their motivation is about using coherence evaluation for Natural Language Generation (NLG), I would have hoped to see at least some initial experiments that show how this model can be used for downstream applications. Equally good would have been a case study on using the proposed coherence model for evaluation. To be fair, the authors go slightly further than the previous literature by testing over different datasets that likely have distribution shift from the training set, but fundamentally the authors leave it implicit what should be done with coherence models.\n\nWhile the single baseline is beaten by the proposed models, I have a few worries about the results.\n\nFirst of all, the only baseline provided does *extremely* poorly, often worse than random! Surely if it is simply the case that all previous models do so poorly under distribution shift then more intermediate models could have been shown that do worse than the proposed model but better than random.\n\nSecond the final proposed model is significantly more complicated than the two simplified versions described in 3.2 and 3.3, but it rarely outperforms the average results of these simplified models by more than a standard deviation.\n\nThird, it seems highly likely that much of transfer this model achieves is a result of using a pretrained model, but this is never properly addressed, e.g. by using pretrained models for baselines, trying to finetune different pretrained models, etc.\n\nFourth, ablations for the final model are lacking, though the exploration of different hyperparameters is enlightening and appreciated. \n\nThe analysis is also somewhat inconsistent, i.e., for the Permuted Document Tasks only one dataset is used from the previous analysis, with a footnote that this setup \"generalized better\" rather than simply showing the full results.\n\nAltogether, I feel that though the motivation of this paper is good, the results presented don't have convincing baselines, ablations, or downstream applications that showcase why the proposed model is something that should be adopted by others. The architecture+training methodology in 3.4 is certainly somewhat novel, but without being properly ablated it is hard to know how useful this novelty is.",
            "summary_of_the_review": "While the problem of coherence modeling is important, I do not think this paper should be accepted. The gains over the single baseline are drastic, without much analysis as to why the difference is so huge or what models might fill the space inbetween, while the complete model isn't properly ablated. Finally, the case for how the specific kind of coherence model that the authors develop can be used isn't clear. I recommend to reject this paper in its current form.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper considers the task of distinguishing between coherent and incoherent texts. The authors propose a new set of techniques for training models in such tasks and show a decent advantage over a previous method: UNC [1].\n\n[1] Moon, Han Cheol, et al. \"A unified neural coherence model.\" arXiv preprint arXiv:1909.00349 (2019).",
            "main_review": "Pros:\n\n1. Designing machine learning models that could produce coherent text is an important goal.\n2. An extensive amount of experiments have been conducted.\n3. A decent improvement over the previous state-of-the-art UNC model [1] is obtained.\n4. The test set is sufficiently different from the training set and the model still works quite well (at least much better than the previous state-of-the-art UNC model [1]).\n\nCons:\n\n1. There has not been a substantial amount of works studying the state-of-the-art UNC model [1]. Hence, it is unclear to me if the improvement over this method is a substantial contribution.\n2. If the goal is to design ML models that could produce coherent text, it is unclear if training a model that could accurately distinguish between coherent and incoherent texts is necessary. For example, the ML model presented here is based on the pre-trained model XLNet [2]. Hence, XLNet is itself a model that could produce coherent texts. The two applications of the task studied in this work seem to be: essay scoring and sentence ordering. However, the contribution of this work seems to have less relevance in the other two applications stated in the introduction: language generation and summarization.\n3. If we use the coherence model trained here (based on XLNet), I have concerns about whether the coherence model can unbiasedly score the coherence of the text generated by XLNet. Maybe the coherence model trained this way will not be able to find coherence errors in text generated by XLNet (or other similar models). The performance shown in LMvLM is also a little low.\n\n[1] Moon, Han Cheol, et al. \"A unified neural coherence model.\" arXiv preprint arXiv:1909.00349 (2019).\n[2] Yang, Zhilin, et al. \"Xlnet: Generalized autoregressive pretraining for language understanding.\" Advances in neural information processing systems 32 (2019).",
            "summary_of_the_review": "The work presents a decent improvement over a previous state-of-the-art approach. However, there is some concern regarding the significance of the task studied here.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}