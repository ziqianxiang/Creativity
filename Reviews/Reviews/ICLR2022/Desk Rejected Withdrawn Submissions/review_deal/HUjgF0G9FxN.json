{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors tackle Semi-Supervised Federated Learning, where clients have completely unlabeled data while the server has a small amount of labeled data, and propose a method called SemiFL, which performs strong augmentations on unlabeled data and trains the labeled server and the unlabeled clients alternatively. SemiFL is evaluated and compared with the existing SSL, FL, SSFL methods on several benchmark datasets.\n",
            "main_review": "This paper is well-written and -structured. The motivation is interesting and the problem definition is clear.\n\nFor the methodology and evaluation parts, however, several critical points are required to be improved or addressed.  \n\n- *Lack of Novelty*: SemiFL seems more like an application of a bunch of prior works to the semi-supervised federated learning scenario. For example, the proposed method consists of three components, (1) strong data augmentation, (2) alternate training, and (3) static Batch Normalization. For (1), I was not able to find novel contribution points from the proposed augmentation technique but realize it is a combination of the existing techniques of FixMatch, MixMatch, and Mixup. Similarly, for (3), adopting sBN is also hard to see their own significant contribution to the ML community. For (2), alternate training seems somehow novel, but still not significant. To the best of my knowledge, one of the prior works, FedMatch, studied disjoint learning with two sets of parameters for supervised and unsupervised learning, which shares a similar concept to the proposed SemiFL method.\n\n\n- *Comparison of SemiFL and FL/SSFL methods*: the results seem strong, but the experimental setup is quite confusing. (1) Can you use the same network for other methods as well? The different choice of the base networks makes analyzing the results much harder. My suggestion is to simply use WResNet28x2 for all methods. (2) Can you use the same normalization technique for other methods? I personally think that adopting the sBN technique should be tone-down and the choice of normalization techniques should be identical across all methods for proper evaluation.\n\n\n- *More Benchmark Datasets Required*: while the comparison with SSL methods uses three datasets (CIFAR10, 100, SVHN), only CIFAR-10 is used for the comparison with FL and SSFL methods. It is highly recommended to use at least two more datasets and show consistent results in order to support your arguments. \n",
            "summary_of_the_review": "As several critical enhancements are desired in terms of novelty and evaluations, I am unfortunately not able to recommend acceptance at this time. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "Previous work related to federated learning (FL) is based on an unrealistic assumption: each client holds labeled data. On the contrary, this paper studied a more practical scenario in FL, that is, the semi-supervised federated learning (SSFL) scenario. To this end, the authors proposed a semi-supervised federated learning framework called SemiFL. In SemiFL, the client holds completely unlabeled data, while the server holds a small amount of labeled data. In particular, SemiFL integrates some mainstream communication efficiency and semi-supervised learning techniques well. Furthermore, the authors provide a theoretical understanding of semi-supervised learning (SSL) using strong data augmentation. Extensive case studies illustrate that SemiFL can outperform many existing FL results trained with fully supervised data and is competitive with the most advanced centralized SSL methods.",
            "main_review": "The strengths and weaknesses of this paper are summarized as follows:\nStrengths:\n+ The problem studied in this paper is important and needs to be solved in SSFL\n+ Good writing\n+ Sufficient theoretical analysis\n+ Interesting theoretical findings\n\nWeaknesses:\n- Need to include more related work that is highly important\n- Need more justifications about the novelty claims \n- Need to add more experiments to show that the communication is efficient\n- Need to add more benchmark datasets\n- Need to add more hyperparameter experiments\n\nComments:\n1.  The following important references are missing, e.g., [1][2]. Furthermore, the format of the references requires careful proofreading—for example, reference [3].\n\n[1] Zhang Z, Yao Z, Yang Y, et al. Benchmarking semi-supervised federated learning[J]. arXiv preprint arXiv:2008.11364, 2020, 17.\n\n[2] Long Z, Che L, Wang Y, et al. FedSemi: An Adaptive Federated Semi-Supervised Learning Framework[J]. arXiv preprint arXiv:2012.03292, 2020.\n\n[3] Jeong W, Yoon J, Yang E, et al. Federated Semi-Supervised Learning with Inter-Client Consistency & Disjoint Learning[J]. Proc. of ICLR.\n\n2.  In the Introduction Section, the authors merely emphasized the practicality of semi-supervised federated learning. However, the authors still need to provide more details on the difficult challenges of combining semi-supervised learning techniques with federated learning. The reason is that this will make readers feel that the authors are just naively combining the two techniques.\n3.  The authors claimed that the proposed method is communication efficient, but there are no relevant comparative experiments to support these claims.\n4.  In this paper, the authors seem to simply combine the current mainstream semi-supervised learning techniques with the existing batch norm techniques, which reduces the novelty of the paper. It would be better if the authors could provide more justifications to enhance the contribution claimed in this paper.\n5.  The semi-supervised learning part in the paper is very close to [1][2]. Then readers may be curious about the difference between this paper and [1][2]. Therefore, the authors need to provide more details to enhance the difference between them.\n6.  The Federated LEAF dataset is a standard benchmark for federated settings, so it would be better if the authors could add an NLP dataset (i.e., Federated Shakespeare dataset) to evaluate the proposed SSFL algorithm.\n[4] Caldas S, Duddu S M K, Wu P, et al. Leaf: A benchmark for federated settings[J]. arXiv preprint arXiv:1812.01097, 2018.\n7.  The number of communicated clients is a significant factor affecting SSFL performance. Therefore, it would be better if the authors could explore the performance of the proposed algorithm under the setting of the number of different communicated clients, like [1].\n8.  Will the alternate training proposed in this paper generates additional computational or operational costs? It would be better if the author could add a more detailed analysis.\n9.  In this paper, the amount of labeled data on the server significantly affects the performance of the proposed algorithm. Therefore, authors should explore the algorithm's performance under different supervised data ratios (that is, the ratio of supervised data to all data). In addition, the author should keep the proportion of supervised data consistent when comparing performance, especially when comparing fully supervised and centralized semi-supervised.\n10. The editorial quality of this paper is not always satisfactory. It contains quite a lot of inconsistent/non-precise descriptions, as also reflected in the above comments.\n",
            "summary_of_the_review": "The paper tackles a very interesting problem, and the many technical considerations, as well as the experiments on various datasets and comparisons with existing SSFL techniques, are commendable. Some issues (unclear communication efficient contribution,  naïve combination issues, and hyperparameter experiments) still prevent me from recommending complete acceptance. More clarifications are necessary, and by adding some more \"realistic'' experiments, I believe the paper could be turned into a significant submission of ICLR. I recommend a \"5'', but my score can be easily increased to 6 by addressing the many clarifications expressed in my review. Further experiments and the concrete use-case would further improve my score.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work is concerned with a very practical scenario of Federated Learning where the participating clients may not have access to labelled data. It proposes a sophisticated approach where clients locally employ a combination of FixMatch and MixMatch to expand the datasets, followed by a convex combination of L_fix with additional data augmentation and a mixup loss L_mix to optimise the model parameters. Experiments are provided in supports of its operability and qualitative arguments are provided to differentiate with respect to other works.",
            "main_review": "Strengths \n- Addresses an important aspect of FL\n- Interesting combination of approaches to extend SSL from centralised to federated setting\n\nWeaknesses\n- Given the fairly involved approach, it is unclear how each part contributes to the overall effectiveness.\n- Similarly, the effect of different hyperparameters can be investigated which currently have been adopted from their centralised counterparts.\n\n\nOther comments\n- Overall the approach was clear, the only drawback is its complexity makes it difficult to interpret the benefits of its different components. For example, what happens if strong augmentation is used at both client and server ends? Or if _lambda_ is set to zero and only L_fix is used for optimising? \n- I am also unclear on what is being referred to as “Parallel” and “Alternate” in Table 2. \n- I would also recommend the use of standard terms, for example, _active users_ wasn’t immediately obvious in the contributions and its definition is only mentioned later in the text.\n",
            "summary_of_the_review": "I think this work demonstrates interesting results for semi-supervised federated learning which are comparable to centralised case. However, as mentioned above a more systematic analysis of why particular design choices were made to devise this approach, and preferably with experimentation would certainly help improve the quality of this work.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}