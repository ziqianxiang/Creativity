{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, cross-task knowledge transferability for continual learning settings is explored. The idea is to equip HAT with knowledge transfer ability via clustering embeddings of tasks to measure their similarities to detect dissimilar tasks. Partially relaxed masks are then used to block the parameters that are important for the dissimilar tasks during model updating to enable knowledge transfer. Experiments on four benchmark CL tasks are provided to demonstrate that the method is effective.",
            "main_review": "Strengths:\n\n1. The problem of knowledge transfer in CL is highly unexplored and is a great research area.\n\n2. The paper is written well.\n\n3. Experiments are convincing.\n\nWeaknesses:\n\n1. The idea is incremental and is only focused on improving an existing approach.\n\n2. Experiments are not extensive enough.\n\n",
            "summary_of_the_review": "I really like the explore direction because as the authors explain, existing methods mostly focus only on catastrophic forgetting, whereas knowledge transfer and accumulative learning are as important for continual learning. However, I have some reservations for this work:\n\n1. In terms of contribution, the method is an improvement of HAT. Hence, the idea contribution is not significant. Yet, I think with more thorough and extensive experiments, this may be resolved.\n\n2. Given the trend in the literature, experiments are performed on relatively simple datasets. I think results on split-Sub-ImageNet and Split-ImageNet need to be included. Even for CIFAR100, I think it is much better to include other common splits, such as 5 and 25 classes per task.\n\n3. FWT/BWT metrics are very important to evaluate this work but from the tables, I am not seeing a significant improvement over the baselines. My guess is this can change if you perform experiments on more challenging datasets because low-level features are going to enable more knowledge transfer across the tasks.\n\n4. Comparison with more works is missing. Some important baselines such as \"memory aware synapses\" or \"deep generative replay\" are missing. I am very curious to see their FWT/BWT performance because they are not restricting the model. Hence, it may be possible that they already enable knowledge transfer.\n\n5. An area of interest is to analyze the learning curves, i.e., performance vs learning epoch. It is helpful to see if the proposed method enables better jumpstart performance when a new task is learned.\n\nIn conclusion, I think this is an interesting direction but needs further exploration.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies the task incremental learning problem, and builds on top of a previous method called hard attention to the task (HAT). This work extends HAT with two objectives, overcoming CF and transferring knowledge among tasks. Specifically,  the authors cluster the tasks to identify task-similar and task-dissimilar set. The accumulated mark focuses only on the dis-similar tasks, while allowing the parameter update for other tasks. This overcomes HAT’s issue of preventing opportunities for improvement on other tasks. Experimental results demonstrate the proposed RPM method performs competitively with other baselines with an efficient computational time. ",
            "main_review": "Pro:\n1.\tThe idea of only partially restraining dissimilar tasks is intuitively correct.\n2.\tThis technical part is clearly written. \n3.\tThe computation time is dramatically reduced compared to CAT.\n4.\tI like the authors discuss the limitation in Section 4.7\n\n\nCon:\n1.\tThis work makes a direct extension to HAT. However, from the results in both accuracy/CT metrics, the improvement is marginal.\n2.\tBecause this is an extension of HAT, the proposed RPM seems not easy to be applied to other methods. The authors mentioned this can be exploited jointly with other techniques, while no clear combination is provided.\n3.\tSome key recent works are missing. For instance, in Masana et al.’s work, a large-margin improvement over HAT is shown. I would strongly suggest including this work as a baseline.\n\n[ref1] Marc Masana, et al., Ternary Feature Masks: Zero-Forgetting for Task-Incremental Learning, CVPRW, 2021\n[ref2]Gobinda Saha, et al., GRADIENT PROJECTION MEMORY FOR CONTINUAL LEARNING, ICLR 2021\n\n4.\tCompared to HAT, PRM has clustering (X-means) as an overhead. I’m wondering why PRM’s CT result shown in tables is less than HAT’s.\n5.\tK-means based clustering method is known for its randomness and instable convergence. Usually 10 repeated runs are needed. I’m also wondering for clustering in this work, how many runs are conducted? Are the performance across runs stable?\n",
            "summary_of_the_review": "This works extends HAT to solve the knowledge transfer among tasks. The idea is intuitively correct. While the results fail to support it. A few other questions/comments need to be addressed to meet the acceptance bar.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper looks at the problem transferring knowledge across tasks in addition to avoiding catastrophic forgetting in continual learning. The paper proposes an extension HAT, which first identifies dissimilar tasks by clustering of the task representation obtained in HAT, and then uses only the dissimilar tasks’ masks to reduce the gradients. This allows opportunities for transferring knowledge for similar tasks, while avoiding forgetting for dissimilar tasks. \n",
            "main_review": "Strengths\n1. The paper looks at an important problem in continual learning, the problem of transfer, which as the paper rightly points out has been understudied in the community compared to avoiding catastrophic forgetting.\n2. The paper is easy to read and understand (except in a couple of places)\n\nWeaknesses/Questions\n1. The contribution of the paper looks very incremental to me. Please correct me if I am wrong. Is the only difference between HAT and the proposed method is that in the proposed method the gradient masking happens only for tasks that are identified as dissimilar? \n2. In relation to the above point, the observed empirical improvement in performance compared to HAT is only in the case of when the celebrity dataset. In other cases it performs almost the same as HAT.\n3. Is it valid to make a binary distinction between tasks being similar or dissimilar? There might be some parts/learned features/aspects of it being dissimilar while some similar/useful for transfer?\n4. If such binary decisions are not possible in principle then by doing this, the proposed method seems to introduce a tradeoff between forgetting and transfer depending on how the DTD is made? Is that correct? Was this observed in the experiments? I think so from the results in the table? Discussion on this would be useful, especially on how to make this trade off.\n\nOther Comments\n1. The beginning of section 3 (page 3), the part before 3.1 is very hard to understand. I think I understand the motivation of giving a big picture overview, but the way its done, it has several terms and descriptions which without introducing HAT first becomes very confusing. I would suggest using a simpler language there and provide the overview keeping in mind readers who are not familiar with HAT\n2. Marking the best numbers in other columns (other than the first) in Tables 2,3 and 4 would be helpful.\n",
            "summary_of_the_review": "Important and interesting problem. The contribution seems incremental for an ICLR publication.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this work, the authors present PRM (partially relaxed masks), which consists of an extension of HAT that allows the modification of certain parameters to encourage knowledge transfer. To find these groups of parameters, the authors use a task-similarity function based on a clustering technique. The similarity between tasks is based on the task embeddings generated by HAT. If the tasks are different, the modification of those weights is inhibited. On the other hand, if the tasks are similar, the model attempts to modify those weights. They show similar results to previous methods, but in less computational time.",
            "main_review": "Strengths:\n- The paper attacks a very relevant problem in Continual Learning: how we can train the weights to improve the positive transfer between tasks.\n- Despite having some writing errors, the paper is well structured, presenting the ideas in an orderly manner to motivate the reader.\n- I like the idea of the 2 phases of training. Why do you think that it got better accuracy w/o these 2 stages of optimization in some cases (Table 5)?\n\nComments and Questions to the authors:\n- Why use a clustering method? Have you tried using other techniques, like similarity metrics (cosine similarity)? Instead of a binary selection of the tasks (similar/dissimilar), have you tried a continuous space where the more similar is weighted by 0, and the more dissimilar is weighted by 1?\n- The method is based on the similarity and differences between tasks. Still, there are no experiments of how similar/dissimilar tasks are in a sequence. For example, I am not so convinced that CIFAR100 generates dissimilar tasks, as mentioned by the authors. Maybe 5-Dataset [1] can be a more proper dissimilar baseline. It would be helpful to know how different the tasks are to confirm if the more different tasks are indeed selected.\n- The results do not show a significant improvement compared with HAT. Is selecting a dissimilar task in Eq 5 the right choice? Does a random selection of tasks obtain similar or worst results?\n- Why does HAT have higher CT than PRM? I understand that PRM has 2 optimization stages, and the first one is very similar to HAT.\n- Section 4.2 mentioned that it is expected that HAT performed poorly in the experiment with similar tasks. However, only the results of #4 support this hypothesis. Why do you think it happens in #3 (Table 3)? \n- I would expect that NCL has a lower Acc than other Continual Learning methods, but in some cases is higher than state-of-the-art methods. Do you also apply the 2 stage optimization mechanism (training the classifier and then the whole model) in this method? Or how do you explain these results?\n- Something not evaluated in the experiments, but maybe interesting, is the variation of the parameters when adding LwPRM. You postulate that by doing LwPRM, you allow the updating of specific parameters, but how much do these parameters change (is it significant?)? What is the percentage of parameters that change during this training stage, compared with using HAT?\n\n[1] Sayna Ebrahimi, Franziska Meier, Roberto Calandra, Trevor Darrell, and Marcus Rohrbach. Adversarial Continual Learning. In Proc. of ECCV, 2020.",
            "summary_of_the_review": "This paper has potential. Focusing on methods that encourage positive knowledge transfer between tasks is not something that is done much in Continual Learning, but I think it is good to research in this line. Despite this, the modification is only incremental to the original method (HAT), and the improvements are not well-founded. The main hypothesis is that allowing the update of certain weights can facilitate the transfer of knowledge. However, the results do not support this idea. I leave some questions that I hope the authors can solve.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}