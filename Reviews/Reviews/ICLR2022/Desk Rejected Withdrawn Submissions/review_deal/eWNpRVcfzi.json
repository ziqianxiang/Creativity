{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents an algorithm for the recently proposed \"deployment constrained reinforcement learning\" setting, which iterates between online data collection and offline training phases. The authors present a new technique for estimating model uncertainty, then using it to guide both model training and data collection. Their demonstrate improved results over the state of the art on five RL baseline environments. They also give some theoretical results, giving some conditions on which their technique is equivalent to maximizing the lower bound of the true value function.",
            "main_review": "Strengths:\n- Exposition is good and the problem setting is interesting.\n- Their new method (MURO) achieves a non-trivial improvement over the state of the art.\n- Experimental results are convincing, including ablation analysis.\n- Authors give some theoretical insights along with the new aspect of their method\n\nWeaknesses:\n- The structure of MURO is largely similar to the state of the art (BREMEN) (Matsushima et al. 2020), using an ensemble of networks to model the environment, using fictitious rollout and TRPO as a base for training. The novelty of MURO seems mostly concentrated in the uncertainty estimation provided by the ensemble of probabilistic NNs. But the work is well enough supported by the theory and results that this is probably ok. What would really help is more description of the difference between MURO vs. BREMEN -- e.g., what was the deficiency in BREMEN that led you to choose to explicitly model uncertainty to deal with that deficiency?\n- One of the evaluations in the experiments is about \"novelty of new data collected\" and shows their method maximizes this metric. However, in many real-world RL settings, it could be dangerous to randomly (or preferentially) explore states we have not seen before, e.g., in medical domains. It would be interesting for the authors to comment on how safe exploration could be built in to their method, and what the tradeoffs would be between data efficiency/safety/optimality.\n",
            "summary_of_the_review": "I recommend marginally above accept because the experiments are convincing and the method well-enough supported, but the advance over the state of the art methodologically seems somewhat small (although backed by new theory), and the deficiencies of SOTA that the authors address are not clearly described. Basically, we see via experiments that the authors' method wins, but we do not have a great understanding as to why. Improving this understanding would improve the quality of the paper.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents Model-based Uncertainty Regularized batch Optimization (MURO), a deployment constrained RL algorithm. For each deployment, MURO learns deterministic and probabilistic transition dynamics models, where the deterministic transition model is used for generating fictitious trajectories, and the stochastic transition model is used for uncertainty-labeler. Then, the policy is optimized via TRPO using the generated trajectories, which are weighted by the uncertainty-labeler. During the data-collection step, it is encouraged to explore the uncertain regions that have large prediction errors. In the experiments, MURO outperforms the baseline algorithms.\n",
            "main_review": "I think providing a deployment-efficient RL algorithm would be a good contribution to the community. MURO presents a natural improvement over BREMEN by adopting uncertainty-weighted policy updates during the offline training and the uncertainty-seeking data collection strategy. However, I think the paper is weak in presentation in its current form, and major improvement is needed for a clear understanding of the reader (especially in the mathematical notations).\n\n1. Above (A2), $D(\\cdot, \\cdot)$ is defined as the discrepancy between two transition models. However, in (A2), D is used as in the form of $D(M, \\pi)$, rather than $D(M, \\hat{M})$, which is very confusing. What is the exact definition of $D_{\\pi_{ref},\\delta}(\\hat M, \\pi)$? Similarly, what does $D_{\\pi_{ref}}(\\hat{M}, \\pi)$ stand for? What is the difference between $D_{\\pi_{ref},\\delta}(\\hat M, \\pi)$ and $D_{\\pi_{ref}}(\\hat M, \\pi)$?\n2. In (A4), $f$ is used without further definition or an example.\n3. In Eq (1), $V_{\\hat M^{(i)}}^\\pi$ itself is a \"function\" that maps from a state to a real value, not a scalar value. Then, what does $|V_{\\hat M^{(i+1)}}^\\pi - V_{\\hat M^{(i)}}^\\pi|$ mean?\n4. In Eq (2), $U_{\\hat M^{(i)}, M^*}(s,a)$ is NOT being defined for each $s,a$. Rather, it only defines the expected value. How is $U_{\\hat M^{(i)}, M^*}(s,a)$ defined for each $s,a$? Also, in the definition of $g(x)$, $V_{\\hat M^{(i)}}^\\pi$ is being used, which is a \"function\" rather than a scalar value, thus it is very confusing how it should be understood.\n5. Similarly, does Eq (3) mean the point-wise inequality? i.e. $V_{M^*}^\\pi(s) \\ge V_{\\hat M^{(i)}}^\\pi(s) E[U(s,a)]$ for all $s$? or how should it be understood?\n6. In Remark 1, given that $U_{\\hat M^{(i)}, M^*}(s,a)$ was not defined for each $(s,a)$ currently (it was instead defined only in terms of expectation in Eq 2), how could we understand that $U_{\\hat M^{(i)}, M^*}(s,a)$ is a scalar between 0 and 1?\n7. In page 5, given that $\\hat P_\\phi$ is an ensembles of \"probabilistic\" neural networks, it should not be defined as $\\hat P_{\\phi}: S \\times A \\rightarrow S$. Instead, it might be $\\hat P_{\\phi}: S \\times A \\rightarrow \\Delta(S)$.\n8.  $\\hat U$ is sometimes used like $\\hat U(s,a)$ and then used like $\\hat U(a, s)$, which is inconsistent.\n9. Below Eq 5, I would use $\\hat s_{t+1}^{(a)} \\textcolor{red}{\\sim} \\hat P_{\\phi}^{(a)}(s_t, a_t)$, rather than $s_{t+1}^{(a)} \\textcolor{red}{=} \\hat P_{\\phi}^{(a)}(s_t, a_t)$, to denote sampling explicitly.\n10. Why are both the deterministic network $\\hat M$ and the stochastic network $\\hat P$ used separately? Couldn't we make an algorithm using only one kind of network for simplicity?\n11. The described data-collection procedure is a bit confusing to me. We need to determine an exploration noise $\\zeta(a,s)$ *before* we take an action in the actual environment. However, in Eq 13, $\\zeta(a,s)$ is computed using $s_{t+1}$, which could be only obtained by taking action $a$ in the real environment. Could you elaborate more on this?\n12. For Table 1, why is cosine distance used instead of Euclidean distance? I thought that Euclidean distance could be more natural since cosine distance is invariant to the scaling of vectors while the magnitude of each state dimension would also be important in control tasks.\n13. In Figure 4c, what is the definition of energy distance?\n14. In the experiments, it would be great to see the result of the online MBRL algorithm, MBPO (Janner et al. 2019) besides MOPO as well since deployment constrained RL lies between online RL and offline RL. Comparison with COMBO (Yu et al. 2021), which significantly outperforms MOPO, would further strengthen the empirical evaluation.\n15. It seems that MURO optimizes the policy via pessimism in the face of uncertainty, while it collects the new data (i.e. interacts with the environment) via optimism in the face of uncertainty. However, I am not fully convinced that this strategy is well-suited to the deployment-constrained RL setting. We aim to optimize the policy that performs well during the real environment interaction. However, if we take exploratory actions in the real environment (i.e. data-collection phase), reward performance might be sacrificed, which would be undesirable. It would be great to see the reward performance obtained by the exploratory policy (i.e. the noisy policy's performance) described in Appendix E.\n\n",
            "summary_of_the_review": "I think the presentation of the paper should be significantly improved to be published. There are many confusing parts in its current form.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a novel deployment constrained RL method, which uses offline data to pretrain and online data to do fine-turning. An uncertainty estimator is adopted to get the model estimation error to weight the advantage and guide exploration. Experiments results show the good performance compared to SOTA methods on MuJOCO environments.",
            "main_review": "The paper is interesting and well-written. The method section is easy to follow and propose an uncertainty-weighted update method based on TRPO to adapt the setting of deployment constrained RL. The proposed method is novel and interesting. However, I still have some concerns listed as follows.\n\n1. For the uncertainty estimator, what is the environment in the ablation study on Appendix G? For Eq.13 and 16, how do you get $s_{t+1}$ during exploration in the online environment (important!)? Since the bonus of exploration should be get before action decision at s_t, the true $s_{t+1}$ cannot be used in Eq.13.\n2. For the training of TRPO, how do you get the initial state for $\\hat{\\tau}$ in Eq.5? What is $\\pi_{\\theta_k}$ in Eq.6? \n3. For the experiments, how do you get the offline training data? What are seeds of different scenarios and how do you get them? Which RL library are you used to implement algorithms? Is the code available online?\n4. For the baselines, I do not agree that you can ignore algorithms like TRPO, BCQ, BRAC by the reason that BREMEN has been compared with them. Since the change of GPU, CPU, version of packages (MuJOCO, python, tensorflow or pytorch), and codes all influence the results, it is necessary to compare with them in the apper.\n5. As we all know, offline RL aims at address the cost or loss of exploration and training online in some scenarios like online recommendation. However, the proposed method explores according to uncertainty leading to huge loss in deployment possibly, which is somehow contrary to the purpose of offline RL. Is there any experiment or data to disprove this?\n",
            "summary_of_the_review": "Overall, my concerns are\n1.\tThe details of implement of the algorithm.\n2.\tThe details of the experiments.\n3.\tDiscussion about the potential loss of exploration. \n\nThe most important issue is how to get s_{t+1} in online deployment. I would like to improve my score if this question is answered appropriately.\n",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper tackles the deployment-constraint RL problem, proposed by Matsushima et al. (2021), where the agents learn a policy offline while minimizing the number of new-policy deployments for data-collection to the environment online. The authors propose a novel algorithm, Model-based Uncertainty Regularized batch Optimization (MURO) that consists of (1) uncertainty-weighted BC, (2) uncertainty-directed exploration, and (3) uncertainty-weighted TRPO, on top of BREMEN. MURO shows better performance compared to BREMEN, and MOPO in totally 500k and 250k transition settings.",
            "main_review": "## Strengths\n- The focus of this paper, deployment-constrained RL seems to be important for real-world applications beyond current offline RL research.\n- The empirical comparison shows the best performance of MURO, compared to BREMEN and MOPO.\n\n## Weaknesses\n- While this paper provides the theoretical analysis first and then its practical instantiation, I'm not sure how this analysis is different from Luo et al. (2018). As far as I compared, the assumption listed as (A1)-(A4) seems the same as (R1)-(R3) and (4.1).\n- Also, Lemma 1 seems the same as Lemma 4.1 in Luo et al. (2018). While the authors consider the deployment-constrained settings, offline update + a small number of data-collection, such conditions aren't considered properly. It can be reduced to standard model-based settings when we change the ratio of policy update and data collection. I think Lemma 1 holds in standard settings, and Lemma 4.1 in Luo et al. (2018) also holds in deployment-constrained settings.\n- Proposition 1 seems to be the result of the deformation of Lemma 4.3 in Luo et al. (2018). I'm not sure what is a new claim.\n- The definition of $d(\\cdot, \\cdot)$ (the closeness of the two policies) and $D(\\cdot, \\cdot)$ (the discrepancy between model and oracle dynamics) sounds loosely defined. Can the authors provide bounds when choosing specific $d$ or $D$, as Luo et al. (2018) describes the case of KL divergence (Proposition 4.2 in Luo et al. 2018). \n- The development of MURO seems incremental on top of BREMEN; MURO added the uncertainty-regularizations to BREMEN, inspired by other offline model-based algorithms like Kidambi et al. (2020) or Yu et al. (2020). For example, weighted-BC to improve the quality of the policy has been actively studied in offline RL (Cang et al. 2021, Peng et al. 2019, Siegel et al. 2020). Also, similar uncertainty-based exploration has been explored in model-based literature (Ball et al. 2020). I think the novelty of the proposed approach might be limited.\n- While the authors say the proposed approximation of Uncertainty-Labeler based on the model discrepancy (Eq 5) is a good approximation of Eq. 2 (supported with Appendix G), I think it largely depends on the choice of $\\alpha$ ($\\alpha$ is chosen by grid search from \\{0.28, 0.028, 0.0028\\}). Also, there doesn't seem to have any mathematical derivation to $\\exp(\\cdot)$ form.\n\n### Minor Comments\n- In Section 3, The authors say \"MURO explore the under-represented region to maximize the information gain\". I'm not sure the proposed method is related to information gain. Can you describe the connection of your exploration methods (in Appendix E) to the other information-gain-based exploration methods, such as VIME (Houthooft et al. 2016)?\n- In (A4), what is $f(\\cdot)$?\n- I'm not sure why PNN for Uncertainty-Labeler is additionally introduced. Isn't it enough to have extra deterministic model ensembles that don't use for policy optimization, to avoid error propagation? And in contrast, why is PNN not used for all dynamics models?\n- Conclusion section seems incomplete.\n\n### Reference\nMatsushima et al. Deployment-Efficient Reinforcement Learning via Model-Based Offline Optimization. International Conference on Learning Representations (2021).\n\nLuo et al. Algorithmic Framework for Model-based Deep Reinforcement Learning with Theoretical Guarantees. International Conference on Learning Representations (2018).\n\nCang et al. Behavioral Priors and Dynamics Models: Improving Performance and Domain Transfer in Offline RL. ArXiv preprint arXiv:2106.09119 (2021).\n\nPeng et al. Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning. ArXiv preprint arXiv:1910.00177 (2019).\n\nSiegel et al. Keep Doing What Worked: Behavioral Modelling Priors for Offline Reinforcement Learning. International Conference on Learning Representations (2020).\n\nKidambi et al. MOReL: Model-based Offline Reinforcement Learning. Advances in Neural Information Processing Systems (2020).\n\nYu et al. MOPO: Model-based Offline Policy Optimization. Advances in Neural Information Processing Systems (2020).\n\nBall et al. Ready Policy One: World Building Through Active Learning. International Conference on Machine Learning (2020).\n\nHouthooft et al. VIME: Variational Information Maximizing Exploration. ArXiv preprint arXiv:1605.09674 (2016).\n",
            "summary_of_the_review": "As I described in Main Review, this paper seems to rely on well-known theoretical analysis by Luo et al. (2018) (the novelty for deployment constraint settings is unclear), and to propose the incremental algorithms on top of BREMEN, combining uncertainty regularization inspired by other offline model-based methods (MOPO, MOReL). Due to the lack of novelty and significant contributions to the ICLR (or RL) community, I vote for rejection.",
            "correctness": "2: Several of the paperâ€™s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}