{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors show that the confident examples selected with the small classification loss on noisy data could be class imbalanced and inaccurate. To solve these issues, this paper has proposed a sample selection method for learning with label noise. In particular, the confident examples are selected according to the estimated clean class posterior obtained by exploiting the transition matrix.  ",
            "main_review": "This paper is well motivated. Firstly, the authors have discussed issues for existing sample selection methods from both empirical and theoretical points of view. Then, a simple and general sample selection method is proposed to solve these issues. Empirically, the quality of selected confident examples has been clear improved by using the proposed method.\n\nHere are some questions:\n\n- In section 2, the authors show that when the noise is asymmetric, the small-loss selection criteria can have a selection bias shown in Theorem 1. It is not clearly explained that if we reduce a multi-class dataset with pair flip noise to several binary classification datasets, will the noise type still be asymmetric for each binary classification dataset?\n- To select confidence examples according to estimated clean class posteriors, the authors use the entropy function. Compared to other metrics such as using infinity norm of clean class posteriors, what is the advantage of using entropy?\n",
            "summary_of_the_review": "This paper has a strong motivation. Using a transition matrix to build risk-consistent has been well studied. However, to the best of my knowledge, this is the first paper to use a transition matrix for sample selection. This simple but novel idea has addressed the issues of existing sample selection methods.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper tackles an important problem of robust deep learning with noisy labels. The authors of the paper leverage two previously proposed techniques for a unified approach to tackle the challenging problem. Specially, previous work has demonstrated the effectiveness of sample selection based on the small loss criteria. However, the authors of the paper show that naively using the small loss criteria can lead to bias in selection in the case of asymmetric noise models. To solve this issue, the authors of the paper propose to first train the model with transition-matrix based loss correction method before sample selection. The authors of the paper demonstrate experimentally the effectiveness of the proposed method. \n",
            "main_review": "Strengths \n- The proposed method is well motivated and technically sound. \n- The paper is well written and easy to follow in general. \n- Thorough empirical experiments were conducted, with good comparison against other benchmark methods. \n- The empirical result seems to be strong, especially in the case of the asymmetric noise model. \n\nWeakness \n- The main weakness of the paper is the lack of novelty. While the combination of the previously proposed sample selection and loss correction approaches are well motivated with theoretical insights, personally I don’t find it very surprising that combining the two approaches can lead to better performance. \n- Based on my understanding, the proposed method involves a two-stage training process. It is not clear to me how well the proposed method compares to other methods that involve two-stage training. For example, [1] is one such similar method. Moreover, personally, I feel that it can be unfair to compare the proposed method with previous methods like co-teaching which select samples based on predictions generated on the fly during training. \n\nQuestions\n- Entropy of predictions is used as the sample selection criteria in this paper. Is this better than other potential metrics? \n- Based on my understanding, the main motivation for using loss correction before doing sample selection is the bias caused by using small-loss criteria when the noise in the dataset is asymmetric. However, empirical results seem to indicate that the proposed method works better in the case of symmetric loss as well. Why is this the case? \n\n[1] Tanaka, Daiki, et al. \"Joint optimization framework for learning with noisy labels.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.\n",
            "summary_of_the_review": "Despite the overall good quality, I personally feel that the lack of novelty is a significant weakness. Moreover, I feel that the authors of the paper lack comparison against alternative two-stage training methods. As such, I think the paper is marginally below the acceptance threshold.  \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a simple calibration mechanism for sample selection and label correction by leveraging the noise transistion matrix. The theoretical analysis reveals the class imbalance issue which will hurt the performance of small loss-based sample selection methods. The authors find that the proposed framework exhibits excellent performance in the quality of label correction especially for large noise rate. This paper is well-written and clearly describes approach, methods, and findings. The proposed method is quite simple and general, and I think this paper could be a good starting point for rethinking the relationship for different streams of label noise methods, and future developments on more effective algorithm by unifying these two paradigms.",
            "main_review": "This paper is well-motivated, and easy to follow. The authors provide an intuitive explanation for why the proposed calibration is sensible, and give a nice proof sketch in the supplementary material. I have minor concerns for the authors to address.\n\n1. The literature review could be better. Recent studies for understanding the label noise should be properly cited [Ref 1], and other streams of methods for label noise should also be properly described, e.g., label smoothing [Ref 2].\n\n2. Considering the performance gain is relatively marginal on the large-scale real-word noisy dataset Clothing1M, the standard deviation of error should also be repoted see the effectiveness of the gain. Also, could the authors explain in detail that why the performance gain is marginal on Clothing1M. Is that because the low noise rate on this dataset? Could the authors provide additional quantitative analysis about the noise rate?\n\n3. The sample selection method used in the proposed framework is quite simple. Combining state-of-the-art label correction methods in the proposed framework would be interesting. Providing the additional empricial studies of it on the real dataset would make the experimental results more solid.\n\n\nClarity:\n1. In Figure 1, \"Circles denote instances with clean positive labels, and triangles denote instances with clean\nnegative labels...\". do you mean positive labels and negative labels, rather than clean p/n labels.\n2. Page 8, Noise Types. \"It is worth to mention...\" --> \"It si worth mentioning\".\n3. Subsection 5.0.1 --> \"5.0.1\" is weired, better to delete the subsection title.\n4. 5.0.1, second paragraph, \"small-loss based method\" --> \"small loss-based method\".\n\n\n\n[1] Liu Y. Understanding instance-level label noise: Disparate impacts and treatments[C]//International Conference on Machine Learning. PMLR, 2021: 6725-6735.\n\n[2] Lukasik M, Bhojanapalli S, Menon A, et al. Does label smoothing mitigate label noise?[C]//International Conference on Machine Learning. PMLR, 2020: 6448-6458.",
            "summary_of_the_review": "This paper adopted the label transition matrix to label correction and sample selection, and provided some informative empirical experiment results. With such interesting trials, the reviewer expected to see the concerns are well addressed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a new method for learning with noisy labels. The key idea of the proposed method is to select the trustable samples and correct their labels, on which the network is then trained. Although such a strategy has been commonly adopted by many existing works, its empirical performance is usually not satisfactory. To handle this issue, the proposed method borrows the strategy of loss correction with noise transition matrix and is able to better estimate the clean label posterior, thereby leading to more accurate sample selection and label correction. Experimental results on synthetic datasets and one real-world noisy dataset show that the proposed method achieves better performance on sample selection.\n\nThe technical contributions of this paper include:\n1. This paper theoretically analyzes the property of the sample selection methods based on the small loss heuristics.\n2. A new method for learning with noisy labels is developed. This method introduces the noise transition matrix into the procedure of sample selection and label correction, leading to a calibrated procedure and improved accuracy.",
            "main_review": "The strengths of the paper:\n1. This paper theoretically analyzes the potential weakness of the previous sample selection methods based on the small loss heuristics. This analysis is novel and intuitive, and is helpful in understanding the motivation of this paper.\n2. This paper develops a new method for sample selection and label correction. Instead of relying on the heuristic of small loss on noisy data, this paper proposes to use the transition matrix to better estimate the clean class posterior, thereby giving higher sample selection quality and label correction accuracy.\n\nThe major weaknesses of the paper:\n1. The theoretical analysis on the property of small loss heuristics is not convincing. In Theorem 1, it is assumed that the label noise is asymmetric, a situation which may not always hold. Also, Theorem 1 does not take into account the hard samples. In particular, when the dataset contains hard samples, the conclusion of Theorem 1 may not hold. This is because when a class contains many hard samples (with clean labels), even if it has a lower noise rate, the examples belonging to this class will still be less likely to be selected as confident samples. As a result, this makes **Theorem 1 not very sound and informative** in practice since real-world datasets usually contain hard samples.\n2. The proposed method is a hybrid of two existing paradigms: (1) sample selection and label correction; (2) loss correction with noise transition matrix. This makes its technical novelty limited. In particular, it simply borrows the method of transition matrix estimation proposed in (Li et al, 2021), and employs it to detect clean samples. In this process, one technical novelty is that the entropy of the posterior distribution is used to measure the cleanliness of the sample. However, this novelty is not significant.\n3. In Table 1, it can be observed that the proposed method only achieves **marginal performance gain** for **symmetric noise**. This is as expected, since the method of transition matrix estimation in (Li et al, 2021) works under the sufficiently scattered assumption. Similar phenomenon can also be observed in Table 2, where the proposed method only marginally improves upon DivideMix. This shows the proposed method is less effective on instance-dependent noise. Finally, it is unclear if the proposed method would still achieve reasonable performance under high symmetric noise rates (e.g., Sym-80%). It is suggested to provide some additional results under such noise settings, as widely adopted in many previous methods.\n4. It is unclear if the proposed method is sensitive to the hyperparameter $\\beta$ in Equation (7). Additional experiments are needed. Also, what is the $S$ in Equation (7)?\n5. From the description of the paper, it is unclear if the proposed method works in an iterative manner. More specifically, after re-labeling the selected samples, will these samples be used for retraining the network? Also, in the experiments, I wonder if it means after training the network for 80 epochs (for CIFAR), the trained network is then used to relabel the selected samples? Or does it mean the relabeling happens at every epoch?\n6. To demonstrate the superiority of the proposed sample selection method, it is suggested to also compare with some other recent works along this direction, such as SELF (Duc Tam Nguyen, 2019).\n\nSome other less significant weaknesses of the paper:\n1. In Figure 1, the illustrative examples are drawn manually, rather than from real experimental results. It would be better and more convincing to use results from real experiments.\n2. In the experiments, only the sample selection quality is demonstrated (see Figure 2). However, no results regarding the label correction accuracy are provided.\n3. For the experiments on Clothing-1M, the accuracy of the proposed method is computed on the 14K clean validation data. However, some the baseline methods in Table 2 evaluate the performance on the 10K clean testing data (e.g., T-Revision). As a result, this makes the numbers in Table 2 not comparable.\n4. What is DMI in Table 1 and 2?\n5. In the paper, it is claimed that the proposed method “is disentangled with the label noise”. However, I find this claim misleading to some extent, since the proposed method becomes less effective under symmetric noise and high noise rates.\n6. In the related works, this paper ignores some existing works that have theoretical guarantees on the consistency of the learned classifier (e.g., Zheng et al. Error-bounded correction of noisy labels. ICML 2020).\n7. There are many typos in the paper. Also, this paper should be reformatted to comply with the ICLR standard. That is, the paper margin is smaller than the standard format.",
            "summary_of_the_review": "The motivation of this paper is convincing, i.e., to unify the two paradigms of learning with noisy labels. More specifically, it is shown that by utilizing the transition matrix, we can better select the samples and correct the noisy labels. However, such a strategy based on the transition matrix also brings some weaknesses. In particular, the estimation of noise transition matrix is non-trivial, and still relies on some assumptions. In this paper, the adopted method in (Li et al, 2021) assumes the posterior to be sufficiently scattered. As a result, this method is less effective under symmetric noise and instance-dependent noise. Also, such a hybrid method is not significant in terms of technical novelty.\n\nIn addition, the theoretical analysis in the paper is not strong. In particular, Theorem 1 does not take into account the clean hard samples, making it not very useful. Also, some claims made in the paper are not accurate and appear misleading to some extent.\n\nFinally, more experimental evaluations are needed to better demonstrate the advantages of the proposed method. Also, the writing of this paper should be further improved. In particular, some descriptions are unclear and need further clarification.\n\nThe above considerations lead to my current recommendation.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper attempts to bridge the two main streams of learning from noisy labels: (1) sample selection and (2) loss correction with noise transition matrix. Since the clean examples are typically chosen based on the noisy class posterior, it can be improved by using the 'estimated' clean class posterior. The two main observations details the weaknesses of existing sample selection approaches under the asymmetric label noise. By combining the two approaches in each direction (i.e., golden loss correction and small-loss trick), the paper shows the merger of them can show much higher robustness in synthetic or real-world noisy benchmark datasets.",
            "main_review": "This paper is well written and easy to understand. The main claim from the paper is very simple - the combination of sample selection and loss correction with label transition matrix can make a synergistic effect in robust learning. In particular, the two observations for sample selection are interesting - the imbalanced and inaccurate selection under the asymmetric noise; also, this foundation is theoretically justified. But it is limited to the simple binary classification scenarios. I have several critical concerns about this paper.\n\n**Mismatch between main claim and justification.** The most interesting part I feel in this paper is the imbalanced example selection under asymmetric noise, where the noise rate varies across data classes. That is, more examples are selected from the class with a smaller noise rate. However, to my knowledge, the pair flip (or pair noise) is only a special case of asymmetric noise. It has a strong restriction in label flipping; the true label is flipped into \"a\" certain label. Thus, all the classes have either a $\\tau$ noise rate or a $1-\\tau$ noise rate. This scenario is very different from the general asymmetric scenario. To justify the main claim, the authors need to consider the scenario of each class having a different noise rate - this is the real asymmetric noise setup. For example, we can inject different noise rates into different classes (not the binary manner of pair flipping). Similarly, the theoretical analysis is also the case of pair flipping since it is binary classification. Therefore, the experimental and theoretical justification is not that convincing.\n\n**Missing analysis on imbalanced selection.** To what degree does the imbalance selection happen? and What is the negative influence from them? It is hard to know the answer to these two questions from the paper. The main experiment of this paper only shows the best (or maybe last) accuracy in test or validation data. In addition, the analysis of selection performance in Figure 2 is not convincing to me because the experimental setting is very ambiguous. The performance should be compared using the selected set of the existing approach (in Table 2). The baseline looks like a simple approach using a naive small-loss trick. Please make it clear 'what method'  the authors used to train the model and 'how to quantify' the amount of selected set (I know, each method quantifies the number of small-loss examples to select by using their individual criterion, e.g., using a known noise rate or GMMs fitted to the training loss.)\n\n**Missing analysis on the proposed threshold $\\beta$.**  As a sample selection approach, the use of a proper selection threshold is very important for high performance. However, I feel hard to find the analysis on that. This is a hyperparameter, which needs to be carefully tuned. If this threshold (the entropy basis) is very high, too many noise examples are misclassified as clean (although using re-labeling). Can the author include the discussion on this newly introduced hyperparameter?\n",
            "summary_of_the_review": "This paper addresses a very important challenge and observes a new problem of sample selection - the imbalanced selection under the asymmetric noise scenario. However, overall, I feel that there are several critical issues as mentioned earlier. Therefore, I think the authors need to add a more detailed discussion and more clear justification for their main claim. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}