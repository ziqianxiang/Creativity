{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper introduces a new method for robustness transfer. The paper starts with the observation that the standard KD can not effectively distill robustness and therefore can give a false sense of robustness. To mitigate this issue, they propose to align the input gradients of students with the robust teacher. For this purpose, KDIGA extends KD as follows: \n\n$\n\\arg \\min_{f^s} \\mathcal{L}_{IGA} =   \\lambda_{CE} \\mathcal{L}_{CE}  +  \\lambda_{KL} \\mathcal{L}_{KL} + \\lambda_{IGA}  distance(\\nabla_x \\mathcal{L}_{CE}^S, \\nabla_x \\mathcal{L}_{CE}^T),\n$\n\nwhere S and T represent student and teacher respectively. \n\nUnder local linearity assumptions, it is also shown that the gap between the adversarial loss of teacher and student can be bounded with the Local Linearity measure of both teachers and students and alignment of teacher and student input gradients.\n\nApart from the standard experiments to show robustness transfer, a few interesting experiments are performed to show different properties of KDIGA. A few interesting observations are as follows: KDIGA works for the normal pre-trained students, and adversarial robustness can be transferred between CNNs and ViTs.",
            "main_review": "**Strengths**\n1. Well written and easy to follow. \n2. Concretely shows how KD may not always be enough for robustness distillation, although [1] has also mentioned this aspect of KD. \n3. Have theoretical guarantees for the method. \n4. Interesting insights about robustness transfer. \n\n**Weaknesses**\n1. The input gradient alignment for robustness transfer has already been proposed in IGAM [2]. Experimental comparison with IGAM [2] and how this method is different from IGAM should be provided. \n\n2. It is mentioned that KDIGA makes achieving adversarial robustness \"much more computationally efficient\". Both Fast AT [3] and ARD [2] also computes one additional input gradient per example. On the other hand, KDIGA computes input gradient for both teacher and student. Therefore, it can be argued that KDIGA may be less efficient compared to both ARD, Adv-ARD [1], and FastAT [3]. An explanation and training time (even if on a few epochs) should be provided. I\n\n3. An ablation study to understand the effect of KD, ARD, and gradient alignment under standard settings can explain the contribution of each part more clearly.\n\n4. Experimental results are hard to compare with standard adversarial training results. This is important because it can tell us how well the robustness is transferred. Moreover, most of the results are reported with non-standard $\\epsilon$, e.g., ImageNet 0.01/1 which translates to 2.55/255.  These results are also not compared with appropriate baselines e.g. Adv ARD or Adversarial Training. For instance, Table 2, 3 reports results for ImageNet with $epsilon=0.01/1=2.55/255$ whereas Adversarial training results in other papers are with $epsilon \\in \\{2, 4\\}$ and only compares it with KD. Please provide a clear comparison with appropriate baselines. \n\n\n**References**\n\n[1] Adversarially Robust Distillation in AAAI 2020. \n\n[2] What it Thinks is Important is Important: Robustness Transfers through Input Gradients in CVPR 2020 as an Oral.\n\n[3] Fast is better than free: Revisiting adversarial training in ICLR 2020. ",
            "summary_of_the_review": "This paper discusses how and when robustness transfers via knowledge distillation. A new input gradient alignment method with theoretical guarantees is proposed. However, several issues need to be resolved. First, it is not clear how this method is different from IGAM [2]. Second, the efficiency claim is not supported with results and from the outset, it may not be true. Finally, a few ablation studies to understand the effect of IGA and a comparison of KDIGA with adversarial training, FastAT, KD, Adv KD, and IGAM can clearly show the contribution of this paper. \n\nI am willing to change my score based on the answers provided by the authors. \n\n**References**\n\n[1] Adversarially Robust Distillation in AAAI 2020. \n\n[2] What it Thinks is Important is Important: Robustness Transfers through Input Gradients in CVPR 2020 as an Oral.\n\n[3] Fast is better than free: Revisiting adversarial training in ICLR 2020. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies how and when the adversarial robustness can be transferred from a teacher model to a student model in KD. The authors shows that standard KD fails to preserve adversarial robustness, and propose KD with input gradient alignment to remedy this issue. Experiments are conducted on ImageNet and CIFAR10 to validate the proposed method.",
            "main_review": "Pros:\nThe paper is overall well written and easy to follow. The problem the authors focus on is interesting and important. Experimental results also show some positive results supporting the work.\n\nCons:\nThe proposed method is somewhat too simple to be a new method for publication in top conferences like ICLR. Although the authors provide some theoretical analyses, some intuitive explanations are still missing, which makes the motivation of the work not so clear.\n\nThe proposed method is very similar to attention transfer [1] in spirit. The authors should make comparisons and clarify the differences between them. In addition, more KD methods should be evaluated and compared to fully validate the superiority of the proposed method.\n\n[1] Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer\n",
            "summary_of_the_review": "As the work is limited in novelty and some important experiments seems missing, I vote for rejection.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper provides a comprehensive study on how and when can adversrial robutstness transfer from the teacher model to student model in knowledge distillation. A gradient alignment technique is introduced to address the adversarial robustness issue. And theoretical analysis is given on the 'when' issue. Experimental results validate the presented analysis using CNNs and vision transformers. ",
            "main_review": "Strength: 1. the how and when problems are important issues for the knowledge distillation. 2. The paper shows that adversarial robustness can be transferred between fundamentally different architectures\nwith KDIGA. 3. Proof is given that that the student model distilled with KDIGA can achieve at least the same certified robustness as the teacher with some mild assumptions.\n\nWeakness: Source code is better to evaluate the presented anaysis. ",
            "summary_of_the_review": "Deep analysis is given in the paper that how and when  an adversarial robustness transfer from the teacher model to student model in knowledge distillation. The contributions of the paper are clear. I would like to see the souce code for further evaluation.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}