{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper combines Gaussian processes (GP) and Conditional neural processes (CNP) for a task efficient meta-learning. They also design a learning curriculum by using a third neural network so that at the beginning of the training, the GP is mainly responsible for making prediction and as the training progresses, the new predictions are gradually made by the CNP module. The proposed approach is evaluated on two synthetic online regression tasks and when compared to the baselines (GP and other NP variants), the model achieves a better performance. ",
            "main_review": "The work is a nice attempt towards task-efficient meta-learning. However, the experimental setup and the current results lack a quality in demonstrating the effectiveness of the proposed method. Specially, the authors focused only on toy regression tasks. It would be nice to see some results on other tasks, such as few-shot classification benchmarks. I am also curious how well simple fine-tuning methods work on those benchmarks. The authors reported the performance-vs-training steps and the final results, and showed that the model can indeed smoothly interpolate between the components as it receives more tasks. However, this is not clear indicator of overall task efficiency. I think it will be helpful to define addition metrics (i.e. number of task or examples to achieve certain performance) that are more directly related to the objective of the paper and use this metrics to show that the proposed method is indeed better than baselines.\n\nFinally, I am not sure I have fully understand the motivation behind the usage of the gating network. Can you simply train both GP and CNP at the same time and switch between them once CNP has seen enough tasks? Or can a simple trainable scalar weight suffice the gating network? \n\nI think the work can clearly benefit from better problem definition and motivating discussions. Currently, the authors mainly discuss about data efficiency in training deep neural networks. Perhaps, it is indirectly related to task efficiency of meta-learning methods but thus far this relation is not clear. It would be helpful to precisely focus on the task efficiency issue in meta-learning to give definition and motivate the work. I found it a bit confusing to read the manuscript from one section to another.  ",
            "summary_of_the_review": "Please see comments below.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper deals with ensembling on shallow and deep neural networks in regression task. They claim that shallow networks are robust and quickly converging while deep networks show powerful performance in high data regime. Based on this motivation, they leverage an embedding-layer based gating mechanism that controls the contribution of shallow and deep neural networks that grants higher weight on shallow network under low-data regime or deep network under high-data regime. ",
            "main_review": "Although the proposed method is simple, it's still nice motivation that we should be able to leverage only the benefits of shallow and deep networks via automatic ensembling. \nThe main claim is that the suggested ensemble model can distinguish situations where the number of given data is huge or not, while the experiment in Section 5.1 and Figure 3 only claims about number of iterations. Shouldn't there be an evaluation with varying the number of training data to support the main claim? Correct me if I'm wrong.",
            "summary_of_the_review": "I provide weak reject because the main claim is not resolved as I understood, but I am more than willing to higher the score if this concern is resolved.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes AnyTime Neural Processes (ATNP), which uses a gate network to adaptively combine the predictions of a CNP and GP estimator, to perform well in both the small- and large-data regimes.",
            "main_review": "Strengths\n- To the best of my knowledge, using a gating network to interpolate between the small- and large-data regimes is novel and seems to have further applications in meta-learning.\n\nWeaknesses and Questions\n- The scope claimed in the abstract, Figure 1, and the initial part of the introduction is much broader than what the paper does. According to the story, this paper combines the benefits of DNNs and SNNs, but the proposed ATNP model only considers CNPs and GPs in a meta-learning setup. It is unclear whether the proposed strategy generally works for standard large and small NNs, and I think this should be reflected in the overall narrative of the paper.\n- Figure 3~4: if the x-axis involves thousands of learning steps, it would have been better if the paper measured the metrics more frequently to show a clearer picture of the metrics throughout training.\n- Figure 3(a) shows that modifying the GP during training results in oscillations in performance. So then, why not just fix the GP hyperparameters and train only the CNP and gate network?\n- Given the gate network convergence result, I don't understand why ATNPs outperform CNP/ANP. If the final result is that the gating network only focuses on the CNP network, why is its final performance better than training only the CNP?\n\nMinor comments\n- Table 1~2: Showing variance in a separate column feels like a strange choice. I think most papers do something like $x \\pm y$",
            "summary_of_the_review": "The paper presents a novel idea: using a gating network to combine the benefits of a CNP and GP. However, I disagree with the overall framing of the paper and see issues with the experimental evaluation. Please refer to my main review for details.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes an ensemble method that combines the predictive distribution of a GP with that of a Conditional Neural Process, using a gate network expected to increase the contribution of the latter as training goes through. The method seems to me as a warm-up approach for the training of Conditional Neural Processes where the GP guides during early stages of the training the CNP. The paper is validated on a rather simple, limiting scenario. ",
            "main_review": "- First and foremost, the paper is a bit poorly written and presented. Authors could have used the Supplementary Material to avoid having such small figures and images that hinder the paper’s readability. The writing and motivation of the paper seem confusing and contradictory in some parts (see below) and the literature review misses some important recent advances in the domain of Neural Processes, with a list containing, at least, the following references:\n\nWilson et al.: Deep Kernel Learning. arXiv 2015.\nKorshunova et al.: BRUNO: A Deep recurrent model for exchangeable data. NeurIPS 2018\nMa et al.: Variational Implicit Processes. ICML 2019\nGordon et al.: Convolutional Conditional Neural Processes. ICLR 2019\nFoong et al.: Meta-learning stationary stochastic process prediction with convolutional neural processes. NeurIPS 2020\n\nBelow I summarize my main concerns with the paper:\n\n- “CNPs can be considered as a DNN-based method that learns approximations to GPs”. This is not true, as CNPs and NPs considers families of exchangeable processes, whereas GPs model more flexible distributions. Indeed, GPs become exchangeable only when the non-diagonal parameters of the kernel are set to a constant. See Foong et al. or Korshunova et al. Also, a CNP and an NP also differ in a crucial aspect, which is that the latter can model output dependencies, whereas the former cannot (i.e. it assumes the outputs i.i.d.). GPs produce consistent non i.i.d. outputs when a batch of samples is set to be predicted together. In my opinion, the formalisms in the paper need to be revisited, clarified, and better explained. \n\n- “As more training data become available, the performance of the proposed method improves significantly, as with DNN-based methods”. Where is this shown in the paper? \n\n- It is said that a target goal is to “transition from Bayesian methods to DNNs sometime during training”, because Bayesian methods face challenges in high-data regimes and hence using Bayesian methods at early training (during low data regime) is better. However, it is later said that “CNPs are trained on many different functions, whereas GPs are usually trained on observations from a single function”. I might be missing something but I find the first part contradictory with the second part. CNPs and family are often used in a meta-learning setting (and are typically tested in such a way) where low-data is available. Then, how is that in the paper is it expected that the GPs (meant to work, resources provided, on large-scale data) are to give way to CNPs (meant to work in a meta-learning kind of scenario) in a transition from “low-data regime to high-data regime”? Please do address. \n\n- I believe the paper is not clearly written and can benefit from a profound revisiting. The goal in the paper seems to train a CNP with a robust warm-up GP-based approach (it is presumed and tested that the weight values for the GP expert converges to zero during the course of training). However, the paper is motivated in an expert ensemble approach which implies other assumptions that are not thoroughly tested in the paper. An experiment showing the superiority of the ensemble method w.r.t. to alternatives for the cases where only few data is available is necessary. In the paper there is episodic training with thousands of examples, making the gate converge to the expected values. However, to motivate the paper as an ensemble of experts there should a performance evaluation when the weights of the experts haven’t converged to the optimal values (i.e. when few data only is available). Also, the paper is formulated in a generic way with a possible number of K experts, with the softmax function being used to approximate the weights after the gate network. However, only two experts are used in this paper, one for the GP and one for the CNP or variant. Why not combining more experts? What would it be expected to happen in such case (does it even make sense?). The paper is driven in a very confusing way. \n\n- I find it hard to fit the Discussion subsection with the rest of the paper. What is the insight? Is it proven somehow with Definition 1 that there “will” be convergence? To me it sounds like a chattery section that does not contribute to the paper in any sense. What are the conclusions that are expected to be derived from the paragraph starting by “Different training regimes of CNPs and GPs”? Please elaborate. \n\n- The paper sticks to the early versions of Neural Processes (Garnelo et al. 2018). However, newer methods have appeared that might require discussion and comparison, such as the Convolutional Conditional Neural Processes (Gordon et al. 2019) or its latent-variable extension (Foong et al. 2020). Other older methods aiming at tackling the problem of combining the power of GPs with that of Deep Neural Nets is that of Wilson et al. 2015, where the parameters of the GP kernel are estimated by a Deep Neural Network. I believe that these scenarios must be discussed and compared with in the paper.\n\n- The experiments are far insufficient of what is expected for an ICLR paper, mainly due the following two reasons:\n\n1.It is mentioned in the paper several times that meta-learning approaches suffer from the problem of defining tasks that are similar to the one expected to solve. However, it seems that in this paper it is not experimentally shown that the proposed method bypasses this issue. A simple study would consist of testing the proposed method on a different kernel-based GP oracle, for instance. Otherwise, the method cannot be casted as “learning NPs on the fly”. \n\n2. The comparison with state of the art methods is insufficient. The qualitative evaluation of the 2-D regression task is rather disappointing (authors could have used the supplementary material to show a larger number of high-quality samples). There is also no performance evaluation w.r.t. a different number of context points. \n\n\nIn summary, I believe that the paper lacks the expected quality for this venue. I would recommend authors to fully address its drawbacks and perhaps approach the paper from the warm-up perspective (just my point of view). \n\n",
            "summary_of_the_review": "I believe the paper does not hold the necessary standards in technical contribution, presentation, and experiments, to be accepted at ICLR at this point. The motivation should be better presented with a proper evaluation. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}