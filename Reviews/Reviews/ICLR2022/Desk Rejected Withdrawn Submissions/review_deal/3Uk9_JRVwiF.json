{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The present method is an add-on to an adversarially trained classifier where the present method denoises/purifies the adversarial inputs. The method has shown efficacies when it is used together with PixelDefend (another purifier) method. The methodology seems fast.",
            "main_review": "Good idea but the presentation is not good enough:\n==============================================\nStrengths\n----------------------------------------------------------------------------------\n1. The overall idea seems light-weight w.r.t the other SOTA mentioned in the paper.\n2. Presentation is fairly well.\n3. Summary of the existing purifiers is shown well. \n\n\nWeaknesses\n----------------------------------------------------------------------------------\n1. The Discriminator, D(.): How are we fixing the discriminator architecture, how many seeds are considered to report the result. The proper justification behind the input to the discriminator is not seen. The discriminator considers *only* the layers but one can argue why not the input too? \n2. Comparison w.r.t. baseline: The reviewer finds that the comparison with the baseline SOAP is not very fair. SOAP considers different SOTA architectures to show their efficacies but the present method doesn't - am I missing something?\n3. AVmixup procedure: it is not very clear what is the mixup ratio, i.e. the value of $u$? What is the range of $u$, is there any effect on the result if we take different values of $u$?\n4. It is evident that the 15th layer by itself can perform well, so why is the methodology considering the 1st layer as an input to $D(.)$ is not very clear. The initial layer of CNN happens to capture edge-like information (generic in nature). Seems like if we consider *only* the 15th layer, i.e. that is the most informative layer IMHO, the present method has very little difference with SOAP-like methods. \n5. Table 3. It appears to the reviewer that the compared methods are a bit old SOTA. Any reason why not any very recent method is not compared. ",
            "summary_of_the_review": "The papers lag a comparison with the recent SOTAs, most of the compared methods are a bit old. The consideration of 1st layer in $D(.)$ is not well motivated and seems like the 1st layer doesn't make any huge difference. Without the 1st layer, the present method is yet another SOAP method.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a plug-in to improve the model robustness against adversarial attacks. Specifically, different from previous generative purifiers, the proposed method put two layers of features as the input of the discriminator. In addition, an AVmixup data augmentation strategy is proposed to train the discriminator. The experimental results illustrate the effectiveness of the proposed method.",
            "main_review": "Strengths:\n\n(1) This paper is well-motivated, the proposed method makes sense to me, and the results are reasonably good.\n\n(2) This paper is well-written and easy to understand. I could quickly understand the proposed idea.\n\n(3) The experiments are done on several base methods, and in most cases, the proposed method could improve the results.\n\nWeaknesses:\n\n(1) I suppose Table 3 is under the white-box attack setting. For this table, I suggest reporting the performance under the different numbers of attack steps (eg, 20 steps, 50 steps, 100 steps, or even more, see [a] for detailed test settings) for the PGD attacks. As far as I know, this method could further show the robustness of the defenses. \n\n[a] Xie, Cihang, et al. \"Feature denoising for improving adversarial robustness.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.\n\n(2) I am also wondering if there are more details about how to select two layers of the features as the input of the discriminaot.",
            "summary_of_the_review": "This paper proposes a reasonable and interesting method, conducts comprehensive experiments to illustrate the effectiveness of the proposed method.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors propose a \"purification\" scheme to help boost adversarial robustness. Similarly, to concurrent papers like the work by Qian et al. (\"Improving model robustness with latent distribution locally and globally\", 2021) and Mao et al. (\"Adversarial attacks are reversible with natural supervision\", 2021), the authors propose to wrap an underlying model with an optimization procedure that \"repairs\" the inputs given to the underlying model. The method consists of training a network to discriminate between clean and adversarial images (using intermediate activations of the underlying model) and use that discriminator to alter the input during inference to make it \"cleaner\". The \"repairing\" process acts as a reverse adversarial attack.\n",
            "main_review": "Overall, the paper is well-written and the text is easy to follow. However, my main concern is that\n1) the experimental results are not thorough enough to demonstrate that the method is robust, and\n2) a local evaluation of the models on CIFAR-10 did not yield the expected results.\n\nA) Except for SVHN, AID-purifier seems to provide almost no benefits (for the Madry networks, CIFAR-10 goes from 51.64 to 52.65, CIFAR-100 goes from 25.09 to 26.34 and TinyImageNet goes from 20.74 to 21.15). Given the cost of running this inference (10x slowdown for 10 steps of purification), this warrants a discussion.\n\nB) The gap on SVHN seems to come mainly from the difference in clean accuracy (67.39% with purification and 89.20% with). It would be good for the author to explain how AID-purifier improves clean accuracy this drastically on SVHN (the clean accuracy of the Madry trained network seems extremely low). \n\nC) Under AutoAttack, the SVHN Madry network only drops to 64.36% compared to 49.85% for the evaluation of Table 3. This suggests that more restarts or using more diverse attacks can decrease robustness drastically. Could the gradients be obfuscated? Can the authors plot the loss landscape or use a boundary attack (which does not depend on the model logits)?\n\nD) Finally, I evaluated the Madry et al. + AID-purifier (with 10 steps of purification) CIFAR-10 model using BPDA (Athalye et al., \"Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples\", 2018). On a single batch of 128 examples, I was able to find 5 more attackable examples in the AID-purified model (vs the non purified model). Allowing the optimization/purification to move by $\\epsilon$ can allow the attacker to move examples as far as $2\\epsilon$ away from the original input and can make such models weaker. I might have been unlucky in my evaluation, so I'm willing to discard this result, if the authors can provide more evaluations using more restarts, other black-box attacks and show the loss landscapes.",
            "summary_of_the_review": "Test-time defenses such as the one proposed in this paper are relevant to the community. However, when proposing such drastic variations, it is important to perform strong evaluations. While this paper was interesting to read, it fails to provide an accurate perspective on the proposed method. In particular, the proposed defense could weaken the defense.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}