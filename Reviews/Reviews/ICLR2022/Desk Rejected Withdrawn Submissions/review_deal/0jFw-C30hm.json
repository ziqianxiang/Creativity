{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In this manuscript, the authors claim they have come up with a new pipeline for landscape analysis of machine learning datasets that enables a better understanding of a benchmarking problem to select a diverse benchmark datasets portfolio and reduce the presence of performance assessment bias. ",
            "main_review": "In this manuscript, the authors claim they have come up with a new pipeline for landscape analysis of machine learning datasets that enables a better understanding of a benchmarking problem to select a diverse benchmark datasets portfolio and reduce the presence of performance assessment bias. Here are some of my main concerns. 1- The manuscript is not well written. Grammatically requires great improvements. 2- The authors assessed different datasets in particular time series ones and claim they can better understand a benchmarking problem. They also claim they provide directions for a better evaluation of time series algorithms. However, I could not find how these claims have been justified in the manuscript. In addition, I do not see any new findings/ information or directions in this paper that are unknown to the experts in the field. Hence I reject this manuscript.",
            "summary_of_the_review": "Here are some of my main concerns. 1- The manuscript is not well written. Grammatically requires great improvements. 2- The authors assessed different datasets in particular time series ones and claim they can better understand a benchmarking problem. They also claim they provide directions for a better evaluation of time series algorithms. However, I could not find how these claims have been justified in the manuscript. In addition, I do not see any new findings/ information or directions in this paper that are unknown to the experts in the field. Hence I reject this manuscript.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper presents an analysis of benchmarks for time series classification. In the proposed approach, time series of all datasets are represented with a common feature description, and then clustered. The distribution of the datasets in the clusters are then used to characterize benchmarks. Comments on the landscape and some suggestions on how to create a correct set of benchmarking sets are also provided. \nThe pipeline is then applied to a set of recent approaches, deriving interesting conclusions.",
            "main_review": "Strengths:\n- The paper is well written and easy to read\n- The topic is definitely interesting\n- The structure of the manuscript is clear\n- Experiments and findings are clear and interesting\n- Choices are well motivated and described\n\n\nWeaknesses\n\nMain comments:\n1. Even if I like the paper and the ideas contained, I think that the main weakness is probably the novelty. Actually the method has a strong relation with the Instance Space Analysis and related methods. I suggest the authors to clearly state the difference with respect to them (only briefly presented at the end of sect 2), also in terms of tools and techniques employed. Moreover, I also suggest the authors to empirically compare their method with the ISA approach. What would be the result of ISA on the TSC corpus? Is there anything that the proposed approach would discover and the ISA method not? I think that adding such comparisons and descriptions is fundamental to assess the real contribution of the proposed pipeline.\n\n\n2. The first step of the proposed method characterizes each time series with a vector of features, thus reducing the time-series classification problem to a classic classification problem in vector spaces. Clearly this is one possibility when dealing with time series, but other approaches working directly with series can also be used (fully exploiting the sequential nature of the problem). Any comment on this? For example, would it be possible to characterize sequences using dissimilarity-based reasoning? Many distances have been proposed to characterize the relation between sequences, being able to better capture the nature of the problem: working with distances would also represent a substantial change with respect to ISA approaches. In this case you can use clustering methods based on distances or even embedding the distances in a vectorial spaces – many embeddings work even if distances are not metric, such as the dissimilarity-based representation paradigm:\n\nRPW Duin, E Pekalska, Dissimilarity Representation For Pattern Recognition, The: Foundations And Applications, World scientific, 2005\n\n\n\nMinor comments:\n- Please rephrase the last two sentences of the introduction as a contribution\n\n- The creation of codebooks is essentially the problem of vector quantization: many different approaches can be used, such as the advanced ones introduced for defining a dictionary in the Bag of Words representation. Please add a comment on this. \n\n- In the whole pipeline there are many different choices made by the authors (methods, parameters, and so on). Choices are very often reasonable, and in most cases well justified: however I’m always wondering how crucial is every choice, and what is going to change when changing one piece of the pipeline. Adding comments in this direction would increase the value of the work.\n",
            "summary_of_the_review": "Interesting paper dealing with the problem of benchmarking Time Series Classifications methods. The approach is interesting, even if of limited novelty. Experiments and findings are clear.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper describes a pipeline to select instances of different datasets with the aim to help researchers to reduce bias when estimating and comparing performance of different classification algorithms, with specific reference to multi-class classification problems. The paper tackles as well the problem of reproducibility which is of fundamental relevance to the research community of machine learning.",
            "main_review": "Strenghts: main strenght of the paper is the specific nature of the problem tackled. Indeed, nowadays it seems that all counting is data and thus except from associated relevant issues as explainability of classification, it is fundamental that the reader can trust the authors performed numerical experiments to show perfomance of their proposed classification algorithm in a fair manner. \nHowever, this is a difficult task for many reasons. Thus the contibution of this paper is extremely important to lay down a common ground to the research community.\n\nWeaknesses: the paper makes design choices that are described in a clear manner but it is not clear to the reader whether using different methods, for example for datasets clustering, will bring to similar or different results. It is a recipe to help researchers but still very difficult to accept it is not the only one or the best one. \n\nAt page 4, I read \"we adopt a decision-making strategy based on maximizing the amount of explained variance as a quality measure\", I kindly ask to motivate such a choice, under which assumptions does this make sense and/or it is justified?\n\nAt page 5, you mention a threshold, I think it is useful to describe how such a threshold has been set.\n\nAt page 5; concerning hypothesisi testing; I think this is a weakness of the proposed approach, you are not training on the same datasets all algorithms but you assume that the clustering you build can act as the same conditions under which the algorithm from the literature have been trained. Maybe this is legitimate under some assumptions that I kindly ask you to clearly state.\n\nAt page 5; you made the decision of 63 features methods, probably effective, but also in this case I would like to know more about the rationale for such a choice. Furthermore, the fact that some feature were not computable for some datasets also tells something about similarity between datasets, or not?\n\nWhen training SOMs, different architecture, number of neurons of te grid, it is not clear whether the learning for each setting has been performed once or many times.",
            "summary_of_the_review": "I think the paper tackles a relvant problem for the machine learning community, the way in which performance of algoritms is estimated and compared, and the issue of experiments reproducibility. However, I think that main issues, not mentioned in the paper are those of paraount importance which go under the names of tranferability and transportability.\nI think that this is a nice step to recognize the relevance of an issues that for many years has been overlooked, while running only to get performance improvement of specifc author's developed algorithm over competing ones.\nThe need for benchmarks still remains but I would kindly suggest to tackle also the issue of transferability (data shift) and that of transportability in the case when classification also deals with decision making.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}