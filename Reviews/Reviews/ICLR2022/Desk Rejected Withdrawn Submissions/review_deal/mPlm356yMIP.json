{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes to sample the output of monocular 3d object detection from a heuristic depth-dependent probability distribution rather than predicting a single estimate, to better approximate the uncertainty in depth. The authors show that, by weighting the detection confidence with the sampled probability the performance of the detector can be improved in the high recall and low precision region increasing the safety of autonomous systems. The results show significant improvement over the state-of-the-art.",
            "main_review": "The proposed method is simple and straight forward, showing a significant increase in AP/mAP performance for many baselines.\nAs the method works independent of the monocular object detection method, a evaluation on 12 different baselines is impressive, but probably not neccesary. It would have been interesting to rather investiagate different distributions and sampling strategies.\nAs figure 1 shows that sota methods are close to the theoretical lower bound (assuming 1 pixel error) below 40 m distance it would be interesting to evaluate the proposed sampling strategy for different depth intervals to see if it correlates with the theory.\nThe sampling in depth/z and reprojection to the image works well for tasks with know groundplane, how would it work in an unconstrained setting? Would one use an euclidean depth parameterization?\nIt would have been interesting to show the ablation studies to find the hyperparameters used for the distribution (lambda) and the sampling points (depth intervals and probabilities) as these have a major impact on the performance and different values have been chosen for the different datasets.\nThe PR-curve shown in figure 5 shows identical curves for high precision/low recall areas and a larger area for low precision/high recall values, but the underlying mechanisms are not discussed. How come that for higher scores (higher precision) there are not more FP due to the additonal samples? Is it due to the fact that the confidence for the additional/wrong samples is too low or is there another reason behind it? How does the PR-curve look like fore the probability based sampling?\nThis would suggest that the introduced additonal samples with low scores reduce the false negatives without increasing the false positives, which seems like a nice tradeoff, in combination with the ablation in table 6 it would be interesting to deeper investigate the hyperparameters as mentioned before and the interplay with the confidence downweighting based on the probability.\n\nFor the ablation on the number of samples in table 5 it is unclear which samples are used as in the the implemetation details only the two choices for 8 samples are given.\n\nWhile the sampling leads to an improvement of AP due to the better recall, does the sampling based evaluation make sense or is there a better way to evaluate 3d object detection taking the uncertainty and theoretical limitations into account? Wgat would be a suitable parameterization for later tasks suchs as prediction and planning?",
            "summary_of_the_review": "The idea and evaluation of the work gives an interesting insight into the depth estimation problem of monocular 3d object detection.\nWhile the proposed method is simple and the initial evaluation could be more detailed to analyse the proposed solution further, the key insight of using a different output parameterization to allow for spatial uncertainty in 3d detection can be used to design new metrics and methods to better accomodate the underlying/systematic theoretical bounds and include the uncertainty to develop methods that have better safety tradeoffs needed for e.g. autonomous robotics.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors of the paper base their work on other state-of-the-art work on monocular 3D detection and the discrepancies between predicted depth (from the detection bounding boxes) and the actual depth of the scene represented in the image (due to the difference in dimensionality between input as 2D images and output as 3D bounding boxes). From these conclusions they propose to reformulate the predicted output from bounding boxes locations and size to a continuous spatial probabilistic distribution. From their experiments they achieve 20% relative improvement of the Average precision of the main state-of-the-art methods for monocular 3D detection, by changing the output representation.\n",
            "main_review": "The paper offers a clear and well constructed explanation of the problem and proposed solutions with a good ablation study and experiment section on average. The paper identifies correctly the challenges coming from the dimensionality gap between images and 3D bounding boxes/depth prediction. Those challenges are clearly identified and quantified in the introduction in particular Figure 1, with a proposed theoretical lower bound (defined in Sec.3.2).\nThe two main contributions in this new output contribution are clearly defined in Sec.4.2 and Sec.4.3. The experiment section is mostly well organised and complete, with a good ablation study of the proposed method added to the baseline (CaDDN Reading et al. (2021)).\n\nHowever several weaknesses arise from the approach and claimed contributions as well as some sections of the paper:\n- Section 3.1 doesn't seem very important in the narrative and the importance of  predicted depth accuracy has already been explained in multiple previous work for 3D Monocular detection. Perhaps this could be moved to the related work section, with a more critical analysis of previous work. For example , how have previous papers tried to alleviate the dimensionality problem between images and depth, such as with the use of birds eye view representation? (for example CaDDN Reading et al. (2021)). And how it differs from what this work is proposing.\n\n- In Equation 2 there seem to be an error, as z12/fyccannot be greater than z12/(fyc-z1) (unless z1 is negative).\n\n- After defining the transform of the discrete prediction to continuous and their sampling, the authors do not explain how it is used in the state-of-the-art methods. Are all the sampled predictions added to the loss function ? Is some kind of non maximal suppression used ? Also during testing, are all the predictions sampled kept ? \nAll these possibilities change greatly the embedding of such output to the state-of-the art methods used, and it is not clear in the current paper.\n\n- One important thing is lacking from the experiment section, and that is visual results compared to the state-of-the-art methods supposedly improved upon using this new output representation. Very impressive improvements are announced but it is difficult to assess how the proposed method impacts the predictions compared to the baselines they compare to without visual support: are the bounding boxes for greater depth distances more accurate around the groundtruth? Are their shifts in depth less important than with other methods?\nThe authors compare also to LPCG Peng et al. (2021) , which is on its own a training framework added on top of other monocular methods. It is unclear how the author embedded the new output representation to this framework, and what improvements were brought to the predicted bounding boxes.\n\n- Mean average precision is reported for all results tables, but it would be good to have the IoU metrics as well, to reflect more the overlap of predicted bounding boxes, especially as the authors claim improving on large depth error and imprecise detection predictions.\nIn Figure 5, it seems the method does not improve on precision compared to other methods but rather only on recall. With such constatation, it seems that the proposed method basically just provides for each prediction (in previous method), 3 to 6 (if one or both sampling methods are used) new bounding boxes. Is there any non maximum suppression step used on these multiple new predictions ? or are the metrics computed on all of them ?\n\n- Finally, if the sampling method used based on depth and probability, ends up just providing more bounding boxes for each bounding box provided by the method the authors build upon. Then could a comparison with a simpler way of doing it be possible ? Such as adding randomly X bounding boxes around the predicted one, with a jitter more important based on the depth similarly to what is proposed in the paper ?\n",
            "summary_of_the_review": "While the challenges and methods are well explained, there are some explanations lacking about how the proposed new output format is embedded in previous state-of-the-art methods. This has consequences on the understanding of the contributions, and impact to the improvements claimed. Furthermore, more detailed experiments (with more precise and meaningful metrics and visual examples of the improvements), would improve the understanding of the contributions and methods. Finally some small typos, and some errors in equations (eg. eq.2), makes the explanations a bit unclear. \n",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new output representation for monocular 3D object detection. This paper address the problems of the conventional 3D bounding box representation, which is discrete 2D representation. So, this paper reformulates output representation with a spatial probability distribution. The authors show that the new representation can be easily applied to the conventional 3D object detectors while it increases the performance by a large margin. The performance on the far depth range is dominantly improved. The paper provides an extensive ablation study.",
            "main_review": "- Strength\n\nThe problem that this paper address makes sense, and it is a very interesting and important issue.\n\nThe proposed method alleviates the errors caused by the discrete 2D representation.\n\nIt improves the performance in a large margin for most of the state-of-the-art methods. The results are impressive.\n\n- Weakness\n\nThe detail of the spatial probability distribution is missing. After sampling and obtaining probability, how to obtain the final output value from the samples and probability. Is it the weighted sum of the sample depth points and the probability?\n\nI am curious about the derivation of theoretical depth error lower bound. Please see the questions about Equations 1 and 2 below.\n\nThe author attempts to explain why the 2D representation for 3D OD is not adequate. I also agree that the 2D representation cannot be the optimal representation for 3D tasks. But the author does not \"clearly explain why\" the dimension gap produces the error. The author says that since the depth error exists on the 2D representation. But why the depth error occurs on the 2D representation? Then, the proposed method can alleviate the error on single image depth estimation as well? Then it should be validated with the monocular depth estimation tasks with the state of the art methods.\n\n- Question\n\n1. Why should compute the depth error from the vertical pixel offset? If the problem is the stereo matching that measures the disparity between two images, such as left and right, then a 1-pixel disparity error can cause that much depth error according to equation 1. However, this work handles monocular images, not stereo. So, I could not understand why 1 vertical pixel offset decides the depth error lower bound?\n2. What is c in equation 2? c_y? I guess the authors represent (f_y*y_2*z_1-f_y*y_1*z_2) as f_y*c*(z_1-z_2). How is the earlier equation converted to the later one?\n3. Q=AP in the 5th line from the bottom on page 4 should be written sQ=AP (s=scale) or zQ=AP (z=depth). Since the author states P is 3D points.\n4. Is the spatial probability distribution used in both training and test time? or only in the test time?",
            "summary_of_the_review": "The authors address the interesting and important problem. And, the authors propose a simple yet effective solution. The dense ablation study is provided. However, more details on the derivation of depth error bound should be provided. More details about the proposed method should be provided. It is hard to figure out how to implement this work only based on the submitted paper. If the authors address the issues well that I pointed out, I am willing to increase the rating.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper seeks to consider the depth information for improving the accuracy of monocular 3D object detection, by modeling the error of depth estimates as spatial probability distribution. The proposed sampling strategy for 3D boxes with the spatial probability distribution improves the detection performance, especially the recall, as shown in their experiments.",
            "main_review": "Strengths:\n- simple approach to apply to other existing 3D detection mechanisms\n- extensive experiments to show improvements of detection accuracy when the proposed approach is used for existing detectors\n- clear writing\n\nWeaknesses:\n- a few missing related works\n  : please refer to existing works on monocular SLAM and 3D geometry, especially for modeling uncertainties of depths from monocular video as normal distributions. (e.g. Yi Ma - Monocular SLAM, Andrew Zisserman - 3D Geometry)\n- not mentioning computational cost of their proposed approach\n  : it is important for having the detection speed fast enough for using real-time autonomous vehicles as they emphasize for the use case.",
            "summary_of_the_review": "The paper proposes a simple reformulation of monocular 3D detection problem to consider the uncertainties in depths as spatial probability distribution. The approach showed improved detection accuracy over various existing 3D detection algorithms through extensive experiments. The concept of considering uncertainties in depths has been researched in monocular SLAM and 3D geometry fields, so the idea is not completely new. However, by using the concept to recent monocular 3D detection, they showed empirical results improved over many existing detectors.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper advocates to use distributions with two proposed sampling strategies as the representation of 3D object detection results. The underlying idea is accurate depth estimation is very hard for monocular camera. Using a set of sampled locations from a distribution is better than a single location. The proposed representation has been applied to some existing methods. According to the experimental results, it improves the evaluation metrics over KITTI and Waymo datasets.",
            "main_review": "Strengths:\n- The distribution representation can be applied to all 3D object detection methods.\n\nWeaknesses:\n- Using distributions for predictions with high uncertainties is a straightforward idea, which has been widely adopted at different places. It's hard to be claimed as a novel contribution to the field.\n- It's not clear how the sampled values are used in evaluation. Are all samples considered in computing the metrics, or the best one is selected for a prediction?\n- The paper didn't explain how the sampled locations will be used by downstream components in e.g. autonomous driving systems.\n- The dimension gap of monocular 3D detection is exaggerated in the paper. For many applications, e.g. autonomous driving, and mobile devices, there are additional sensors to help measuring 3D, e.g. IMU. Even without additional sensors, technologies like SLAM and visual odometer can help.\n- In Equation (2), it should be z1-z2, instead of z2-z1.",
            "summary_of_the_review": "The paper presents a simple idea that is widely used in many places. How the sampled positions are used is not explained in the paper.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}