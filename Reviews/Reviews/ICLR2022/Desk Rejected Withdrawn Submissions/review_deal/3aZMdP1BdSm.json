{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a novel algorithm for finding Decision Trees using Monte-Carlo Tree Search.\nThe experimental results on two domains show superior performance to other baselines.\n",
            "main_review": "Strengths.\n- Interesting problem formulation.\n- The algorithm guarantees the convergence to the optimal.\n\n\nWeaknesses.\n- There is a recent work that uses MCTS for Learning Decision Trees.\nThe related work section should compare or discuss this work.\n  https://ieeexplore.ieee.org/document/8614095\n- The comparison is limited.\nThe algorithms are compared for two domains only, and I think the TYGEM problem is probably not a good testbed.\nFrom the viewpoint of an amateur Go player, I could not understand the TYGEM results.\nMaybe this is too difficult for all tested algorithms.\n",
            "summary_of_the_review": "As I described in the weaknesses, I have a concern about the experiments and the novelty.\nIf these are resolved, I am happy to raise my score.\n\nMinor comments.\n- It is just about the notation, but why \"s\" is written in lower case (\"MCTs\")? It usually is all capitals (\"MCTS\").\n- There are several punctuation errors. Such as extra spaces after \"Fig.\" and no blank before a parenthesis. Possibly LaTeX problems.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I did not find any ethical concerns.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper introduces a tree-based algorithm to identify interactions between categorical predictors in empirical risk minimization. The algorithm uses a tree induction + pruning approach to explore the set of all possible interactions – with the goal of assigning zero-weights to certain interactions by optimizing an L1-penalized loss function. The paper includes several results to characterize the behavior and optimality of the algorithm as well as an empirical evaluation of the proposed method against baseline techniques to mine interactions. The latter section shows that the proposed method can choose interactions in ways that reduce test loss and improve test AUC on ([2 datasets] x [3 model classes]) ([Avazu, TYGEM] * [LR, FM, fmFM]).",
            "main_review": "## Strengths\n\n- [Contributions] New algorithm for an interesting problem\n\n## Weaknesses\n\n- [Major] Evaluation: The empirical evaluation of the proposed algorithm is lacking in several ways (only 2 datasets, few baselines, and a limited number of metrics used for evaluation). In light of the methodological focus of this work, the paper would benefit considerably from a more comprehensive evaluation.  Specifically: (1) evaluate the algorithm on a synthetic dataset with ground truth (easy to do); (2) include a comparison of the proposed methodology against \"no interaction terms\" and \"1-interaction term\"; (3) report measures on training data as well as test data (I understand that test performance is the ultimate goal, but your algorithmic guarantees should be on training data)\n\n- [Major] Exposition: The paper is very text heavy with convoluted notation (i.e., $S_{n^d_{i,j}}$) and lots of theorem environments (without context). The exposition would be greatly improved if the authors were to: (1) introduce tree-based notation with a Figure (possible Figure 1); (2) simplify their notation (e.g., drop subscripts that are not required and define filtering functions for 'node-specific datasets'); (3) add some guiding text to key results (e.g., what is the meaning of Assumption 1).\n\n- [Minor] Scope : Paper is singularly focused on optimizing average loss. This misses an opportunity to leverage the fact that the tree-based approach could be used to handle other criteria (e.g., group fairness). \n\n",
            "summary_of_the_review": "See above.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "\nA method based on decision trees (DT) is presented for data with only categorical predictors. The idea is to obtain interpretable interactions in the model. The interpretability is a side-effect of decision trees. The idea is much like random forests dealing with categorical variables. But a key difference is that the proposed DT cannot repeat a predictor in its path. The DT nodes are given by predictors or the categorical levels of the parent node predictor. Leafs are given by the categorical levels of the parent node predictor. The prediction is given by weights associated with leaf nodes. Leaf nodes are fitted with a penalized least squares. The penalty is based on a lasso penalty on the weights. A key ingredient is the setup a dedicated measure for tree splitting. In this manuscript, this is based on Oracle like bounds. These bounds are estimated with data. The manuscript claims that their method converges to an optimal solution. Proofs or tentative proofs of the claims are given in an appendix. A safe-screen lasso is used in order not to have spurious weight parameters associated with no data or with too few data. The manuscript claims that the safe-screening makes their method much faster than a simple lasso-penalized least-squares on the ensemble of DTs. The prediction is achieved with an ensemble method constructed from several DTs. These trees are obtained through bootstrap of the data at hand. Experiments show that the proposed method works well, and sometimes better than alternative methods.\n",
            "main_review": "\nStrengths.\n--------------\nA key ingredient is the setup a dedicated measure for tree splitting. In this manuscript, this is based on Oracle like bounds. These bounds are estimated with data. The manuscript claims that their method converges to an optimal solution. Proofs of the claims in the main document are given in an appendix.\n\nA safe-screen lasso is used in order not to have spurious weight parameters associated with no data or with too few data. The manuscript claims that the safe-screening makes their method much faster than a simple lasso-penalized least-squares on the ensemble of decision trees.\n\nThe prediction is achieved with an ensemble method constructed from several decision trees. These trees are obtained through bootstrap of the data at hand. \n\nExperiments show that the proposed method works well, and sometimes better than alternative methods.\n \nWeaknesses (issues).\n------------------------------\nAlthough, the manuscript claims that the safe-screening makes their method much faster than a simple lasso-penalized least-squares on the ensemble of DTs, probably the reason of most of the speed-up is the changing of the original problem (1) to a simpler one (2) that separates the DTs in order to find optimal trees. Note that problem (2) is not the same as problem (1). In problem (2) each tree has to fit the target variable. In Problem (1) the ensemble trees has to fit the target variable; so some trees might fit well different parts of the target. In problem (2) the trees are all doing similar work. Problem (1) is more similar to boosting, while problem (2), is more similar to random forests.\n\nThe manuscript claims that their method converges to an optimal solution, but it is not clear to what probability space the statements are referring to. For example, the statement in the staement of Theorem 5: What does it mean $\\lim_{t\\rightarrow \\infty} P( \\hat{T}^*_t = T^* ) = 1$? I guess this means $T^*_t$ converges in probability to $T^*$. In which probability space?\n\nTheorems 1 and 4 are key to the proposed method. These are about the safe-screen lasso. Basically, this measure is equivalent to checking if the node has enough data to be able to offer a good estimate (or weight, as they are called in the manuscript). The usual decision trees have the option to stop splitting nodes if the bucket (number of data falling in the child nodes) is nearly empty. It would be interesting to compare the proposed measure for safe-screening to the old usual bucket size.\n\nIn Theorem 4, one needs to show that the conditional expectation $E_{(x,y) \\sim D}( y | S_{n^d} )$ is a constant; otherwise one cannot separate the two terms. How can this be a constant when $S_{n^d}$ depends on the sample?\n\nThe Monte-Carlo part of the methods is rather a data bootstrap, just as in random forests. I noticed that in the comparison results, random forests are not used; only boosting trees is shown. I think a comparison with random forests is pertinent.\n\nResults on the two datasets do not show how the interpretability of the interactions given by the proposed model offer any gain in predicting the game. One has to go to the appendix to see how interactions help predicting moves. \n\nMinor issues.\n------------------\nThe manuscript uses names such as \"backpropagation\" to show the updating formulas. There is no need to refer to the updating formulas as 'backpropagation' since the document is not about neural networks.\n\nPages 2 to 3: The exposition on the decision trees in the document reminds me of the  work on pattern recognition of Amit & Geman (\"Shape Quantization and recognition with randomized trees\", 1997) with labeled decision trees. For completeness, this work should be looked at, and at least some citations should be added to the current document. \n\nProbabilities and expectations are not defined properly at all.\n\nEquation (7) needs a proof.\n\nIn Table 1, most of the results are very similar; that is, all methods do about the same; the only exception is TYGEM with model LR. I wonder if the results are based on just one random sample of the data. Results should be obtained for several random samples of the data to make comparisons more reliable.\n\nLemma 4: The probability statement is empty. The event is not well defined (perhaps a missing bound in the event).\n\nLemma 1: (i) Notation is very cumbersome. (ii) The lemma assumes that the indicators $1_{i, n^d_{\\theta, \\tau} }$ are independent random variables. Please state this, and explains why this is the case. (iii) What are the functions $1_{i, n^d_{\\theta, \\tau}} indicating? (iv) change 'iteraions' to 'iterations'.\n\nPage 14: change 'bernoulli' to 'Bernoulli'.\n\n",
            "summary_of_the_review": "\n\nThis is a paper that brings both theory and practice. Both aspects present issues. The exposition on theoretical statements is not very clear. It needs more effort to push the message. Some claims are not clear, and might not have correct proofs. To understand the empirical results, one has to read the appendix.\nThis is a general problem in the paper: the main document cannot exists without the appendix. But together, the appendix and the paper makes 20 pages, far more than the 9 pages allowed. In general, a manuscript with many theorems without proofs or postponed proofs to the appendix is not a complete paper. Documents are written like this so as to have space to show experimental results. But this paper also sacrifices experimental results. I do not consider this manuscript as a complete enough document without having to read the appendix. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper introduces a decision tree-based approach for identifying high-order interaction terms between categorical features, which can then be added to shallow models to improve their performance. The benefit of using decision trees vs. current work on identifying high-order interactions via neural network embeddings is that the decision trees provide some interpretability for the interactions which could be useful in applied settings.",
            "main_review": "The paper addresses an interesting and relevant problem. The use of safe-screen methods from statistics for augmenting the feature representation of shallow models is a nice contribution. However, I found key aspects of the paper to be very unclear, and did not understand the motivation for much of what is developed in the paper. Perhaps other readers will find these aspects to be clear, but I feel like I should be able to understand the paper and largely did not, so I think improving clarity and motivation of the paper will be important for other readers. The empirical evaluation also addressed only the issue of accuracy, and did not touch on much of the stated contributions of the paper around computational performance (e.g. the use of safe-screen), and interpretability and stability (the need for DTs).\n\n**Clarity**:\nI had a hard time following the technical content in the paper. A couple early examples:\n\n* Notation is generally difficult to follow. For instance, l_T and l^T_x refer to entirely different quantities (one is an index, the other is a number of elements). Similarly, x_j and x_j^{T_i} mean totally different things. Typically a subscript of superscript would not change the meaning of the quantity so much.\n\n* The paper provides no background on safe screening, so (1) really comes out of nowhere and should be given more description of what the various parts are doing. The quantity w_{down arrow} is never defined, though I was able to infer later in the page what it means. w'_{down arrow} and w''_{down arrow} are defined only mathematically, and would benefit from a text description of what they represent. L and L_s are never defined, though I can infer what they are.\n\n**Motivation**:\n* Feature selection is done by fitting a large collection of decision trees, and then using the leafs as a feature encoding and doing LASSO on a linear model with the DT forest features. Is there some justification for this procedure? I haven't seen it done before and so was a bit surprised to see a linear model fit on top of a DT forest, when one would normally use the forest directly as the predictor. Would it not be possible to do a feature selection / pruning directly of the DTs using the forest predictions as opposed to fitting a linear model? Should we expect a linear model fit to DT forest features to perform well as a predictor? If not, then it doesn't seem appropriate to me to use it for feature selection. The decision trees are implicitly doing feature selection wrt the tree prediction, and it isn't clear to me that that feature selection would be appropriate for a linear model.\n\n* The MCTs-IS procedure for selecting an optimal tree is motivated by saying \"it would be infeasible to choose the best one from a large number of candidates.\" I don't know how to square that with the fact that feature selection is done using a large number of candidates. If it was feasible to do a lasso regression on a large number of candidates, why isn't it feasible to then choose the best one from that same set? If it's because the trees used for feature selection are too far from optimal, then why do we think they would do a good job for feature selection? This aspect of the motivation was very confusing to me. It would also be helpful to show empirically that the MCTs-IS procedure is important and that a procedure operating directly on the output of the safe-screening isn't sufficient.\n\n* How big is the lasso problem being solved here? It will be n_tree * n_leaf, but it'd be helpful to provide some context for what those numbers are in order to motivate the use of safe screening. The paper states that \"the optimization problem in Eq. 1 does not have a closed-form solution, which requires a computationally expensive numerical method to minimize the loss function.\" At a conference like ICLR, minimizing regularized least squares even for very large numbers of features (i.e. neural networks) is not generally seen as an intractable problem, so some extra context here will be valuable.\n\n**Empirical evaluation**:\n\n* As described in the paper, recent work on identifying high-order interactions has focused on neural network embeddings. The empirical evaluation should compare to this recent work. Currently it only compares to GBDT, a classic but not-state-of-the-art method. The introduction describes how NNs have a downside of unstable and uninterpretable. The paper should show this empirically by comparing to an NN feature extractor on these problems and showing that the MCTs-IS results are more interepretable and more stable. Currently the paper does not provide any evidence to support the claim that MCTs-IS is more interpretable or more stable than current embedding approaches from the literature.",
            "summary_of_the_review": "The paper addresses an interesting problem but was hard to follow. I am not certain that I fully understand the method, but am also not convinced that it is necessary or useful because the empirical evaluation did not address the stated contributions or compare to state-of-the-art.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}