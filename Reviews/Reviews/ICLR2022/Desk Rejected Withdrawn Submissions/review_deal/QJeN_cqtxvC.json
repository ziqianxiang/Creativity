{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper explores a non-backprop learning algorithm for neural networks leveraging the \"multiple shooting\" method from control. The key features include: (1) local, asynchronous updates to each layer/block of the network, similar to Online Alternating Minimization by Choromanska, et al. (2019) and others; (2) the class labels are used to produce targets for each layer/block; (3) information about the upper layers trickles down to the lower layers throughout training as in Target Prop. The main contributions of this paper  Experiments are done comparing the proposed DistProp algorithm to SGD (backprop),  ",
            "main_review": "Strengths: \n- This paper explores an interesting non-backprop learning strategy.\n- This paper is written clearly.\n\nWeaknesses: \n- Online Alternating Minimization from Choromanska, et al. (2019) seems closer to this work than is described (that paper does claim to work on mini-batches). This paper claims to build upon that work by making the algorithm more stable, but there was no direct comparison. \n- I didn't think there were enough details in the experiments. It seems that the neural network is split into two parts (the convolution layers and the fully-connected layers), and the algorithm is applied to those two parts, and not every layer. Assuming this is the case, then I don't expect training to be much different from regular backprop --- ultimately the result will be similar to the stack of conv layers being trained in a supervised manner, then the dense layers being trained (i.e. stacking). I am not convinced these experiments demonstrate the effectiveness of this algorithm on big datasets. ",
            "summary_of_the_review": "While I found this paper interesting, I didn't feel like it did a good enough job at placing its contributions within the context of previous work, and experiments lacked sufficient detail.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The work presents an interesting and novel approach for optimizing a neural network. The optimization problem is reformed into a constrained one, in hope to parallelize the evaluation of each layer and stabilize the optimization process. An algorithm to allow a mini-batch version is designed. The paper presents some experimental results that shows a better convergence and accuracy performance.",
            "main_review": "I appreciate the novelty of the idea to go beyond the conventional optimization paradigm. The writing is also generally smooth. But I think the paper may not be fully ready for publication.\n\n* I still do not fully understand why bother this new formulation, as people often prefer an unconstrained form anyway.\n  - On the point to parallelize evaluation of each layer, will it be a significant problem in optimization? Is it worth the cost to optimize more parameters and the risk of more subtle convergence ($h = f(x;w)$ is made an explicit constraint which then may not be exactly achieved and incur error or convergence issues)? An experiment for comparing the performance-time curve is also expected.\n  - On the point to stabilizing the optimization process, why the mini-max optimization is better? My prior knowledge is even that it is worse. The result in Fig. 2 also seems to indicate the proposed method may maintain a large defect along the optimization procedure.\n  - On the point to make a scalable (i.e., which allows a mini-batch optimization step) constrained optimization method, some techniques are designed but not well explained. Particularly, why can't we simply implement the naive mini-batch version of GDA or ExtraGradient? Their constraint function can also be estimated using a mini-batch. What's the benefit over this naive baseline?\n  - The authors also mentioned the scalability restriction of second-order methods, but what about quasi-Newton methods and adaptive gradient methods? Empirical comparison is also expected.\n\n* Most context in the paper focuses on describing the designed algorithm, but I cannot quite understand why it is like this.\n  - \"Instead of maintaining a unique auxiliary variable and Lagrange multiplier pair per example, our algorithm tries to learn a mapping between a data point and a corresponding learned prototype.\" How does the mapping solve the constrained optimization problem? Also, learning a mapping from data point to a class-specific prototype seems already no easier than learning a classifier.\n  - The mean and variance only depends on the label $y$. Why should this happen? Instances of the same class $y$ have a certain diversity (e.g., images labeled \"human\" have different background, facial expression, body pose, etc), so do the low-level representations e.g. $h^{(0)}$. But this constraint forbids such diversity.\n  - How does the mean and variance learned? This is not explained in the algorithm or the text.\n  - \"..., but this time evaluated over a smaller set of synthetic hidden states generated by drawing $N$ samples from ...\": Why is this eligible (i.e., does not change the optimal solution, does not introduce convergence unstability) and why do we want this?\n  - How does the introduced $g(h,w)$ function at the bottom of Page 4 relate to the $g$ in Alg. 1?\n\n* Possible typos:\n  - Introduction Line 6, \"have been only been\".\n  - Page 3 Line 2: unclosed bracket g(x^k).\n  - Page 3 Line 4: \"an may otherwise\".\n  - Fig. 1 is not mentioned in the text.\n  - Fig. 2 caption: \"it's\".\n\nIn all, I hope the authors could make the algorithm more convincing for its eligibility and advantages. Concrete explanations (e.g., convergence theory or theoretical insights) supporting these points are expected.",
            "summary_of_the_review": "See above.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a method to locally update by leveraging Lagrangian multipliers. A naive way requires the number of Lagrangian multipliers proportional to the number of training points, which is infeasible for modern datasets. The authors circumvent this issue by introducing class-wise prototypes with noise. This trick enables training with mini-batches and to scale the method to a large-scale dataset, such as ImageNet.",
            "main_review": "# Strengths\n\n* This paper scales the Langrangian-based training method to large-scale problems, including ImageNet classification.\n* The authors verify the robustness of the method to hyperparameter selection and try to guarantee the reproducibility as much as possible.\n\n# Weaknesses\n\n* According to the introduction, one of the motivations of the proposed method is to train a model in a local or layer-wise manner, which is thought to be biologically plausible. However, the discussions about how such a local update is achieved and how the method is biologically plausible seem unclear to me. I think that the former may be alleviated by generalizing Algorithm 1 to arbitrary $n$ layer cases.\n* The proposed method has no theoretical guarantee that the obtained representation by the method is similar to or better than that by the standard training protocol, which is known to achieve high performance. Therefore, this needs to be supported by experiments. Yet, the manuscript only contains experiments with LeNet for MNIST and CIFAR10 and AlexNet for ImageNet. The claim should be verified on more variety of neural network architectures. Additionally, important ablation studies are missing, such as the choice of $N$, where authors say \"(we) found that $N < 8$ and often $N = 1$ is sufficient to achieve good performance.\"\n\n\n# Comments and Questions\n\n* If I understand correctly, the proposed method is named \"DistProp.\" Yet, this name is not clearly defined in the paper and suddenly appears in P6.\n* I think Table 1 shows the main results, but the main text does not explain it. Similarly, Figure 3 is also a part of the main results, but the main text lacks descriptions.\n* References should be fixed. For example, Gidel et al. 2019 in P10 is written as \"ICRL 2019 (to appear)\".\n* Table 1 shows several numbers, such as 0.9881 for test accuracy using SGD and 0.00072 for final defect using GDA. Are these precisions significant?\n* Are figures in Table 1 results of single-shot experiments, the champion values, or averaged values of several runs?\n* Belilovsky et al. 2020 \"Decoupled Greedy Learning of CNNs\" is also a layer-wise training method, but it achieves comparable performance with SGD. Löwe et al. 2019 \"Putting An End to End-to-End: Gradient-Isolated Learning of Representations\" is also related. Are these works related?\n* One of the main measures of the performance \"defect\" is not defined. Is it a common idea in the machine learning community?",
            "summary_of_the_review": "Though the idea itself is interesting, I think this paper needs to be polished more.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper considers a constrained optimization problem for DNN training. The activations of the l-th layer are considered as separate variables h_l, with conditions h_l are equal to the true activations of the DNN, and the final loss depends on the activation only via \"fake\" activations of the last layer h_L. The motivation for the constrained problem is the possibility of parallel (layer-wise) training and also breaking possibly instabilities caused by the depth of the model. The paper solves the constrained optimization problem via a variant of Lagrangian optimization. The paper demonstrates the performance of AlexNet and LeNet-style models on MNIST, CIFAR-10, and ImageNet.",
            "main_review": "**The novelty:** To the best understanding, the main novelty is the application of the ExtraGradient [Gidel et al. (2019)] (GAN-like optimization) to the constrained version of DNN training.\n\n**Clarity:** I find the paper hard to follow.\n- The paper lacks the proper structure, the main selling points (the contributions, main problems that the paper attempts to solve) are not properly defined.\n\n- There are clear statements e.g., \n\t- \"without a separate forward propagation phase\", which is unclear as the algorithm recomputes activations at some point. \n\t- \"increased training stability over a long optimization horizon by “pinning down” the unstable modes\", it is quite unclear what instabilities are meant.\n\t\n- The final algorithm presentation (Section 3.0) is confusing.\n\nThe paper demonstrtate the following **results** (top-1 acc):\n- MNIST: 0.9903 (the proposed method) vs 0.9881 (SGD)\n- CIFAR-10: 0.6540 (the proposed method) vs 0.6309 (SGD)\n- ImageNet: 0.1110 (the proposed method) vs ------ (SGD)\n\nFrom the results, it is hard to conclude that the method is able to provide a decent level of performance or can be properly scaled to the modern DNNs.\n\nThere is also no comparison with the relevant baselines e.g., \"Putting An End to End-to-End: Gradient-Isolated Learning of Representations\" https://arxiv.org/abs/1905.11786.",
            "summary_of_the_review": "1. The presentation is confusing and hard to follow.\n2. The main selling points e.g., stability are not properly shown.\n3. Results are shown of the non-conventional models that are beyond of state of the art level of performance.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}