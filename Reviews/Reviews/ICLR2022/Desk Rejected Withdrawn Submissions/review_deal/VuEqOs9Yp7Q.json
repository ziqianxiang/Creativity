{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents a simple temporal action localization method that directly predicts a global segmentation mask of the input video instead of processing proposals or anchors. Compared with existing methods, the proposed method enjoys a faster training and inference speed. Good experimental results are achieved on ActivityNetv1.3 and THUMOS14.",
            "main_review": "Pros:\n1. The overall framework is simple but seems to be effective. Good experimental results are achieved on two benchmark datasets.\n2. Extensive experiments and ablation studies are conducted. Visualization results are also provided to verify the effectiveness of self-attention.\n\nCons:\n1. It is not new to see Transformer-based methods can be used to generate action proposals or localize actions [a][b]. The main contribution of this method mainly lies in the formulation of the TAL task. This paper localizes actions by predicting a 2D temporal segmentation mask. However, converting the 1D temporal prediction problem into a 2D map prediction problem has been explored in previous works, such as BMN and [c].\n2. The losses are dice loss and bIoU loss lead to significant performance gain but both of them are adopted from image segmentation methods (from 2D to 1D). In this sense, using these losses does not provide enough insights. \n3. The authors claim that the proposed method is more effective in solving the training data challenges of TAL. However, they did not provide some theoretical or empirical analysis on this point. Could the proposed method achieve good results relying on fewer training samples?\n4. The smallest prediction unit (the temporal resolution is limited by the receptive field of a unit) is a snippet. To obtain better performance, the authors should manually determine the size of a snippet for each dataset, as shown in Table 12. More critically, this method involves too many hyper-parameters that should be carefully determined in the inference stage, making it difficult to apply this method to real-world applications. \n5. The authors did not compare their method with sota methods, such as [d]. More importantly, when using the same TS features, the proposed method brings minor improvements compared with TCANet. \n\n[a] Relaxed transformer decoders for direct action proposal generation. ICCV2021\n[b] Temporal context aggregation network for temporal action proposal refinement. CVPR2021.\n[c] Deep Concept-wise Temporal Convolutional Networks for Action Localization. ACMMM2020.\n[d] Enriching Local and Global Contexts for Temporal Action Localization. ICCV2021.",
            "summary_of_the_review": "The proposed method is simple and easy to follow. However, the overall technical contribution is not significant. Transformer-based methods have been explored in TAG or TAL tasks, this paper did not convey enough insights.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper addresses the task of temporal action localization in untrimmed videos. The authors suggest a transformer-based architecture that simultaneously predicts class-labels and segmentation masks for videos. In an empirical study, the authors compare their approach against state of the art approaches on two widely used datasets. They further provide an extensive evaluation, showing results on runtime, an ablation on model and loss components, and an analysis of types of errors produced by their model.",
            "main_review": "Throughout this review, I mark strengths and positive aspects with a \"+\" and weaknesses and negative aspects with a \"-\".\n\n(A) Novelty.\n* (+) the authors introduce a novel loss that is well motivated by addressing boundary constraints\n* the idea of a global segmentation mask is somewhat novel but comes with disadvantages (assuming fixed video length, see below)\n* (-) the way the task of temporal action localization is addressed follows the standard recipe that most approaches in the field follow: the method is driven by standard backend video features and has a classification and segmentation branch.\n* (-) architecturally, the approach is a stack of widely used standard modules: a I3D backend, a transformer, and temporal convolutions for class label and mask prediction\n\n(B) Technical Presentation\n* (+) the overview figure gives a good high-level idea of the model\n* (-) the details on the segmentation mask do not seem to be sound. There is a significant unclarity that I could not resolve from the details provided by the authors: the mask branch predicts a T x T output from T x d input using 1D convolutions. Since d is the feature dimension, the only way to go from d to T using 1D convolution is if T is a fixed value that can be the output channel size of the convolution. However, videos do have varying length, so it seems this method is limited to videos with a maximal length of T. Particularly, *any* predicted segmentation mask will be of the same size. In fact, the authors note that every video is downsampled to 100 (Thumos) or 256 (ActivityNet) snippets. This seems to be a major limitation when it comes to modeling real world video data of arbitrary length and with an arbitrary amount of action classes. The presented model design is highly tailored towards the statistics of the two datasets used for evaluation but lacks generalizability to real world applications.\n* (-) it is unclear why the predicted segmentation mask is of size T x T because in the loss (Eq. 6) only a 1 x T segmentation vector is used. How is this 1 x T vector selected from the T x T segmentation mask? Where/how are the other (T-1) x T elements of the segmentation mask M used?\n* (-) the model requires prior knowledge of the dataset statistics as the paragraph on implementation details in Section 4 reveals: multiple hyperparameters must be explicitly tuned for each dataset, depending on the video lengths and average number of action instances per video. Similarly, the inference to obtain the final segmentation requires prior knowledge about the video characteristics, as the authors state in Section 3 (\"Inference\"): \"For long actions, [description of inference mode A] .... For short actions [description of inference mode B]\".\n\n(C) Experiments\n* (+) The ablation study is extensive. The authors show that their loss formulation benefits the model performance and provide an analysis of error cases in Figure 5.\n* The results in Table 6 are inconclusive: 0.5 MAP values indicate that class label ground truth is more important than segmentation, Avg MAP indicates the opposite. What is the conclusion that can be drawn from this result?\n* (-) The authors do not compare to recent work on temporal action localization that achieves better results. For instance, \"Multi-shot Temporal Event Localization: a Benchmark\", Liu et. al., CVPR 2021 outperforms the presented approach on all MAP values on Thumos 14. Similarly, \"Temporal Context Aggregation Network for Temporal Action Proposal Refinement\", Qing et. al., CVPR 2021 report better results for Avg MAP and 0.75 overlap MAP on ActivityNet.\n* (-) the evaluation of inference time is misleading. The most expensive part should be Optical Flow + I3D forwarding for the test video. It looks like this cost is neglected in the evaluation, instead only the inference cost of the TAL module after feature extraction is reported (I might have misunderstood this, please clarify). Taking the feature extraction cost in account, the time spent on the TAL module is minor and the approach does not have significant inference runtime advantages over other methods.",
            "summary_of_the_review": "The submission unfortunately does not contribute convincing and valuable enough insights and results to the research community. The main weaknesses are:\n\n(1) Novelty: Neither algorithmic novelty nor experimental insights provide sufficient novel insights that would push the field forward. This is particularly evident considering that the results are below state of the art (see point 3).\n\n(2) Technical design: The technical section needs improvement on some unclarities (see above) and the approach relies on knowledge about dataset characteristics. Specifically, the inference algorithm depends on wether the actions are expected to be short or long.\n\n(3) Experimental results: Recent works (CVPR 2021, see above) report stronger results than the results presented in this paper. These works are neither cited nor compared against.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper addresses the task of temporal action localization by employing a novel framework inspired by Transformer architecture. The proposed method generates the global segmentation masks of all action instances and thus removes the necessity of the proposals, which was an essential part of the prior methods. The experimental results show that the proposed method works better than the prior methods.",
            "main_review": "#### Strengths\n- Removing the need for the proposals seems quite an impactful contribution of this work. Indeed, using a large number of the pre-computed proposals is exhaustive as the authors mentioned.\n- The idea of using a single global segmentation mask seems interesting and novel.\n- From the experimental results, the proposed method demonstrates remarkable gain on the benchmarks.\n- The paper contains some novel analyses which have not been conducted much from the previous works. Especially, I like the results from Table 6 & 7, which are analyses on each component. From both tables, it seems that the classification branch of the proposed framework is relatively weak and has much room for improvement. The cross-domain experiment (Table 10) is also an interesting approach. \n\nDespite the strengths, I have some concerns about the paper.\n#### Weaknesses (Concerns)\n- The claim that the proposed method (GSMT) addresses the challenge of a small number of training data is not supported well. It is well known that one of the main challenges for transformer-based frameworks like Vision Transformer or other language transformers is that they need a lot of training data. The authors only mentioned that less labeled training data leads to the need for the proposals, but I think it is not sufficient to support the claim above. Even though the GSMT removes the proposals, I think the method (or the task itself) still suffers from the small training data. It would be nice if the authors provide a more concrete discussion about this issue.\n- The authors claim that one of the advantages of the proposed GSMT is to eliminate the set prediction and instance query, which leads to a more compact design than object detection transformers like DETR. However, one of the contributions of DETR is that DETR removes the hand-designed NMS by set prediction. Indeed, the proposed GSMT requires multiple sets of thresholds and SoftNMS for the inference, which seems quite complex. So, why is eliminating the set prediction and instance query advantage?\n- For Figure 4, although that (c) demonstrates a more precise boundary than others, it seems that the foreground snippets share a more similar feature representation in (a) than (c). It will be nice if the authors add more discussion about the figure.\n- For Figure 6, why all three figures do not represent the results from the same video?\n- Table 10 demonstrates the cross-domain results for the 12 classes as mentioned in the text. In the text, the authors mentioned that the results are worse than Table 1. However, Table 1 is a result of the entire dataset, not 12 classes. Could you share the results for the 12 classes?\n\n#### Typos\n- Figure 6 (b) : gBIOU -> bIOU\n- Page 15, the last sentence : thus6being -> thus being\n\n",
            "summary_of_the_review": "I really appreciate the main contribution of this work, which is eliminating the need for proposals. In spite of the good performance and the contributions, I have some concerns mentioned above. Especially, most of my concerns are related to the claims and their supports. Therefore, I recommend rejecting the paper as my initial rating.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a Global Segmentation Mask Transformer (GSMT) model for Temporal Action Localization problem. The proposed method is based on transformer and predicts the class label directly at the snippet level. The experimental results show that GSMT outperforms existing TAL methods on two datasets. ",
            "main_review": "Strengths: \n\n1) The idea of predicting the snippet-wise class label instead of proposal-based prediction is interesting. The context-aware snippet embedding learned by the self-attention is new. \n\n2) The proposed method shows advantages on both two datasets.\n\nWeaknesses:\n\nMajor: \n\n1)\tThe statement “All existing TAL methods rely on proposal generation” in Introduction and the statement “Although all existing TAL methods use proposals, …” in Related Work are not true. Please see [1].\n\n[1] Single shot temporal action detection, ACM Multimedia 2017. \n\n2)\tThe term \"global segmentation mask\" is understandable, but ambiguous. It should be clarified, especially against the spatial 2D segmentation mask. It is suggested to use the term \"actionness\" because this term has been more recognized by the action localization community. \n\n3)\tThe novelty of the proposed method is limited. The contribution of modifying the object detection transformer to solve TAL is not that significant.\n\n4)\tIt is expected to have a discussion on the relationship between the new formulation of TAL based on snippet level prediction and the action segmentation problem in the related work since they share similar formulation.\n\nMinor:\n\n5)\tWhat are the training devices and the memory usage? The memory usage is expected to be huge due to the global self-attention.\n\n6)     There are many hyper-parameters to tune in “Inference” of Section 3.3.\n\n7)\tThe incorrect line break under Figure 4.\n\n8)     The typo \"loss an a random\" in Figure 6. \n\n",
            "summary_of_the_review": "Overall, this is an interesting paper which adapts object detection transformer to TAL problem and the experimental results show its advantages compared to previous work. But the idea is quite straightforward and not that exciting. Therefore, I recommend “Marginally above the acceptance threshold” if the weaknesses above can be addressed. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}