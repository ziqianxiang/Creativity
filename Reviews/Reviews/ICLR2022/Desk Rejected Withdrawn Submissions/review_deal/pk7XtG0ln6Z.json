{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents an incremental learning approach for object detection by performing knowledge distillation at detection head to focus on the classification and regression responses. An adaptive pseudo-label selection is proposed to improve response selection. Experiments on MS COCO demonstrate good results and close the gap to full training.   ",
            "main_review": "Strengths\n\n** Investigate knowledge distillation at detection head for incremental learning.\n\n** Improve response selection by the proposed adaptive pseudo-label selection strategy.\n\n** Good performance on MS COCO under different incremental learning settings.\n\n\nWeakness\n\n** Although this paper claims to be the first work to explore knowledge distillation at detection head for incremental learning, the overall novelty of this paper is limited as knowledge distillation at different levels has been widely studied at various domains from image classification, semantic segmentation, to object detection.   \n\n** Lack of systematic evaluation to validate the advantage of distillation at detection head. How is the approach compared to the distillation conducted at detection backbone or detection neck under the same experimental setting? What if multi-level distillations are performed when combining the different network components?\n\n** It's unclear why the thresholds (lines 8 and 19 in Algorithm 1) of the adaptive pseudo-label selection are computed in this way (i.e., the sum of mean and standard deviation). No motivation, theoretical or empirical validation is provided.\n\n** In Table 4, the proposed \"adaptive response\" is inferior to the plain \"all response\". How to determine when the incremental response in the new classes is sufficient to be safe to use the adaptive method?  \n",
            "summary_of_the_review": "This paper studies the knowledge distillation at detection head for incremental learning of object detection, and demonstrates good performance. However, due to the limited novelty and lack of justification on several important designs (as detailed above), this work is not ready for being published at ICLR.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes to use response-based knowledge distillation for tackling the incremental object detection task. The distillation for the classification and regression branches are from previous works [RILOD] and [LD]. The main valuable contribution of this paper is the adaptive pseudo-label selection (APS) which determines where to be distilled for classification and regression respectively. The experiments part shows some interesting results.",
            "main_review": "Pros: \n+ The idea of selecting response-based knowledge in the head part for classification and regression is interesting. \n+ The experimental results show some improvements compared to previous works in \nincremental object detection task.\n\nHowever, there are several concerns. \n\nCons: \n1. Limited novelty since the classification distillation and regression distillation methods are from previous works. To this end, the APS seems more interesting, therefore the paper should focus more on this part.\n\n2. Overall, the method part is not well-organized. Specifically, in sections 3.2 and 3.3, the eq.5 and eq.6 are just from [LD] without the relative context. The notations are not consistent.\n     - $C_s$ and $C_t$ in the left part are not shown in the right part of eq.4, as well as $R_s$ and $R_t$ in eq.7. So they are quite confusing.\n     - Should a $J$ be added to eq.4 to show the selected responses for classification distillation?\n\n\n3. In Algorithm 1,\n     - The notations are hard to follow. Is $G_c$ a set? In L10, where does the k come from?\n     -  $B$ should be the probability matrix of the bbox, not the bbox. (One of L160 and eq.5 is not correct.) If $B \\in R^{n \\times 4}$, what is the dimension of $G_B$? \n\t-  What is the $G_{S_b}$ in L21?\n\n\n4. The experiments are not sufficient to support the claim in the paper. \n      -  All experiments are based on one model and one teacher. It is better to show not just one model to evidence the proposed approach.\n      - The incremental experimental scenarios are weak in the paper. It will be interesting to have experiments such as with 40 classes, +5 classes for each incremental step. \n      - Because the APS is the main contribution of the paper, it would be better to have some ablation studies on it. For example, how do the thresholds affect the performance? How do different teachers perform?\n     - Usually, the classification predictions and bbox predictions from a detector are relevant. Do the authors think about using the joint set of $C'$ and $B'$ ?\n\n5. How can the proposed distillation be used if the head dimensions of the teacher and student are different?  \n\nMinor comments:\n1. What is 'xk' in Figure 1?\n\n[RILOD] Li et al., RILOD: near real-time incremental learning for object detection at the edge.\n\n[LD] Zheng et al., Localization Distillation for Object Detection.\n",
            "summary_of_the_review": "Overall, the authors should clarify their contributions properly and reorganize the paper accordingly. Besides, current experiments are not sufficient to support their claim. I am afraid this paper is not ready at this time.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a new method for incremental learning on object detection with knowledge distillation. It mainly has two contributions: Firstly, they hope the student detector can maintain its ability for localization when learning new categories. Secondly,  it proposes adaptive pseudo-label selection strategies to select distillation nodes. Experiments on COCO in three different incremental settings are conducted for evaluation. Their method outperforms previous methods by a clear margin. Detailed ablation studies have been given in discussion.",
            "main_review": "Strength.\n1. Incremental learning on object detection is a novel and not well-studied topic.\n2. Their method has achieved significant performance. Ablation studies have shown the effectiveness of different modules clearly.\n\nWeakness.\n1. As claimed by the authors, there have already been a lot of works on knowledge distillation for incremental learning and knowledge distillation for object detection. One of the key ideas (Line46-Line48) \"use the response on the location where teacher detector generates high-quality predictions as the ground truth\" is the well-known method in the study of knowledge distillation for object detection.  And their knowledge distillation on classification and regression is also not novel in the study of knowledge distillation for object detection. The design of APS is also very naive  Based on the above observations, I think this paper is a new but very simple combination between KD for object detection and incremental learning for object detection. It's not very novel.\n2. About the experiments. It will be better if experiments on more detectors can be provided.\n3. In equation 4, usually we distill the probability distributions with KL divergence. Why equation 4 uses L2 loss? In equation 6, why can incremental localization knowledge of bounding boxes be distilled to students by distilling the probability distribution? I think distilling probability distributions indicates distilling knowledge about categories, instead of localization.\n4. Line 199-200, the author asks the question \" What are the bottlenecks in our method\". However, I don't find any discussion for this question.\n5. The fonts of figure2,3,4 are too small. Please make them larger. The writing of this paper is not very bad but still should be improved.\n\n\n",
            "summary_of_the_review": "I think the main problem of this paper is the lack of novely. It's a simple combination of knowledge distillation for detection and incremental learning for detection. Besides, some sentences are confusing to me. Thus, I tend to rejct.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}