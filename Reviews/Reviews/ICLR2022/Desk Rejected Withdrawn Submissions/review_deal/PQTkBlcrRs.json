{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "Ideally, at each AutoML run time, one could produce different ensembles of deep neural networks (DNNs) regarding the desired trade-off between accuracy and inference speed.\n\nHowever, ensembles of DNNs suffer from three main limitations:\n1. There is a lack of understanding about the best way to build base DNNs to construct an ensemble.\n2. There is a lack of control of the computing cost of the produced ensemble.\n\nProposed methodology:\nHyperparameter optimization: Hyperband Li et al. (2020) is a robust hyper-parameter optimizer that is used\nto combine DNNs\nEnsemble selection: The authors propose ”Scalarized Multi-Objective with Budget Forward greedy” (SMOBF) to greedily add DNNs to ensemble based off of a weighted sum consisting of computing cost and accuracy.\nEnsemble combiner rule: Simple Average is used to combine DNN predictions from the ensemble.\n\n",
            "main_review": "There are many grammatical errors, to the point where it is difficult to understand parts of the paper.\nThere are many organizational issues, like when the author mentions that there are 3 main limitations, but never states a third limitation.\nI would recommend a major rewrite before resubmission. ",
            "summary_of_the_review": "I would tend to reject this paper based off of writing quality.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Although Automated Machine Learning (AutoML) made tremendous efficiency improvements in the last years, most approaches focus on improving the predictive quality of deep neural networks (or other models). However in many applications, computational complexity is another very important metric. Building on recent advances of constructing ensembles of well tuned ML models, the authors propose a scalarized multi-objective, weighing the predictive quality of a deep neural network (DNN) and its computational complexity. By sampling different scalarizations the authors show that they are able to identify heterogeneous architectures by running asynchronous Hyperband as an established AutoML technique. The concrete optimization of the objective is performed by an established greedy forward selection.",
            "main_review": "### Correctness (and Clarity)\n\nAlthough I will not discriminate against the authors based on the assumption that they are most likely not native English speakers (same applies to me), the language is a fair stumbling block, making it very hard to read the paper and sometimes even to understand its statements. I strongly advise to employ a native speaker for review prior to submission. This includes among others orthography and grammatics. Some sentences are fairly hard to decipher. Further, the paragraph structure leaves much to be desired, since it is not obvious what the statement of the paragraph could be. Even the count of arguments (first, second, ...) is inconsistent with the provided ones, which let’s me wonder whether parts of the text are missing. Last but not least, the authors are repetitive and redundant in several places. \n\nThe definition of the AutoML is fairly confusing for me:\n1. AutoML is not limited to a DNN search space\n2. What is the returned “one hyperparameter value”? I would expect that some kind of loss or error is returned.\n3. There is an expectation over x and y missing.\n4. Are x and y vectors or scalars?\n5. How is the validation set sampled? Cross-validation, hold-out, bootstrapping, …?\n6. AutoML will typically only return one best performing  \\lambda^*  and not all; s.t. it should be \\lambda^* \\in \\argmin\n\nThe authors motivate their work with a cost-accuracy trade-off that is supposed to limit the computation required for inference (as the cost suggests). However, I believe that they ultimately seek to find a Pareto front for this trade-off since it is often not known a priori how to trade off both criteria. Consequently the weights used in the scalarized form are arbitrary. As the authors state, the scalarization is “a convenience”; but lacks a proper justification in this case. \n\nOverall, I’m struggling to understand the whole motivation. The moment inference time gets really important, ensembles are most often prohibitively expensive because several models have to be queried. The improvement of predictive quality by using ensembles does not compensate for this sufficiently in many applications. If the authors think otherwise, I would like to ask for thorough arguments and descriptions of explicit use-cases. \n### Technical Novelty And Significance\n\nRegarding the contents of the paper; an application of scalarized multi-objective optimization as the main contribution is fairly shallow and holds little novelty even when embedded with sophisticated methods such as asynchronous hyperband. Further, the complexity notion of an ensemble is insufficient; it is the sum of the ensemble members’ time to predict 2K images. \nRegarding novelty and literature review, the authors missed, among other papers, Auto-PyTorch [Zimmer et al. 2021 IEEE TPAMI], which is already an ensemble algorithm and indeed works for DNNs. A discussion how their approach differs and how it empirically compares is missing.\n\n \n### Empirical Novelty And Significance\n\nRegarding the experiments: it would be sensible to check the diversity of the resulting ensembles from different HPO methods. However, considering the random nature of both random search and Hyperband based methods for the construction of their portfolios, it is unsurprising that these HPOs create more diverse libraries to choose from. \n\nTable 2 seems to have missing entries in C100 - No Ensembles, that are not explained in the text or the caption --- maybe only the “-” are missing?. \n\nUnfortunately, any stdev across repeated runs is missing in Table 1 -- fairly important for these kinds of highly non-deterministic algorithms. \n\nFigure 4 and 5 provide most likely the most important results; however as far as I understood it, the proposed algorithm has to run over and over again to find this Pareto front, which would not be very efficient compared to established multi-objective approaches. To summarize, the experiments provide some first intuition that the approach could be promising, but unfortunately, not very convincing.\n\nFurther minor issues:\n* please use \\citep properly\n* At the beginning of Section 3.1, the authors motivate their choice of Hyperband by experiments. That’s not very convincing at this moment since the empirical justification follows only later.\n* Table 1 and 2 are poorly formatted \n* The highlighting in Table 1 gives the impression that there is no clear winner\n* Figure 4 has no axes labels",
            "summary_of_the_review": "The novelty of the paper is fairly shallow. It gives me the impression that the authors only extended ensembling in AutoML by using a modified (regularized) cost function, roughly motivated by multi-objective optimization. The experiments are partially incomplete and the applied measures (such as an ensemble’s prediction time for a fixed amount of data or the “workflow error in %) are not too convincing.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "In this work, the authors present the method SMBOF to create an ensemble from a library of models with diverse runtimes. SMBOF greedily adds members to the ensemble by taking the objectives test loss and computing budget into account. A linear combination of both objectives is solved multiple times, each time with different weights, and the best performing ensemble is finally chosen.",
            "main_review": "The strong point of this paper is the work on a relevant problem, its weaknesses are the presentation, lack of novelty and empirical comparison.\n\nFirst, I want to question the premise that the topic of this paper is on AutoML with ensembling. I agree that the author use both in their work, however, hyperparameter optimization methods are only used to generate the library of models. The creation of the library can be done completely at random or by using expert-chosen models instead.\n\nThe authors claim that their contribution of combining AutoML with ensembling for neural networks is novel is false. The work in [1] uses hyperparameter optimization and combines trained networks to an ensemble post-hoc. This work should be discussed and possibly compared to.\n\nThis work’s only contribution is to greedily optimize for an objective that does not only take the validation loss but also the budget into account. However, a comparison to more than the most naive baseline is missing. Following the notation in Equation (2), at least a comparison to a greedy method optimizing for $score_a = E(y_a,y) + P_a$ must be conducted.\n\nThere is no comparison to any ad-hoc ensembling method. The authors should discuss this work from last year as well [2].\n\nAdditional comments:\n\nThe authors mention that they use a forward-greedy ensembling strategy and refer to the work by Cuarana 2004. This was confusing to me because while Cuarana 2004 proposes greedily adding members to the ensemble, they use other techniques such as bagged ensembling, selecting with replacement and initialization as well. It is even more confusing that Cuarana 2006 is cited which adds another technique. Therefore it is not fully clear what serves as a baseline and how exactly the proposed method works.\nIn my understanding, the authors are simply using a forward-greedy strategy without replacement and use exactly the same strategy as a baseline. The authors can improve their description by correctly describing how ensembling in Cuarana 2004 works (it is not simply a forward-greedy approach) and clearly describe what they do differently. Also, use the reference to Cuarana only in the context it exactly matches what you are doing, e.g. \"We use a greedy-forward selection (Cuarana 2004)“ leads to confusion. Maybe explicitly mention \"similar to“?\n\nI recommend to use \\citep wherever applicable to improve readability.\nI advise to spend some time on carefully checking the grammar.\n\n\n**References**\n\n[1] Lucas Zimmer, Marius Lindauer, Frank Hutter: Auto-PyTorch Tabular: Multi-Fidelity MetaLearning for Efficient and Robust AutoDL.\n\n[2] Marton Havasi, Rodolphe Jenatton, Stanislav Fort, Jeremiah Zhe Liu, Jasper Snoek, Balaji Lakshminarayanan, Andrew Mingbo Dai, Dustin Tran: Training independent subnetworks for robust prediction. ICLR 2021",
            "summary_of_the_review": "In summary, in comparison to the many other submissions, this work’s contribution is insufficient. A proper empirical evaluation is missing and some of the authors’ claims are wrong.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This work focuses on using AutoML methods to find model ensembles that can achieve good performance with a low computational cost. They propose a workflow to find such an ensemble, which first generates a library of models using HPO and then selects ensembles based on the budget. For ensemble selection, they also propose a method called “Scalarized Multi-Objective with Budget Forward greedy” (SMOBF) to build ensembles using the generated library of models. The authors conduct experiments on CIFAR-100 and Microfossils to validate their method.\n",
            "main_review": "1. The proposed workflow does not have much novelty. The workflow in Figure 1 makes sense but it is just a straightforward way to use HPO algorithms to find model ensembles, which does not have any novelty.\n\n2. The novelty of the proposed SMOBF method is also unclear. It seems the main trick in SMOBF is to scalarize the multi-objective function (validation loss & computational cost). But this is a common practice in many previous works, e.g., MnasNet (https://arxiv.org/abs/1807.11626).\n\n3. The main takeaway of this work is unclear.\n\n    Table 1 compares the performance of many existing HPO algorithms when being applied in the workflow of finding model ensembles. Table 1 shows that HyperBand is usually the best one. This is informative but is this the main message that this work aims to convey? HyperBand is known as one of the best HPO algorithms, so it is not surprising that it works well here.\n\n    Table 2 compares post-hoc ensembling against ad-hoc ensembling. Is this work trying to show that post-hoc ensembling is better?\n\n4. The citation format needs to be updated. In many places, ‘citep’ should be used instead of ‘cite’. Also, some citations in the main text are missing from the Reference section. For example, I didn’t find FLAML Wang et al. (2021) in the reference list.\n\n5. Experiments are conducted on small datasets, like CIFAR-100. It will be more convincing if there are results on larger datasets like ImageNet.\n\n6. The authors choose ResNet-based architectures as the configuration space and use HPO algorithms to find the best architecture. But recently there have been tons of literature on NAS methods and search spaces. It would be much more convincing if this work uses some NAS methods (e.g., DARTS) here, instead of previous HPO methods that were not specifically designed to find the optimal architecture.\n\n7. Since this work is trying to find efficient ensembles, the following works should be relevant:\n\n    BATCHENSEMBLE: AN ALTERNATIVE APPROACH TO EFFICIENT ENSEMBLE AND LIFELONG LEARNING. (https://arxiv.org/pdf/2002.06715.pdf)\n\n    WISDOM OF COMMITTEES: AN OVERLOOKED APPROACH TO FASTER AND MORE ACCURATE MODELS (https://arxiv.org/pdf/2012.01988.pdf)\n",
            "summary_of_the_review": "The proposed method does not have much novelty. The empirical results do not provide much new insight on how people should build model ensembles. The experimental design (e.g., using HPO instead of NAS methods to find the best architecture) is also a concern. Also, the experiments are limited to relatively small benchmarks. So, I am inclined to reject this work.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}