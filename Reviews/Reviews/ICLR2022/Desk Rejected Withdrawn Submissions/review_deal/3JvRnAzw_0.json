{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper investigates the extent of Adversarial Weight Perturbation (AWP) that is helpful to avoid robust overfitting and thereby proposes a criterion to regulate the extent of weight perturbation to improve the potential of adversarial training. Authors categorize the adversarial examples into different groups based on their proposed Loss Stationary Condition (LSC), which simply groups the adversarial examples based on their classification loss values. They show that model tend to overfit to the adversarial examples with lower classification loss. Hence, they propose to introduce weight perturbation only to the adversarial examples with lower classification loss and no weight perturbation for higher classification loss samples. Results show that this criterion allows the adversarial training to improve the robustness on SVHN, CIFAR-10 and CIFAR-100.\n",
            "main_review": "Strengths:\n* Problem motivation is clear\n* Well written and structured\n* Provide a study with regards to the relation of weight perturbation size to adversarial robustness and shown that both are not mutually beneficial.\n* Provide an illustration of weight loss landscape that reveal the adversarial examples with lower classification loss are fit overfitted.\n* Propose a simple add-on criterion that enable and disable the weight perturbation based on the classification loss value.\n* Propose a new strategy to generate the weight perturbation.\n* Improves adversarial robustness on SVHN and CIFAR-10 over the baselines.\n\nConcerns:\n* Different groups in LSC for CIFAR-10 are manually created. How to choose the groups for a new dataset or dataset with different number of classes or a new model?\n\n* Improvement of results for CIFAR-100 are marginal over the baseline. Does it require carefully tuned LSC groups? or the rate of overfitting is not as evident as CIFAR-10? How does the weight loss landscape look here?\n\n* The improvements of TRADES-AWP and RST-AWP on CIFAR-10 are also marginal. This reveals that the rate of overfitting is not prominent for these models. How does the weight loss landscapes look for these models?\n\nMinor comments:\nGiven the statement \"Without an appropriate criterion to regulate...... unleash its full power\" in the Introduction, a teaser figure or table supporting this statement would have been helpful.\n\n",
            "summary_of_the_review": "The paper proposes a simple approach to reduce the robust overfitting and improve adversarial training robustness. However, the results are limited to certain settings e.g. Standard adversarial training with CIFAR-10. Results are marginal for different datasets and more robust models. This raises question on the wide range of applicability of the approach. Hence, I tend to rate below acceptance. However, I am willing to reconsider my rating based on other reviews and author responses.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a variant of adversarial weight perturbations to improve adversarial robustness obtained through adversarial training. Specifically, they compute the adversarial weight perturbations only based on adversarial examples not exceeding a particular loss threshold.",
            "main_review": "Strengths:\n- The paper is clearly structured, related work is discussed sufficiently and experimental setup is provided in detail.\n- Addressing the question which adversarial examples to compute the weight perturbation on is interesting as adversarial training is known to overfit badly, especially on non-robust examples where large loss values are observed.\n- Experiments with vanilla AT, TRADES and RST, evaluation using the standard AA benchmark.\n- Ablation studies with respect to key parameters.\n- Adversarial robustness is shown to improve over baselines.\n\nWeaknesses:\n- Writing can be improved at times. For example, in the abstract it is not clear how weight perturbations undermine robustness or what is meant with “excessive weight perturbation”. Similar sentences are found throughout the paper. A more specific language would be useful.\n- Also, regarding the last paragraph in the introduction, the AT-AWP paper actually provides some experiments highlighting the extent to which weight perturbations are useful (how many iterations, comparing to random weight perturbations etc.).\n- In Figure 1: what is gamma? Is it chosen as in the AT-AWP paper?\n- A recent ICCV paper [a] also shows that the performance gain of AT-AWP is mainly due to avoiding robust overfitting. It also includes a training plot with loss on correct and incorrectly classified examples that can be seen as a very coarse set of two LSC groups and it actually measures flatness instead of visualizing loss landscapes.\n\n[a] https://arxiv.org/abs/2104.04448\n\n- Why are the LSC groups in between 0 and 3? [a] shows that loss can be significantly larger, especially on mis-classified examples throughout training.\n- Regarding the results in Figure 2, I am not convinced that I see a significant trend as highlighted in the text. The text says that in between epochs 100 and 120 the loss on small loss adversarial examples is “obviously” sharper. But I find that very difficulty to see from plots (f) to (k). In fact, I find the blue curve to be flatter in many cases, at least for smaller neighborhoods around zero. Generally, I find these visualizations not very insightful in supporting the authors point. Maybe the authors could try actually measuring flatness following [a] throughout training for the different LSC groups?\n- I am also missing a discussion about not computing the gradient on large loss adversarial examples with weight perturbations but without. Until the end of section 3, it reads as if it might hurt to apply weight perturbations on large loss adversarial examples. But as with AT-AWP, the weight perturbation is applied on the full batch – the difference is just which adversarial examples are used to compute the weight perturbation. Wouldn’t it be meaningful to change that and just apply weight perturbation on low loss adversarial examples, too? Of course it is somewhat unclear how to do this with mini-batch training.\n- Is the weight perturbation initialized randomly before the K_2 = 10 steps are run?\n- How would the results in Table 1 look with AA? Currently, there are no result son SVHN or CIFAR100 with AA, so it is impossible to compare those with other papers or results. Especially as improvement on SVHN and CIFAR100 are less significant for PGD-20 already, so they might reduce more with AA.\n- The improvement is most significant for TRADES, while being less significant for AT or RST. It is also unclear how “lucky” the improvement of 0.31 for RST is? Did the authors run multiple models to get an idea of the standard deviation?\n- In the AT-AWP paper, one iteration works best, what is the authors opinion on why K_2=10 suddenly works better with RWP?\n- Also, using K_2=10 adds a significant computational overhead, so I am unsure whether the comparison in Table 2 is entirely fair. Did you try other baselines that use double the computation? AT with 20 iterations instead of 10, or AT-AWP with more weight perturbation steps. It is essentially missing in Table 2 that AT-AWP needs one forward/backward passes more than AT, but AT-RWP needs 10 forward/backward passes more than AT. I would expect some more meaningful baselines. I can imagine that the small improvement, e.g., for AT, from 54.04 to 54.61 is partly due to more computation (as this is also a WRN-34-10 with quite a bit of capacity).\n- I also assume that Table includes early stopped results, right? This should be made explicit in the text or table.",
            "summary_of_the_review": "I am not sure whether this paper is ready for ICLR. While the results indicate a slight improvement, the analysis of the proposed method is not convincing to me (mainly due to the visualizations) and experiments could be improved (AA on SVHN, CIFAR100, discussion/fair comparison regarding number of forward/backward passes used). I encourage the authors to address the points raised above in their rebuttal.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors aim to achieve a balance of decreasing robust over-fitting and adversarial robustness. They propose a criterion, termed loss stationary condition (LSC), by dividing the adversarial examples into different groups based on their classification loss. With such partition, they find that the network would first overfit these adversarial examples with small classification loss. Following these empirical observations, they further propose to only perform the weight perturbation strategy on examples with a smaller loss. Experimental results show that such a strategy can improve adversarial robustness.",
            "main_review": "[Strengths]\n1. This paper is clearly written and very easy to follow.\n2. This paper has a good motivation for the research problem (i.e., how to address the conflict between the current weight perturbation methods and the adversarial robustness).\n3. This paper shows an interesting observation: the adversarial examples group with small classification error has a significantly larger generalization gap. Based on the above observations, this paper proposes a new robust weight perturbation strategy, which can improve the adversarial robustness.\n\n\n[Weaknesses]\n1. The contribution is incremental. Based on some new empirical observations, this paper proposes a trick to improve the AWP method. The novelty is limited, and the contribution is incremental.\n\n2. The goal of this paper is not clear. Compared to the original weight perturbation method AWP, is the essence of this paper to further decrease the robust over-fitting or improve robustness? The two issues both will lead to the improvement of testing robustness, how to distinguish them in this paper? This paper only compares the testing robustness between the proposed method and AWP, but does not clarify their differences in the generalization gap.\n\n3. The proposed method is not well-motivated, i.e., the authors don’t introduce the motivation of the LSC criterion. Why do the authors analyze the generalization gap from such a LSC perspective? Such flaw makes the work seem to be built upon an accidental discovery from empirical observations.\n\n4. There lacks a thorough explanation of the rationale behind the proposed method. This paper is mainly built upon the empirical observations in Figures 1 and 2. However, why such examples will result in a larger generalization gap? A theoretical analysis (even informal) is expected. Instead, the author only mentioned that “the model will first memorize some easy-to-learn adversarial examples, and then spread to the entire training dataset”.\n\n5. The empirical observations lack enough evidence. The phenomena in section 3, including “weight perturbation robustness and adversarial robustness are not actually mutually beneficial” and “The DNN first overfit adversarial samples with small classification error”, are just verified on a few architectures and datasets. Hence, it cannot be guaranteed that such phenomena are accidental or regular. The authors are suggested to perform abundant experiments with different DNNs on different datasets to verify the ubiquity of such phenomena.\n\n6. Experimental results are not convincing enough to support the claim of this paper : the proposed method “prevents deep networks from over-fitting while avoiding the side effect of excessive weight perturbation, significantly improving the robustness of adversarial training.”\n(1) The authors have not conducted experiments to indicate that the proposed method can reduce the robustness over-fitting. \n(2) Results reported in Table 1 only shows a marginal improvement, compared to other baseline methods.\n\n\n7. The setting of the hyper-parameter is questionable. This paper used two parameters 1.7 and 2.2, which is not usual in the grid search setting. Why not use sequences like [0,0.5,1,1.5,2]? Such a parameter setting would be deemed to be cherry-picked, such that the effectiveness and generalization capacity of the proposed method would be doubted.\n\n8. The term “loss stationary criterion” sounds inconsistent with the paper. Actually, the LSC is a simple operation to divide the adversarial examples into different groups based on their classification loss. I can’t get why to use the word “stationary” and “criterion”. How to understand “stationary” and “criterion” here?\n\n\n[Minors]\n1. The legend in the figures are too small to read. \n",
            "summary_of_the_review": "The main problems of this paper lie on the following two aspects: (i) The proposed method is not well-motivated, i.e., it is unclear why the authors chose LSC to divide the adversarial samples into several groups. Such flaw makes the work seem to be built upon an accidental discovery from empirical observations. (ii) This paper proposes a trick to improve the AWP method, so the contribution is incremental. In addition, this paper lacks a thorough explanation of the rationale behind the proposed method. Therefore, I suggest a rejection decision. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper upgrades AWP (weight perturbation on all adversarial examples) to RWP (weight perturbation on adversarial examples with small classification loss) to enhance adversarial training, which helps to improve robustness. ",
            "main_review": "Main Concerns:\n\n**Page 5, 6:**  \n**Authors: LSC View of Robust Overfitting: the model will first memorize some easy-to-learn adversarial examples, and then spread to the entire training dataset...**\n\nThis conclusion is only supported by one experiment in Figure 2. Is this phenomenon general across datasets, networks, and attack methods? Can you provide more empirical results? Can you provide an insightful (theoretical) analysis to answer why the model will first memorize some easy-to-learn adversarial examples...?  \n\n\n**Authors: LSC view of Adversarial Weight Perturbation: adversarial weight perturbation on adversarial examples with small classification loss is sufficient to suppress robust overfitting, adversarial weight perturbation on adversarial examples with large classification loss leads to worse adversarial robustness.**\n\nThis paper also argues the above conclusion through one experiment in Figure 1c. Is this general? What are the real reasons behind it? \nIn Figure 1c, what's the percentage for each LSC group? Does the percentage affect results? \n\n\n**Authors: Do We Really Need the Worst-case Weight Perturbation?: conducting worst-case weight perturbation on these adversarial examples is not necessary, since it will also deteriorate the adversarial robustness.**\n\nWhy it will deteriorate the adversarial robustness, can you provide more theoretical and empirical analyses? Compared with AT, I don't see 'deteriorate' in Figure 1c.\n\n\nMinor concerns.\n\n**Page 4:** \nAlthough AWP is not proposed by this work, the explanation for 'Does Weight Perturbation Robustness Lead to Better Adversarial Robustness?' through one experiment (Figure1(a)) is weak. Is the experiment in Figure1(a) general across datasets, attack methods, networks? Or can you provide some theoretical analysis?\n\nAs the article structure and empirical settings are (very) similar with AWP https://arxiv.org/pdf/2004.05884.pdf, why don't compare with MART in Table 2 (which is shown in AWP Table 2)?\n\nThe improvements in Table2 under auto-attack are narrow  (for me, auto-attack is an important target) ",
            "summary_of_the_review": "This paper provides a minor upgrade for AWP to enhance adversarial training, but it is short of enough contribution (ref Main Concerns).  e.g., no clear and enough (empirical and theoretical) explanations for  'LSC View of Robust Overfitting, LSC view of Adversarial Weight Perturbation, Do We Really Need the Worst-case Weight Perturbation?'.\n\nIn conclusion, 1. the current version is a small modification of AWP (RWP is AWP with loss threshold), 2. and just uses a small number of experiments in Figure 1, 2 to explain the motivation of RWP. Lack of theoretical analysis or insightful view. 3. Some experiment results are narrow (e.g., auto attacks in Table 2). For these reasons, I think this paper has a lot of room for improvement.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}