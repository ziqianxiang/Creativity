{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Reject",
        "comment": "The paper investigates the effect of temperature in the loss function for graph contrastive learning and proposes a novel method to dynamically adjust it during learning (GLATE). The paper is concerned with an interesting problem, is timely, and relevant for the ICLR community. \n\nAfter author response, reviewers did not come to a full agreement on the paper, with two reviewers indicating (weak) reject and two reviewers indicating (weak) accept. Reviewers highlighted the potential impact of improvements in graph contrastive learning as well as the theoretical analysis as strengths of the paper. However, reviewers raised also concerns regarding scope, novelty of the contributions, and clarity of presentation (method, evaluation, etc.). While the authors' response addressed some concerns regarding aspects of the experimental evaluation, it did not change the overall evaluation of reviewers. \n\nTaking author response and reviewer feedback into account, I narrowly agree that the manuscript is not ready yet for acceptance at ICLR due to the aforementioned concerns. However, I encourage the authors to revise and resubmit their work based on the feedback of this reviewing round."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The authors explore the role of the temperature in the loss function for graph contrastive learning. They argue that global uniformity and local separation are both necessary to the learning quality and this can be controlled by the temperature. Thus, they develop a simple but effective algorithm GLATE to dynamically adjust the temperature value in the training phase. Experiments are also conducted to demonstrate the effectiveness of the method.",
            "main_review": "Strengths: The problem faced by the paper is interesting and timely and the proposed approach seems reasonable. The article is well written, the method is clearly described, and the overall quality is good. The authors also provide the source code to facilitate experimental replication.\n\nWeakness: \n1. The contrastive loss function used by the authors is not universally applicable to various graph tasks. In Section 3.1, the general loss function proposed by GRACE/GCA for both inter-view and intra-view is used for node classification tasks. The exploration of temperature in this paper is limited to the node level, which is slightly inadequate.\n2. Changes in the initial contrastive views can also affect the balance between the different views. I would like to have an analysis of this, such as the effect of changing the dropping rate on temperature selection.\n\nSuggestions:\n1. It is suggested that more content be added to explain and analyze why the truncated strategy cannot be applied to graph contrastive learning. This will help in the understanding of dynamic temperature estimation.\n2. I am well aware that the current GCL methods cannot be applied to large-scale graph datasets, such as the OGB series, due to the limitations that multiple encoders require a lot of memory. If possible, it is recommended to add results on larger datasets, such as amazon and coauthor datasets also used in GCA.",
            "summary_of_the_review": "The overall quality of the paper is good, with a clear narrative. The problems addressed are also much needed. The methodology is straightforward and clear, and there is also theoretical guarantee. So I recommend it for acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes the importance of dynamically changing the temperature in the contrastive loss with a momentum style that will boost the graph contrastive learning. It also provides theoretical analysis on the reason to dynamically adapt the temperature.",
            "main_review": "My major concern is that the results are not very trustable. For example, one fixed tau already outperforms all baselines. In table 1, Frozen temperature (τ = τinitial = 0.8) 84.32±0.15 67.83±0.22 85.23±0.08. This method actually is the typical InfoNCE CL on the graph. How it can easily outperform all baselines? some baselines even adopted sophisticated techniques. At the same time, the results(86.8 ± 0.5) reported by MVGRL [27](which was also reproduced by some other papers) on Cora is much more than the authors’ reports(83.50±0.40).  This makes me highly suspect the superiority of the proposed approach. It is possible the improved performance is only marginal. BTW, it is also suggested to conduct experiments on the graph classification task on MUTAG etc.\n\n\nIn figure 1, “we use the shape and color to represent its class and embedding”. Basically, what does the embedding mean? Does each point correspond to one node sample in the graph? If that is true, color means the label, what “shape” is used to denote?\n\n\n\nFormula (9) is not clear. IB_{ssl} = I(Z 0 , X) − βI(Z 0 , X0 ). I know the authors want to express the case for SSL. But Information bottleneck is used for compression with the form IB= I(Z, X)- βI(Z , Y). Here the term is Y, not the augmentations. The authors directly claim such a formula without any proof or citation that also used this formula. \n\n\nMinor:\n\nTypos:\n\nit decrease faster later==>it decreases faster later\n“Raw features” only uses the node==>“Raw features” only use the node\nmake the model optimizes consistently==>make the model optimize consistently\n\nbesides, there are some newly accepted papers on graph CL by NeurIPS 21, they are suggested to be included in the related work, such as:\n\nErlin Pan, Zhao Kang, Multi-view Contrastive Graph Clustering,  NeurIPS 21\nDongkuan Xu, Wei Cheng, Dongsheng Luo, Haifeng Chen, Xiang Zhang InfoGCL: Information-Aware Graph Contrastive Learning,  NeurIPS 21\nYanqiao Zhu, Yichen Xu, Qiang Liu, Shu Wu, An Empirical Study of Graph Contrastive Learning,  NeurIPS 21",
            "summary_of_the_review": "Basically, the idea is novel and the theoretical analysis is also reasonable.  My major concern is focusing on the experiments part. Basically, the baselines reported by the authors are much smaller than those reported in the original papers. This makes me suspect if the contribution is only marginal in boosting the results.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies the temperature parameter of graph contrastive learning models. Especially, two terms, global uniformity and local separation are proposed to disentangle the contrastive objective and a GLATE algorithm is proposed to adaptively adjust this parameter.",
            "main_review": "# Strengths\n* The temperature parameter is important for understanding the behavior of contrastive objectives for graph-structured data.\n\n# Weaknesses\n* This paper is hard to follow. For example, the authors never clearly define global uniformity and local separation. It is suggested that, for better readability, the authors shall provide a background of dissecting contrastive objectives and then proceed to the problem of existing formulation.\n* It feels to me the presented analysis is limited to GRACE that leverages the InfoNCE loss. How does the analysis generalize to other graph contrastive algorithms, e.g., GraphCL that excludes inter-view negative samples, DGI that uses JSD objectives, bootstrapping losses that eschew the need of negative samples?\n* The transductive experiments are conducted on three small datasets, which are not convincing. Evaluation on large-scale datasets such as open graph benchmarks is needed.\n* The analysis given in Section 3.3 seems to be insufficient. What is not clear to me is that the information bottleneck term counts for noise in node features only. What if the structure is perturbed as well? How does the presented analysis differ from one recent work (https://arxiv.org/abs/2106.05819)?\n* One particular concern with the presented formulation is that, unlike visual data, negative samples involved in graph contrastive learning are not aligned with their semantic relationship (Figure 1, https://arxiv.org/abs/2110.02027), partially due to the neighborhood smoothing operation of graph neural networks. That being said, the more closeness to negative samples (in other words, more difficulty negative samples) will lead to wrong selection of false negative samples (i.e. true positives). Therefore, I in person believe that simply adjusting the temperature parameter is not sufficient to account for the balance of global uniformity and local separation. \n\n## Minor problems\n* Property 2 seems to be off-context to the main text.\n* \"Interpretable\" contrastive representations in Section 2 is not accurate.\n",
            "summary_of_the_review": "Overall, I appreciate the deep insights on the temperature parameter. However, many serious problems exist in the current version. Therefore, I recommend rejection.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper investigates a crucial problem, i.e., how to generate good node representations under the InfoNCE contrastive loss in graphs. By studying the feature of loss function’s gradients, it finds that the dynamic temperature change is beneficial to learn uniform node representations. The proposed method GLATE is simple but effective. The analysis of the connection between GLATE and the information bottleneck principle helps us understand why GLATE is so effective. The presentation of this paper is clear and well-organized. For example, Figure 1 makes us easy to understand the problem generated by the static temperature and the significance of this work. The experiments are laid out in detail. The results on the tasks of transductive and inductive learning show GLATE’s advantage over the SOTA graph contrastive learning algorithms.",
            "main_review": "Pros:\n1. An interesting and valuable problem. Understanding the contrastive representations is helpful to improve the current contrastive learning framework and promote the development of contrastive learning. Although some works have explored what is a good image representation and proposed the golden rule of alignment and uniformity, there is a lack of studies about the quality of node contrastive representation in a graph.\n2. Simple, provable, and effective algorithm design. Inspired by Momentum, the authors dynamically adjust the temperature’s value along with the representation’s uniformity degree in the training phase. This solution is easy to implement and its performance on the downstream task is pleasing.\n3. The experiment’s design is rigorous and its results are persuasive. For instance, in the inductive learning task, the comparison between baselines and GLATE is fair due to using the same choice (GraphSAGE-GCN) for the encoder network.\n\nSuggestions:\n1. Lacking the discussion of time complexity or experimental results of time costs. I notice that the authors have described a specific experimental setting in the appendix (A.2 EXPERIMENTAL SETUPS), but whether the setting of changing temperature every 20 epochs is reasonable and why the number is 20? I think authors should discuss the effect of this setting and show the comparison of time costs between GLATE and baselines.\n2. It is interesting to see whether it is necessary to use different initial temperature values for different graph datasets. For different graph datasets, their initial uniformity degrees are different, so I think it will be better to design a task-dependent GALTE, not a task-agnostic GLATE.",
            "summary_of_the_review": "This paper investigates an interesting and valuable problem, solve it via  simple, provable, and effective algorithm. And the experiments are sufficient.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}