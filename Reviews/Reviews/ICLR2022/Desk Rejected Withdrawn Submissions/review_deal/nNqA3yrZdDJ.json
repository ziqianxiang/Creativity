{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies the sample complexity of offline policy evaluation/optimization under deep ReLu neural network approximation. The theoretical analysis is conducted under the new proposed framework named \"Besov dynamic closure\". Under such a framework, the issues of \"deadly triad\" that commenly encoutered in practical offline RL can be avoided and the convergence of LSVI under nonlinear function approximation can be guaranteed. The author also compare their theoretical results with some state-of-the-art results that under tabular or linear MDP assumptions to demonstrate the significance of their work.",
            "main_review": "Offline RL with nonlinear function approximation is a very challenging problem. Under practical settings, neither the convergence nor the global optimality guarantee can be guaranteed. This paper provides a new framework to establish some interesting results. However, compare with the \"state-of-the-art\" (SOTA) works of offline RL, this paper does not provide too much new insights or new practical solutions. Specifically, compared with SOTA theoretical works, this paper avoid the core issues of offline RL (poor sample coverage, over-optimistic issues etc) via strong distribution coverage assumption (Assumption 3.1). Moreover, compared with SOTA practical works, this paper does not provide better offline RL methods but only study a less practical algorithm under strong assumptions (Assumption 3.2) with a general framework (Definition 3.1, 3.2). Given the SOTA offline RL results, the results in this paper is not suprising and not novel enough.\n\nMy detailed comments are listed below:\n\n(1) Assumptions: although the author provided some verifications for their assumptions, those verifications are either out-of-date compare with SOTA works (reference like Chen & Jiang 2019) or not practical (examples 3.1, 3.2). The Assumption 3.1 is stronger than most of SOTA works of offline RL [1][2][3] (to name a few) that only require weak coverage over samples. Assumption 3.2 is very similar to linear MDP assumptions that required Bellman operator to be restricted within a given function space. However, as I have mentioned before,  Assumption 3.2 is a \"strong assumption\" under a \"general framework\". Such a completeness assumption can help avoid many offline RL difficulties just like what linear MDP and tabular MDP assumption did. Although those two assumptions are mathematically more general than linear MDP, it serves as similar roles as linear MDP when establishing the theoretical results. If these two assumptions do cause additional challengings, I hope the author can highlight the novelties of proof in the rebuttal.\n\n(2) The comparision in Table 1 miss many SOTA offline RL results. I also suggested the authoer to add a column about the sample coverage assumptions as such an assumption is very important to evaluate the novelty of the results in the offline setting.\n\n(3) Motivation and empirical verification: since the motivation of this paper is to investigate core issues of offline RL in a more practical setting than previous works with linear MDP or tabular MDP assumptions, empirical verifications are needed to support the theoretical results and assumptions.\n\n[1] Jin, Y., Yang, Z., & Wang, Z. (2021, July). Is Pessimism Provably Efficient for Offline RL?. In International Conference on Machine Learning (pp. 5084-5096). PMLR.\n\n[2] Uehara, M., & Sun, W. (2021). Pessimistic Model-based Offline Reinforcement Learning under Partial Coverage. arXiv preprint arXiv:2107.06226.\n\n[3] Chen, L., Scherrer, B., & Bartlett, P. L. (2021). Infinite-Horizon Offline Reinforcement Learning with Linear Function Approximation: Curse of Dimensionality and Algorithm. arXiv preprint arXiv:2103.09847.",
            "summary_of_the_review": "I do not recommend this paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies offline RL (OPE and OPL) with deep ReLU neural nets approximation under the distribution shift parameter and Besov closure assumption. It introduces the Besov smooth space, and shows the generality of the Besov closure property. Further, it proves that the simple FQI-style algorithm has sample complexity $O(\\kappa^{1+d/\\alpha} \\cdot \\epsilon^{-2-2d/\\alpha})$. ",
            "main_review": "For the positive side, the expression and organization of the paper is okay. The setting studied in the paper is very important, as it is crucial to connect traditional RL theory with deep neural nets to explain the possible benefits of deep RL. The explanations and examples are adequate to show the generality of the assumptions (Besov space and Besov closure property, the distribution shift, etc.). The theorems appear to be correct to me.\n\nFor the negative side, I have a few questions:\n1.\tThe bound appears to be confusing, as it depends on $1/\\epsilon^{d/\\alpha}$ linearly. Here $d$ is the dimension of the state-action space, and $\\alpha$ is the smoothness parameter. Although assumption 3.2 requires $d/\\alpha < 2$, I am not clear why the lower of $\\alpha$ appears here. Is it because the bound become vacuous if $d/\\alpha$ is large? How much the capacity of the Besov close space reduce with or without the lower bound on $\\alpha$?\n2.\tThe technical contributions are not clear to me. I believe the Besov closure property is novel in the paper and the uniform convergence argument is novel, but how about the analysis of algorithm 1? Please highlight the contributions over the analysis of Yang et al. (2019).  \n3.\tIt is not clear why the bound is better than that of Yang et al. (2019). It appears to me that the scale of the smoothness parameter $\\alpha$ may be different in Besov close space and Holder smooth space, which also makes the bound vacuous. Please make it clear.\n\n**I hope the authors can address my concerns, or I may reduce my score.**\n\nThere are several suggestions on the writing:\n1.\tThere are too many notations in the paper, so it will be convenient to establish a lookup table.\n2.\tSince you are studying the theory with deep ReLU net approximation, it will be appreciated to do some experiments to check if LSVI combined with deep ReLU net really work in OPE and OPL, and how much it deviates from the predicted sample complexity. \n",
            "summary_of_the_review": "The paper is generally an okay paper. The novel Besov smooth property is surprising, but I do have some concerns. I will recommend for acceptance at this time.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors study the sample complexity of OPE/OPL with deep ReLU network function approximation. The sample complexity bound depends on the distribution shift coefficient, the dimension of the state-action space, the smoothness parameter, and the scale of the error. The function class satisfies Besov dynamic closure and the correlated structure.",
            "main_review": "My first comment is about the writing. The paper is easy to follow, and the main text does not include much technical part. I think the authors may consider moving the comparison table (table 1) to the introduction so that readers can get the main results easier. In addition, it would be very beneficial to provide a high-level proof sketch in the main text.\n\nThe abstract mentions “two novel considerations”. Besov dynamic closure is an assumption and the restriction as we would like to make minimum assumptions. Without proving it is necessary, I don’t think it is appropriate to say it is novel.\n\nOne of the assumptions mentioned in the abstract is ‘correlated structure’. After reading Section 4.2, I am still very confused by its meaning. If I understand correctly, it means that if we use a union bound over the function class, then there is no need to split the data to K-fold. I believe it is what is done in most of the prior work. (1) Is my understanding correct? (2) In the intro, you mentioned that  “the prior results (Munos & Szepesva ́ri, 2008; Le et al., 2019; Yang et al., 2019) improperly ignore this structure or avoid it using an inefficient data splitting approach”. I think Munos & Szepesva ́ri, 2008 does not apply the data splitting, then do you mean their analysis is wrong? I think in most places you just mentioned that the analysis in Le et al., 2019 is wrong. (3) What is the specific mistake in Le et al., 2019? They have a log F term in the bound so I assume they also use the uniform convergence (or so-called correlated structure) and there is no such issue? I believe union bounding over the function class F + Bernstein's inequality have been applied in many paper.\n\nAssumption 3.1 is quite strong for the general setting. For example, in the linear MDPs, we don’t need the coverage over the entire state-action space, and the data with a good covariance matrix is enough. Is it possible to reduce the current result to the common setting, e.g., linear MDPs? I feel the sample complexity for the directly reduction would scale with |S||A| instead of the dimension d in the linear MDP.\n\nBesov dynamic closure is different from the more standard closedness/completeness assumption, i.e. T^pi f \\in F. In general, we need the T^pi f \\in F so that the solution of the regression problem is still in the original function class F. Could you provide some comment on why the assumption is different and how the current algorithm still works under this different assumption? Does the result also hold under the more standard one?\n\nIt is quite hard to understand how general the function class with Besov dynamic closure can capture. Could you comment on that? I was also wondering whether there are some natural examples.\n\nThe technical analysis doesn’t appear to be very novel. They are high related to standard OPE/OPL analysis and local Rademacher complexities (Duan et al., 2021). \n\nI believe the theoretical result for the under covered data (or pessimism in the recent literature) and the function class with only realizability would be interesting extensions.",
            "summary_of_the_review": "Overall, the paper is easy to follow. The result is new to me, but it is lack of motivating examples. The technical analysis does not seem to be very novel. In addition, some discussions are quite unclear and the writing can be improved.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies the offline reinforcement learning problem with deep ReLU networks. Specifically, it considers both the off-policy evaluation (OPE) and off-policy learning (OPL) problems. It assumes that the state space is possibly infinite and the action space is continuous. To deal with this setting, the paper considers the case where a deep ReLU network is used to approximate the value functions. It then analyzes an NN-version of the well-known LSVI algorithm and provides sample complexity bounds. Their bound depends on the specified error, the distributional shift, the dimension of the state-action space, and a smoothness parameter.",
            "main_review": "Strength: \n1. I think the biggest contribution of the paper is that it considers a correlated structure in the value function estimation. Specifically, in the version of the LSVI algorithm analyzed by this paper, there is no data splitting (i.e. splitting into K independent folds), which was adopted by previous works. This helps them save a K factor in the sample complexity upper bound. \n2. This paper is well-written and conveys the ideas well. It is easy for the readers.\n\nWeakness: \n1. The proof technique of the paper is hidden in the appendix and from the main text I do not understand how the analysis is done and more importantly, how it is different from the previous work. Since this is a purely theoretical work, it is very important to discuss the technique novelty compared to the previous work. For example, it would be good to have a section of technical overview or proof sketch discussing why the previous work requires the K-fold splitting and how this work avoids the splitting while still being able to analyze the correlated structure. Given the current presentation of the paper, it is hard to tell what is the technical novelty.\n2. Although the paper saves a factor K compared to the previous work, it also brings an extra factor of 2 in front of the dimension d in the exponent. Therefore, it is hard to tell whether the current is really a significant improvement or not. Besides, given that the technical novelty is not clear from the current writing, my concern is whether the extra factor is a tradeoff for removing K. This would then be uninteresting. \n\nI recommend the authors discuss the technical novelty of the paper/at least explain the proof. This is important for theoretical works.\n\nMinor comments:\nIt might be good to also include the following related works: https://arxiv.org/abs/2102.01748, https://arxiv.org/abs/2106.11960,  ",
            "summary_of_the_review": "From the current writing, it is unclear about the theoretical novelty of the paper.\nMy current recommendation is borderline reject.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}