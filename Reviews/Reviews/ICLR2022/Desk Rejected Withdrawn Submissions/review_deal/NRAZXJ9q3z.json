{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a DTW-preserving shapelets transformation that considers Must-Link (ML) and Cannot-Link (CL) constraints. The method stems from LDPS (Lods, et. al. 2017), but also adding constraints as penalties to the loss function. The method is evaluated using a clustering task on the UCR time series dataset. Compared with direct K-Means and COP-K-Means, the transformed representation is shown to lead to better NMI scores (better clustering quality).",
            "main_review": "Shapelet transformation is shown to be useful in time series analysis. DTW-preserving shapelet learning is an interesting task that construct shapelets while maintain pairwise DTW-distance. This paper extends the Lods, et. al. 2017, to a more general scenario that involves ML and CL constraint, thus making the DTW-preserving shapelet transform more useful in practice.\n\nHowever, I found the paper lacks a few key components that can justify the proposed method:\n1. The main difference than Lods, et. al. 2017, is equation 6. But a discussion on motivation, potential choices and impacts, are missing, except only being mentioned inspired by contrastive learning.\n2. More critically, in equation 6, the $phi$ term is added as a regularizer. I didn't see how it enforce the hard constraint as claimed by the paper. I didn't find any projection to the constraint set in the context. If this is not solved, the paper cannot claim its major contribution, constraint shapelet learning. If I am wrong, please elaborate.\n3. The usefulness of the proposed method, is only evaluated using a clustering task, and compared with rather simple baselines (K-Means). There are many time-series related tasks (classification, generation, etc). If the method indeed learned a better representation of the original time series, evaluation on a variety of tasks should be carried out.\n4. Typically, UCR is used as a whole set for evaluation. UCR contains more than 40 subsets. It seems the paper cherry-picks a few of them for evaluation, making it very hard to justify the effectiveness of the method.",
            "summary_of_the_review": "Though the paper tries to extend the LDPS method to a more general case, I think the paper has major technique flaws in both methodology and experiments. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concerns",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes CDPS, a constrained version of Learning DTW-preserving shapelets (LDPS) method to integrate must-link and cannot-link constraints in the learned representations. The modeling follows a similar procedure as the original method with a change in the loss function inspired by the contrastive losses/learning literature to integrate such new constraints. Experiments on a sample of the UCR datasets show the benefits of this approach.",
            "main_review": "Strong points:\n\n1) Learning time-series representations, especially under non-Euclidean assumptions, is a timely and difficult problem\n2) Semi-supervised solutions have the potential to integrate some human labeling to significantly improve performance vs. unsupervised solutions.\n\nWeak points:\n\n1) Novelty is rather limited. The method is an extension of LDPS is a rather straightforward way (change of loss function, which is adapted as well) - No new problem or solution.\n2) Experimental evaluation is very weak - many baselines are missing.\n\nDetails:\n\n1) The core method, LDPS, is not explained or ever properly introduced in the preliminaries.\n\n2) The new method is a simple extension of the loss function. Simplicity is not an issue, but here the solution is plugging in a new loss function, which is adapted from previous work, which is not clearly introduced before either. Novelty is limited.\n\n3) Considering the limited novelty, at least some extensive experimentation would be necessary. Comparing your approach against k-means or k-means with constraints it's not sufficient considering how much progress there has been in the area of time-series clustering.\n\n- k-Shape is a state-of-the-art unsupervised method [a]. It should be used as a baseline and likely it can be extended with such constraints in the same manner as k-means\n- FeatTS is a semi-supervised method working in a similar spirit as yours but by extracting features first instead of DTW-preserving representations [b]\n- Are DTW-preserving representations the best? there are GRAIL representations [c], RWS representations [d], etc. Why LDPS? DTW is the most popular measure but not the best [e]\n- Likely this appeared after your submission, but also lots of work from the DNN literature as well [f]\n\n[a] Paparrizos, John, and Luis Gravano. \"k-shape: Efficient and accurate clustering of time series.\" Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data. 2015.\n\n[b] Tiano, Donato, Angela Bonifati, and Raymond Ng. \"Feature-driven Time Series Clustering.\" EDBT. 2021.\n\n[c] Paparrizos, John, and Michael J. Franklin. \"GRAIL: efficient time-series representation learning.\" Proceedings of the VLDB Endowment 12.11 (2019): 1762-1777.\n\n[d] Wu, Lingfei, et al. \"Random warping series: A random features method for time-series embedding.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2018.\n\n[e] Paparrizos, John, et al. \"Debunking four long-standing misconceptions of time-series distance measures.\" Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data. 2020.\n\n[f] Lafabregue, Baptiste, et al. \"End-to-end deep representation learning for time series clustering: a comparative study.\" Data Mining and Knowledge Discovery (2021): 1-53.\n\n4) UCR has 128 datasets. Shouldn't all of them be used? Are the trends the same across all of them? Is the method scalable to all of them? What's the complexity? Runtime results?\n\n5) No code is available as claimed in abstract.\n",
            "summary_of_the_review": "Lack of novelty and appropriate baselines to support the claims of the paper. The time-series clustering literature has substantially grown in the last decade and the work seems to focus on a very narrow path. An improvement over a 20-year old technique seems somewhat obsolete at this point. Significant expansion in the experimentation could improve the paper.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The manuscript presents a method for constrained clustering of time series aiming to exploit the availability must-link and cannot-link constraints among pairs of time series. The approach is based on first learning time series representations using a proposed method that is called constrained DTW-Preserving Shapelets (CDPS) and then performing k-means clustering of the representations.",
            "main_review": "The proposed CDPS approach relies on a previous method called LDPS (Learning DTW-Preserving Shapelets). CDPS is a rather straightforward extension of LDPS in aiming to integrate the available constraints. This is done by adding a term \\phi_{ij} in the LDPS loss function (eq 5). The CDPS method constitutes marginal adaptation over LDPS, therefore the paper is of limited novelty. \n\nIt should be stressed that the \\phi_{ij} term (defined in eq. (6)) employs a distance function Dist_{ij} that is not defined in the paper. It also includes three hyperparameters (\\alpha, \\gamma, w) that should be specified by the user. Note that in the experimental part it is not mentioned how w has been specified.\n\nIn what concerns experimental validation, the authors ignore the important case where the LDPS is first applied and then constrained k-means is used on the representations provided by LDPS. This would actually reveal whether it is preferable to exploit the constraints in representation learning phase or in the clustering phase.\n",
            "summary_of_the_review": "The paper  relies heavily on a previous approach (LDPS), thus its novelty is limited. Moreover there are serious unclear issues and the experimental validation is insufficient.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper introduces a new formulation of the problem of learning shapelets representation for time-series involving ML and CL constraints.",
            "main_review": "The paper is not clear enough. Several sentences may be improved and very few insights are given. \n\n- Must-Link constraint and Cannot-Link constraint should be properly defined.\n- In Section 3.2,  you should recall w.r.t. which parameters you want to minimize the loss depicted in equation (5). Indeed, in Section 3.3, you are searching for shapelet transformation minimizing the loss (5) but there is no dependence in $S$ in the loss. Therefore, you are computing gradient w.r.t. shapelets where shapelets are not even a parameter of your loss. The algorithm is not detailed enough to be linked with Section 3.2. \nIn Appendix A, the notation $\\widehat{DTW}$ is a little confusing since there is no link with $DTW$.\n\nIn addition, the experimental part seems very light for non-mathematical papers. I think experiments on parameters $L_{min}$ and $S_{max}$\nshould be done since your loss involves additional terms compared to \\textsc{Lods et al., 2017}.\n\n\n- Experimental part: \n\n- What about the algorithm complexity? time computation? It seems of high computation cost regarding (5) and (7) since DTW can be computed as $L_{TS}^2$ and is involving in gradients. It would lead to $\\mathcal{O} \\left( K^2 L_{TS}^2\\right)$ at each iteration of the algorithm (without batch).\n\n- Results with additional clustering metrics, such as Adjusted Rank Index, for example, should be added in the Appendix to remove the variance from the chosen evaluation metric.\n\n\nMathematical problems:\n\n- Euclidian score equation, i.e. equation (2): I think the letter $l$ should be $L_S$. Furthermore, $T_{i,m-L_S:m}$ is compared to $S_k$, not $T_{i,m:L_S}$.\n- Equation (3): $m$ should start at $1+L_S+1$.\n- Constraints Sets: K is already the number of shapelets, you should use another letter. Furthermore, operators $\\neg$ and $\\wedge$ must be defined.\n- Equation (6) $Dist_{i,j}$ is not defined.\n- Equation (7), $K$ is assumed to be lower than $N$ ?\n\n\n\nMinor comments:\n\n- Last line of the abstract: \"are demonstrated\"\n- Line 5 of the introduction: \"images which are\" \n- Line 14 of the introduction: \"to provide\"\n- The second to last paragraph: \"they and the constraints\"  needs to be reformulated.\n- Line 15 of related work: \"have introduced\"\n- Line 17 of related work: \"as\" instead of \"to be\".\n- Line 10 of definitions and notations: \"time-series\"\n- Equation (4): use \\ldots instead of \"..\"\n",
            "summary_of_the_review": "The paper appears to be an incomplete work. Indeed, the mathematical part that introduces the CDPS approach is not clear enough and objects are not properly defined in addition to several typos. The experimental part is not investigated at the lens of several parameters it involves. Furthermore, even if the introduction and the related work are correctly written, very few insights are given and motivations of the paper are then weak.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}