{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a bi-level optimization technique that can then craft the poisoned data for a backdoor attack. The poisoned data are claimed to be difficult to visually detect because of the \"hidden trigger\" and clean labels. The proposed technique is shown to have high attack success rates for models that are trained from scratch or fine-tuned on CIFAR10 and ImageNet, compared to other similar methods.",
            "main_review": "In general, I find that the paper proposes a novel algorithm to make the backdoor attack more difficult to be detected. The paper has the following strengths:\n - The paper is well written.\n - The proposed technique is novel and the paper describes efficient approaches to solve the optimization.\n - The experiments are generally very good to support the main goal of the paper, which is to carry out the backdoor attacks under the clean sample/label setting.\n\nFor the experiments, I, however, have some concerns:\n- The perturbation is quite large. Specifically, the perturbed images under the selected $l_{\\inf}$ norm are visually different than the original images. Previously, Nguyen et al and Doan et al show that a human inspection test can detect poisoned samples quite effectively if the triggers are not visible. While the labels are not flipped, a similar test like in these works will render this attack less effective. \n - How is the attack performance under different values of $l_{\\inf}$ norm? I believe these experiments could further analyze the practicality of the proposed attack.\n - While it is true that there're \"robustness-accuracy trade-offs\" under the existing defenses, the fact that the model can be detected under model inspection defenses such as Neural Cleanse should be explicitly discussed. For example, for clean models, Neural Clean will report an anomaly index of less than 2; or for Spectral Signature, it is also beneficial to show whether the proposed method has a large difference in \"spectral correlation\". It can be seen that for Spectral Signature (in Table 7), the clean accuracy does not drop very much while the attack performance drops significantly. This is probably because of the fact that there's a large difference in the spectral correlations. \n - More experiments on the defense should be conducted, instead of only using CIFAR-10. It is probably not appropriate to make a conclusion on only 1 dataset, and instead should be done on multiple datasets with  different characteristics, e.g. MNIST, traffic signs, faces (see Nguyen et al and Doan et al)\n\nNguyen et al. https://openreview.net/forum?id=eEn8KTtJOx\n\nDoan et al 2021. LIRA: Learnable, Imperceptible and Robust Backdoor Attacks.",
            "summary_of_the_review": "I really like the proposed approach and think that it is novel. However, I also think that there are missing andimportant aspects that are relevant to a backdoor attack work and should be discussed in the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose Sleeper Agent, a backdoor technique for vision-based models that do not involve placing the trigger into the training data at all. Instead, the adversary crafts imperceptible noise to add to the training data that would lead to the intended (mis)behavior of a model trained on that data. The authors use a combination of several \"tricks\" like gradient matching, data selection, model retraining, ensembles, etc., to successfully launch attacks for large-scale models and datasets. Evaluation of several mitigation techniques shows how the attack can bypass most defenses easily.",
            "main_review": "# Positive feedback\n\n1. The idea of aligning gradients of poisoned training data with expected data gradients (with inserted patches) is quite impressive. Most works try to use the same patch triggers while training and actually launching the attack. Decoupling the perturbations added to training data and the patch triggers is quite interesting.\n2. Even with 50 data points (0.1% training data), the triggered source accuracy is ~61%, which is quite impressive. The authors should emphasize the effectiveness of their attack in the low-budget regime, bringing it closer to being a practical threat.\n3. Table 9: The increase in attack success rates with variations in the attack setup, along with linear-like addition of performance gains on combinations of different techniques, is remarkable and goes to show how the proposed modifications each have valuable contributions to the adversary's goal.\n\n---\n# Criticism\n\n1. The features proposed that Sleeper Agent seems to explore have already been explored well in the literature, especially gradient matching, ensembles (Witches Brew considers ensembles), data selection [1], and black-box settings [2]. The current work feels more like system engineering, given the lack of any technical contributions.\n2. It feels odd that images with triggers/backdoor perturbations are missing for CIFAR-10, which is the main focus of experiments, from both the main text and supplementary material. Please include them in either one of these.\n3. Section 3.2: \"..for all but a small fraction of indices...\" I do not see why this would be true. There is no such regularization term in the loss function. The perturbation may be bounded per data point to be in some $L_{\\infty}$ norm ball, but that does not imply the perturbations would be non-zero for a small fraction of data. How is this enforced? Is there some $L_0$ norm regularization on the overall perturbations that is missing from the equation? \n4. Table 2: Is there a reason to stop at 1%? It would be interesting to see curves for attack success rates with different poison rates.\n5. Table 4: What metric is being reported here? Please clarify. Assuming they are attack success rates, they are not very impressive, especially when the adversary gets to include the victim architecture. Also, reporting the average for this metric is not the best idea. From the victim's perspective, using VGG11 is significantly worse, so it would avoid such architectures. From the adversary's perspective, the damage in the worst case (lowest attack success rate) should be on interest since it puts a lower bound on its effectiveness.\n6. Results on ImageNet seem to be not as impressive (Table 5). Why is there a decrease in attack success rate (and a significant increase in the variance of results) from 0.05% to 0.1% poison budget? The adversary can always poison only 0.05% out of 0.1% in the latter case and thus should always perform at least as well as the former. Moreover, performance for (S=1, T=0) in the best case (~27%) is not too far off from the worst-case performance of (S=1, T=2), i.e. (~27%), despite a higher poison budget as well.\n7. Table 6 seems to incorporate benchmark results only from a few works that are not the most recent ones (both from 2019). Please update with evaluations on more recent attacks. E.g. [4] and others.\n8. Section 4.2: \"Note that the difference in results between Table 1 and these.....benchmark setup\". Please elaborate- why should saving and loading images result in a difference? If it is a precision issue, it is always possible to save images in matrix form with a specific precision instead of relying on an 8-bit map for images. \n9. Table 7: The evaluated defenses do not seem to include detection methods, where an algorithm aims to detect the presence of backdoors. \nMoreover, results indicate that it may be possible to reduce attack success rates significantly. The 'Spectral Signatures' method seems to drop attack success rates to 37%, with minimal damage in clean validation accuracy. The victim could thus always use this defense if it suspects foul play and still get away with a significantly lower attack success rate.\n\n---\n# Minor comments\n\n1. Please use a last name-first name format for your bibliography. Paper titles are also not capitalized properly.\n2. Not the best idea to refer to a specific attack in the Abstract itself (where you cannot add References). \n3. Introduction: \"...incorrect class and contain a visible trigger\" - not sure if \"often\" is the right word. A lot of works across several domains (images, audio, even text) use imperceptible perturbations.\n4. Figure 1 represents a standard backdoor pipeline and is not specific to the proposed attack. It might be better to remove the figure or use a smaller one if the authors retain it.\n5. Section 2: \"...a certain target image or set of target images\". The notion of poisoning is not specific to image data- please alter your definitions throughout the document accordingly.\n6. \"...and often fail to induce misclassification on more than one...\". Please look into the field of targeted sub-population attacks that, in fact, do manage to achieve this [3].\n7. Please number all equations, e.g., \"the one after (4)\".\n8. Section 4.1: \"...especially since some classes contain very large objects...\" Not sure how true this statement is for a dataset like CIFAR-10\n9. Section 4.1, 'The black-box setting': \"Now that we have...\" -> \"Having.\"\n10. Figure 4: What scale is the X-axis on? \n11. Figure 5: The distortions may be \"imperceptible\" but are fairly obvious upon close inspection. Data-auditing would probably make it easy to detect such patterns (which model trainers could do before training).\n\n--- \n\n## References\n\n[1] Fowl, Liam, et al. \"Preventing unauthorized use of proprietary data: Poisoning for secure dataset release.\" arXiv preprint arXiv:2103.02683 (2021).\n\n[2] Ning, Rui, et al. \"Invisible poison: A blackbox clean label backdoor attack to deep neural networks.\" IEEE INFOCOM 2021-IEEE Conference on Computer Communications. IEEE, 2021.\n\n[3] Suya, Fnu, et al. \"Model-targeted poisoning attacks with provable convergence.\" International Conference on Machine Learning. PMLR, 2021.\n\n[4] Huang, W. Ronny, et al. \"Metapoison: Practical general-purpose clean-label data poisoning.\" arXiv preprint arXiv:2004.00225 (2020).",
            "summary_of_the_review": "The paper, in its current form, seems to have a lot of minor yet non-trivial flaws. If the authors can address them substantially, I would be happy to update my rating. Even though the individual components do not provide significant technical contributions, the resulting attack seems to perform exceptionally well, as demonstrated via thorough evaluation. I would like to see an assessment against detection defenses (instead of ones that merely make updates to the data), as well as images with perturbations for CIFAR-10.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a novel data poisoning method for backdoor attacks, aiming at the models trained from scratch. The triggers used is hidden and invisible to human, with the advantage of stealthiness. Experiments show the effectiveness on ImageNet in black-box settings.",
            "main_review": "Strengths:\n1. The motivation is very compelling and meaningful, hidden triggers and the data crafting stage attack is well under-explored. We appreciate the pioneering efforts in this paper.\n2. The experiments are detailed and sufficient, especially the comparison against a list of existing defense methods.\n\nHowever, I still have some issues to discuss with the authors, which might be helpful for further revising the paper.\n1. Some explanation about special words like “poisons” in 3.3. Does it refer to “poisoned training samples” or “poison types”?\n2. Unfair setting of “model retraining”? From my perspective, it is hard for the attackers to access the victim model trained on poison data, right? Is this setting too favorable to the attacker?\n3. Clarification about evaluation results is needed in Tables 1 & 2. What is the difference between the “triggered source accuracy” and the “poison source accuracy”? \n4. The performance needs to be improved in Tables 5 & 6. The best attack success rate is still under 50%, which may consider ineffective under most settings. Also, the required poison budget size of 1% is too ideal. For a modern architecture like openAI CLIP[1], 1% means 4 million pictures. The probability of poisoning so many pictures is quite negligible.\n\nReferences:\n[1] Radford A, Kim J W, Hallacy C, et al. Learning transferable visual models from natural language supervision[J]. arXiv preprint arXiv:2103.00020, 2021.",
            "summary_of_the_review": "This paper identifies a very meaningful problem in the backdoor security of neural networks. However, the results of the proposed method need to be further improved as well as the writing. In conclusion, I think this version is not ready for publication yet.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes an invisible poisoning-based clean-label backdoor attack against image classifiers training from scratch. The authors develop their attack based on the Hidden Trigger Backdoor Attack, which combined the benefits of both poison-label attacks and clean-label attacks whereas is effective only in the transfer learning scenarios (rather than training from scratch). Specifically, the author formulates this problem as a bi-level optimization and solves it with the classical gradient alignment technique. To ensure the generalization of the proposed attack (towards different parameter initializations and model structures), the author introduces three techniques, including (1) ensemble, (2) retraining, and (3) random patch. The author also propose a poison selection module to further enhance the attack success rate. The proposed method is tested on both CIFAR-10 and ImageNet datasets with different DNNs. ",
            "main_review": "Pros\n1.\tThe topic is of sufficient significance and interest to ICLR audiences.\n2.\tThe paper is well written and easy to follow.\n3.\tTechnically, the proposed method is moderately novel. In practice, the invisible poisoning-based clean-label backdoor attack is one of the hardest yet important problems in backdoor learning. To the best of my knowledge, there is still no work that is effective especially on large-scale datasets (e.g., ImageNet).  \n4.\tThe author have also discussed the resistance to existing backdoor defenses (although the experiments are somehow insufficient), which should be encouraged.\n\n\nIn general, I enjoy the reading of this paper and recognize its significance and contributions. This judgment is mainly based on the aforementioned third pros. However, I still have some concerns, as follows:\n\n\nCons\n1.\tSome illustrations are somehow misleading. \n-\t‘The task of crafting backdoor poisons which simultaneously hide the trigger and are also effective at compromising deep models remains an open and challenging problem.’ in Introduction is probably only true for clean-label backdoor attacks. Poison-label attacks are very easily to succeed. \n-\t‘The latter scenario has proved very difficult for existing methods (Schwarzschild et al., 2020), although it is more realistic.’ in Introduction is still probably only true for clean-label backdoor attacks.\n-\tGiven the aforementioned misleading cases, I suggest to rewrite the motivation parts in Introduction. Illustrate the significance and difficulties of invisible clean-label backdoor attacks would be better.\n-\t‘Note: clean-label backdoor originally did not use l_ infty bounds, so we adjust the opacity of their perturbations to ensure the constraint is satisfied’ in Section 4.2 is not true. If you read their paper and codes carefully, you will find that their trigger is additive and therefore the modification budget is the \\epsilon defined in l_ infty ball.\n\n2.\tMissing important references. \n-\tIn Related Work, I think the author should review more (advanced) backdoor attacks. I do understand that the space is limited. Citing some related surveys (e.g. [1, 2]) would probably be a good choice.\n-\tUsing ensemble to improve generalization is not a new idea even in backdoor learning. Please cite some related works to better clarify it.\n\n3.\tProvide a pipeline figure to illustrate the main process of the proposed attack is necessary since there are many components in the proposed method. At least, the author should provide a subsection to summarize the whole process.\n\n4.\tThe experiments are insufficient and some results may have problems.  \n-\tThe performance of label-consistent backdoor attack is wired. According to their paper and my experience, this method is effective on CIFAR-10 dataset under your settings (although it may fail on large-scale datasets, such as ImageNet). Since the codes and training details of the label-consistent attack are not provided, I can hardly identify where is the problem. Probably you forget to conduct the adversarial attack before adding trigger.\n-\tThe experiments in Table 6 should also be conducted on ImageNet. \n-\tI don’t understand the results of STRIP and Neural Cleanse* in Table 7 since these methods are detection-based defenses.\n-\tThe author should also examine more advanced and different types of defenses (e.g., [3-4]) to avoid over-claim about the threat of the proposed attack.\n\n5.\tI would like to know the additional computational consumption compared with the standard training process and baseline attacks.\n\n\nI will increase my score if the author can (partly) address my concerns.\n\n\n\nReferences\n\n- [1] Backdoor Learning: A Survey. arXiv, 2020. \n- [2] Data Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses. arXiv, 2020.\n- [3] Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks. ICLR, 2021.\n- [4] Rethinking the Trigger of Backdoor Attack. arXiv, 2020.\n\n",
            "summary_of_the_review": "An interesting paper regarding an important problem but may contains some unconvincing results and statements.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety"
            ],
            "details_of_ethics_concerns": "This paper reveals a new threat in training DNNs whereas the discussion about how to prevent it is limited.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}