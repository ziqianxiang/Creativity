{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents a new benchmark called ICY, which measure compositional inductive bias of ML architectures. This is explored in the context of a signaling game, where the Sender should map an object (e.g., \"red box\") to a message (e.g., \"adab\"), and the Receiver should encoder this message back into an object.\n\nThe benchmark is obtained by transforming an original Sender/Receiver dataset in various ways. These transformations are meant to reduce the compositionality of the dataset, but the goal is that ML models can still fit to them easily. They also propose a new architecture called HU-RNN, which has an inductive bias that helps it do well on ICY. Finally, the propose a new metric for compositionality called Compositional Entropy.",
            "main_review": "Strengths:\n* Understanding the relation between what we humans perceive as \"compositional\", and whether this type of compositionality is difficult to learn for ML models is an important topic. \n* The ICY benchmark is potentially interesting since it contains a number of variations on a compositional language, allowing one to view compositional from different angles.\n\nWeaknesses:\n* Overall, In my opinion, the paper is currently too messy and the presentation is too confusing for acceptance. I provide a list of suggestions for improving the presentation below.\n* The motivation of the paper is not clear to me. The paper state \"we search for languages which models can fit to easily, but which a human might consider non-compositional\" --> why is this interesting? Can you make this more specific? Just looking at this motivation, it actually seems very easy to me to find such languages. Consider for instance a language that maps A -> 0, B -> 1, C -> 2. Now we can create examples {[ABC, 012], [AAA, 000], ..., ]. It seems very easy to me for a model to learn this, but they are not perceived compositional to humans. I suspect you actually have a much better motivation in mind, but believe you aren't explaining it properly.\n\nOveral suggestions for improving the presentation:\n* Better separate out related work. Currently there is quite some related work in Section 1, Section 2, and Section 4 (which is actually called \"Related work\", but contains almost no related work).\n* Reduce the number of sections. Currently it almost seems every paragraph has its own subsection, which doesn't read well.\n* Better motivate the ICY dataset. Why exactly are these transformations important?\n* Better introduce the dataset. Right now I miss some basic introduction to the dataset. What does it contain exactly? How big is it? How did you build it?\n* Add more details to the new architecture you are introducing: HU-RNN. That seems to be one of the innovations, but it isn't explained well. In fact, it almost has as much explanation as an RNN (which I think everybody knows, so you can just leave out the formulas there).\n\nThere are plenty of mistakes / typos / unclarities, of which I list a few below:\n\n* Section 1\n  - metrics such as posdis and bosdis are sometimes capitalized and sometimes not\n  - and measuring --> and measure\n  - only considers --> only consider\n\n* Section 2\n  - Are n_{att} and n_{val} disjoint? They are drawn from the same vocabulary so it may be worth clarifying that.\n  - It looks like all attributes have the same about of possible values, since the vocab size is n_{att} x n_{val}. Is this true? It is a somewhat odd assumption.\n  - co-ordinate --> coordinate\n  - When trained using REINFORCE you obtain a language with |G| messages. It seems this language is holistic since it has one message for each (attribute,value) pair, is that correct? It may be worth clarifying.\n  - composition function --> composition functions\n  - to be concatenation --> to be the concatenation\n  - explicitly incorporates --> explicitly incorporate\n\n* Section 3\n  - What is the o_n^{(j)} notation? How does it relate to Section 2.1 explaining the general framework?\n  - sub-message w_{j, o_n^{(j)}} --> shouldn't the first \"j\" be an \"i\"?\n  - Generally Section 3.1.1 is quite confusing\n  - in Sect 3.1.4 you hypothesize outputs from an NN can be arbitrarily rotated. Do you mean projected? This is namely what you need to do in this subsection.\n  - time-step, project it --> time step and project it\n  - In the def of an RNN there is a right parenthesis missing.\n  - In Section 3.2 you say \"Since we have c_{len} > n_{att}\", how does this follow?\n  - \"We wanted to use in addition residual entropy\" --> consider rephrasing\n  - In Section 3.3 you say you will introduce the models in the experimental section, but you actually seem to do it directly afterwards.\n \n* Section 5\n  - Figure 2 actually seems to show that TRE gives lower scores to COMP and PERM, but that seems to disagree with what is said in the text. Perhaps TRE assigns lower scores when the compositionality is higher? If so, please clarify this.\n  - Table 2: it would be useful to explain what the numbers mean in the caption as well.\n  - This section does experiments on the ICY benchmark, but the benchmark itself is never introduced, while it seems to be the main contribution of this paper.\n  - In Section 5.3 you conclude \"Thus, current neural models will tend to produce output that is not considered compositional to humans\". I find this conclusion very odd and I don't agree with it.\n\n* Section 6\n  - communicaiton --> communication",
            "summary_of_the_review": "In my opinion the paper is currently not mature enough to be accepted to this conference, and I am not convinced that it addresses a meaningful problem. I provide a number of suggestions for improvements in my main review.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Potentially harmful insights, methodologies and applications"
            ],
            "details_of_ethics_concerns": "The topic of this paper is emergent languages in AI. We as AI researchers know that this usually is not a dangerous field by itself, but a lot of sensationalist articles have appeared claiming \"AI started to invent their own language\". I think the authors should at least address this in the \"Ethics\" section, which now does not contain any reflection. While I can understand these things may seem redundant for us as researchers in the field, I think it is good to be explicit about it.",
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper introduces a series of artificial grammars, including a non-compositional holistic one and a compositional one where object attributes map to message \"words/sub-messages\". Other grammars include various types of transformations of the compositional grammar that retain the compositional information in the message but do not abide by our intuitive visual notions of compositionality nor previously-proposed metrics in the literature to measure compositionality. The authors also propose a metric to measure compositionality, called the compositional entropy (CompEnt), which measures the reduction in the entropy of an object attribute on observing the message partition (based on token location) that has maximum mutual information with that attribute in the dataset. The authors evaluate models such as MLPs, RNNs, transformers, and a new architecture HU-RNN, on the proposed grammars using CompEnt and other metrics from the literature. They show which models have an inductive bias for which grammars by studying the speed of acquisition of the grammars for the different models. Specifically, the authors show that it is possible to construct languages with compositional information which perform poorly on existing compositionality metrics but are acquired quickly by popular neural models.",
            "main_review": "Strengths:\n- I like the chosen artificial grammars, they show that neural models don't necessarily have a preference for acquiring the compositional representation that is intuitive to humans or measurable by existing metrics.\n- I appreciate the benchmarks verifying if the observed results are dependent on the number of parameters or not.\n\nWeaknesses:\n- There is some inconsistency between the models reported in Tables 2-5. Why are transformers from Table 2 not reported in Tables 3-5? HU-RNN can possibly be included in Tables 2-5 to make the same point as they do in Table 6 and the text can refer to those instead of a separate table.\n- There are no indications of the variance of these results. What are the confidence intervals? The LSTM in Tables 2-5 seem to have different results than the one in Table 6, is this due to different sampling? If so, it shows that there can be a significant variance in these numbers.\n- The paper has a few different goals, but it is not clear by the end of the Abstract and Introduction how they tie together in a coherent story.\n- There is no statement that clearly describes what \"Icy\" is. I understand it as \"a collection of grammars to evaluate compositional inductive bias of models\". Is this correct?\n\nFurther questions and suggestions:\n- Are the PROJ grammars always bijective? One can imagine based on some P that the same message can be reused for different objects. In such a case, perhaps it is expected that some models acquire this \"partially collapsed\" language faster than COMP.\n- For SHUF, in the text \"the permutation is sampled uniformly once per utterance in the language\", does it mean you take a compositional language, and then shuffle the sub-messages of every possible message randomly? Why is this grammar proposed? It seems to just be adding noise to compositional grammars and is not reported in the results.\n- For SHUFDET, my understanding is that the permutation for each possible message depends on the value of the first attribute. The example in Table 1 does not make this clear: is SHUFDET based on the last (right-most) attribute in this case?\n- In Section 5.8, how well are the senders and receivers trained on their respective grammars? If they all acquired their grammar equally well, the accuracies in Figure 3 would start at the same point. For instance, it looks like PROJ has an advantage because the PROJ senders and receivers have acquired PROJ better than, say, the COMP senders and receivers acquiring COMP.\n- Slightly more discussion in words of what equation 1 represents and why that's a meaningful metric to use would be helpful.\n- What are HUSendZ:RNN, HUSendZ:dgsend? The paper describes what HUSendZ is, but not what these suffixes indicate.\n- The Related work section is awkward. If it only has one paragraph of extra information, maybe it can find a place in Section 1 or 2 while removing the related work section altogether (since the references are already discussed in Introduction and Background).\n- For visualizing the examples in Table 1, perhaps you can add in color? For example, color every \"word\" of COMP differently, and then maintain those colors for the rest of the examples so we can visually see how the message parts are being moved around for PERM, SHUF, SHUFDET. The other grammars can remain black to indicate that the mapping is not straightforward.\n- In Section 2.1: In the text \"is the number of objects in the object space O, e.g. Lazaridou et al. (2018)\", it is more clear to write something like \"same as Lazaridou et al. (2018)\". In \"When presented to a neural network, o is represented as the concatenation of $n_{att}$ $n_{val}$-hot vectors.\", do you mean $n_{att}$ $n_{val}$-dimensional one-hot vectors instead?\n- In Section 3.1: In the 3.1.x subsubsection headers, I would suggest putting the shorthands in parentheses instead of separating by commas.\n- In Section 3.2: The text \"section 7 tre\" is a bit awkward -- TRE should be capitalized and perhaps the acronym TRE7 should be introduced beforehand and used here. Also, write \"compositional entropy (CompEnt)\" here as you are introducing this shorthand for the first time.\n- In Section 3.5: Regarding HU-RNN, generally, the \"upper\" layer comes after the \"lower\" layer in deep learning convention, so the wording is a bit confusing because in this case, the lower layer seems to be after the upper layer.\n- First sentence in Section 5.8 mentions \"end-to-end\" twice.\n- In Section 6: \"Icy\" in the conclusion has different typesetting than the rest of the text.\n- \"In our own work\" feels overused. I think you can cut \"own\" from this. You can also get away without having to justify something is \"due to space constraints\". You can present what you can and refer to the rest in the appendix, and the readers will know that it is to keep space.\n\nAnother interesting question to ask: Consider what happens when a subset of a grammar is used as training data. This would give an indication into what kind of grammars are actually likely to be acquired in the realistic scenario where the training data does not encompass the entire data space. It would be interesting to see which grammars have quick training data acquisition but retain poor generalization -- these are languages that the models have an inductive bias for that do not necessarily generalize.",
            "summary_of_the_review": "This paper provides valuable insights for the future direction of language emergence research. It helps us understand which languages models have an inductive bias for and their relationship with compositionality and the existing metrics to measure compositionality. I would be in support of accepting this paper, but there are various concerns detailed in my review that I would like to see addressed for the paper to be ready for publication.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors begin by making the case that current emergent communication models do not have a proper inductive bias towards compositional languages, but that testing this in general is expensive and difficult, despite the selection of previously proposed metrics for compositonality. They then suggest that by finding a transform that renders a language difficult for humans to learn but just as easy for machines to learn we can measure how \"apparently non-compositional\" such languages are to humans. In the background section, signaling games and accompanying notation are defined, and related work on compositionality is described, though how exactly the current work fits into the frame given is not made clear.\n\nThe authors then describe their methodology, starting with a \"perfectly compositional grammar\" and listing the transforms they apply to it, including a random baseline, basic permutations of the original,  a random projection transformation of the tokens, cumulative rotation to take advantage of the RNN's accumulated hidden state, and various shuffling transforms. Next, the metrics of composotionality are briefly mentioned and the main neural architectures of interest are described and then examples of the grammars along with the compositionality numbers for these grammars, averaged over 5 random seeds.\n\nThe main experiments of the paper revolve around showing that current neural architectures used in emergent communication work can quickly fit languages that the authors consider to be non-composotional. Further experiments are conducted to make sure that the given results are not merely a result of the number of parameters in each model, that fixing the number of training steps does not much change relative performance, and that not updating the hidden state of RNNs shows better compositional inductive bias, according to the metrics in the paper. Additionally, the authors search for models that have low bias against one of the shuffle transforms, with mixed results. Finally, they experiment with supervised training.",
            "main_review": "I agree with the authors that a benchmark for measuring the inductive bias towards compositionaly in models used for emergent communication would be quite useful. Furthermore, I agree that current metrics do not properly measure inductive bias implicit in models.\n\nHowever, this paper relies too much on unproved intuitions. Consider the following paragraph from page 4:\n\n> In natural language, whilst it is not the case that all sentences can be permuted without changing the meaning, it is the case that many sentences can be re-arranged, without much affecting a human’s understanding. For example, we feel that ‘box red’ is easily understandable, zero-shot, by a human. Thus we feel that a Receiver with similar compositional inductive bias to a human should be able to acquire quickly SHUF.\n> \n\nWhile an interesting motivation, it is unfair to use the intuition that \"box red\" *should be* understandable to a human as a basis for saying what a machine should be able to do in a completely unnatural scenario that looks nothing like natural language. The primary reason I find this problematic is that we really have no immediate evidence that this kind of compositionality is amenable to humans, especially in arbitrary scenarios. Is \"review open\" the same as open review? I think there is room for much disagreement.\n\nThe second, more important, reason why this is not appropriate reasoning is that the authors focus on that kinds of things that they claim humans *shouldn't* be able to do, for which there is even less evidence, especially since it is harder to prove a negative. There is a basic flaw in the argument of this paper: surely we believe there are plenty of machines which can capture human-like compositionality in many cases, but can also capture inhuman composotionality just as well? Yet the authors depend on the idea that a machine that has inductive bias towards compositionality should not be able to learn compositional languages that the authors consider too artificial to be human-like, without providing quantitative evidence that these languages are so artificial they violate compositional inductive bias. The \"No Free Lunch\" theorem is even brought-up, as *not* being violated by divergence among learning rates for the different categories of grammars by different models. But if acquisition speeds between the different grammars presented aren't at least partially mutually exclusive, then why does the bench mark rely on relative acquisition speeds as a metric for measuring inductive bias?\n\nIn terms of reproducibility, the paper is somewhat messily reported, making reproducing it difficult. For instance, Tables 2 & 3 do not describe the units for the numbers they report. Code will help with this, but the main body of the text should be clear and accurate without reference to the code.\n\nDetails:\n\n- Page #1—\"Tabular rasa\" → \"Tabula rasa\"\n- There is no reason for a related work section (4) that merely points at the introduce, simply include the extra paragraph in the introduction.\n- Tables 2 & 3 do not list units for any of the values.",
            "summary_of_the_review": "I feel that the motivation of measuring how quickly models fit a non-compositional grammar in order to measure compositional inductive bias is fundamentally flawed, at least in the very general way the authors claim it can be used. On top of this, the authors often rely on intuitive reasoning to justify what a given metric or experiment is measuring, intuition that sometimes seems valid, but is not solidified by evidence. To make a claim about this benchmark actually measuring compositionality, I feel that the authors would need to make that case that agents that do poorly on it are not likely to make compositional languages and that case is simply not made in this paper. Therefore, I recommend this paper to be rejected.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper seeks to use non compositional perturbations to artificial compositional data in order to measure the compositionality of emergent communication systems. By requiring the communication systems to learn non compositional tasks, we can see that a capacity to represent these data effectively will indicate a lack of compositional inductive bias. ",
            "main_review": "First, some things I liked about this paper. The results section covers the possibility of model size being the dominating factor in learning these tasks. In 5.4, the authors present evidence that the model architecture is the dominating factor by varying the number of parameters. These experiments were very satisfactory. 5.5 covers inductive bias in the context of limited data resources, and while these experiments would have benefited from a view of sample complexity that covers more than just single data size, I was happy to see it brought in. I would have liked to see some discussion of the models themselves in order to clarify where these inductive biases might come from.\n\n“comp” appears to be a linear translation. I’m unconvinced that this should be considered a compositional grammar at all. Conversely, I felt that the permuted grammar is likely to be compositional if the rules of permutation are consistent (we do consider translation generally to be compositional even though the words are reordered).\n\nThe paper seems to heavily abuse the word compositionality, without really describing what it means. Because compositionality itself has many facets, I would like to see a discussion of what particular properties the compositionality metrics are measuring. I would look into https://arxiv.org/abs/1908.08351 both for an in depth discussion of aspects of compositionality and an example of artificial tasks which measure these various aspects. Given that the paper is concerned with artificial tasks in order to measure compositionality, I would consider it highly relevant.\n\nThere were a number of references to human perceptions of compositionality (“apparently non compositional to a human”, “neither a human nor compositional metric can recognize …”) but I did not actually see any human trials. \n\nIn terms of presentation: There were several points where I struggled to understand what was happening because of undefined notation. I was uncertain about why you included the work you did in the related work section, while leaving out other crucial related work. I didn’t see this work as being particularly related to probes at all, but I did see it as strongly related to other work that uses artificial data in order to test the capacity and inductive bias of models (as just one example, https://arxiv.org/pdf/2106.01044.pdf, Examining the Inductive Bias of Neural Language Models with Artificial Languages)\n\nMINOR\n- “tabular” should be tabula\n- posdis vs PosDis consistency \n- “Section 7” on page three should be in parentheses\n- all notation should be introduced in 3.1.1 before being used\n\n\nQUESTIONS\n- What is the “attribute of the utterance” (page 4)?\n- What sort of distribution is used as p() in the mutual information computation in 3.2.1?\n",
            "summary_of_the_review": "The paper seems incomplete in its analysis and also is not specific in how it defines compositionality. The experiments are reasonably comprehensive but the particular tasks and perturbations selected are not well analyzed in terms of what they tell us about the inductive biases.\n\n**EDIT:** I have raised my score slightly to reflect significant improvement in the paper, through the addition of some human trials and the use of more explicit definitions and reasoning around compositionality. While I don't think that the paper is quite ready, I think that the significant changes show potential value in this work.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}