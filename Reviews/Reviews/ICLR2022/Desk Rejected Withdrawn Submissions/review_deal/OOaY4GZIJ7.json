{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper tackles two problems in semi-discrete optimal transport: 1. The computation of the quality of approximate solutions. 2. Mode collapse when approximate transport plans do not map any mass to some point in the support of the target distribution. The authors propose the Maximum Relative Error as a heuristic and the L1 measure between distributions to alleviate this problems respectively. The main contributions are in Theorem 1, where sample complexity is analyzed to determine the quality of the solutions, and Theorem 3 where a prescribed rate and iterations provably guarantees a desired MRE.",
            "main_review": "Strengths:\n- The paper is well written and well explained and motivated.\n- Mode collapse is an major problem in GANs and this seems to point towards a solution.\n- Theoretical results are well spelled.\n- There is sufficient elaboration in the numerical results\n\nWeaknesses:\n- There is certain lack of formality in certain statements for example: \"cannot truly reflect how well the source space is divided\" This is a rather bold statement about approximate solutions. Certainly in some cases it can.\n- It is not entirely clear how algorithm 2 induces small MRE in expectation. \n- Theorem 1, it is very strange to have high probability lower and upper bounds in expectation. specially since some of those would be vacuous in many cases.\n- I cannot really follow a story with this paper. It really shows as a number of statements towards the goal, but there is some disconnection between the contents.\n- L1 and MRE seem to be two separate problems tackled in parallel. Or maybe they are of the same flavor? Can we use one of them only instead of both of them?\n- Some of the sample complexities are stated without formal proof.",
            "summary_of_the_review": "Correctness:\n- I believe the statements are correct, but did not check every line of the proofs in detailed\n\nNovelty\n- The heuristic proposed seem to be new and directly tackling two major problems in semi-discrete OT.\n\nEmpirical Novelty:\n- Numerical analysis seem to be sufficient to support their theoretical claims.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a way to integrate the MRE in the training of the SDOT's transportation map. The proposed method SDOT-EGD is well motivated by the theoretical supporting arguments and produces favorable results on the quality of the transportation map on the simulated datasets, as well as on the GAN training on the real datasets.",
            "main_review": "**Strengths**\n\n1) The idea is simple and interesting. The authors provide relevant theoretical results which help design efficiently the SDOT-EGD algorithm.\n2) On the simulated data, SDOT-EGD seems to generate better transported distributions than other methods. On the real data, it also improves the quality of images generated by GAN and WGAN-GP, in terms of FID.\n\n**Weaknesses**\n\n1) It is not difficult to see that: $\\text{MRE} \\geq \\max_{i \\in I} \\vert v_i - p_i \\vert \\geq \\frac{\\vert\\vert v - p \\vert\\vert_1}{n}$. So, small MRE clearly implies small L$1$ distance. By design, as MRE is minimised during the procedure of SDOT-EGD, I do not really see the real interest/relevance of introducing the L$1$ distance in the paper.\n2) I don't think the outperformance of SDOT-EGD vs other methods in terms of MRE and L$1$ distance should deserve that much discussion or illustration. \n- First, MRE is a not fair measure to evaluate the effectiveness of SDOT-EGD vs other methods, simply because SDOT-EGD uses MRE as the stopping condition (so that MRE itself is minimised during the training), while it is not the case for other methods. As a result, it is not surprising to see that SDOT-EGD achieves the lowest MRE, and MRE decreases during the training, but not the case for other methods, because they optimise something else!\n- Second, the inequality above between MRE and L$1$ distance can help explain why we observe very similar behaviors of MRE and L$1$ distance.\n- For these reasons, neither MRE nor L$1$ distance is a fair measurement of effectiveness, and the comparison of convergence in terms of MRE is also not appropriate.  On the other hand, I found that other figures and tables in the appendix seem to be more relevant and interesting, thus should be presented in the main paper.\n3) The application side is somewhat limited:\n- While the results on GAN are very encouraging, only four experiments on GAN on two datasets are performed. So, it is hard to tell if the these favorable observations of SDOT-EGD can generalise well. For example, it would be also interesting to see more and diverse experiments on GAN.\n- It would be interesting to compare SDOT-EGD with other methods on other real-life tasks, for example style transfer, texture synthesis, not just on the toy examples.\n4) I'm not sure whether the theorem 3 is correct or not, due to some unclear arguments in its proof: in the equation (50) \n+ Why do we have: $\\max_{i \\in I} \\vert p_i - \\nu_i \\vert = \\vert\\vert f'(\\bar{v}) \\vert \\vert_{\\infty}$?\n+ $\\vert\\vert f'(\\bar{v}) \\vert\\vert_{\\infty} \\leq \\vert\\vert f'(\\bar{v}) \\vert\\vert$ => which norm is used on the right-hand side?\n5) Other remarks:\n- Please be precise:\n  + What does it mean by \"$\\varepsilon$-suboptimal estimate of the SDOT\"?\n  + The assumptions under which the sample complexity holds.\n- In section 2, in the line just after the equation (2), $x \\sim \\mu$ => $X \\sim \\mu$.\n- Please provide more details on the architectures used in the discriminator and generator of GAN and WGAN-GP. Are there any pretrained embeddings used in training GAN? \n- The plot 12 is not very visible, it would be better to use the log plot, or loglog plot.\n- In the proof of theorem 2, in the line just after the equation (45), can the authors be more precise about the claim: \"For optimal transport, the gradient is the difference between two distributions. Thus the gradient norm is upper bound by $2$\"?\n- It would be helpful to discuss the limitations of the proposed method, and/or the future work.\n",
            "summary_of_the_review": "- While the idea of integrating MRE in the SDOT is intuitive and well founded (especially the theoretical results help design efficient algorithm), it deserves more exploration on the application side.\n- Introducing L$1$ norm seems unnecessary.\n- Using MRE (or L$1$ distance) as an evaluation metric to compare methods is not reasonable.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the authors proposed Maximum Relative Error (MRE) and the L1 distance between the target distribution and the transported distribution to account for the limitations of the previous semi-discrete optimal transport (SDOT) algorithms. ",
            "main_review": "Here are my comments on the paper:\n\n(1) In Theorem 1, the number of i.i.d. samples from $\\mu$ depend on $1/ \\nu_{\\min}$. When $\\nu$ has several supports, $\\nu_{\\min}$ is close to zero. It suggests that we need to draw a lot of samples from $\\mu$. I think this assumption is quite restrictive in large-scale settings. Is it possible to relax this assumption further?\n\n(2) I do not understand the natures of the upper and lower bounds of MRE in Theorem 1. What if we have $\\min(\\hat{d} + 2 \\omega + 2 \\xi, (1 - \\nu_{\\min})/ \\nu_{\\min})$ larger than 1? Does that mean that the upper bound becomes non-useful? Note that, when $\\nu_{\\min}$ is small, $(1 - \\nu_{\\min})/ \\nu_{\\min}$ becomes very large. Therefore, we only have the term $\\hat{d} + 2 \\omega + 2\\xi$. To guarantee that $\\xi$ is sufficient very small, it means that the tolerance $\\epsilon*$ also needs to be very small. How can we choose that $\\epsilon*$ in practice? What if $\\hat{d}$ is already larger than equal to 1? Then, it does not matter how to choose $\\omega$ and $\\xi$ since the upper bound is already larger than 1. Finally, the number of i.i.d. samples from $\\mu$ also depend on $1/ \\xi$, which becomes very large when $\\xi$ is small. As a consequence, I am not convinced about the usefulness of the upper bound of MRE in Theorem 1. The authors need to provide more explanation on that upper bound.\n\n(3) For the time complexity of the algorithm 1, in practical applications, we have $\\nu_{\\min} = \\frac{1}{n}$. From the results with the upper bounds of $L_{1}$ distance, we need to choose $\\epsilon* \\leq C/ n$ such that the rate of the L1 distance is parametric. It demonstrates that the time complexity becomes $O(nd/ ((\\epsilon*)^2 \\nu_{\\min}) = O(n^4 d)$, which is not appealing as $n$ can be very large. Given that insight, I am not sure about the practicality of Algorithm 1. \n\nNote that, if we would like to reduce the time complexity, we need to choose $\\epsilon*$ smaller, but at the cost of a worse L1 bound. For example, if we choose $\\epsilon* = n^{-\\alpha}$ for $\\alpha < 1$, the rate of L1 distance becomes $n^{-\\alpha/2} and \n\nthe time complexity of Algorithm 1 improves to $O(n^{2 + 2 \\alpha} d)$. As we let $\\alpha$ goes to 0, i.e., we choose $\\epsilon*$ as fast as logarithmic function of $n$, the L1 rate becomes $1/ \\log(n)$, which is impossible to use while the time complexity is $O(n^2 \\log n^2 d)$. As a consequence, from the statistical and computational trade-off, Algorithm 1 does not seem appealing to me.\n\n(4) Both Theorem 2 and Theorem 3 are standard. One thing that I do not like is the dependency of $C_{\\infty}$ on the diameter of $X$, which is quite undesirable. Can it be removed with a more fine-grained analysis?\n\n(5) One major issue that may hinder the understanding of readers is that the paper has too many notations, such as $\\epsilon, \\gamma, \\delta, \\xi$, etc. I need to go back and forth to check the meaning of these notations to make sure I do not miss any important points. The authors may consider improving the paper on this front. \n\n(6) I am not so sure why Algorithm 2 is useful at all. From the argument in my point (3), the time complexity of Algorithm 2 is $O(d n^4 C_{\\infty}^2/ \\epsilon^2)$ when $\\nu$ has uniform weights. If we choose $\\epsilon$ to grow with $n$, that time complexity becomes even worse. The authors may consider explaining that issue in the rebuttal.\n ",
            "summary_of_the_review": "In my opinion, the algorithms proposed in the paper are not very practical, especially for large-scale settings. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes to use Maximum Relative Error (MRE) and $\\mathbb{L}_1$ to evaluate transportation maps from semi-discrete optimal transport solvers. In particular, MRE will penalize a transport map that contains 0 probability value in the transported distribution. Moreover, the paper introduces Epoch Gradient Descent (SDOT-EGD) algorithm which can guarantee an arbitrarily small expected value of the MRE. Experiments on toy examples and GANs are conducted to demonstrate the performance of  SDOT-EGD that is evaluated by proposed MRE and $\\mathbb{L}_1$ (using estimated lower-bound and upper bound).",
            "main_review": "**Strengths**:\n- The paper is well-written, easy to follow.\n- Introducing a new method to evaluate transportation maps of semi-discrete optimal transport is definitely a good contribution.\n- The SDOT-EGD can guarantee to produce a transportation map that has an arbitrarily small expected value of the MRE while still having a comparable convergence rate.\n- The qualitative comparison and quantitative comparison on toy examples are convincing that SDOT-EGD gives a more appealing transportation plan than conventional methods.\n- The generated images on CelebA-HQ and Xray are good.\n**Weaknesses**:\n- The authors do not explain why we need to use both $L_1$ and MRE. What happens if we use only $MRE$ or only $L_1$?\n- The experiments on GANs are quite incremental. SDOT is only used for estimating the transportation from uniform noise to the hand-craft features of images. The obtained transportation map then is utilized for sampling mini-batches in Wasserstein GAN. The whole procedure is quite black-box since it is not clear which type of divergence is minimized for the generative model. Could we train GANs directly with semi-discrete OT?  Why do authors use the generative models that were proposed in AE-OT and AE-OT GAN?\n- The authors do not compare SDOT-EGD with other SDOT solvers on generative modeling. Why?\n- The memory complexity of SDOT at a time seems to be at least $O(nd)$ where $n$ is the number of supports of the target measure. What happens if $n$ is a few million? Could we do stochastic optimization here (random sampling from the discrete measure)? In the paper, the largest $n$ is about 30000 in CelebA-HQ which is not really large for me. Also, reducing the dimension of data manually to the size of noise is quite tricky in general, namely, losing information could harm the mapping between noise and data. \n- The source code has not been published yet, so, I could not check the algorithms in the paper.\n**Questions and Comments**:\n-  How is $C_\\infty$ computed in practice?\n-  Could we derive a notion of \"sub-sampling\" for SDOT when $n$ is large? Is is related to recent papers [1], [2], [3], [4], [5].\n- The experimental part for GANs should be written in more detail.\n\n[1] \"Minibatch optimal transport distances; analysis and applications\" Fatras et al 2019\n\n[2] \"Learning with minibatch Wasserstein : asymptotic and gradient properties\" Fatras et al 2020\n\n[3] \"Unbalanced minibatch Optimal Transport; applications to Domain Adaptation\" Fatras et al 2021\n\n[4] \"On Transportation of Mini-batches: A Hierarchical Approach\" Nguyen et al 2021\n\n[5] \"Improving Mini-batch Optimal Transport via Partial Transportation\" Nguyen et al 2021",
            "summary_of_the_review": "In summary, the paper is a great contribution to the development of semi-discrete optimal transport, namely, understanding the quality of the transportation maps and proposing new solvers. However, the experiments in the paper are quite incremental and safe. I believe the paper can become stronger if the authors consider improving the practical aspect. So, I recommend a weak reject. I am willing to change my score based on the response of the authors.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}