{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a variant of sharpness-aware minimization (SAM) which accelerates the training process by exploiting the merit of large-batch training. Based on the deeper observation on the training behavior of gradients during SAM, the authors devise a sporadic update rule and boost it by adding a layer-wise adaptive learning rate for SAM. Those proposed methods are called LookSAM and Layer-wise LookSAM correspondingly.\n",
            "main_review": "**Strengths**\n\nThe proposed method tackles the scalability of sharpness-aware minimization and it helps to accelerate the training of large-scale models. This kind of work is critical for our community in the era of so-called big models and global warming and climate change.\n\n**Weaknesses**\n\nI conjecture that the paper is not finalized yet and has room for improvement.\n\n\n*The contribution of the paper can be displayed in a better manner*\n\nI think that table 1 can be displayed in a better way. How about plotting the accuracy over training times or GPU hours? Training time is not equivalent to each other. it makes me hard to guess the benefit of the proposed method. I suggest that referring to figure 3 in [Deformable DETR] which plots the epoch-wise validation performance.\n\n*Is large-batch training helpful?*\n\nIn table 2, the performances on the smallest batch size 4k achieve the best performance for all algorithms. What are the performances of smaller batch sizes than 4k? And what is the meaning of training accuracy?\n\nThe performance of SAM in table 1 is 79.3/103.1s and LAMB + SAM 78.6. I think that table 2 with AdamW is also needed for a fair comparison.\n\n*The other datasets and architectures*\n\nIt proposes an optimization scheme, which means that the raised statement is generic to the other dataset and architecture. However, the proposed scheme is only validated at the combination of ImageNet + ViT. Can the proposed method be applied to the other scheme? CIFAR10 and CIFAR100 are too small datasets. Does the proposed optimization scheme still work on the other architectures like ResNet or DenseNet families?\nFurthermore, does the proposed algorithm work with a pre-training method based on contrastive learning, such as [SimCLR, MoCo]?\n\n**Question**\n\nWhy does large-batch training archieve weaker performance than small-batch cases? Do you think that LAMB+Look-LayerSAM 32k model with 77.1 on table 2 can achieve better performance by further training with smaller batch size?\n\n[Deformable DETR] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai, Deformable DETR: Deformable Transformers for End-to-End Object Detection, ICLR 2021\n\n[SimCLR] Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, A Simple Framework for Contrastive Learning of Visual Representations, ICML 2020\n\n[MoCo] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, Momentum Contrast for Unsupervised Visual Representation Learning, CVPR 2020\n",
            "summary_of_the_review": "Though the proposed idea and concept seem to be good for large-scale training, the shown experiment is not in a good form and lacks appropriate ablations. Refer to the main review section.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a method LookSAM to reduce the training cost of SAM. The basic idea of LookSAM is to reduce calculations of sharpness calculations in SAM from once per iteration to once for every k iteration. The authors also propose LookLayerSAM, which combines the layer-wise adaptive learning rate scaling method from LAMB, to train DNNs with a large batch size. The proposed LookSAM can effectively reduce the computations of SAM. The authors verify the effectiveness of their method in the ImageNet dataset.",
            "main_review": "Strength:\n1. The authors focus on the efficiency issue of the SAM, which is a practical problem to SAM. The proposed LookSAM significantly increases the training efficiency with comparable performance than SAM. \n2. This paper is well-written and easy to follow. The authors provide sufficient experiments in CIFAR10/100 and Imagenet datasets to support their claims. \n3. The theoretical part of the appendix is appreciated. The authors also provide complete experimental settings in the appendix.\n \nWeakness:\n1. The motivation of this paper is not clear. The authors seem to solve the efficiency issue of the SAM. But they discuss the large-batch training first and then introduce the efficiency issue of SAM. Lastly, they solve the large-batch training issue of SAM. The vague motivation makes the connection between LookSAM and LooklayerSAM weak. Moreover, the adaptive layer-wise learning rate is not the contribution of this paper. It makes the proposed LooklayerSAM heuristic and redundant. \n2. The derivation of LookSAM is based on the empirical observation, $g_v$ changes little in the following iterations. The observation in Figure 2 is not convincing, as the metric to describe the gradient difference is not clear. Cosine similarity is better to describe the difference. Lastly, the supporting theoretical content of Eq(5) is not convincing. The magnitude of RHS is correlated to the gradient changes, which is also verified by the empirical observation in Figure 2. To my understanding, this is a circular argument. \n3. The assumtion of Eq(4) is that $g_v \\perp g_h$. However, this assumption is not promising. The complete proof in A.1 gives the assumption directly.  This assumption leads to the main contribution of this paper but is questionable. By the way, the ref of the line above eq(5) is wrong; it should be A.1 instead of A.5. \n4. The main contribution of this paper should be LookSAM, but the authors emphasize LooklayerSAM which only combines the layer-wise adaptive learning rate scaling method and cannot be considered as the contribution of this paper. As for the contribution of LookSAM, the theoretical derivation is a circular argument. Lastly, LookSAM may result in performance degradation, as the  SAM's accuracy results in Table 1 are worse than the reported results in [1]. \n5. The authors should also report the results of  SAM + LAMB + layer-wise lr in Table 4.  I am worrying that the improved accuracy of LooklayerSAM is contributed by the layer-wise lr and LAMB optimizers. \n\n[1] Chen, Xiangning, Cho-Jui Hsieh, and Boqing Gong. \"When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations.\" arXiv preprint arXiv:2106.01548 (2021).",
            "summary_of_the_review": "To summary, the main contribution of this paper is the improved efficiency of LookSAM. However, the derivation of LookSAM is based on empirical observation and a strong assumption. Secondly, the reduced computational overhead of LookSAM may lead to an accuracy drop. If the authors can answer the weakness in the main review, I will vote for acceptance of this paper. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work proposes a technique called LookSAM, which reduces the computational overhead of the Sharpness-Aware Minimization (SAM) and maintains the SAM’s benefit in generalization. LookSAM reuses $g_v$ (the difference between the SAM gradient vector and its projection onto the mini-batch loss gradient vector) and skips the additional gradient computation required for the original SAM for a certain number of steps (e.g., 5, 10). In addition to LookSAM, a layer-wise adaptive version of SAM, called LayerSAM and its combination with LookSAM (Look-LayerSAM) are proposed. The proposed methods are tested in training various sizes of Vision Transformer (ViT) models on ImageNet-1k classification with the batch size of {4k, 8k, 16k, 32k, 64k}. In each training setting, Layer-LookSAM (combined with LAMB optimizer) takes a computation time similar to AdamW (~x1.25) while matching or exceeding the validation accuracy by the original SAM.\n",
            "main_review": "Strengths\n- Simple yet practically useful idea to reduce the computational overhead of SAM while preserving its benefit on generalization. \n- This is the first empirical demonstration that shows that the validation accuracy of a ViT model trained with an extremely large batch size (up to 64K) can be improved by using (approximate) SAM (and strong data augmentation).\n\nWeaknesses\n- The motivation for accelerating SAM in large-batch training is not well explained.\n    - [Why SAM?] This work is motivated by the previous work [1] that showed that SAM can improve the validation accuracy of ViT models without using strong data augmentations. Although [1] reported that training with SAM showed a slightly better validation accuracy than training with strong data augmentations (mixup and RandAugment), it is still unclear whether it is worth the cost of computing and tuning SAM (and LookSAM). Instead of paying an additional computation cost per epoch for SAM (and LookSAM), we can train for longer epochs using strong data augmentations. Even if the computational cost per training is the same for both, the latter requires fewer hyperparameters, and the tuning process could be more straightforward (i.e. fewer training trials). Therefore, for motivating the acceleration of SAM, I believe it is necessary to verify that strong data augmentation (with longer training epochs) is not sufficient to achieve a similar validation accuracy to SAM. \n    - [Why SAM in large-batch?] The authors state that the additional gradient computation in SAM “will be a bottleneck in large-batch training”. But if the number of training epochs (i.e., the total number of examples to compute gradients) is the same, then the mini-batch size has nothing to do with the number of additional gradient computations in the SAM. In this work, however, a common number of epochs (200) is used regardless of the mini-batch size, and large-batch training shows lower validation accuracies. It is known that data efficiency (test accuracy vs. the number of examples processed) decreases as the mini-batch size is increased [2]. In other words, a larger number of epochs is required to achieve the same accuracy in large-batch training as in small-batch training. In this case, the additional gradient computation in SAM can indeed “be a bottleneck in large-batch training”. Therefore, I recommend including a discussion of data efficiency in introducing the motivation for speeding up SAM in large-batch training. In addition, it is interesting to see if large-batch training with a longer number of epochs can achieve the validation accuracy of small-batch training (otherwise, it is difficult to state that you “are able to train ViTs” “with a 64k batch size”).   \n- Lack of justification for the idea (reuse of $g_v$) on which the proposed LookSAM is based.\n    - First of all, Figure 2 (Difference of gradients) requires to contain more information.\n        - What is the definition of the “Difference”? Is this the relative norm (e.g., $||g_t - g_{t-5}||/||g_{t-5}||$)?\n        - What are the training setting (e.g., network, batch size, dataset) and model performance (e.g., training loss, validation accuracy)? \n            - The meaning of the number of steps depends on the mini-batch size, learning rate, model performance, etc. (In the extreme case, if this plot represents ImageNet-1K training with a mini-batch size of 1, the training would be less than one epoch.) \n    - The authors explains the reason why the “difference” (I assume it is the relative norm) of $g_v$ is relatively small by stating that “As calculating SAM gradients leads to a relatively flat part of the region, the second order derivative is very small” with equation (5).\n        - In order to support this, I think at least observation of the value of the second-order derivative during training is required.\n        - Training with SAM is expected to reach a flat region, but that does not necessarily mean that it is in a flat area (i.e., where second-order derivatives are negligible in equation (5)) throughout the training. Hence, reuse of $v_g$ is not always justified by this explanation. \n    - If the “difference” means the relative norm ($||g_t - g_{t-5}||/||g_{t-5}||$) and the difference is small throughout the training, adjusting the norm of $g_v$ (i.e., $g_s=g+\\alpha||g||/||g_v||$) looks unnatural. Instead, it seems that bringing $g$ ($-\\nabla L(w_t)$) closer to $g_h$ would be a more effective use of the invariant $g_v$ (i.e., $g_s=\\beta g+g_v$) for reproducing the original SAM gradients. In any case, in addition to the final validation accuracy, I believe it is also necessary to test if each LookSAM step (with the stale $g_v$) can reproduce the original SAM step. Otherwise, we cannot get an intuition to explain the success of LookSAM (replicating the accuracy of SAM at a low cost), and we cannot determine whether LookSAM is universally useful even if SAM is universally useful. \n- The search space for hyperparameters (especially for baseline optimizers) and their sensitivity to accuracy are not reported.\n    - From the manuscript, it is unclear how much the hyperparameters for each setting are tuned.\n    - Existing work [3] shows that carefully tuned Adam and Momentum SGD can match or exceed the results of LARS and LAMB in large-batch training of ResNet and BERT models. This result suggests that the layer-wise adaptive method is not essential for large-batch training. I do not mean to deny the usefulness of LayerSAM or Look-LayerSAM based only on the results of [3], and since [3] is a fairly recent preprint paper, I do not expect it to be included in an ICLR’22 submission paper (it is recommended to include a discussion about it in the revised version though). However, I believe it is necessary to report the hyperparameter search space and the sensitivity of the hyperparameter to the validation accuracy in order to argue for the usefulness of the proposed methods (including LookSAM).\n\nOther comments\n- Title: I do not think “training vision transformer in minutes” is appropriate to represent the result which takes “0.7 hours” and is >4% lower accuracy than the previous result [1] with the batch size of 4k. \n- Ba et al 2017 [4] instead of Martens & Grosse 2015 should be cited as an example of “recent works (that) attempt to use adaptive learning rate to scale the batch size for ResNet-50 on ImageNet.“ The latter proposes a second-order optimizer and tests it in a small setting (e.g., MNIST) while the former applies it to an ImageNet scale.\n- Equation (5): the outermost $||$s should be larger.  \n- The number of the processors, ViT architecture type, and the validation accuracy should be reported with the statement “Look-LayerSAM can achieve ~8x speedup over the training settings in Dosovitskiy et al. (2020) with a 4k batch size, and we can finish the ViT-B-16 training in 0.7 hour.”\n- Figure 1 (AdamW vs SAM vs SAM-5): What is the batch size?\n- Figure 3 (Visualization of LookSAM): \n    - It is helpful if the $\\theta$ is shown in the figure.\n    - Figure 3 is referred to earlier than Figure 2. Switching their positions might be better. \n- Algorithm 1: I believe $sign(g)$ should be $g/||g||$ according to the equation (2). \n\nReferences\n- [1] X. Chen et al. When vision transformers outperform resnets without pretraining or strong data augmentations. https://arxiv.org/abs/2106.01548\n- [2] C. J. Shallue et al. Measuring the Effects of Data Parallelism on Neural Network Training. https://arxiv.org/abs/1811.03600\n- [3] Z. Nado et al. A Large Batch Optimizer Reality Check: Traditional, Generic Optimizers Suffice Across Batch Sizes. https://arxiv.org/abs/2102.06356\n- [4] J. Ba et al. Distributed Second-Order Optimization using Kronecker-Factored Approximations. https://openreview.net/forum?id=SkkTMpjex\n",
            "summary_of_the_review": "This work provides an empirical data point that further supports the effectiveness of SAM in training Vision Transformers and ResNets (even in large-batch training), and the proposed methods (LookSAM, LayerSAM) are useful ideas to improve the speed and accuracy of SAM.\nHowever, the motivation for using/accelerating SAM in large-batch training, cf. strong data augmentations (fewer hyperparameters) or small-batch training (>4% better accuracy), is unclear, and the general usefulness of the proposed methods is questionable due to the lack of a convincing explanation and detailed analysis. \nSo I put this work marginally below the acceptance threshold, but I am open to discussion with the authors.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces LookSAM, an efficient algorithm accelerating ViT's training.  The motivation of this work is to utilize and improve the SAM algorithm to optimize the performances of vanilla training baseline (LAMB) meanwhile achieving a competitive speed. Experiments conducted on ImageNet-1K demonstrate the effectiveness of this paper.",
            "main_review": "1. The main point that concerned me is the real contribution to the HPC field since quantized comparisons in this work seem not very outstanding. There is not an obvious gap (even more time-consuming) between training efficiency with baseline method (LAMB). \n2. The number of baselines is very limited. Two years passed since the LAMB was proposed, the experimental setting lacks other LAMB-inspired methods. Please give more recent methods joining in the comparison. \n3. And also, the reviewer is confused about the setting of training. It is unclear whether all baseline methods are training with a fair setting.\n\nOthers issues.\n1. Lacks of enough visual samples. Such as the loss changes during the training. If the latter sections can supplement more visual details, this paper will be more convincing.",
            "summary_of_the_review": "Pros.\n1. The start point that introduces a better training setting is valuable in the industry.\n2. The authors provide some interesting opinions and point out that SAM cannot be applied to large-batch training. So it is reasonable to combine recent SAM and LAMB to a better version.\n3. This paper is written clearly, the reviewer can easily get the core contribution.\n\nSuggestion.\n1. The authors organize splendid discussions about the technology itself. However, it is better to talk about the potential application in the real industry. For example, is that helpful when utilizing the optimization method in large-scale pre-training, such as multimodal Kaleido-BERT[1] using in real-life Ecom website, which may train more than 50 million image-text pairs. What are the differences with [2] when applying your method to real-life practices?\n\n[1] Kaleido-BERT: Vision-Language Pre-training on Fashion Domain. CVPR2021\n[2] M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining. ArXiv2021\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}