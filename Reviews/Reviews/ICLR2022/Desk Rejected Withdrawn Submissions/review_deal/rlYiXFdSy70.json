{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Poster)",
        "comment": "This paper proposes a new approach to goal-oriented RL (GoRL) which constructs a state-transition graph from experience and uses this to guide exploration. Compared to prior approaches, this work innovates on (1) how subgoals are selected and (2) how relevant experience is sampled from the replay buffer. This approach outperforms a number of baseline methods across a fairly wide range of environments. The paper also includes extensive ablation and generalization studies.\n\nThe reviewers agreed the problem tackled by the paper was important, and were impressed by the empirical evaluation. Several reviewers (6obU and Sjwy) appreciated the theoretical grounding of the method as well, and generally found the approach to the problem compelling, with Reviewer 6obU writing that “defining the optimal goals to explore on the graph by looking one-step (or one-episode) ahead is a very interesting idea” and Reviewer EMRL noting a strength of the paper was a “goal generation method that explicitly considers only having access to a partial graph of the MDP”. The main concerns raised by the reviewers had to do with clarity and the simplicity of the experiments; however, the reviewers felt these were sufficiently addressed by the rebuttal. I agree the approach seems well-motivated and interesting and recommend acceptance as a poster."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper improves exploration and learning efficiency in Goal-oriented Reinforcement Learning (GoRL). In particular, a new framework graph-enhanced GoRL (G2RL) is proposed by leveraging the state-transition graph. (1) G2RL produces under-explored goals (or goal states) by planning on the “one-episode ahead” graph to conduct directed and effective exploration. In the actual implementation, the goal is generated by a differentiable generator trained in a hindsight manner.  (2) In addition, G2RL proposes to use “relevant trajectories” based on the graph neighborhood information to improve the efficiency of policy learning. By combining (1) and (2), G2RL demonstrated strong performance on various environments. Theoretical analysis is provided on deterministic MDPs to support the design of the algorithm.          ",
            "main_review": "Strengths:\n+ Overall, the paper is well-written, organized and technically sound. The motivation is clear, and I believe exploration in GoRL is an important and practical problem. \n+ Defining the optimal goals to explore on the graph by looking one-step (or one-episode) ahead is a very interesting idea and makes sense to me. Major claims and the algorithmic design are well supported by both theoretical and empirical results. \n+ The paper provides comprehensive experiments on various settings to demonstrate the effectiveness of G2RL, including both discrete and continuous tasks, environments with vectorized state inputs and imagery state inputs, environments with static structure and changing structure. In addition, ablation studies are also included on various aspects of the algorithm. The empirical performance looks promising and convincing.   \n\nConcerns:\n1.\t[Clarity] Some parts of the paper are not clear enough to me. Please see the list of items below for details. \n-\t1.a. How is Certainty of State defined for problems with continuous action spaces? If K-bins discretization is applied, will that result in too many states being uncertain? and too many possible states being considered in the boundary $\\partial \\mathcal{G}_t^e$? Any issues observed in the experiments? \n-\t1.b. I understand the motivation of using out-degree to define the boundary, but empirically why not simply using the state/node visitation counts? E.g., consider the most infrequently-visited 10% or 20% nodes/states in the graph as the boundary? What might be wrong with using visitation counts?   \n-\t1.c. In Definition 2, how to select the goal $g_{e+1}$? I assume it is selected based on $argmax_s V(s, g_{target})$ for $s \\in \\mathcal{G}^{e+1}$, but not 100% sure. It would be helpful to clarify this in the main paper. \n-\t1.d. On page 5 last paragraph, it is not clear to me that how the features $f_1, …, f_N$ for each group $C_n$ are encoded. Could you elaborate more on this (or provide one example) and add to the paper?  \n-\t1.e. Equation (5) confused me and unfortunately simply saying “$V(s, g_{e-1})$ used here is consistent with $V(s, g_{e+1})$ in Eq. (2)” is not very helpful to me. Could you elaborate more? I would suggest add more clarification to Eq. (5) in the paper. \n\n2.\t[Experiments + Baselines] \n-\t2.a. I like the design of Stochastic Ant Maze that changes the structure of the problem. It would be more interesting to see how well G2RL can handle different types of stochasticity. For instance, one could introduce Gaussian noises (with certain probability) to state transitions of the MDP. It could be interesting to add as an additional experiment. \n-\t2.b. As mentioned in 1.b., since visitation count (or its proxy) has been widely used in the RL community as intrinsic signals for exploration, it would be helpful to compare the proposed Certainty of States with the simple visitation count. To my understanding, it suffices to only change the candidate states in $\\partial \\mathcal{G}_t^e$ to include the 10% or 20% infrequently-visited states/nodes by counts, and the rest of G2RL can remain the same, for example either of the two grouping strategies may be used. It can be added as an additional baseline/ablation study. \n-\t2.c. I wonder how the performance of G2RL would compare against HRAC [1]? (which is also cited several times in the paper). HRAC improves the reachability of goals by considering only an adjacent region, leading to improved learning efficiency for HRL. However, unlike G2RL, the exploration in HRAC is not directly guided towards the boundary. Comparing against HRAC in some of the environments can be helpful to further demonstrate the effectiveness of graph-enhanced guided exploration by G2RL.                                    \n\nMinor comments: \n-\tEq. (1), $V^{\\pi}(s, g)$ should be summed over $a \\sim \\pi$; if here $\\pi$ represents a greedy deterministic policy, please specify before Eq. (1).  \n-\tEq. (12), line 3, a typo, I believe should be $\\mathcal{G}^{e-1}_1$.    \n \n\n[1] Zhang, Tianren, et al. \"Generating Adjacency-Constrained Subgoals in Hierarchical Reinforcement Learning.\" arXiv preprint arXiv:2006.11485 (2020). \n\n\n\n",
            "summary_of_the_review": "Exploration in goal-oriented RL is an important problem. The proposed framework G2RL improves exploration and learning efficiency by generating goals from the next episode graph, which is a new and interesting idea. Comprehensive results are provided to support the algorithm and major claims made in the paper. My main concerns are about the clarity in some parts of the paper and some additional baselines (please see Concerns above for details), which I believe can further improve the paper if addressed. Overall, I am satisfied with the submission and vote for accept.  \n\n\n\n\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper presents a goal-oriented RL framework based on state-transition graph. Compared to prior work like SoRB, GoalGAN, HIRO, the proposed method G2RL demonstrates higher success in exploration on discrete and continuous control benchmarks. \n\nThe paper addresses that inefficiency of randomly sampling past trajectories for policy learning by (1) modeling state-transition graph (2) relevance sampling of the nodes that are the current state or its neighbors (assuming that they have close goals or similar goal-oriented policies). The goal is sampled from the candidate set, selected by self-attention over sets of neighboring and uncertain nodes.\n\n# Key ideas\n\n1. Differentiable goal generator can be trained with optimal goals for supervision by planning algorithm,\n2. (avg) out degree of the node in state-transition graph (also called certainty of a state in the paper) shows how much has the state been explored.\n3. sampling relevant trajectories based on goal proximity.\n",
            "main_review": "# Strengths\n\n\n- Compares with existing work on Goal-oriented RL showing that the proposed approach learns faster and has at par asymptotic performance in different experimental setups such as Maze, AntMaze, Fetch push, Fetch obstacle push and Vizdoom.\n- Ablation analyses to discuss the impact of relevance sampling, group selection, discretization. Also show generalization to larger sizes of AntMaze, complex AntMaze with obstacle, and unseen environments.\n- Discusses generalization performance in larger sizes of AntMaze, complex AntMaze with obstacle, and unseen environments.\n\n# Clarification\n\n- Does this method assume that every state would have same number of out-degree? What if this method is applied to environments with obstacles - where a state node near or containing the wall will not have the same out-degree as a state node in the middle of a corridor? How would the goal proximity be calculated in this case?\n- In the appendix, the goal generated are visualized and compared to that with HER. But not discussed with SoRB, which seems as the strongest baseline.\n- The theorem implies that the \"candidate set for each goal generation can be limited to the boundary, which can reduce the goal space with optimality preserved\". This idea seems similar to frontier-based exploration ([B. Yamauchi](https://ieeexplore.ieee.org/author/37354780800) 1997 [https://ieeexplore.ieee.org/document/613851](https://ieeexplore.ieee.org/document/613851)). Can you please clarify the novelty of this? How this theorem motivates the proposed group selection (as the group selection extending to neighbor nodes seems counter-intuitive to the theorem)?\n- Can you clarify in the introduction \"In such a way, our method is scalable, as it only needs to build a fix number of groups.\"? Is it regarding the hyperparameter choice in group selection? How does it make the method more scaleable?\n\n# Minor suggestions\n\n- Typo in Theorem 1 (Goal Optimality ~~Preverved~~ Preserved in Graph Boundary)\n- What does *next episode graph* mean in the definition 2 of Optimal Goal?\n-  \"We then build these groups by extending scales in two different perspectives to include more states\" - what does \"scales\" mean here?",
            "summary_of_the_review": "I think that the paper provides impressive empirical performance as compared to the baselines on some continuous control tasks in locomotion in AntMaze, Vizdoom, and manipulation with Fetch in simulated scenarios. The paper is well-structured yet needs minor typing corrections and clarifications.\n ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this work, the authors describe a novel graph-augmented RL method for goal conditioned tasks. The results shown outperform a number of baseline methods. The method is well motivated and theoreticaly grounded.\n\n\n\n",
            "main_review": "Main Review:\nStrengths:\n\nThe method is well motivated and clearly written.\nThe theoretical analysis is detailed and provides performance gaurantees.\nA detailed abalation study compares the performance of the approach in number of experiment configurations.\n\nWeaknesses:\n\n1. While the method may be able to generalize to new goals in the same environment configuation, I do not see how this method would be applied when generalizing the unseen environments which where much Deep RL research is currently applied. \n2. The environments where this method is evaluated are rather simplistic, it would be good to see an evaluation in a photo realistic simulator such as Habitat / Gibson / Matterport.\n3. How does the method account for visual aliasing in continuous environments?\n\nOther comments: SPTM is mislabelled in figure 4. I stated this in my previous review and it is still incorrect.",
            "summary_of_the_review": "I had the fortune to review this paper in my NeurIPS batch, unfortunately it did not pass. There were a number of things the authors stated they would include in the next revision, such as comparisons in a photo realistic simulator such as Habitat, which they have not done. \nI look forward to the authors explaination of why this is the case in the rebuttal. \n\nUPDATE AFTER REBUTTAL: I thank the authors for their detailed replies and additional experiments, they have demonstrated that this approach is applicable in photorealistic 3D environments and have addressed many of my concerns. I raise my rating to 6. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a goal generation procedure in order to improve goal-oriented reinforcement learning (GoRL) by relying on the proposition that the degree of exploration of a state/node can be related to its out-degree. They propose a solution that leverages this proposition while takin into account that we do not have access to the full graph. The paper also proposes to a method sample from the replay buffer, named relevance sampling, with the goal of improving the sample efficiency. The experimental setup considers continuous control domains as well a a visual navigation experiment on VizDoom. In these experiments, modest improvements are presented when compared to other GoRL methods. The authors also provide an ablation study to evaluate the effects of different components of the algorithm.",
            "main_review": "Strengths\n------------\n-Goal generation method that explicitly considers only having access to a partial graph of the MDP\n\n-Comparison with many baselines is provided\n\n\n\nWeaknesses\n-----------------\n-Clarity and presentation of the method is hindered by heavy notation and theory\n\n-The derivation relies on quite a limited definition of uncertainty and strong assumptions on the MDP\n\n-Ablation analysis doesn't show critical improvement\n\nDetails\n---------\nGoRL is an important are of RL and has to the potential to improve exploration as well as the sample efficiency of learning agents. The papers proposes to work on this problem by considering explicitly the underlying graph of the MDP. Although this can be intractable in practice and might require approximation, for the specific environments presented in this work, the method seems to be well adapted. The idea is intuitive and could work in various settings, although the presentation itself is convoluted and can definitely be improved. Section 2.2 and 3.1 includes definitions, propositions which really don't add much to clarity and in fact hide some of the fundamental assumptions. For example, the definition of state uncertainty is really limited in scope and very specific to the particular setting the authors tackle. Similarly, regarding the whole derivation, it is only later in the remarks that the fundamental assumption of deterministic dynamics is revealed. This should be clearly stated at the beginning. \n\nThe goal generation idea is intuitive and can be explained clearly, I am wondering why the authors choose not to do so. In my opinion that would add to the quality of the paper rather than diminish it. That being said, the paper considers a specific setting, which is also okay, however I would appreciate to see a simple toy example to make more clear in what setting would the method work, and also a toy example where it would not work (when some of the assumptions are violated). Some of the details of the method, such as \"extending in scales in different perspective\" for the groups is very obscure and would clearly gain in importance through a toy example.\n\n\nThe experimental results show modest improvements in terms of final performance, but show good improvements in terms of sample efficiency. In particular, in the stochastic ant maze domain, the methods works better than in ant maze, which goes against the assumption of the theoretical derivation. Could the authors expand on this difference?\n\nWith respect to previous work, there seems to be three differences: a way to generate the graph, a way to sample the goals and a way to sample from the replay buffer. However, when looking at the ablation study, its unclear which of these contributions is fundamental to the performance of the agent. There is indeed a slight drop in performance in Fig5abc, but its not that conclusive. What exactly brings improvement then? Do the baselines also use positional observations to construct the graph? The authors mention that previous work introduces distance function, but the current method also seems to rely on a distance function for the VizDoom domain. \n\nFinally, the authors do cite some important recent work on GoRL, but seem to miss some important landmark papers such as (Schaul et al. 2015, Kaelbling, 1993) only to name a few. Additionally, important recent work on graph generation is also missing (Jiang et al. 2021, Klissarov et al., 2020). Each of these papers proposes with different strategies to approximate the graph. In particular the latter is related to the current work as it also tries to improve sample efficiency. I would be interested in seeing a comparison with it, although I don't think its necessary as the authors include many baselines already.\n\nMinor points:\n1. What happens if there are very valuable states a few layers after the graph boundary, would the algorithm be able to find these?\n2. When generating episode e+1, is the agent using the policy from episode e? \n\n\nSchaul et al. 2015, Universal Value Function Approximators\n\nLeslie Pack Kaelbling 1993, Learning to achieve goals.\n\nJiang et al., 2021 Grid-to-Graph: Flexible Spatial Relational Inductive Biases for Reinforcement Learning\n\nKlissarov et al. 2020, Reward Propagation Using Graph Convolutional Networks\n",
            "summary_of_the_review": "The presentation of the paper is convoluted and could really be improved. Much of the theory is unnecessary to understand the contribution/algorithm and in fact seems to inject assumptions that are hard to satisfy. The experiments show modest and good improvements, however the ablation study doesn't really shed light on what really helps. Some important related work is missing.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}