{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes to address the dataset bias prediction for few-shot image classification. The proposed method consists of a bias prediction network to recover the color of the input image from the feature embedding. The output of the bias prediction network is used to force the main network to extract feature which is difficult for bias network to recover. The proposed method is simple, and bring performance promotion on several datasets of few-shot image classification. ",
            "main_review": "The proposed method is simple and straightforward, and the author only validates the proposed method on color bias. I think the author should add some experiments for other bias not only color.\nIn the equation 2, Z is used to compute the cross-entropy loss in equation 3 and 4, but according to the last line in page 4, Z is the feature before softmax, is there any wrong about these equations?\nHow about the influence of background color to the proposed method?\nThe writing needs to be improved, there are many repetitive expressions in this paper.\n",
            "summary_of_the_review": "line in page 4, Z is the feature before softmax, is there any wrong about these equations?\nHow about the influence of background color to the proposed method?\nThe writing needs to be improved, there are many repetitive expressions in this paper.\n\nSummary of the review:\nThis paper proposes a simple method to alleviate the color bias in few shot classification. As a method for dataset bias prediction, this paper only validate the effectiveness of proposed method on color attribute, no convictive experimental results are shown. More discussion and experiments should be added to improve the quality of this paper.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes to learn unbiased features for few-short classification by adversarial training of a bias prediction network. The bias prediction network is trained to predict bias, e.g., color, based on the features of the classifier, and the classifier tries to fool the bias prediction network by suppressing dataset bias from the features. Experimental results show that the bias prediction network can improve the performance of existing few-short classification models. ",
            "main_review": "Strengths:\nIt is interesting to study the dataset bias problem in few-shot learning as potential bias is highly likely to exist when we have a dataset of very limited size. \n\nWeaknesses:\n1. Novelty is limited. Using a bias prediction network to unlearn target bias was explored in Kim et al., 2019a. It seems this paper just trivially applies the existing method to the existing few-shot classification models. \n2. The proposed method may not actually work. Experimental results show that the bias prediction network only slightly improves the performance of the classifiers and sometimes even causes performance drops. \n3. The paper is not written clearly and is hard to follow. \n4. Lack of explanation in the experimental results. It is not clear what metrics they are using for performance evaluation. ",
            "summary_of_the_review": "This paper has limited novelty and is poorly written. The reported experimental results do not support the claims. It seems the authors were not expecting acceptance. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies the problem of few-shot image classification from the perspective of removing color bias that could exist in the feature encoder. Based on an image classification network, it adds a bias prediction network which is trained to recover the color of the raw images while the classification network is trained not to provide features that help the color prediction of the bias prediction network. The method is tested on several datasets and shows improvements over the baselines.",
            "main_review": "1. The paper is nicely organized but the presentation quality could be improved. For example, in Figure 3, there are red spell checks for some keywords in the figure. The font in the figure could also be changed to a clearer one. PDF rendering will be preferred. Another suggestion is to explain what BP stands for in the tables.\n\n2. The paper introduces the method from a general definition of bias but in reality, it only studies the color bias that potentially exists in the feature extraction. I think it would be clearer if the authors narrow down the introduction and the title to color bias only.\n\n3. Bias is harmful in general, but it’s not clear if color bias is helpful or harmful for few-shot image classification, especially fine-grained classification. The paper will be strengthened by discussing when bias is harmful or helpful for different kinds of few-shot image classification.\n\n4. The paper uses total loss in 3.2 but in fact there are also other losses. This is confusing when reading it the first time.\n\n5. Section 3.4 is important, and should be highlighted at the beginning of Section 3. I think the overall training pipeline is in the adversarial style, which could also be highlighted in the figure 2.\n\n6. In Table 1, the performance of adding BP is comparable or even lower than the baseline for 5-shot results. Why is this happening?",
            "summary_of_the_review": "In summary, I think the paper writing and presentation could be improved. The method is simple and intuitive, but the improvements are not significant.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper focuses on mitigating the color bias in the few-shot image classification task. To this end, the method uses a bias prediction network to predict the color and uses the entropy loss to encourage the feature not to contain the color information. To train the bias prediction network, the proposed method uses the raw RGB colors as the label. Experiments conducted on three datasets show that the bias prediction network can improve the performance of three few-shot learning baselines.",
            "main_review": "Strength:\nThe paper is easy to understand. However, the writing is verbose and needs to be improved. \n\nWeakness:\n1. In the related work section (Sec. 2), the paper only mentions (Kim et al., 2019a) but fails to discuss other bias mitigation methods [1-13].\n\n2. The proposed method is not novel as it is a straightforward ensemble of two methods: 1) bias prediction network (“We implement the bias prediction network with the algorithm introduced in (Kim et al., 2019a).” in Sec. 3.1); 2) uniform confusion [12-13] (entropy loss term in Eq. 1).\n\n3. Experiment: 1) the experiment does not have any comparison methods. 2) No error bars are provided, making it hard to judge if the improvement (e.g., “71.07% -> 71.19%” in Tab. 1) is significant or not.\n\n\n[1] Z. Wang et al., “Towards Fairness in Visual Recognition: Effective Strategies for Bias Mitigation,” in CVPR, 2020.\n\n[2] V. V. Ramaswamy, S. S. Y. Kim, and O. Russakovsky, “Fair Attribute Classification through Latent Space De-biasing,” in CVPR, 2021.\n\n[3] D. Krueger et al., “Out-of-Distribution Generalization via Risk Extrapolation (REx),” in ICML, 2021.\n\n[4] F. Ahmed, Y. Bengio, H. van Seijen, and A. Courville, “Systematic generalisation with group invariant predictions,” in ICLR, 2021. \n\n[5] E. Z. Liu et al., “Just Train Twice: Improving Group Robustness without Training Group Information,” in ICML, 2021\n\n[6] S. Sagawa*, P. W. Koh*, T. B. Hashimoto, and P. Liang, “Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization,” presented at the ICLR, 2020.\n\n[7] J. Nam, H. Cha, S. Ahn, J. Lee, and J. Shin, “Learning from Failure: Training Debiased Classiﬁer from Biased Classiﬁer,” in NeurIPS, 2020.\n\n[8] H. Bahng, S. Chun, S. Yun, J. Choo, and S. J. Oh, “Learning De-biased Representations with Biased Representations,” in ICML, 2020.\n\n[9] J. Zhao, T. Wang, M. Yatskar, V. Ordonez, and K.-W. Chang, “Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints,” in EMNLP, 2017.\n\n[10] T. Wang, J. Zhao, M. Yatskar, K.-W. Chang, and V. Ordonez, “Balanced Datasets Are Not Enough: Estimating and Mitigating Gender Bias in Deep Image Representations,” in ICCV, 2019\n\n[11] L. A. Hendricks, K. Burns, K. Saenko, T. Darrell, and A. Rohrbach, “Women also Snowboard: Overcoming Bias in Captioning Models,” in ECCV, 2018\n\n[12] M. Alvi, A. Zisserman, and C. Nellaaker, “Turning a Blind Eye: Explicit Removal of Biases and Variation from Deep Neural Network Embeddings,” in ECCVW, 2018\n\n[13] E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko, “Simultaneous Deep Transfer Across Domains and Tasks,” in ICCV, 2015.",
            "summary_of_the_review": "Since the paper 1) lacks many related works to discuss; 2) has limited novelty; 3) the experiment has no comparison methods, I recommend “strong reject” to this paper.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}