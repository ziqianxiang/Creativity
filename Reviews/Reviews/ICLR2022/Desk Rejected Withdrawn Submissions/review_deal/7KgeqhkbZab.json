{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper introduces BOOST,  a self-supervised model which learns structural and functional properties of source code by means of contrastive learning. Rather than leveraging a large dataset with human labels, data augmentation is used to generate positive and hard-negative samples, which in turn are fed to the contrastive learning pipeline. This strategy results in a relatively compact model which obtains SotA results on a few selected ML4code tasks.",
            "main_review": "STRENGHTS:\n- the proposed data augmentation pipeline is well-designed towards a few downstream tasks, such as vulnerability and clone detection\n\nWEAKNESSES:\n- the ML4code literature has produced several papers presenting architectures that can learn from both the code sequence and its structure. Among the recent ones based on the Transformer architecture, GREAT (https://openreview.net/forum?id=B1lnbRNtwr) and Code Transformer (https://arxiv.org/pdf/2103.11318.pdf) should be both cited and used as baselines. It is not clear why this paper introduces a new (arguably less advanced) architecture to tackle this learning problem, rather than building on top of already available encoders. At least the paper should justify the choice.\n\n- The node-type MLM objective is not a novel contribution, as it is one of the several pre-training objectives introduced in CodeT5: https://arxiv.org/pdf/2109.00859.pdf It is worth noticing that the CodeT5 paper appeared on arXiv only a month before the ICLR 2022 deadline, so it is understandable if the authors did not have enough time to substantially update their paper on a short notice. That being said, CodeT5 should be at the very least cited, and the node-type MLM objective should not be referred as one of the main contributions of this paper.\n\n- The paper presents BOOST as a generic architecture that matches or outperforms SotA models on code understanding and generation tasks. Yet, the evaluation is very narrow, selecting only a few specific downstream tasks, for which the data augmentation strategy (leveraged by the contrastive learning) has been optimized. Given that CodeXGLUE is used extensively in the experiments, the reader is left to wonder why only 3 of the 10 tasks have been selected for the evaluation. With the current experimental protocol, there is no evidence that BOOST is indeed a SotA model on code intelligence tasks. I recommend to either extend the evaluation to all the tasks in CodeXGLUE, or to narrow down the unsubstantiated claims in the paper.\n\nTYPOS:\n- \"it should a variable\" on Page 5\n- \"the the effectiveness\" on Page 7\n- \"pre-traing\" on Page 8",
            "summary_of_the_review": "Overall, using contrastive learning for ML4code models is proving to be a fruitful endeavor, so I encourage further work on this direction.\nThat being said, the paper has a few important flaws (especially in terms of evaluation) which don't make it ready for publication in a venue like ICLR.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The submission present a self-supervised pre-training methodology for sequence models operating on source code. The pre-training combines three objectives: (1) standard Cloze task (masked language modelling), (2) standard Cloze task on syntax information associated with each token, (3) learning to recognize semantically equivalent samples. The paper discusses a number of approaches to generates samples suitable for objective (3).\nFinally, an evaluation shows that the resulting pre-trained model is competitive with existing baselines, and outperforms them slightly on a number of tasks.",
            "main_review": "Overall, the presented training methodology seems straightforward, and empirical results indicate that it is effective.\n\nHowever, digging deeper opens a number of questions:\n1. The \"Bug Injection\" heuristics in Sect. 3.1 are only tersely described, and their details are missing. In particular:\n  * (Q1) \"Misuse of Data Type\" talks about shorter/longer data types, but its example application in Fig. 1 changes a `float` to an `int`. Dependent on the context, the code in Fig. 1 may lead to a trivial compiler error as well, which would be much easier to detect. Could you clarify how you pick types to replace / their replacements?\n  * \"Misuse of Pointer\" sounds like a trivial bug to recognize, and modern compilers will usually do this.\n  * \"Misuse of Variables\" does talk about compilability, but does not discuss if necessary type checks are performed to establish that.\n  * (Q2) \"Misuse of Values\" is again assigning a constant to a variable and would be caught by a compiler. Could you clarify what \"immediately before the division\" means anyway?\n\n  The reason I ask these questions is that I'm wondering about the importance of these proposed bug classes, and whether they contribute equally to the pre-training. In particular, I believe that very simple pertubations (like setting points to NULL/divisisors to 0) may not help at all.\n\n2.  Similarly, \"Similar Code Generation\" has problems with trivially to detect modifications.  In particular, I'd expect that the model learns that the presence of the `FUNC_i` and `VAR_i` tokens is a clear indicator that the code under analysis is a \"positive\" sample. \n\n  (Q3) Do you have any empirical analysis of the influence of the heuristics used to generate similar/different code?\n\n3. Experimental description is lacking: what is the objective used by MLM+CLR^+? If it doesn't use negative samples, it can't be Eq. (2), so what is it instead?\n\n4. Experimental results are lacking obvious baselines: you failed to include the top entries of the CodeXGLUE leaderboard in your submission. In particular, both Tab. 2 and Tab. 4 are missing the current leaders, which outperform BOOST substantially. Most of the missing entries are timestamped 2021-04-* (or earlier) and so were clearly public at ICLR submission time. This is a pretty big mistake, and could be interpreted as trying to lie to the reviewers.\n\nDetails:\n* Sect. 4.2.3: \"N pairs of $(\\mathbf{h}, \\mathbf{h}^+, \\mathbf{h}^-)$\" - replace \"pairs of\" by \"triples\".\n* Sect. 6 / Fig. 3: did you use the same batch size for all three models here? If yes, did you consider that the results could be explained by smaller gradient steps due to a lower loss (as MLM+CLR+NT-MLM will have three loss components that seem to just be added up, vs. a single (?) loss for MLM-CLR+)\n",
            "summary_of_the_review": "The paper proposes a new self-supervised objective for pretraining on code. Sadly, the experiments don't provide enough data to understand the contribution of the different parts of this novel contribution in details. Finally, egregious mistakes in citing prior experimental work make this a clear reject to me, which the authors could improve upon by properly including prior results.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "Boost is an unsupervised pre-training technique for software based on BERT's masked language model objective. In addition to MLM, it introduces a masked token-type objective, that seeks to unmask the syntactic role of a token (the type of node it corresponds to in the abstract syntax tree, as well as the type of its immediate parent). Beyond twists on the MLM objective, Boost also introduces a contrastive loss term, that seeks to contrast a sample to a similar mutated sample (generated via semantics-preserving transformations, such as variable alpha renaming) and to a semantically different sample (generated via a number of semantics-altering, but lexically small, transformations, such as changing types and removing initializations). The resulting pre-trained model is fine-tuned on understanding tasks (such as security-vulnerability detection and clone detection) and generative tasks (code summarization), and compared to models fine-tuned after a much longer pre-training period, showing benefits in most cases.",
            "main_review": "I enjoyed reading your paper. It does sensible things to improve the inductive bias of pre-trained models on code, and it shows promise on hard tasks, even with relatively limited pre-training.\n\nMy biggest concern with the paper is novelty. Although what it does is sensible and put together in a reasonable fashion, none of it is novel. \n\nFor example, a number of research articles have argued for augmenting code datasets with both semantics-preserving and semantics-destroying transformations. Some recent exemplars are `Allamanis, Miltiadis, Henry Jackson-Flux, and Marc Brockschmidt. \"Self-Supervised Bug Detection and Repair.\" arXiv preprint arXiv:2105.12787 (2021).` and `Ramakrishnan, Goutham, Jordan Henkel, Zi Wang, Aws Albarghouthi, Somesh Jha, and Thomas Reps. \"Semantic robustness of models of source code.\" arXiv preprint arXiv:2002.03043 (2020).`\n\nThat said, some of the mutations described do seem to target harder, security-oriented bugs, which is different from prior mutations used on code models.\n\n\nSimilarly, predicting node token types is not terribly different from GraphCodeBERT's predicting of edges, as far as pre-training on syntax and semantics is concerned, nor is it particularly different in embedding syntax structure into a form of \"positional encoding\" from something like `Kim, Seohyun, Jinman Zhao, Yuchi Tian, and Satish Chandra. \"Code prediction by feeding trees to transformers.\" In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), pp. 150-162. IEEE, 2021.` or even `Zügner, Daniel, Tobias Kirschstein, Michele Catasta, Jure Leskovec, and Stephan Günnemann. \"Language-agnostic representation learning of source code from structure and context.\" arXiv preprint arXiv:2103.11318 (2021).`\n\nThe contrastive learning aspects of the work were of course developed before, but applied to similar and different exemplars in the code domain here, which is novel.\n\nOverall, although an interesting approach, I found the novelty in this work rather limited.\n\n# Smaller Issues\n\nSection 3.2: what are \"library calls\"? Do you mean `libc`? Something else? How do you choose?\n\nSection 5.3: Why are different metrics used for POJ-104 (MAP@R) versus BigCloneBench (precision/recall)? If they're both code-clone detection datasets, wouldn't the same metric make sense there? Also, you mention precision/recall/f1 but only show the former two.\n\n",
            "summary_of_the_review": "Interesting work but limited novelty.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "==+== A. Paper summary\n\nThe paper proposes a new method to do model pretraining for programming languages.\nBy leveraging masked language modeling and contrastive learning, the pre-trained model can surpass the performance of the traditional MLM method. To obtain the datasets for contrastive learning (positive and negative samples), the paper proposes to use `bug injection’ and variable/function renaming to synthesize the dataset.\n",
            "main_review": "==+== B. Strengths \n\nThe idea is good and novel. The paper leverages some key features feature of PL to synthesize the positive and negative samples. \n\n\n==+== C. Weaknesses  \n\nThe evaluation is weak.\nA sensitivity analysis is missing.\nSome important implementation details are missing.\n\nQuestions and concerns:\n\nThe evaluation is weak in my opinion. CloneDetection/POJ-104 and Vulnerability checking are three easy benchmarks in CodeXGLUE.  Also, I don’t know why you cite DOBF but not compared in your evaluation. \n\nFrom Tables 3 and 4, I can hardly see the performance improvement by leveraging your contrastive learning mechanism. Specifically, the performance gap between MLM_CLR+ and MLM_CLR± is not obvious. If you only use MLM_CLR+, the idea would be very close to DOBF which leverages function/variable renaming to represent codes that are similar.\n\nAnother important sensitivity analysis: the size of your dataset v.s. Performance is missing. \n\nRegarding the implementation details, A.4 is not enough to rebuild your experiment. 1) First the subtasks are using different configurations in CodeXGLUE. Are you using the same configurations across all the subtasks? 2) The pretraining tokens_per_sample is extremely small (256). I don’t think this is enough for a regular-size program. Also, I don’t think using batch_size=64 for pretraining is enough. 3) According to Figure 2, you need to forward propagation 3 times before 1 backprop. How did you manage your GPU memory to prevent the OOM issue during pretraining? 4) I don’t believe that a BERT-small can converge on 2 V100 GPUs within a day. Can you show your learning curve? \n5) You did not even mention the machine learning framework that you used to build the framework. \n\nMinor: I am not sure if there are some licensing issues to train your model directly using GitHub repos. You can follow DOBF and use BigQuery. \n",
            "summary_of_the_review": "I think the overall idea of the paper is good. But the implementation and evaluation are too weak. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}