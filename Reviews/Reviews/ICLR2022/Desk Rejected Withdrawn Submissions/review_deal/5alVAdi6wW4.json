{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents a multi-scale conditional generative model based on conditional IMLE for one-to-many image generation. Images are downsampled to different scales and the IMLE loss is applied at each stage. A different late code is sampled at each scale. The paper improves image quality and diversity substantially over previous IMLE based approaches.\n",
            "main_review": "Pros:\n\n1. Multi-scale architectures are a really nice extension to cIMLE and the paper presents a fast sampling approach to deal with new latents being sampled at each scale. \n2. Strong empirical results across several benchmarks.\n\nCons:\n\n1. I think the paper misses a few crucial dimensions to the ablation study. 1) LPIPS vs VGG feature matching (Zhang et al. 2018) as the image distance metric to optimize. Are gains coming from directly optimizing the target objective? Li et al. 2020 seem to use the latte while this paper uses the former. 2) How does quality and diversity change as you change the number of scales (1, 2, 4, 8, etc.)?\n\n2. The differences in this paper over Li et al. 2020 seem to be just the multi-scale formulation, which isn’t particularly novel.\n\nGeneral Comments & Questions:\n\n1. How was the cIMLE baseline implemented? Was it using code from Li et al. 2020?\n2. The paper misses some general discussion of some GAN-based approaches that have successfully done many-to-many image generation with latent variables (Almahairi et al. 2018)\n\nReferences:\n\nZhang et al. 2018 - The Unreasonable Effectiveness of Deep Features as a Perceptual Metric \nLi et al. 2020 - Multimodal Image Synthesis with Conditional Implicit Maximum Likelihood Estimation\nAlmahairi et al. 2018 - Augmented CycleGAN: Learning Many-to-Many Mappings from Unpaired Data",
            "summary_of_the_review": "This appears to be a strong empirical paper that presents a meaningful improvement over conditional IMLE methods for one-to-many image generation. While it does not propose a particularly new architecture, it still extends existing methods to leverage multi-scale information. A few additional ablations should make this a stronger paper.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work considers a one-to-many modeling problem, where multiple outputs are possible given an input. In particular, this work focuses on the problem of conditional image generation. Building on top of the training objective of conditional implicit maximum likelihood estimation (Li et al 2020), this work proposes to use a hierarchical generator and supervise intermediate outputs using downsampled images. To improve the training efficiency of Li et al 2020 which requires many samples, this work proposes a hierarchical sampling scheme, which samples noise from lower levels to higher levels in a cascaded manner while only keeping good samples from lower levels. Experiments on several conditional image generation tasks show that the proposed approach not only achieves better quality than existing one-to-many and even one-to-one works, but also generates more diverse images.",
            "main_review": "Strengths:\n1. This paper is well written and easy to follow.\n2. Empirically this work achieves strong results, outperforming both one-to-many and one-to-one existing works in terms of generation quality and generation diversity.\n3. The proposed hierarchical sampling scheme is a clever way of mining samples during training.\n\nWeaknesses:\n1. This might not be a weakness of this paper since the problem setup comes from a prior work, but I want to point out that the one-to-many problem setup is not new. Many existing works consider the problem of modeling P(output | input), which encompasses both one-to-one and one-to-many modeling tasks. While some existing methods such as GANs (and sometimes even VAEs with too powerful decoders) have a mode-collapsing issue, that doesn't mean their problem setting is inherently one-to-one. Furthermore, many followup works of GANs and VAEs propose to fix the mode-collapsing issue and target both generation quality and generation diversity.\n\n2. Related to 1, this work should be compared to other GAN papers that target mode collapse or diversity in table 3 (such as mode-seeking GANs https://openaccess.thecvf.com/content_CVPR_2019/papers/Mao_Mode_Seeking_Generative_Adversarial_Networks_for_Diverse_Image_Synthesis_CVPR_2019_paper.pdf and self-conditioned GANs https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Diverse_Image_Generation_via_Self-Conditioned_GANs_CVPR_2020_paper.pdf, to name a few).\n\n3. As acknowledged by the authors, the proposed hierarchical generator architecture is not very novel: it's been proposed in Denton et al 2015. This is not a problem on its own, but it's claimed to be one of the three contributions of this paper.\n\nMinor Issues:\n1. the axis titles of Fig. 3 are a bit small.",
            "summary_of_the_review": "Overall, this is a well-written paper with strong empirical results. However, I am not convinced that one-to-many modeling is a new problem setup, and I think the empirical results would be more convincing if experiments compare to more GAN baselines, especially those targeting mode collapse and generation diversity. Therefore, I am not planning to recommend the acceptance of this paper unless I missed anything.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a method for generating samples conditioned on a single observation in the training set, using a generative model trained with a hierarchical version of the conditional implicit maximum likelihood (cIMLE) loss. They demonstrate modifications to the sampling method and underlying model architecture that accelerate learning and allow for broader application as compared to the implementation in the original cIMLE paper. They compare empirical results for several image generation tasks in terms of perceptual quality and diversity against cIMLE and related baselines.",
            "main_review": "This paper presents several modifications to the cIMLE method of Li et al. (2020), framing the overall approach as a solution to a supervised learning problem in which predictions are \"one to many,\" in the sense of there being continuously many \"correct\" outputs corresponding to a given input. This framing is somewhat awkward and obscures potentially relevant connections with conditional generative modeling. The modifications lead to relatively clear if incremental improvements over baseline cIMLE in terms of quality and diversity of the generated samples.\n\nA main idea in this paper seems to be that since cIMLE avoids mode collapse by ensuring that each training sample has at least one nearby generated sample, a generative model trained by this method can be used to produce reasonable-looking samples conditioned not on class-level information but rather on information unique to a single point in the training set. This is a creative and interesting way to leverage a specific advantage of cIMLE. However, I find the framing as a \"one to one\" supervised learning task that yields a \"one to many\" predictor somewhat confusing. In particular, is supervision really \"one to one\" given the loss function? Clearly, the learned parameters depend on all the data in the training set. Are most of the actual problems addressed not just a matter of conditional generation, where we condition on information more refined than a class label? \n\nSome of this confusion carries over to the toy example in Section 2:\n- In this example the conditioning information (top 10 PCs) is likely sufficient to specify the class label, so this seems like class-conditional generation with extra conditioning information (exact match to top 10 PCs). Variants of this experiment could have separated these concepts further, e.g. with fewer PCs specified the conditional distribution may extend across classes.\n- It is dubious to claim that the prediction problem is \"one to many\" when the heuristic plausibility of the result is essentially just with respect to the class distribution, of which there are many examples in the training set. Moreover, conditioning on the first 10 PCs already gets you most of the way towards such a plausible class-specific generation.\n- The authors do also attempt to judge the results in terms of the distribution that results from conditioning specifically on the first 10 PCs, but this is difficult as there is only a single observation satisfying the condition, and thus no real way to assess ground truth for the conditional distribution. The results in Fig 1b, for example, seem difficult to interpret. Why not construct an example in which every setting of the \"condition\" of interest had many corresponding observations, but only one each was used in training?\n\nFraming aside, the methodological contributions consist of a hierarchical supervision and sampling method based on cIMLE and specification of some architectural settings that improve on previously reported results. The hierarchical sampling method seems to deliver clear improvements in terms of the number of samples required for a given perceptual distance to the output. It would have been helpful to see the method written somewhere in an algorithm block. One aspect that is unclear to me is how altering the sampling method in this way might influence the shape of the local mode around each training point. In cIMLE, for instance, smaller $m$ would seem to imply more peaked modes for a fixed value of the loss, and thus less diverse generations at test time. Here, however, the authors report improved diversity even with $m$ lower than in cIMLE. \n\nOther comments:\n- Figures 1-3 lack detail and are not well described by their captions. In Fig 1: The KDE contour plot is not described at all in the caption. There is no scale for the contour heatmap. In Fig 3: What is CAM-Net? \"Required samples\" is not explained until several pages later. Standard errors should be reported along with the average.\n- The related work section contains quite a few concepts and generic citations that don't seem relevant to this work, especially in the second paragraph.\n- In $\\S$4.2 the authors write \"So, the more samples that are generated during training, the more modes of the output distribution cIMLE can model.\" Is this true? The cIMLE objective essentially forces the model to maintain a mode around each training point. This does not seem to depend on the number of samples $m$ generated per training point (as long as $m \\geq 1$). ",
            "summary_of_the_review": "The authors introduce some methodological extensions for cIMLE that deliver incremental improvement in terms of generation quality and diversity on some conditional generation tasks for image data. The hierarchical sampling method appears to deliver a significant improvement in efficiency over naive cIMLE, but the implications of this non-i.i.d. sampling for the learned generator are insufficiently explored. The method and results are cast in terms of a \"one to many\" prediction framework that creates an unclear and likely unnecessary distinction from conditional generative modeling.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper addresses the issue that most supervision tasks learn only one-to-one between input and target while multiple predictions (one-to-many) could be considered correct in the real world. They emphasize three-fold that their proposed method, cIMLE, could 1) generate multiple-scale output with intermediate supervision in SR task, 2) work well in diverse tasks without modifying model, and 3) outperform the base model, IMLE [Li et. al., 2020].",
            "main_review": "+) The topic of handling one-to-many tasks in diverse fields is interesting.\n\n+) They verify their improvement through extensive experimental results (i.e., SR, Colourization, and Decompression) including supplementary materials.\n\n+) The qualitative improvement compared with the baselines seems meaningful.\n\n-) When a low-resolution image with code is given, the proposed method cIMLE generates many samples and finds the most similar one in the target space to decrease the objective function. I think that this setting is not one-to-many but many-to-one. Please refer to the recent paper Jo et. al., CVPR2021 that aims to tackle the ill-posed SR with adaptive target generation. They adaptively transform the target to match with SR output from a single input. I think their approach is more suited to a one-to-many setting.\n\n-) There are no training details such as dataset, training scheme, hyper-params, etc.\n\n-) They only consider the perceptual quality assessment. The general image quality metric like PSNR and SSIM is needed. Although they aim to produce the diverse SR output (or alternatives), at least one of the outputs should be similar to the corresponding HR target and outperform other SR techniques in order to verify their model's superiority. \n\n-) More interpretation is needed in the ablation study.\n\n-) Figure 2 makes a misunderstanding since the image scale is not considered.\n\n-) There is a lack of theoretical interpretation for their model.\n\nQ) I think that almost similar results can be produced when learning is performed while adding random noise to the low-resolution image in the SR process. What do you think about this?\n(If we test by adding noise which is the same way during the training, several images with small structural differences could be generated.)\n",
            "summary_of_the_review": "Although the topic is interesting, I’m not sure that the proposed work is designed well for one-to-many tasks. Overall, the interpretation of the experiment is insufficient and it is necessary to consider whether the comparison is fair; metric, baseline model, and etc.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}