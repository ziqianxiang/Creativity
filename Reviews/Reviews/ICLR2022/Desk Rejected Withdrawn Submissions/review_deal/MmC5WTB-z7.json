{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes two loss functions, and two image processing methods for improving image quality. In experimental evaluations it is not clear how exactly the revised image is generated. I assume a serious of the proposed methods are being applied, and the losses are computed over the revised images. \nThe revised images are then evaluated by human observers, as well as by off-the-shelf models trained for scoring memorability and aesthetics, and scores of both the original and revised images are compared.\n",
            "main_review": "\nPros:\n\n•\tThe paper attempts to bridge an idea from Game theory (Harsanyi dividend) to formalize interactions between internal image patches. \n \n•\tI appreciate the authors effort to match their predictions against human vision \n\nCons:\n\n•\tThe learning component of this very limited, or at least do not sufficiently clear to me. What is the learning model? How learning is done? What is the training procedure for the experiments? All these should be elaborated and better explained.   \n\n•\tThe processing and representation of global image information has a\nlong history in both computer vision and human vision. The authors mention the novelty of interactions between internal image regions (the proposed hypothesis) but learning representations of holistic image features is not a novel idea. Their implementation for this approach might be novel, but I would then expect to comparison to previous work. \n\n•\tThe technical contribution of this paper concludes to two loss functions that aim to push a learning model towards interaction of internal region. Little is mentioned on how these loss functions are implemented within a DNN model (beyond use of intermediate features), and more importantly it is not clear why and how this adds to the global information processing that already exists in deep CNNs. The contribution of the proposed image operations is also not clear (how do they add to the standard image processing methods for changing brightness/color/sharpness etc.?)\n\n•\tThe paper is difficult to understand and the main contributions to the machine learning community are not clearly mentioned\n",
            "summary_of_the_review": "Interesting approach to bridge ideas from Game theory, but technical novelty is limited. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a method to enhance images by enhancing regions that seem to be typical of the category label, using CNN responses to determine which parts of the image are most typical of the category. The enhancement involves increasing the colour/contrast/sharpness of the category-typical regions. Experiments suggest that this enhancement improves categorisation performance and makes the image more aesthetically-pleasing/memorable, according to CNN-based measures of aesthetics/memorability.",
            "main_review": "The approach is interesting and the method used to identify typical regions of an image using CNN features may be novel.\n\nOne weakness of the paper is the lack of grounding in cognitive science -- the problem addressed in this paper is not very clearly defined, but I gather that a \"cognitive\" image is equivalent to something like a \"prototypical\" or \"canonical\" image in cognitive science. However, none of the relevant literature on prototype-based recognition or canonical images is cited. The authors claim that it is not possible to study prototypicality in humans (\"The cognitive process is subjective, and there is no proper method to directly model the information processing mechanism of the human brain.\") but this is incorrect -- there are plenty of standard tasks to identify the prototypicality of an image, including name agreement tasks, timed image recognition, memory tasks, and visualisation or drawing tasks. (There's also plenty of debate in cognitive science about whether human object recognition is actually based on prototypes.)\n\nAnother major weakness of the paper is the experimentation, which relies on comparisons between original, positively enhanced (to be more typical of a given category) and negatively enhanced (to be less typical of a given category) images. The positively enhanced images generally have higher-saturation colours, higher contrast, and less blur than the original images, while the negatively enhanced images are low-saturation, low contrast, and blurry. The experiments show that the enhanced images are more recognizable (to CNNs and humans) and score higher on CNN-based measure of aesthetics and memorability, but it's unclear whether this is simply due to the increased saturation/contrast/sharpness or the specific method used here. As a control, it would be important to run another set of images that had similar low-level image enhancements in all regions of the image (or randomly-selected regions). The results would also be more convincing if compared to some alternative methods for selecting \"typical\" or \"important\" image regions, such as a method that simply boosted the central region of each image, or boosted regions that were judged \"salient\" by some saliency method. It's not clear whether this method finds more relevant regions to enhance than a saliency method would.",
            "summary_of_the_review": "The method may have some novel aspects, but the problem of reducing \"cognitive difficulty\" is not well defined or grounded in theory, and the experimental evaluation of the method has significant problems.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "In this paper, the authors propose a heuristic method to manipulate images to increase (or decrease) their “cognitive difficulty”. The method is applied to a set of ImageNet images, and corresponding human ratings are collected in order to test whether judgments are consistent across participants. Existing algorithms are also applied to the modified images to quantify their “memorability” and aesthetic value.",
            "main_review": "- The paper clarity and structure could be significantly improved. For example, the Introduction starts with a general overview of the idea of modeling the cognitive difficulty of an image, then switches to the specific technical details of the game-theoretical approach proposed by the authors, along with examples. The authors then lay out their hypothesis, and in the “Related work” section they go back again to general issues and links with cognitive science literature. There are also several redundant parts throughout the paper that can be merged / removed.\n- The definition of “cognitive” concept is left quite unspecified and is not always consistent throughout the paper. For example, the sentence “an object with bright colors, clear contours, and simple contents is usually believed to be more cognitive” seems in partial disagreement with the notion of cognitive concept advocated by the authors, which is based on the notion of “interaction patterns” from a game-theory perspective and thus seems to entail the presence of more structured and complex contents.\n- The statement “Cognitive difficulty of an image is an important issue, but it does not receive much attention” might be partially true from a machine learning perspective, but it is hazardous from a cognitive (neuro)science perspective. There are plenty of papers discussing how the content of visual scenes (perceptual, semantic, emotional, etc.) alters cognitive load and allocation of attentional resources.\n- The comparison between “cognitive concepts” and saliency is quite confusing and does not seem to be fairly stated. Most of saliency detection algorithms do not just focus on the single pixel, but indeed consider super-pixel aggregates or even top-down information.\n- I am concerned that the task instructions given to the participants (“we asked each participant to sort the cognitive difficulty of these three images”) might not have been precise enough to make sure subjects are not actually rating saliency or color-related features rather than “cognitive difficulty”.\n- I was very surprised to see that the authors completely neglect recent (and extremely popular) approaches that are used in the deep learning community to establish the “importance” of super-pixels using SHAP values [1].\n- “For the ResNet-18 model, we used the feature after the first block as f. For the VGG-16 model, we used the feature of the conv3 3 layer as f .” What was the rationale behind this choice?\n- The discussion of the results is often qualitative and should be supported by more precise quantitative analyses. The proposed method should also be compared with at least an alternative method, which might just be based (for example) on automatic image improvement tools provided by most computer graphic software or smartphone cameras.\n- The authors should better explain the details of their experimental procedure, such as participants’ demographics and experimental setup (online surveys? controlled lab conditions?).\n- Using pre-trained classifiers to evaluate image memorability or aesthetics might be a quite poor baseline of human judgments.\n\nReferences\n\n[1] Lundberg SM, Lee SI. A unified approach to interpreting model predictions. Advances in Neural Information Processing Systems. 2017. pp. 4766–4775.",
            "summary_of_the_review": "Although the proposed approach incorporates some interesting ideas, there are several concerns that seriously undermine the overall strength and relevance of the paper for the ICLR community.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}