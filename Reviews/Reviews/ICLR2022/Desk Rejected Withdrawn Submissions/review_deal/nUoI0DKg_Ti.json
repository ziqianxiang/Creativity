{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper deals with the problem of gradient free optimisation. Specifically, it builds on finite differences by guiding the search along a direction that is decided by a policy trained using DDPG.",
            "main_review": "I have some fundamental concerns about the proposed method that, together with the lack of certain information about the experiments makes me doubt the presented results. Detailed comments are as follows:\n\n1. Formulating this an RL problem means that the dimensionality of the action space is same as that of the state space (so in Sec 3, p should be =q?). The parameter space for function approximators (neural nets or otherwise) for even moderately complicated problems should run into at least hundreds of dimensions. I find it somehow difficult to reconcile the fact that training an RL agent with DDPG with such high dimensional action spaces yields reasonable estimates of the guiding vector. \n\n2. RL methods (and that includes DDPG) is not exactly known for its sample efficiency. In the experiments, at each iteration the algorithm only sees 20 samples when using random sampling or 2 when using the policy. There is no mention of how many iterations it operates for, so assuming its in O(100), that means the RL policy is being learnt on only O(1000) samples. I don't see how DDPG can learn a meaningful \npolicy with so few samples.\n\n3. In algorithm 2, line 10, rho gets updated if it is < epsilon. So once this condition is met, random samples are no longer drawn? This would imply that the vector sampled through the DDPG policy can point to the completely wrong direction in all future iterations, but theres no way to recover.\n\n4. DDPG is known to be highly sensitive to its hyperparameters. There is no mention of how these were set in the experiments. The baselines considered in the experiments all have very few hyperparameters, and relatively speaking, those don't have a huge effect on their performance. I find it hard to believe that the proposed method can learn a good policy without requiring extensive hyperparameter tuning, which would lead to the proposed method being inefficient in practice.\n\nMinor comments:\n1. There is a lot of notation reuse that makes the text difficult to follow, for example, q is used for dimensionality of state space, and number of randomly sampled vectors in the experiments, K is total number of iterations of the algorithm, and number of classes in the experiments.\n2. There are numerous grammatical errors throughout the text (for example, 'accurateness' is being used throughout).",
            "summary_of_the_review": "I have significant concerns with the proposed method, as outlined in the full review.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes an RL-based algorithm for derivative-free optimization. It recast the problem in the RL framework by treating optimization variable as state, perturbed direction as action, and function value change as reward. It proposes to optimize the sampling policy of $u$ by DDPG. Moreover, the algorithm can incorporate other sampling or parameter updating strategies.",
            "main_review": "### Strengths\nThe paper proposes a framework for derivative-free optimization using RL. \n\n### Weakness\n1. In Figures 4-6, it is not clear to me whether the number of queries contains the the number of queries used in the pre-training stage for ZO-RL-LSTM. According to Section 3.3, the algorithm first trains two LSTM before running ZO-RL. However, the number of queries used in that stage seems not reflected in the figures.\n2. There is no discussion about the computation time of the proposed algorithm. ZO-RL-LSTM is much more computationally intensive compared to other algorithms like guided-ES. So the comparison of algorithms with the same number of queries is not completely fair. \n3. The two functions considered in the experiments are both smooth functions. As far as I know, derivative-free optimization is mostly used to optimize non-smooth functions. For the binary-classification problem, since the gradient exists and can be computed easily, it would be better to compare the proposed algorithm with some baseline algorithms, e.g. SGD with exact SGD.\n4. The paper does not fully justify why an RL agent is capable of learning the \"best perturbed direction\". According to the top of page 2, the best $u'$ should satisfy $\\nabla_{u'} f(x) u'=(u u'^\\top)\\nabla f(x) =\\nabla f(x)$. This means $u'$ is proportional to $\\nabla f(x$). If the RL agent could learn $\\nabla f(x)$, then wouldn't one use the gradient estimator directly instead of using Equation (1)? Moreover, if the function $f$ is not smooth but highly fluctuate, which is when ZO is mostly useful, it's hard to imaging how the histories of (action, state, reward) could be informative when $x_t$ comes to a new region in the landscape of the function where it has never been to before.\n\n### Minor comments:\n1. The formula at the bottom of page 1 should be $\\lim_{\\mu\\rightarrow0}\\frac{1}{\\mu}(f(x+\\mu u) -f(x)) $, instead of $\\lim_{\\mu\\rightarrow0}(f(x+\\mu u) -f(x)) $.\n2. The formula in the second paragraph of Section 3.1 should be $\\theta'\\leftarrow\\tau\\theta+(1-\\theta)\\theta'$, instead of $\\theta'\\rightarrow\\tau\\theta'+(1-\\theta)\\theta'$.",
            "summary_of_the_review": "I do not recommend accepting this paper because the proposed method is not fully justified and the experiment is not convincing enough.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors explore a new direction, i.e., learn an optimal sampling\npolicy based on reinforcement learning (RL) to generate perturbation instead\nof using totally random strategy, which make it possible to calculate a ZO gradient\nwith only 2 function evaluations. The given experimental results with different ZO\nestimators show that the proposed ZO-RL algorithm can effectively reduce the query complexity\nof ZO algorithms especially in the later stage of the optimization process,\nand converge faster than existing ZO algorithms. The paper seems to contain interesting results. \n",
            "main_review": "Overall, the paper is clearly written. Some comments to improve the quality of the paper are summarized below. \n1) On page 3, when the star projects are listed, Mnih et al., 2015 is overlapped. \n2) When perturbed vectors are generated according to the learned sampling policy, how can we obtain multiple vectors when the policy is deterministic?\nThis could be clarified. \n3) The proposed algorithm performs better than other methods in the experiment section. \nHowever, the computational complexity of the proposed method can be some cost to pay. Therefore, this could be discussed. \n4) The performance has been compared using some limited number of examples. More comprehensive comparisons with examples would be more convincing.\nIt would be good to add some discussion about this issue. \n5) Overall, the paper contains interesting results. \n",
            "summary_of_the_review": "Overall, the paper is well written, and contains potentially interesting results. \nSome discusssions on the numerical comparison can be added to make the results more convincing. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes to combine RL methods with gradient-free optimization strategies. The core idea is to train an RL agent to predict the gradient direction, from which an estimation of the true gradient is obtained with finite differences. The paper conducts two numerical experiments and compares the performance of the proposed method with a few gradient-free baselines.\n\nIn my opinion, the paper’s major contribution is its idea of applying RL to estimating true gradient directions. \n",
            "main_review": "Overall, I feel this paper did a reasonably good job of describing its core idea, but the technical methods and the experiments did not convince me enough. I will elaborate on my opinion of the pros and cons of this paper below:\n\n**Strengths:**\n\n1. This paper starts with a fairly clear motivation. The overview of gradient-free methods is adequate enough to convince me that a more strategic choice of directional derivatives will boost the gradient estimation. The proposal of embedding RL in this ZO framework caused me to raise lots of questions (you will find some of them below) when I was reading the paper, so in some sense, their proposed method is at least thought-provoking, which is probably a good thing.\n\n2. In their related work section, the authors commented that their approach is the opposite direction of utilizing gradient estimation techniques to improve RL algorithms. In my opinion, this is an interesting point that might inspire more comprehensive discussions in some follow-up works.\n\n**Weaknesses:**\n\n1. I feel this paper owes readers a more in-depth explanation of the intuition behind this RL + ZO pipeline. Training an RL agent requires possibly many samples, and it is not very clear to me, at least when I read the paper for the first time, why this is more efficient than directly using these samples to run vanilla ZOO. It seems that the idea is to feed RL agents with random samples (which should have roughly the same time complexity as the vanilla ZOO) and hope the actor network can regress the mapping from x to its gradients. To me, it is only in the later stage (after the RL agent learns to behave less randomly) that the proposed method starts to save time and samples compared with vanilla ZOO methods. Having one paragraph explaining the intuition more clearly before or after Alg. 2 would be ideal.\n\n2. The hard switch from rho < eps to rho >= eps in Alg. 2 is questionable to me. The whole algorithm reads like a two-stage algorithm, where the first stage (rho < eps) is dedicated to collecting true gradients and regressing, then suddenly it switches to the second stage (rho >= eps) and does not collect true gradients anymore. I do not see how the actor network can improve itself as a true-gradient estimator in the second stage since it will no longer see new supervision. Why not use the standard epsilon-greedy algorithm?\n\n3. The choice of network structures (CNNs) seems not very well-motivated. I am not sure using CNNs in this approach for inputs that are not images or videos is a good idea, unless this paper is dedicated to minimizing objectives on images/videos.\n\n4. I think the experiment section needs quite a lot of improvement:\n- I was expecting to see results from more challenging problems/datasets instead of MNIST. Even on MNIST, the improvement seems marginal to me in some cases (Fig. 4). Given that Alg. 2 is much harder to implement and tune than a vanilla ZO-ES, I would expect a substantial performance boost to justify the value of the proposed approach.\n- There is not enough information for people to replicate the results: the dataset link at the end of page 8 seems broken, and information about the hyperparameters used in DDPG and Alg. 2 is sparse.\n- Epsilon seems to be a crucial hyperparameter in Alg. 2, and an ablation study should be provided.\n- It looks like the actor did not really learn to predict a good gradient direction, as can be seen from the cosine angle in Fig. 6. Therefore, it is hard to directly attribute the performance boost (if there is any) to the cleverer choice of gradient samples as motivated at the beginning of the paper. Some ablation studies on each component in Alg. 2 should be provided to demystify the source of efficiency (if there is any).\n- I would also suggest an experiment, or at least a discussion, on the generalizability of the RL agent: from what I understand, whenever there is a change to the loss function f, the RL agent needs to be re-trained. Although this issue is not uncommon in RL problems, it is still desirable to have some theoretical justification or experimental proof on whether/how the trained RL agent can be generalized to a new optimization problem, e.g., by warm-starting Alg. 2 when solving a slightly modified f, or by some meta-learning on a family of {f} instead of a single instance.\n\n5. Some minor, less technical comments:\n- Last line on page 1: I believe 1/mu is missing in “lim f(x+mu * u) - f(x)” if you want to claim it is “exactly calculating the directional derivative”.\n- Fig. 1, 2, and 3 do not seem to be very informative compared with the space they occupy. In particular, these three figures seem to present the same idea multiple times (at least to some extent).\n- The review of DDPG (Sec. 3.1) and batch normalization (Sec. 3.2) feel redundant and can be largely shrunk or completely removed. I think it is pretty safe to assume ICLR paper readers are familiar with these concepts.\n- The margin above and below Sec. 4 title seems unusually small.\n- Fig. 4, 5, and 6: It is probably better to have (semi-)transparent curves instead of solid curves that occlude each other.\n- The first baseline is named “ZO-GS” in the main text but “ZO-SGD” in Fig. 4, 5, and 6. Also, I thought ZO-GS should predict a pretty accurate gradient direction, albeit with possibly much more samples. From Fig. 6, it seems that ZO-GS failed to estimate the gradient direction correctly. Why?\n",
            "summary_of_the_review": "Given the current status of the paper, I am afraid I cannot recommend acceptance at this point unless substantial improvement can be made both to the technical method and to the experimental results.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}