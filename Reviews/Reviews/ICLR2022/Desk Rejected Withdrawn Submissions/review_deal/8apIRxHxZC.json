{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a (differentiable) procedure for adaptive data collection tasks, such as best-arm-identification in MABs and active learning. Given a measure of complexity of the problem instance (that is given sometimes via information-theoretic lower bounds), the method aims to perform well (in a minimax sense) for those problem instances of the same difficulty. What is obtained is robustness against the problem complexity. The authors provide an algorithm that approximates this robust policy by defining a nested sequence of problem classes (sorted by their complexity) and finding a policy that is parameterized via a neural net. They do experiments on two datasets and do a synthetic analysis as well. ",
            "main_review": "The paper was a nice read until section 3.1. From there I could not read the paper with the same ease. So one general comment is to improve the readability of the paper (see also a few pointers below). The main issue that I had with the paper was to understand its stance among other papers in the community of bandits and active learning. The paper provides no theoretical results, as well as weak experimental results. If this paper was supposed to be more of a practical/experimental paper, I would have expected a more involved and complete experimental section. However, there are only two experiments done on very small datasets. Moreover, I have concerns about the algorithm and the fundamental ideas of the paper, which I list below without a specific ordering:\n\n- In the algorithm 2 (and its full version, algorithm 3 in the appendix), there is no trace of $b^{r(\\theta)}$. It seems that $b$ is considered as just a constant, but its dependence on $\\theta$ is crucial. I agree that in the inner gradients, there should be no $b$, but in the outer gradients, $b$ should be there. Specifically, in the fourth line of algorithm 3, the left-hand-side is $\\min\\max \\ell'$ while the right-hand-side is $\\min\\max L_T$ (which does _not_ include $b$).\n\n- The algorithm relies on turning the objective defined over all $r>0$ into an objective that is defined over a nested sequence of $r$s. The argument that the authors provide is since $\\mathcal{C}$ is smooth, this approximation is valid. However, the objective also depends on a maximization over policies and it is not clear for me if the approximation argument would remain valid. \n\n- The experiments are all done over very limited and small datasets (10, 100 dimensions, with $r = 4$ for jokes, and 100 questions with $r=4$ for yes/no). Even though small, the authors claim that the running time is very high (and they had to sub-sample).\n\n- The paper also introduced applications on active learning. I found no experiments/results or away to find $\\mathcal{C}$ for these instances. \n\n- The budget is extremely low. It would be nice to see how the method scales with larger budgets.\n\n- There are lots of notation inconsistencies ($k$ instead of $i$ in page 7, $i_t$ not defined in page 3, $\\tilde{\\pi}$ instead of $\\pi$ in page 4, to name a few).\n\n- The usage of $\\mathrm{arg inf}$ in the figure 1 is not understandable for me. If the $\\inf$ is attained, then it is a $\\min$. If not, there is no _infimizer_.\n",
            "summary_of_the_review": "Even though the paper introduces a rather interesting solution to an important problem, it suffers from one main weakness: it positions itself as an experimental paper, while its experimental section is not strong enough for a submission to ICLR. Moreover, I have concerns about the algorithm that I explained above. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a min-max framework that achieves better performance uniformly for the problem instance.",
            "main_review": "The idea of constructing the min-max object (2) is interesting. However, the paper lacks many details.\n\n1.  The paper did not provide how the policy is parametrized by weights $\\phi$. In particular, do the authors assume the Markovness? If not, the policy space is generally very huge.\n2. The authors did not provide sufficient details on the gradients g^w(i,r),g^{\\tilde{Theta}(i,\\tau), g^\\phi(i,r). In particular, the authors seem to take gradients on $L_T()$. However, $L_T$ is generally a zero-one loss. I am not sure how the authors can obtain a reasonable gradient estimate.\n3. The training for the min-max problem is notoriously unstable. How do the authors comment on the stability of the algorithm?\n4. It seems to me Algorithm 2 is very time-consuming.  Especially, the hyperparameters are chosen to be $N_it = 480000, M = 500$. The authors did not report any time complexity.\n5. The writing is really not clear to me.\n    1. arginf in Figure 1 is not a proper notion mathematically.\n    2. $\\ell'(\\pi,\\theta)$ is not formally defined until Algorithm 2.\n    3. I did not quite get the difference between N and M at the end of Section 3.",
            "summary_of_the_review": "The writing is not clear to me and the authors omit many details. And the proposed algorithm seems not practical.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper tackles the problem of closed loop learning e.g., active learning. Most existing work does not work well in the low query setting. \n\nThe authors highlight that minimax optimisation only optimises the worst-case performance, and thus often leads to suboptimal performance for \"easier\" problems. To overcome this, the authors suggest minimising the worst case performance gap between the learnt policy and the minimax policy across multiple problems sets, where the problems are subsetted according to their difficulty being below a threshold. Intuitively, the policy must perform close to optimal to the \"the easiest problems\", the \"easiest and slightly harder problems\", etc, and thus should perform well for the easiest problems, as well as the hardest problems. \n\nPractically, the authors use two rounds of adversarial training to attempt to solve the posited optimisation problem. \n\nNote that I am not familiar with the related work of the paper, nor the standard baselines used in this field, and not even the standard knowledge. This is reflected in my confidence score. ",
            "main_review": "Strengths:\n* The idea of stratifying over the space of problem instances by difficulty is neat, and a natural way of improving upon the minimax objective. The authors write that the for any r>0, the minimax policy for r \"may not perform well on easier instances\"—it might be worth clarifying that the minimax objective has no \"incentive\" to improve performance on easier problems, even if is possible. I found Fig. (3) also explained this well. \n* The relaxation in Eq. (6) to (8) is a neat and natural way of tracking over multiple different theta. \n\nWeaknesses:\n* The authors write that other algorithms require concentration of measure, and do not perform well in the low query setting. Indeed, the authors write \"in practice, they [such algorithms] are so conservative that for the first 20 samples, they would just sample uniformly\". I would prefer that this point is demonstrated empirically, or at least proven. Additionally, there is a difference between the theoretical guarantee of an algorithm (which might work well only in the high query setting) and it's empirical performance in the low query setting.  \n* The paper pitches itself as \"learning to actively learning\", but active learning is only discussed through the link to combinatorial bandits. The exact relationship between active learning is not explained precisely within the paper, which is fine is the paper is mostly pitched to the bandits community. I wonder if categorising this paper as an active learning paper is somewhat of an oversell. \n* The experimental evaluation of the paper is underwhelming. The tasks considered are at a very small scale. Moving forwards, I would suggest trying to scale up the method, for example even simple experiments on MNIST. \n* The authors claim that the adversarial approach works in the low query setting. I don't think this claim is particularly well supported. For example, why does the proposed approach work in this setting. The authors write that the adversarial setup allows \"policies to be aggressive\", but the reason for this is unclear to me. \n* The proposed method appears to be highly computationally costly. The approach relies on multiple stages of adversarial training, which is notoriously unstable and difficult to get working. At minimum, there should be a discussion of the limitations, especially in regard to computational cost, in the paper. While the proposed technique works better than standard minimax training, it is also substantially more costly, since we effectively have to perform the minimax optimisation multiple times. It's worth looking into computational savings; the minimax problems that are solved here are inherently linked to one another (i.e., over subsets). Perhaps this can be leveraged somehow for computational savings. \n* I would have liked to see more discussions of the weaknesses of the proposed method included by the authors within the paper. \n* The abstract reads \"adversarial training over equivalence classes of problems derived from information theoretic lower bounds\". I do not follow why the bounds in the paper are \"information theoretic\". \n* The authors claim that a limitation of existing work is that they require a prior over possible problems. The introduction reads \"In contrast, our approach makes no assumptions about what parameters are likely to be encountered at test time, and therefore produces algorithms that do not suffer from mismatching priors at test time\". While it is true that the proposed method does not require assigning relative probabilities between different problems, it does require a space of possible parameters/a set of possible problem instances. \n* The authors write that the proposed method is \"robust\" e.g., \"a framework for producing algorithms that are .... as effective and robust as possible\". I do not understand what robustness means in this context. Robust to ... what? And in what sense is the technique developed in this paper robust?\n* I think that the technical presentation of the paper could be improved, especially for readers within the general ICLR community. A few specific examples (not comprehensive): i) Explain what the sets \\mathcal{X} and \\mathcal{Z} represent before they are introduced. ii) Explain the difference between multiarmed and combinational bandits. iii) the definition of rho_* has a number of terms/symbols that have not been defined. \n* The figures of the paper could use a good deal of work. The font is small, titles are missing. The figures are also difficult to interpret without reading the relevant text within the body of the paper, making the paper difficult to understand \"at a glance\". \n* The authors write that adopting a prior over environments is overcoming by the \"adoption of the C(\\theta)\" function. The set of theta needs to be defined, which has similarities to placing a prior over environments. \n\nClarifications:\n* Please discuss the variance of the gradients that are computed using the log-derivative trick (as far as I can tell). These gradients are often high variance; how is this difficulty overcome? Is gradient variance a limitation of the method. \n* The authors move from Eq. (2) to Eq. (3), where we now look at a finite grid of values for r. I assume that this change is made for computational tractability? Please explain in the text why this has been done. \n* Why is there no learning rate for algorithm 2? Or, the learning rate has been assumed to take the value one. \n* Please provide more intuition about the threshold task. The description is mathematic, and I found it difficult to intuitively grasp. ",
            "summary_of_the_review": "I find the idea of mini-max training over multiple different problem difficulties intellectually appealling, but have a number of concerns about the submission at present. In particular, the proposed approach appears to be very costly computationally, the experimental evaluation within the paper is limited, and the presentation is not particularly clear. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethical concerns. ",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}