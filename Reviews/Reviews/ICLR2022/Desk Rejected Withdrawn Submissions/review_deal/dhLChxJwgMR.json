{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents a token pruning framework for vision transformers. The paper proposes a multi-head selection module and a token packaging technique to improve performance of pruned model. It also designs training strategy from hardware perspective, and proposes a hardware-constraint sparsity loss. Experiments have validated the proposed framework can improve performance over other ViT pruning methods, and deploy ViT model on edge devices.",
            "main_review": "1. This paper presents efficiency as its goal (through pruning), but most contributions of the paper is actually not to improve efficiency but accuracy after pruning. The idea of pruning baseline setup is similar to DynamicViT, then on top of that, the paper proposes improvements such as multi-head selector, head score and package token. As the ablation in Table 6 shows, all of these are mainly to improve accuracy after pruning, while efficiency is mostly unaffected. And the accuracy improvements of them are also a bit incremental (about ~0.1% each).\n\n   1.1. The layer-wise progressive training schedule is seen in other transformer optimization papers, such as in MobileBERT, and its improvement is incremental too (+0.02%).\n\n2. The paper emphasizes on \"hardware-friendly\" which is unique from other existing works. The main idea directly related to hardware consideration is the new Hardware-constraint Sparsity Loss. However, I didn't find any experiment designed specifically to evaluate this new loss (e.g. comparing to without the new loss).\n\n   2.1. One opportunity to demonstrate the \"hardware friendliness\" is to compare to other pruning methods deployed on edge devices. But Table 4 and 5 only show HFSP over baseline, so it's hard to say how much advantage the proposed framework has over others on hardware.\n\n3. Two minor comments:\n   \n   3.1. The paper states that Eq.(10) is derived through Eq.(8) and (9). But if I understand correctly, (8) and (9) is mainly used to calculate the pruning rate based on hardware constraint, and the value is then used in (10) for the new loss, but the equation in (10) is unrelated to (8) or (9). So I find the logic of this \"derive\" statement a bit confusing. For easier understanding, I suggest moving the rationale of (10) (\"In order to achieve per-image adaptive pruning rates...\") before the equation is presented.\n\n   3.2. The description of \"Phase Merging\" is quite vague - how to define if pruning rates of adjacent modules is \"similar\"? It might also be helpful to provide more data or analysis on how many phases are left after merging in practice.",
            "summary_of_the_review": "The paper presents several ideas or designs that could be helpful when applying token pruning on vision transformer for hardware deployment. Comparing to existing pruning work such as DynamicViT, these ideas are mainly to improve accuracy after pruning, and the improvements are rather incremental. The experiments are not convincing enough to demonstrate why the proposed method is more \"hardware-friendly\" than other methods. In summary, I find the paper has some merits, but not significant enough, so I'm leaning forwards reject.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Paper proposes a method to enable adaptive inference for visual transformers. Method consists of 2 techniques: 1) token selection module that estimates importance of tokens, and 2) packaging of non informative tokens. Results are presented on ImageNet1K dataset, the method was applied to multiple models.",
            "main_review": "Authors do a great job on motivating their work by analyzing contribution of different modules. Related work section summarizes recent trends quite well. Overall explanation up to the section 4.2 is clear (didn't check on FLOPs in Table 1 yet).\n\nExplanation is a bit confusing when in comes to the framework description in section 4.2:\n1) Eq 1 introduces local and global features. It is not clear why there are 2 sets of them and how they were computed. Seems there is an MLP so all the outputs should be global , local and global? Not clear as both are obtained with MLP, probably there is some tricks with multi head interaction.\n\n2) Why features in Eq 1 are of integer space ($\\mathbb{Z}$)? Probably authors misuse it, and it should be a set of real numbers $\\mathbb{R}$. This seems to be an issues for all variables and all equations. \n\n3) In Eq 1, why $t_i$ is of 2 in the second dimension? It is expected to be 1 having a single value per token.\n\n4) After Eq 4, it seems that 2 is probably referred to 2 hypotheses assigned to every token as 2 classes: 1) important, 2) non important.\n\n5) \"We apply the Gumbel-Softmax technique to generate the keep decision D for input tokens.\", not clear what D is. Usage of GS trick should be explained more. Is it used only during the training?\n\nOverall, this section needs more work to make expatiation clear and careful usage of mathematical annotations. \n\nPackaging token idea is interesting and helps with information loss once tokens are removed.\n\nSection 4.4:\n\n6)  Eq 8 shows some kind of FLOPs representation, eq 9 has HardwareCost that is \"can be obtained by measuring the real hardware performance\". It is not clear how analytical estimate of the compute (8) can be constrained by real measurements in (9).\n\n7) Eq 10 has upper bound $L$ that was already used in section 4.3 and indicates the number of less important tokens. The connection between these two is confusing. Also in correct mathematical annotations, the discrete summation operation has lower bound at the bottom and the upper bound at the top, therefore it should be: $\\sum_{l=1}^L$. Probably there is another reason for it to be reversed in eq 10.\n\nResults:\n\n8) Table 2 présenta results that are confusing. Results of the proposed method are very close to Dynamic ViT, S$^2$ViTE has better results in some comparisons. What is more important is that with model size increase from Tiny to Base we see a larger drop in accuracy of the proposed method with respect to the baseline. It seems that results are cherry picked in the text (less than 0.5$\\%$ accuracy drop) rather than represent general picture. \n\n9) Throughput measurements.  In main results section we see throughput numbers being mentioned for a first time. However, we see no mentioning of the hardware or other specifications. It is even not clear if those are real or theoretical. \n\n10) Number for mobile platforms are great, but in my opinion, we can not count running ViT on mobile CPU with 30FPS to be a significant contribution. Specifically if we look at the accuracy it is below other CNN models with same accuracy.\n\n11) It would be important to compare ViT with dynamic inference to more novel Transformer architectures like SWIN, LeViT etc.\n\n12) Paper emphasizes the fact of being hardware oriented, however, we don't see performance on GPU/TPU devices. For example, I am curious on how much real overhead will be introduced by Token Selector.\n\nMore details are needed on the training/finetuning setup. Were backbones trained at first, and then fine-tuned with selectors? Why authors decided to use 3 selectors only? \n",
            "summary_of_the_review": "Paper studies important application of adapting ViT inference by removing tokens. Overall, the method sounds interesting, however presentation of the method leaves more questions than answers. Details of experiments and comparison to novel transformer architectures are missing. Finally, paper lacks any insights we can learn from the proposed technique; for example, how token distribution changes along the depth, which images are harder etc.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes an approach to prune ViT to achieve a better trade-off between computational demand (MACs) and accuracy. Token pruning for ViT is an active area of research as noted by the authors in related work, and the proposed work claimed to be novel in the proposed hardware-related loss function. The proposed method includes an attention-based multi-head token scoring function, token packing to achieve soft pruning, and gradually placing the scoring function from deep to shallow. The experiments demonstrate the effectiveness of the proposed approach over the selected baselines for various ViT on ImageNet. Ablation studies are included for the reader to understand the contribution of each proposed component.",
            "main_review": "## Strength\n\n- The overall approach is clearly demonstrated and the readers can understand how each part is placed under the full procedure.\n\n- The research topic is important. Given the importance of vision transformers in computer vision, it is important to think about how to better deploy these models.\n\n- The proposed approach is effective. Meaning the proposed approach is shown to be better than related ViT pruning methods.\n\n## Weaknesses\n\n- A lot of emphasis for the method is put on being \"hardware-friendly\", however, to encourage sparsity the used loss function is not hardware-specific. The loss function is really the number of MACs involved, which is not hardware-centric. Additionally, I do not find such losses \"novel\" as claimed (it is at least well-known for pruning CNNs, [1]). Moreover, it is not clear how D is computed in equation 10 and should be made clear. I'd imagine a \"hardware\"-friendly method would use the information of some \"hardware\" such as [2].\n\n- The proposed method involves many hyperparameters but only achieves minor improvements over DynamicViT. More specifically, the pruning rate for each block ($\\rho_i$ in equation 10) has to be determined by users and is non-trivial. Additionally, how long one should fine-tune each selector during the gradual pruning phase? What is the threshold to consider a token unnecessary and thus packing them? One may argue the improvements actually come from a careful selection of these hyperparameters. Also, the concept of gradual pruning is not really new (see [3]) and perhaps applying gradual pruning to DynamicViT would make it similarly competitive as the proposed approach. Note that I'm not claiming the exact approach can be used by DynamicViT but rather the gradual pruning part, and gradually prunes network is known to be effective.\n\n- It is not clear what are the limitations to DynamicViT (the most relevant pruning method). While the proposed treatment is shown to be effective in experiments (including ablation study), the motivation for them is not clear.\n\n\n[1] Gordon, Ariel, et al. \"Morphnet: Fast & simple resource-constrained structure learning of deep networks.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.\n\n[2] Cai, Han, et al. \"Once-for-all: Train one network and specialize it for efficient deployment.\" arXiv preprint arXiv:1908.09791 (2019).\n\n[3] Zhu, Michael, and Suyog Gupta. \"To prune, or not to prune: exploring the efficacy of pruning for model compression.\" arXiv preprint arXiv:1710.01878 (2017).",
            "summary_of_the_review": "Overall the authors present an intuitive approach for improving token pruning in ViT, however, I have concerns regarding comparisons with the related methods, the number of hyperparameters introduced, and the \"hardware\"-friendly part of the method.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}