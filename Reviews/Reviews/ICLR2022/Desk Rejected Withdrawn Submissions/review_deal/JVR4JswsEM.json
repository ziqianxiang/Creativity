{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes an efficient linear-time alternative to the vanilla quadratic self-attention step in the Transformer architecture. Modifying Transformers and ConvNets to use this \"dot product-free\" attention, they compare against recently-proposed efficient transformer architectures like Synthesizers, Performers, Linear Transformer, Reformers on image classification and language modeling tasks.",
            "main_review": "There have been a lot of recent work on efficient transformers, which makes this paper of interest to a large audience, but at the same time makes extensive experimental validation extremely crucial (especially considering that these architectural changes are not theoretically grounded and likely arose through a lot of tuning on benchmarks, which increases the chance of overfitting to them). \nNovelty here, in my opinion, is very limited. While the image experiments are ok, and the choices of efficient transformer baselines to compare against are pretty reasonable, I would like to see more NLP experiments -- LM on enwiki8 is insufficient. In particular, I would suggest running:\n* SuperGLUE\n* GLUE\n* Optional but highly recommended: closed book QA (natural questions, web questions, triviaQA) and machine translation.\nI would also suggest reporting DAFT's performance on the Long Range Arena (https://github.com/google-research/long-range-arena) tasks.\n\nTypos: Some typos in the \"implicit attention\" fraction in Eqn 2.\n\n",
            "summary_of_the_review": "The proposed method DAFT is, in my opinion, not very novel. Since evaluation of empirically-driven architecture papers is largely on the scope and quality of experiments, I can't recommend accepting this paper without seeing strong performance from DAFT (the proposed method) on standard NLP benchmarks like SuperGLUE and GLUE.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a variant of Transformers called Dot Product Attention Free Transformer (DAFT), which eliminates the dot product in self attention. The idea of DAFT is to make the computation of an attention head additive and decomposable; and regard each feature dimension as an individual head, such that all the operators are element-wise. The paper shows that DAFT has the time complexity which is quadratic to sequence length and the space complexity which is linear to sequence length. The experiments show that DAFT is competitive to existing approaches on image classification, image modeling, language modeling tasks.",
            "main_review": "Strengths:\n+ The paper focuses on a main drawback of vanilla Transformer networks.\n+ The idea of the paper makes sense to me. The modification to the transformer architecture is novel. I generally like the idea.\n+ The paper conducts experiments on both vision and natural language tasks.\n\nWeaknesses:\n- The main weakness of the paper is that DAFT doesn’t seem to be much stronger compared to existing efficient variants of transformers (e.g., Performer). Although the paper shows that DAFT has a smaller space complexity by a factor of O(log d), which is usually very small, compared to Performer, the time complexity is still quadratic to the input length. The paper argues that LAT models do not maintain the same exp nonlinear, however, this is not convincing to me that using exp nonlinear is problematic. (In fact, Performer also uses nonlinear mappings (either sin/cos or exp) when it computes attention values).\n- The main point of the paper is that the proposed approach has a smaller space complexity (in Table 1, the paper shows the space complexity is O(Td)). However, the paper doesn’t include a section to analyze the complexity. For example, in Algorithm 1, doesn’t $w$ will take O(T^2) space?\n- The paper lacks some discussion on some detailed designs. For example, one design the paper makes is that the number of attention heads is equal to the feature dimension. What is the motivation for doing this? Is this only an efficiency consideration or also beneficial for model performance?\n\nTypos:\n- Section 5.1: … resulting in a sequence of length **T=1 + H/16 * W/16**.",
            "summary_of_the_review": "The paper proposes a novel approach to reduce the space complexity of transformers, however, the significance of the approach seems vague and the approach and analyses can be written/presented in a better way. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents Dot Product Attention Free Transformer (DAFT), an efficient transformer variant that has a memory complexity linear w.r.t. both the context size and the dimension of features. The authors further introduce DAFT-conv that takes advantage of locality and spatial weight sharing. Experiments are conducted on ImageNet-1K classification, CIFAR-10 and Enwik8 autoregressive modeling.  ",
            "main_review": "Strengths: \n\n1) Writing: The paper is generally well written (though in some places, the text is hard to follow and understand).   \n\n2) Novelty: The proposed DAFT method and its variant DAFT-conv contains certain level of novelty. \n\n3) Experiments: The authors have conducted comprehensive experiments on multiple benchmarks, such as ImageNet classification, and autoregressive modeling.  \n\nWeaknesses: \n\n1) Clarity: To this reviewer, I find the paper is hard to follow. \n\na) For example, in both Abstract and Introduction (e.g., the 3rd paragraph), after reading the text, it is hard to understand what DAFT does. Only after reading Sec. 3.1, the DAFT method becomes much more clear.\n\nb) Sec. 3.2, and Eqn. (8) is also hard to follow to this reviewer. Can the authors explain a little bit more clearly how this is derived?\n\nc) The designed DAFT method shown in Eqn. (4) and (5) is heuristic, so it is unclear to me why empirically, this will translate into better performance. More interpretation is appreciated. \n\n2) Experiments: The reviewer also has some concerns regarding experiments. \n\na) Table 2 shows results on ImageNet classification. For Linear Transformer and Performer, are the corresponding results obtained via self-implementation? If so, can the authors detail more about how they are implemented, and what are the key hyper-parameters chosen?\n\nb) In Table 2, the authors show DAFT-conv results with 384 resolution. What are the results for other methods (like Linear Trans. and Performer) with this higher resolution?\n\nc) For Table 2, can the proposed method be combined with the hierarchical structure as used in the Swin Transformer and Vision Longformer paper? If so, these results can be used to have a more direct comparison with them who also advocate the use of efficient transformer for ViT design. \n\nd) The motivation of using efficient transformer for ImageNet classification is less ideal, since the input sequence is not long. Typically, for object detection, it requires a much higher resolution, which could motive the use of efficient transformer better.  \n\ne) The performance differences between different methods in Table 4 & 5 are all very small. How significant are these differences?\n\nf) Besides the current benchmark results, the reviewer is wondering why the method is not tested on benchmarks like LRA?",
            "summary_of_the_review": "In summary, in many places, this paper is hard to follow, and the current experimental results are not convincing enough to show the effectiveness of the proposed method, therefore, the reviewer decides to give a Borderline Rejection recommendation at this moment. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a replacement for attention, which is more memory efficient and faster than the original proposed by Vaswani (2017) while outperforming previous approximations to the attention mechanism. Their model, DAFT, performs well on image classification, image modelling and language modelling tasks.",
            "main_review": "# Strong points\n-   Good performance on many tasks\n-   It often outperforms Transformers\n-   It should use less memory\n\n# Weaknesses\n-   The proposed mechanism (formula 4) is very unintuitive for me: It is basically a softmax over scalar keys multiplied by a sigmoid of queries. Why is this a good thing to do? I would like to see some explanation on why this makes sense. \n-   The analysis of the proposed mechanism is minimal (visualization of the biases). It would be nice to see the importance of the biases, keys and queries independently.\n-   Although it has a linear dependence on T in activations, the DAFT-full uses T^2 biases for each attention head, which results in similar memory complexity compared to the Transformer. Even the low-rank approximation version computes the full T^2 bias matrix in Algorithm 1. What is the advantage then?\n-   The notation is sloppy in a few places. For example, in formula (4), the quantities are vectors, yet standard multiplication is used. In formula (9), the authors probably wanted to use a different symbol instead of r^i on the right and left sides of the formula.\n-   The results are reported on a single seed without standard deviation.\n-   The authors misleadingly highlight their model in the columns of different result tables.\n\n# Questions/suggestions\n-   In Table 5, the authors highlighted the Train BPC column, where their model is the best, which is misleading. Please fix it.\n-   In Table 4, “Sparse Transformer strided” has lower test loss than DAFT, yet DAFT is highlighted. Please fix it.\n-   Table 1 shows a memory complexity of O(Td), whereas Algorithm 1 explicitly computes w, which is O(T^2). What is then the memory requirement? Is the O(Td) only for the convolutional one?\n-   Please provide an analysis of the proposed mechanism\n-   Why is there a -1 in the biases for the DAFT-conv but not for the DAFT-full?\n",
            "summary_of_the_review": "I feel like the paper is borderline. The proposed model outperforms the baseline models, but, on the other hand, the paper lacks an explanation of why the method works. Moreover, the proposed mechanism looks pretty restrictive, but in contrast, it seems to perform well. The complete separation of the \"keys\" and \"values\", and treating them independently as scalars make conditional matching impossible, which was the core idea behind the original attention. To this end, to recommend acceptance, I would like to see more analysing why and how it works (like visualisation of the contribution of K, Q and w/r components) and ablation studies on the importance K, Q, and w/r.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}