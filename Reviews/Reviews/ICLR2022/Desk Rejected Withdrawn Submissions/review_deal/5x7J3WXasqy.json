{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors approach hierarchical reinforcement learning from the perspective of easier spatial abstraction rather than temporal abstraction. They effectively decompose spatial space by finding locally optimal intermediate goal positions for every inter-vehicle region (IVR). They propose two-level policies that work in the same policy space. High-level policy selects one of the candidate subspaces, and then sub-policy elaborately finds a short-term goal position within a selected subspace. They obtain human-like tactics, especially on narrow and complex roads that haves multiple merging or curved lanes.",
            "main_review": "The problem that they focus is clearly defined. The overall structure of the proposed model and the roles of the four constructed models were well explained. However, it is not novel to find short-term goal positions in HRL. Moreover, quantitative comparison with other recent methods in a reliable benchmark environment is lacking. Compared with other methods that similarly define a short-term goal as a position in state space, it is missing how much the proposed method has improved. Furthermore, the visualization of the subspace selected by the high-level policy is not sufficient.",
            "summary_of_the_review": "It is desirable to emphasize the differences between the proposed methodology and the explanation of the existing methods by separating them by section. The description of the methodology is detailed, but there are not enough experiments to demonstrate its merits.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a two-level spatial hierarchy for reinforcement learning with application to motion planning for autonomous driving on highways. The high-level policy selects a behavioral sub-policy (e.g., “stay in current lane”, “change lanes”, etc.) and candidate inter-vehicle regions (IVRs) that should be “kept in mind” by the low-level policy. The low-level policy generates a short-term goal position within one of the candidate inter-vehicle regions. Both levels share the same value network so the network structure uses the same memory as a single-level actor critic, making the network “seamless.” The approach is evaluated in an autonomous driving scenario that takes in information about other vehicles, range information, long-term goal region, etc. and outputs short-term goals. In comparison with “vanilla”  hierarchical reinforcement learning, the proposed approach results in higher rewards, especially on narrow and complex roads. \n",
            "main_review": "*** STRENGTHS ***\n\nThe paper addresses an interesting problem and, at a high level, the proposed approach is reasonable.\n\n*** WEAKNESSES ***\n\nUnfortunately, I have several concerns against this paper, in particular:\n\n(1) The paper is poorly written and appears incomplete: the grammar throughout the paper is rough and there are typographical errors apparent throughout the manuscript. The writing quality poses a significant challenge to understanding the paper. Some examples: \n- A grammatical mistake in the first sentence of the first paragraph.\n- The authors’ in-text notes in Section 2.2 “**** add math!!!”\n- Section 4.2 refers to “three different designs” but only two are mentioned.\n- Figure 2’s third subplot has an error on the y-axis labels. The bottom four should be negative.\n- An improperly formatted “alpha” in Section 5.2.\n- The last sentence in section 5.3 has a grammatical mistake.\n\n(2) The review of the state of the art is not sufficient: the literature review covers hierarchical reinforcement learning (HRL) for autonomous driving but does not address other approaches to the problem (e.g., non-RL methods, rule-based methods, etc.). Furthermore, the proposed work is not particularly motivated in the context of the existing literature. What does the proposed method offer that others do not?\n\n(3) The problem formulation and algorithm description are unclear: the authors do not clearly formulate the problem and the description of the proposed algorithm is disorganized, with information almost arbitrarily distributed between the main body and the appendix. The description of the proposed method also lacks grounding in the application considered in this paper and therefore remains vague and abstract.\n\n(4) The experimental evaluation appears insufficient, in particular:\n- The experimental design should be better presented, for example, why do vehicles stay stuck for a short time and then disappear after a collision?\n- The results are not sufficiently analyzed and discussed.\n- A “baseline HRL method” is compared against, but not detailed.\n- Training results are provided, but no test set is considered. Thus, there is no indication of the method's ability to perform or generalize outside of the training environment.\n- Figure 3 is not well explained and lacks annotation.\n- Various claims are not substantiated by the results, for example: (i) the abstract claims “[the] method finds the nearly optimal policies from early episodes”, but It is not clear what “nearly optimal” means and this claim does not seem substantiated by the results; (ii) the contributions claim that these methods show “fast optimization speed,” however, no runtime comparison is presented. \n",
            "summary_of_the_review": "This paper tackles an interesting problem, but it has several weaknesses, including (1) poor writing style, (2) insufficient discussion of the state of the art, (3) unclear problem formulation and description of the solution approach, and (4) insufficient experimental evaluation.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed a new hierarchical RL method for highway autonomous driving. The main contribution is the inclusion of state space in HRL. The high-level policy selects both the behavioral sub-policy and safe regions for future state space.",
            "main_review": "Strengths:\n1. The motivation of this study is clear and the proposed method is well explained in a solid way.\n2. The combining of state and policy in HRL is innovative. \n\nWeaknesses:\n1. The paper needs more proofreading. There are some minor errors in the paper. For example, in page 3, \"***** add math!!!\"\n2. I would also like to see comparisons with more baselines. Currently, only base HRL is compared with. Maybe including some regular RL baselines.\n",
            "summary_of_the_review": "The paper is well written but needs more effort and experiment. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}