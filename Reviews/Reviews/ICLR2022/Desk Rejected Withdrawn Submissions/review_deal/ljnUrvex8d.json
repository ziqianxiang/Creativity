{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a new divergence metric between two point clouds of equal size with a 1-to-1 correspondence, called Representation Topology Divergence (RTD). They compete against other representation comparison metrics like Canonical Correlation Analysis (CCA) and Centered Kernel Alignment (CKA), and other variants. The authors proposed RTD as a good way to compare representations learned by Deep learning networks. They introduce a new barcode called R-Cross-Barcode(P,P^\\tilde) which captures the difference in topology of both point clouds in question. RTD is then applied to data from a wide variety of domains spanning CV and NLP, to name a few.\n\n",
            "main_review": "-- Strengths:\n\n1. The paper is mostly well-written (with minor grammatical errors) and easy to follow. \n\n2. As mentioned in the description, the paper proposes a new divergence based on persistent homology of both point clouds being compared. \n\n3. The experiments show that the proposed RTD divergence of the two NN representations (viewed as point clouds) has interesting correlations with the difference in accuracies of the two underlying NNs. This could be an interesting starting point for further investigation, especially from a deep learning theory perspective. \n\n-- Weaknesses:\n\n1. While the paper presents an interesting new barcode to compare point clouds, its superiority over other proposed metrics is unclear. There are several statistical measures that can also be used to compute a “divergence” or even better, an actual metric distance between point clouds, which might do a better job. E.g.: Mahalanobis distance between points treated as instantiations of random processes. \n\n2. I found the synthetic experiments too contrived as they favor homology where the points are positioned in such a way that they create “holes” which the 1-dimensional homology captures quite easily. Why not try more complicated point clouds like the Swiss roll or other more random representations?\n\n3. The paper does not present a time and space complexity analysis of its R-Cross-Barcode and RTD generation algorithms 1 and 2. How does this scale when we have very large representations being generated in high dimensional space?\n\n4. In the experiments, I could not find any comparisons to other baselines. In Sections 3.2, why is there no baseline to compare entanglements and how well a metric can distinguish them. The dSprites dataset is again too simple and does not present a challenge for RTD, or any other comparison metric that takes into account the geometry of the representation. In Section 3.3, again there is no well-known scoring function that is being compared to over here for the NAS-bench-nlp dataset. A comparison to “Neural Architecture Search without Training” - (https://arxiv.org/pdf/2006.04647.pdf) would be good. Look at Figure 3 in the referenced paper. How do your results in Figure 7 compare to their Figure 3? I believe the NAS without training paper elucidates the differences in architectures quite nicely with their scoring function on a much larger set of NNs. \n\n5. In most TDA literature, when proposing a new topological signature it is customary to also theoretically study the stability of the signature. In your paper, I found this stability analysis missing. How robust is the new R-Cross-barcode to perturbations caused due to noise in the output, or in the weights, or other adversarial attacks?\n",
            "summary_of_the_review": "This paper proposes an interesting RTD divergence measure which helps compare two representations from neural networks, represented as point clouds. RTD divergence is interesting, but I don't think it compares against many known methods in neural network literature and to other such metrics, scoring functions etc. There isn't much theoretical justification as to why RTD is superior to other methods like CCA and CKA.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work proposes a topology data analysis (TDA) based divergence to compare neural network representations  in different ambient spaces. It is interesting to see that the topology insights was introduced to the neural network and applied in applications such as computer vision and NLP.",
            "main_review": "strengths: This work proposes a new metric Representation Topology Divergence (RTD) to score the dissimilarity between different representation. RTD score is sensitive to cluster  and enjoys correlation with disagreement of models predictions. \n\nWeaknesses: 1. In  introduction section, author claimed \"Representation Topology Divergence (RTD) score which measures a dissimilarity between two point clouds of equal size with one-to-one correspondence between points\". The question is how to explain \"one-to-one correspondence between points\", what does \"one-to-one\" mean here? It seems this is an important part of contribution, but there is no specific explanation about this so it is confusing me.\n\n2. The writing is not clear. For example,  some concepts in section 2 are not well explained, such as Barcode,  Vietoris Rips complex and homology group are not be familiar with ML community. It should be better to add some figures and intuitive explanation about these abstract topology terms. \n\n3.As the key contribution, author claimed that RTD score is sensitive to cluster and verify this in experiments. However, any theoretical or topological sides should be explained for the sensitive RTD score? \n\n4. It seems that the proposed RTD could be applied to measure any vectors with same size. Why it is specifically works for network representation? ",
            "summary_of_the_review": "It is good to see advanced topology tool was applied to neural network. But the the proposed idea in this paper is not well-supported and  some basic topology concepts are not clear presented. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper focuses on measuring the differences in data representation. The authors propose to evaluate it with Representation Topology Divergence, which estimates the dissimilarity on multi-scale topology. The effectiveness of the proposed method is tested on multiple tasks in the experiments. ",
            "main_review": "Strengths:\n\nIn general, the proposed method looks novel. I have not seen a similar method before. \n\nThe experimental results shown in Figures 4 and 5 look promising. It looks like the proposed method better measures the embedding differences than the competitive methods.\n\nWeaknesses:\n\nIn the introduction, the motivation of this paper is not very clear. The authors might want to better define topology and describe why topology is important in comparing the representation. It is also not clear why CCA and CKA methods cannot capture topology and the problems associated with it.\n\nI have read both the main text and the supplementary; however, I am still unable to understand the details of the method. In Section 2, the authors use the terms \"barcode,\" \"homological methods,\" \"Whitehead theorem,\" \"Vietoris-Rips filtered simplicial complex,\" and \"births\" and \"deaths\" of topological features. I do not understand any of these concepts. It looks like the authors need to briefly introduce the background knowledge about these terms and provide the references properly. The authors also need to explain the intuition why we need to generate the 2N by 2N graph.\n\nIn the experiments shown in Sections 3.2 and 3.3, the methods are not compared with competitive methods either quantitatively or qualitatively. It is not clear whether the proposed method outperforms its competitive methods in these tasks.  In section 3.4, it is not clear why IMD and SVCCA are not tested in the experiments.",
            "summary_of_the_review": "Based on the experimental results, it looks like the proposed method is promising. However, I do not suggest the acceptance of the current version. The authors might need to introduce more background knowledge such that the reader can understand the method proposed. The Experiments Section can also be further improved.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors proposed a method called “representation topology divergence (RTD)” for comparing structured data (e.g., point clouds and sets of embeddings derived by neural networks). The authors applied RTD to various machine learning tasks and compared it with other classic similarity measurements of structured data (e.g., CKA, SVCCA, IMD). Experimental results verify the rationality of the proposed method. ",
            "main_review": "Pros:\n1. The proposed RTD is solid, which captures the similarity between the multi-scale topological information of different point clouds. \n2. The complexity of the proposed method is given.\n3. The authors considered sufficient application scenarios, which demonstrates the usefulness and the universality of the proposed method.\n\nCons:\n1. The paper is not well-written, which requires its reader has sufficient background knowledge of topology. In particular, some content in section 2.1 is hard to understand. Before introducing the Vitoris-Rips complex, the authors should help the readers to build some basic concepts, e.g., given a point cloud, what is its $i$-dimensional topological features? what are the birth and the death of each feature? what is the definition of “barcode”. Although these kinds of stuff are basic for the readers having experience on persistence diagram, the authors should consider broader readers. In my opinion, it may be useful to add a figure in section 2.1, illustrating the basic concepts accordingly in a toy example.\n\n2. According to Algorithm 2, the sampling step will introduce some uncertainty to the calculation of RTD. However, the numerical results in the Tables and the figures shown in the paper did not quantify the uncertainty. I suggest the authors analyze the influence of the number of trials (i.e., the “n” in Algorithm 2) and that of the batch size (the “b” in Algorithm 2), and show confidence interval or standard deviation in the Figures and Tables.\n\n3. If my understanding is correct, the computation of RTD seems to require the point clouds to have the same number of points and their correspondence is known. If it is true, this condition is too strict for some applications — in many cases, the numbers of points may be different and their correspondence is unknown. I am interested in the comparison between RTD and other tools like Gromov-Wasserstein distance [a] and FID [b]. Maybe the authors can consider adding a discussion about the limitations of RTD and its connections to other methods.\n\n[a] Mémoli, Facundo. \"Gromov–Wasserstein distances and the metric approach to object matching.\" Foundations of computational mathematics 11.4 (2011): 417-487.\n\nMinors:\n1. Is RTD a metric?\n\n2. The runtime comparison between the RTD and its baselines is necessary.\n\n3. The claim in the first paragraph that \"Only few methods study learned representations without relying on their quality on a specific downstream task\" is incorrect in my opinion. For structured data like graphs and point clouds, many efforts have been made to learn their representations in an unsupervised way, e.g., contrastive learning, autoencoders, GAN, and recent pretraining strategies. All these methods do not rely on downstream tasks.\n\n",
            "summary_of_the_review": "In summary, this paper indeed provides an interesting idea to compare sets/point clouds, the writing and the experimental part need to be enhanced.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}