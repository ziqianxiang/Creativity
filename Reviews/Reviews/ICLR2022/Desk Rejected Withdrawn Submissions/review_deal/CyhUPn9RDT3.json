{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes IQNAS, a method for  constrained neural architecture search (NAS) demonstrated with the application of designing latency-constrained neural networks. The proposed method is intended to learn a simple and interpretible neural network performance predictor, used in combination  with  a scalable search method with theoretical guarantees. They achieve  this by proposing a quadratic accuracy estimator that measures the accuracy contribution of individual design choices and then maximizes this objective under quadratic latency constraints. The quadratic program  can be solved using an out-of-the-box Mixed Integer Quadratic Constraints Programming toolbox.  Experiments show that the method obtains some improvement in performance over existing methods, especially in terms of reducing the computational cost of search.\n",
            "main_review": "Strengths:\n1. The paper proposes a performance predictor for constrained NAS that can be solved as a quadratic program. This is the main advantage of the proposed method over existing work that uses parametric performance predictors  (e.g. neural nets). \n2. The interpretable nature of the quadratic program allows us to understand what design choices are important for creating a constrained architecture. \n\nWeaknesses/Questions\nThe search space and many design choices in the paper seem identical to HardCoRe-NAS (Nayman et al., 2021). While HardCoRe-NAS uses a different, differentiable method for optimization, the performance gain of IQNAS over  HardCoRe-NAS in terms of obtaining more accurate architectures  with same latency is very small (or in several cases HardCoRe-NAS outperforms IQNAS). The main advantages of IQNAS seem to be a reduction in search cost of about 50%. Can authors comment on the significant improvements in this work over HardCoRe-NAS?\n\nMinor comment: Figure 1 in this paper is identical to Figure 2 from HardCoRe-NAS (Nayman et al., 2021). Please add a citation to that paper in the caption.",
            "summary_of_the_review": "The paper proposes a more efficient to optimize method for constrained NAS and shows modest improvement over prior work using the same search space. I am not sure if the methodological improvement is enough to warrant a publication as an ICLR paper. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper recommends an constrained integer quadratic optimization based surrogate model to predict the accuracy and the latency of a given architecture. They use a macro search space first proposed by HardCoRe-NAS. For every stage, the formulation takes into consideration the individual contribution of every configuration for a given block and the number of blocks in a stage. They have a quadratic predictor and a bilinear predictor.\n",
            "main_review": "Strengths:\nThe paper proposes a novel integer quadratic programming based algorithm to predict an architecture's accuracy and latency. Based on their formulation, they are also able to show how much changing a specific aspect of the architecture such as increasing blocks, changing an design decision such as an operation etc influences the final accuracy of the model. Their search algorithm using the predictor is able to find networks on par with HardCoRE-NAS. \n\nWeakness:\n1. The paper is using the search space and the latency constraint from HardCoRe-NAS paper. They also reused the notations and Figure 2 as is without modifying. I request the authors to rewrite those parts\n2. The method relies on predicting the improvements of each decision made in the search space. However, this might not be very easy to compute accurately. Could you please provide how effectively the formulation is able to predict these improvements wrt the a sample of 500 trained test networks in terms of \"mean squared error\"\n3. Could you also show 2. varies as the number of samples increase similar to Figure 3?\n4. How is the accuracy predictor able to work very well on very few samples when it requires the individual contributions?\n5. For Figure 3., Could you please provide additional baselines used in [1]? While a lot of them work on NASNET search space, some of them can be adapted to work on your search space as well.\n\n[1] How Powerful are Performance Predictors in Neural Architecture Search? White et al.\n",
            "summary_of_the_review": "This paper is very not well written. It was a bit difficult to read it in one go. In general, while they claim that the paper provides insights into contributions of various components there by making it interpretable, they are yet to demonstrate it's accuracy empirically. \nIn general also, the algorithm has two beasts to tackle: (1) getting training data of individual contributions for various components (2) solving an integer program, both of which are not easy in practice. It's performance is not better than HardCoRE-NAS in most cases.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed a method to estimate the accuracy and latency in NAS problem. Specifically, it uses a quadratic formulation for both accuracy and latency predictor, and shows that this simple form is accurate enough in predicting the accuracy of a trained weight-sharing super-network on ImageNet classification. Following the idea of super-network, the authors proposed a NAS framework which use weight-sharing to train a supernet, then use the proposed method to estimate a quadratic model of accuracy predictor and latency predictor.",
            "main_review": "Strength:\nThe main contribution of this paper is proposing the quadratic accuracy predictor and showing such simple model could be accurate enough to reach the comparable NAS accuracy and correlation (Kendall-Tau). The significance of using quadratic estimation model is obvious: they are well studied problem and many tools can be used to get a reasonable good solution.\n\nWeekness:\nHowever, it's still not clear in the advantage of using such accuracy predictor. In Figure 2, the authors show that the correlation is high with this accuracy predictor, but it's not clear how those samples are generated and how is the sampling space, which are very important. For example, if different samples have distinct capacity differences, it's easy to get the predictor to predict the larger model to have better accuracy. However, in the NAS search, we want to know with the same model size/flops/latency, how is the accuracy's ranking. So it's better to show that given the similar latency, how is such rank correlation. In the experiments, to fully validate the effectiveness of the proposed accuracy predictor, I suggest the authors to apply the proposed predictor onto other NAS frameworks, instead of proposing another one, which makes the advantage of the accuracy predictor not clear. I would also recommended the authors to conduct experiment on other search space and tasks, to further validate the proposed accuracy predictor.\n",
            "summary_of_the_review": "Except the accuracy predictor part, the latency predictor and NAS framework parts are very similar to the HardCoRe-NAS paper. I can only access the contribution of this paper over HardCoRe-NAS to be marginal. As the author didn't show convincing results on purely using the accuracy predictor.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The Quadratic form itself is somewhat interpretable.\nThe experiment results surpass or are comparable with more complicated predictors.\nSome insights related to the architecture design are provided by analyzing the derived Quadratic form.",
            "main_review": "1. The predictor baselines compared are insufficient. There are more predictors other than MLP or Bilinear. More baselines in White (2021) are suggested.\n2. The way to estimate the contribution of different terms of the accuracy estimator (4.2.1) seems unclear. A question for authors: How are the two variants implemented? By simply setting the coefficient in the optimized quadratic form to 0, or set them to 0 before optimized?\n3. Though interesting, the insights are not verified by further experiments. We cannot identify whether the insights are correct.",
            "summary_of_the_review": "This paper proposes an interesting view to solve nas problem using an interpretable way. The proposed a simple Quadratic accuracy estimator provides some insight on how to build architectures.\nHowever, there are still many unclear parts about the algorithms, experiments, and insights. Further analyses and explanations are needed.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The main contribution of this paper is the introduction of a quadratic programming accuracy predictor based on the architecture parameter.\nSuch predictor enjoys a closed-form solution and is more sample-efficient than commonly used MLP-based predictor.\nCombining the predictor and the latency constraint, the author also proposes an optimization method based on the Frank-Wolfe algorithm.",
            "main_review": "**Strong Points**\n- I like the idea of using simple quadratic formulation for the accuracy predictor, which is more concise and easy to interpret.\n- It's good to present the insights for the architecture design from the learnt predictor like increasing the expansion ratio for deeper stages.\n\n**Weak Points and Question**\n- This paper resembles the HardCoRe-NAS [1] to a large extent, the writing for the introduction is identical, Figure 1 in this paper and Figure 2 in [1] are exactly the same. Theorem 3.3 and Theorem 3.1 in [1] are similar. Besides, [1] also introduces the quadratic programming formulation and they both use the block coordinate SFW to solve the optimization problem. Moreover, the performance of IQNAS proposed in this paper over HardCoRe-NAS is marginal and not consistent.\n\n[1] Neyman et al. HardCoRe-NAS: Hard Constrained diffeRentiable Neural Architecture Search. 2021.",
            "summary_of_the_review": "This paper is well-written and professional, but it has lots of overlap with HardCoRe-NAS including algorithm and experiment, and only achieve similar results, so I vote for rejection.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}