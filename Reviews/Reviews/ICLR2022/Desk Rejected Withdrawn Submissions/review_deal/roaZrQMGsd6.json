{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes CARD, which is a certified robust machine learning pipeline by taking advantage of knowledge integration. Instead of using a single classifier, CARD first gets a predictor vector from knowledge predictors, and then use a one-layer reasoning network to get final prediction results. The predictor vector contains prediction results from main predictor, attribute knowledge predictor, and hierarchy knowledge predictor. The paper shows that, with additional assumption on knowledge predictors, CARD has a tighter possibility bound than traditional randomized smoothing methods. This paper also evaluates CARD on AWA2 and Word50 dataset to empirically show the effectiveness of CARD.",
            "main_review": "Strength:\n+ This paper theoretically shows that CARD achieves better-certified robustness with additional assumptions on knowledge predictors.\n+ The evaluation results show that CARD has better-certified robustness compared to traditional randomized smoothing methods.\n+ Writing is clear and easy to follow.\n \nWeakness:\n- Although CARD is proved to be more robust than traditional methods, the additional assumption (assumption 3.3) is used. Hoeffding’s inequality cannot hold without this assumption. From a theoretical perspective, it is not surprising to see CARD has a tighter bound with the additional assumption.\n \n- Authors only evaluate CARD on AWA2 and Word50, which are not common datasets for evaluating model robustness. A potential issue is that other SOTA works may achieve a much worse performance due to a lack of necessary adjustments. Compared to traditional randomized smoothing, extra information (attributes) is used to train the model, which could be a potential reason for the improvement on the robustness.\n \n- This method has limitations on the dataset. To support the training of knowledge predictors, datasets need additional annotations, which could make CARD difficult to apply on other datasets.\n\nQuestions:\nThis paper doesn’t discuss the properties that knowledge predictors should have. I am wondering if we can define binary knowledge predictors like “is a cat”, “is a dog”, “is a frog” for the CIFAR10 dataset? In this case, we can evaluate CARD on CIFAR10 or other datasets commonly used for robustness evaluation.\n",
            "summary_of_the_review": "The theoretical results that CARD can achieve better robustness with assumption on knowledge predictor are novel but not surprising. My concern is it may not be compared with other baseline methods fairly.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to overcome limitations of data-driven-only defenses to integrate domain knowledge from exogenous information to improve certified robustness of ML models. Intuitively, it prevents attacks in a certain category. For example, it can represent domain knowledge of characteristics of certain cats (e.g., persian cats). They then embed the reasoning framework based on first-order-logic rules into a 1-layer neural network. They compare their method against a few \"certifiable robustness\" baselines, and show their method improves the overall robustness even in presence of slightly different knowledge rules.",
            "main_review": "Strengths: \n- The question addressed by this paper is highly interesting and relevant to progress the field of robustness \n- Thorough theoretical modeling\n- Experimental evaluation with four SotA certifiably robust baselines \n\nWeaknesses:\n- Requires manual definition of knowledge rules\n- The neural network used to integrate knowledge integration could be victim of an adversarial attack itself \n- Unclear what happens if knowledge rules are incomplete or introduce contradictions (e.g., for more complex situations)\n\nComments:\n\nI truly appreciate the task tackled by the authors of this paper. I think that integrating domain knowledge within the classifier is a fundamental step, and here the authors propose an interesting approach based on first-order logic and adapting the knowledge rules to a 1-layer neural network. The results clearly show that this method achieves a better certified robustness also in presence of slightly different knowledge rules. \n\nI have one major concern: recently, in [1], it's been shown that you can attack post-hoc explainability methods since they are based on statistical methods as well. So you cannot rely on post-hoc explanations to assess the robustness of the model per se, at least in the context of deep neural networks. Similarly, here the authors \"embed\" the knowledge rules in a 1-layer neural network, and apply it \"post-hoc\" (e.g., Figure 1). My concern is that, although knowledge rules make sense, an attacker may want to attack the \"Reasoning\" phase, thus making the \"constrained logic encoded neural networks\" victim of an adversarial attack themselves. I feel this is a very important evaluation and aspect to consider, as it may compromise the effectiveness of the overall methodology, leading to a false sense of security. In other words, for this first, major point I'd suggest to perform adversarial attacks directly to the reasoning framework itself.\n\nApart from this, I also have some other recommendations and comments:  \n- It would be nice to have some brief experiments to empirically evaluate how good the neural network approximates the first-order logic knowledge rules. For example, if you embed rules for \"persian cats\", does it work well on vanilla datasets and learner? or does it commit mistakes?\n- I appreciate that to embed domain knowledge you are somehow obligated to do it manually. However, for more complex domains I imagine that manual definition of knowledge rules may involve also contradictions or similar things. What would be a strategy to prevent this, or which impact on false positives / false negatives do you envision?\n- Related to my first point: in a sense, it seems you are proposing an \"ensemble\" of two learners to improve the robustness of a baseline learner. How does your work relate to ensemble learning literature which attempted to improve robustness?\n\nReferences:\n- [1] Zhang, Xinyang, et al. \"Interpretable deep learning under fire.\" 29th {USENIX} Security Symposium ({USENIX} Security 20). 2020.\n\n\n\nEDIT:\nThe answers to my concerns from the authors are strong and convincing, with relevant citations and specific pointers to the paper. I think the authors should integrate these answers to improve clarity of the paper itself. ",
            "summary_of_the_review": "My major concern is with the fact that the proposed method is similar to an ensemble, and without an empirical evaluation of the robustness against attacks of the \"reasoning\" part, I feel this architecture may be giving a false sense of security. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes to integrate domain knowledge, phrased through logical formulas, into the training of ML models\nin order to increase certified robustness to local perturbations. They show how these rules can be phrased as probabilistic graphical\nmodels, and then mapped to 1-layer neural network which enables efficient training. In the experiments, proposed method is compared\nto prior randomized smoothing approaches.\n",
            "main_review": "**Strengths:**\n\nAt a high level, the idea seems promising - if model makes more explainable decisions such as using knowledge based rules,\nit should be more difficult to find adversarial examples for it. I think writing is generally fine and ideas are properly exposed (except in Section 3.2 where it would benefit from more details).\nIn the experiments, authors compare to several prior works on randomized smoothing to show benefits of their approach.\n\n**Weaknesses:**\n\n\nMy biggest problem is that I am not quite sure that the Assumption 3.3 generally holds, which makes robustness certificate different to the one provided by prior works on randomized smoothing.\nNamely, authors argue that correlation between different predictors is low, and thus they assume that these predictors are independent.\nBut even looking at the example from Figure 1, this assumption is clearly too strong.\nFor example, \"primate detector\" and \"tail detector\" are obviously very correlated, thus violating the assumption.\nGiven that the certificate of robustness of CARD in Theorem 2 is based on this assumption, this certificate is weaker than\nother certificates based on randomized smoothing such as those by Cohen et al. or Salman et al., which do not need such assumptions.\nFor example, instead of this strong assumption, authors could use union bound to compute the guarantees for CARD, but then the results would likely\nnot be as good. Further, when we are directly comparing CARD with other randomized smoothing approaches in Figure 3, for a fixed noise level sigma\nconfidence of different methods is likely very different, and I would suggest authors to comment on this issue.\n\nThere are also few other things that could be improved.\nIn Figure 1, I would suggest to annotate each predictor f_0, f_1, ..., f_n,\npredictor vector s, and the reasoning component r. Right now figure is very high level and does not connect well to the notation used in Section 3.1\nI believe Section 3.2 can be rewritten with much more clarity. There are several different heuristics used to train the CARD pipeline which are right now\nnot elaborated in details. One option would be to include some pseudocode that explains different steps and how they interact together.\n\nTypo: robsut -> robust\n",
            "summary_of_the_review": "While I find the idea of the paper nice, my biggest problem is that I do not think that the Assumption 3.3 generally holds, which makes robustness certificate weaker than the one provided by prior works on randomized smoothing. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a novel method to encode human knowledge in a DNN model and prove the robustness of its proposed framework. Technically, the proposed framework has a prediction and reasoning component. The prediction component has a set of predictors that decides the True/False of each knowledge. The reasoning component encodes the logical rules and leverages the prediction results of knowledge predictors to make the final decision. The paper compares its method with three existing defenses on two datasets to demonstrate its superiority. ",
            "main_review": "This paper proposes an interesting idea of building robust DNN classifiers by encoding human knowledge into the classifiers. I like the idea of encoding human knowledge/heuristics into learning models. Besides improving adversarial robustness, properly encoded heuristics could also help reduce the amount of training data required and potentially improve the convergence speed of the training process. The technique proposed in this people is mostly correct with some minor flaws (discussed below). The evaluation is comprehensive, and I appreciate that the authors conducted an ablation study and provided the source code in the appendix. The paper is overall well-written. The written quality is above the acceptance bar. However, the following limitations pause me from giving an acceptance. \n\n1. The paper is a little bit overclaimed. In the abstract and introduction, the paper states that the key limitation of existing certified defenses is they cannot guarantee robustness against new attacks beyond the defined attack space (i.e., norm balls). Motivated by this limitation, the paper proposes its own method. However, from the theoretical and empirical results, the paper actually does not show its proposed method could tackle this limitation. Instead, it shows that under the same norm ball, the proposed defense performs better than some existing defenses. IMHO, this makes the contributions a little bit incremental. In short, the paper motivates its technique as a ground-breaking one. Later, it turns out that the proposed technique is yet another defense that may still suffer the limitation of cannot defending against out of defined norm ball attacks. \n\n2. Another major limitation of the proposed technique is it may open more attack space. Existing defenses have a relatively simple architecture, and the adversarial attack space mainly adds perturbations into the input. As discussed above, existing defenses can guarantee to defend against this adversarial attack within their defined attack space. Regarding the proposed attack, despite it can achieve a better robustness guarantee than existing defenses against the existing adversarial attacks, it may open some new attack spaces. For example, attackers could encoder wrong knowledge into the model or manipulate the outputs of the predictors to fool the reasoning part. Given these potential new attacks, the robustness guarantee provided by this paper may be even weak than existing defenses. Overall, as a defense paper, it is important to show its effectiveness against existing attacks and, more importantly, discuss and evaluate the potential countermeasures when attackers are aware of this defense mechanism.\n\n3. I appreciate the authors providing a theoretical analysis. In the theorem, the authors show that the proposed technique gives a higher prediction probability than the main predictor on the true class. Here, a problem is the authors do not specify what this main predictor is. Say we have a vanilla DNN F and an existing defense F'. I assume this theorem only works for the case when F' is the main predictor. It may no longer hold if using F as the main predictor and compare the defense capability with F'. In other words, the proposed method is like an add-on to existing defenses. I would appreciate it if the authors could clarify this point. \n\n4. The evaluation results are not clearly explained. Specifically, I have two questions, (1) As radius increases, the proposed method may work even worse than baselines (the upper left figure in Fig. 3). What is the reason behind this result? (2) Fig. 3 also shows that the performance of the proposed method may drop dramatically as the radius increases. I would suggest the authors also comment on this. \n\n5. I am also concerned about the computational complexity and scalability of the proposed technique. Regarding computational complexity, the proposed method requires training many neural networks. Regarding scalability, what if the datasets do not come with predefined heuristics? I would suggest the authors comment on these aspects.\n\n6. The paper misses an in-depth discussion on ethical concerns. \n\n",
            "summary_of_the_review": "Strengths:\n1. The paper proposes an interesting and novel idea of defending against adversarial attacks through encoding human knowledge. \n\n2. The paper conducts a comprehensive evaluation.\n\n3. The paper is overall well-written, and the proposed technique is clearly presented. \n\nWeaknesses:\n\n1. The paper is a little bit overclaimed. The theoretical and empirical results cannot fully support the original claim of this paper, making it a little bit incremental.\n\n2. The paper does not discuss or evaluate the potential countermeasures (See above for detailed discussion).\n\n3. The evaluation results are not clearly explained. \n\n4. The paper misses an in-depth discussion on ethical concerns. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety"
            ],
            "details_of_ethics_concerns": "The paper claims its proposed technique will not trigger any ethics concerns. IMHO, I do not agree with this argument. Although it is designed for benign users to improve the robustness of their networks, it can also be utilized by attackers to launch new types of attacks. This will trigger new concerns on the security of the DNNs. In addition, if the encoded human knowledge is biased, the corresponding network would be unfair, which triggers another ethics concern. I would suggest the author dig deeper into each possible ethical aspect and provide a more in-depth analysis of the ethical concerns.",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}