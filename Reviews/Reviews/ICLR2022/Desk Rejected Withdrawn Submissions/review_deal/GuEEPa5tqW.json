{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies multivariate time series classification by combining signal processing (Fast Fourier Transform and Inverse Fourier Transform) and machine learning (CNN) approaches. Authors make analysis of the impact of each component to the efficiency and accuracy of classification by conducting ablation study, pruning and pareto analysis.",
            "main_review": "Strenghts:\n\n1. The main idea of making analysis of the efficiency and accuracy is interesting\n2. For reproducibility of the approach, the implementation details are provided\n\nWeaknesses:\n\n1. The decrease in accuracy is quite high with component pruning\n2. The analysis discussion is poor especially in Section 4.7 Pareto Analysis, which is listed as contribution of this paper\n3. After the analysis there is no clear proposed model for an efficient model \n4. There is no comparison to efficient other approaches\n5. The choice of time series datasets, almost half of the time series have length of less than 100-150 data points. It is not clear if the analysis applies to much longer time series dataset\n\nThe paper misses the comparison to the efficient time series models with sparse attention.\nLi, S., Jin, X., Xuan, Y., Zhou, X., Chen, W., Wang, Y. X., & Yan, X. (2019). Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. Advances in Neural Information Processing Systems, 32, 5243-5253.\nWu, S., Xiao, X., Ding, Q., Zhao, P., Ying, W. E. I., & Huang, J. (2020). Adversarial Sparse Transformer for Time Series Forecasting.\n\nThe lengths of the datasets chosen for Pareto analysis are 50 (Finger Movements) and 405 (Heartbeat). As the efficiency becomes more important for longer time series, it would be interesting to see the Pareto analysis for longer time series, see Table 3 in Appendix.\n",
            "summary_of_the_review": "Although the efficiency analysis idea is interesting, the discussion of the results is poor (see Section 4.7), it is not clear what authors suggest to use as a final optimal model. Furthermore, one expects to see “the proposed model” to be compared to the existing approaches in terms of accuracy and efficiency. There is no comparison to the existing approaches, only the analysis of the impact of each component to the accuracy and efficiency of the model. Currently, it is not clear if the proposed approach is better than the existing approaches. I believe the paper can be submitted to other venues after improving the conclusions of the analysis and completing the experiments with comparison to existing approaches.\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concerns.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposed a transformer-based architecture for time series classification.\nThe architecture consists of 1D convolutional embedding followed by multiple encoders, each encoder consists of 2D Fast Fourier transform followed by Inverse Fast Fourier transform then the typical multi-head attention and a feed-forward layer after the encoders there is a global averaging pooling layer to the output over the entire time dimension and a linear layer used for classification. The paper investigates the effect of each layer on the efficiency and its effectiveness by layer by layer pruning and calculating the accuracy after pruning each layer long with the number of parameters and training time. The paper investigates the trade-off between efficiency and performance using Pareto analysis. Experiments were done on 18 datasets.\n",
            "main_review": "Strength:  \n1- The paper applied the proposed architecture to 18 datasets.  \n2- The ablation study investigating the effect each layer has on accuracy and efficiency was interesting and informative.  \n3- The use of Pareto analysis to analyze the accuracy and efficiency was novel.  \n\nWeakness:\n\n1- Missing a lot of related work including informer[1], log sparse transformer[2], Reformer[3] all of which are transformer-based architecture designed to enhance the efficiency of the typical transformer.  \n2- It is unclear what the is purpose of FFT directly followed by IFFT don't they cancel the effect of each other?  \n3- It is unclear why the proposed architecture does not contain a typical add and normalize layer (is it for efficiency reasons).  \n4- There was no comparison with other baseline transformers in terms of accuracy and efficiency.  \n5- The novelty of the work is very limited, the difference between a typical transformer is the FFT directly followed by IFFT and the removal add and normalize layer (and again there was no discussion on why this is particularly useful).  \n \n\n[1]Zhou, Haoyi, et al. \"Informer: Beyond efficient transformer for long sequence time-series forecasting.\" Proceedings of AAAI. 2021.  \n[2]Li, Shiyang, et al. \"Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting.\" Advances in Neural Information Processing Systems 32 (2019): 5243-5253.  \n[3] Kitaev, Nikita, Łukasz Kaiser, and Anselm Levskaya. \"Reformer: The efficient transformer.\" arXiv preprint arXiv:2001.04451 (2020).\n",
            "summary_of_the_review": "My main concerns are the following:\nThe paper proposed an interesting way to measure the efficiency and accuracy of each layer in the proposed transformer. \nBut it is unclear why the proposed architecture is more efficient than a typical transformer. There was also no comparison with other baselines transformers in terms of accuracy or efficiency. So basically, the paper is an analysis of different layers of a transformer. This makes the significance and the novelty of the paper quite low.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Paper solves the time-series classification problem in a computationally efficient way using modified transformer architecture that includes the Fourier transform inside the transformer block.",
            "main_review": "The topic of the paper is interesting, the architectural approach makes sense. I have two major concerns about the paper.\n- The architectural approach is not novel. In this paper (note it already has 30 citations): https://arxiv.org/pdf/2105.03824.pdf the FFT has already been used to accelerate Transformer. Moreover, I find the approach of the arxiv paper more elegant and potentially more compute efficient, since they entirely remove the self-attention layer and unlike the current approach they do FFT only once per block. I don't understand, by the way, why FFT is done twice in the current approach?\n- The empirical results only include the ablation study of the proposed approach. I do not see any accuracy and compute efficiency comparisons with existing approaches. The empirical results are not at all convincing.",
            "summary_of_the_review": "I recommend reject, because novelty is limited and empirical results are unconvincing.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors propose a mixing framework based on Transformer and Fourier transform. By pruning each module of the network separately and sequentially, the paper investigates the impact of each module on predictive accuracy. The paper conducts comprehensive experiments on 18 benchmark MTS datasets. Ablation studies are used to evaluate the impact of each module. Through module-by-module pruning, the results demonstrate the trade-offs between efficiency and effectiveness, as well as the efficiency and complexity of the network. Finally, we evaluate, via Pareto analysis, the trade-off between network efficiency and performance.",
            "main_review": "Strengths: \n1. Develop the first mixing model architecture based on attention and Fourier transform for processing MTS data.\n2. Experimental results indicate the trade-off between model efficiency and its effectiveness, as well as its complexity.\n3. Propose to investigate the trade-off between efficiency and performance using Pareto analysis. \n\nWeaknesses:\n1. The motivation is unclear. The authors claim that the proposed method can work on high-dimensional or long-sequential time series. However, the datasets used in the paper do not support such a strong assumption. \n2. Baselines are weak, some representative methods mentioned in related work are not included in the experimental comparison. \n2. More theoretical studies are expected. It would be better to analyze the proposed method quantitatively and explain why and how to tune the parameters between DFT and Transformer. ",
            "summary_of_the_review": "This work proposes a mixing network based on Transformer and Fourier transform for MTS classification. Experiments are conducted on 18 MTS datasets, including ablation studies on different modules of the network, module-by-module pruning evaluated in terms of the predictive performance, training speed, and the number of learnable parameters.  However, the paper lacks theoretical studies, why combining DFT and Transform is efficient is not discussed. Plus, more datasets can be used as the testbed of high-dimensional or long-sequential time series, as claimed the main strength of the proposed model. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}