{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This work present a recursive operation on ViT to improve its parameter utilization. Specifically, it make the weights shared across depth of the ViT. However, such a naive recursive operation will cause additional computation, so this work propose an approximating method to slice the self-attention into different groups across recursive layers to save the computation cost (i.e., FLOPs). The experiments in ImageNet shows good performance as compared to vanilla ViT, and such an operations can help to building ViT with 100 or even 1000 layers.",
            "main_review": "Strengths\n> + All experiment settings are clearly demonstrated, which is helpful to reproduce this work.\n> + Including both the CV tasks and NLP tasks\n\nWeaknesses\n> + The motivation to save number of parameters is not clearly demonstrated: The core motivation of this work is \"improve parameter utilization\", but this is not clearly demonstrated. I would like to know how the number of parameters will help the real device performance under the same FLOPs? i.e., what's the benefit of reducing the number of parameters but same FLOPs (although this work also propose the group self-attention to reduce FLOPs) ?\n> + The novelty is limited: As mentioned in the related works of this paper, Recursive operations has already been explored in Transformer for NLP tasks, applying it ViT seems to be trivial and there is no discussion on what is special problems when using recursive operations in VIT as compared to Transformer for NLP tasks. Also the sliced group self-attention also seems to be similar with the window self-attention proposed in Swin [1], I think it would be better to discuss more about the differences of this work from Swin [1].\n> + The spatial pyramid design seems to be a very strong components, while it is not proposed by the author, I think more detailed ablation study on it is needs: For example, the authors show the hierarchical feature maps in Figure 6, and claim it is \"enabled by recursive and spatial pyramid \", however, the baseline in Figure 6 is only DeiT, which only verifies effectiveness of the spatial pyramid design, so I think Swin/PiT can also be included into the consideration to show the effectiveness of recursive operations. Same suggestions to Table 1 to include a non-recursive version models to better demonstrate the performance comes from better architecture design or recursive operations.\n\n[1] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021.",
            "summary_of_the_review": "The main points to affect my scores are:\n1) the novelty is limited and 2) insufficient experiments/analysis on the performance comes from better architecture design or recursive operations. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes to share the weights of the blocks in the vision transformers in order to reduce the number of parameters used. The paper also proposes an approximating method through multiple sliced group self-attentions across recursive layers which can reduce the cost consumption. The paper has results in image classification on ImageNet and in neural machine translation.",
            "main_review": "### Strengths:\n- **Simplicity:** The method is simple and easy to understand\n- **idea:**  Trying to make transformers more efficient is an important research topic that can have applications in many fields.\n- **Code:** The paper promises to share its code which is a good practice.\n\n### Weakness:\n- **Efficiency:**  It would be interesting to add the number of FLOPs, memory consumption and speed in all the tables. For example, the number of parameters is not a perfect metric because it does not consider the activation. It is currently difficult to judge the interest of the method.\n- **Baseline:** Why use the Spatial Pyramid Transformer model as baseline ? The ViT architecture seems more classical. Table 4 It would be interesting to have results of the proposed method with different architectures in order to validate its interest and to see if it adapts easily to different architectures.\n\n- **Comparison (training):** The paper states : \" On ImageNet-1K, we mainly follow the training settings of DeiT [Touvron et al., 2020] for fair comparisons.\" but Table 6 shows that the training parameters used for SReT and DeiT are different. Indeed, SReT uses a batch size twice as small and so makes twice as many updates during the training. It is therefore difficult to separate the impact of the training from that of the proposed method.\n\n- **Comparison (backbone):**   the paper state \"SReT-T also outperforms DeiT-T by 3.8% with 15.8%↓ parameters and 15.4%↓ FLOPs.\" p7 but the \"backbone network is a spatial pyramid [Heo et al., 2021] architecture.\"  the comparison with DeiT seems therefore unfair as the proposed method takes advantage of the improvements of Heo et al.\n- **Distillation:** \"Our loss is a soft version of cross-entropy between teacher and student’s outputs\" p6  and \"Soft Distill (Ours)\" Table3. The formulation of the loss of soft distillation used here does not seem to be new and therefore does not constitute a contribution of the paper. The use of \"ours\" seems odd. In any case, if it is a contribution of the paper, the actual comparison seems insufficient to judge the superiority or not of this approach compared to competing approaches. Indeed, the comparison is made with a single unconventional architecture( because it uses the method proposed by the paper). \n- **Image classification dataset:**  For image classification there are only results on ImageNet it would be interesting to have results in transfer-learning, on ImageNet-v2 and ImageNet-Real to see if the method is general.\n- **Standard deviation:**  There is no standard deviation which makes it difficult to evaluate the significance of the results.",
            "summary_of_the_review": "Improving the efficiency of transformers is a very important research topic. Nevertheless, the paper does not allow us to judge adequately the interest of the method since it lacks some metrics to evaluate its efficiency. Moreover, due to the different backbone and training procedure used, it is difficult to determine if the comparisons are correct.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents recursive transformers, which can reuse the weights through recursive operations. In this way, the transformer architecture can be very deep without introducing more network parameters, and thus the model size is kept small. Moreover, in order to reduce the computational complexity of global self-attention, the paper also proposes \"group/local self-attentions with permutation.\" This operation can approximate global self-attention with few FLOPs. The paper conducts experiments on both image classification and neural machine translation, showing impressive performance.\n",
            "main_review": "Overall, the paper presents a good transformer architecture and conducts comprehensive experiments and analyses. The performance is also good. But I have some concerns about the novelty of this paper.\n\n1) The idea of using recursive operations between layers of neural networks has been proposed in [1] many years ago (in 2015). To me, this paper just applies this idea to transformers.\n2) The proposed \"group/local self-attentions with permutation\" is essentially similar to the trick of decomposing one 7x7 conv layer into three 3x3 conv layers. This is very common in CNNs. Even in self-attention, a similar idea was proposed in OCNet [2] (Fig. 2).\n\n[1] Recurrent Convolutional Neural Network for Object Recognition, CVPR 2015.\n[2] OCNet: Object Context for Semantic Segmentation, IJCV 2021.\n\nIn addition, there is a minior point in this paper I do not understand.\nFor Eq. (4), why do the authors think this tends to learn a trivial solution?\n",
            "summary_of_the_review": "Overall, I acknowledge this paper is an OK paper but not novel enough.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a recursive operation on vision transformers (called SReT) by sharing weight across the depth of transformer networks.  In this way, the authors can build transformers with more than 100 or even 1000 layers under a small model size (13-15M). As a result, the proposed method can obtain a substantial gain of 2% and reduce the cost consumption by 10~30%. Moreover, many experiments demonstrate the effectiveness on ImangeNet over state-of-the-art methods.",
            "main_review": "1. The expandability of the proposed scheme should be discussed in this paper. The authors conduct experiments using DeiT Transformer. Recently, Swin Transformer attracts more attention in many computer vision tasks. Can SReT be used in Swin Transformer and improve the performance? In addition, the recursive way can be used in a video sequence. Can the proposed SReT be used in video tasks, e.g., video classification? It would be better to provide more discussions about these.\n\n2. The novelty of the proposed method should be highlighted.  The proposed recursive Transformer seems to be a trick by sharing parameters of Transformers. In Eqn. (6), the transformer is used twice in every block. In this sense, the transformer trains the data twice in every epoch. It may be fair to compare Swin-T with more epochs. In addition, sharing parameters in different layers in Transformer may damage the performance since features in different layers have different representations.\n\n3. The recurrent neural network is a type of recursive neural network. What is the difference between SReT and RNN? SReT does not learn a hidden state. Does SReT have an advantage over RNN based model?\n\n4. This paper provides a theoretical analysis for the equivalency of global self-attention and group self-attention with recursive operation on FLOPs. More importantly, it would be better to prove the approximation error between the global attention and multiple sliced group-attentions.\n\n5. In the experiment, the authors only compare with Swin-T. It would be better to compare SReT with Swin Transformer with different sizes of parameters.\n",
            "summary_of_the_review": "The novelty of the proposed method should be highlighted, and the experiments can be further improved.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work presents a recursive operation for improving the model size of vision transformers (ViTs) by weight sharing across depth/layer. It is motivated by the observation that the weights and activations of adjacent layers in ViTs are similar. Preliminary experiment show that naive recursion with ViTs can improve the performance on ImageNet-1K. Both ViTs and All-MLP are considered with recursion. To reduce the computing cost, the authors further apply group self-attention in replacement of standard global self-attention. Experiments on ImageNet-1K and IWSL'14 German to English, and WMT'14 English to German are conducted to validate the proposed model. ",
            "main_review": "**Strengthes**\n\n1) Reducing model parameters for ViTs is an interesting and worth-to-study problem\n\n2) Very good writing with clear & clean description throughout\n\n3) Extensive study in both recursive related deep models with applications to both ViTs and MLPs\n\n4) Extensive experiments on vision and NLP tasks\n\n**Weaknesses**\n\n1) The group self-attention is essentially local window self-attention which is not new and not specific for recursive operation, for example see [a, b, c]. I reckon this is the most fundamental defect of this work. Besides, the recursive operation has been also used Universal Transformer [Dehghani, ICLR 2019], it is not the first time for this work.\n\n2) While recursive operation can reduce the model size, it will multiply the FLOPs or compute cost. Often, most resource-limited platform are constrained in both memory and compute power, rather than in only one aspect. It would be helpful if the authors can specify some special cases. \n\n3) Table 1: The comparison is not complete as the baseline with the same depth is not presented for baseline eDiT-Tiny. As a result, it is unknown if a SReTwill outperform its baseline with the same depth, and how much.\n\n4) Experiments: To explicitly examine the effect of SReT models, I would think the best way is to directly train and test them on each selected dataset and compare with the counterpart without the recursive operations, under both the same-parameter and same-depth settings. However, the authors choose to use DeiT style with the use of knowledge distillation. This will make the evaluation hard to understand since extra training steps are added so that the results are mixed with unrelated effects. Besides, I do think group self-attention or local attention is not specific to recursive operation, and comparing with existing methods without using local attention is somewhat unfair in both compute cost and accuracy. \n\n**Refs**\n\n[a] Stand-Alone Self-Attention in Vision Models. NeurIPS 2019\n\n[b] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. ICCV 2021\n\n[c] Transformer in Transformer. NeurIPS 2021\n\n",
            "summary_of_the_review": "I would think the technical novelty is some limited as stated above, and also the experiment setup is somewhat lacking explicitness and some fairness. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}