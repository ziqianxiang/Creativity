{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes an efficient regression-based method for epistemic uncertainty quantification in neural networks. Using uncertainty produced by resource-expensive methods as ground truth uncertainty, a regression model is trained to predict uncertainty efficiently. Experiments show the effectiveness of the proposed method in uncertainty calibration, uncertainty ordering correlation.",
            "main_review": "The work is well-motivated and efficient uncertainty quantification is an exciting research direction. However, after reading the paper, I raised several key concerns. See my detailed comments below.\n\n---\n\nThe first concern is about the significance. It seems that the whole novelty of the paper is in Sec.3, which merely described a straightforward way to distill the predictive uncertainty from a time-consuming method such as MC-dropout (Gal & Ghahramani, 2016), into a separate regression model such as decision tree or neural networks. The performance of the proposed method therefore largely depends on the uncertainty produced from the 'teacher' model (i.e., the dropout network in the experiments). However, no theoretical analysis about the quality of uncertainty can be found in the paper.\n\nSome important related works are not discussed. Since one of the main motivations of this work is to avoid resource-expensive uncertainty quantification techniques such as dropout, the authors should discuss several recent works that obtain uncertainty from deterministic neural networks [1-3]. Note that these methods can also avoid multiple inferences of a deep neural network.\n\nThe significance of the experimental results is limited. Throughout the experiment, only different regression plugins are compared, which is more like an ablation study of the proposed method. No competitive baselines (like DUQ/SNGP) were included in the evaluation. The results are thus not convincing enough. A very confusing point is that, when using neural networks as the regression model, the architecture (512,255,128,64) is much larger than the classification model (Table 1). Does this violate the original intention of application in time/resource-limited scenarios?\n\nFinally, there are several misleading claims. For example: In Sec. 2.1.2, *True epistemic uncertainty can be calculated through a Gaussian process.* Generally speaking, this is incorrect. It is true only in some toy examples where the target function is really sampled from a predefined GP (i.e., no misspecification of prior).\n\n---\n\nReference:\n\n[1] Van Amersfoort, Joost, et al. \"Uncertainty estimation using a single deep deterministic neural network.\" International Conference on Machine Learning. PMLR, 2020.\n\n[2] Liu, Jeremiah Zhe, et al. \"Simple and principled uncertainty estimation with deterministic deep learning via distance awareness.\" arXiv preprint arXiv:2006.10108 (2020).\n\n[3] Mukhoti, Jishnu, et al. \"Deterministic neural networks with appropriate inductive biases capture epistemic and aleatoric uncertainty.\" arXiv preprint arXiv:2102.11582 (2021).",
            "summary_of_the_review": "The novelty and significance of the proposed method are questionable. Important related works are not discussed. Also, the experimental results are not convincing enough.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to train a 2nd neural net to predict the outcome of MC-dropout on both training examples (of the main predictor) and validation examples, in order to estimate epistemic uncertainty (aiming in particular at what happens away from the training data). Experiments comparing regression trees, boosting and MLPs as regressors for this purpose are conducted.",
            "main_review": " predicted uncertainty removing, in 10% in- crements between 0% and 50%, the most uncertain predictions and re-evaluating accuracy\": this is a very unusual way of quantifying the quality of uncertainty estimates. It is not at all clear that uncertain examples should be outliers, in fact it could be the exact opposite, they could be the pillars of good generalization far from the main of the training data cluster. Please read the literature and implement some of the more common yardsticks for epistemic uncertainty estimation, e.g., using a known generative model (for which aleatoric uncertainty is known) or a dataset for which we suspect that aleatoric uncertainty is very small, and comparing with the true error minus the noise magnitude, or alternatively trying to detect OOD examples using the epistemic uncertainty score, etc. Finally, MC-dropout is often not the best epistemic uncertainty predictor! Please have a real benchmark against the leading contenders, not just one.\n\nIn terms of benchmarks, see this paper: arXiv:2107.00649\nIn terms of other epistemic uncertainty estimation by training a 2nd neural net to do the job, see this paper: arXiv:2102.08501",
            "summary_of_the_review": "\nThis paper is below the bar of leading ML venues like ICLR in many ways:\n * naive treatment of the relevant issues, concepts and relevant methods\n * lack of comparisons against other epistemic uncertainty estimators besides MC-dropout\n * using home-made metrics and ignoring existing metrics for quantifying the accuracy of epistemic uncertainty estimators\n * using small and old datasets that are not representative of modern deep learning, or at least of those typically used in other recent papers on epistemic uncertainty estimation for deep nets",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "In this work, the authors propose a technique that allows epistemic uncertainty to be estimated using learned regression algorithms. More specifically, the uncertainty regression algorithms take the original input and classification probability as input and output the uncertainty directly. The dropout uncertainty is used as ground truth to train the uncertainty regression algorithms.",
            "main_review": "Strengths:\n\n1.  The authors propose a technique that allows epistemic uncertainty to be estimated using learned regression algorithms.\n\nWeaknesses:\n\n1. The technical novelty of the proposed model is limited. The proposed method is native and straightforward.\n2. Some important baselines are missing, such as [1][2][3].\n3. To evaluate the performance of the uncertainty metric, it is better to conduct misclassification detection and OOD detection.\n\n[1] Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles. NeurIPS 2017.\n\n[2] Predictive Uncertainty Estimation via Prior Networks. NeurIPS 2018.\n\n[3] SDE-Net: Equipping Deep Neural Networks with Uncertainty Estimates. ICML 2020.",
            "summary_of_the_review": "This paper is not ready to be published at ICLR. The proposed method is native and straightforward. The details of the proposed method are not clear. The experiment is not extensive at all. Some important baselines are missing. In addition, to evaluate the performance of the uncertainty metric, it is better to conduct misclassification detection and OOD detection.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper claims to propose a method for estimating epistemic uncertainty for regression. From what I understood, the paper proposes to train a classifier while withholding some data and then training an extra uncertainty predictor with withheld data.",
            "main_review": "The background and related work sections are outdated and miss many relevant papers of the last few years. The paper spends a lot of space on high-level motivation and the same time falls short of properly explaining the (contributed) method. While the introduction, and most of the section suggest that regression is the main focus of the paper, in section 3, the paper starts to talk about m-class classification. This is very confusing. Critical details like what the labels/targets of the regression training set are and how the uncertainty estimates $u$ are represented are not properly explained. In particular it is not properly defined what is meant by “dropout uncertainty”. In general, the crux of epistemic uncertainty estimates is that they are meant to be indicative of and reliable for out-of-distribution samples. I do not see how the proposed method of withholding a “regression training set” can cope with that.\n\nFinally, the experimental methodology is questionable. For instance, why is the “dropout uncertainty” is a used as a reference for assessing the uncertainty estimates (i.e. by measuring the correlation coefficient). Is there any reason to assume that the dropout uncertainty estimates are good / reliable. In addition, the calibration plots in Section 5.2. are weird. Usually, the accuracy is plotted against the predicted confidence level (e.g. see [1,2]).\n",
            "summary_of_the_review": "Overall, the paper clearly falls short of what we expect for a publication at ICLR – clarity, a significant contribution and meaningful experimental results. From my side, a clear reject.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}