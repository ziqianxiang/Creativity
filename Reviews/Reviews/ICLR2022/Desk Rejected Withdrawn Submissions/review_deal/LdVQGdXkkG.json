{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposed a method for predicting future frames of a video conditioned on an action category and constituent objects involved in it. The proposed method overcomes the need to use additional supervision (like bounding boxes) by encoding the conditioned action into a concept module, this module guides the generation of the next frame. The authors create two datasets building up on existing datasets and annotate another dataset with action categories to support their experiments.",
            "main_review": "Strengths of the paper:\n- novel formulation for action conditioned video prediction task without additional supervision\n- annotated datasets to evaluate the proposed scenario\n\nConcerns:\nEvaluation:\n1. Section 2.2. describes deterministic and stochastic setting, where are the ablations for different settings shown? Table 1 refers to MAC model, I assume it's the stochastic variant. Did you experimentally found it better, what's the motivation to presenting different scenarios?\n2. How is the human study conducted? How many humans were involved in the study? How many videos were generated for computing the accuracy in Table 1? Is the accuracy referring to human study? This needs to be clarified, without this information, we can't judge the results.\nI like the phrasing inverse of action recognition, why not train a simple model to automatically predict the accuracy of the generated models in terms of action recognition.\n3. The compositional setting only shows result on predicting an unseen object, however the actions were present in the training set. A more complex scenario could be training on a set of actions A and objects C and actions B and objects D and testing inversely on actions B and objects C (and actions A and objects D). It's also difficult to conclude the compositionality from a single qualitative example, is there a metric that could show the correctness of your method?\n4. In all shown examples, the object mentioned is unique and not occluded, how does the model resolve ambiguities like this?\n5. The experiments involving object detection are also difficult to judge, what is a baseline model this can be compared to? What if instead of using MAC encoder you use resnet? Why is this experiment not included in the main section, please include the results on different datasets. \n\n\nMethod:\nThe method has hard coded 2 objects and 1 action, what is inputted to the model in case there is only single object? How does the method generalize to multiple objects?\n\nClaims:\nUsing the term counterfactual prediction, while can be argued true, seems very general and overpromising. ",
            "summary_of_the_review": "The paper provides a novel set-up for action conditioned generation, however I have some major concerns regarding the evaluation and generalization ability of the model. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes to extend the video prediction task beyond simple actions by introducing a semantics-conditioned version. The goal is to predict the spatial-temporal dynamics of the future frames given a more complex language-based query of the form <action, object 1, object 2>. The paper then proposes MAC, a modular approach to video prediction, that conditions the future dynamics based on the language constraints to predict the future frames. The approach is evaluated on three benchmarks and shows improvement over the relevant baselines.",
            "main_review": "Strengths:\n-------------------\n+ The task of semantic-conditioned video generation is very interesting and presents the natural next step for video prediction. It allows for a more complex task that requires perception, language understanding, visual-semantic grounding, and some understanding of physics for future prediction. The three datasets (2 synthetic and 1 real-world) appear to provide a solid foundation for exploration in this direction.\n+ The evaluations performed seem to be comprehensive and provide all the highlights of the proposed approach that is required to outline contributions.\n+ The quantitative and qualitative results are impressive and show improvement over compared baselines.\n\nWeaknesses:\n-------------------\n- While the paper is very well-written, there are some details (mostly implementation) that are missing that would make it more self-sufficient and a little bit more reproducible. For example, how are the semantic instructions encoded? Are these pre-trained embeddings or learned during training? Are the visual feature encoders pre-trained on ImageNet or similar datasets? It says that the network is \"similar\" to a VGG network. Some more details would be good to have at least in the supplementary for the sake of completeness. \n- I think the claims of generalization to novel objects may be a little exaggerated or not described well. During the experiments in Section 4.4,  I understand that the object \"red cube\" was unseen during training. However, I assume that the composition of the unseen object, \"red\" and \"cube\", were still encoded within the concept slot. While still impressive, it is not a complete generalization (i.e., true OOD) but rather \"zero-shot\" in nature where the model exhibits generalization to novel compositions of the known properties. \n- I see from Figure 2, and somewhat from the descriptions in Section 2.2, that there is some sort of \"routing\" going on that associates each concept and its description (\"green\" and \"bowl\" are associated with object 1, etc.), but there is no description of \"how\" or evaluation of this ability. Is this whole process learned? If so, it suggests to me that there is some dependency on the semantic embedding, yet there is no discussion on this aspect of the approach.\n- The counterfactual generation is an interesting task setup and the results do seem to be very promising. However, again, it is hard to evaluate these results without some additional details. For example, what is the termination condition for video generation? Does the approach send a <EOS> token or is it a fixed-length prediction? If it is a <EOS> token, then it seems correct to say that this can be used for predicting counterfactual results (as done in Figure 5 and Section 4.3) since it does give one way to actual evaluate the plausibility of such an action sequence. But if it is an extrapolation to a fixed length, I am not sure if this can be claimed as such.\n- I understand that the reduced dependency on bounding box annotation (and hence scene graphs) is considered to be an advantage since it adds another layer of training complexity. However, they do provide some interesting properties that can be used to constrain future predictions. For example, consider two objects that are placed on top of each other. Understanding the spatial and semantic relationship between the two objects will help constrain future predictions to consider the physical interaction between the two connected components. The proposed approach will have to learn this dynamic from training data without this auxiliary information (that may come through inference). I would like the authors' thoughts on this scenario and how it is addressed (if so) in the approach.\n- While the qualitative visualizations are nice, it would be good to see some examples of failures to understand the next steps to be addressed in this challenging task and assess whether the proposed approach can be extended in that direction.\n- Some proofreading to correct minor issues would be good. Some terms are introduced without reference that is hard to follow. For example, AG2Vid is mentioned in Section 3 but does not have a reference until Section 4.1.",
            "summary_of_the_review": "The paper is well-written and proposes a difficult and interesting task. The proposed approach to tackle this task is reasonable and provides quantitative gains over baselines. However, there are missing details and a lack of discussion on some claims and experimental setup that need to be addressed to make the work self-sufficient.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The submission aims to generate future video frames conditioned on action inputs (in the form of object_1, verb, object_2). The authors proposed a Modular Action Concept Network which first grounds a fixed set of pre-defined concepts from the visual inputs, then combined the grounded visual representations with a gated neural network. The representation is decoded into video frame predictions based on techniques introduced by SVG and ConvLSTM. Along with the proposed method, the authors also collected two synthetic datasets for the described task, based on CLEVR and Sapien. Experimental analysis demonstrated the effectiveness of the proposed approach over a few baselines.",
            "main_review": "Strengths:\n+ Action-conditioned video prediction is an interesting task, it is especially interesting that the submission appears to be able to handle longer-term video predictions, when conditioned a sequence of actions (Figure 4).\n+ The qualitative evaluations, particularly the \"counterfactual\" ones, are interesting.\n\nWeaknesses:\n- The proposed task appears to me a bit constraining and might not be as general as AG2Vid. From Figure 1, I had the impression that the action conditions are natural language descriptions, whereas the system seems to expect the descriptions are already parsed into verbs and objects in a pre-defined vocabulary. Personally, I found the \"action graph\" used by AG2Vid more expressive and flexible than the \"actions\" used by this work.\n- The motivation of Modular Action Concept Network is interesting and appears to be simple to implement in practice. However, I found myself missing ablation studies of the proposed MAC network given that there are many potential ways to instantiate the idea. I'm also crying to understand more details on the concatenation baseline (what are in the MAC pipeline that are missing from the concatenation baseline?), and why the proposed approach outperforms concatenation by such significant margins.\n- Another ablation I found useful would be the impact of video prediction framework (ConvLSTM + SVG), especially when compared with AG2Vid.\n- I could have missed it but it'd be great if the authors could help me understand how $c^j$ in the concept slot module is used.\n- It'd be great if the authors could discuss related work on compositional neural networks (e.g. work by Jacob Andreas).",
            "summary_of_the_review": "The submissions focuses on the action-conditioned video frame prediction task and appears to achieve good quantitative and qualitative results on their proposed benchmarks. I found the proposed solution well-motivated, but could be explained more clearly. The assumption that the actions are represented as verb, object triplets in a fixed vocabulary might be also a bit constraining. On the experiment side, the submission would also benefit from a more extensive ablation study on the MAC network and the video prediction network. Hence I currently recommend marginally below the acceptance threshold.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work proposes a new task called semantic action conditioned video prediction. Given a input video frame with semantic action labels, the model outputs future video frames that is semantically consistent with the action. The author also introduces two synthetic datasets to validate the proposed model.  The proposed MAC outperforms baseline method and previous AG2Vid approach on CLEVR-Building-blocks and Sapien-Kitchen dataset. Experiments also show the ability of the model to work on out-of-distribution scenarios and can generate unseen objects during inference. ",
            "main_review": "Strengths:\n1) This work formulates a novel research problem to predict future frames given semantic action supervision and observed frame. It is a challenging problem on its own and the proposed method MAC is able to work on both synthetic and real datasets, as demonstrated in Section 4 in terms of various evaluation metrics commonly used in image quality assessment. \n2) In contrast to previous works, the proposed model does not require bounding box as supervision which alleviates the workload of manual labeling.\n\nWeaknesses:\n1) In Section 2.2, the author needs to further explain the \"learned prior\", what's the physical meaning of \"z\"?Readers may get confused when it first comes to this symbol although it is not the main contribution of this work.\n2) Although the experimental results are strong, the author may need to discuss the explainability aspect of their proposed \"MAC\" architecture to justify why it works, rather than only showing the experimental results.",
            "summary_of_the_review": "Overall I recommend for acceptance. The author address a new task and introduces two datasets tailored for this problem, and an effective model called \"MAC\" is proposed to predict future frames that performs better than prior art and a competitive baseline. I feel that the work can be better presented by adding more justifications on the explainability part of the proposed model to understand why it works.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}