{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "In this paper, authors proposed a method named sonification for interpreting intermediate feature representations of sound events. It was applied to raw waveform sound event recognition (SER) CNNs, and mapped intermediate representations back into discrete-time signal domain via deconvolutions. Visualization figures were provided, and experiments/analysis were conducted to better understand the connection between intermediate representations and inference tasks.",
            "main_review": "This paper presents an interesting idea on how to visualize and interpret intermedia representations of SER CNNs, providing visualization, experiments/analysis on its impacts for a couple of downstream tasks, with different training approaches. Overall this paper is clear written, and it provides some insights for better understanding representations for SER.\n\nOverall, I find methodologies presented in this paper relatively straightforward, with limited novelty. This paper focuses on visualization, and validation is conducted for two inference tasks with different training approaches. It may better fit speech/audio focused forums. \n\nOne questions: For \"deep supervised layers learn the presence of more abstract concepts\" at the end of Sec 3.3, what does \"abstract concepts\" refer to?",
            "summary_of_the_review": "Though this paper presents an interesting idea on interpreting representations with SER CNNs, overall its methodology has limited novelty, and its experimental work has limited contributions for broader community.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper introduces the idea of sonification, an approach to interpret hidden representation learned by a CNN-based model taking the raw waveform as input. This approach is applied to the Sound Event Detection task. The sonification algorithm is first presented and then applied to two training cases: contrastive and supervised traning. The resulting sonification are then presented and discussed. Several studies are then presented, showing that sonification signals are an important sub-structure of the input signal, analysing the two training approaches and analyzing the importance of the layers.",
            "main_review": "Strengths:\n- The main idea of the paper, sonification, is very appealing and signiifcant: interpretability of audio-based neural networks is an open question and the proposed approach is a solid avenue of research for this issue. \n\nWeaknesses:\n- The paper is very strangly structured, making it very hard to understand. Here are my main concerns:\n    - Section 3.1 especially is very hard to undersand, as nothing is really explained. It simply provide references that the reader must know in order to understand the paper. I think this is quite bad, a paper should be self-contained as much as possible. Section 3.1 should be 1-2 pages long and explain clearly every step. \n    - Section 3.2 is also missing definition, for instance \"s\" is not defined at the beginning. The algorithm is not easy to grasp, a figure would have help a lot. Also, it's still unclear to me what the loss function is to train the Transposed Convolution layers for the sonification.\n   - Section 3.3 is impossible to parse. Figure 1 and 2 have way too much information, and very little indication of what these figures are supposed to represent. Trying to follow the third bullet point observations is way harder than it should be, and it's clearly a pain point for the reader. My advice here is to isolate the interesting sonification plots and create a figure for each case, with a clear caption.\n   - Section 3.4 has the same problem as 3.3: too much information, and I don't know what I am looking at.\n   - Some useful information are in the appendix, i suggest that the two pages figures should be trimmed and some info about the architecture and the experimental setup should be brought in the main paper.\n\nOverall, these issues about clarity make the proposed visualisation technique hard to evaluate. The audio examples provided are also quite confusing, as the input is very noisy, hence it's not clear what the sonifiaction signal means. I think selecting better examples would help, specially the ones mentionned in Section 3.3, for instance i would like to listen to the house/trap music example.",
            "summary_of_the_review": "The goal of the paper is very noble, interpretability of neural networks is an important area, specially in audio-based model. The proposed approach of sonification is quite novel and significant, and overall a good step towards intepretable audio model. However, the paper is very hard to read and makes the proposed approach hard to understand and to evaluate.\n\nI was very excited by this paper when i read the abstract, but in the current form i cannot recommend acceptance. I will be happy to increase my score if the authors improve the clarity of the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The manuscript covers an interesting topic, mapping feature representations back into the discrete-time input signal space for a better interpretation as intelligible acoustic events.\nthe authors have proposed to use sonification as a method to interpret intermediate feature representations of\nsound event recognition convolutional neural networks (CNNs) trained on raw waveforms. Utilizing the sonification methods, the CNN representation of the features is mapped back into the discrete-time input signal domain, highlighting substructures in the input that maximally activate a feature map as intelligible acoustic events. \nThe authors claim that by using sonification it is possible to compare supervised and self-supervised feature hierarchies and show how the interpretability of SER models.",
            "main_review": "interpretability of the intermediate CNN features in the SER applications is a useful tool that could improve the state-of-the-art models significantly if the extracted information is fed back to the network to improve the training process or the evaluation process during the test. \nThe authors in this manuscript have made the following claims: \n\n1- revisiting transposed convolutions (a.k.a. deconvolutions) for interpreting intermediate feature representations of raw-waveform based 1D-CNNs for sound event recognition (SER) by sonification, highlighting maximally activating substructures as intelligible\nacoustic events.\n** This claim is supported in the manuscript. \n2- Using sonifications, to compare and contrast supervised and self-supervised feature representations in the audio signal input space\n** I can't find enough supporting material for this claim. \n** The authors could conduct a supervised and self-supervised feature representations training procedure utilizing the proposed method, then compare the final feature representation on unseen data and perform some evaluation metrics to support this claim. \n** The authors have presented the evaluation results in Figures 6-8, however, the message is not conveyed properly nor clearly. \n3- Demonstrating how sonifications facilitate the visualization and spectral inspection of SER models, working synergistically with signal processing techniques.\n** There is not enough supporting material for this claim. \n\nI highly recommend to the authors conduct more comprehensive experiments and analyses to assess how this approach could provide improvement compared to a regular embedding learning mechanism. \nHow could this work be compared to some of the existing SER embedding networks such as L3 network? ",
            "summary_of_the_review": "It is interesting work! However, it requires more materials and analysis to fully support the proposed idea and demonstrate its effectiveness. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper reports an analysis of CNN-based audio classification models by sonification of feature activations using transposed convolutions (DeconvNet). The CNN models are trained either by supervised learning or self-supervised learning. The authors delineate the observation of sonification by displaying them and comparing different sonification patterns in all layers and the two training methods. They validated the sonification results by showing the performance degradation when they are subtracted from the original input, and computing the coherence between them in spectrum. The appendix provides the details of model architecture, benchmark evaluation on the datasets, and exemplary proof of time-invariance of the input and corresponding sonification to support the main text.      ",
            "main_review": "This paper shows a lot of interesting observation to help understanding CNN models for audio classification and the effect of training methods (supervised vs self-supervised). I respect the effort of the authors for comprehensive experiments on three datasets with different training settings and investigation of the trained models using the DeconvNet. However, this paper has the following major issues to be accepted. \n\nFirst, this paper made no technical contribution. This is a serious drawback as an ICLR submission. They simply applied an old (considering fast advance of deep learning) visualization technique to trained CNN models and briefly describes the observations. Furthermore, the authors unfortunately missed the following works that already used the same method to understand CNN models for audio classification. \n\n- Auralisation of Deep Convolutional Neural Networks: Listening to Learned Features, Keunwoo Choi, Jeonghee Kim, György Fazekas, Mark Sandler, ISMIR LBD, 2015\n\n- Explaining Deep Convolutional Neural Networks on Music Classification, Keunwoo Choi, George Fazekas, Mark Sandler, Arxiv, 2016 (the extended version of the ISMIR LBD paper)\n\nThis previous work used the same DeconvNet to auralize the feature activation for CNN trained to classify music genre. Although they used spectrogram as input and music data, the overall idea is the same. They used the term \"auralization\" instead of sonification. This submitted paper definitely more observations, for example, comparing supervised learning to self-supervised learning and validation of sonification. However, the core idea was already tried long before. \n\nSecond, the validation experiments are also quite plain. Considering the nature of deconvolution, it would be hard to obtain a good quality of reconstructed sonification from deeper layers (they have more paths going through more layers). Therefore, the coherence between the original input and the corresponding sonification are supposed to be lower for deeper layers. The authors might ignore this issue. \n\nThird, the analysis is not convincing. The observations of sonification are described in a terse manner in session 3.3. I could not find any insight particularly in deeper layers. In addition, when I listened to the audio examples in the footnote link, the sonification examples were not straightforward to understand. Many of audio examples are very noisy and mostly contain high-frequency content (to my ears).   \n\nIn addition to the major issues, I have the following questions.\n\n- What is the purpose of Section 4.3? The content is not related to sonification. \n\n- Most sonification examples contain high-frequency content. I think a regularization or smoothing method in the waveform level might be necessary to suppress the trend. Unlike images, small discontinuity in audio waveforms can cause annoying high-frequency noise. \n\n- It is unusual to use two entires pages of the main text for displaying images. I understand they are important images to explain the main idea but the style of presentation makes the paper look like a technical report. \n\n",
            "summary_of_the_review": "This paper has some interesting observations of CNN-based audio classification using transposed convolution. However, it has critical weakness in technical contributions. Furthermore, the important reference which already attempted a similar analysis is missing. Therefore, I recommend strong reject.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I cannot find any ethical issue. ",
            "recommendation": "1: strong reject",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}