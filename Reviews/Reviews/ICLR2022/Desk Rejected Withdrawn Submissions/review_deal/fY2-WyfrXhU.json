{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a novel framework, MemREIN, which considers Memorized, Restitution, and Instance Normalization for cross-domain few-shot learning. An instance normalization algorithm is explored to alleviate feature dissimilarity, which provides the initial model generalization ability. A memorized module is further proposed to separate the most refined knowledge and remember it. A novel reverse contrastive learning strategy is proposed to stabilize the distillation process. Extensive experiments on five popular benchmark datasets demonstrate that MemREIN well addresses the domain shift challenge, and significantly improves the performance up to 16:37% compared with state-of-the-art baselines.\n",
            "main_review": "++   The motivation of this paper is very clear.\n\n++   Overall, the paper is well written. In particular, the METHOD section has a nice flow. Despite the method having limited novelty, the overall method is clearly described.\n\nConcerns: \n - Cross-domain few-shot learning has been studied a bit, and this paper does not give a corresponding presentation in the INTRODUCTION and RELATED WORK.\n - The main problem of the paper is the lack of rigorous experiments to study the usefulness of the proposed method. Although the paper does comparison experiments with more methods, the comparison methods used are not convincing enough for the effectiveness of the method proposed in the paper. It would be better if other methods from 2021 were included as comparison methods, such as:\n\n[1] Hanwen Liang, Qiong Zhang, Peng Dai, Juwei Lu. Boosting the Generalization Capability in Cross-Domain Few-shot Learning via Noise-enhanced Supervised Autoencoder, In CVPR, 2021.\n\n[2] Yuqian Fu, Yanwei Fu, Yu-Gang Jiang. Meta-FDMixup: Cross-Domain Few-Shot Learning Guided by Labeled Target Data. ACM Multimedia, 2021: 5326-5334.\n\n - For the results section, hyperparameter adjustment processes are not given. \n\n - In addition to this, the tSNE visualization comparison (see Supplementary Material) mentions that the proposed method forms clear clusters of different classes compared to the previous methods. However, the improvement is not so obvious in Figure(f) with respect to Figure (d).\n\n Minor comments: \n* In the 4th line in 2.1 FEW-SHOT CLASSIFICATION, it should be \"encodes\" instead of \"encode\". In the page 6, it should be \" defined in \" instead of \" defined is \".\n\n* The flowchart of the overall algorithm should be refined a bit more as possible. \n\nUpdates:\nAfter reading the comments from other reviewers, I find that the main idea of this paper is extremely similar to [1], which also tries to solve the cross-domain gap by instance normalization, and restitution to handle the Reid task. Though they solve different tasks, the key idea is the same. As such, I degrade my score to reject.\n\n[1] Style Normalization and Restitution for Generalizable Person Re-identification, in CVPR 20.",
            "summary_of_the_review": "Regarding the lack of experiments and the techniques used are not new, e.g., memory, contrastive, and etc. I tend to reject this paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose a modular approach to boost performance for both cross-domain few-shot learning (CDFSL) and traditional few-shot learning (FSL). The modular approach involves applying the MemREIN module at the outputs of residual blocks. The MemReIN module includes an instance normalization layer followed by a disentanglement step. The disentanglement step involves extracting discriminative features from the residual of the instance normalized features by training a reverse contrastive loss. Results show substantial improvement in recognition performance for both CDFSL and FSL.",
            "main_review": "Strengths:\n\n- The concept of memorized restitution and reverse contrastive loss is novel in the context of CDFSL.\n\n- The comparison studies are quite extensive both with CDFSL and traditional FSL although the dataset benchmark might be an issue.\n\n- Results show significant improvement in recognition performance.\n\n\nWeaknesses:\n\n- The ability to generalize across domains has only been demonstrated over relatively small domain disparities e.g. compared to that present in the BCDFSL [1] setting which consists of large domain shift between natural images and satellite/ medical images. The current benchmark only considers natural images.\n\n- The authors could discuss related work on the topic of partial/open-set/universal domain adaptation that does not assume that the source and target categories are the same and hence different from traditional domain adaptation. Also related work (just mentioning only published ones in main venues) on other recent works [2-6] on CDFSL should be compared/mentioned, even though some of them have been evaluated on a different benchmark. More importantly, it would be great to have a separate related work sub-section on cross domain few-shot learning. If space is an issue, domain generalization and traditional few-shot learning sections can be reduced since they are already well known by  the community.\n\n- It seems that the exact method of normalization and restitution is already used in person re-identification but the authors do not refer the paper [7]. If one goes into the details of [7], the contribution of this paper seems to be very minor.\n\n- The authors claim “instance normalization inevitably removes some discriminative information from the original feature maps ...”.  It would be great if authors mention some related work that shows that this is the case.\n\n- “SE-like channel attention”. The Squeeze and Excite paper should also be referred here.\n\n- I think there is lack of clarity in how the training and testing is taking place. How the memory restitution is done and how do you do forward and backward passes for reverse contrastive losses and how is batch-sampling done ? To address this, the authors could have an algorithm block (maybe in the supplementary material) to explain the complete training and testing process.\n\n- Hyper-parameter sensitivity of lambda should also be carried out.\n\n- The authors could do with a visualization of “important” and “contaminated” features maybe through T-SNE or feature maps that highlights the discriminability difference and disentanglement of the two feature types.\n\n- Some design choices could be better justified maybe through experiments or theoretically:  Why the MemREIN blocks are applied after the last batch normalization layer of all the residual blocks and not every layer and not some layers ? Why avg pooling is used in Eq. (6) and max pooling in Eq. (8) ? What is the memory limit of the memory bank - is there any method to replace and store feature maps ?\n\n- The authors could describe some limitations of their method: E.g. Can this MemREIN module be applied if we just have access to a source trained model without having access to source dataset for training ?\n\nReferences: \n\n[1] Guo, Yunhui, et al. \"A broader study of cross-domain few-shot learning.\" ECCV, 2020.\n\n[2] Zou, Yixiong, et al. \"Revisiting Mid-Level Patterns for Cross-Domain Few-Shot Recognition.\" ACM Multimedia, 2021.\n\n[3] Phoo, Cheng Perng, and Bharath Hariharan. \"Self-training for few-shot transfer across extreme task differences.\" ICLR, 2021.\n\n[4] Liang, Hanwen, et al. \"Boosting the Generalization Capability in Cross-Domain Few-shot Learning via Noise-enhanced Supervised Autoencoder.\" ICCV, 2021.\n\n[5] Islam, Ashraful, et al. \"Dynamic Distillation Network for Cross-Domain Few-Shot Recognition with Unlabeled Data.\" NeurIPS, 2021.\n\n[6] Fu, Yuqian, Yanwei Fu, and Yu-Gang Jiang. \"Meta-FDMixup: Cross-Domain Few-Shot Learning Guided by Labeled Target Data.\" ACM Multimedia, 2021.\n\n[7] Jin, Xin, et al. \"Style normalization and restitution for generalizable person re-identification.\" CVPR, 2020.\n",
            "summary_of_the_review": "Overall, the idea is novel only in the context of CDFSL but it has been previously explored for person re-identification. Another major downside is that the method does not  demonstrate generalization across bigger domain disparities like the BCDFSL benchmark, which has large domain shifts between source and target categories.\n\n-------- POST REBUTTAL ---------\n\nThe authors address some of my concerns about the paper. However, the concerns about novelty of the idea, evaluation on BCDFSL benchmark with recent approaches [2-6] and alternative model choices still remain. Hence, I would like to keep my current score.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a cross-domain few-shot learning method, namely MemREIN, which consists of Instance Normalization and Memorized Restitution modules. An instance normalization layer is proposed to reduce the feature discrepancy between samples and enhance the overall generalization ability. A memorized restitution module is utilized to distill fine-grained discriminative knowledge about categories from the filtered out features. Beisdes, a reverse contrastive loss which encourages the further disentanglement of discriminative features and general features is proposed to stabilize the distillation process. Experiments on five benchmark datasets demonstrate that the proposed method outperforms the previous state-of-the-art approaches.",
            "main_review": "Strength: The proposed MemREIN method is simple yet effective. Compared with state-of-the-art baselines, the improvement of performance is significant.\n\nWeakness:\n1. Some explanations and descriptions of the method are unclear and insufficient. For example,  why use the memorized mechanism in the memorized restitution module? And how to update the memory vector \"M\"?\n2. The ablation study is not very sufficient. There is a lack of the experiment verifing effectiveness of the memory bank in the memorized restitution module, which is an important term in the paper title.\n3. Although the instance normalization layer is proposed to enhance the generalization ability, the ablation study in Table4,7 doesn't verify the effectiveness of it and the performance of \"GNN+IN\" is even worse. How about just using original feature F to obtain D and G? It seems that there should be an experiment about \"GNN+MemREIN w/o IN\" .",
            "summary_of_the_review": "The MemREIN method proposed in this paper is simple and significantly improves the performance. But some explanations and descriptions about the memorized mechanism are unclear and insufficient. The corresponding ablation study is also lacking. More sufficient experiments should be included, e.g., verifying the effectiveness of the instance normalization layer. I would like to see the detailed response to my concerns in the rebuttal.\n\nUpdates: Although the authors address some of my concerns in the rebuttal, the question about  the effectiveness of the instance normalization layer still remains. So I would like to keep my current score.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a general framework, MemREIN, for cross-domain few-shot learning. It utilizes instance normalization and memorized restitution to strength the discrimination ability. In addition, the authors propose a novel reverse contrastive learning strategy to stabilize the distillation process. The experiment is sufficient and shows a significant improvement compared with state-of-the-art baselines for cross-domain few-shot learning, and gets comparable results to the best method for normal few-shot learning.",
            "main_review": "The paper has the following strengths:\n1.\tThe proposed method is a universal and method-agnostic module, which can be inserted to existing metric-based methods for cross-domain few-shot learning.\n2.\tThe method uses instance normalization to alleviate the discrepancy across training sample, and design a memorized restitution module to adaptively distill the long-term fine-grained distinctive knowledge.\n3.\tA novel reverse contrastive learning strategy is proposed to disentangle general feature and discriminative feature. In this part, authors define reverse contrastive loss which makes anchor feature close to positive samples feature and separate from negative samples feature. \n4.\tExtensive experiments on five popular benchmark datasets demonstrate that MemREIN well addresses cross-domain few-shot challenge.\n\nThere are some weaknesses:\n1.\tThe differences of the proposed instance normalization from other normalization methods, such as batch/group/layer normalization, should be explained in detail. What’s more, its advantages should be elaborated.\n2.\tThe memorized restitution sounds to be the most important contribution of the proposed method. For memorized restitution, why not consider to combine general feature map G with a memory bank? On the other hand, as D is updated with a memory vector, G can be computed as R - the updated D. Please clarify the logic behind using current G. \n3.\tIt would be better if more information about memory vector M is provided. To be specific, what is the relation between M and D in multiple iterations?\n4.\tIn Table 2, under the leave one out setting the proposed method only be compared to “+LFP”. As ATA is a bit better than FP according to the results in Table 1, it would be more convincing to also including it in the comparison.\n5.\tThe ablation study is helpful to check the significance of each module. It would be better to also show the results of w/o memory bank and w/o restitution.\n",
            "summary_of_the_review": "This paper proposes a new framework which is universal and method-agnostic, and conducts sufficient experiments. However, the novelty of the paper is not that significant, as the methods described in this paper are a little reproducible in prior works. The design of the modules is empirical, while more solid theoretical proof should be considered. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "To deal with the domain gap issue, this paper proposes a novel framewok named MemREIN that can enhance the generalization ability while maintaining discrimination ability. They first apply the instance normalization algorithm to improve the generalization. Then, a memorized and restitution strategy is proposed to distill the discriminative information. To ensure the decoupling of discriminative features and general features, they design a reverse contrastive loss. This method is simple and easy to reproduce, and the experiments show that the method has excellent performance.",
            "main_review": "Positive \n1.\tThe method is simple and easy to reproduce.\n2.\tThe results are good.\n\nNegative\n1.\tI doubt the effect of the loss $\\mathcal{L}_{r c l}^{-}$ that pushes the feature with negative samples closer to the anchor feature. The combined feature of G and $ \\widetilde{F} $ has only general information, but it doesn’t mean that the feature $ \\widetilde{f}^{-} $ needs to be close to the negative feature. Please provide a more detailed explanation.\n2.\tThe ablation study is not sufficient. We cannot conclude from Table 4 that the instance normalization strategy is valid. More experiments are needed.\n3.\tWhy does the model need to obtain R via a residual structure? If you want to distill and purify the discriminative information, the feature F is enough. \n4.\tThere is a lack of experiments to prove the validity of memory bank. If there is no concatenation operation, is it valid to use D directly? (Eq. 8)\n",
            "summary_of_the_review": "I feel the paper is borderline, with a slight inclination to reject. Although the method has achieved good results, the intent of many operations is not clear. Also, some important experiments are missing, e.g., the instance normalization strategy and memory bank.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}