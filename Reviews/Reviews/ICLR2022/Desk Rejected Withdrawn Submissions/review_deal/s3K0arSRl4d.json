{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper integrates a Transformer into the recurrent state-space model of Dreamer, and shows that this leads to improvements (both in trajectory generation and policy learning) on synthetic tasks that require long-term memory.",
            "main_review": "Strengths:\n- Both transformer models and world models are of high interest to the community \n- Clear presentation with exhaustive background on Dreamer and detailed description of the integration of the transformer.\n- Exhaustive experiments on toy tasks that clearly demonstrate the advantages of the proposed model over Dreamer.\n\nWeaknesses:\n- This paper seems to be weak in terms of reproducibility. While the architecture is described in much detail, hyper-parameters are missing, and there is no accompanying source code.\n- I wouldn't fully agree with the claim in the abstract that TransDreamer uses a \"transformer-based\" policy. If I understood correctly, the policy uses the fixed world model (which in turn uses a transformer) to obtain a state representation.\n- Evaluation only on very few, easy Atari games. Dreamer v2 shows learning curves for all games despite not addressing exploration explicitly. I wonder why TransDreamer is not evaluated on a handful of harder games as well?\n- The Atari learning curves only show the very beginning of training. This is fine for Boxing, Freeway and Pong, but in Tennis there would be additional learning progress for Dreamer (cf. http://arxiv.org/abs/2010.02193, Fig. D.1).\n\nQuestions:\n- The 3D object room task seems presents 3D observations, but it seems like agent movement is restricted to 2D? What's the action space for this task?",
            "summary_of_the_review": "Overall, I think this paper would be of high interest to the community. However, I think that reproducibility is crucial here -- the \"meat\" of the paper is modifications and additional tricks (e.g. prioritized experience replay) applied to Dreamer, and it would be great to either have an accompanying implementation or a detailed description of all hyper-parameters. Finally, I would really like to see further results on Atari.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents TransDreamer, which replaces Dreamer’s RNN-based stochastic world model (RSSM) with a transformer-based stochastic world model (TSSM). TransDreamer outperforms Dreamer on a new task requiring long-term memory, and asymptotically approaches comparable performance to Dreamer on a few tasks from Atari and DeepMind Control Suite that do not require long-term memory. Detailed analysis shows that TSSM produces better images and reward predictions than RSSM. ",
            "main_review": "This work is very timely, given the success of the Dreamer architecture, and the growing adoption of transformers in many areas. Since both Dreamer and transformers are quite complex, combining the two is non-trivial, and this paper provides a good discussion of the decisions made in the design process. The paper is well-written, and the Hidden Order Discovery (HOD) experiments are well-designed.\n\n**TSSM’s transformer architecture**\n\nMany different transformer architectures exist, so the paper needs to explicitly state which architecture is used by TSSM. The paper discusses GTrXL (Parisotto et al., 2020), leaving the reader to tacitly assume that TSSM is based on GTrXL, as if that were the only reasonable option. But in fact, most applications of transformers and transformer-style attention to RL (see Kurin et al., ICLR 2021, for a recent summary) are based on the original Transformer architecture (Vaswani et al., 2017), which differs significantly from Transformer-XL (Dai et al., 2019) upon which GTrXL is based. \n\nGiven the differences among transformers, the paper over-generalizes when it says that “Transformers have notably had stability issues when used in RL settings.” In fact, such instability has only been reported for the Transformer-XL architecture, not for transformers in general, and specifically not for the original transformer encoder (Vaswani et al., 2017) which has been successfully applied to diverse RL tasks (Loynd et al., 2020). And even for GTrXL, Lampinen et al. (NeurIPS 2021) find that GTrXL’s GRU gating layers are not necessary to achieve stability, and are in fact not beneficial on their tasks, just as they are found to be unnecessary for TSSM. This is worth explaining, without over-generalizing the stability issue to all transformers.\n\n**DMC & Atari experiments**\n\nThe authors are to be commended for including the experimental results on DMC and Atari, where TransDreamer’s sample efficiency is actually worse than Dreamer’s. These results are well-explained, providing a balanced picture of TransDreamer’s capabilities.\n\n**Hidden Order Discovery (HOD) experiments**\n\nThe results on the HOD tasks, both 2D and 3D, suggest that TransDreamer outperforms DreamerV2 on this domain by virtue of its extended memory. Unfortunately, the methodology appears to be flawed. According to section A.4, DreamerV2’s hyperparameters are taken (without additional tuning) from the crafter configuration, even though Crafter is a very different kind of environment, giving us no reason to expect that those hyperparameters are well-suited for HOD. In order for DreamerV2 to serve as a valid baseline on this domain, its hyperparameters would need to be carefully retuned. By contrast, TransDreamer’s many additional transformer-specific hyperparameters were almost certainly tuned on HOD, for instance, to arrive at 6 transformer layers. The paper says nothing about this tuning process, and since the baseline’s hyperparameters were not tuned at all, we cannot draw firm conclusions from this experiment. (In addition, the paper does not report the number of separate runs for the plotted curves.)\n",
            "summary_of_the_review": "Since the hyperparameters of the baseline Dreamer agent were not tuned for the HOD experiments, the results do not support the conclusion that TransDreamer’s replacement of RSSM by a transformer-based world model results in an improved agent. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces a variant of recurrent state space model proposed in [Hafner'19a; Hafner'20a] that utilizes Transformers instead of GRU. The main motivation for replacing GRU with Transformers is that Transformers have shown a large success in various domains by replacing RNNs so authors also expect the trend also holds for model-based RL. The paper introduces some modifications to RSSM and some techniques to utilize Transformers for RSSM. They investigate the performance of the proposed method on some order discovery tasks they introduced, and standard DeepMind Control Suite and Atari environments.\n",
            "main_review": "Strengths\n- Clear writing and presentation\n- Strong performance on Interesting new order discovery tasks\n\nWeaknesses\n- Not enough investigation of the architectures based on Transformers and just try to exactly follow the previous architecture\n- Weak performance on standard benchmarks\n\nDetailed comments\n- The paper only tried to replace RNNs with Transformers in the specific architecture of RSSM, and it's not clear why the world model based on Transformers should follow this architecture and there are not enough supporting experiments to show this. For example, it's not clear (i) why the objective should be reconstruction instead of contrastive loss, (ii) is stochasticity really required for Transformer-based world models, (iii) how does policy learning compare to planning based on MPC, ... It will be much more interesting and helpful to revisit the questions that led to the development of RSSM based on RNNs and its training objectives, and provide the investigations and insights that comes from using the Transformers. In current form, the paper just reports 'it works' but lacks meaningful insights or observations that come from utilizing Transformers instead of RNNs.\n- Performance is a bit weak on standard DMC / Atari tasks. I (weakly) agree that it might not require long-term reasoning, but evaluation on synthetic discovery tasks are not enough for showing the benefit of TSSM over RSSM. From the current results, it's not sure why we should use TSSM instead of RSSM because it just underperforms RSSM and requires more sample, and performances are only reported for a small subset of various tasks so we do not know the results or trends on various tasks with different characteristics. For example, How does the method perform on more (visually complex) tasks like Humanoid, or long-term tasks like manipulator? Is there any trend on the results from easy-exploration games and hard-exploration games on Atari? Evaluation on robotics tasks that require long-term reasoning would be also nice to demonstrate how the proposed method could be better. Also, It would be nice to provide the quantitative results on Table 2 also for DMC / Atari tasks. How does the predictions from TransDreamer differ from the ones from Dreamer on these standard benchmarks?\n- How does TSSM compare to RSSM in terms of compute resources? It would be nice to provide the number of parameters, wall times, ..",
            "summary_of_the_review": "The paper showed that Transformer-based state-space model can also work for model-based RL, but various design choices for the architecture and training objectives are not justified and investigated in the paper. The performance on standard benchmarks are not strong and conducted on a small set of tasks, and analysis on such benchmarks that discusses why this happens or how this could be improved are not provided in the paper. Hence my evaluation remains closer to the rejection of the paper (closer to 4 than 3 if available), but willing to increase the score when my concerns are addressed in the rebuttal period.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces TransDreamer for model-based RL using Transformers, following Dreamer's architecture. The core contribution of TransDreamer is the Transformer State Space Model (TSSM), which improves the long-horizon belief tracking over RNNs and benefits from the batched processing in Transformers. In the experiment, the authors have compared TransDreamer with DreamerV2 on a series of environments and show that TransDreamer achieves better long-horizon reasoning and modeling than DreamerV2. ",
            "main_review": "Overall this is a good paper.\n\n1/ The paper is well written and easy to follow\n2/ The paper has investigated an interesting and important direction\n3/ The structure of TransDreamer is well-designed, which can benefit from both the dynamic modeling from Dreamer and the batched processing from Transformers.\n4/ The experiment results are good\n\nWeaknesses:\n\nI think there are several main issues:\n\n1/ For Transformers, we have to take in the whole sequence. In model-based RL setup, in order to have a good estimation of the dynamics, we need to estimate the current state s_t according to the entire history {s_{1:t-1}, a_{1:t-1}}. As the history accumulates, it will get more and more expensive to process the sequence. Doing a truncation might be okay for model-free RL, but for model-based, losing history is critical. I noticed that currently, the maximum episode length is only 100, this might be a bit short for many other RL tasks, and I think it might not be sufficient to claim the ability of performing long-horizon reasoning. Can we experiment on some other more challenging tasks with even longer episode lengths? Do you have any guess how TransDreamer will perform in that case?\n\n2/ The proposal distribution, i.e., representation model, q(z_t | x_t), considers only the current observation x_t. Although the deterministic state h_t captures the history, but the q(z_t | x_t) might not be a good proposal distribution for a stochastic state. Have you considered using something like q(z_t | x_t, x_{t-1}, ...), which essentially replaces hidden state h_t with the raw history? How will this compare with the current proposal distribution?\n\n3/ The paper considers DreamerV2 as the only baseline. It will be useful to see how TransDreamer compare with other model-free Transformer-based RL methods, e.g., GTrXL [1], in terms of the absolute performance on these benchmarks.\n\nReferences:\n[1] Parisotto, Emilio, et al. \"Stabilizing transformers for reinforcement learning.\" International Conference on Machine Learning. PMLR, 2020.",
            "summary_of_the_review": "Overall I think this is a good and interesting paper. It has made the first step towards combining transformers with model-based RL. However, there are some additional issues and concerns, as discussed above. Thus, I would vote for a weak acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "**Summary:**\n\nThe paper picks up on the work that proposed the \"Dreamer\" agent and improves it in several ways: by replacing the world model recurrent neural network by a Transformer, it allows for better addressing 1) long-term dependencies in the environment and 2) parallel / efficient world model training. The paper is very well written, structured, and clearly presents the differences to Dreamer. In contrast to Dreamer, this work focuses more on long-term dependencies which is reflected in the author's choices of tasks in the experiment section. \n\n",
            "main_review": "\n**Strengths:**\n\nThe paper is well structured and the \"space\" used in introducing Dreamer is well chosen. The authors rightfully recognized that a clear introduction of Dreamer is central to understanding the paper as it allows them to contrast their work to it. Overall, the authors did a very nice job in highlighting the differences both in text as well as graphically (Table 1 & Figure 1). \n\nThe areas in the related work section are well-chosen and the most important body of work is mentioned. Nevertheless, I would kindly ask the authors to assess and contrast works like Procedural Content Generation, i.e. Paired open-ended trailblazer (POET), Generative Playing Networks [1] as well as recent approaches building on top of Generative Teaching Networks [2], particularly applied to RL: Learning Synthetic Environments [3]. In particular, I feel these and (potentially other MBRL-related works) should be at least  acknowledged as being related.\n\n[1] https://arxiv.org/abs/2101.09721\n[2] https://arxiv.org/abs/1912.07768\n[3] https://arxiv.org/abs/2101.09721\n\n**Weaknesses:**\n\n*Environments chosen:* My biggest critique points target the experiment section. The authors, rightfully, claim that focusing on environments entailing long-term memory tasks makes sense in their case. However, I believe this also should give rise to a larger set of long-term memory task environments and notably, with a higher degree in variety. For example, the authors could have used goal-conditioned tasks from the OpenAI Gym Robotics suite, Atari or gridworld environments such as four-rooms (2d object collection environment proposed in Barreto et al., 2017). By moving the experiment section “short-term memory tasks in DMC and Atari” to the appendix, the authors would be able to free space for, what I believe, would be more relevant experiments. I agree that 5.5 is a sanity check and such it could be very well placed in the appendix. Without a larger variety in the set of evaluated environments, I believe the capabilities of coping with long-term memory tasks are insufficiently addressed. Moreover, can the authors please elaborate how the subset of DMC environments was chosen? It seems to me that for a proper comparison and sanity check, one should choose an almost identical set or a close subset of the DMC environments shown in the Dreamer paper.\n\n*Footnote 1:* I do not understand the reasoning behind the claim that the exploration problem is not addressed in this work. Can the authors please elaborate on this because I believe the Hidden Order Discovery tasks is very much an exploration problem. Moreover, I see world models as a way to cope well with suboptimal exploration capabilities in agents. Frankly, why should we not use world models for addressing hard exploration problems? Also, the way Footnote 1 is written reads that TransDreamers would not be able to cope well with exploration. In summary, I believe the argument given in Footnote 1 is weak and even motivates an experiment where the proxy task (world) resembles a difficult problem *due to* hard exploration. Can the authors please also elaborate on how the difficulty to learn a world model with TransDreamer would scale with an increased need of exploration in the world/proxy task? I believe that in the Hidden Order Discovery task we did not see the TransDreamer break when going from 4 to 6 balls, indicating that the sensitivity of the chosen environment is too low to show scalability from easy to very hard exploration and long-term memory task problem.\n\n\n**Minor points:**\n- Parameter count comparison: Can the authors please compare the number of parameters between the Dreamer and TransDreamer in the paper?\n- Figure 3: why is there no standard deviation for the red curve in the top right (2D 6-Ball) subfigure?\n- Table 2: can you please point out explicitly what the numbers 60, 70 and 80 in Table 2 a) and b) correspond to? During my first read it did not become clear to me.\n- Code: Can the authors please provide a link to an anonymous code repository that contains runnable scripts to reproduce some of the results, e.g. Figure 5 or Table 2)?\n- Presentation of results: I believe the work could be better presented/visualized by showing videos (gifs) of the imagined trajectories on a simple google template website.",
            "summary_of_the_review": "I look forward to an interesting discussion and thank the authors in advance for the work and responses. If the authors agree to provide additional experiments for 1) additional long-term memory task environments as mentioned and 2) an explanation why the DMC environments have been chosen or, even better, showing directly the results for the missing environments from the Dreamer paper, I would be very happy to increase my score.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}