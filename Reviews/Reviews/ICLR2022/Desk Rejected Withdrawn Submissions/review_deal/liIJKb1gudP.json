{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a regularization based model that leverages the center loss regularization. The paper has a similar idea to less-forgetful learning but focuses on the efficient training and memory requirement. The joint objective of centre loss with the standard loss encourages each task's feature distribution to be close to the old task feature map. Therefore the subsequent task has minimal deviation in the feature space hence minimal forgetting. The results on the small scale dataset like MNIST/Permuted-MNIST/Rotated-MNIST shows a slight performance improvement while having the efficiency advantage.",
            "main_review": "Comment:\n\n1: Efficiency is one of the most prominent parts in the continual learning domain. The proposed model focuses on the regularization based model, where compared to the previous approach like LFL, the proposed method is efficient.\n\n2: Although the paper is handling a critical problem, the contribution is very limited. Adding the center loss is trivial once we know that we can overcome the catastrophic forgetting if we keep the feature distributions close and LFL already shows it.\n\n3: The paper claims that the proposed model is scalable while not showing any result on the large/medium scale dataset or task. I would like to suggest the author please experiment on the CIFAR100 dataset for the 10 and 20 splits; then, it would be clear how the model performs over the large task sequence. Also, please report the training and inference time wrt other approaches (especially with LFL). Again, the given experiments are very limited, and it isn't easy to evaluate the proposed model using the given experiment.\n\n4: The notations are not adequately defined or confusing, e.g., what $c_{y_i}^{(o)}$ means in Eq:2? As written $c^{(o)}$ is the old task's feature center and $x_i$ is the current task data, then what $c_{y_i}^{(o)}$ represents?\n\n5: As written in the motivation section, architectural approaches suffer from scalability, which may not be entirely correct. There are a few recent approaches [a,b] that are highly efficient.\n\n[6] The paper has not discussed and compared with the recent approaches. I would suggest to the author, please compare the proposed approach with the recent methods. There are many regularisation-based models (Please check the recent conferences) that show promising results on large and small scale datasets. \n\n[a] Efficient Feature Transformations for Discriminative and Generative Continual Learning, CVPR-21\n\n[b] Supermasks in Superposition, NeurIPS-20\n\n \n",
            "summary_of_the_review": "Contribution is limited and the evaluations over the small scale dataset are only given, which are not sufficient to justify the claim made by paper e.g. scalability etc. Also, there are no comparison and discussion with the recent regularization based model, which make it more difficult to evaluate the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a regularization-based method for domain incremental learning (CLR). It utilizes the center loss to constrain the change in the high-level representation of previous tasks.  It also uses the idea of fixing the weights of the classification layer after the first task from the LFL method. The method is evaluated on variations of MNIST benchmarks along with other domain adaptation benchmarks.",
            "main_review": "**Strong points:**\n- A simple yet effective method for domain incremental learning.\n- An ablation study of the two main components of the method is provided.\n- Well-written paper and easy to follow.\n\n**Weak points:**\n- Similar previous work for domain incremental learning using the same idea of class centers is already available [1].  The paper should include the difference between the proposed method and this method. An empirical comparison should be also provided.\n- I would be interested to see the performance of the approach on more complex commonly used benchmarks for continual learning i.e. CIFAR10/100.\n- The proposed method is claimed to be more computationally efficient than others especially LFL. It should be supported by computing the computational costs for each; similar to what you did for the memory cost in section 3.3.1. Given the description from the paper, I could not recognize a significant difference between LFL and the proposed method in terms of computation costs. Please elaborate on this. \n- Not all the existing architectural methods increase the model size by adding new neurons or connections as stated in the paper. There is quite a number of papers that use fixed-capacity models [2,3,4]. Please consider the reference and discuss these papers. A comparison with [2] would also be interesting since it evaluates the same scenario evaluated in this paper. \n- The number of epochs used to train each task for permutated MNIST and rotated MNIST is quite low (1 epoch). This is reflected in the low accuracy even for the first task in Figure 1. Is there a motivation for this choice? \n- It is not clear whether you use the reduced learning rate (for tasks>1) for the baselines as well? I believe this is one main factor that helps you reduce forgetting.  \n- The paper discussed the scalability of the architectural methods. However, the length of the evaluated sequence (10  tasks) is less than the length of the sequences evaluated in the baselines (i.e. A-GEM evaluated sequences of 20 tasks).\n\n**Some writing issues:**\n- Page 2 Sec 1.2 “rehearsal-based strategies generally require large memory”. Please support your claims. Could you quantify “large”?  Consider also the size of the buffer used in previous papers (Check [5] as an example). \n- Page 2 Sec 1.2 “regularization strategies computationally less expensive” This fully depends on how you regularize the weights (ex. Fisher information matrix computed in EWC could be more expensive than other methods in other categories).\n- Page 8 Section 4: I found the second and third sentences contradicting each other ”task boundary may not always available” “exploit task descriptor for forward transfer”. \n- Page 8 Section 4: “exploit our approach for supporting task-incremental and\nclass-incremental learning”: I could not imagine how the approach could be extended for task incremental learning or class incremental learning. What is the benefit of projecting the new tasks near the center of previous tasks?  \n- Page 4 Equation 2: what is FL-1, x?. You should state this clearly in the paper, not in the appendix.\n- Page 4 Section 3: Please state that the experimental details are found in Appendix A. \n- Page 5 section 3.2: The description of the two oracles baselines is not clear.\n- Page 5 section 3.2: The first paragraph contains repeated information for the baselines already described in section 1.1. Consider using the space to describe your proposed method.\n- Page 5 section 3.2: “UCL solves the drawbacks of VCL” which drawbacks are you referring to?\n- Page 6 section 3.3 “ the overall performance of CLR in Appendix Figure 3” which Appendix?\n- Page 2 Section 1.1 Replay or rehearsal approach: the end of the second line contains an incomplete sentence which only has a citation.\n- Page 3 Section 2: The should be a full stop or a sentence leading to Equation 1.\n- I encourage the authors to consider writing the related works in a separate section apart from the introduction. \n\n[1] Tao, Xiaoyu, et al. \"Bi-objective continual learning: Learning ‘new’ while consolidating ‘known’.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 04. 2020\n\n[2] Wortsman, Mitchell, et al. \"Supermasks in Superposition.\" Advances in Neural Information Processing Systems 33 (2020).\n\n[3] Sokar, Ghada, et al.. \"SpaceNet: Make Free Space for Continual Learning.\" Neurocomputing 439 (2021): 1-11.\n\n[4] Mazumder, Pratik, Pravendra Singh, and Piyush Rai. \"Few-Shot Lifelong Learning.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 3. 2021.\n\n[5] Chaudhry, Arslan, et al. \"On tiny episodic memories in continual learning.\"",
            "summary_of_the_review": "The paper shows the effect of using center loss that decreases the intra-class separation and constrains the change in previously learned representation. Similar to the previous method, LFL, the weights of the classifier are frozen after the first task. The method is evaluated for the domain incremental setting. The method is simple yet it shows its effectiveness over some of the state-of-the-art methods. \nThe related previous similar work should be discussed and a conceptually and experimentally comparison to it should be considered. The claims should be well-supported. The details of the main proposed method are better to be in the main paper instead of the appendix. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a new regularization strategy for continual learning, called center loss regularization (CLR). For each previous task, the average of the embeddings of the task is stored. For new tasks, the new embeddings are forced to stay close to the old centers using an L2 regularization. The paper has an extensive experimental evaluation, showing that the method performs better on a number of standard benchmarks, up to small-medium sizes (mostly MNIST variants).",
            "main_review": "My main concern is that the description of the method is very short, and after reading through the paper several points remain unclear. The description is composed of ì3 equations (1)-(3), a short description, and a pseudo-code. However:\n\n1. What is \"m\" in (2)? Is it the batch size or the size of an external replay buffer? Or is it the number of tasks seen so far? The description of the paper hints at the third option, but it is not very clear.\n2. The pseudo-code (Algorithm 1) is very generic and could be applied to any neural network training.\n3. The authors talk multiple time of \"projecting the representations of new tasks close to that of old tasks\", but theiy are just forcing a distance measure, not explicitly projecting them (unless I am missing something).\n4. What is the \"softmax loss Ls\"? Is this a cross-entropy applied on top of the output of the network?\n\nThe paper is also missing a clear motivation. Keeping the embeddings close to multiple centers seems highly restrictive. What is the rationale behind this application? It would probably benefit also from discussing the idea of the center loss in the introduction itself.\n\nThe paper should also provide closer analysis of the relation of the method wrt the literature. Currently, this is done only for LFL (beginning of Section 2), but what is the relation with A-GEM (which also exploits averages) or embedding regularization (which also exploits embeddings)?\n\nSection 1.1 also requires a more careful proofreading (e.g., \"and learning rest task-specific parameters\").",
            "summary_of_the_review": "The paper idea appears to be novel. It combines existing concepts (center-wise representation of tasks, embedding regularization) in a new way. However, the paper requires a better motivation and a better description of the method. I would be glad to increase my score afterwards.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "The paper does not discuss ethical concerns. In my opinion, reducing a single task to a single embedding will amplify the effect of majority groups in a dataset. However, these concerns are rarely explored in the continual learning literature, so a single mention in the conclusions can suffice.",
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper focuses on regularization-based approach for continual learning. It proposes to freeze the weights of the last fully connected classification layer to keep the decision boundaries unchanged similar to previous work. The goal is then to make the learned features for new tasks localize nearby corresponding old task features in the latent space. It is addressed by using the center loss (previously introduced in  Wen et al. (2016)) as a regularization penalty for the softmax loss. The model is forced to learn the features for the new task in the proximity of the stored old class feature centers. For the new task learning, the learning rate of feature extractor is reduced to prevent significant changes.",
            "main_review": "On the positive side, the method is very efficient in terms of memory requirements and is very simple. The results show minor to moderate accuracy boost compared to direct competitors.   There are, however, many negative points. The contribution is very incremental: uses the idea from Jung et al. (2016) for continual learning and keeps the new class features close to old ones using the center loss regularizer from Wen et al. (2016). The writing is poor: The formulation is not well defined. The setting  is not defined at all. Part of the notations are not defined, e.g., x_i,y_i, L-1, m . It’s not clear what scenario was used. It is said explicitly that domain incremental learning is done in PACS. The task is not specified clearly in the rest of the experiments.",
            "summary_of_the_review": "My major concern is contribution and very incremental novelty (as detailed above).  Additionally, I think that the paper needs major rewriting.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposes a regularization strategy for task incremental learning (a.k.a, domain incremental). The strategy is tested on Rotated-MNIST, Permuted-MNIST, Digits, and PACS datasets.  \nThe contributions claimed are:  (1) a novel regularization-based strategy for CL (center loss regularization, aka CLR), (2)  computationally and memory efficient ,(3) with competive performance wrt SOTA CL strategies in domain incremental scenarios.",
            "main_review": "The approach is quite simple and easy to follow, which is a good thing.\nHowever, while digging a bit deeper in the paper, some essential details are difficult to find, such as the dimension of c^{(0)} or even a clear description of what is precisely one center: \"the learned values of feature centers from old task\" is not completely clear to me.\n Is it the mean feature vector for one class/task?\n\nThe motivation paragraph should be reworked to be more rigorous, I believe there is interest in studying regularization-based methods, but for example, \"the regularization strategies are generally memory efficient and computationally less expensive than the other two approaches [Architectural & Replay].\" is probably false.\nFor example, on rotated-MNIST (one of the experimented datasets in this paper), a vanilla rehearsal strategy is probably more memory and computationally efficient than most regularization strategies while being very effective. On the other hand, architectural approaches could also be efficient in such settings.\nTherefore, I believe that the motivation section should be better grounded to be more convincing.\n\nConcerning the approach, what is the risk of freezing the output layer to learn new tasks? Might it create some problems in some domain incremental settings, e.g. the first task with very simple data, then a second task with much more complex data?\n\nThe experiments contain interesting results with many baselines. I  particularly appreciated the results on Digits and PACS datasets.\nHowever, I did not fully understand why the baselines used for Rotated/Permuted MNIST are different from the baselines used for Digits and PACS. For example, why are Meta-DR results not in Rotated/Permuted MNIST experiments? This approach looks quite effective without replay in the other experiments.\nWhy is Gdumb not reported in Tables 5 and 6 while it is reported in table 2?\nI believe the experiment section should be reorganized to present better what the authors want to show.\nI think that important regularization baselines are missing, such as \"mean-IMM\" or \"mode-IMM\" \"Overcoming Catastrophic Forgetting by Incremental Moment Matching\" Lee et Al, and KFAC-Ewc \"Online structured laplace approximations for overcoming catastrophic forgetting\" Ritter et al.\nMaybe there is a good reason not to compare to them, but it should be explained.\n\n\nThe authors claim the computational efficiency of their approach. However, they do not provide any measurement of computational cost. They do not provide an estimation of the scaling behavior of their approach.\n\nSome Remarks:\nSection 1.1  reviews the bibliography but does not link the bibliography with the proposed approach. The link is more explained in the motivation section (1.2), but then 1.1 does not explain how the proposed approach fits the existing bibliography.\n\nThe results of tables 4/5/6 should not be after the conclusion but inside (or eventually above) the results section.\n\nRegarding results, since the classes are the same for all tasks, it could be interesting to evaluate the accuracy of future tasks to understand the progress better.\n\nIn Table 6: Unit of measurement is not given for additional memory requirement.\n\nIn Conclusion: \"Our method was effective in overcoming catastrophic forgetting when applied to the standard continual learning benchmarks as well as continual domain adaptation benchmarks.\" it would be more relevant to say that the approach was effective in incremental domain benchmarks.",
            "summary_of_the_review": "The papers introduce a potentially interesting regularization method for domain incremental learning that is lighter in memory and seems to be more computationally efficient. However, the computation efficiency is not estimated, and many details in the results section make the assessment difficult.\nI believe the authors should be more rigorous in presenting their results and on the motivation of their approach to improve the paper significantly. Moreover, some details should be fixed (some examples in the remarks).\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}