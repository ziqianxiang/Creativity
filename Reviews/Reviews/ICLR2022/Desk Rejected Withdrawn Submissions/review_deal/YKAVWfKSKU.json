{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes the deep Dirichlet process mixture model (DDPM). In a nutshell, DDPM uses deep neural networks (feature extractors combined with normalizing flows) for the likelihood model of Dirichlet process mixture models. A Gibbs sampler combined with stochastic EM for parameter learning of deep neural networks is presented for the inference.",
            "main_review": "This paper is well written with a detailed description of the algorithm. However, I vote for rejection for two reasons.\n\n1) Lack of novelty: the main contribution presented in the paper is an incremental combination of existing approaches. Roughly speaking, the proposed method is literally using deep neural networks with DPM. Moreover, the corresponding inference algorithm is a straightforward application of well-established existing approaches. I don't see any technical difficulty in inference where one would need a standalone contribution that is novel enough to write a paper. \n\n2) Missing recent baselines: Probably the proposed approach's main competitors are unsupervised clustering algorithms with deep feature extractors. Deep embedding clustering is one of the earliest methods among them, and there are numerous follow-up works better than DEC. To list a few,\n\nJiang et al., Variational Deep Embedding: An Unsupervised and Generative Approach to Clustering.\nCaron et al., Deep Clustering for Unsupervised Learning of Visual Features.\nYang et al., Towards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering.\n\nWhen it comes to the image domain, there also are numerous deep unsupervised clustering methods which make use of data augmentation to further boost up performance. I think the proposed method should be compared with more recent baselines to fairly assess the performance.",
            "summary_of_the_review": "The main contribution of the paper is an incremental combination of existing approaches, and the experiments are not convincing.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a probabilistic generative model that combines a Dirichlet Process Model (DPM) with a Neural Density Estimation Neural Network (NN) model. Sampling from this model consists of sampling clusters from the DPM with a Normal-Gamma base distribution where each cluster is a Gaussian, then sampling from these Gaussians and transforming these samples into a feature space with an invertible neural network. The paper proposes an iterative algorithm that alternates between updating the DPM and NN based on Monte Carlo Expectation Maximization. \n\nThe model is demonstrated on four datasets against K-Means, a deep learning clustering method called DEC, an algorithm called G-Means, and a DPM without the invertible NN. Clustering is evaluated using F score, V score, and ARI. ",
            "main_review": "Strengths: This paper presents an interesting model that combines advantages non-parametric hierarchical Bayesian models and neural networks. This work builds off of the SB-VAE model to use an invertible NN rather than the VAE, so experiments demonstrating that this works are valuable to the community.\n\nWeaknesses: \n- The use of an autoencoder in the first layer is never really explained. I assume this was done to reduce the dimensionality of the data so that it's easier to train the invertible NN. But it is not obvious that this representation will help the other clustering methods. I think one would want to compare results on both to be convincing. \n- The metrics used for evaluation on clusters are heuristics and not entirely convincing. I think it would be much more convincing if the paper compared algorithms based on the (estimated) log-likelihood. For this, it would make sense to compare to a Gaussian Mixture Model rather than KMeans.\n- In practice people choose K by optimizing it just like any other hyperparameter. So in fact K-means can easily be turned into an algorithm that infers k, probably with less computation than this algorithm. So I think it is totally fair to compare the proposed model with K-means, and the proposed algorithm does slightly worse than K-means on MNIST. \n- It's not clear to me what sort of inductive bias this architecture provides. It seems very sensitive to the hyperparameter choices (even more so than the choice of k in k-means). In particular, if you optimize the NN too much on the first step, I don't imagine the DPM parameters will change much after that. So in order to make use of the complicated structure, you need to tune things relatively slowly, and without any intuition for why we expect the inductive bias to be helpful we're left with an empirical question of whether this method is useful. ",
            "summary_of_the_review": "This is an interesting model and algorithm, but the motivation for such a complex model is weak and the experimental results aren't enough to convince people to use it.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors proposed the Deep Dirichlet Process Mixture Model, which simultaneously performs feature learning and Dirichlet process mixture clustering.\nThe core of the method is Monte-Carlo EM algorithm, iterating between DPM clustering and parameter update of the network used for extracting the features.",
            "main_review": "The paper is well written easy to follow, and theoretically sound. It has some nice ideas, creating a model which can perform DP mixture clustering while benefiting from the properties of deep learning is a well sought-after target.\nHowever, I do not think the authors have achieved their goal in that regard.\n\nRecent DPMM inference tools (Chang and Fisher, 2013; Hughes and Sudderth, 2013; Ge et al, 2015; Wang and Lin, 2017; Dinari et al, 2019)\nalready provide scalable inference for DPMM, which can handle large high dimensional datasets.\n\nWhat seemingly lacks in the current proposal is motivation - Why would I need a Deep Dirichlet Process Mixture Model.\nIn my opinion, it would be to operate in a setting where the aforementioned works are not sufficient, or the results are not satisfying enough.\n\nThat is however not the case in the current proposal:\n\nThe authors have separated the feature learning from the DPM clustering, while the two parts clearly benefit each other, the clustering itself does not utilize deep learning.\nMoreover, unless I have missed something, the sampling process is using the CRP based Gibbs sampling scheme, which is notoriously slow compared to other methods,\nthus the clustering process itself suffers from the same problems as other DPM clustering methods.\n\nFurthermore, the features learning seems limited, the authors did not adequately demonstrate that their model can learn more complex features.\nThe experiments where limited to smaller toy datasets, where a pre-trained ResNet50 was used to extract features for the STL-10 dataset.\nAn interesting experiment, which could put the proposed method above others, is applying it to raw vision datasets, such as Cifar or ImageNet,\nsuch settings are very difficult for DPM clustering, and no meaningful results can be achieved without some feature extraction method.\n\nThe experiments have room for improvements as well. A key selling point of DPM is that we do not need to define K,\n the authors have rightfully mentioned this in the introduction. However, examining the experiments, \n it seems that their method does not capture the true K well enough, taking MNIST for example, 81 classes is a major overestimation,\n and no wonder the 81-means has performed badly on it. \n Note that performing DPM inference with a well-calibrated NIW prior, over a PCA of the raw MNIST dataset, can not only better estimate the K,  but also outperform the proposed model.\n\nThe experiments suit need to improve in order to justify the method, comparing with k-means which deviate so much from the true K is handicapping the other methods.\nAdditionally, I would expect to see a comparison with other methods that perform the entire pipeline of feature-learning and clustering, such as Caron et al, 2018; Guo Et Al 2017, Caron et al 2020; and the such.\n\nIf the authors could clarify the next point - \nWhen comparing the DPM and the DDPM, they operate on different spaces, that is, the former on h_e(X) and the latter on f(h_e(X)), however, you have used the same hyperparameters for both?\n\nAnother thing that worries me - The authors have only performed clustering with isotropic Gaussians, while the learned features suppose to solve this problem,\nthe DPM is very rich, and can use much more complex distributions, where conjugacy is involved, it is also relatively simple. This seems like artificiality reducing the expressibility of the model,\ndoes the feature learning method cannot work with other types of distributions?\n\nLastly, timing should be added to the experiments, being one of the DPM weaker points.",
            "summary_of_the_review": "While the paper proposed idea is great, I do not think the current proposal achieves that idea.\nThe DPM inference method is not DL-based, thus the paper focuses on leveraging DPM for better learning of features - This is a common practice in papers that handle deep clustering.\nHowever, the authors did not demonstrate that the proposed model can learn complex features, and has only performed on the output of other neural networks, on relatively simple datasets.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a deep Dirichlet process mixture (DDPM) model, which combines flow-based generative model with Dirichlet process mixture model. Thus, it takes both advantages from deep feature learning and nonparametric clustering. The experimental results show DDPM gains significantly over DPM in a bunch of widely used datasets. ",
            "main_review": "Leveraging features learnt from deep learning to improve clustering has been extensively studied, and it is a natural solution to target nonparametric clustering. Unlike previous approaches, this paper unifies two generative models (DPM and generative flow), which makes both learning and inference easier. \n\nOriginality: the authors propose to choose generative flow for further feature learning and DPM for clustering. Unlike the existing methods, it makes the whole learning/inference process easier and efficient. The experiment results demonstrate that it works. One concern is that DDPM is built over the existed embedded feature space (stack autoencoder). It will be more convincing if its input is from raw data.\n\nClarity: overall, the paper is well-written and easy to understand. It gives detailed background on DPM and generative flow, as well as how to unify these two models in math. \n\nQuality/Experiment: The theoretical part of this article is comprehensive. The experimental part should be enhanced by adding some necessary ablation studies, for example whether DDPM is sensitive to initial embedding or not.\n\nBelow are some questions and suggestions for this paper.\n(1) From equations 17 to 19, is there any formula missing? It would be better if more details are given.\n(2) Overall, the experimental results can support the claim that DDPM can indeed improve performance on some tasks. To enhance the paper, it can either add an ablation experiment to check whether it is sensitive to initial embedding, or it learns from raw data.",
            "summary_of_the_review": "Overall, it is a good paper, which leverages both advantages from generative flow and DPM for nonparametric clustering. In addition, the generative model makes the whole learning / inference much easier. There is similar work VAE+DPM for nonparametric clustering, so the idea here is incremental, which uses generative flow instead. Another concern, the input of DDPM is from stack autoencoder, it would be more convincing if it learnt from raw data or another ablation experiment whether it is sensitive to initial embedding. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}