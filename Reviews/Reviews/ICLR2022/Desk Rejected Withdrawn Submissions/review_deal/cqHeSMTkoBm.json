{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes a framework for learning a joint curricula for reinforcement learning. A hyper-RNN is trianed to output weights of three different base networks and each base network generates a different curriculum: A sub-goal generator, an initial state generator and a shaped reward generator. Additionally, a memory mechanisim is proposed to generate abstract curriclum. ",
            "main_review": "### Strength\n* Automatic generation of curricula for RL is an important topic and the proposed method of using a hyper network for coordinating different curricula is interesting.\n\n### Weakness\n* More baselines are needed. Currently, the method only compares with GoalGAN(Florensa et al. 18) and ALP-GMM (Portelas et al., 19). There are some simpler baselines missing: PPO with a single curriculum such as adapting initial state [1], or reward shaping [2].\n* The robustness of the proposed curricula is unclear. The curricula for generating sub-goals and initial states aim to find goals or initial states where bellman loss is low, indicating that the agent has mastered the states. However, in the extreme case, the network can learn to generate curricula where the sub-goals are too easy to achieve or the initial states are too easy. It is not clear how robust the current method is to such cases.\n* Many technical details are not described clearly and not sufficient for reproducing. \n     * Sec. 3.1 described the training objectives in terms of a Q function, but PPO is used in the experiments. How is PPO adapted to learn with Q function?\n     * How does Q function take in a sequence of sub-goals $c_{goal}$ as input?\n     * In Sec. 3.3, how to alternate between the outer level optimization and the inner level optimization?\n\n### Minor\n* Notations Sec. 2 are explained in a minimal way and are not self-explanatory. \n\n[1] Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, and Pieter Abbeel. Reverse curriculum generation for reinforcement learning. CoRL 2017\n\n[2] Andrew Y. Ng et al. Policy invariance under reward transformations: Theory and application to reward shaping. ICML 1999",
            "summary_of_the_review": "The paper presents an interesting approach to combining multiple curricula. But multiple technical details are not described clearly and simpler baselines are missing.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the authors propose an approach that learns to combine multiple curriculum learning strategies for reinforcement learning. A set of base-RNNs are used to generate the initial state, subgoals, and reward function as the curriculum. And a hyper-Net is used to compose the curriculum strategies. The proposed method is evaluated in the CasualWorld environment and outperforms Goal-GAN and ALP-GMM.",
            "main_review": "Strengths: \n\n- The paper tackles the problem of automated curriculum learning by composing multiple curriculum strategies, which is novel and interesting.\n\n- The proposed method consistently outperforms the GoalGAN and ALP-GMM baselines in the CasualWorld environment.\n\n- The authors provide a detailed ablation study to discuss the importance of each module in the proposed framework.\n\nWeaknesses:\n\n- Could the authors provide a concrete definition and some intuitions to explain the latent curriculum information in Sec. 3.2? It is a bit confusing what the hyper-Net aims to learn to capture. It would be even better if the authors could visualize such curriculum information in the latent space in the experiments.\n\n- In Sec. 3.1 the authors describe that the base-RNN can be used for shaping the rewards. However, there is no experiment demonstrating the generated reward function or how reward shaping helps improve performance. It would be better if the authors could provide qualitative results as well as an ablation study. \n\n- It is surprising that the random baseline achieves a 90% success rate in Sec. 4.4, which is significantly better than GoalGAN and ALP-GMM. Could the authors explain why that is the case? Given these results, I believe it would be necessary to also include a Random baseline for the subgoal curriculum and incorporate its curve in Figure 2. Without such comparisons, it would be hard to justify the effectiveness of the proposed method.\n\n- In Figure 3 (Stacking) and Figure 5, it is a bit weird to see that all the curves of baselines show a very uniform trend while the full model behaves significantly differently. This result does not seem to be entirely reasonable. Could the authors explain why? \n\n- Several important references on curriculum learning and environment generation are missing, such as:\n>- Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey. Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, Peter Stone. JMLR 2020.\n>-  Adaptive Procedural Task Generation for Hard-Exploration Problems. Kuan Fang, Yuke Zhu, Silvio Savarese, Li Fei-Fei. ICLR 2021.\n>- Source Task Creation for Curriculum Learning. Sanmit Narvekar, Jivko Sinapov, Matteo Leonetti, Peter Stone. AAMS 2016.\n>-  Discovering Generalizable Skills via Automated Generation of Diverse Tasks. Kuan Fang, Yuke Zhu, Silvio Savarese, Li Fei-Fei. RSS 2021.\n>-  Mix & Match - Agent Curricula for Reinforcement Learning. Wojciech Czarnecki, Siddhant M. Jayakumar, Max Jaderberg, Leonard Hasenclever, Yee Whye Teh, Nicolas Manfred Otto Heess, Simon Osindero, and Razvan Pascanu. ICML 2018.\n>- Curriculum design for machine learners in sequential decision tasks. B. Peng, J. MacGlashan, R. Loftin, M. Littman, D. Roberts, and Matthew E. Taylor. IEEE Transactions on Emerging Topics in Computational Intelligence 2018.",
            "summary_of_the_review": "This paper tackles an important problem and proposes an interesting solution. However, the experiments are insufficient and not entirely convincing. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes using a hypernet, along with a memory module, to synthesize 3 curriculum strategies that are combined to better train Deep RL agents.",
            "main_review": "## **Paper Strengths**\n\n **Motivation:** This is a pretty straightforward paper, motivated by the need to improve upon automatic curriculum learning (ACL) by using a combination of different techniques.\n\n**Paper Clarity:** The method is clearly described, and its contribution of combining the memory module, multiple curriculum, and a hyper-RNN is clearly established. \n\n**Experiments:** The authors perform a pretty comprehensive analysis of the contributions of individual MOC components.\n\n## **Paper Weaknesses**\n\n**Experiments:** \n\n- It seems like Vanilla DRL outperforms or closely matches the baseline ACL methods on a couple tasks, when comparing Figures 2 and 3. This is a little strange, can the authors explain why? Have they performed sufficient hyperparameter tuning for baseline ACL methods? The authors should describe the hyperparameters tuned over and used in the appendix for baseline methods.\n- Evaluation: The authors don't mention how exactly the curves are generated. Presumably the rewards/success rates shown are during evaluation of *fixed* curricula in Figure 2, but the authors should explicitly state this.\n\n**Results:** The performance improvements seem a little marginal on some comparisons in Figure 3. This is still fine; not a big issue. \n\n## **Questions**\n\n- Out of curiosity, why did the authors not evaluate on the same tasks as GoalGAN or ALP-GMM?\n\n## Minor Issues\n\n- Typo in \"Shaping reward with Base-RNN\", in the equation for y (missing parenthesis).\n\n",
            "summary_of_the_review": "Overall, this work seems well-motivated and presents meaningful results. I am currently recommending an accept.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a curriculum reinforcement learning method that fuses multiple curriculum paradigms into a neural network architecture. More precisely, curricula over startings states, goal states and reward shaping functions are realized by recurrent neural networks parameterized by a hyper network. Additionally, the authors experiment with what they refer to as an abstract curriculum. This curriculum is realized as an additional memory mechanism from which the agent reads additional information. This additional information is aimed at improving the agents learning performance. The hypernetwork is trained simultaneously to the RL agent by minimizing various Bellmann errors.\n\nThe authors provide experimental evidence that their proposed way of learning curricula can improve the performance compared to baseline automated curriculum reinforcement learning methods in the CausalWorld environment suite.",
            "main_review": "Before stating my review I want to disclose that I already reviewed this paper for another conference. However, I do not know about the identity of the authors as the paper has not been accepted back then. At the previous conference, the paper was rejected e.g. because a lack of baseline comparisons, untangible experimental results and a suboptimal presentation of the method.\n\nIn general, I see that the paper has been improved substantially w.r.t. baseline comparisons and the presentation of experimental results. Also the presentation of the method has been improved. Nonetheless, I still see potential for improving the clarity of the presentation:\n\n* The objectives for the individual curricula seem to use Q-functions with different inputs. For the subgoal curriculum, an additional input $c_{goal}$ is added. For the initial state curriculum, this sub-goal is, however, missing in the objective. The abstract curriculum then again uses a Q-function defined over the abstract curriculum output $c_{abs}$ but not incorporating $c_{goal}$. This raises questions about the form of the Q function that the agent is using. Is it defined over all the previously mentioned inputs?\n* Does the goal state generated by the subgoal module influence the reward function? Unfortunately, this seems to be not discussed.\n* In the last version of the paper, the authors discussed that the abstract curriculum module needs to be pre-trained on some related tasks before being used. In the experimental section of this version, this discussion is not included anymore. Did the authors find a way to avoid this form of pre-training or is it still necessary? In the latter case, the authors should describe this pre-training.\n* The pick-and-place task as a comparison w.r.t. to ALP-GMM and GoalGAN seems to be missing. Is this a mistake or are there reasons why this task is missing for the baseline comparisons but present for the ablations?\n\nApart from the issues w.r.t. presentation and clarity of the paper, I have additional conceptual questions:\n\n* Given that the memory mechanism is write-only for the hyper network, shouldn't it be possible to just directly predict the abstract curriculum information $c_{abs}$ instead of reading it from a memory? Theoretically, the recurrent nature of the hyper network should already allow to realize a form of memory. Maybe an ablation of this external memory could shed additional light on its importance on the performance.\n* Is the reward shaping curriculum allowed to change the shaping function in each environment step of the agent training? In this case, it may not be in line with the theoretical framework by Ng et al. To clarify: This question came up in the last review during the reviewer discussion so that the authors did not have a chance to answer to this question back then.\n* Finally, I am missing a motivation why the Bellmann error is a suitable objective for training the hyper network. How does it relate to optimizing the agent performance? I am asking because being able to correctly encode the expected policy performance by minimizing a Bellmann error does not necessarily mean that the corresponding policy achieves high expected returns. Consequently, a discussion of this objective would improve the paper.\n",
            "summary_of_the_review": "In general, I think the paper is an encouraging work. However, the still many questions w.r.t. clarity and concept of the paper do not allow me to say that this work is very close to the acceptance threshold. Nonetheless, I think that many of my questions can be addressed in a revised version during the rebuttal. If this is the case, I am happy to improve my score.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}