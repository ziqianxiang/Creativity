{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a methodology for fair representation learning.\nThe core novelty of the proposal is the satisfaction of a sufficient rule.\nExperimental results claim good performance with respect to other approaches in terms of an accuracy-fairness tradeoff, in which a sufficient gap is measured.",
            "main_review": "The core weakness of this paper is in terms of presentation. The writing lacks self-sufficiency, while it also contains language errors that confuse the reader.\nThe core concepts of sufficient rule, sufficiency, and sufficient gap are brought up several times, but never adequately defined.\n\nThe closest we come to an intuitive statement is this statement:\n\"the sufficient rule is encouraged, which reflects the same likelihood of recidivism irrespective of the individual’s group membership\"\nIt is not defined what \"reflects\" means in the above, and the rule, as explained, is not connected to a mathematical definition.\nThen, the concept is invoked in a mathematical manner in Proposition 1. However, even there, the concept of sufficiency is invoked, but left undefined.\nEventually, in Section 5.1, we see a definition of the sufficient gap, which is claimed to recover a predictive parity gap.\nHowever, both concepts are left without a discussion of the intuition and semantics underlying them.\n\nDue to the above gaps, the main claim of the paper to demonstrate that \"the bi-level objective is to fulfil the sufficient rule\" is left shaky.",
            "summary_of_the_review": "The paper does not define the main concepts is used in the claims it makes.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper considers the fair representation learning problem. In particular, the fairness notion utilized is (close to) the sufficient rule, and this paper proposes a bi-level implicit path alignment algorithm to (approximately) achieve it.\n\nThe contribution of the paper includes (1) an empirical algorithm for DNN to (approximately) achieve the sufficient rule, (2) theoretical analysis w.r.t. effectiveness of the algorithm as well as its application scope (classification and regression).",
            "main_review": "Overall, the paper is relatively well-written. It would be greatly appreciated if the authors can kindly clarify the following questions/comments:\n\n### q1: the exact characterization of the term \"invariant\"\n\nI understand that (as indicated by the paper) one of the primary goals of finding fair representation is to clear the way for downstream tasks.\nWhile I am not questioning intuition, I found the characterization of the term \"invariant\" in the paper is a little bit confusing. In particular, at the very beginning of Page 2, it was mentioned that \"the optimal predictor learnt on the embedding space [is] invariant from sub-groups\", which according to my understanding is saying that the downstream predictor $h$ is irrelevant to the value of the protected feature; on Page 3 (the paragraph **Learning Invariance**) the word \"invariance\" is used to describe the OOD behaviour of the learnt representation, i.e., w.r.t. the embedding function $\\lambda$. Which kind of invariance exactly is the one readers should focus on? (or both?)\n\n### q2: potential extension to cases where the protected feature is non-binary\n\nFrom the setting and the theoretical results, it is clearly presented that the discussion is limited to the binary protected feature scenarios. I am just wondering can we naturally extend the proposed framework to non-binary protected feature cases (e.g., the ratio of people of certain demographic backgrounds in the community is a continuous variable)? If such extension is highly nontrivial (which I suspect so), what is the biggest impediment (theoretical or empirical or both)? Including such a discussion would be very helpful in order to understand the practical significance of the proposed approach.\n\n### q3: regarding the \"implicit path alignment\"\n\nTo the best of my knowledge, using implicit path alignment w.r.t. downstream predictor is novel in the fair representation learning literature. I have a question regarding the way it is implemented. As indicated in the first equation on Page 4, the paper considers the t-th update (instead of the whole path), but in the experimental setting, it is mentioned that the algorithm encourages the \"identical one-step optimization path\". I personally find this a bit confusing. If we were to keep each one-step optimization identical, is it explicit path alignment instead of implicit?",
            "summary_of_the_review": "Overall, the paper clearly presents the intuition and the proposed approach. It would be better if it authors can kindly clarify the questions/comments.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper considers the learning of fair representations (and classifiers). In particular, the paper considers the (group) sufficiency rule for fairness and optimization path alignment to achieve the fairness requirement. The paper considers a representation fair if the optimization path (gradient steps) of subgroups are equivalent / invariant, with respect to optimal subgroup classifiers. To this end, the paper introduces a bi-level optimization problem for a representation function, which is solved by approximating gradients with the implicit function theorem. The approximation error of the gradients and the convergence of representation function (appendix) are analysed. Their algorithm is then experimentally tested under various datasets and compared to various baseline approaches.",
            "main_review": "Strengths:\n  - The proposed path optimization framework of fairness is intuitively easy to understand.\n  - Optimization of the proposed bi-level optimization problem has a clear connection to group sufficiency + predictive parity (Prop. 1).\n  - The implicit path algorithm is well motivated through the implicit function theorem approximation of the gradient.\n  - The experimental section is extensive and shows promising results.\n  - Very well presented with useful figures.\n\nWeaknesses:\n  - A majority of the paper considers approximate versions of the classifiers $ h^{\\varepsilon}_{i} $. However, the fairness proposition (Prop. 1) requires optimal classifiers and also requires each subgroup classifier to be equal. How does the proposition adapt when these assumption break? In particular, is there a way to relax the assumption to give approximate predictive parity fairness?\n  - Time taken in the experiments could be documented to compare approaches.\n  - It would be useful to clarify the limitation / cost of considering sensitive attributes beyond the binary case.\n  - Discussion about the link between the complexity of classifiers $ h $ and the changes of accuracy / fairness would be interesting. For example, in the CelebA dataset a two layer neural network was used (in contrast to the other dataset which use a linear predictor). Do you achieve similar results with just a single layer?\n  - In the experiments, did the representations change much from the original representations? I.e., did you observe some type of Lipschitz condition on learnt $ \\lambda $.\n\nMinor:\n  - Typo on Figure 1, red loss gradient should have subscript \"1\".",
            "summary_of_the_review": "Overall I would recommend an accept for the paper. Although I have a few questions regarding Prop 1 / fairness when considering more relaxed assumptions, the paper provides a nice intuitive framework and theoretically well motivated algorithm. Furthermore, the extensive experiments show that the proposed algorithm has promise when considering group sufficiency fairness.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper considers fair representation as a bi-level optimization problem where the data representation is first learned then the fair predictors are optimized based on this fixed representation. A theoretical analysis on the the error gap of the implicit algorithm is conducted, and some positive results have been shown in the paper. The paper presents an interesting and topical problem and is easy to follow.  \n",
            "main_review": "\n-- Lacks proper motivation. The paper first motives this work by stating learning a fair representation with respect to the sufficient rule is necessary and appealing for the specific real-world intelligence system designing which is a universal motivation applicable for most of the fairness studies. The computational complexity of gradient-descent directions is later discussed but there is no experimental results to support this claim.   \n\n-- The paper is not self-contained. Some key terms such as the sufficient rule and sufficient gap are not well covered.  \n\n-- My other concern is that whether the positive results obtained can be generalized to other setups: if the sensitive attribute is highly associated with the class label, the learned data representation for overall loss purpose will be naturally highly associated with the sensitive attribute. Then the subsequent 'fair' predictors learned based on this fixed representation are not guaranteed. A correlation analysis might be of helpful for justification purpose. One relevant question is that whether the dataset is divided into subgroups based on the sensitive attribute for fair predictors training? If so, how about the potential information loss?  \n\n-- Some baselines from adding fairness constraints during the training process are considered. However, fundamental ones such as the \"Learning fair representations\" work is not compared against although discussed in the related work. In addition, non deep learning approaches such as fairness-aware decision trees enjoy low computationally and memory complexity, which is one motivation of this work, in comparison to deep learning methods, are ignored. Can the proposed method outperform these efficient fair models when the complexity is higher? Otherwise the merit of this approach is limited. ",
            "summary_of_the_review": "The paper presents an interesting problem. However, the concerns is that the paper lacks proper motivation. The paper also lacks sufficient justification of the proposed method.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}