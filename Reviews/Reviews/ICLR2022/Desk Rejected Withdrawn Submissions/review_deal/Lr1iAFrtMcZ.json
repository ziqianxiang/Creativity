{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper improvises the well-known UCB1 algorithm for the stochastic Multi-Armed Bandits problem by introducing a new concept of 'bandit distance,' which measures the distance between the empirical mean estimates of different bandits (arms). The authors propose an algorithm named UCB-DT that uses bandit distance to get tighter confidence intervals for mean reward estimates.  They also compare the performance of the proposed algorithm with the existing algorithm.",
            "main_review": "The only thing I liked about the paper is the idea of reducing the number of times sub-optimal arms (bandits) selected to maximize the total rewards. A similar idea is used for pure exploration in multi-armed bandits problems (see https://proceedings.mlr.press/v49/garivier16a.pdf). But I am not happy with the implementation. The following are problems with the paper:\n\n1. In the multi-armed bandits literature, different choices or actions or options are called arms, not bandits. I find it very inconvenient to read. It is the first thing that needs to be changed (bandit --> arm). Now I am using 'arm' instead of 'bandit.'\n\n2. Statement from Abstract: \".. improved performance as measured by expected regret by preventing the Multi-Armed Bandits (MAB) algorithm from focusing on non-optimal bandits, which is a well-known deficiency of standard UCB.\" All bandits algorithms have to play sub-optimal arms to estimate their mean reward for making a better decision (finding optimal action/arm). Hence, the statement is a bit misleading. \n\n3. I am not sure how the authors come up with Eq. (2). As I know,  any policy is called asymptotically optimal if $\\lim_{T \\rightarrow \\infty} \\frac{R_T}{T} \\rightarrow 0.$ Further, Lai & Robbins, 1985 give a regret lower bound for the MAB problem, which is not the same as in Eq. (2).\n\n4. When N_i(t) is updated in Eq. (5), it is unclear how the arms closer to the optimal arm will have larger confidence bounds than sub-optimal arms. For example, when the bad sub-optimal arms are often selected initially due to high observed rewards.\n\n5. The issues with figure 1: i) What are the values of the Y-axis. ii) Plot (a) is not needed.  iii) Due to the thickness of the bar, it isn't easy to know the exact value on the X-axis. iv) Statement in Plat (b): \"Learning that $B_3$ is far from B1, $d_t(1, 3)$ will be close to 1, which inhibits its confidence bound to grow larger.\" It is not clear whose confidence bound will grow. $B_1$ or $B_3$?\n\n6. There is no intuition given why the equation for distance in Eq. (6) is used and how it will work for sub-optimal arms closer to optimal arms than the bad arms.\n\n7. It is good to give some idea of how you will be extending UCB-DT when the mean reward is not smaller than 1.\n\n8. There are tighter bound than what is used in Eq. (8) (Check Theorem 2 of https://arxiv.org/pdf/1204.5721.pdf). Therefore, Eq. (9) is not correct.\n\n9. The lower bound given in  Lai & Robbins, 1985 will lead to the lower bound on the reward instead of what is given in Eq. (10).\n\n10. To find $N_{bargain}$, the horizon $T$ and sub-optimality gap needed to be known. Knowing $T$ is reasonable. But if one already knows the sub-optimality gaps (assuming you don't know which arm has a larger mean), then this information can be used to decide how many times the arm needs to be selected (based on the lower bound given in Lai & Robbins, 1985).\n\n11. It is shown that Thompson Sampling achieves asymptotically optimal regret bound (maximum rewards) for MAB when rewards of arms have Bernoulli distribution (see Theorem 1 of https://arxiv.org/pdf/1205.4217.pdf). Hence it is necessary to compare the performance of the proposed algorithm against Thomson Sampling, when rewards have Bernoulli distribution.\n\n12. It is unclear how many times the experiments are repeated (average regret of 1 run or 10 runs or 100 runs). Also, there is no confidence interval given. Since MAB algorithms performance depends on rewards observations, it is important to have average over multiple runs with confidence intervals.\n",
            "summary_of_the_review": "Strong Reject",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not applicable.",
            "recommendation": "1: strong reject",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper provides a novel insight into over-exploration analysis and, influenced by it,  proposes a novel variant of the UCB algorithm called UCB-DT, but lacks providing a clean result that characterizes the advantage of UCB-DT over UCB. It also provides some novel insights on over-exploration, but not in the form of a principled methodology.\n",
            "main_review": "The paper proposes a novel variant of the UCB algorithm called UCB-DT that is supposed to fix the over-exploration issues of UCB.\n\nStrengths:\nNovel insights into over-exploration analysis, a proposal to fix over-exploration of the UCB algorithm. Experimental results show convincing performance.\n\nWeaknesses:\nAnalysis is hand-wavy at points, formal claims are lacking, \n\nIn more details:\nThe analysis is hand-wavy at points (see in more details below Further remarks), and formal claims are not made. However, the main results seem to be that UCB-DT satisfies the sub-UCB condition (just like UCB), and a novel insight into over-exploration analysis based on the notion the authors call \"Exploration Bargain Point\". Additionally, the authors claim that Exploration Bargain Point can be used to prevent over-explanation - but it is not made clear how to do that. \n\nThe experimental results are more robust, and they indeed show that in many cases UCB-DT outperforms all other major UCB variants. (Not by magnitudes, but significantly - although it can also underperform in many cases.)\n\n\nFurther remarks:\n---------------\np1: \"parmeterizable\"  -> parameterizable\np2: \"the above forms a regret upper bound\" - should be lower bound?\np1: epsilon-greedy - if I understand the claim on p4 correctly, the algorithm degrade to greedy (i.e., 0-gredy)\np2: \"asymptotically optimality\" -> asymptotic optimality?\np5: (7) does not make sense. Both sides of the inequality inside the probability is a deterministic expression. One fix could be to replace \\Delta by the difference of the empirical means.\np6, Eq (10): The motivation for the formula is unclear. Why would the weighted sum of G_full with N=N_1 and with N=N_2 make sense? G_full is the expected cumulative reward. How can \"we write G(N_2) >= G_full\"? What is the guarantee that G(N_2) is the actual cumulative reward of the algo? The explanation is too hand-wavy.\nSection5: What distance did you use for UCB-DT in the experiments?",
            "summary_of_the_review": "The paper proposes a novel variant of the UCB algorithm called UCB-DT that is supposed to fix the over-exploration issues of UCB.\n\nStrengths:\nNovel insights into over-exploration analysis, a proposal to fix over-exploration of the UCB algorithm. Experimental results show convincing performance.\n\nWeaknesses:\nAnalysis is hand-wavy at points, formal claims are lacking, \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a modification to the classical UCB (Auer et al., 2002) algorithm, dubbed UCB-DT (distance-tuned), that is shown to have promising empirical performance vis-\\`a-vis other variations of UCB that have been studied earlier in the literature. The proposition is to modify the sample-count $N_i(t)$ appearing in the denominator of the bias term of the UCB score to $N_i(t) + \\sum_{j\\in[k],j\\neq i}d_t(i,j)N_j(t)$, where $d_t(i,j)$ is a notion of \"distance\" between arms $i$ and $j$. The underlying idea is that an \"appropriate\" distance function may ensure that the policy engages in the \"right\" amount of exploration for optimal regret performance. In the corner case, setting $d_t(i,j)=1\\ \\forall i\\neq j$ identically reduces UCB-DT to the degenerate greedy policy, while fixing $d_t(i,j)=0\\ \\forall i\\neq j$ maps it to vanilla UCB which is known to be overly optimistic in exploration. The authors also propose a few candidate distance functions. The regret performance is evaluated as a function of the policy's \"exploration budget,\" and the authors establish the existence of an \"exploration bargain point\" at which a performance equal to that under vanilla UCB is achieved, albeit with a significant reduction in exploration. This, I believe, is the main takeaway from the paper; that it is possibly to prevent UCB from indulging in unnecessary exploration without compromising on its regret performance. The paper proposes doing this (making UCB \"less optimistic,\" broadly construed) by shrinking the confidence intervals.",
            "main_review": "The paper is very accessible and well-written overall. The fundamental premise underlying this work is also interesting and relevant in practice. However, because the paper is primarily empirical in nature, many important questions remain unanswered, including those pertaining to the actual merit of the authors' propositions. For example, Figure 4 shows that the margin of improvement is significant only over vanilla UCB and, in fact, KL-UCB and KL-UCB++ almost always outperform UCB-DT. In light of the many developments in the area of optimism-based algorithms since vanilla UCB (2002), it is unclear if the variation proposed in this paper has any non-trivial benefit compared to the state-of-the-art in UCB-inspired bandit algorithms. Of course, the suggested algorithmic modification and the ideas of optimal, bargain exploration points is very interesting and seemingly has a lot of theoretical mileage. However, considering the absence of supporting theoretical results in this paper, I am not convinced the contributions are substantial enough for ICLR; this is a promising piece but also a work in progress.\n\nMISCELLANEOUS. On a different note, specification of the distance function is a non-trivial exercise and should be fleshed out more rigorously and formally (perhaps axiomatically or based on certain \"limits and achievability\"-type considerations). Also (IMHO), it is more common to refer to the $i^{\\text{th}}$ arm of a multi-armed bandit as \"arm $i$,\" instead of \"bandit $i$.\" The latter might create confusions as to multiple instances of the problem.. ",
            "summary_of_the_review": "Based on the aforementioned reasons, I would vote for a (weak) reject. I must clarify that while I do appreciate the line of approach in this work, my assessment is purely on the basis of theoretical contributions. I think this could be a rich direction from a theory as well as practical standpoint, but needs a lot of work to iron out the missing bits.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The paper introduces a modification of the standard UCB method that calibrates the \"degree of exploration\" by incorporating the distance\nbetween an arm and others. The authors argue that using such information can avoid \"over-exploration\" of the UCB method, thereby improving the performance of UCB.\n \n",
            "main_review": "Strengths: \n\n1. The idea of incorporating the information on the distance between arms to avoid over-exploration of the UCB method is quite novel.\n\n2. The paper is clearly presented.\n\nWeaknesses:\n\n1. In choosing \\gamma, one needs to know N_bargain, which requires knowledge on the sub-optimality gap. But this is a big assumption, and knowing this piece of information changes the problem --- so the proposed method should be compared to a different class of algorithms. In the case of K arms, what is the rule for determining gamma, and do we need to know all K-1 suboptimality gaps?\nPlease correct me if I misunderstood anything.\n\n2. The analysis of the finite-time regret relies on an assumption (Assumption 1) that has not been rigorously justified. \n\n3. In the simulations, how is \\gamma selected? Is it chosen with knowledge of \\Delta? I wonder in terms of real data, what is the rationale of choosing \\gamma, and will the newly proposed method be consistently better than UCB?\n\nMinor:\n\n1. In the first line of Page 3, the format of the citation should be \"proposed by Gittins (1979)....\".\n2. Page 5. Why can we assume N_1 >= N_2?\n3. Page 5 eq (7). Should n be T?\n4. Page 6, eq 10. The probability refers to the chance of choosing an arm step T -- how does this connect to the cumulative regret?\n\n\n",
            "summary_of_the_review": "The idea of incorporating distance between arms to improve UCB is novel; however, the improvement and theoretical/empirical guarantees of the newly proposed method need more justification.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}