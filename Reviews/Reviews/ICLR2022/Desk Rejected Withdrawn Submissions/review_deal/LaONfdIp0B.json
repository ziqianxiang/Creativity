{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies finite-horizon continouous-time dynamic games with deterministic transitions. For such a game, the authors characterize the Markov perfect equilibrium by applying the maximum principle to the Hamilton-Jacobi-Issacs equation. Moreover, the authors propose to use neural networks to represent the control policies and thus the HJB equation yields a mean-field control problem in the space of neural network parameters. From this perspective, this work establishes a generalization error bound when the NNs are trained from data. ",
            "main_review": "1. [Reinforcement learning vs control] It seems that this paper might not concern with reinforcement learning because the cost $L$ and transition function $f$ are both known. This paper mainly considers how to solve the dynamic game (in continuous-time) using neural network policies. Moreover, in reinforcement learning, the setting is discrete-time (as opposed to continuous-time in this paper) and usually, there are stochastic transitions/dynamics (as opposed to deterministic dynamics in this work).  I think it might be better to regard this paper as solving a control problem.\n\n2. [Deterministic transition] It seems the Pontryagin principle hinges on the assumption of deterministic transition, which is very restrictive. Can you handle stochastic dynamic systems? \n\n3. [Continous time vs discrete time] In RL, it is more common to consider discrete-time. And it is possible to also study the Pontryagin maximum principle under the discrete-time setting. See [1-5]. \n\n4. [Novelty] This paper seems to be incremental given existing work. First, it is well-known that Pontryagin maximum principle can be used in learning MDP or optimal control (e.g.,[6]) and it has been also applied widely in adversarial training. Thus combining these strands of research naturally yields the PMP for the dynamic games. Second, learning neural networks via the mean-field perspective is also well-known (e.g., [7, 8]). Third, the generalization error bound is borrowed from [Mou et al]. \n\n5. [Related work] It seems quite surprising that this paper fails to acknowledge many related bodies of literature. It would be great to provide a detailed discussion on the related work. Some of the closely related research include:\n- Pontryagin principle in adversarial training, MDP and optimal control, ($H_\\infty$-)robust control, dynamic games\n- reinforcement learning for solving mean-field games and mean-field control \n- mean-field perspective of neural network learning\n- generalization bounds in deep learning \n\n\nReferences:\n \n - [1] Robust Deep Learning as Optimal Control: Insights and Convergence Guarantees\n - [2] Pontryagin Differentiable Programming: An End-to-End Learning and Control Framework\n - [3] You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle\n - [4] Safe Learning in Robotics: From Learning-Based Control to Safe Reinforcement Learning\n - [5] Robust Discrete-Time Pontryagin Maximum Principle on Matrix Lie Groups\n - [6] Maximum Principle, Dynamic Programming, and Their Connection in Deterministic Control \n - [7] Mean Field Analysis of Neural Networks: A Law of Large Numbers\n - [8] Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit\n",
            "summary_of_the_review": "This paper considers a rather restrictive setting (known model, deterministic system) and the proposed methodology and theory seem to be rather incremental given existing works. Moreover, this paper fails to acknowledge numerous related works. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "\n",
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper formulates adversarial reinforcement learning as a mean-field optimal control problem and uses this framework to state a number of theorems about the convergence to a globally optimal solution. Under strong assumptions, the authors then analyze the uniqueness of their mean-field two-sided extremism principle. Finally, the authors prove two bounds on the generalization error, comparing the mean-field to the learned model. ",
            "main_review": "The authors write that their work is the first to pursue foundations for adversarial RL. I would comment that at this point there are many works that formulate adversarial training with a setup quite similar to the one proposed here. For example, the papers by Hsieh et al, Domingo-Enrich et al on finding mixed Nash equilibria in two-player games. In fact, it seems to me that the two-sided extremism principle is quite similar to this analysis. \n\nOne aspect of the paper that I found was not sufficiently well-explained was why it is acceptable to simply absorb the action function $a$ into the reward function. This seems to deviate from the standard set up for RL, namely that of a Markov decision process. In that setting, the reward would not simply be a function of the current state and action, because the MDP would conditionally same the reward and next state. I would say this constitutes my central reservation about the paper---it is simply not very clear to me that it is closely connected with RL. \n\nThe loss function (2) is defined on the finite time horizon with no discounting. Could the authors comment on the necessary changes if discounting is used?\n\nThe assumptions on Theorem 4.2 are very strong. Under what conditions should we expect the hamiltonian to satisfy the convexity properties in the third bullet point?\n\nMinor comments:\n\n\"dependence in $f$ via the usual trick\" --- what is the usual trick?\n\nSection 4.3 lacks adequate context; something more should be said about the subsequent use of the Fr\\'{e}chet derivative. ",
            "summary_of_the_review": "The paper is well-written and rigorous, providing detailed arguments that extend the results in E et al (2019), for a mean-field optimal control problem following the framework of Eq. (2). The connection to reinforcement learning is not sufficiently well articulated. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies adversarial deep networks using a mean field optimal control viewpoint. When the number of layers is very large, deep networks with some residual architectures can be approximated by an ordinary differential equation that describes how the input is transformed into the output by the neural network. Here, the authors consider two deep networks that have adversarial loss functions. At equilibrium, the parameters of the two networks should correspond to the solution of an inf-sup problem optimal control problem.  The loss function depends on the distribution of the data points, which are pairs of input and target. The problem can then be formulated in the formalism of mean field optimal control. Under some conditions, the authors first prove uniqueness of the solution in short time and then derive optimality conditions in the form of a mean-field Hamilton-Jacobi-Isaacs equation. They conclude with bounds on the generalization error in terms of number of training samples.",
            "main_review": "The idea of bridging adversarial reinforcement learning (RL) with mean field optimal control is interesting. The authors bring tools developed in the context of mean field control to the analysis of deep neural networks. However I have several concerns:\n\n1) I do not see what the link with RL is. The only explanations I could find are in section 2, below equation (1). It seems that the only connection is to identify the steps of RL environment with the layers of the deep neural network. But this connection looks quite arbitrary and not related to the rest of the paper. If I missed something, it would be important to clarify this point.\n\n2) I understand that this paper is a theoretical contribution. However, the assumptions seem at first sight very restrictive o it would be nice to provide a few examples of networks that would satisfy the conditions. I imagine that continuity is fine, but in Theorem 4.2 for instance, it seems unlikely that concavity-convexity of H with respect to the parameters would hold. \n\n3) It is stressed (e.g., in the abstract) that the bounds “do not explicitly depend on the dimensions, norms, or other capacity measures of the parameter”. Do they depend implicitly on such quantities? In any case, since the network has a continuum of layers, considering $\\theta$ over even a small time interval $[t, t+\\epsilon]$ already contains an uncountable number of parameters. So I would expect good generalization properties without dependence on the dimension of each parameter. It would be great to clarify why the error bounds obtained in this work are surprising and why one should have expected a dependence on “the dimensions, norms, or other capacity measures of the parameter” in the present setting.\n\n4) The comparison with related works should be improved, both the mean field control literature and the machine learning literature. For example on page 9, in the “Comparison with existing works” paragraph, not a single reference is provided. At several other parts of the paper, the authors simply mention the most related work that has inspired their proof but not works from the literature. Also, from the technical viewpoint, it seems that the proofs are quite straightforward given the existing works (in particular (E et al., 2019) and (Mou et al., 2018)). If there some techniques are new, it would be worth stressing them more clearly. ",
            "summary_of_the_review": "Overall, although the idea to use mean field control to analyze adversarial RL is interesting, it is not clear to me what kind of information the results presented in this work really provide for the theoretical understanding of adversarial RL.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper focuses on adversarial reinforcement learning with the mean-field quantitative differential game. In this framework, this work analyzed the convergence and generalization guarantee and proposed the mean-field PMP, which gives a necessary condition for the optimality of this dynamical system.",
            "main_review": "Weakness\n1. This work first mentions the K layer neural network in equations (1),(2) and later replaces the discrete dynamics with a continuous dynamical system (see equation (3)). I realize that using gradient flow or the mean-field method to approximate the dynamics of a neural network is a standard method. However, the approximate error is close to zero only when the learning rate $\\eta$ is close to zero and time $t$ is larger enough. There is no such a lower learning rate $\\eta$ in this work, and the number of steps is fixed and small (the number of layers is usually small). Therefore, there is no guarantee that the approximate error is small, and it seems not reasonable to make this replacement without more claims about the error.\n\n2. For adversarial reinforcement learning, the author considers a mean-field quantitative differential game (See equation (7)). However, the adversary in adversarial reinforcement learning can be arbitrarily, and it seems improper to assume that the adversary can also be represented as a neural network with parameter $\\theta$. Equation (7) only considers a particular adversary, which limits the compact of this work.\n\n3. The quantitative differential game is a concept of game theory. The definition of a mean-field quantitative differential game is more like a mean-field two-player zero-sum game rather than a mean-field adversarial reinforcement learning. It seems that the word “adversarial” is not proper. \n\nIn addition, this work is more like a combination of fundamental analysis of the two-player zero-sum game with the mean-field technique. Based on these previous works, the novelty of this work is also limited.\n",
            "summary_of_the_review": "Based on previous weaknesses, the contribution and novelty of this work are limited, and it is marginally below the acceptance threshold.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}