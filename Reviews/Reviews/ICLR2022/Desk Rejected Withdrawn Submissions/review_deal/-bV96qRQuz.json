{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes to rank architectures by a two-stage training method and achieves a strong ranking correlation compared to some zero-cost baseline methods with lower search time. ",
            "main_review": "The method proposed in this study is simple and easy to use. It enjoys the data and task-dependent benefit compared to the zero-cost methods. It shows better efficiency than the methods using reduced epochs of training. However, I mainly have the following concerns:\n\n1.\tThe method in this study is over-simple. It lacks novelty and can only be regarded as an engineering practice. The feature extraction capability is still measured by the architecture performance. The two-stage method just enjoys efficiency than entire training based on the previous studies observing that early layers train fast and become stable within a few epochs. But it fails to offer any in-depth understanding on how neural architectures affect their feature extraction capabilities. \n2.\tIn Figure 6, it seems that the proposed method does not surpass the best zero-cost method. It does not show significant superiority over the “shortreg” baseline either.\n3.\tThe motivation to introduce the fast and shallow learner of the accuracy threshold is not clear. The authors did not validate its necessity in experiments. Can we skip the threshold learner and begin with the stage 1 for a pre-defined accuracy or number of training epochs? Besides, why do the authors ensure all candidate architectures attain the same threshold accuracy in the stage 1? Does that mean all candidate architectures have the same feature extraction capabilities after stage 1? \n4.\tThe method has a limited applicability. Since it requires to train each architecture separately in the two-stage manner, we can only rank a subset of architectures (1000 in this paper) in a given search space, which impedes the efficacy to get the optimal architecture in the search space.\n",
            "summary_of_the_review": "I suggest a reject for this paper due to the drawbacks listed above. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The authors present a method to rank models during NAS. In particular, they propose to train networks to a reasonable but not-too-high accuracy, and then only fine-tune the last few layers to rank the models according to the accuracy. This final fine-tuning step can be done very quickly, thus speeding up the search. The experiments on several data sets show benefits of the approach.",
            "main_review": "While the idea behind the paper is reasonable and intuitive, the execution of the paper is quite subpar. The paper lacks focus, and the authors failed to make their easily readable. A lot of details are missing, and a lot of important details are relegated to the appendix, which is unacceptable (even some of the more important results are only in appendix, which is not why the appendix is there for).\nFor more detailed, point-by-point feedback please see below.",
            "summary_of_the_review": "Detailed comments:\n- Below Fig 2, \"weak architectures\", what does this exactly mean. \"weak\" should be defined, and same goes for \"stronger\".\n- Immediately below, the authors say they used \"1000 architectures\" to plot the results, but they don't explain which 1000 models were used here.\n- In the below paragraph titled \"Motivation\", the authors don't really explain how the final stage works, how is it tested, on what, and a lot of other important details are missing.\n- Similarly in the experiments, \"cells\" and \"edges\" is also not explained. The authors didn't try hard to make this work readable.\n- One of the first results in Section 4.1 that is discussed is actually in the appendix. As mentioned above, this is not acceptable.\n- Along the lines of the lack of clarity that is seen in the entire paper, in Section 4.2 the authors mention \"inner loop\" without actually defining which loop is that. This makes the paper not self-contained, and adds more confusion.\n- In the same paragraph, the authors say that they \"show\" something, but none of the results is actually shown in the section or in the regular paper pages.\n- They say that they vary \"both batch sizes and number of epochs\", but they don't describe the choices. Again, details missing.\n- The authors initially just give a layer until which the model is frozen, and focus on analyzing impact of batch size and epochs. But these hardly seem as relevant as the layers for the purposes of investigating the authors' proposal, and should be analyzed only later.\n- x-/y-labels in Fig 3 are missing.\n- The authors introduce a number of baselines, but in Section 4.3 they use quite a weak baseline for their main experiments.\n- A lot of other important results are also relegated to appendix but discussed in the experimental section, such as Table 3, and many other figures mentioned in the section.\n- \"Finding the training threshold\" paragraph should be moved much earlier.\n- \"5-10 epochs is enough\", but how many were actually used?\n- More details on random search should be given.\n- \"had finished 7 runs at the time of writing\", so the paper shows incomplete results? Why not just wait for the full experiments to complete and then submit a paper?\n- In Section 5, the authors first say that data-agnostic measures are enough, and then later that the performance is \"a function of the dataset and task\". These two statements seem to be in conflict.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper introduces FEAR, a method to rank architectures in a predefined architecture search space. The method is simple and is composed of 3 steps: 1) train a simple classifier that provides some training or validation accuracy threshold; 2) train a given architecture from the search space until the threshold computed in step 1; 3) freeze most of the architecture and train some of the last layers/cells for a few more epochs. The authors compare their method to zero-cost proxies such as synflow, snip, etc., as well as to a baseline which is obtained by training architectures with a lower fidelity (less epochs, larger batch size, etc), which they dub *shortreg*. The majority of the experiments were conducted on the NATS-Bench tabular benchmark, where FEAR outperforms in general *shortreg* and the zero-cost proxies (with x200 more computation time cost).",
            "main_review": "**Structure and Clarity**\n\nI think the paper could be structured better. For instance,  the introduction and related work can be compressed to just 2 pages instead of 3.5 pages and more experiments can be added in Section 4. There are a lot of relevant results in the appendix which are just referred in the main paper, making it not fully self-contained. Section 5 totally refers to the appendix for the results. In this case, I would rather have that as a paragraph of Section 4 or move results from the appendix to Section 5. In general, the writing quality is good. The baselines and evaluation metrics are clearly presented. In Figure 3 it would be nice to label the x axis with the proper metric (seconds or minutes on average per architecture?).\n\n**Novelty and significance**\nI did find the proposed method simple and useful in general. However, considering that the contributions in terms of novelty are scarce, I think that the authors need to provide more empirical evaluations in order to assess the effectiveness of their method. Please find below some more concrete points:\n- First of all, I think only evaluating the proposed method on NATS-Bench is not enough. I would recommend the authors to look at the experimental set-up proposed by White et al. 2021, since it provides an evaluation framework for such methods and not only: it compares under some predefined settings (such as runtime or query time) learning curve predictors, model-based predictors, one-shot methods, etc. It would be useful especially to compare to model-based methods (e.g. XGBoost, GCNs, etc.) since their query time is low after training them and they can achieve strong predictive performance generally.\n- As done in White et al. 2021, I recommend evaluating FEAR on NB101 and NB301 too.\n- The paper by Ru et al. 2021 is mentioned in page 8, however I did not see a comparison to the method in that paper, i.e. to the Training Speed Estimator (TSE) that takes an exponential moving average on the training losses (TSE-EMA).\n- The authors mention in the abstract that they evaluated local search with FEAR, however I did not find that experiment throughout the paper. It would also be useful to evaluate FEAR inside a Bayesian Optimization or Evolutionary Strategy framework.\n\n**References**\n\nWhite et al., How Powerful are Performance Predictors in Neural Architecture Search?, NeurIPS 2021\n\nRu et al., Speedy performance estimation for neural architecture search, NeurIPS 2021\n\nYing et al., NAS-Bench-101: Towards Reproducible Neural Architecture Search, ICML 2019\n\nSiems et al. NAS-Bench-301 and the Case for Surrogate Benchmarks for Neural Architecture Search, 2021",
            "summary_of_the_review": "With the current state of the paper I recommend a rejection for the sole fact that the paper does not provide enough empirical evaluations considering the fact that the methodological contributions are scarce.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a simple but powerful method called FEAR for ranking architectures. FEAR firstly trains a fast but shallow learner to get a reasonable training/validation error threshold and then trains the architecture in a two-stage procedure. In stage 1, the candidate architecture is trained till threshold accuracy. And in stage 2, most of the architecture is frozen and the last few layers will be trained for a few epochs. The various experiment results illustrate the effect of their method.",
            "main_review": "Strengths:\nFEAR is based on the assumption that \"no architectures that train slowly but go on to achieve good final test accuracy\", which is consistent with the observation in Figure 2. Furthermore, FEAR is a simple approach and reduces the evaluation wall-clock time. The experiments on different datasets illustrate the effect of FEAR compared with some competing methods.\n\nWeakness:\n1. FEAR is so heuristic and there are so many hyperparameters in their methods. For instance, \"last few layers\" will be trained for \"a few epochs\". The vague definition will make the algorithm confused. Even though the authors have discussed these hyperparameters in the Ablation Study section, I still think it's a heuristic method that lacks theoretical support.\n2. In Section 4, the authors uniformly randomly sample 1000 architectures from the search space. I am curious about how many repeated experiments have been performed and the error bar of the results. Since FEAR is a heuristic method and experiment is much important, it's important to consider the randomness for clarification.\n3. As discussed in the paper, FEAR can be injected into some other NAS methods. Can you provide some empirical results about that? For example,  FEAR can be combined with EA and RL methods.",
            "summary_of_the_review": "FEAR is a heuristic NAS algorithm based on too many vague settings. Even though the ablation study illustrates the effect of different hyperparameters, it still lacks theoretical support.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}