{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "The authors proposed a new BP (backpropagation) basde feature attribution methodology called TR-GBP. The model is built on top of a recent theorem about estimating feature attribution. The authors claim the feature attributions generated by the proposed model are more sharp and accurate, will have bounded noise, won't provide accurate attributions when the model's paramteres are random, and can be used to indicate prediction errors in some cases. The authors provide through many experimental analysis to support their claims.   ",
            "main_review": "Pros:\n1. The idea of getting a predicatable feature attribution model is novel and has lots of benefits. Other than providing seemingly good explanations, a feature attribution model should make its users confident to use its results. In this paper, the authors leverage a recent theorem to show that their proposed model has several nice properties therefore the model has certain predictability.\n2. The experiments provide a fair amount of contexts for readers to evaluate the authors claims. And these results look good. \n\nPotential improvements\n1. It might be worth to discuss whether other GBP models will benefit from theorem 1 and proposition 1 and explain why. The experimental results show there are some differences between the proposed models and other models, and adding some discussions could be benefitial.\n2. It would provide more contexts if the authors can provide results from more testing examples in Figure 9 and 10.",
            "summary_of_the_review": "I think this paper presents an inspiring topic with solid contexts to the research community and is well written.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper focuses on feature attribution for vision models. Specifically, it proposes a modification of the GuidedBackpropagation (GBP) method. The paper first discusses shortcomings of existing methods, especially the lack of \"predictability\". After recapping the theoretical results about GBP, it extends it by treating bias terms as inputs and increasing the class sensitivity by subtracting the saliency maps from different classes. \n",
            "main_review": "I found the paper overall well written and, for most parts, easy to follow. The questions about the limitation of existing attribution methods and how they can be improved are interesting, especially the connection between theory and end-users. However, I find the work needs substantial improvements and many fronts -- I would doubt that \"fixing\" GBP is possible. My concerns are listed below. \n\n### What is Predictability?\n\nI am puzzled about what the authors understand under predictability. It sounds similar to forward simulation (Doshi-Velez & Kim, 2017 https://arxiv.org/pdf/1702.08608.pdf). And forward simulation would be aligned how predictability is defined in the introduction:\n\n>  users are able to predict the possible behaviors of a given cause\n\nwhich would also be in line with section 2.1:\n\n> Considering that the ultimate goal of model explanations is helping people understand and trust the target models, we focus on the attribution results that can be clearly perceived by humans and the status of the components which decide the decision processes. [...] \n\nHowever, section 2.1 (Scopes) ends with:\n\n> Therefore, predictability can be defined intuitively as the capabilities to predict the human-understandable interpretation behaviors of a given decision-making process.\n\nThis would mean that predictability is about how well researchers can predict how well humans can understand interpretations. This is not forward-prediction and would be in line with how section 2.1 (Measurements) finishes:\n\n>[...] so predictability can be shown as the capabilities of answering these\n>questions with theoretical analysis.\n\nSo is predictability about using theoretical analysis to predict human behavior (I will call this theory-simulation)? Or is it how well humans can predict the model using interpretations (forward-simulation). Unfortunate, the definition of \"predictability\" is not clear.\n\n### Theoretical limitations:\n\nThe theory part leaves many questions unanswered. Most importantly, what is the justification of the ReLU in backward pass? GBP has no theoretical motivation (Springenberg et al., 2014). (Nie et al., 2018) provided a theoretical analysis of its unwanted behaviors. However, why would it be a particularly good idea to use ReLU in the backward pass? If the goal is input reconstruction,  better methods such as GANs, VAEs, and INNs exists.\n\nEven if one would consider GBP a valid approach to achieve input reconstruction, it only happens for randomly initialized neural networks, as pointed out in (Nie et al., 2018). The paper fails to differentiate between random weights vs learned weights. It is not given that the inputs would be reconstructed as aimed by the authors when the weights are no longer independent.\n\n### Presentation Weaknesses\n\nThe presentation could be improved here and there. For example, the figure placement is suboptimal: Figure 1 & 2 are on the same page. I would also not explain the proof of theorem 1 in detail but rather discuss the theorem itself or use the space for a more extensive conclusion. \n\n### Requirements of Predictability\n\nThe authors discuss in section 2.2. Requirements of Predictability. For different methods, they discuss how they fail the requirements. However, I am not convinced that their own method would pass these requirements. \n\nFor example, they say \"Representation of local behaviors\" would be important. But in the evaluation, it is not mentioned how GBP or TR-GBP represent local behaviors. Reconstructing the input does not a represent the local behaviors as a reconstruction is static and behaviors should be about change.\n\nThey argue that an additional failure of existing work is:\n\n> Unstable to choices in other parameters (other inputs, baselines,\n> hyper-parameters, etc.)?\n\nHowever, in your method, the internal biases are upscaled and added. Which layers are picked to compute the biases and how are they weighted? Would this not also be an ambiguous parameter.\n\n>  Use of internal activations renders activation-based attributions black-boxes\n\nHere again, where is the difference between using internal activations and attributing to internal biases as proposed in section 4.1?\n\n### No User Study\n\nThe predictability concepts (independent whether forward-simulation or theory-simulation is meant) should be evaluated in a user study. Or at least the paper should comment on why it was not evaluated. In addition, the authors should cite different studies that evaluated saliency maps in user studies (Alqaraawi et al., 2020 https://dl.acm.org/doi/abs/10.1145/3377325.3377519; Adebayo et al., 2020 https://arxiv.org/abs/2011.05429).\n\n### Minor Comments\n\nGrammar:\n\n* which can use the Central Limit Theorem to obtain.\n",
            "summary_of_the_review": "The concept of \"predictability\" is claimed as a major contribution, yet a clear definition is not provided. Further, I find the paper does not provide a sound theoretical grounding. \nThe theoretical part fails to differentiate between learned and randomly initialized weights.  I would also like to see a human subject evaluation for claims about \"predictability\".  Therefore, I would recommend to reject this paper.\n\nThe novelty is limited as the method only suggests using differences and bilinear upsampling of the bias terms, already suggested by others for other attribution methods (Srinivas et Fleuret, 2019 https://arxiv.org/abs/1905.00780).  \n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This manuscript introduces a concept of predictability and claims that Guide Backpropagation (GBP) has a potential to be recognized as the state-of-the-art approach in showing desirable properties, i.e. lucidity and predictability, as a “good” explanation as long as the proposed fixes are applied to GBP. Therefore, the main contribution of this paper is an explanation approach, TR-GBP, based on GBP. The authors further demonstrate TR-GBP shows higher scores on several metrics used to evaluate attribution methods. ",
            "main_review": "My main review will cover the following four areas. Under each subsection I will first describe the strength, followed by the weakness and my concerns.  \n\n### Novelty\nThis paper is a follow-up work for a recent discovery that GBP might be an unfaithful explanation approach because it lacks sufficient sensitivity to the network’s parameters. The idea of solving this problem is novel and the proposed technique is somewhat new and aspects of contributions are motivated from the prior work. However, the motivation of this work is not well-explained and somewhat ambiguous to me, which prevents me from acknowledging the contributions made by the authors. I will elaborate. \n\n1. What is the difference between the introduced property for an attribution method, predictability, and the (in)fidelity [1] or faithfulness [2]? When a formal definition of the predictability (a paragraph surrounded by a Def environment) is missing, I am quoting the following sentence as the definition the authors use for the predictability: “Specifically, predictability means that users are able to predict the possible behaviors of a given cause”. Can the authors explain more about what are correct terms the reader should use to fill the placeholders, “behaviors” and “ a given cause”, in order to define the predictability for an attribution method. It seems to me that the authors attempt to mean that an attribution method has a property of predictability if by using the attributions humans can predict the model’s behavior. With that being said, predictability seems to be a relaxed version of (in)fidelity [1] or faithfulness [2] as these two properties also emphasize that the attributions  should describe the model’s local behaviors with reasonable sensitivity to the input. Without a formal (and potentially measurable) definition of predictability, it provides a hard time for the reader to understand why this property is important to study.\n\n2. How does predictability motivate the paper to study possible fixes for GBP? The transition from studying predictability to potential fixes to GBP is somewhat sudden to me because the logic the authors present is not clear to me. The authors seem to use the following logic: GBP is the go-to choice among explanation approaches because it enjoys lucidity and predictability so the rest of work should focus on fixing any possible problems in GBP -- namely not faithful to parameters. Firstly, properties of explanations do not seem to be equally important or mutually exclusive. That is, even if a method is showing less noisy visualizations (lucidity), when it is not faithful to the parameters (infidelity), it should not be considered as an explanation in the first place. Being less noisy or not seems to be totally insignificant for an unfaithful method because users are not expected to use it anyway. Secondly, the authors base the reconstruction theories to argue that GBP satisfies the notation of predictability but can the authors help me to understand why the reconstruction theory is the best measurement of predictability. This seems to go back to my previous bullet point. That is, without a clear and measurable definition of predictability, the argument that GBP is predictable does not stand firmly.\nThe authors argue that all work mentioned in Section 2.2 fail at predictability but do not intend to present either theoretical or empirical justifications. This makes me confusing that, if the authors aim to present a somewhat strong criticism like this, “but all fail at predictability”, it would be better to accompany the argument with evidence (please correct me if I miss to locate the relevant evidence). \n\n### Technical Quality \n\nI have several concerns regarding the technical quality of the proposed approach TR-GDP.\n\n1. Firstly, the assumptions why GBP fails to be faithful described in Section 4.1 are not validated either empirically or theoretically. The authors firstly blame the missing bias term in the construction of GBP. However, all gradient-based attributions, i.e. Integrated Gradient [3], do not consider bias. Therefore, why are these approaches faithful? On the other hand, a lot of backbone networks trained for computer vision tasks do not use bias terms in practice. Does this mean that for a bias-free network, GBP is faithful. If so, why and how does the architecture of the network impact the faithfulness of GBP, or potentially all gradient-based attributions are missing in the current manuscript. In summary, without first justifying the assumptions the authors made about GBP are the relevant causes for the unfaithful results GBP presents, all techniques presented in the paper can fix problems that may not exist. I would suggest presenting more evidence about the bias and class similarities before rushing to the fix, which by itself can be a significant contribution to the community. \n\n2. Secondly, using the subscration between output scores, (b) Removing similarities shared by different classes, is not first proposed in this paper. For example, the relative quantity of interest in Leino et al. (2018) [2] have discussed the similar concept.\n\n3. The reasons why the current baseline approaches are chosen over others are not convincing. The authors argue that \"These methods are the newest or the most classical attribution methods of three kinds of attributions listed in Section 2.2\". It is counter-intuitive that the choices of baselines are not the best approaches in the lucidity and the fidelity competitions in the prior work instead of \"classical\" or \"new\" when the authors aim to promote the use of TR-GBP over all other approaches.  \n\n### Clarity\n\nThe paper has spent a lot of space discussing the related work, which helps the reader to understand what the paper is trying to solve. However, several parts are confusing to me. Firstly, what are the purposes to show the proof of Theorem 1 in such a detailed way when neight theorem 1 nor proposition 1 is a contribution of this paper. What are the purposes to show Figure 2b? It seems to be safe to me to assume all readers in this venue have a ML-related background to know what convolution is before reading this paper. Besides, \n\n[1] Yeh, Chih-Kuan, et al. \"On the (in) fidelity and sensitivity of explanations.\" Advances in Neural Information Processing Systems 32 (2019): 10967-10978.\n\n[2] K. Leino, S. Sen, A. Datta, M. Fredrikson and L. Li, \"Influence-Directed Explanations for Deep Convolutional Networks,\" 2018 IEEE International Test Conference (ITC), 2018, pp. 1-8, doi: 10.1109/TEST.2018.8624792.\n\n[3] Sundararajan, Mukund, Ankur Taly, and Qiqi Yan. \"Axiomatic attribution for deep networks.\" International Conference on Machine Learning. PMLR, 2017.\n",
            "summary_of_the_review": "Overall I incline to a rejection. Even though I believe this paper's focus is quite interesting and the results can be significant to the community to understand why GBP is not faithful and how should we fix it, the current version of the manuscript has a lot of space for the improvement and is not ready for publication in this venue. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}