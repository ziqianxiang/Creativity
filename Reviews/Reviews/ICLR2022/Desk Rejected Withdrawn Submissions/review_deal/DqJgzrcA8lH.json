{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes an algorithm to solve the individual fairness problem by employing Glow generative models (for similarity measures) and randomized smoothing (for certificated fairness). This paper first constructs \"similar identities\" from the Glow latent space; moving the input image towards the target attribute direction in the Glow latent space. Then, the LASSI encoder is trained by (1) classification loss (2) \"adversarial loss\" -- computing the maximum distance within the similar identity set (3) contrastive loss -- enforcing pairs with different labels to have a large margin. Finally, the certificated classifier is trained by the randomized smoothing technique upon the LASSI encoder latent space. Experimental results are shown in CelebA dataset, where sensitive attributes are \"pale skin\" and \"young\" while the target task is \"smiling\". The experimental results show that the proposed LASSI approach shows better certificated fairness compared to the vanilla counterpart.",
            "main_review": "## Strengths\n\n- (+) Although each component of the proposed method is well-known and not very novel, I think it is novel to bring the combination of a bijective function (Glow), a certificated robustness technique  (randomized smoothing) to individual fairness. Especially, the proposed algorithm works well on high-dimensional inputs, while the previous methods are not available to scale up to input dimensionality.\n- (+) The way to define the set of individuals similar to \"x\" is promising (as Glow paper already observed it works well in practice)\n- (+) The proposed \"adversarial loss\" is sound and practically works well.\n\n## Weaknesses\n\n### Lack of discussions on hyperparameters and experimental settings\n\nThe proposed method needs a lot of hyperparameters. $\\lambda_1$ (adversarial loss weight), $\\lambda_2$ (contrastive loss weight), $k$ (number of samples for \"adversarial training\"), $\\delta$ (hard margin for contrastive loss), $\\epsilon$ (maximum perturbation level for adversarial training), $\\sigma_{cls}$ and $\\sigma_{enc}$ (hyperparameter for randomized smoothing), $\\alpha_c$ and $\\alpha_s$ (hyperparameter for randomized smoothing). Furthermore, the proposed method needs many additional modules including: Glow encoder ($E_G$), Glow decoder ($D_G$), LASSI encoder ($E_L$), auxiliary classifier ($C_L^{aux}$) and the smoothed classifier $C_L$. Each of them needs hyperparameters, e.g., the Glow model needs to set the number of flows $K$. Indeed, there will be additional optimization hyperparameters, such as initial learning rate, learning rate schedule, and batch size.\n\nI cannot find a detailed hyperparameter search protocol in this paper. \"Experimental Setup\" in section 5 partially covers my concerns, but I think the current text is not enough to resolve my concerns. For example, why $\\epsilon$ is set to 1? Since the latent space is not a bounded space as the pixel space (which is bounded in [0, 255]), setting $\\epsilon$ is not trivial. Similarly, it seems that other hyperparameters could be very sensitive to the final performance. I wonder how the hyperparameters are chosen because if the method is very sensitive to hyperparameters, the method could not be easily extended to more challenging and large-scale tasks.\n\nI also wonder why this paper only chose \"Pale_Skin\" and \"Young\" as the sensitive attributes as well as \"Smiling\" as the target (Similarly, there is no discussion why \"orientation\" is chosen for 3D Shape experiments). There are 40 attributes in CelebA, and some other fairness studies use more attributes. For example, Kim et al. \"Counterfactual fairness with disentangled causal effect variational autoencoder\" (cited in the paper) selected multiple attributes for their tasks: \"Mustache\" as the protected attribute and \"Wearing Lipstick\", \"Mouth Slightly Open\", \"Smiling\", \"Male\", \"Eyeglasses\", \"Bald\", \"Narrow Eyes\" and \"Young\" are the target attributes. Quadrianto et al. \"Discovering fair representations in the data domain\" selected gender as the sensitive attribute and attractive as the target variable. I think the reason why pale skin, young and smiling are chosen among 40 attributes should be discussed in the paper. Also, if it is possible, I would like to suggest testing more attributes with LASSI. Especially, I wonder whether the Glow model can capture sensitive attributes such as gender (CelebA has \"male\" attribute), complex attributes such as \"Eyeglasses\", or sparse and inexact attributes such as \"attractive\" or \"Wearing Lipstick\".\n\n### The paper is somewhat hard to follow; this paper is not self-contained\n\nTo me, this paper is somewhat hard to follow. The proposed method contains many big components (previous works), such as Glow, center smoothing, and randomized smoothing. Although this paper contains the background section (Section 3), it is difficult to understand how the method works without reading external articles. For example, this paper uses the terminologies such as \"data consumer\" and \"data producer\". I found these terminologies from the previous paper (LCIFR, Ruoss et al.), but as far as I understood, these terminologies are not necessary to explain the proposed framework. These terminologies keep confusing me while reading the paper.\n\nAlso, I feel descriptions of the randomized smoothing and the center smoothing are not enough. Especially, I cannot understand how center smoothing and randomized smoothing are different (it is briefly described in Section 3, but it is confusing to me). Note that center smoothing is a very new technique (with 0 citations yet), so I think the explanation of center smoothing is necessary to understand the method well, especially the proposed method certifies modules by using the difference of certificated radius obtained by center smoothing and randomized smoothing.\n\n### Theorem 4.1\n\nIn my opinion, Theorem 4.1 is hardly considered a technically correct theorem. There is no underlying assumption for the theorem as well as detailed proofs. Because the theoretically guaranteed fairness (certificated fairness) is one of the key contributions of this paper (\"However, unlike our work, these methods only empirically test for bias and do not provide certification guarantees.\" -- Bias in high-dimensional data Section 2), I think this theorem should need more detailed proofs. If it is impossible to provide technical proof to the theorem, I think theorem 4.1 should be removed from the paper. As an alternative, I suggest changing the main claim weaker.\n\n### More baselines?\n\nAs a novice reader on individual fairness, I wonder why there are no other baselines in Table 2. For example, as far as the reviewer understood, \"Baseline\" in Table 2 is not a simple cross-entropy classifier, but it has the same pipeline as other rows, i.e., Image -> Glow encoder -> Glow decoder -> LASSI encoder -> classifier (\"The inputs of CL here are the outputs of EL ◦ DG ◦ EG\" Training $C_L$ in Section 4). I wonder why there are no other baseline methods, such as \"cross-entropy only\" baseline, or other methods which cannot provide a certification guarantee (described in \"Bias in high-dimensional data\").\n\n### Questions\n\n**Why the adversarial training is done by sampling?** I wonder why the inner loop of $L_adv$ is computed by sampling. For example, since $S_l(x)$ is a well-defined line segment, one can apply multiple gradient updates as Madry et al. I wonder why the sampling method is chosen and how the method is sensitive to the sampling parameter $k$.\n\n**Will LASSI work well on low-dimensional datasets such as tabular datasets?** If it does, I would like to see comparisons of LASSI and previous individual fairness methods, such as LCIFR. In my opinion, if LASSI does not work well as much as LCIFR in the low dimensional inputs, this paper should include the discussion why LASSI does not perform well in the low-dimensional dataset (e.g., fitting Glow is not trivial to a low dimensional tabular dataset; thus LASSI's practical usage can be limited if Glow is not fitted well to the target dataset). I think this is not a case of rejection, but it should be discussed as its limitation.\n\n### Minor comments\n\nNot a major comment, but the notations are confused to me.\n\n- the module notations are confusing $E_G$, $D_G$, $E_L$, $C_L$, ... I read through the paper multiple times since it is hard to distinguish $E_G$ and $E_L$ at first glance. I would suggest using a single capital character for each module.\n- $x_i$ is not defined (in contrastive loss)\n- $\\widehat{E_L \\cdot D_G}$ is very hard to understand at a glance. Please consider to define smoothed function in a different way, e.g., $\\text{smooth}(f)$. Similarly, I would recommend replacing repeated notations such as $E_L \\cdot D_G$ with a short notation.",
            "summary_of_the_review": "This paper proposes a novel combination of Glow, center smoothing, and randomized smoothing for individual fairness. The proposed method performs well in high-dimensional inputs, while previous individual fairness with a certificated guarantee cannot be extended to large-scale inputs. However, the main technical contribution of this paper (except the combination) is somewhat unclear to me.\n\n- The proposed method needs a lot of hyperparameters (which is not discussed in detail)\n    - I would like to see a detailed hyperparameter search protocol used for the experiments. If there is a specific reason why the hyperparameters are set to the proposed values, I suggest adding discussions to the revised paper. Also, I would like to see the results by changing the key hyperparameters, such as $\\lambda$s, $k$, $\\epsilon$, $\\delta$, $\\sigma$s, and $\\alpha$s.\n- While there are 40 attributes in CelebA, this paper only shows results with \"pale skin\", \"young\" (sensitive attributes), and \"smiling\" (target task) without enough discussion of why the attributes are chosen. The reason why \"orientation\" is chosen for Shape 3D is not discussed as well.\n    - I would like to see more experiments on different combinations of attributes, especially I wonder whether the Glow model used in this paper can capture other attributes as well.\n- Center smoothing and randomized smoothing are not described well in the paper. Please provide a more detailed explanation of them, particularly center smoothing (in the appendix if there is not enough space).\n- To me, it is hard to say that Theorem 4.1 is a technically correct theorem. Because Theorem 4.1 is the key explanation of why and how Algorithm 1 works, I think this theorem needs concrete statements, assumptions, and proofs.\n    - Please provide a technically correct statement and proofs, not an informal theorem.\n- (Minor) I wonder why there are no other baselines. For example, \"baseline\" is not a simple cross-entropy baseline but uses the Glow model as well as LASSI. If it is possible, I would like to see more baseline results (e.g., methods described in \"Bias in high-dimensional data\")\n    - If the methods are not comparable in high-dimensional inputs, I wonder whether LASSI works well in low-dimensional datasets. In this case, as far as the reviewer understood, LASSI can be compared with LCIFR and other previous methods.\n\nOverall, I feel this paper is a borderline paper. Hence I recommend \"marginally below the acceptance threshold\" for my initial recommendation.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents an input similarity metric for high-dimensional data and a representation learning method with individual fairness certification. It aims to learn individual fair representations for real-world high-dimensional data. To be specific, the authors use the generative model Glow (Kingma & Dhariwal, 2018) to encode images into latent spaces and get the attribute vector by subtracting the positive features and negative features. Similar samples with different sensitive attributes can be generated by interpolating along the attribute vector. After that, they apply adversarial training on the interpolated feature space to ensure fair representations. Classification loss and Contrastive loss are also added to guarantee the validity of the learned fair representations. At last, center smoothing (Kumar & Goldstein, 2021) and randomized smoothing (Cohen et al., 2019) are applied to certify individual fairness.",
            "main_review": "Strength:\n1. As mentioned by this paper, the key challenge in individually fair representation learning is to scale it to high-dimensional data and real-world models. This paper firstly proposes a representation learning method for certifying individual fairness of high-dimensional data.\n2. According to the experiments on CelebA and 3D Shape dataset, the proposed method can successfully increase the certified individual fairness.\n\nWeakness:\n1. In my perspective, this paper over-claim its effectiveness on high-dimensional real-world data. As the CelebA and 3D Shape are not as diverse as real-world datasets like imagenet or MSCOCO. Specifically, this paper use normalizing flow (Glow) to generate latent space of attributes, yet, the normalizing flow itself is known to suffer from high-dimensional input data, e.g. 224x224. Besides, 3D Shape is not a real-world dataset, so the experiments of the certification with disentangled ground truth data cannot prove the effectiveness of real-world data. Therefore, the overall method might be limited to a general real-world high-dimensional dataset like ImageNet.\n\n2. The CelebA dataset has attribute labels. Therefore, supervised disentanglement methods can also reduce the high-dimensional pixel-level input into low-dimensional disentangled latent representations, where the conventional tabular methods can be applied. However, the authors neglect such works. It can be clearly seen from the Shape3D dataset with full attribute annotation, which guarantees disentangled representation, that the proposed method can achieve 100% accuracy. This again demonstrates that a full disentanglement feature can also achieve fairness.\n3. The proposed representation learning method is not self-contained, because its effectiveness is only built upon the quality of the given Glow model, which itself might be flawed. \n4. The certification of individual fairness in Section 4.3 is directly modified from the center smoothing (Kumar & Goldstein, 2021) and randomized smoothing (Cohen et al., 2019). The proposed algorithm 1 just simply concatenates the above two algorithms. Such a simple combination is trivial in theory and fails to provide any inspiration for later researches.\n5. The design of experiments is also far from convincing. Only 2 attributes in CelebA and 1 orientation in 3D shape are studied, which cannot prove the general effectiveness in the real-world high-dimensional dataset. Such a selective report can easily mislead the audience. Besides, the ablation study is also too simple, which cannot provide any deep understanding of the proposed algorithm.",
            "summary_of_the_review": "The study of individual fairness for the high-dimensional data is important, yet, the proposed algorithm simply combines the existing generative model Glow and other two certification algorithms, which is not novel in theory. Moreover, this paper only selectively reported 2 attributes in CeleA and 1 attribute in 3D shape in experiments, which is not convincing. ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n.a.",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes LASSI for achieving individual fairness with fairness certification. Based on Glow, the method obtains the set of similar individuals to the input image by editing along the latent direction that represents the protected attribute. To achieve individual fairness, the authors use adversarial loss and contrastive loss to encourage similar individuals to share similar representations in encoder $E_L$. Finally, the authors present the certification algorithm (Alg. 1) to certify individual fairness. The experiments conducted on CelebA and 3D Shapes dataset demonstrate the effectiveness of the proposed method.",
            "main_review": "Strengths:\n1. The proposed method using Glow to obtain the similarity metric for individual fairness on high-dimensional data is novel.\n2. The paper is well-written and easy to follow.\n3. The proposed method can further facilitate fairness certification, which is novel since previous methods all empirically test the biases.\n\n\nWeaknesses:\n1. In Sec. 4.1, the authors use the difference of averaged latent vectors to obtain the attribute direction $\\mathbf{a}$. Is this method stable enough when the data is imbalanced in terms of the protected attribute? For example, if the dark-skinned face has much fewer samples compared with the light-skinned ones in the data, wouldn’t it make the mean of dark-skinned data to be inaccurate? Why not follow the method in [7] or (Balakrishnan et al. 2020) to rebalance the data and train an SVM in the latent space to obtain $\\mathbf{a}$?\n2. In the experiment on CelebA, can authors explain why choose “Smiling” as the target attribute? As analyzed in [8] and (Ramaswamy et al., 2021), the classifier will take the shortcut of the protected attribute to make prediction only when the protected attribute is easier than the target attribute. (Ramaswamy et al., 2021) analyzed that the “Smiling” attribute is easier than the “Young” attribute. Therefore, I do not think choosing “Smiling” as the target attribute and using the “Young” attribute is appropriate. \n3. The experiment on 3D shapes is not extensive enough to support the results. I encourage the authors to report the results of each pair of attributes in 3D shape, serving as protected and target attributes. Otherwise, could the authors give an explanation why choose Orientation and Object_Hue specifically?\n4. In the experiment, there are no comparison methods except the baseline. Can the authors add the experiment to compare with (Ramaswamy et al., 2021), which is closely related to this work because it also uses generative models to mitigate biases? \n\nDetailed comments:\n\nIn Subsec. “Bias in high-dimensional data” of Sec. 2 and Subsec. “Glow” of Sec. 3, I encourage the authors to also cite some recent works on the counterfactual explanation and fairness problems based on generative models, which are also closely related to the topic of the submission.\n\n\n[1]S. Liu, B. Kailkhura, D. Loveland, and Y. Han, “Generative Counterfactual Introspection for Explainable Deep Learning,” in IEEE Global Conference on Signal and Information Processing (GlobalSIP), Nov. 2019, pp. 1–5. doi: 10.1109/GlobalSIP45357.2019.8969491.\n\n[2]M. O’Shaughnessy, G. Canal, M. Connor, M. Davenport, and C. Rozell, “Generative causal explanations of black-box classifiers,” in NeurIPS, 2020\n\n[3]S. Singla, B. Pollack, J. Chen, and K. Batmanghelich, “Explanation by Progressive Exaggeration,” in International Conference on Learning Representations, 2020.\n\n[4]O. Lang et al., “Explaining in Style: Training a GAN to explain a classifier in StyleSpace,” in ICCV, 2021.\n\n[5]Z. Li and C. Xu, “Discover the Unknown Biased Attribute of an Image Classifier,” ICCV, 2021.\n\n[6]R. Luss et al., “Leveraging Latent Features for Local Explanations,” in Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, New York, NY, USA, Aug. 2021, pp. 1139–1149. doi: 10.1145/3447548.3467265.\n\n[7]Y. Shen, J. Gu, X. Tang, and B. Zhou, “Interpreting the Latent Space of GANs for Semantic Face Editing,” in CVPR, 2020.\n\n[8]J. Nam, H. Cha, S. Ahn, J. Lee, and J. Shin, “Learning from Failure: Training Debiased Classiﬁer from Biased Classiﬁer,” in NeurIPS, 2020.",
            "summary_of_the_review": "Although the proposed method is novel, the experimental part is still weak, e.g., no comparison methods, problems of experimental setting in choosing protected and target attributes. Therefore, I recommend “marginally below the acceptance threshold” to this paper. I am willing to increase my rating if the authors can address my concerns.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This manuscript proposes a method, LASSI, to certify individual fairness through learning the representations. The main idea is to combine existing robustness certification methods, e.g., randomized smoothing, over the feature space learned from a generative model through normalizing flows. The authors also performed experiments over the CelebA dataset to verify the effectiveness of the proposed method. \n",
            "main_review": "Overall the paper is clear and easy to follow. The proposed model, however, is quite complicated in my opinion. Essentially, the overall model contains four parts: encoder/decoder of the normalizing flow (GLOW), the feature encoder, and the classifier. The objective function amounts to a combination of three losses, e.g., the classification loss, the adversarial loss for feature encoder, as well as the contrastive loss. \n\nMy main concern of the current paper is that, it lacks an explicit definition of the metric used to measure the similarity of individuals, and the current proposal of using the direction from the encoding of a GLOW model seems quite arbitrary and unjustfiied. As the authors have already noticed, for individual fairness, the most important part in problem formulation lies in the choice of the metric to measure the similarities. While I understand that for many practical applications this is often complicated and may not be simplified using a mathematical definition, but at least for the datasets used in this manuscript (CelebA), it has already provided annotations of these sensitive attributes, so why not just using these annotations to define the metric? In other words, it is not clear to me why the difference of the mean vectors from two groups in the encoding space of GLOW is a good representative of the sensitive attribute? Why not the difference of the medians? Or the barycenters? Furthermore, due to the potential correlation between the sensitive attributes and other attributes in the images, the representative of the sensitive attribute could instead be a subspace, not a line. The current manuscript completely misses a justification for this design choice. In my opinion, this justification is very important, as the definition of individual fairness basically is determined by the metric. \n\nBesides the above concern, technically, I feel the current manuscript is too incremental compared to [1]. For example, on the CelebA dataset, since the annotations of the sensitive attributes are given, we can just apply the logic rule in [1] to define the similarity metric. The central idea of using certified robustness methods for individual fairness has already been observed and developed in [1-3]. The main novel component of this paper lies in the use of a GLOW model to first encode the images. However, GLOW is also an existing technique. \n\n[1].    Learning certified individually fair representations\n\n[2].    SENSEI: SENSITIVE SET INVARIANCE FOR ENFORCING INDIVIDUAL FAIRNESS\n\n[3].    Training individually fair ML models with sensitive subspace robustness\n\nEmpirically, the current paper does not compare to any of the existing methods in the literature for individual fairness, e.g. [2-3]. Because of this, I am not sure how effective the proposed method is. \n\nSome follow-up questions:\n-   Since the GLOW model already provides a feature space given by $E_G$, why do we need to further use $E_L$ to find an encoding after the decoder $D_G$? To ensure similarity of similar individuals, couldn't we just ensure that $E_G$ is both bijective and Lipschitz smooth? It seems to me $E_L$ is redundant here. \n\n-   The introduction of the contrastive loss is a little bit unnatural to me. Why do we need this? \n",
            "summary_of_the_review": "An important justification on the design of the similarity metric is missing. In my opinion this is probably the most important part in the problem formulation about individual fairness. On the technical level, the proposed method could be viewed as a combination of several components, i.e., GLOW, randomized smoothing, center smoothing, etc. The empirical comparison is lacking -- there is no comparison to existing methods for individual fairness using representations. \n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}