{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes scheme regularizing the latent space of auto-encoder. It proposes to partition the latent space using a set of prototypes where each prototype correspond to the mean of a gaussian with unit variance. During training, the encoder features are associated to prototypes using the Sinkhorn algorithms. The representations are then constrained by adding a maximum discrepancy penalty from the prototype distribution.\n\nPaper empirically validates its ideas on the MNIST and CIFAR10 and CIFAR100 datasets. On MNIST and CIFAR-10, the authors qualitatively show that their approach can learn meaningful prototypes.  Authors also show that their model obtain better reconstruction loss than a regular VAE or InfoVAE. It also shows few samples on the MNIST dataset.\n",
            "main_review": "The paper proposes a way of regularizing the latent representation of an auto-encoder. This is a critical problem of unsupervised learning as limiting the information content is necessary to learn ‘useful’ representation that does not just copy the input. The paper\n proposes a simple regularization scheme based on prototypes.\n\nWhile the specific prototype-based regularization appears novel, the idea is similar to VQ-VAE which also partitions the latent space using a set of prototypes. However, no direct comparison with VQ-VAE is provided in the empirical section. It is therefore unclear what is the current advantage of the proposed scheme over VQ-VAE.\n\nAdditionally, the empirical section mostly focuses on reconstruction for CIFAR. Reconstruction alone does not demonstrate that the representation learned is meaningful, an auto-encoder without any bottleneck can obtain perfect reconstruction. How does the model perform when generating novel images or on some downstream tasks such as classification?\n\nFinally, the empirical section focuses on small datasets and architecture with limited complexity. It is unclear if the approach would scale to larger dataset or more complex architecture.\n",
            "summary_of_the_review": "While the approach is sensible, the paper in its current form has some major limitations:\n\n1)\tNo empirical evaluation with VQ-VAE despite the similarity in the approach. It is not clear why the proposed regularization has some advantage over VQ-VAE currently.\n2)\tThe paper mostly focuses on reconstruction and provide no quantitative results for generation. \n3)\tThe empirical evaluation mostly focuses on simple dataset (MNIST/CIFAR). It is unclear if the approach would scale to more complex dataset such as imagenet.\n\n",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes the use of prototypical (exemplar) latent vectors for VAEs, shows that the method improves reconstruction quality (in MNIST and CIFAR-10) and the computed prototypes may help with interpretation of models. The prototypes are created with a method based on Optimal Transport.",
            "main_review": "Strengths:\n+ This is a solid VAE extension paper that apparently shows improved reconstruction quality in MNIST and CIFAR-10\n+ The prototypes could help in interpreting the model's internals\n+ The quantitative comparison shows reconstruction (MSE) better in MNIST, CIFAR10 and CIFAR100, in comparison to e.g. InfoVAE and regular VAE. (Though it was not clear to me exactly what VAE-KL is?)\n+ The Optimal Transport -based computation of prototypes in this context seems novel to me.\n+ t-SNE results seem to confirm good clustering behaviour of the prototypes.\n\nWeaknesses:\n- Similar ideas have been explored in literature before, and e.g. [1] seems to make more extensive use of exemplars, shows results in higher resolution, and it is not cited here. I would kindly ask the authors to explain the relative pros and cons.\n- While the use of OT for computing the exemplars is interesting, it is not surprising that the use of exemplars improves reconstruction results. My worry is that this comes at a steep price in complexity. It is not clear how the solution scales beyond very low-resolution datasets (up to 32x32 here).\n- The need for a 'warmup' period adds to my concerns about the generalizability and scalability of the approach. What happens if there is no warmup?\n- It is not clear how the optimal number of prototypes would be determined in the general case, except with trial and error?\n- As a personal opinion, I do not find VAE experiments in low resolutions (32x32) sufficient in 2021. Experimenting with 64x64 e.g. with CelebA datase is often done in VAE variants, and should certainly be computationally feasible. Working in a bit higher resolution may expose weaknesses not present in the lower resolutions.\n- The only quantitative metric used is reconstruction error. I find this insufficient. In VAE literature, some papers extend this with likelihood measures, and some with FID metrics etc. Either direction (and others) can be justified, but relying on reconstruction quality alone as a quantitative metric, on low-resolution images, is just too narrow.\n- Minor: The use of MMD reminds me of the MMD variant of Wasserstein Autoencoder, which the authors do cite. I understand there are differences, but this would make a good baseline comparison, especially since I think their solution is open-sourced. I'd also request the authors to state the differences between the two.\n\n[1] Norouzi et al., Exemplar VAE: Linking Generative Models, Nearest Neighbor Retrieval, and Data Augmentation. NeurIPS, 2020.",
            "summary_of_the_review": "The paper is a solid introduction of a new VAE variant which might perform better than other similar variants.\n\nHowever, I find the experimental support too narrow, and have concerns about scalability of the method as well as insufficient comparisons to similar approaches.\n\nI am thus inclined to reject, and recommend the authors to consider expanding the paper with more experiments that at least measure the coverage in the latent space, not only reconstruction error, and re-submit.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed a new class of autoencoders regularized by the maximum mean discrepancy between encoder representation and trainable prototypes. The prototype assignment is also regularized by cross entropy of an equipartition-constrained assignment implemented by the Sinkhorn-Knopp algorithm for the optimal transport. The experimental results shows the improved reconstruction quality of the proposed method.",
            "main_review": "Strengths\n1. The proposed idea that regularizes the latent space by trainable prototypes is intuitive, and is effective for learning class-concept for unlabeled data.\n\n2. The combination of existing techniques such as MMD regularization and the optimal transport is well-motivated.\n\n3. The paper is well-written, and easy to follow.\n\nWeakness\n1. The experiment is not conducted quantitatively. Although it is very important to check the sample quality of deep generative models by human eyes, it is also necessary to quantitatively evaluate them using commonly used metric such as Inception (https://arxiv.org/pdf/1801.01973.pdf) and FID score (https://papers.nips.cc/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf).\n\n2. The most attractive feature of likelihood-based models such as VAEs is we can estimate likelihood of data through the trained model. This paper does not provide any guideline to leverage this feature using the proposed model (if it is not possible, it should be discussed in the paper).",
            "summary_of_the_review": "This paper is well-motivated and provide a nice combination of existing techniques for the VAE literature, however, it does not contain proper experiments for the publication.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}