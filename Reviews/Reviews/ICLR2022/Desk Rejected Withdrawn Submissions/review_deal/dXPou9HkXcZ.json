{
    "Decision": "",
    "Reviews": [
        {
            "summary_of_the_paper": "From frontal video of people walking, this paper estimates the following gait parameters:\n\nCadence\nStep length\nWalking velocity \nDouble Stance \nTime Single Support Time\n\nA transformer is trained on tracked joint positions and trained with ground truth data to learn these gait parameters. The results show decent correlation to fair to good truth. ",
            "main_review": "The main strengths are:\n\n1. The method is relatively simple, building on an existing tracker and adding a transformer to learn the parameters. \n2. The results are fair to good.\n\nThe weaknesses are:\n\n1. The paper lack many details, such as enough description of the transformer to reproduce it. There is no description of the test set used in Table 2, or how the dataset was broken into a development  / validation / test set. If the results in Figure 2 use the dataset as the test set then these results are invalid. \n2. The paper should give the gait parameters accuracy requirements for use in clinical applications. The standard error of the ground truth should also be given.\n3. There is missing discussion such as why only frontal video is used (side video should be more accurate for these parameters), and why such low resolution 480x720 Is used.\n4. The reviewed work is insufficient. For example https://www.nature.com/articles/s41598-021-93530-z appears to have better results for gait parameter estimation. \n\n",
            "summary_of_the_review": "The main strength of the paper, using transforms and existing tracking methods, are outweighed by an incomplete description of the method and review of related work.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a deep learning-based human pose estimation model that can be used for gait analysis. Pre-trained deep learning models are used to infer the 3D joint locations from monocular videos. Then transformer models are trained to measure gait kinematics from the 3D joint locations. The generalization performance of their proposed approach is measured on a publically available dataset.\n",
            "main_review": "Strength: state of the art deep learning transformer model is built to measure gait kinematics. \nA unique dataset including instrumented gait analysis from 770 subjects with cerebral palsy, spina bifida, stroke, traumatic brain injury, spinal cord injury, amputation, and abnormality of gait. \n\nWeakness: It is not clear how the transformer model outputs gait kinematics (also mentioned gait timing in this paper). The results of the proposed approach are not compared with other baseline models in the literature that are commonly used for gait and human pose estimation, eg. CNN, LSTM. It is also unclear how the transformer model produces interpretable features and also how the sequences of 3d joint position are tokenized before being fed into the model. The model is trained on the gait of individuals diagnosed with cerebral palsy, spina bifida, stroke, traumatic brain injury, spinal cord injury, amputation, and abnormality of gait. So, there are a lot of gait variations in the dataset that should be addressed or at least discussed as a limitation. ",
            "summary_of_the_review": "The paper should explain very clearly how the input sequence data (3d joints) is tokenized and embedded with MLP. It should also explain how the transformer outputs gait kinematics. How the attention mechanism helps with the measurement of gait analysis is not explained here. The results lack comparison with other sequence-based NN models. Transformer models are very huge and require large amounts of data and it is not clear in this paper if it was fine-tuned or trained from scratch. The size of illustrations should be increased.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "In this submission, authors set out to tackle the problem of  clinical gait analysis from monocular videos. The problem is interesting and could have high impact on the clinical setting. Authors described that gait analysis is a crucial part for patients with disabilities. However, recent works on 3D human pose have focused more on absolute errors in joint positions, but not on higher level biomechanical measurements such as step length and cycles. If machine learning can be used to compute these higher level metrics, it could make gait analysis easier and more accessible to more people. \n\nThe main contributions from the paper include:\n* A new dataset captured in a clinical setting for gait analysis. This dataset contains information about the person's spatiotemporal gait parameters, which is novel compared to other existing human pose datasets. This dataset also includes people with disabilities, or requires assistance in walking, which is also a novel contribution. \n* A method to predict spatiotemporal gait parameters from monocular videos which achieves much higher accuracy than using 3D joint locations alone. \n",
            "main_review": "Strength:\n* The new dataset could be very useful for future research in gait analysis. I also applaud the authors for taking extreme care when creating the dataset (e.g. consult with IRB), and being considerate and inclusive in selecting participants (e.g. including patients who require assistance in walking). \n* The new model architecture achieves higher accuracy than existing method. \n\nWeakness:\n* The approach lacks novelty. While this might be the first work which applies the transformer architecture to the problem of gait analysis, there is very little novelty in the model architecture. \n* There is not enough analysis on the model's success and failure modes. While the model does produce better results than baseline methods, there is not enough ablation studies and results on the model choices. For example, authors claimed \"we augment the keypoints by applying a random rotation to the entire 3D keypoint sequence, with 50% probability.\" (Chapter 4). However it is not clear why the model could not be made viewpoint invariant, why the data augmentation is designed as such (versus for example jittering the 3D keypoint locations), and why \"Lesioning the rotational augmentation worsened the generalization over views.\" for Human 3.6M dataset. I suggest authors to analyze these algorithmic choices more closely, and include results in revision.\n* The algorithms authors used for comparison are not sufficient. While authors did include comparisons based on a recent SOTA 3D human pose prediction model, and included a variant on using a MLP on top of the 3D pose prediction, I would recommend authors to also consider optimization algorithms based on kinematic constraints and spatiotemporal coherency as baselines to compare the transformers against. ",
            "summary_of_the_review": "Overall, I like that this submission brings the attention to an overlooked problem domain where machine learning can significantly be beneficial. However I find that the methodology used lacks novelty and the submission does not contain analysis or insights which could influence the community at large. I recommend a reject from the conference, but would encourage the authors to revise the submission and consider resubmission to a different venue. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors present a gait analysis pipeline for monocular videos. Existing algorithms have been evaluated for their performance for measuering kinematics. In order to improve the accuray of existing algorithms e.g. joint angles,  a transformer was trained to map 3D joint location sequences onto the output within a gait cycle. These new features are shown to allow accurate cycle to cycle estimates of common gait parameters. The code and the model will be made available. A main contribution is the application of transformers to gait analysis.",
            "main_review": "This paper is very well structured and clearly written. The high number of monocular videos used for the training is remarkable, it seems that they are not available for download. Aspects of fairness and transparency of the proposed approach are introduced, but this should be further explained. The kinematic outputs from the transformer model are interpretable, how does that contribute to better fairness for the given application. The gait parameters are computed from trajectories and this implies better explainability. How is that information used or what is the benefit? \nThe dataset contains people with age from 2 to 80 with  variouis abnormalities of gait. How can these abnormalities be described? Is that also the definition of clinical population? How representative is the recorded scenario for the daily practice of a clinic? Making the data public is highly recommended (not possible for privacy reasons?)\n\nRelated work does not describe other approaches for gait analysis like floor sensors or wearable sensors. More important then that are markerless approaches based depth data. See gait analysis from 3d Sensing like Kinect or others. A comparision with such an approach is valuabel to better understand the given approach.\nHow does the inital tracking influence the output? The frontal views are augmented for better generalization, how was the probability of 50% chosen? \nThe accuracy of the kinematic trajectories is described by a comparison: the transformer prediction was \"much more \" accurate and \"less noisy\": What does that mean, was that calculated?",
            "summary_of_the_review": "The paper describes an interesting application of a transformer model to get more accurate gait analysis. The pipeline presented concatenates know approaches, and misses a number of more detailed explanations and descriptions .",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Responsible research practice (e.g., human subjects, data release)"
            ],
            "details_of_ethics_concerns": "It is not described how ethical issues are tackled when the data was recorded (in a clinical setup?)",
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}