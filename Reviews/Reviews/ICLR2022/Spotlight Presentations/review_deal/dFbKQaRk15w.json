{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "Improving the expressiveness of GNN is an important problem in the current graph learning community. Its key idea is to generate subgraphs from the original graph, then encode the subgraphs into the message passing process of GNN. The proposed method is proven to be strictly more powerful than 1-WL. The authors also quantize how design choices such as the subgraph selection policy and equivariant neural architecture can affect the architecture’s expressive power.\n\nAfter the rebuttal, all reviewers are glad to accept this submission.\n\nDuring the discussion, while reviewer B3oK has shown some concerns on the concurrent works in NeurIPS 2021, it should not affect the decision of the submission once the authors have discussed them in the main text. The authors have done this in their revision, thus an acceptance (spotlight) is suggested."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper addresses known limitations of the expressiveness of message passing neural networks (MPNNs). Key idea is to generate subgraphs from the original graph, to encode each subgraph with a GNN, and to aggregate the resulting set of subgraph-encodings. Since generating all possible subgraphs is computational infeasible for larger graphs, only a smaller subset of subgraphs is used in practice. The paper shows that their approach is more expressive than 1-WL. Moreover, the paper shows that different variants of the proposed method can outperform the base model.",
            "main_review": "Strengths:\n- The paper is well-written, easy to read and provides lots of details at the same time.\n- The proposed idea is rather simple, which is kind of an advantage and a disadvantage at the same time.\n- The proposed method shows (small) improvements over the base encoder in many cases.\n\nWeaknesses:\n- Even though the proposed method shows improvements, it should be mentioned that the improvements are rather smaller, especially given the increased complexity and computing requirements of the proposed method. Hence, it seems rather unlikely that it will be widely adopted in its current form.\n- The paper does not provide details about the size of the models (e.g. in terms of number of trainable parameters). This problematic since it remains unclear if the performance improvement stems from the increases expressiveness or is simply due to a larger model size.\n- Highlighting the top three models (e.g. in Table 1) is highly misleading since only one version of each prior method is compared against 16 versions of the proposed idea. In general, this comparison does not make sense since the propose method has 16 trials to achieve a good result while all reference models only have a single trial.\n- No code is provided to verify the results.",
            "summary_of_the_review": "To summarize, the paper proposes a fairly simple but reasonable idea that achieves some improvements. However, since the improvements are rather small and the complexity of the model increases substantially, it is unclear if this idea will receive a lot of attention.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes to improve the expressivity of MPNN by using H-Equivariant layers to process bags of subgraphs accounting for their natural symmetry. The proposed DS(S)-GNN is proven to be equivalently powerful as DS(S)-WL, which can be strictly more powerful than 1-WL. It also provides theoretical results about how design choices such as the subgraph selection policy and equivariant neural architecture affect the architecture’s expressive power. In addition, a stochastic sampling scheme is proposed to mitigate the computational overhead.\n",
            "main_review": "Pros:\n- This paper gives solid theoretical formulation and analysis of graph representation learning over bag of subgraphs. The technical contribution of this paper is sound and significant.\n- This paper gives detailed formal analysis of design choices such as base graph encoders and subgraph selection policies.\n- The presentation of this paper is clear.\n\nCons:\n- In the experiment results, some DS(S)-GNN variants have inferior performance than basic graph encoders on some graphs.  Can there be any insights or guidance on whether to use DS(S)-GNNs for specific kinds of graph data? As we might want to quickly estimate this DS(S) architecture can be beneficial before actually conducting its heavier computation.\n\n\nSome work that might be relevant:\n- [1] is on the empirical side of using the aggregation of subgraph representation to represent a graph. It tries to factorize the original graph into disentangled subgraphs, then to utilize the factor subgraph embedding to represent the whole graph.\n- [2] shares similar high-level ideas with this paper, which is to encode the graph through different subgraph channels then aggregate them.\n\n[1] Factorizable Graph Convolutional Networks, Yiding Yang, Zunlei Feng, Mingli Song, Xinchao Wang, NeurIPS 2020\n\n[2] Graph Neural Network with Automorphic Equivalence Filters, Fengli Xu, Quanming Yao, Pan Hui, Yong Li, arxiv 2020\n",
            "summary_of_the_review": "This paper provides a solid foundation on the theoretical formulation and analysis of graph representation learning over bag of subgraphs. The contribution is technically sound and the solution is well supported by detailed analysis of design choices. Thus I recommend acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper aims to improve GNNs' expressive power based on bag-of-subgraph\nrepresentation. The authors develop a Equivariant Subgraph Aggregation\nNetwork, which uses equivariant layers to encode subgraphs and aggregate\nsubgraph representations. The authors have performed theoretical analysis of\nthe proposed framework, subgraph selection policies, and their expressive\npower. The experiments show that such an encoding can lead to a better\nexpressive power on several real and synthetic graph classification datasets.\n",
            "main_review": "This paper develops a Equivariant Subgraph Aggregation Network to enhance the\nexpressive power of GNNs. The idea is to 1) represent graphs as bags of\nsubgraphs 2) use permutation-equivariant layers to encode the subgraphs 3)\naggregate the subgraph representations into an invariant graph representation.\n\nStrength:\n\nS1. The idea of using equivariant subgraph representation and aggregation is novel.\n\nS2. Very solid theoretical analysis showing the expressive power of DS-GNN and DSS-GNN.\n\nS3. The experiments are comprehensive as they conducted experiments on various\nreal and synthetic graph classification datasets and include most existing\nmethods on expressive GNNs. The results are mostly positive, showing the\nbenefits of DS/DSS-GNN architectures.\n\nWeakness:\n\nW1. It'll be better if the authors can perform a detailed breakdown. Of the\nexperimental results to show. To provide some insights above, when does the\nDSS. Architectural works well and when does it not? The.\n\nW2. The model introduces extra time overhead. Although the authors introduced\na stochastic strategy that can mitigate this issue to some extent, the time\ncost is still several times larger than standard GNNs. Also, it's better to\nshow how the method scales with graph sizes.\n\nW3. How will this method work for other graph related tasks beyond graph classification? \n",
            "summary_of_the_review": "This is a solid paper on improving the expressive power of graph neural networks. The proposal approach of equivariant subgraph representation and aggregation is novel, the theoretical analysis is solid, and the results are positive. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposed a novel way of increasing MPNN expressive power without using higher order node tuples representation, thus proposed method has lower complexity compare to naive powerful GNN methods. The core idea is to create a set of subgraphs for each graph in the given dataset by using single node or edge deleting,...etc policy. They create a virtual graph that adjacency is summation of subgraph's adjacencies and node feature matrix is the summation of subgraph's feature matrix. Each subgraph's next layer representation will be obtained by aggregation of self subgraph's new representation by baseline MPNN and also this virtual graph's representation by another MPNN. In application this virtual graph is replaced by original graph or in another version is basically neglected. After arbitrary number of layers, they apply subgraph level pooling to obtain invariant representations. They see all subgraph's final representation as a set of representations. Thus to obtain final graph level representation, they apply another layer on set of subgraph representations such deepsets.\n\n\n",
            "main_review": "### Strong Points\n1. Searching more expressive GNN is hot topic and has a lot of importance both academic and industrial perspective. The main motivation and their approach is definitely eligible. This problem is one of the most important problems in GNN and recently there are a lot of different approaches. \n2. The way of explanations and writing manner of the author are nearly perfect. Seems they spent a lot of effort to present the idea clear and didactic way.\n3. Theorems and their proofs are clear and sufficient.\n4. The researches that are in the same track with this paper, usually use predefined handcrafted features and/or structures which needs domain expertise among nearly infinite number of selections. Proposed approach does not need that kind of engineering efforts.\n\n### Weak Points\n1. Increasing expressive power of MPNN by applying baseline MPNN onto each subgraph of given graph and later somehow aggregate them in order to get graph level representation is very smart idea. However, It is not new at all. Recently, one (maybe concurrent research) already published the node deleted subgraph idea in [1]. Their proposal almost the same with DS-GNN in this paper. In addition their proposal of sampling strategy is exact the same. Just the differences is that in inference time, they used full sampling to get invariant graph representation.  That paper in [1] was not seen in this paper. I think the differences between these two papers are not significant.  But I am happy to hear the differences by author point of view.\n\n2. If the baseline MPNN has linear time and memory complexity, proposed method with full sampling has quadratic complexity (when one node deleted subgraphs is used) that makes it infeasible easily w.r.t increasing number of nodes. Stochastic sampling of subgraphs seems reasonable solution, but this time the method lose the invariance property. Though this shortcoming, it may gives comparable results in the benchmark datasets, but how can we say it is more powerful than 1-WL in terms of separation power? It may separate non-isomorphic graphs but also it separates isomorphic graphs as well. Basically it separates all given pair of graphs because of stochastic sampling in inference time.\n\n3. Experimental part of the paper is very weak unlikely to the theoretic parts.  Since TU datasets have limited number of instance and results standard deviation is very high, we cannot make reliable comparison. I think, TU datasets result does not deserve place in main paper but maybe in appendix for just giving some idea. In the paper, just EXP datasets is the unique result that gives idea about separation power of GNN. However, this dataset is very small and all pairs are 1-WL equivalent. The paper also mentioned SR dataset which consist of 3-WL equivalent graphs. I would like to see separation power analysis of SR dataset. I guess it is missing because of stochastic sampling. It basically separates all graph pairs even though pairs are isomorphic. Also Circular Skip Link dataset in (Murphy  2019) would be great expressive test of proposal method. But I strongly think that proposed methods fails, because that dataset includes isomorphic graphs that needs to be map on the same point in latent space. It is known that even though having more powerful GNN does not mean we will have better generalization on various datasets. But at least in some dataset such as Zinc12K, powerful methods outperformed 1-WL ones with a huge margin. I would like to see your method result on ZINC with compare to relevant recent powerful GNNs. In addition to Zinc, we know that in some artificial task such as substructure counting, 1-WL equivalent MPNNs fail ridiculously. So that kind of tasks would be great test to show your methods expressive power experimentally. \n\n### Reference\n[1] Cotta, L., Morris, C., & Ribeiro, B. (2021). Reconstruction for Powerful Graph Representations. NeurIPS 2021.",
            "summary_of_the_review": "Since the idea is not original if Cotta et al. work is seen, and it has lack of experimental tests in initial submission as mentioned above, I would recommend rejection to this work.\n\nBut as the author mentioned that Cotta et al. work should be seen as contemporaneous work and there are a lot of additional test in second version, I changed my rate to 6.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}