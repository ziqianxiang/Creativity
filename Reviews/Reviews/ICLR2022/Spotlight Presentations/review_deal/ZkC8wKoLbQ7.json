{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "The paper analyzes the learning behavior of deep networks inside RL algorithms, and proposes an interesting hypothesis: that many of the observed difficulties in deep RL methods stem from _capacity loss_ of the trained network (that is, the network loses the ability to adapt quickly to fit new functions). As the paper points out, some of these difficulties have popularly been attributed to other causes (such as difficulties in exploration) or to less-specific causes (such as reward sparsity: the paper proposes that capacity loss mediates observed problems due to sparsity). \n\nThe paper investigates its hypothesis two ways: first by attempting to measure how capacity varies over time during training of existing deep RL methods, and second by proposing a new regularizer to attempt to preserve capacity. These experiments are set up well, and their results are convincing &mdash; while there is likely no perfect way to measure or preserve capacity, the methods chosen here make sense. \n\nThis is a strong paper: it proposes a creative, appealing, and interesting hypothesis about an important problem (difficulties in training deep RL methods), and conducts a well-designed evaluation. We expect and hope that it will inspire interesting follow-on work.\n\nWe thank the authors for their thorough and helpful participation in the discussion period, including updates to improve the clarity of the paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a new regularization method for deep reinforcement learning, that is designed to prevent representation collapse and thus retain the ability of the network to fit functions that evolve in time, such as the sequence of Q-functions generated by approximate dynamic programming methods. This method consists in regularizing the network's features towards a set of initial linear transformations of their (the features) values at initialization.",
            "main_review": "This is a very well written and very insightful work. I took great pleasure and interest reading it.\nThe writing is flawless, the structure is very clear and the presentation of ideas unfolds naturally.\nThe contributions are significant, their evaluation is convincing, both in the method and in the results, and the discussion has an appreciable depth.\n\nI identify two weaknesses. First, some evaluation aspects are omitted, and I question their effect on the mechanism of InFeR. Second, some parts of the discussion could maybe be condensed to develop other aspects more in-depth, both in the main text or as references to the appendix. The following remarks list the aspects upon which I would like to see more details and/or an improved discussion in the paper.\n\nThe introduction of network capacity and, more importantly, the characterization of a network's effective dimension are well-developed ideas, with enough nuance with other approaches from the literature to be relevant contributions in themselves. In a sense, the effective dimension seems very similar in spirit to a VC dimension. Could the authors elaborate a little on this (both here and in the paper)?\n\nIn Figure 3, I fail to understand why the initial effective dimension of the QR-DQN network is always the smallest, except for the sparse pong task. This seems very counter-intuitive and should be justified.\n\nThe idea to regularize against a set of projections of the initial features is equivalent to defining a set of corresponding auxiliary tasks. I would appreciate seeing more details on how these $g_i$ projections are designed in the first place. Are they random projections? Is there a rationale in their construction? How dependent on their choice is the overall training?\n\nOne common remark on Deep RL papers is the very low number of seeds used to assess the statistical significance of experiments. This paper makes no exception: the canonical number of 3 seeds is used. I am well aware of the compute cost issue and I don't see this number of seeds (despite the associated low statistical confidence) as a reason to cross out these results. Nevertheless, the presented experiments display large variance in the results. Specifically, the performance gain of InFeR seems to come along an strong increase in performance variance. It seems unavoidable to discuss the causes for this variance. Is there a link with the construction of $g_i$? Or the variance of gradients for the ADP loss in sparse rewards environments? Is it related to the games InFeR is tested on? What do the corresponding policies actually do (a video would definitely be nice)? Do they act well consistently? I believe the discussion could be quite deeper on this question.\n\nIn the testing of hypothesis 1, concatenating with a fully random network is a rather radical comparison point. First, I believe the way this network is initialized might have an impact, and this should be at least mentioned explicitly (if not discussed). Second, I believe an in-between solution would consist in adding lateral connections, in a fashion similar to that of progressive neural networks. In a sense, this can be compared to adding random projections after each layer. This might provide a few more insights regarding the claim that the effect of InFeR on earlier layers is crucial to its success.\n\nAn interesting aspect which I would have liked to see is the evolution of the effective dimension at different depths of the network. Does representation collapse occur abruptly at a certain depth? Is it caused by early or deep layers in the network? Does it manifest within convolutional layers? Do shared weights (under-parameterization) act as a regularizer? By the way, what is the network architecture used? Classical DQN-style 3 convolutional layers + 2 fully connected ones (with the appropriate variations for QR-DQN)? What would happen with deeper networks such as the IMPALA architecture? \n\nMinor remark:  \nPage 2. I believe the $(X_t, A_t, R_t, X_{t+1})$ tuple of equation 2 was actually introduced as $(x_t, a_t, r_t, w_{t+1})$ a few lines above. Making this consistent would help.",
            "summary_of_the_review": "Overall this is a solid contribution on the question of preventing capacity loss in deep RL. The ideas are novel and the point of view developed on representation collapse, and the proposed way to prevent it, is a significant contribution to the community. A number of points could fruitfully be discussed, both in the main text and in the appendix, which would increase the practical impact of the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Summary:\n- Over course of training, deep RL agents experience \"capacity loss\", where networks are unable to quickly fit new functions. This problem is further exacerbated by non-stationary predictions (such as bootstrapping) over the course of training.\n- To prove this problem, the author runs two experiments (Atari and MNIST), and show that training error against a randomly-initialized network increases over time. They also define effective dimension (rank of feature space) and show that effective dimension is tightly correlated with performance.\n- The authors introduce INFER, which regularizes Q-value loss with error w.r. to randomly initialized network, and evaluate this on Atatri-57, showing most benefits in sparse reward environments (Montezuma's revenge)",
            "main_review": "Strengths:\n- Very nice paper that is well corroborated and is clearly presented in logical order\n- Solid experiments, good job!\n- Great analysis in Section 4.2\n\nWeaknesses:\n- Needs to better logically connect 3.1 and 3.2.  Why is effective dimension representative of/correlated to capacity? A good mathematical proof would work, ideally in Appendix B.\n- INFER seems very similar to RND exploration, except that the L2 error is moved from intrinsic reward to the loss function. Ablate on this! (such as Rainbow + RND)\n- Novelty of problem. This problem has also been shown in supervised learning, where neural networks become less adaptable over time, and I believe in several other works in RL, such as in Kumar's prior work. The paper, especially in introduction, present it like it is a new problem. Good to downplay this.\n- Figure 3 and Figure 4 make the same message (correlation between effective dimension and performance). Recommend combining both figures and move some repeated information (such as 4(b)) to the Appendix.\n-In Appendix, show how robust INFER is against hyperparameter tuning. It looks like beta=100 is a pretty arbitrary value.\n-Paper seems to be rooted in Atari environments. What prevents extending this to continuous control environments such as Mujoco?\n\nQuestions:\n- In Figure 2 (MNIST experiments) (clarification), is the label y_i = 1[y<i] a mask applied over the random network output?",
            "summary_of_the_review": "Overall:\nThis is a well written paper which gives strong evidence that capacity loss/representational collapse is indeed a root problem for deep RL. As long as the authors address the weaknesses, I am willing to bump this up from weak accept to strong accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Ethics Concerns:\nNone",
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "Summary of paper:\n- The authors identify a problem of \"capacity loss\" during RL training with neural networks, this is the problem of agents gradually losing the ability to fit new functions while trainig. \n- The authors argue this is a key factor that hinders learning and is most prominent in sparse reward environments, propose 2 different measures of capacity loss and conduct empirical analysis to show this phenomenon exist in some Atari tasks.  \n- The authors present a number of empirical analysis on this problem, and propose a simple regularization scheme to mitigate this issue, and presented some analysis on the effect of this scheme, giving interesting insights. \n- The proposed method improve performance especially on Montezuma's revenge when no specialized exploration techniques are used, supporting the paper's claims",
            "main_review": "==========================\n\nStrengths:\n- novelty: this problem is not adequately studied in the RL setting, so I believe the authors are making some interesting contribution\n- interesting empirical analysis and insights are provided. \n- empirical result: the result on Montezuma's revenge show that the proposed method (which is very simple) can improve performance a lot, which seems to support the claims of the paper. \n- writing quality: good\n\n==========================\n\nWeaknesses: \n\nMajor concern:\n\nFigure 4 (b) I'm a bit concerned whether this figure actually supports the argument that agent gradually lose capacity during training. Authors claim that this figure shows that learning happens only after agent *recovers* from representation collapse. Note that Figure 1, 2 show that agent gradually lose capacity during training. But figure 4 (b) shows this agent's capacity is \"collapesed\" right from the very beginning of trainining, and then later this capacity is *significantly increased*. To make it more clear: \n\n1) Figure 1, 2 seem to show capacity (measured with MSE) starts *high* then gradually get *lower*\n\n2) Figure 4 (b) seems to show capacity (measured with effective rank) starts \"low\" then can get \"high\"\n\nSo it seems to me these 2 conclusions are actually... disagreeing with each other? Would be great if you can address this concern.\n\nAn potentially interesting experiment is... if InFeR is added to middle of training that sparse pong unlucky seed, should we see the capacity immediately increase? To be specific, I'm talking about comparing 2 curves, the first curve trains that unlucky seed as usual, the second curve uses the same seed, but when trains to sth like 50M frames, InFeR is added. In this case, will we see InFeR immediately increase the effective rank, and potentially immediately after bring performance up? \n\n\nOther concerns and comments:\n\n- One thing seems a bit lacking to me is comparison to other methods that improve representation learning. Have you try compare InFeR with any other methods that center around representation learning? For example, contrastive learning methods like CURL, will they achieve the same capacity-preserving effect? What is the advantage of using InFeR compared to other representation learning methods? \n\n- A disucssion on the potential negative effect of the proposed regularization scheme would be good: in some environments the performance dropped a bit after adding INFER, what could be the reason? And if the initial values are simply not good enough, will this method lead to performance drop? \n\n- Page 4 when authors explain Figure 2, \"We further consider a ‘sparse-reward’ version of MNIST...\" after this there is one sentence to explalin the MNIST experiment setting, and this seems a bit too concise. I can still get what you mean, but would really prefer you try to expand the explanation a bit and maybe give an example or schematic, you can put this in the appendix if needed, it would be really helpful for readers to understand it fast and prevent potential confusion. \n\n- page 1 in introduction, some very recent results on data augmentation (mainly DrQ and DrQv2, and this is also used in CURL, temporal augmented contrast etc.) actually find that data augmentation can boost RL performance quite robustly. Perhaps you want to consider rewrite this part slightly? ",
            "summary_of_the_review": "Overall I think the paper makes some interesting and novel contributions, the problem of capacity loss is rarely studied in the RL setting, and research in this direction might help us better understand the difficulty in DRL training. Though I currently do have some concerns. I look forward to reading the authors' rebuttal. I will consider increase my score if my concerns are fully addressed. \n\n========================================================\n\npost-rebuttal\n\nI now increase my score to 6 since I feel a lot of the concerns are indeed addressed in the response. However the authors should go through the paper carefully and try to make things more clear in the paper, would be nice to incorporate some of these reviewer discussions into the paper or appendix. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors tackle a convergence issue in RL where the networks lose expressiveness during the training and are unable to regain it as the policy improves, ultimately leading to bounded performance. To solve this issue the authors propose adding a regularization term that forces the trained networks to regress to a random function of their initial output.",
            "main_review": "Strengths:\n\n1. The paper tackles a super important question which is the stability of training in RL. \n\n2. The high computational effort in running all Atari games for 200M frames.\n\nWeaknesses:\n\n3. The paper has no theoretical result, and even its capacity definition is thrown away quickly for a more practical measure. \n\n4. Despite the heavy experimentations, I wasn't convinced by some of the plots, and some seem overbearing:\n- Figure 1: Why use 3 algorithms to show the same point.\n- Figure 2: Not sure I understood what is shown here besides the difference between activation functions - is this the point of the figure? because it seems a bit outside the scope of the paper.\n- Figure 3: Does it make sense that at Frame count=0 not all plots start roughly at the same point - there wasn't training yet so why do we see a difference in the dimension? Also, we see the effective dimension can grow larger, if this is the case why would a collapse occur at all? Since the point in this Figure is to show a connection to sparsity, why not include a plot of the sparsity as well (like num steps where R!=0 divided by num steps)\n- Figure 5 (b): The results for Robotank doesn't really follow the theme of the paper - both methods reach the same score even though one loses capacity. For Montezuma there is no capacity loss.\n- Figure 5(c): The improvement does not look very consistent across games.\n\n5. The proposed method of Infer looks a bit naive to me - why preserve random functionals and not for example an autoencoder of the state or some other meaningful feature. This method forces the network to keep random functions of the state instead of learning the underlying structure. You can also try to regularize for the rho-hat directly using some relaxation.\n",
            "summary_of_the_review": "While the paper tackles an important problem, the proposed solution and it's empirical exhibition are insufficient in my view for acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}