{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "All three reviewers recommend acceptance. The paper introduces an interesting study and insights on the connection between local attention and dynamic depth-wise convolution, in terms of sparse connectivity, weight sharing, and dynamic weight. The reviews included questions such as the novelty over [Cordonnier et al 2020] and the connection to Multi-scale vision longformer, which were adequately addressed by the authors. The findings in this paper should be interesting to the ICLR community."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "Recently local attention based vision transformers achieved state-of-the-art results on various visual recognition tasks. This paper rephrases local attention as a channel-wise spatially-locally connected layer with dynamic connection weights. By analyzing local attention form the view of sparse connectivity, weight sharing and dynamic weight computation, the paper discusses the similarities and differences between local attention and dynamic depth-wise convolution. Motivated by this connection, the paper experimentally compares local attention with depth-wise convolution and its dynamic variants on three vision tasks using the macro architecture of Swin-Transformer. The results show that dynamic depth-wise convolution-based models perform on par or better when compared to local attention while being computationally more efficient.\n",
            "main_review": "Well-written and easy-to-follow paper\nForms interesting connections between local attention and dynamic depth-wise convolution in terms of sparse connectivity, weight sharing and dynamic weight computation.\nExperimentally demonstrates that dynamic depth-wise convolution is on par or better than local attention in terms of performance on various vision tasks\nThe proposed inhomogeneous dynamic depth-wise convolution variant is computationally (slightly) better than local attention module",
            "summary_of_the_review": "The main difference between local attention and the inhomogeneous dynamic depth-wise convolution (sec 2.4) is in terms of how the dynamic aggregation weights are computed. The on par or superior performance of I-D-DW-Conv when compared to local attention suggests that dot product-based weight computation (which is core to attention)  is not crucial. Dynamic weights that are predicted using only the query/center pixel features also work well. I think this is an interesting for the community to know.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper connects the local attention and dynamic depth-wise convolution and validates that empirically.\n",
            "main_review": "Strengths\n\nThe paper is easy to follow and well-written and builds the connection between local attention and dynamic depth-wise convolution.\n\nWeaknesses\n\n1. Regarding the empirical validation of equivalent, I think simple accuracy is not proper as it summarizes the whole dataset. E.g. prediction disagreement from Swin, mean class accuracy or even centered kernel alignment would be better whether or not the networks act similarly.\n\n2. I am not quite clear about the main difference from Cordonnier et al 2020. The authors pointed out that this work focuses on local attention to depth-wise convolution while the prior art discussed normal convolution and normal self-attention. Nonetheless, the depth-wise convolution is a special case of normal convolution and local attention is a special case of normal attention (local attention is masked attention of normal attention).\n\n3. As depth-wise will slide the window while Swin does not. Why author pick it as an example rather than ViL (Multi-scale vision longformer, Zhang 2021)? ViL adopts both local attention and sliding windows. Or what is the connection to ViL?",
            "summary_of_the_review": "The paper qualitatively connects local attention and dynamic depth-wise convolution and they validate this connection empirically.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper reveals the connection between local attention and dynamic depth-wise convolution. It empirically shows that the models based on depth-wise convolution and the dynamic variants with lower computation complexity perform on-par with or slightly better than Swin Transformer for ImageNet classification and other downstream tasks such as COCO object detection and ADE semantic segmentation.\n",
            "main_review": "Strengths:\nThis work is simple and easy to follow. It raises valuable insights on what is the essential components for the \"superior\" performance of Swin Transformer. With its clear and precise experiments, we can see that the local attention-based networks, Swin Transformer, and the depth-wise convolution-based networks, perform on par in both classification and other downstream tasks such as COCO object detection and ADE semantic segmentation. \n\nI think the single above fact is valuable and enough for acceptance, as it helps us \"narrow down what features of other models are most valuable\" (comes from a Reddit comment for another work ConvMixer). Besides, this work also analyses three individual properties of local attention and depth-wise Conv, showing more valuable details of their connection. \n\nWeakness:\nI am curious about the performance of larger models with the ImageNet 22k dataset, especially for downstream tasks such as detection and segmentation. Based on some current works and my own exps, for the larger size of model and data, attention-based models (Swin, CSwin) do have some advantages on downstream tasks. So I think it would be helpful to include these experiments if the authors have sufficient resources.\n\n",
            "summary_of_the_review": "I think this paper provides valuable insights on the connection between local attention and dynamic depth-wise convolution and I suggest accepting it without a doubt. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}