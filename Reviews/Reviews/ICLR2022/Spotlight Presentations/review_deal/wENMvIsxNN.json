{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper introduces a new technique for discovering closed-form functional forms (ordinary differential equations) that explain noisy observed trajectories x(t) where the \"label\" x'(t) = f(x(t), t) is not observed, but without trying to approximate it. The method first tries to approximate a smoother trajector x^hat(t), then relies on a variational formulation using a loss function over functionals {C_j}_j, defined in terms of an orthonormal basis {g_1, …, g_S} of sampling functions such that the sum of squares of all the C_j approximates the theoretical distance between f(x) and the solution f*(x). These sampling functions are typically chosen to be a basis of sine functions. The method is evaluated on several canonical ODEs (growth model, glycolitic oscillator, Lorenz chaotic attractor) and compared to gaussian processes-based differentiation, to spline-based differentiation, regularised differentiation, and applied to model the temporal effect of chemotherapy on tumor volume. \n\nReviewers found that the paper was well-motivated and easy to follow (EBvJ), well evaluated (EBvJ), offering new perspectives to symbolic regression (79Ft). Reviewer vaG3 had their concerns addressed. Reviewer ZddY had concerns about the running time (a misunderstanding that was clarified) and the lack of comparison to a simple baseline consisting in double optimisation over f and x^hat(0) using Neural ODEs (the authors have added a Neural ODE baseline but were in disagreement with ZddY and 79Ft about their limitations).\n\nReviewers engaged in a discussion with the authors, and the scores are 6, 6, 8, 8. I believe that the paper definitely meets the conference acceptance bar and would advocate for its inclusion as a spotlight in the conference."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper suggests a variational formulation for discovering ODE systems in a closed-form. The main advantage of such a formulation is that it can find the system based on the observed trajectories (and some analytical testing functions) rather than estimating (possibly noisy) derivatives of them. The variational objective is equal to the distance between estimated and true velocity vector fields on the manifold when the estimated trajectory converges to the true one in L2 sense and there are infinitely many testing functions (while a finite number of functions are okay empirically). The proposed method outperforms its counterparts for various ODE systems and tumor volume dataset (whose true dynamics is unknown).",
            "main_review": "The paper is well-motivated and easy to follow. The proposed variational approach is sound for by-passing time-derivatives of noisy observations. It is also a model-agnostic framework, thus might be widely applicable regardless own’s specific setting.\n\nEmpirical validation on the proposed framework (D-CODE) shows that D-CODE is the right way to recover ODEs under some undesirable but unavoidable measurement errors.\n\nThe tumor growth experiment is very interesting because its closed-form ODE is indeed unknown. D-CODE can capture an interesting behavior (tumor volume increase due to drug resistance?) while the baseline cannot.\n\nI have not found any major drawbacks to this paper. For these reasons, I think this paper is worthy of publication for ICLR. The followings are some (relatively minor) questions:\n\n* Q1. Treating second-order non-autonomous systems as a set of first-order autonomous ODEs requires non-trivial effort since the number of variables increases to 2J + 1. Is there any suggestion about when one should consider such a situation (without prior knowledge of the target system)? It seems that the authors have found non-autonomous ODE for the tumor growth task, thus it will be nice if they can explain their modeling procedure briefly.\n\n* Q2. Is the search space for mathematical operations used in this paper enough for arbitrary systems? If not, and at the same time if the variational objective does not converge to 0 well for an unknown target system, then what would be the best option for the next behavior, e.g., adding some additional operations, or considering higher-order non-autonomous systems such as Q1? It may depend on the target problem, but it will be nice if the authors can discuss about it.\n\n* Q3. The authors state that neural ODEs should estimate the unknown initial condition (thus they are not very effective for chaotic systems), but it seems that one can rather directly use y(0) = x(0) + eps(0) as an initial condition for neural ODEs. Are neural ODEs still bad for such a case, though it might depend on the noise level and Lyapunov exponent of the target dynamics? The current evaluation of neural ODEs in Figure 4 seems to be extremely poor (and I felt the evaluation is slightly unfair), thus I wonder it is truly a limitation of neural ODEs or due to a bad estimation of the initial state.\n",
            "summary_of_the_review": "As I mentioned above, I think the paper is interesting and thus recommend the acceptance of this paper.\n\n***\nPost-rebuttal: I appreciate the authors’ clarifications on my questions. I'd like to keep my score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a new methodology to infer symbolic ODE representation from observed time series. In contrast to previous methods, they bypass the inference of the time derivative and rather proceed by first estimating a continuous approximation of the trajectory and then optmizing a novel objective function. The first step can be performed with the interpolation method of choice (GP or splines). The second step can use any optimization scheme that does not require derivative (such as genetic algorithm). The authors then evaluate their approach. on a series of dynamical system and show improved performance compared to the baselines under consideration.",
            "main_review": "The paper is well written and addressing an important problem for scientific discovery and interpretability of ODE methods. \n\n\n- My main comment is that I think another baseline to investigate the importance of the proposed objective function would be the following : you optimize over an initial value and the function f and you minimize the reconstruction error. Is this a baseline you could compare against ? As it does not involve computing the derivative either, it could also be competitive and really assess the added value of the proposed objective function. I also believe this setup would fit Theorem 1. If this is not a good idea, can authors explain why ? As I don't see a clear advantage of the proposed method over the one I just described, it currently undermines my evaluation of the significance/novelty of this work. However, I'd be very happy to change my mind on this topic.\n\n- In Table 2 it seems that the performance differ massively between both dimensions (8) and (9). Yet, at a glance, their functional form look really similar. It would be nice to provide some extra information or insights in that regard in the text.\n\n- Because all these symbolic regression approaches make use of non-differentiable optimizers, the computational cost can be quite high. I would appreciate some discussion of the computational complexity of the different methods and to have an order of magnitude of the computation time.\n\n- It's also not exactly clear to me how the interpolation steps work in the multi-dimensional setup. In practice do you use single dimension GPs for each dimension or do you use multi-task GP to model the correlations ? \n\n- In Equation 3, the first integral should have $f_j$, as $f$ is not a scalar function.",
            "summary_of_the_review": "I think this is a nice paper but to fully motivate the approach, it should compare against another obvious baseline. I would like the authors to motivate why the setup I described above is not considered in the experiments. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Authors propose D-CODE, a framework to approximate closed-form ODEs using symbolic regression. The method first estimate the trajectory by smoothing the observations (using any convenient smoother) before inferring a closed form (finite sequence of operations) transition function. This allows for better interpretability of the system. \n\nAuthors test D-Code on 5 simulated datasets, where they show high performance of the method, and on one real dataset where they use D-Code to model the temporal effect of chemotherapy on the tumor volume, and compare it to Total variation regularized differentiation (SR-T), the closest method in the literature. ",
            "main_review": "Using symbolic regression to approximate closed-form ODEs is novel and very interesting. The results presented in the paper are compelling but I would want to see more comparisons to other methods, as well as results on more datasets. \n\nThe method section is very well described, with useful and precise details found in the appendix.\n\nThe weakness of this paper lies in the experiments and results section. \n\nAuthors have selected five dynamical systems governed by a wide range of nine closed-form ODEs. However, all these system have independent Gaussian noise (with different noise level). I assume that if the noise is properly handled by the smoother, this will not impact the model, but I would like to see experiments on datasets where the smoother does not necessary perform very well (low signal to noise ratio). In that case, will D-CODE be robust enough to recover correct parameters of the ODE? \nIf this is not the case, I don't really see the advantage of D-CODE in addition to \"converting\" an already inferred trajectory into an ODE for the sake of interpretability. \n\nSimilarly, how will it handle sparse observations? One advantage of methods inferring ODE/SDE parameters for dynamical systems are the ability to approximate trajectories in regions with few observations. Here, won't the method suffer from the smoother if not able to capture the trajectory correctly? \n\nTo understand better the advantages of D-CODE over other methods, I think that showing results on more than one real dataset (I understand the dataset presented here is very challenging), with comparisons to more than one method (for example, Neural ODE, discussed here in the simulated data experiments). \n\nAlso, is D-CODE restricted to one dimensional observations? (would it be possible to perform symbolic regression for higher dimensional latent variables?) If it is possible, then an extension of this framework to higher-dimensional observations and latent processes would be very compelling and allow for comparisons with many models from the Dynamical systems literature that infer latent ODEs.\n\nMinor comment: the figures (especially Figure 2) could be made bigger and clearer (by removing white space between the panels).\n\nPost rebuttal : I increased my score from 6 to 8 given that D-CODE allows to approximate the latent ODE even when the smoother does not perform well. ",
            "summary_of_the_review": "I would lean towards acceptance of the paper. Using symbolic regression to approximate closed-form ODEs is novel and very interesting, and the results presented in the paper are compelling. However, I would want to see more comparisons to other methods, as well as results on more datasets to gain a better understanding of when this method will be useful and outperform existing methods. I am hoping that the authors will be able to to answer my concerns by presenting more results. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper addresses the problem of learning closed form ODEs from observational data, when the observation can be noisy and not frequent enough that instantaneous derivatives can be estimated with low enough error. The work uses a variational criterion on the solution of the ODE, circumventing the need to evaluate instantaneous derivatives. It uses existing symbolic regression techniques but instead of the regression loss, it minimizes a loss term quantifying the violation of the variational constraints. It is a work combining existing results in a novel way, but offers new perspective and there are synergies between the different parts applied.",
            "main_review": "The method is clearly described, and the potential benefits are clearly understandable (see summary). However, I find the baseline comparison somewhat incomplete.\n\nMy main questions are about the NeuralODE comparison. I do not agree that learning a chaotic dynamics directly with a NODE is not possible. The author argue that we need to estimate the initial condition what makes it impossible due to the chaotic instability. Only thing we actually need to train a NODE is to minimize the error in a shorter time horizon. Specifically, if we have samples more often than the Lyapunov time we should be able to learn the mapping. We integrate the dynamics forward inside a smaller than Lyapunov time window, and use this error to learn. \nPlease clarify this point, and train the NODE with a loss appropriate, and show that result on the rightmost plot of Figure 4. A simulation should stay on the attractor, of course the concrete trajectory is impossible to reproduce.\n\nThe valid argument still remains that NODE is not interpretable while closed form ODE is. I have no doubt on this point. However if NODE training is actually possible, it can be used similarly as GP and others in the two-step methods, and the gradient networks can be distilled to a closed form equation.\n",
            "summary_of_the_review": "The work is derivative in methodology, but provide enough insight to be interesting. I have some questions on the Neural ODE comparison what would need some clarification. If this concern is addressed it would make it easier to support acceptance.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}