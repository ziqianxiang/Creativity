{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "The paper proposes an interesting hypothesis about deep nets' generalization behavior inside RL methods: it suggests that the nets' implicit regularization favors a particular form of degeneracy, in which there is excessive aliasing of state-action pairs that tend to co-occur. It proposes a new regularizer to mitigate this problem. It evaluates the hypothesis and the regularizer empirically, and it provides suggestive derivations to motivate both.\n\nThe reviewers praised the comprehensive empirical analysis, the insights into learning, and the combination of empirical and theoretical evidence. The authors participated responsively and helpfully in the discussion period, and addressed any concerns raised by the reviewers.\n\nThis is a strong paper: it derives and motivates a novel hypothesis about an important problem, and analyzes this hypothesis both mathematically and experimentally."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper provides empirical and theoretical evidence that value-based\nmethods that optimize TD errors with SGD have an implicit \"regularizer\" that\nincreases the dot-product of the representation at successive states.\nThe theoretical analysis shows that this can be offset by a term that\npenalizes large dot products, which motivates the proposed regularizer\n(DR3). The paper shows empirically that these large dot products are a\nsource of divergence and that training with the DR3 regularizer can\nstabilize learning.\n",
            "main_review": "Strengths\n---------\n\n-   Interesting theoretical analysis, while somewhat compromised to make\n    the algorithm tractable, still provides insight into the problem\n    addressed. It might be good to further show evidence that the noise\n    model is well-justified in the RL regime, which would justify the\n    leap from theory to empirics.\n-   Empirical analysis is comprehensive, clearly demonstrating the\n    problem and algorithms stability when the problem is addressed\n    (through the proposed regularizer).\n\nWeaknesses\n----------\n\n-   While the empirical analysis is comprehensive, the performance of\n    the algorithms with the DR3 regularizer in the experiments are not\n    completely compelling. Although the mean performance seems better,\n    the statistical significance is not there in Figure 3 and Table 2\n    (and Figure 6, Seaquest). The results in Atari across all games are\n    more impressive, but the aggregation and normalization across games\n    can hide idiosyncrasies.\n\nDetailed Comments\n-----------------\n\n-   Section 3, Experimental setup: For TD-Learning, it says an action is\n    sampled. Shouldn't this be an expectation over the policy (and hence\n    use every action, without sampling?). If all the actions are\n    included in the target through an expectation, the problem of\n    divergent action-values should be less pronounced if not absent. The\n    problem seems to be that some action-values are left dangling (i.e.\n    the action-values of actions that are not sampled). By including\n    them all through an expectation, any problematic danging\n    action-values should be corrected immediately. The problem remains\n    for Q-learning, but further analysis needs to ablate maximization\n    bias and other confounders.\n\n-   Sec 3.1, Paragraph 2: What is meant by learned behavior policy?\n    Shouldn't the behavior policy be given or fixed?\n\n-   I think there is a leap in reasoning by connecting states aliasing\n    and high dot-products. Dot products can increase between two vectors\n    without any aliasing. If the magnitude of both vectors increase then\n    the dot product will increase without any change in the angle\n    between the vectors.\n\n-   Increasing dissimilarity between successive states is intuitively\n    reasonable, but this can also be a hindrance if the successive\n    states are indeed similar. Is there any way to account for this?\n\n-   Shouldn't a target network prevent the representations from being\n    similar? The networks are different and hence the representations\n    will be a few gradient steps away.\n\n-   Section 2 Equation 1: While we do take the gradient of this\n    expression, it is not correct to say that this is a proper\n    optimization objective. It is not minimized in practice (and often\n    diverges). In addition, methods that actually minimize this\n    objective, such as residual-gradient, do not result in a good\n    control policy.\n\n-   Figure 2: The left and center plots communicate the issue clearly,\n    except that the training loss plot on the right is not reflective of\n    the problem. As you point out, bootstrapping algorithms do not\n    minimize any objective function as the gradient does not belong to\n    any objective function.",
            "summary_of_the_review": "Overall, the paper raises and simultaneously addresses an interesting\nproblem for RL algorithms that employ function approximation. The paper\nis somewhat held back by the empirical analysis, specifically in the\nlack of statistical significance in several of their experiments. I am\nalso not sure of the correctness of the intuition that the dot-products\nresult in aliasing in reinforcement learning. The paper is still above\nthe acceptance threshold, but I hope that my comments below contribute\nto improving the paper.\n\n\nEdit: After discussion with the author and reading the back-and-forth between the authors and reviewer 5pgu, I have increased my score from 5 -> 8.\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper discusses how the implicit regularization effect of SGD could be harmful in the offline deep RL setting, due to degenerate feature representations, aliasing the representations for state-action pairs that appear on either side of the Bellman backup. To address this issue, the paper proposes a simple explicit regularizer (DR3) that counteracts the undesirable effects of this implicit regularizer. When combined with existing offline RL methods, DR3 has strong performance and stability (Atari 2600 games, D4RL domains and robotic manipulation from images).",
            "main_review": "The paper is overall well-written and clear. The method proposed is simple, quite clearly motivated and the claims are quite well supported by the experiments. Here are a few elements/questions that might need some clarifications:\n- the dot products can increase either because of an increased co-adaptation or because the magnitude of the vector of features considered increases. How can be sure that it is due to a co-adaptation and not because of an increase in the magnitude of the features?\n- In Fig 3, 5% and 25% of the data used by Singh et al. (2020) are used to make these more challenging. Would it also be useful to provide the results for the case of 100% of the data? It seems that even when data is relatively abundant, it would be interesting to report the performance with and without the regularizer. The same comment applied for Figure where a plot with 100% of the data could also be of interest.\n- The authors mention that they will release scores for individual runs as well as open-source their code. Is it possible to have access to that during this rebuttal period?\n\nMinor remarks: \n- typo: \"(...) and the the dot-product (...)\"",
            "summary_of_the_review": "Paper that investigates a simple regularization technique for off-policy value function learning in deep RL. The idea is quite well motivated by showing a few insightful values through learning (not only the score). The empirical results are quite convincing, even though they could be more complete.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper propose to study the under-parametrization/co-adaptation effect (observed while optimizing over-parametrized networks with SGD) arising in TD learning based RL algorithms which looks correlated to performance drops. They prove an implicit regularization effect, following previous works focussing on supervised learning, that could explained the co-adaptation phenomenon. They derive an explicit regularizer and show its effects on plethora of experiments.",
            "main_review": "Overall the paper is well written, pedagogical, and the problem at stake is really interesting. But I find the presentation of experimental results misleading which makes me doubt about the performances of the proposed solution.\n\n### Major revisions/questions/remarks\n  * Roughly 3 papers out of 5 refer to non peered reviewed papers (arXiv)\n  * Which algorithm is used to learn the Q-value in Figure 5? I am assuming it is CQL since the results match the ones for CQL in Kumar et al. 2021 \n  * \"it also alleviates rank collapse, with no apparent term that explicitly increases rank\" (Section 5), and \"This is potentially suprising, because no term in the practical DR3 regularizer explicitly aims to increase rank\" (Appendix A.3):\n    * The only srank's behavior exposed in this paper are the ones observed while learning the Q-network with the CQL algorithm. What about DQN, REM, and BRAC? Is the alleviation of rank collapse is specific to CQL+DR3?\n    * Looking at the simplified loss the authors based their intuition on, composed of 3 (maybe 4?) terms:\n      * 1. TD error: $||r + {\\phi^\\prime}^\\top w$ - $\\phi^\\top w||^2$;\n      * 2. CQL OOD samples regularization: ${\\phi^\\prime}^\\top w$ (at least);\n      * 3. Implicit regularization: $||\\phi||^2 - \\[\\[{\\phi^\\prime}^\\top\\]\\] \\phi$;\n      * 4. Potential L2 regularization on weight: $||w||^2$ (at least);\n      * It looks like adding the explicit regularization is an explicit way to increase rank, even if this regularization is more computationally efficient than Kumar et al. 2020. Am I missing something?\n    * Plotting the norms of $\\phi$, $\\phi^\\prime$, and their cosine distance with and without DR3 could be quite informative\n  * Comparing Figure 6 (resp. A.4) to Figure 5 (resp. A.3) is misleading the reader:\n    * At first sight it looks like the performances are somehow related to the srank not shrinking\n    * But at 50x62.5K steps the srank of CQL is higher than CQL+DR3 for Breakout for example.\n    * Why the Average Return of Asterix is missing (Figure 6)?\n    * Looking further at the results reported in Kumar et al. 2021, the authors are missing what seems like the interesting region w.r.t. their claims. It is only after more than 50x62.5K gradient steps that the performance drops appear. Why did the authors clipped the plots if the experiments were done for at least 200x62.5K gradient updates. This could help conclude about the training stability the authors claim reporting in the paper (e.g. p. 2 \" giving rise to methods that train for longer, reach a better solution, and remain stable at this solution \").\n    * Nevertheless I notice what looks like a significative improvement w.r.t. the average return\n  * Same goes for Figure 4\n    * The x-axis' scale for 1% Uniform Replay is different from the other ones, why? It looks like CQL+DR3 suffers a performance drop and that with more gradient step the performances with or without the explicit regularization would be similar.\n    * Is that the case for the 1% Uniform Replay experiment at 200x62.5K gradient updates?\n  * \"we evaluate DR3 in conjunction with CQL on the harder D4RL domains (antmaze, kitchen domains)\" (Section 6), \"we use the harder AntMaze and Franka Kitchen domains for evaluating CQL, since these domains are challenging for CQL.\" (Appendix E.2)\n    * W.r.t. which metric those domains are harder than the other ones? From Kumar et al. 2021: \"These tasks are especially challenging, since they require composing parts of trajectories, precise long-horizon manipulation, and handling human-provided teleoperation data\". In this case, does the potentially poor performances are correlated to the co-adaptive phenomenon? Reporting sranks after 2M gradient steps would help conclude.\n    * While trying to put emphasis on \"stability\", what looks like the main focus of the paper, I was expecting the authors to focus on environments for which previous methods suffer from performance drop and the under-parametrization phenomenon such as Ant-v2, Hopper-v2, and Walker2d-v2 as shown in Kumar et al. 2021 or hopper-medium-v0 as shown in Kumar et al. 2020b\n  * I do not understand why the effect of DR3 on D4RL is discussed using BRAC and not CQL nor REM. This would have had more impact since the paper mainly focus on the effect of DR3 w.r.t. CQL and REM. \n  * Overall a rigorous statistical significance test, to highlight significatively better results in the tables, is missing\n\n### Minor revisions/questions/remarks\n  * Does light colored regions on the plots always refer to 95% CI?\n  * Does the under-parametrization phenomenon and the drop in performances are specific to over-parametrized NNs?\n  * What optimizer the authors are using to optimize their models? This could be interesting since the theory developed in the paper assume an SGD optimization scheme.\n  * It would be better if all the algorithms are referenced in the main paper, e.g. COG has only its attached reference in the appendix (?)\n  * Writing \"This protocol is standard in Atari\" seems a bit far-fetched when the references point only to Google related papers\n  * What does bold format mean in the paper tables?\n\n### Some typos\n  * $\\theta$ missing for some gradients\n  * p. 12 \"A eoretical comparison\"\n  * p. 13 \"in ou practical\"\n  * p. 16 \"(Equation'4)\"\"\n  * p. 22 \"atleast\"\n  * p. 24 \"wprked\"\n\nAuthors answering remarks and questions during the rebuttal period might lead to a higher grade. But, as is, important claims of the paper are not well-supported and the way the results are presented is suspicious.",
            "summary_of_the_review": "The paper looks theoretically sound some questions remain regarding the experimental efficacy of the proposed regularizer claimed by the authors.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper first empirically notices the feature co-adaptation phenomenon which will be exacerbated in TD-learning with out-of-sample next actions. The authors theoretically show that the implicit regularization of (noisy) SGD encourages maximizing the inner product between the encoded features of current and next state-action pairs, which is responsible for the observed phenomenon. Thus, the authors propose a novel explicit regularizer for offline TD-learning to encourage smaller inner products. Extensive experiments on both video game environments and robotics demonstrate the effectiveness of the proposed regularizer, with various offline TD-learning algorithms as the testbeds.",
            "main_review": "Strengths:\n1. The identified phenomenon is very critical: When the consecutive state-action pairs are encoded to have similar representations, the learned value functions, as well as corresponding policies, would perform poorly.\n2. This paper provides theoretical evidence to explain the feature co-adaptation phenomenon, which meanwhile explains why it is severe for out-of-sample next actions.\n3. The empirical evaluation strongly confirms the effectiveness of the proposed regularizer.\nWeakness:\n1. There are some typos, e.g., \"stableif\" in Theorem 3.1.\n2. The organization can be improved further. It is not easy to follow this story, and there are many preliminaries about offline RL that are indispensable for readers to understand the discussions.",
            "summary_of_the_review": "This is a good paper which studies an important problem from both empirical and theoretical aspects and brings in sufficient novel stuff. I appreciate the contributions made by the authors and recommend this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}