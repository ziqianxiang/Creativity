{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "In this paper, a new learning scheme for minimizing the confidence set by conformal prediction is proposed. Most of the reviewers agree that the idea is interesting and novel. This is an important contribution to trustworthy ML, with theoretically sound considerations and thorough experimental validation."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "In split conformal prediction, conformalization is applied using a model that has been trained on a separate training set. However, the training procedure usually optimizes an objective that has little to do with the ultimate goal of producing informative prediction sets. For the problem of classification, the paper proposes an alternative training procedure that simulates conformalization on mini-batches so that the learned model is directly optimized to produce smaller prediction sets. In addition, the proposed alternative training procedure can accommodate other loss functions, which can induce certain structures in the prediction sets. The performance gains are demonstrated through experiments.",
            "main_review": "## Strengths\n\nThe paper builds a convincing case for the proposed method by calling attention to the mismatch of goals in traditional training procedures and conformal prediction. Once one accepts this motivation, the proposed method appears quite reasonable. The experiments appear to be quite thorough. The extension to other structure-inducing losses is interesting, although its actual deployment may warrant caution.\n\n## Weaknesses\n\nI found no significant weaknesses in my initial assessment.\n\nA few questions for consideration are as follows:\n\n1. How large should the size of each mini-batch be? This probably depends on $\\alpha$, but are there other things that should be taken into account? Also, I feel like Section G ought to appear in the paper.\n\n2. In **Conformal Predictors for Training**, it is reported that \"Bel or ConfTr do not necessarily recover the accuracy of the baseline. When training from scratch, accuracy can be 2--6% lower while still *reducing* inefficiency.\" Here, I understand \"accuracy\" to mean the accuracy of marginal coverage. In that case, why is there any loss of accuracy at all when all methods are calibrated on a separate held-out set?\n\n## Typos\n- (p. 3) on test example -> on a test example\n- (p. 6) Put parentheses around \"see Tab. 1 for the main results\"\n- (p. 9. The last line) loose -> lose\n- (p. 10. The 2nd paragraph) loosing -> losing\n- (p. 14. Section B) Remove the parentheses around \"Vovk, 2021; Barber et al., 2019b\"\n- (p. 17. Table B) Batch Size and Learning Rate switched in the row corresponding to \"Camelyon, ConfTr+$\\mathcal{L}_{\\text{class}}$\"",
            "summary_of_the_review": "This is a well-argued paper backed up by extensive experimental results. To my knowledge, the method represents a novel contribution. I would recommend it for acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "**Summary:**\n* The paper considers putting conformal inference into the process of fitting deep neural networks, i.e., the paper considers \"differentiable conformal prediction\" / back-prop'ing through conformal inference.\n* The goal is to reduce the size of the prediction sets generated by conformal inference -- and more specifically to reduce the size variably in different parts of the input space.\n* Some experiments are given to show that the confidence set sizes are indeed smaller w/ the proposal than w/o it.\n",
            "main_review": "**Strong points:**\n* The idea of putting conformal inference into the neural network training loop is interesting.\n* The goal of reducing confidence set size is important.\n\n**Weak points:**\n* The improvements (e.g., in Tables 1,2) seem somewhat minor to me.  Am I missing something?\n* It's actually not totally clear to me that you get coverage at the nominal level -- can you justify that?\n* I don't really see what the proposal here offers over Bates et al. (2021) -- can you explain?\n* I found the paper somewhat hard to read -- I think things could have been explained a bit better in places.  But also notation was often used without defining it first, and some non-standard terminology was used.\n\n**Questions:** see my \"Weak points\" and \"Additional feedback\" sections.\n\n**Additional feedback:**\n* How do the results change as you vary the mini-batch size?\n* Eq 3 -- I think you forgot to put $|\\cdot|$ around the confidence sets inside the max?\n* You appear to use $E_\\theta$ before defining it.",
            "summary_of_the_review": "**Recommendation:** reject.  I have some questions about the novelty of the empirical results and the methodology.  Happy to change my mind if I missed something.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Conformal prediction methods generally operate via post-processing on black-box classifiers to produce a confidence set. In this paper, the authors integrate this post-processing step into the training procedure in order to reduce the inefficiency of conformal prediction while providing the same coverage guarantee.\nThe proposed method uses soft thresholding and differentiable sorting to convert two popular conformal post-processing method classess (THR - Sadinle et al. and APS - Romano et al.) into differentiable modules, although they only use THRLP (the method from Sadinle et al. applied to log-probabilities) in the training process for the results (in Table 1).  Additionally, they propose the use of weighted inefficiency regularization (also termed the size loss) and a \"configurable\" classification loss that can yield desirable behavior w.r.t. class-conditional inefficiency and mis-coverage (or coverage confusion) respectively. \nThe proposed method is shown to outperform a competing method by Belloti on 5 public multi-class datasets (including CIFAR100) and a binary dataset (wineQuality).",
            "main_review": "Strengths\n\n1. The proposed method addresses an important problem and the associated technical area has seen a surge in publications including a workshop at ICML 2021.\n\n2. The proposed method combines several novel elements and appears to be theoretically sound.\n\n3. The proposed method is shown to outperform a competing method by Belloti in terms of the inefficiency measure (for the same coverage guarantee) on 5 public multi-class datasets (including CIFAR100) and a binary dataset (wineQuality).\nThe authors attribute this to the fact that Belloti's method does not incorporate the calibration step within the overall training process.\n\n4. The authors convincingly demonstrate how the size loss term's weight can control the class-conditional inefficiency in Fig. 3 and how the \"configurable\" classification loss can control coverage confusion and mis-coverage in Fig. 5\n\n5. The thorough appendices cover important aspects as impact of hyperparameters, random trials, python source, etc.\n\nWeaknesses\n\nThe weaknesses are relatively minor and mainly related to clarity or typos:\n\n1.  In Table 1, the authors report additional inefficiency improvements over ConfTR with the configurable L_{class}, but they don't seem to provide the configuration parameters for L_{class}.\n\n2. In the text below Table 1, the authors mention: \"we found that Bel or ConfTr do not necessarily recover the accuracy of the baseline\". It would be nice if the authors report the actual accuracies in a table (with the specified value of \\alpha) and also define exactly how the accuracy given a predictive set was computed (presumably only singleton sets could count towards accurate predictions).\n\n3. Mis-spelling: \"CIRFAR10\" in Table 2\n\n4. Grammar / wording in a sentence in Sec. 4.2 can be improved: ... the more difficult class 3 (“cat”) obtains higher inefficiency than to the easier class ...\n",
            "summary_of_the_review": "The authors propose a novel, impactful and theoretically sound method for conformal training and convincingly demonstrate its efficacy.\nThis reviewer has some minor concerns regarding clarity that can be readily addressed. Therefore, I recommend acceptance.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Conformal prediction is a method that intends to provide set-valued, calibrated prediction, in the sense that they cover the true class with a guaranteed marginal statistical accuracy. The paper mainly proposes to learn the conformal predictor at the same time as the predictive model is used, through an end-to-end procedure.\n\nTo this effect, conformity scores and threshold values (using quantiles) are approximated by differentiable surrogates that are directly injected into the learning framework. It is also argued that the framework can offer some solutions to better control class-wise predictions, or class-wise constraints on the conformal sets. ",
            "main_review": "I think the paper intent is very interesting, as it would clearly be interesting to learn optimal conformal predictors rather than plugging them in once the model is learned, possibly using a loss function not entirely correlated to the purpose of providing set-valued calibrated predictions. \n\nI cannot say that I was able to fully follow the development of the proposed method, as it is densely presented in 2 pages. I would have appreciated some more elaborated examples or illustration of how the derived loss function and subsequent techniques approximate the whole conformal prediction framework, but even the (many) appendices did not provide additional intuition for the less expert read, and focused mostly on providing additional experimental results. In terms of accessibility to a larger audience, this is clearly a weakness that could be resolved by being more pedagogical (possibly in the appendices). \n\nThis said, I still have a couple of questions about the developed method:\n\n* A first one is that it is claimed that the proposed approach retains the conformal guarantees. However, I am not sure I fully understand why this should be the case theoretically, as what is used in the paper are differentiable approximations of the classical approaches. If those are approximations, why should those retain the same theoretical properties of calibration as the initial procedure? What guarantees, theoretically, that we will not be too far from the calibrated results? If there is no such guarantees, then at least some empirical calibration lines (in the appendices?) should be provided to see if indeed validity is still reasonably preserved. \n\n* A second one is that it is often claimed in the paper that conformal prediction is unable to handle class-wise issues, which is partially untrue  as such problems can be handled, for instance, by using so-called Mondrian conformal predictors over various categories, that can be user-defined and is typically used to have class-wise conformal guarantees. Could it be explained how the presented framework would handle such an issue? Indeed, most of the discussion about application-specific losses focus on efficiency and the shape of the conformal prediction sets, but not how one could control class-wise errors (the task that Mondrian predictors can handle quite well)\n\n* While I think that the proposal to better control the shape of the predicted set is actually quite interesting, I would be interested in having a more detailed argument about why this would preserve conformal guarantees (in the same way that lossless formulation would)? I am also missing a more critical discussion about the risks of privileging higher accuracy on some instances over a lower accuracy on some potentially critical instances? Classical marginal conformal predictors are known to have such undesirable effects, but it is unclear to me whether the proposed formulation can control in any sense this kind of behaviour? Would not an increased efficiency for a class/case lead to a lower accuracy for this class, possibly counter-balanced by a higher accuracy for the class for which we allow the learning method to be more uncertain about? I believe this would at least deserve a refined calibration/validity study along with the analysis of result efficiency. \n\n* Small comment: In the abstract and beginning of introduction, I would not especially refer to posterior probability estimates, as conformal predictions merely need score-valued predictors (whether those are brought back to the interval [0,1] by a softmax or another transformation does not matter much). I would also avoid talking about Bayes decision rule being the most probable class, as this is only true if one consider a zero/one loss? ",
            "summary_of_the_review": "The paper proposes a differentiable scheme to learn conformal predictors. This is clearly an interesting approach that could lead to improve conformal predictors, which are gaining increased attention in ML (but existing as a consolidated approach since about 15 years). The technical part of the paper is densely presented, making it difficult to really follow. It is also unclear to which extent the validity guarantees coming with plug-in conformal approaches are actually preserved. However, although no theoretical guarantee of that is fully discussed, the differentiable approximation seems close enough to preserve at least a reasonable empirical validity. The additional loss-based formulation and its various adaptations look quite interesting, but it is here even less obvious what are their relation to class-wise conformal methods (i.e., Mondrian ones), and what are the impacts of choosing different losses over validities (particularly class-wise ones). ",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}