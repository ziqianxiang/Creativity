{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This work presents an approach to learning good representations for few-shot learning when supervision is provided at the super-class level and is otherwise missing at the sub-class level.\n\nAfter some discussion with the authors, all reviewers are supportive of this work being accepted. Two reviewers were even supportive of this work being presented at least as a spotlight.\n\nThe approach presented is well motivated, experiments demonstrate its value and include a nice application in the medical domain, making the work stand out relatively to most work in few-shot classification. Therefore, I'm happy to recommend this work be accepted and receive a spotlight presentation."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper addresses the problem of transferring knowledge from abundant data with low granularity labels to the fine grained data with few labels. The model uses gaussian mixture approach to link the structures in the low granularity data with the few fine grained labels.",
            "main_review": "Pros:\n- paper is clearly written and well motivated\n- the approach is novel\n- empirical results are convincing and very solid\n- authors provide code and instructions to recreate datasets for reproducibility, datasets are public\n\nCons:\n- Please provide bullet point list of contributions. I advise to replace the last paragraph in the intro, which is too wordy and is not to the point\n- There have been some work studying the relationship between coarse and fine-grained level supervision in the zero-shot classification domain, for example https://arxiv.org/pdf/1906.11892.pdf. Please discuss how your work is different.\n- Additional experiments on well established fine-grain classification datasets such as CUB and Flowers would significantly strengthen the results of the study. I beleive those datasets are much more relevant to the task than CIFAR100, for example.\n- Testing only on seen / unseen superclasses is limited in that is overlooks the problems that arise in the practical setting when both seen and unseen classes are expected to be mixed. This has been pointed out in relation to generalized zero-shot learning https://arxiv.org/pdf/1712.00981.pdf. ",
            "summary_of_the_review": "I recommend accept since there is clear novelty in the paper, problem is important and well motivated, empirical results are convincing",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper presents SCGM, a new technique for solving the Cross-Granularity Few-Shot learning (CGFS) problem.  CGFS is defined as the problem of adapting a classification model trained on coarse (“superclass”) labels to perform well on fine-grained labels, which consist of multiple “subclass” labels within each superclass.  The paper presents a new generative modeling approach that enables end-to-end classifier training in a manner that (a) incorporates information about superclass-subclass hierarchies (b) does not rely on explicit subclass enumeration (c) provides improved empirical performance on CGFS and (d) provides systems benefits with respect to existing baselines.  The authors provide detailed description of their approach and characterize its relationship with existing methods, and present empirical results that support their methodological choices.",
            "main_review": "Strengths of the paper include the following:\n\n(1) The problem addressed is an important and understudied one.  While previous work has identified the CGFS problem (though not necessarily called it by that name), methods to address it in a technically principles way remain in their infancy.\n\n(2) The method proposed both naturally follows from the existing treatment of the problem in the literature and presents a novel approach to address it.  The generative modeling framework presented by the authors is well-motivated, described with a reasonably high degree of precision, and its relationship to other approaches (e.g. GEORGE, contrastive representation learning) is analyzed in a compelling manner.\n\n(3) Empirical results are generally compelling, and comparison between the proposed method and existing work (a) takes into account relatively current approaches to the problem and (b) evaluates performance along several important axes, including both classification performance and systems-level performance.\n\nWeaknesses of the paper include the following:\n\n(1) Though the presentation and analysis of the proposed method is reasonable, comparisons to other methods are at some points overstated in the opinion of this reviewer.  Several statements (mentioned in the “detailed comments” below) should be either refined, repositioned, or omitted to accurately reflect the contribution of this work.\n\n(2) Empirical performance gains with respect to ANCOR in particular are not always convincing.  There exist a number of cases where the two are very close, and it is not clear how evaluations of superiority/non-inferiority between different methods was performed.  The authors should make their procedure for computing confidence intervals and determining superiority/noninferiority clear, and should ideally provide appropriate statistical tests/analysis for these claims.\n\n(3) It would be desirable to have some analysis of theoretical guarantees about worst-case subclass performance.  The paper does not provide any such analysis in its current state.  It also appears (at least as far as this reviewer understands it) that the current formulation imposes a constraint that subclasses be of equal size.  If true, this should be discussed in substantially more detail, and would make the worst-case subclass performance analysis even more important.",
            "summary_of_the_review": "Abstract:\n\n”It is crucial to fields where fine-grained labeling (e.g., breeds) requires strong domain expertise thus is prohibitive, such as medicine, but predicting them is desirable.” --> I found this sentence confusing, consider rephrasing.\n\nIntroduction:\n\n“During the treatments…task” --> I understand the general point here, but the example seems a bit strange.  For instance, if I could predict an event before performing dialysis, would I not consider delaying the procedure?  If I delay the procedure, the events never happen.  If it's useful to be able to say \"we predict event type A vs. event type B\" to help physicians assess the risk of performing the procedure, that would make sense...but that doesn't seem to be how it's presented here.  Consider reframing this to make it reflect the situation you're trying to describe more precisely (or consider providing a slightly different example).  This is a minor issue, but the framing could throw some readers off.\n\n“The data constitute a support set” --> In the sense that they provide support for the different fine-grained annotation schema?  Or for training/fine-tuning?  Or for both?  Additional precision would help here.\n\n“Unseen siblings of the superclasses” --> What is meant by this phrase?\n\n“Subclasses may arbitrarily spread within each superclass” --> Maybe point out \"and in real datasets, it is often the case that they are unevenly represented in a way that causes subclass performance gaps.\"\n\n“However, their approach is confined to images” --> Why is this true?  Certainly the referenced work focuses on images, but it is not obvious to me why it could not be directly extended to the type of data in medical records.  I am also not sure I agree that there are not reasonable/standard approaches for obtaining positive/negative pairs on other types of data.  Even if this were true, however, I would argue that it's not a downfall of Bukchin et al.  Recommend rephrasing or cutting this piece.\n\n“Also, since contrastive learning does not model subclasses…Sec .4).” --> At this point in the paper, it's not clear at this point how you support these statements in Sec. 4, and the claims may seem unwarranted.  Recommend either deferring this discussion or providing some quantitative previews of your results here to mitigate this.\n\n“It explicitly represents the unobserved subclasses as latent variables…” --> It may be worth clarifying here that you do not assume knowledge of the subclass identities.\n\nRelated Work:\n“…cannot trivially be extended to non-image data…” --> See comment above, I don’t think this is a great argument.  Recommend cutting or rephrasing.\n\n“…cannot leverage coarse training to guide their pseudo-labeling…” --> This claim is unclear. Coarse training explicitly does guide the pseudo-labeling in e.g. GEORGE (Sohoni et al. 2020), as clusters w/in the coarsely trained representation define the pseudolabels.  The argument about end-to-end optimization of the pseudo-labels/model parameters seems reasonable, but not sure this statement is accurate.  Maybe rephrase to indicate that a two-step approach could lead to suboptimal psuedolabeling rather than arguing that the coarse representation doesn’t contribute at all in these two-step methods.\n\n“These methods are unsupervised, and cannot be used…” --> Why does the unsupervised nature of these approaches mean they are not capable of application so this problem?  I found this part unclear…I suspect it may be a writing issue rather than a technical issue, but clarification would be appreciated.\n\nSuperclass-conditional Gaussian Mixture Model:\n\n“unseen superclass” --> What does an unseen superclass mean?  Is it an unseen subgroup that spans existing superclasses?  Or an OOD example?  Need a more precise description of what this means, recommend providing mathematical definition.\n\n“Unlike some previous works (Sohoni et al., 2020), which allocated a predefined number of subclasses to every superclass…” --> This isn't true, as far as I understand it.  Sohoni et al. perform a search over the number of subclasses w/in each superclass and optimize an unsupervised cluster quality metric.  One could argue that fewer searches are required in the method this paper proposes – because all of the searching over number of subclasses is baked into the r parameter – but I don’t think that argument is particularly compelling from a practical standpoint.  I would suggest cutting or softening this claim. \n\nFigure 1 --> Appears to be a typo in the caption, all of the subclass means have the same subscript (“1”)\n\n“…and tune \\sigma to adjust the relativity between super- and sub-classes” --> What range does this usually fall in?  Worth reporting this.\n\n“transport polytope” --> Many readers may not be familiar with this.  Recommend providing a brief description/background on this in the Appendix (and reference this in the text)\n\n“It is noteworthy…” --> This is an interesting set of observations.  Nit: typo in “contrasitive”\nExperiments:\n\n“visual granularity” --> What exactly does this mean?  A comment would be helpful.\n\n“20/6/8 splits” --> So the superclasses are disjoint in train/test/val?  That seems a little confusing.  Suggest clarifying\n\n“which adds an embedder” --> What exactly does this embedder do/why is it added? More representational capacity?  More detail would be welcome.\n\n“dim” --> “dimension”\n\nTable 1: Was there a statistical test done here to support improvements over ANCOR?  Seems to be overlapping confidence intervals in a couple of cases.  Please provide detail on how confidence intervals were computed and what statistical approach was used to determine superiority/non-inferiority.  Also, what's similar about the cases where the method outperforms and doesn't outperform ANCOR?  Is there any additional explanation that can be given here?\n\n“baselines on contrastive learning cannot be applied…\" --> see above comments, I don’t think this is a particularly strong argument.  I would cut it, or at least acknowledge that this is not a fundamental limitation of the contrastive approaches.  Augmentation schemes certainly exist for non-image data types.  The strongest response would run these baselines using data augmentation approaches for these other modalities.\n\nAppendix A:\n\nEq. (10) --> I am not following exactly how Jensen’s inequality results in the move from line 1 to line 2 here.  I could be missing something, but it would certainly help if the authors provided more explicit detail in the intermediate steps (e.g. laying out the terms in the inequality clearly – i.e. a_i and x_i here https://en.wikipedia.org/wiki/Jensen%27s_inequality).\n\n“the second step in Eq. (11)…” --> I think this is a typo…should be Eq. (10) referenced here?\n\n“\\lambda \\ge 1” --> Am I correct in understanding that there is strict equality for \\lambda = 1?\n\n“equal partition constraint” --> If I understand correctly, this statement deserves more discussion.  Does this imply that the constrained transport polytope formulation here effectively assumes that subclasses are of equal size?  This would appear to be consistent with Eq. (6), and if this is indeed true this would represent a substantial limitation of the method that should be further discussed.  On this note, what is the distribution of subclass sizes in the experiments run in this paper?  Are the subclasses all of similar size, or does it vary?\n\nEq. (13) --> the last equality/definition here seems a bit strange.  You remove the 1 constant, and define the term without the 1 as equal to the term without the 1?  Why is this true/necessary?\n\nAppendix B:\n\n“…robust learning rate…” --> Could be worth searching over robust learning rate for this baseline.\n\nAppendix C:\n\nTable 8 --> Suggest adding bolding to this table as well to maintain consistency with other tables.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper introduces a setup with the goal to adapt from a coarse pretrained model to unseen fine-grained labels. This problem is formulated as a superclass-subclass latent model and learned with maximum likelihood via expectation-maximization. The proposed approach, super-class conditional Gaussian mixture (SCGM) model, defines a hierarchical Gaussian distribution on the class hierarchy that models both the superclasses and subclasses. SCGM is evaluated on 6 image-net related datasets and a a real Dialysis-Event dataset collected by hospitals, and demonstrates competitive results under two evaluation setups: generalizing to seen and unseen superclass.",
            "main_review": "Strengths:\n1. The paper is well-motivated and addresses the important problem of few-shot multi-granularity adaptation.\n2. The paper includes comprehensive evaluation across various datasets and demonstrate the effectiveness and stability of the proposed approach.\n3. Visualization confirms the proposed approach learns the superclass-subclass structure.\n\nWeaknesses:\nThe main focus of the proposed approach is learning embeddings, but the evaluations are focused on classification. If the goal is classification, why not directly build a classification model? If the goal is learning embeddings, it would be better to put more focus on embedding based evaluation and clarify the relationship between embedding and classification performance.",
            "summary_of_the_review": "The paper introduces the interesting problem of few-shot multi-granularity adaptation through hierarchical Gaussian mixture and demonstrates its effectiveness across multiple datasets. The relation between the goal (embedding learning) and evaluation (classification accuracy) is not very clear.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}