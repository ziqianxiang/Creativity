{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "Exploration can happen at various levels of granularity and at different times during an episode,  and this work performs a study of the problem of exploration (when to explore/when to switch between exploring and exploitation, at what time-scale to do so, and what signals would be good triggers to switch). The study is performed on atari games.\n\nStrenghts:\n------------\nThe study is well motivated and the manuscript is overall well written\nStudies a new problem area, and proposes an initial novel method for this problem\nextensive study on atari problems\n\nWeaknesses\n--------------\nsome clarity issues as pointed out by the reviewers\nno illustrative task is given to give a more intuitive exposition of the \"when to explore\" problem\ncomparison to some extra baselines like GoExplore would have been insightful\n\nRebuttal:\n----------\nMost clarity issues have been addressed satisfactorily. It has been explained why some requests for extra baselines would be challenging/or not relevant enough. While the authors agree that GoExplore would be an interesting baseline, they seem to have not added it. An illustrative task was not provided.\n\nSummary:\n------------\nAll reviewers agree that this manuscript opens up and tackles a novel direction in exploration, and provides an extensive empirical study on atari games (a standard benchmark for such problem settings). While I agree with the reviewers that point out that this paper could have been made stronger by adding an illustrative task and additional baselines like GoExplore, there is a general consensus that the provided empirical study on this novel problem setting is a good contribution in itself. Because of this I recommend accept."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes to study exploration at different levels of granularity. Current methods either explore at the level of individual steps (e.g., \\epsilon-greedy), or at the level of experiments (e.g. first a reward-free exploration phase, followed by a task-dependent learning phase using the gathered data). This paper proposes to study exploration at the intra-episodic level, i.e. where the agent switches between exploration and exploitation within the same episode. \n\nThey discuss various design choices to perform exploration at this level, for example switching after a certain number of steps or with a certain probability, or switching based on the discrepancy between the predicted value and actual experienced value. \n\nThe experimental results show that including intra-episodic exploration gives a modest benefit over other exploration schemes when using an R2D2 base agent. Other insights are also included, which show that the proportion of exploration does change throughout the learning process, indicating that different degrees of exploration are useful at different stages. They also show that the informed switching component learns switching behaviors which are non-uniform throughout the episodes.",
            "main_review": "Strengths:\n- this paper investigates a novel area, which seems very important at a high level.\n- it also proposes novel methods to start to address this area.\n\nWeaknesses:\n- the paper does not feel very focused. There are lots of different ideas and methods presented, but the takeaways are unclear. \n- the figures are a bit hard to parse \n\nFirst, I applaud the authors for tacking a new and unexplored area. However, the takeaways and next research steps are unclear. At least on Atari, the benefits of intra-episodic exploration seem limited. \n\nSome suggestions for improving the paper:\n- It would be helpful to have the different algorithm variants in Section 2 spelled out, especially regarding the different switching mechanisms. The current descriptions are quite high-level and it would be helpful to make them concrete.\n- The tuple notation (shown in Figure 2) is a bit hard to parse. It would be helpful to show several examples, so that the differences in the different elements of the list can be seen more clearly.\n- I think the paper would be a lot stronger if there were some tasks which more convincingly demonstrated the benefits of intra-episode exploration. This could be a new task which the authors design themselves. I think including the Atari experiments is useful in that it shows their methods can improve performance on a standard benchmark, however, the monolithic exploration methods already work quite well on these tasks, and it is not clear if there is that much more improvement to be had by exploring at a finer granularity. I do agree with the authors that in the “big picture”, monolithic exploration is likely suboptimal and more informed exploration will be necessary. I found their motivating example of learning to ride a bike while maintaining necessary daily activities to be very helpful. Can you design some task which distills this task in a simpler format? For example, some setup where the agent must regularly find food from a predictable source (exploiting) but also must explore when it can between finding food. Introducing new tasks which measure the ability to optimally switch between explore and exploit modes would also be useful to the community in building on this work. \n\n",
            "summary_of_the_review": "I'm on the fence about accepting this paper. On one hand I think it is good that the authors are exploring a new and important area and the ideas are interesting. On the other hand, this work still feels preliminary and the benefits of intra-episode exploration are not yet convincingly demonstrated.  I am not strongly opposed to accepting this paper since it could at least be a starting point for research in this area. However I think that if the authors could introduce new tasks where intra-episodic exploration convincingly helps, then I think this would be a very strong submission to a later conference.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper studies switching between exploit and explore modes in reinforcement learning. It discusses switching mechanisms based on time (\"blind switching\") and based on state (\"informed switching\"). Studying seven Atari games, an empirical analysis of different switching mechanisms is performed.",
            "main_review": "Overall, the paper is very strong, well-motivated, and empirically sound. Most of my concerns are with clarifying details of the experiments and improving the exposition. These are listed below:\n\n- paragraph 2 of the introduction: the typical answer to \"when to explore\" is when the agent is unfamiliar with the environment or its structure, i.e., early in its interactions with the environment. I would highlight why the heuristic of \"explore earlier\" should be challenged here instead.\n\n- paragraph 3 of the introduction: it's not clear what the connection to schizophrenia here is, other than that this was a study of explore vs. exploit; are these individuals somehow impaired in choosing between these options, etc.?\n\n- paragraph 1 of section 2 (methods): not sure that the example of riding a bicycle works - is the targeted acquisition of a new skill really exploration?\n\n- section 2.1, description of \"episode-level\": it would be more useful to define \"episode\" in the context studied here, rather than using the example of training games vs. tournament matches.\n\n- caption to Figure 1: the differences for D-G are unclear just by looking at this figure and the caption, other than that they are different intra-episode approaches; it would be better to clarify this\n\n- section 2.2, description of \"blind switching\": this refers to \"fractional episode length\", but if we don't get to choose the length of an episode, how do we implement fractional episode length ex-ante?\n\n- section 2.3, description of \"bandit adaptation\": the citations should be in-text citations\n\n- section 3: it would be useful to explain the observed differences between these domains; are there hypotheses for why certain domains are better suited for different switching mechanisms? The discussion in Section 3.3 starts to get at this, but does not specifically address why the differences may arise.\n\n- section 3.2: in appendix A.3, the compute budget is mentioned as being 2B frames; please clarify this difference\n\n- section A.1, regarding using the full action sets: does this mean that some games have meaningless actions, i.e., actions that cause no effect in the world? how does this affect learning and why was this choice made?\n\n- section A.1, regarding no life-loss signal: what exactly is an episode here if there is no life-loss signal? Is it just 108,000 frames (and why this number?) How does scoring work?\n\n- section A.1: why was the choice to use raw, unprocessed frames made? in particular, what is the effect of keeping color information for learning?\n\n- section A.2: is 1 TPU used (as mentioned here) or 2 TPUs (as mentioned in Section A.3)?\n\n- section A.2: citations should be in-text citations; \",\" should be \";\" after Schaul et al. (2021) in paragraph 2, line 4.\n\n- section A.3: \"while 2 TPUs\" should be \"using 2 TPUs\".\n\n- The captions to Figures 9 - 15, especially 9 and 10, should be expanded to clarify what the reader can learn from this figure (e.g., any hypotheses on what accounts for the differences between different environments)",
            "summary_of_the_review": "The paper studies the relatively under-explored question of when agents should explore, introduces a novel exploration trigger called \"value promise discrepancy\", and performs a thorough empirical analysis in the domain of seven Atari games. Please see the main review for detailed comments and suggestions for improvement.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper investigates when to switch between exploitation and exploration and how long to stay in each exploration mode during RL learning. It proposes new ways to explore the subject, especially with intra-episodic exploration variants. It presents a large body of study results (10 pages of appendices!!), and concludes with very thought-provoking suggestions and discussions.",
            "main_review": "This paper conducts a series of experiments aimed at answering the question of when we should switch between exploitation and exploration during RL learning.\nAs positive points we can cite the wide related bibliography, from analogies with the animal system (human included) of exploring to other techniques such as the use of options. Another point to emphasize is the clear, careful and pleasurable writing that the authors developed in the paper. The authors managed to transmit a moment of reflection and expansion of thoughts about the possible behaviors shown by agents who learn with RL. Moments of reflection in an area that transforms at a very high speed are always welcome.\n\nPerhaps the negative point is the limitation of the contribution -- since it is still a study that must be deepened in order to provide effective and efficient guidelines for RL system developers working in real applications.\n\nHowever, it is an in-depth study, with interesting results. The article is worth being divulged to remind everyone working in the RL area that there is still a lot to study, investigate and evaluate in order to have robust, efficient and effective systems.",
            "summary_of_the_review": "The paper brings an in-depth study, with interesting results and very well written. It is worth being divulged to remind everyone who works in the RL area that there is still a lot to study, investigate and evaluate in order to have robust, efficient and effective systems.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not applicable.",
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a mode-switching strategy for the exploration/exploitation dilemma instead of monolithic behaviour policies in order to obtain more diverse behaviour. Different granularities for the timing of the switches as well as different switching mechanisms are investigated (blind vs. informed switching). The focus for exploration is not on how, but when. For exploration, they both use Random Network Distillation (RND) as well as a uniform policy. Their experiments are conducted on the Atari Learning Environment (ALE), where they provide performance and diversity results.",
            "main_review": "The idea of mode-switching is interesting. However, the presentation of the motivation as well as the results seem somewhat weak in my view. The main comparison baseline for the mode-switching architecture is the monolithic variant, where typically sparse rewards in hard exploration tasks are augmented with intrinsic reward signals. This means that the modes that they switch between in this paper are merged homogeneously in time. A valid problem that the authors point out for the monolithic case is that the scale of the intrinsic reward signal has to be tuned and may need to change in time. But there is no comparison to these methods and the superiority of their method to the monolithic variants are not highlighted well enough.\n\nThe authors try to circumvent the performance comparisons with other baselines by saying that they can obtain more diverse behaviours (in terms of exploration strategies?) and say that they don’t aim to show improved performance. However, this diversity argumentation is not strong enough in my view. First, it is not clear to me if the authors want to focus on the diversity of behaviour that is obtained within one variant of the mode-switching, e.g. informed switching, or whether they want to highlight the diversity across different granularities and switching mechanisms. If it is the latter, the diverse behaviour doesn’t necessarily translate into performance for the different games. And best performance for different games might be obtained with different strategies, but since this strategy is fixed prior to the experiment, it is not clear to me how this helps the diversity argument of the method itself. The authors point out that Montezuma’s Revenge and Phoenix have their best performance with different mode-switching behaviours, but these results have a really high variance. Perhaps, 3 seeds are just not enough and more seeds are needed to draw reliable conclusions.",
            "summary_of_the_review": "I believe that the idea presented in this paper is interesting but the results are lacking. The related work section briefly covers some similar methods, and I think comparisons are still needed with these other methods. E.g. GoExplore also focuses on the *when* question of exploration and should be included as a baseline as well as works with monolithic behaviour policies where the mode-switching is replaced by a weighting problem of external and intrinsic rewards of a single behaviour policy.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}