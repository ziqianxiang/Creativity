{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "The paper investigates the use of equivariant neural network architectures for model-free reinforcement learning in the context of visuomotor robot manipulation tasks, exploiting rotational symmetries in an effort to improve sample efficiency. The paper first provides a formal definition and theoretical evaluation of a class of MDPs for which the reward and transition are invariant to group elements (\"group-invariant MDPs\"). It goes on to describe equivariant versions DQN, SAC, and learning from demonstration (LfD). Experiments on a set of different manipulation tasks reveal that the proposed architectures outperform contemporary baselines in terms of sample complexity and generalizability, while ablations demonstrate the contribution of the different model components.\n\nThe idea of structuring a neural architecture to exploit symmetry present in a domain as a means of improving sample complexity is compelling and principled. The contributions of the paper are two-fold. First, while the idea of exploiting symmetry in the context of deep RL is not new, the paper describes a variation of equivariant DQN that is effective for visual control domains (visuomotor control) that are more challenging and realistic than those considered previously. Second, the paper proposes novel equivariant versions of SAC and LfD and validates their effectiveness through extensive experiments. Following a detailed author response to the initial reviews together with the inclusion of additional experiments and other updates to the paper, the reviewers largely agree on the significance of these contributions and value of the paper as a whole."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper explores to integrate equivariance deep learning to robotic applications. The authors propose two main contributions: 1) define and theoretically characterize an important class of group-equivariant MDPs, 2) and integrate equivariant variations to DQN, SAC, and LfD. The paper provides many different sets of experiments. The results are promising which shows benefits of the proposed approach.\n",
            "main_review": "Strength:\n\n- Sound theory of group-invariant MDP. The theory also characterizes both the invariance and equivariance cases.\n- Applications on realistic robotic manipulation\n- Showcasing on different RL (SAC and DQN) and robot learning (LfD) algorithms.\n\nWeakness:\n- The description sometimes is hard to follow\n\n\nIn general, the paper pursues an interesting research problem. it's well written. The proposed idea address the main well-understood challenge in robot learning. I would have only following comments.\n\nRegarding to the invariant-MDP, the proposed approach only deals with visual state spaces, so is it possible to extend to include proprioceptive information in the state space?\n\nIs the group element $G$ pre-defined and does it contain only a single rotation operator $g$ as defined in Experiment? If so would it be too simplistic? More ablation studies regarding this choice would be more helpful.\n",
            "summary_of_the_review": "The paper proposes interesting ideas that might be useful for robot learning tasks. Experiment results are positive. There are some questions as raised in the above section.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper uses rotation equivariant CNNs for model-free reinforcement learning. More specifically, it is argued that for robotic manipulation tasks the corresponding MDP is invariant under translation and rotation, and therefore one could more efficiently learn equivariant/invariant policy/value functions. This idea is applied to both DQN and SAC for control with finite and continuous action spaces respectively. Experimental results on several tasks demonstrate substantial improvement over the competing methods.\n",
            "main_review": "I found the paper to be clearly written and generally well-executed. The main strength of the paper is the experimental results which are very supportive in a range of different tasks. Its main weakness is lack of novelty, on the theoretical side. This is because, as noted by the paper itself, equivariant networks have been used for learning equivariant policies and invariant Q functions in previous work. However, given the importance of this domain, I believe the focus on this group and proposed methodology still has interesting contributions (e.g., in learning from demonstrations).\n\nQuestions/comments:\n \nProposition 4.1 is not new, and proper references should give credit to early works on symmetric MDPs (e.g., “Symmetries and Model Minimization in Markov Decision Processes”)\n\nOn “preventing critic from becoming overconstrained”: while I understand the proposed argument, it doesn’t seem valid to me, since you have the option of having many equivariant layers before the scaled mean pooling operation. While using max pooling instead of min pooling is fine, the argument seems tangential. Any comments?\n\nSince the results are surprisingly good compared to the model that replaces C8/C4 equivariant layer with ordinary convolution, could you please also provide the results for C2 equivariant layer? Such a layer would only have a factor of 2 improvement over the CNN (in data-efficiency) and therefore it would help make sense of the results.\n",
            "summary_of_the_review": "see above.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper develops equivariant architecture for RL, specifically DQN and SAC. The theoretical exposition is general and a specific SO2 instantiation is shown to outperform baselines on image based RL tasks.\n",
            "main_review": "Strengths\n+ Very clearly motivates sample efficiency + generalization and articulates inductive bias from the perspective of adding equivariance to the model vs doing data augmentation.\n+ Results do show better data efficiency compared to baselines.\n\nWeakness/Comments\n- Results don't get into generalization but the claims/motivation in intro and related work leave the impression that this axis would be investigated,\n- The exposition of the approach is a bit hard to parse and sprinkling in some intuition and ground things to the application would be helpful.\n- What is the input? Is the kinds of images shown in Fig 5 or the depth map in Fig 2b? Some of the assumptions for this depth map don't make sense from a real application perspective: (i) if a depth camera is mounted near the robot end-effector it won't be possible to get the image in base frame, if it is overhead mounted then there will be occlusions so the object may not be always visible; (ii) depending on the task the gripper will often partially/fully occlude the object -- how much is this a problem? (iii) both Fig 5 and 2b are very idealized and in practice many other visual distractors would be present.\n- The claim in Sec 7 about \"transfers easily to real-world environments\" is not well supported and given the points above (and possibly other) this is in fact going to be quite challenging. A more nuanced coverage of limitations would be helpful to constraint where this method works and where it doesn't, rather that surface level limitations of learning in simulation and delta state based action.",
            "summary_of_the_review": "Bringing inductive bias to mostly unstructured policies is useful for RL. The results are promising, but some claims are overblown or not well supported.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper defines and theoretically characterizes a class of group-equivariant MDPs and studies its invariance and equivariance properties. It introduces two new models, one for discrete action spaces (equivariant DQN) and one for continuous action spaces (equivariant SAC). It compares the performance of the proposed methods against strong competitive baselines for multiple robotic manipulation tasks and establishes their superior performance. ",
            "main_review": "Strengths:\n\n1. The proposed method and propositions are backed by solid theoretical proofs. \n\n2. The claims made about the proposed method are validated through extensive experimentations and comparison against strong baselines.\n\n3. The related works are well-covered. This paper is novel in its identification of equivariant properties of optimal Q-functions and optimal policies and its application to problems in robotics such as object manipulation (as opposed to previous applications to toy settings). It shows the improved performance of an equivariant DQN on these tasks and introduces an equivariant SAC model. It is closely related to Wang et al. (2021) which focuses on the translational and rotational invariance of Q-functions whereas this paper focuses on group equivariance. \n\n4. Overall presentation of the paper is good\n\nWeaknesses:\n\n1. Is the claim of proposing an equivariant version of DQN as a novel contribution correct? The authors reference Mondal et al. (2020) which claims to introduce equivariant DQN. Is what the authors proposing different in nature from Mondal et al. (2020)? The two works seem to use different symmetry groups (this paper talks about SO(2) and Mondal et al. talks about E(2)) but this and any/all other differences need to be better highlighted in the paper. \n\n2. The improvement in the reward shows the ability of the Equi DQN and Equi SAC to faster learn the policies but can it be ensured this is because of the inductive bias of the model architecture as claimed? One suggestion is to include a generalization experiment to test the robustness to equivariance where vanilla DQN fails. Additionally, the equivariant models are said to be applicable to rotationally symmetric problems. How would they perform in other problem settings without the symmetry assumption/ in the presence of other types of symmetry? Would they still beat the vanilla models? \n\n3. In the experiment section, the baseline CNN DQN only says “DQN with conventional CNN instead of equivariant network”. Is there a reason why the Equi DQN and baseline DQN have different numbers of convolutional layers and is this a fair comparison? What is the significance of the extra two fully-connected layers? Does this mean the Equi DQN needs more parameters to learn compared to the baseline DQN. If so, then there should be a baseline DQN that has the same number of parameters.  \n\n4. The paper has minor grammatical errors/omitted words. A careful proof-reading should be helpful. \n\n",
            "summary_of_the_review": "The paper has an interesting premise about the advantage of encoding symmetry as an inductive bias in the data. It is novel in terms of introducing equivariant SAC but the claim of introducing equivariant DQN is ambiguous as it has already been introduced in previous work (Mondal et al. 2020).  Their experiments show improved performance of equi DQN and equi SAC over other methods for rotationally symmetric problems in robotic manipulation (which is different from previous works).  Although their results look very promising,  their claims may be better supported by additional experimentation  such as evaluating the performance of the proposed models in non-symmetric environments. Additionally, in order to ensure fair comparison, their equi DQN needs to be compared with a baseline DQN model with the same number of parameters. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "In this paper, the authors present an application of equivariant neural networks to reinforcement learning. They demonstrate new DQN and SAC architectures that make use of equivariant representations in order to improve sample efficiency. Results on manipulation tasks demonstrate sample complexity reduction compared to other techniques. ",
            "main_review": "Although I liked the paper, I would like to kindly ask the authors the following questions: \n\n1. The authors have provided a definition of G-invariant MDPs and demonstrated their effectiveness in visual tasks. This definition rather introduces restrictions on the class of MDPs considered. My question is apart from visual tasks like those solved in this paper, can the authors provide examples and intuitions into when those assumptions hold? Is it the case that G-invariant MDPs are exclusive to image state representations? \n\n2. Similar question as the above goes for the action spaces. What assumptions are we really making on the reward and transition functions? Do these relate to Lipshitzness at all? Can we handle collision-avoidance scenarios for example? \n\n3. In the experiments, the authors have considered relatively low-dimensional action spaces. Can we please see a demonstration in higher dimensional action spaces? Would the method apply to ATARI domains as well? Or is it that the type of discontinuities present in ATARI prohibit the application of the proposed method? It would be great to have a section discussing the limitations in terms of MPDs that could be considered when using the current approach. \n\n4. Although a minor point, I am a bit confused about the novelty of the current approach beyond applying equivariant architectures to reinforcement learning. Can the authors please help me understand the major contributions of this work? ",
            "summary_of_the_review": "Please see above.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}