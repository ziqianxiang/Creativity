{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "The paper provides a method for with tuning continuous hyperparameters (HPs). It is closely related to a previous work (Lorraine, 2019) that was limited to certain HPs, and in particular could not be applied to HPs controlling the learning such as learning rate, momentum, and are known to be influential to the convergence and overall performance (for non-convex objectives). \nThe reviews indicate a uniform opinion that the paper tackles an important problem, that its methods provide a non-trivial improvement over previous techniques and in particular those of (Lorrain, 2019), and that the provided experiments are extensive and convincing. The initial reviews had several concerns about technical details in the paper such as the analysis or how the meta-hyperparameters are tuned. However, in the discussions the authors provided adequate responses, resolving these concerns. I believe that with minor edits that are possible to get done by the camera-ready deadline the authors can incorporate their responses into the paper making it a welcome addition to ICLR."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper tackles the hyperparameter optimization problem with a one-pass approach that alternatively optimizes over machine learning model parameters and hyperparameters. To optimize over a bilevel optimization problem, this paper generalizes Lorraine et al. (2019) by replacing the gradient update of parameters with a more general parametric form, thus allowing the hyperparameters to be updated alongside parameters by gradients coming from truncated Neumann series. ",
            "main_review": "**Strengths:**\n\n- The technical deductions look correct (although I did not check every line).\n- The authors are candid about the extent of their contribution and properly cited relevant literature.\n- The writing is overall clear. A number of clarifications and illustration figures were provided.\n- There are abundant experiments, with many comparisons among different methods and ablation studies of the proposed method.\n\n**Weaknesses, and questions to authors:**\n\nDespite the above strengths, there are some vague parts in the current version. I would raise my score if the authors could properly address the questions, and would suggest the authors incorporate them into the revision.\n\n- As (honestly) claimed in Section C.4, the technical advancement from Lorraine et al. (2019) seems incremental: In the framework of Lorraine et al. (2019), $\\mathcal{L}_T(\\lambda, w)$ could still be parameterized by optimization hyperparameters in the framework of the previous work, so that $u(\\lambda, w)$ could be expressed as a closed form. \nThe authors may want to provide more details around Algorithm 1 or Section 2.5 on how $u(\\lambda, w)$ is computed, and show a table of  examples on simple tasks (e.g., SGD on ridge regression) to clarify what $u(\\lambda, w)$ would look like, so that we can see why it is different from $\\eta \\partial \\mathcal{L}_T (\\lambda, w)/ \\partial w$. \n- The mean and median performance in Table 1 and 2 may not be quite meaningful in the hyperparameter tuning context: for the methods that do not optimize over optimization hyperparameters, “Best” instead of “Mean” or “Median” may be more interesting to practitioners. The 200 random initializations can be regarded as candidates selected in random search of hyperparameters, and users would want to pick the one with the smallest error (Of course, the small variance is still a plus). Are there examples of the best models (whose performance are in the “Best” column) found by different methods, so that readers can see how different they are? And, is the proposed method stable in terms of finding more similar models from different initializations? This type of information helps to show that learning rate tuning really matters on these datasets, since the best test MSEs are often achieved by other methods.\n- Why would a large $i$ be a problem for Algorithm 1 (as described in Section 4.3), when the weight $w$ is fixed in the “for j in 1 to i” loop? In fact, the first approximately equal sign in Equation 6 would become an equal sign here. \n\nAlso, the link to the source code points to an empty GitHub account.",
            "summary_of_the_review": "This paper overall does a good job in presenting the technical contribution with deductions, pseudocode and experiments. Ambiguities do exist; I would encourage the authors to address them, so as to make the paper more readable. \n\n********* Updates after the discussion period *********\n\nThis paper is a good paper. I have increased my score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "Authors propose an extension of Lorraine 2019 to handle hyperparameters that control the convergence speed of the model: Lorraine 2019 \"only\" handled hyperparameters that were part of the model.\n\nThe extension seems a little incremental, but the paper is well written (except for the theoretical part), code is provided, the bibliography is quite extensive, and experiments show interesting improvements.\n\n",
            "main_review": "Comments:\n1- \"Many machine learning methods are governed by hyperparameters: quantities which, unlike model parameters or weights, control the training process itself (e.g. optimiser settings, dropout probabilities)\"\nI am not sure I fully agree with this sentence. I am not an expert, but from what I understand, the dropout parameters have a direct influence on the model: different dropout probabilities leads to different models, whereas different learning rates should yield the same results (with different time of convergence).\nDo authors agree on this? Or did I miss something?\n\n2- In my opinion part 2.1, 2.2, and 2.3 and too handy, the theoretical result should be encapsulate in one proposition.\nWhat do authors mean by the approximation sign in (7)?\n\n3- I would say that the proposed algorithm is a \"one-pass\" algorithm.\nIn algorithm 1 there are somehow hidden nested for loops with the while + for t in 1 to T. It seems that authors are not proposing a \"one-pass\" algorithm, but more a warm-restart algorithm.\nAuthors themselves wrote: \"We emphasise training is not reset after each hyperparameter update — we simply continue training from where we left off, using the new hyperparameters. Consequently, Algorithm 1 avoids the time cost of multiple training restarts\".\nDo authors consider changing the name of the paper? Maybe something in the idea of \"Implicit differentiation to jointly tune model-based and non model-based hyperparameters\" ?\n\n4- Figure shows that the learning rate seems to converge toward a value around $1O^-1$, but the weight decay does not seem to converge. Do you think that the weighted decay is not converging? Or may the stepsize is too large for the weight decay? Could authors comment on that?\n\n5- Figure 4.b is extremely interesting. Just a question to be sure, it is written 400 hyperparameter updates, does it mean that the time budget for each \"square\" is different?\n\n6- In the experiments I would like to see the exact formula of the optimization problem which is solved (either in the main paper or in appendix). In my opinion this would make the paper clearer.\n\n7- What does \"myopic convergence to the local minima\" means?\nI a more general way, I my opinion, authors make a lot of statements that could be described as \"hand wavy\", for instance\n\"Similarly, we suppose a ‘momentum’ could, at every point, locally *squash the loss surface in the negative-gradient direction* and stretch it in the positive-gradient direction.\nIn aggregate, these transformations straighten out optimisation trajectories and *bring local optima closer to the current point*.\"\nThe word \"interpret\" is present 34 times in the paper: this contrasts with the lack of theoretical results.\n",
            "summary_of_the_review": "In conclusion I vote for week rejection because the theoretical part is too hand wavy and IMO the paper cannot be published right now. In particular, IMO, the name of the paper should be changed.\nHowever, except this part, the paper is well written, and experiments are very interesting. If authors take into account the comments, I will raise my score.\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper is concerned with tuning continuous hyperparameters of a neural network setup (model, loss function and optimizer parameters) by way of a cheap approximation to the hypergradient. It is closely related to earlier work (Lorraine, 2019), which however overlooked that the idea can be generalized to optimizer parameters (e.g., learning rate, momentum), which is effectively done here. In a range of well-done experiments, the method is shown to very convincingly outperform (Lorraine, 2019), and in some cases even the \"gold standard\" approach to differentiate through the whole learning trajectory. Code is provided.\n",
            "main_review": "This paper starts from (Lorraine, 2019) and that it can be generalized in a rather straightforward manner. Crucially, this generalization allows to tune optimizer parameters as well, such as learning rate and momentum. It is well known, and acknowledged by the original authors, that (Lorraine, 2019) is very sensitive to the correct choice of the step size eta, and the current work claims that by including this parameter into the optimization, these problems can be circumvented. At least in the experiments presented here, this is very convincingly the case. The paper certainly excels at fixing issues with (Lorraine, 2019). I am not so sure it adds very much on top of that.\n\nFirst, while the experiments cover a good range of different setups, they are a little set up to make the proposed approach look good compared to (Lorraine, 2019). Only 3 HPs are optimized over, 2 of which are optimizer parameters which the other method cannot tune, and the authors just leave them fixed. These problems do not seem to be very hard, given that the simple baseline of training 3x with random initialisations and picking the best is very competitive on all of them. And why only go for 3 HPs if you can compute a gradient? I have comments on the experiments below.\n\nTo me, an important question is what this work adds to the HPO state of the art in general. (Lorraine, 2019) is obviously quite limited, but so is the current work and all gradient-based HPO. What do you do with integer or categorical hyperparameters? How do you make use of parallel computations (which is key in practice)? How do you support modern optimizers like Adam or AdaGrad, which are not just a simple equation like (4)? Most importantly, how do you ensure robust behaviour, and in particular avoid catastrophic failures which hypergradient approximate methods are prone to? I'd love the paper to make some statements, and in general tell the reader what is done more here than fixing (Lorraine, 2019).\n\nFor a start, I'd love a much better explanation of what you say in 2.5, which I did not fully understand, including Figure 2. What you say here seems to be key to explain the advantages over (Lorraine, 2019). Can you underline this with some analysis and experiments?\n\nNext, is there a way to make your approach \"one step more accurate\"? Right now, you just do the same as (Lorraine, 2019) in terms of the assumptions (which are strong). Why not for example take w_i and w_{i-1}, then assume that derivs at w_j, j < i, are all equal to derivs at w_{i-1}, and that w_i -> w_*? Is that still tractable? Do you need new interesting tricks to make it work? This would add something to the machinery of (Lorraine, 2019).\n\nFinally, more comments on the experiments. They do serve to show that the current approach works better than (Lorraine, 2019) and another hypergradient approximation, on a setup which mostly tunes optimizer parameters. But I feel they do not do much more. First, overall running times are missing, so it is nearly impossible to see what the results mean. Nowadays, competitive HPO experiments show curves with wall-clock time on the x axis.\nMore importantly, an obvious baseline is missing: population based training (PBT). This is also tuning models while training them, and is strictly a constant factor more expensive than training a model once. This factor is spent on parallel compute. PBT is implemented in Ray Tune and should work out of the box. If you claim your method costs 3x more than one training, just use 3 parallel workers. Another meaningful baseline would be ASHA (https://arxiv.org/abs/1810.05934), also available in Ray Tune. You can run it with a time cutoff of about 3x a single training run.\n\nFor gradient-based HPO, there are probably two ways forward. Either (a) show that a method can work robustly and better than something simple like asynchronous Hyperband (ASHA) or PBT (see below), none of which are even close in methodological complexity, work in general settings. Or (b), find a meaningful setup which has a lot of HPs (they need to be continuous, though), argue why it is important, and then apply gradient-based HPO, because other methodology will not apply. But if you evaluate your gradient-based approach on simple setups like the ones used here, with 3 HPs, then you really have to do it in a way that includes non-gradient-based SotA HPO methods.\n",
            "summary_of_the_review": "This work provides a simple extension of earlier work (Lorraine, 2019), which ends up being rather important, since the former simply does not work very well at all on standard HPO problems. However, the work does not do much more than that, It stays methodologically very close to this earlier work, and does not really address any of the shortcomings of gradient-based HPO.\n\nThe experiments are well designed to show superiority over (Lorraine, 2019) and other gradient-based HPO, but lack comparison to relevant other HPO baselines (ASHA, PBT), and results are not presented in terms of time spent, so they are hard to interpret.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper aims to accelerate HPO optimization by allowing for retraining when new hyperparameters are proposed, while being faster and more flexible that existing hypergradient-based one-pass methods. This results in a novel HPO one-pass technique that works with any continuous hyperparameter within a differentiable model weight update, such as learning rates. In a good range of experiments, the authors show that their proposal is faster than baselines.",
            "main_review": "**Clarity**\n\nThe paper is well written and generally easy to follow. It also includes useful visualizations, such as in Figure 1, that give the reader good intuition on the proposed derivation. It also fits the work well in the context of related work both on standard and one-pass HPO techniques (e.g., Lorraine et al. (2019)).\n\n**Reproducibility**\n\nNot only do the authors carefully provide details on the baselines, benchmarks, dataset sources, compute machines used and experimental setup, but they will also open source their code to reproduce the experiments upon acceptance. While the authors have linked to an anonymous repository, code is not visible yet. It would have been ideal to release that already, but apart from that the work meets a good bar in terms of reproducibility.\n\n**Technical**\n\nThe work is well evaluated through experiments, which are performance on a large number of repetitions and illustrated with properly defined error bars. The proposed approximations are also theoretically grounded and converge to the ground truth best-response Jacobian as the size of the look-back window grows.\n\nOne weakness of the proposed approach is that it relies on a set of meta-hyperparameters, which the authors acknowledge. Ideally, their setting should be make automatic. In the absence of that, how would the author suggest setting them on a new problem? Is tuning on validation set required, or do they recommend some default values / can they give guidance on how to set them?\n\nRelated to that, I would have liked some clarifications on the following statement \"As some settings habitually chose unstable high learning rates, we clip these to [10−10 , 1]\" Why is that the case that unstable learning rates are used? Is this clipping a general recommendation?\n\nFurther, it was unclear to me why WD+HDLR+M would not perform well against the scalar baselines. The authors hint to learning rate regularisation for the high-dimensional dynamics setting, but some early results on that front would have further strengthened the paper.\n\n**Minor**\n\nA few of the listed references are pointing to arXiv when the respective paper was also published at a peer-reviewed venue. I recommend the authors to double-check those and update accordingly.\n\n",
            "summary_of_the_review": "I am inclined towards acceptance, only weakly so at this stage as guidance on setting the introduced meta-hyperparameters could be improved and the unexpected results obtained with WD+HDLR+M are lacking follow-up experiments. That said, this is overall solid work which is well-evaluated both theoretically and empirically.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}