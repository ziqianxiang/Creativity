{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper has a deep analysis of the over-smoothing phenomenon in BERT from the perspective of graph. Over-smoothing refers to token uniformity problem in BERT, different input patches mapping to similar latent representation in ViT and the problem of shallower representation better than deeper (overthinking). The authors build a relationship between Transformer blocks and graphs. Namely, self-attention matrix can be regarded as a normalized adjacency matrix of a weighted graph. They prove that if the standard deviation in layer normalization is sufficiently large, the outputs of the transformer stack will converge to a low-rank subspace, resulting in over-smoothing.\n\nIn this paper, they also provide theoretical proof why higher layers can lead to over-smoothing. Empirically , they investigate the effects of the magnitude of the two standard deviations between two consecutive layers on possible over-smoothing in diverse tasks.\n\nIn order to overcome over-smoothing, they propose a series of hierarchical fusion strategy that adaptively fuses presentation from different layers, including concatenation fusion, max fusion and self-gate fusion into post-normalization. These strategies reduce similarities between tokens and outperforms BERT baseline on a few datasets (GLUE, SWAG and SQuAD).\n\nOverall I agree with reviewers that this is a good contribution."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper explored the over-smoothing problem in BERT from the perspective of graph. In detail, empirical analysis first has been provided for demonstrating over-smoothing exist in BERT. Then, the authors theoretically prove the observation through the comparison between self-attention and graph. The authors claim that \"layer normalization plays a key role in the over-smoothing issue, namely, if the standard deviation of layer normalization is sufficiently large, the output of Transformer stacks will converge to a specific low-rank subspace and results in over-smoothing\". To alleviate the over-smoothing issue, layer fusion methods (which have been widely explored in previous studies) have been presented and verified to be effective in downstream tasks. However, the over-smoothing problem is neither well theoretically analyzed nor empirically proved to be addressed via layer fusion methods. ",
            "main_review": "Pros:\n1. Addressing the over-smoothing problem in BERT is a promising direction, which will further the research on the study of transformers.\n2. The theoretical analysis on the over-smoothing problem in BERT from the graph perspective is new and provides interesting findings. \n\nCons:\n1. The authors claim that they propose three layer fusion methods which have been explored in previous methods in my understanding. The authors should carefully claim that the fusion methods are \"proposed\" in this paper. A sufficient survey should be conducted on layer fusion of BERT.\n2. The over-smoothing problem is neither well theoretically analyzed nor empirically proved to be addressed via layer fusion methods. \n\nQuestions:\n1. Is it possible to theoretically prove that layer fusion can alleviate the over-smoothing problem like Section 5?\n2. Is it possible to empirically verify the layer fusion methods' ability to address the over-smoothing problem on more datasets. In fact, I notice that Fig. 6 has proved cosine similarity between BERT and BERT(self-gate) on three datasets. However, the paper should provide more experimental setting information. In my understanding, since \"the last layer is replaced with the aggregated representation\", should the cosine similarity of BERT(self-gate) at the 12th layer be lower than some of the layers before (since the last layer is the aggregated representation of all the 12 layers)?",
            "summary_of_the_review": "A marginally above the acceptance threshold is given. Overall, this paper addresses the over-smoothing problem in BERT. Theoretical analysis on the over-smoothing problem in BERT from the perspective of graph is new and provides interesting findings. However, proposed fusion methods are not that novel and not well theoretically and empirically proved to be able to address the over-smoothing problem.  ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper discusses the oversmoothing in transformer models such as BERT. It provides a theoretical analysis on the existence of oversmoothing in transformers and proposes a hierarchical fusion method as a solution to oversmoothing. ",
            "main_review": "Pros:\n\na) The theoretical analysis on BERT is a good contribution. The analysis methods is not completely new but applicable to analysis on transformers.\n\nCons:\n\na) Experimental results shown in  Table 1 and Table 2  do not show any significant improvement to overcome oversmothing. Most of the proposed method only give a small improvement which let me ask the question whether proposed fusion method is good effective enough for oversmoothing. Can authors provide further experiments with different datasets or applications to demonstrate the effectiveness of the proposed method?\n\nb) Lack of theoretical support for the proposed fusion method. Is it possible to theoretically demonstrate that the fusion method overcome oversmoothing? ",
            "summary_of_the_review": "The analysis of oversmoothing in transformer is a good contribution. The limited empirical evidence and lack strong improvement over oversmoothing by the proposed method are the main limitations of this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The current manuscript explored the token over-smoothing problem in Bert transformer blocks. Empirically, the authors showed that the token over-smooth problem is increasingly severe in deeper layers of a Bert model. The token over-smoothing is potentially harmful to task performance. As the over-smoothing problems have been discussed in graph networks extensively, the authors established a connection between transformer blocks with graph networks and looked at the over-smoothing problem of transformer blocks analogously. The authors revealed that layer normalization in the transformer blocks contributed to the over-smoothing issue both theoretically and empirically. Moreover, the authors proposed to improve the over-smoothing issue by hierarchical fusion strategies, that is utilize token representation from earlier layers. Overall, the author applied a novel approach to shed light on an insufficiently investigated problem. The paper was well written.   ",
            "main_review": "•\tThe authors have provided sufficient deductions to establish the theory of token over-smooth in deeper transformer layers. I feel convinced that layer normalization could lead to the over-smoothing issue. One question, it is not obvious to me that how theoretically fusion strategy would help prevent token over-smoothing. Could authors discuss the question in terms of v in equation 8? How does the distribution of the layer normalization term look like in the fusion models (replicate figure 5)? A further question is whether the fusion strategies improved the over smoothing issue by reducing the layer normalization relatively. \n\n•\tThe second question is about the contribution of self-attention block and feedforward block to token over-smoothing. The theory in section 5 considered both the self-attention block and the feedforward block, and the layer normalization of both blocks seem to be equally contributed to token representation smoothing. However, the shared attention test in figure 3 seems to suggest a dominant role of the self-attention block. I’m wondering how authors interpret this. ",
            "summary_of_the_review": "Overall, I vote for accepting this paper. I like the idea of establishing an analogy between self-attention and the adjacency matrix of a graph model, and I like the theory that establishes the relationship between layer normalization and dimensionality reduction of token representation. My major concern is about the clarity of some parts of the paper and some additional improvement. I hope the authors could address my concerns in the rebuttal. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper tries to understand the over-smoothing phenomenon of Transformer-based models such as BERT. The analysis is from the perspective of viewing BERT and Transformer as graph neural networks. By analogy and analysis of graph neural networks, authors show some theoretical analysis and find that layer\nnormalization plays a key role in the over-smoothing issue. Specifically, if the standard deviation of layer normalization is sufficiently large, the output of Transformer stacks will converge to a specific low-rank subspace\nand results in over-smoothing. Then to alleviate the over-smoothing problem, authors\nconsider hierarchical fusion strategies, which combine the representations from\ndifferent layers adaptively to make the output more diverse.",
            "main_review": "This paper tries to understand the over-smoothing phenomenon of Transformer-based models such as BERT. The analysis is from the perspective of viewing BERT and Transformer as graph neural networks. By analogy and analysis of graph neural networks, authors show some theoretical analysis and find that layer\nnormalization plays a key role in the over-smoothing issue. Specifically, if the standard deviation of layer normalization is sufficiently large, the output of Transformer stacks will converge to a specific low-rank subspace\nand results in over-smoothing. Then to alleviate the over-smoothing problem, authors\nconsider hierarchical fusion strategies, which combine the representations from\ndifferent layers adaptively to make the output more diverse.\n\nAn accept has been recommended for this paper for the interestingness of the topic and the novel perspective of theoretical study of BERT by graph neural networks.  The over-smoothing study with layernorm via graph is useful to both BERT, optimization, and graph community. This will inspire some new directions for researchers and hence an accept is recommended.\n\nAuthors however need to address several concerns in the review. \n1. If LayerNorm cause issue, Why does it help then? What do you suggest to replace it, e.g., will using prenorm by Di He et al resolve the issue?\n\n2. The concat and max fusion has appeared in the graph paper that author cited. It's interesting to see this can help BERT as well and in that sense this is novel but authors should clearly indicate where this has appeared before as well.\n\n",
            "summary_of_the_review": "An accept has been recommended for this paper for the interestingness of the topic and the novel perspective of theoretical study of BERT by graph neural networks. This will inspire some new directions for researchers and hence an accept is recommended. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}