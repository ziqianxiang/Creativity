{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "All reviewers have converged to an unanimous rating of the paper, highlighting, in the paper or during the discussion, many strengths, including a compelling approach clearly relevant to applications and its solid range of experiments. \n\nA clear accept, and I would encourage the authors to push in the final version the experiments and discussions following the threads with reviewers (in particular, Vo8C and ULvk).\n\nThanks also to authors and reviewers for a thorough discussion which helped to strengthen further the paper's content.\n\nAC."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This work provides an algorithm to ensure robust training of an ML model with just black-box knowledge of it i.e., input and output access. The algorithm relies on using Denoised Smoothing with zeroth-order optimization where the gradients are estimated using random perturbations (finite-differencing). They avoid the computational burden and high variance of these estimates by first training an auto-encoder to reduce the inputs to a low-dimensional subspace. They show over different architectures and multiple datasets (Cifar10, STL10, image reconstruction over MNIST) that the proposed algorithm performs better than the baseline which is the Denoised Smoothing algorithm by Salman et al.",
            "main_review": "Strengths:\n- They show comparison with the baselines over a sufficiently wide range of experiments.\n- The argument for the use of the Autoencoder is intuitive and clearly works. The algorithm itself is simple, building on tools that have been known to work.\n- The algorithm itself is truly black-box in that it does not require knowledge of the model that is being robustified.\n\nConcerns/Comments:\n- Though it is fair that the algorithm is only compared against other black-box algorithms, it would be useful to show the certified accuracy(CA) of a whitebox algorithm like (Cohen et al., Madry et al.) as a comparison. This would illustrate the gap between the two regimes.\n- While the autoencoder reduces the variance of the gradient estimates, the fact that $2q$ forward passes need to be made for every gradient computation, should indicate that turning this method into a ZO-method increases the computational burden. I did not see this discussed in the paper. A comparison would be useful.\n- I believe there is a typo on page 4 where $\\mathcal{A}_2$ is used when it should be $\\mathcal{R}_2$. Either choice of nomenclature is fine as long as it is consistent.\n- Is there any justification for choosing $q=192$ in the experiments on Cifar10?\n- The authors mention that pre-training $D_{\\theta}$ causes it to get stuck at a local optima. However, this doesn't seem to happen to $\\theta_{ENC}$. I was also curious to see if any experiments were conducted where the autoencoder was fixed after pre-training and only $\\theta$ was trained. Does this perform poorly?\n- Is my understanding correct in that the accuracies reported in Table 2 are over different sets of data? It seems like the algorithm outputs a certified radius for each point and we compute the CA over only points which have a certified radius greater than $r$. I find this confusing because Salman et al. report a SA for every $r$ while in this setting SA only makes sense for $r=0$. Can you clarify?\n- In Table 2, RGE and CGE for $q=192$ are quite different. However, if the AE converges correctly then the random directions in RGE should all be linearly independent and provide an equivalent estimate to the standard basis directions. Is there any intuition for this difference?",
            "summary_of_the_review": "Overall, I find the algorithm to make sense intuitively and the experiments are quite thorough. The results are superior to the current baselines and therefore I am satisfied with this paper.\n\n~However, I have a few concerns and clarifications. If they are resolved, I would recommend the paper to be accepted.~\n[Update]: I am satisfied with the author's response. I am increasing my score.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper proposes a novel approach to robustify black-box models to address the problem of black-box defense, which arises due to the concerns of privacy. The approach proposed by the authors essentially incorporates zero-order optimization on a prepended denoising smoothing(DS) module, with an autoencoder connecting the DS and model, termed as “ZO-AE-DS”, which the authors claim that it can robustify models using only model queries.",
            "main_review": "Strength:\nThe the proposed ZO-AE-DS paradigm is novel to me. The idea of black-box defense is very interesting, which is actually a very important real-world application scenario. Compared to the competing method FO-DS, the authors eliminate the need for white-box model by applying zero-order optimization, and also use an autoencoder to avoid dimension issues for calculation. Then they demonstrate it with experiments that AE can also improve robustness. The paper is overall easy to follow, and clearly written. Experiments are thorough and well organized.\n\nWeakness:\nThe performance of ZO-AE-DS (zero-order) does not outperform FO-AE-DS (first-order) since the latter needs a surrogate model whereas ZO only requires access to model’s queries. The question is that it is not too hard to obtain a surrogate model if we have queries access. This paper can be further strengthened if benefit of avoiding surrogate models are articulated. \n\nTypo: “a recently-developed A2-type approach (Salman et al., 2020),” should be “R2-type”\n",
            "summary_of_the_review": "Question: Both ZO-AE-DS and FO-AE-DS outperforms their counterparts without AE module. Considering the denoising nature of AE, the DS+AE can be viewed as a type of stacked DS. Have the authors tried other combinations of stacked DS? Is it possible to also increase the performance like inserting AE to FO-DS? Overall I think the proposed zero-order optimization for black-box defense is a very promising application scenario. Although the method seems doesn’t outperform FO version with surrogates, I think it has broader applicability owning to its minimum assumptions.\n\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The authors formulate the problem of black-box defense and propose a novel black-box defense approach called the Zero Order AutoEncoder-based Denoised Smoothing (ZO-AE-DS). Black-box defense corresponds to situations in which the defense model information cannot be obtained due to privacy protection in real scenarios. ZO-AE-DS introduces zero-order optimization on the structure of denoised smoothing (DS) to estimate the gradient and uses an Autoencoder (AE) to connect the denoiser with the model so that zero-order optimization can be conducted in a (low-dimension) feature embedding space.",
            "main_review": "Pros:\n1.\tThe concept of black-box defense is novel and interesting, and this paper is the first work to tackle the problem of query-based black-box defense.\n2.\tThe author shows the idea of how to solve the black-box defense step by step. The paper ingeniously applies the zero-order optimization method to the DS method and leverages autoencoder to bridge the gap between first-order and zero-order optimization.\n3.  The paper is clearly written and easy to follow.\n\n\nCons:\n1.\tThe experiments are conducted on CIFAR-10 and STL-10. Also, It can be observed that the larger the picture, the worse the defense effect. Is this method still effective on ImageNet? If ZO-AE-DS cannot be applied to ImageNet, then the method is of little significance. After all, black-box defense is designed to solve the problem in real-world scenes.\n2.\tThe references are all before 2020. The authors should add more recent works. \n3.\tThe baselines are not strong enough. The experiments are all compared with DS and its variants.\n",
            "summary_of_the_review": "Overall, this paper formulates the problem of black-box defense and provides a feasible black-box defense framework. It is clearly written and easy to follow.The updated experiments are comprehensive and convincing.\n\n[Update]: Though the method still can not really handle ImageNet, the authors' rebuttal addresses most of my concerns. Also, the setting of black-box defenses is novel and should be encouraged. Hence, I am increasing my score to 8.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper explores an exciting direction, i.e., black-box defense, where defense methods cannot access parameters of the target model when endowing the target model with certified adversarial robustness. However, the black-box defense setting considered in this paper has been studied in [1]. ",
            "main_review": "The contribution of this work may be relatively incremental. This is because the black-box defense setting has been considered in [1]. The differences between this work and [1] are: 1) query-based black-box defense is studied in this work 2) the authors introduce an AutoEncoder to reduce the variance of gradient estimation. Moreover, introducing AutoEncoder to promote gradient estimation has been studied in [2].\n\nMaybe, above comments are somewhat harsh. In fact, I have felt that. Thus, if the authors can highlight the contribution of this work, especially compared with [1], I am willing to raise my rating.\n\nI guess one contribution of this work may be that the authors have shown that query-based black-box defense is simple yet effective and improving the efficiency of gradient estimation is a good strategy for the query-based black-box defense setting.\n\n[1] Denoised Smoothing: A Provable Defense for Pretrained Classifiers. NeurIPS2020.\n[2] AutoZOOM: Autoencoder-based Zeroth Order Optimization Method for Attacking Black-box Neural Networks. AAAI2019.\n",
            "summary_of_the_review": "I like this work, but I do hope the authors can further highlight the contribution, especially when comparing this work with [1].",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}