{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper proposes a hierarchical reinforcement learning approach that exploits affordances to better explore/prune the subtasks, and thus making the overall learning more efficient. \n\nThe idea of the paper is novel and interesting. \n\nAfter the rebuttal, all the reviewers agree that the paper is a solid contribution.\nTherefore, I recommend acceptance of this paper."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper proposes Hierarchical Affordance Learning (HAL) that predicts sub-task affordance grounding on the current state. The affordance prediction can be used to prune impossible subtasks leading to a more efficient exploration.\n\nThe paper proposes a trick called \"predicting achievement context, \" which filters out false-negative samples during affordance classification.\n\nThe paper presents experiments on CRAFTING and TREASURE that demonstrate the proposed method's effectiveness and provides extensive ablation studies.",
            "main_review": "Strength: \n1. I like the idea of learning affordance and using them to prune impossible subtasks during exploration.\n2. Grounding on the current state, the affordance prediction can get rid of the history of subtask completion and handle stochasticity.\n3. The paper proposes an \"achievement context\" trick that is used to filter out false-negative samples during affordance classification.\n4. The paper presents experiments that demonstrate the effectiveness of the proposed method and provides extensive ablation studies.\n5. The overall writing is easy to follow.\n\nWeakness and comments:\n1. Why affordance classifier is learning faster than the meta-controller? Do we need to collect tons of samples to train an affordance classifier? When coming to a novel state, why is the classifier able to make a correct affordance prediction? Could you provide more intuition or motivation behind the affordance classifier?\n2. For methods using \"atomic propositions\", it seems that they can also handle the stochasticities if they model probabilistic transitions instead of deterministic transitions.\n3. \"each additional milestone must correspond to a behavior that is necessary for achieving the final milestone\" confuses me. Why is this true? Can we have redundant milestones?\n4. For CRAFTING, \"The full set of milestones contains items that are either craftable or collectable.\" Do the milestones include the number of items? What if we need multiple blocks of wood to make a pickaxe?\n5. Currently, the milestones are manually defined, which involves human prior. Do you have any idea how to select milestones automatically? This may be important for the proposed method to be generalized to various tasks.\n\n6. page 3: $\\mathcal{G}$ appears but is not explained. (explained on page 4).\n7. page 3: Could you use another symbol to replace $b_g$, since there is another $b_g$ on page 4.\n8. page 4: $\\mathbb{E}\\left[\\sum_{t=0}^{N} r_{t}+\\max _{g^{\\prime}} Q_{m}^{*}\\left(s_{N}, g^{\\prime}\\right) \\mid s_{0}=s, g_{0}=g, a_{t} \\sim \\pi_{g}, s_{t} \\sim P\\right]$. what is $g_0$ here? Is $N$ fixed and why is it so?\n9. page 4: what is $T$ in $t_{0}<t<T$?\n10. page 4: Does $b_{g}^{t}$ indicate the completion of subtask at time t only, or also include the completion before t.\n11. \"Crucially, unlike atomic propositions, the history of milestones alone need not define which subtasks are currently possible.\" confuses me.\n12. \"we have achieved something notable without having to assign it utility.\" confuses me.",
            "summary_of_the_review": "I like the idea of this paper, and the experiments also convinced me.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Given a task hierarchy specified as a dependency graph of milestones, this work employs a two-level hierarchical reinforcement-learning method similar to Kulkarni et al. 2016.  The lower level learns to achieve a given milestone in a classical fashion, while the upper level learns to choose milestones such that ultimately the agent will achieve the overall goal.  The core contribution is a method for pruning candidate milestones (upper-level actions) that are not currently achievable (afforded to the agent by the environment), using an affordance classifier trained on the fly.",
            "main_review": "Strengths:\n- The paper addresses an important problem and contributes real progress.\n- The paper is mostly really well written, using clear wording and notation.\n- The results, though limited in quantity, are indicative and make a solid case for the method.\n\nWeaknesses:\n\nNot many, none serious.\n\nI'm not sure I fully understand the contrastive loss function; its description is definitely too terse.  Unlike earlier, $N$ does not appear to be the segment length but some constant.  The anchor is apparently chosen from the current segment, but this is not stated.  What does it mean for $s_j^p$ to be chosen \"according to a segment-truncated normal distribution\"? I guess it is its rank within the segment that is chosen from a normal distribution centered at the rank of the anchor point.  This should also be reflected in the mean of the normal distribution; $a$ does not make sense.  And shouldn't $s_j^a$ be excluded from this choice?\n\n\"We show in Figure 5 that this parameter is relatively easy to set\": If all runs shown used the same parameter value then the results are suggestively reassuring, but they do not show how performance would differ if (say) specially-tuned values had been used.\n\nSome Details:\n\n- The fact that the milestone dependency structure must be pre-specified becomes clear pretty late into the paper.  This should be noted in the introduction, if not in the abstract.\n- The elements $b_g^t$ (4th row of Sec. 4) are the same as the elements $b_{g,t}$ near the bottom of p. 3 but the latter uses a lower instead of an upper index.  Reading the paper, it is quite obvious that this is in fact the same $b$, but it would be nice to make this connection explicit since it is key to understanding the method.\n- Two crippled sentences on p. 5:\n  - \"subtask the others\"\n  - \"failed to collected\"\n- p. 6, \"Context learning\": $f^*(s_i) = f^*(s_j)$ should read $f^*(s_t) = f^*(s_{t+1})$\n- p. 6, \"set used in the KNN procedure\": I found this wording misleading and suggest its replacement by \"$k$ closest positive points in $D_g^+$\", or perhaps simply \"$d_q^k$\".\n- Fig. 5 left: It would be interesting to know why the HAL curve drops sharply upon removal of the 5th milestone.\n- Fig. 5 center and right: it would be interesting to know how policies trained without edge stochasticity would perform in environments with various levels of edge stochasticity.\n- Fig. 6: Quantify the depth of each milestone within the task hierarchy.",
            "summary_of_the_review": "The paper addresses an important problem and proposes a novel and effective solution that is likely to be used and built upon.  \"Do not exist in prior works\" is a strong statement; there is certainly work this paper builds upon, but, if one draws the line around the \"contributions\" tightly enough, I cannot point to specific prior work in which \"aspects of the contributions exist\".",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper incorporates the concept of affordances (the idea that humans and agents perceive the world in terms of relevant action possibilities) from ecological psychology with hierarchical reinforcement learning. While affordance theory has been previously incorporated into agent action-reasoning-perception models and more recently with reinforcement learning, this work specifically looks at incorporating the concept with HRL. This is significant because many complex problems need to be broken down into subtasks. One of the challenges of many RL domains is the large action spaces that agents have to explore. While HRL helps to reduce this burden by breaking down a complex task into sub-tasks the incorporation of an affordance concept can further reduce the action space which the agent needs to consider. This paper combines these two ideas and has the potential to allow researchers to use RL in more challenging domains.\n\nThe paper develops a theoretical framework for combining affordances with HRL and conducts a number of experiments in two simple grid world domains allowing for comparison with baseline RL algorithms.",
            "main_review": "# Strengths\n- The central idea of the paper is very good.\n- The paper is well written and structured.\n- The motivation is clearly explained and I liked the running example with the pasta making to explain some of the concepts. As I was reading it, I was hoping to find an experiment with an agent using the approach to learn to make pasta. That would have been a nice example.\n- I like Figure 1 that explains the different approaches to the problem.\n\n# Weaknesses\n- One of the advantages of the approach proposed is that you could potentially tackle problems with more complex action spaces. I'm not 100% convinced that the two example domains (Crafter and Treasure) are necessarily the best domains to show what the approach can do. \n- In the introduction (page 2, 2nd paragraph) you discuss the completion of tasks. You mention that if a task is not completed some approaches need to introduce a symbol that indicates that tasks are undone. However, there are existing approaches (especially in the planning and agent communities) which can specifically deal with goals not being achieved. In fact in the BDI (beliefs, desires, intentions) agent literature there are agent programming languages that can specifically handle plan failure.\n- Something that was just briefly touched on in the paper was the relationship between an affordance and an agent's intent. If we look Gibson's, Chemero's and the work of Norman in affordance theory from the cognitive sciences literature we see that the relationship between an agent's goals, their intentions and the affordances available to an agent; these are deeply linked. It would have been good to explore this a bit more especially since this a link to HRL where goals and sub-goals play a much more prominent role compared to regular RL. Perhaps something to look at for future work.\n- This is just a minor point, but it is perhaps more accurate to say that your model was inspired by theory of affordances (or perhaps a model of affordances) rather than saying you have incorporated affordances into your approach. \n- One of the arguments for introducing affordances into these type of algorithms (and you have made a similar argument in this paper) is that it has the potential to reduce the action space the agent needs to explore. This would imply that by incorporating affordances in a HRL model that my learning times for a particular task/game could be reduced. Comparing the learning times for the same task using the same RL algorithm (with and without affordances) would have been nice to see. This would have been a nice complement to Figure 4.\n- Evaluation needs some additional work. A discussion of limitations and future directions is needed. \n\n# Questions / Issues\n- Many references are missing publication details. For example (Kulkarni 2016), the conference where this paper was published is not shown. This is the case for many of the references and should be fixed before the paper can be accepted. \n- Question about the terminology \"milestone\". Is this basically goal or sub-goal achievement?\n- First sentence in Section 5. Reads a bit off. I think you might have missed a word in this sentence. \"...for selecting any subtasks the others.\"\n- It wasn't 100% clear to me what you meant by the oracle function. I noticed this also in the results when you are plotting the Oracle against other RL algorithms. How is the oracle computing which action to take?",
            "summary_of_the_review": "- I think this is a good paper. There are some issues that need to be addressed but not enough to not recommend it for acceptance.\n- The central idea is a good one. I would say this is an incremental improvement to existing work, but has the potential to reduce the action space that agents have to explore and potentially reduce learning times. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper is in the realm of hierarchical reinforcement learning, and the aim is to employ a particular kind of affordance model: milestones of subtasks which can be achieved and a coupling between states and \"affordable\" subgoal completion (from that state). Basically, this makes it possible to limit action choices (or more general: bias choices) to actions/subpolicies for which it is known from experience that they are in reach from this state with this action/subpolicy. The paper introduces the HAL method, in which an indicator function for the milestones, a state generalization function (mapping similar affordance abilities to similar abstract states), and low level and hight level Q-functions are all learned in parallel. Using two domains, the method is evaluated and put against competetive baselines and it is shown that there are benefits in general, but also for specific aspects such as inexact milestone knowledge or stochasticity.",
            "main_review": "Overall an interesting paper with a simple idea, yet a fairly complicated algorithm to achieve the envisioned gains. The paper is well written and polished (although I have spotted several grammatical errors here and there) and the experimental evaluation is nice and fairly extensive and includes proper baselines. Related work is done fairly well too, and up-to-date. The conceptual steps (state abstraction which is induced from data, milestones and the f-function, their interdependence, and the mix with HRL) are clear although quite some practical choices are made and it is hard to see how robust these choices are (e.g. can the method diverge with a poor choice for some of the elements/hyperparameters?). Overall, I see several reasons for accept, but I also have a couple of things that are not yet entirely clear to me:\n1) The option framework is chosen early, but after that the formalization is not appealing to the simple option formalism at all. I think it would be more clear to define the milestones etc in an option framework first, and only then go to the deep approximations. Now things are mixed, and not clear enough. A more thorough definition of the approach and the underlying (semi)MDP first would be preferred.\n2) Related to the options, the affordances --as modeled here-- are actually some indication of a mix of option completion prediction and estimating the applicability of subpolicies, but not entirely clear enough (also not in the context of related work). Work like Ketharpal et al uses affordances purely for limiting action selection (ie. what is used here too) but based on \" intended effects\", whereas here it is about \"possibility to obtain the milestone/effect\" but it is also about \"completion\" of subpolicies and this is less clearly defined. Even more; the paper is unclear about the relation between subgoal policies and the vector induced by the f-function. The last sentence of section 6.1. says that for this domain \"the set of milestones contains each object the agent can successfully interact with\" whereas in the caption of Fig 4 it is said that \"sub-policies, on average, receive the correct milestone when called\", and earlier in the paper I wrote in the margin something like \"f(.) =?= #options???\" meaning that in the right part of Fig 2 f() is used as a mask over Q-values (of subpolicies) but does this mean there is a milestone for each subpolicy? This triggers the question again about \"option completion prediction\" (and related work) but at least it signifies that there is something unclear about the definition of f and what the milestones are in relation to the subpolicies.\n3) The experiments are nice, and informative. Yet, if affordances are modeled as they are here, as ways to mask out particular \"probably unsuccessful options\", they seem to mainly improve sample complexity, with as cost the increased computational complexity. That is, the 4 parallel learning tasks are more complex than a standard HRL method (like H-Rainbow here). I wish more would be said about this tradeoff. In addition, I think that some kind of ablation study on the four learning tasks would be useful: they do influence each other very much, hence if one or more would be absent (or stable by using an oracle) it would be easier to see how they help and interrelate. Also, because many practical choices have been made (and thresholds, and hyperparameters been chosen) I would like to see how robust the method is over these choices (but the general case for this is maybe out of scope for this paper).\n4) Based on some of the unclarity in 1-3, the comparison to related work is also not entirely clear. The comparison against recent LTL-style representations in RL is no strong anyway, since these are very different settings (with very different aims and goals). I do see that the authors try to make the case that they do not need complex symbolic structures, and there is some overlap with the specifications of (hierarchical) tasks, but the work in this paper is much more practical on a deep HRL method with an extra signal (milestones) and I do not see a deep connection. Also: the supply of the milestones can also be quite a bias and something one has to deliver as input (although, indeed, HAL can also work if this specification is incomplete). One aspect that could be made more explicit is the link between \"milestones\" in this paper, and \"task monitors\" as used by LTL-style work, in which several papers feed these into a deep RL system to enrich the state with \"information about progress on a task\" (which is, to me, very related). I would expect also more connection to HRL and the discovery of options and subgoals maybe.\n\nIn addition, some other issues are raised throughout the paper:\n- Caption Fig 1 grammar error (\"defines\").\n- There is no real definition of \"affordance\" in this paper. It is only implied, even though there are many definitions in the literature.\n- Section 5, 2nd sentence is key: \"restriction of subtask selection\". It would be good to introduce this more strongly earlier in the paper, since many other affordance models (e.g. in robotics) are about probabilistic models (between actions, objects and effects).\n- Page 5: line end-3: \"to collected\" (grammar)\n- The part about context learning is ok (page 6) but if there is space, it could be expanded to say more about the rationale and the underlying technical components to make this work. \n- Page 6: explain \"KNN\" and connect it to the method.\n- The experiments are well done, and \"learning efficacy\", \"robustness\" (twice) are clear, but the task-agnostic setting is not at all.\n- Fig 5 shows how relative the performance is/can be (see also my remarks above about the experiments). With half of the milestones, H-Rainbow+HER is as good as HAL. Turning it around: HAL needs at least half of the milestones to be better. Not much is said in the paper about this.\n- Fig 5 also shows the stochasticity results, but I prefer a more thorough explanation in terms of how the algorithms work.",
            "summary_of_the_review": "Overall, the approach is interesting, and valid, and significant. There are some unclarities about particular aspects (see review), and I think that the positioning can be done better, but my feeling is that they do not hinder acceptance too much.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}