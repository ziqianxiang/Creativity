{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "The paper proposes a rotationally equivariant transformer architecture for predicting molecular properties. The proposed architecture demonstrates good computational efficiency and good results on three benchmarks.\n\nAll four reviewers recommend acceptance (two weak, two strong), citing the novelty of the architecture, the good computational efficiency of the model and the good empirical results as the main strengths of the paper. The reviewers expressed minor criticisms and recommendations for improvement, some of which were addressed by the authors during the reviewing process, which led to an increase in scores.\n\nOverall, this is a nice contribution of machine learning to science, and I'm happy to recommend acceptance to ICLR."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper presents an equivariant transformer model for predicting quantum mechanical properties from an atomic graph. The model obtains SOTA or near-SOTA results on three popular datasets while maintaining good computational efficiency. The primary novelty in their method is a new way to compute the attention score using edge features. The paper also presents a detailed analysis of the attention weights that give insights into what the model is attending over. This is interesting from a chemistry perspective.\n",
            "main_review": "Strengths\n1. The presented transformer model is novel and is applicable to atomic graphs because it is equivariant to rotations and includes edge features in the attention computation. These edge features include an envelope term that prevents discontinuities in the loss landscape. This model is also well motivated by the underlying physics of the problem.\n2. The model obtains excellent results on a variety of datasets showing its generality.\n3. The paper includes detailed analysis of the learned attention weights.\n4. The presented model has good runtime performance compared to models like Dimenet++ that require higher order interactions, while achieving good performance. In practical applications like  catalyst discovery, these models are often used to search through millions of examples making efficient inference important.\n\nWeakness\n1. Other equivariant transformers have recently been proposed in the literature (for e.g. SE(3)-Transformers: https://arxiv.org/abs/2006.10503). Since these models are comparable to the method proposed in this paper, it would be good to compare these models. Also, clarifying the differences between these models would make the contributions of this paper clearer.\n2. The analysis of attention seems difficult to follow for somebody who is unfamiliar with chemistry terminology. Since ICLR is a general DL conference, I would urge the authors to edit the text to make it easier to follow.\n",
            "summary_of_the_review": "The paper presents a new rotation equivariant transformer model for atomic prediction. The resulting method obtains good results on 3 popular datasets, while being computationally efficient at inference time. The paper also provides a nice analysis of the attention weights that shed light on the inner workings of their model. While I would recommend the authors to clarify this analysis for the final version, I am leaning towards accepting the paper in its current form.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors introduce a novel architecture for ML force fields, the Equivariant transformer (ET). It is based on the Transformer approach and can be used to predict energies (and forces) and other molecular properties (e.g., QM targets). The performance on standard benchmarks such as QM9 and MD17 is impressive. The authors inspect the attention weights.",
            "main_review": "# Strengths\n\n1. The idea to put attention at the very center of an ML force field is novel.\n2.  The performance of the method is state-of-the-art.\n\n# Weaknesses\n\n1. The expose of the attention mechanism and update layer lacks detail.\n2. The value of inspecting attention weights is not clear to me.",
            "summary_of_the_review": "1. The introduction is well-written and relates the work to previous works.\n2. Sections 2.1 to 2.4 are not detailed enough. Considering that these Sections is at the very center of this work, they deserve more details:\n   1) is the function $a_n$ defined anywhere? \n   2) where are the matrices (?) Q and K coming from?\n   3) how is this attention matrix A computed?\n   4) how did the authors arrive at the update rules in equations (8) and below?\n3. Specification of the training details is welcome.\n4. Did you consider using the updated version of MD17, rMD17, for your experiments? Why did you decide against using the updated dataset?\n5. There is a typo in Table 3 (malonaldehyde).\n6. In the abstract, the authors claim to have gained \"valuable insights into the black box predictor\". In the manuscript, the value of their insights is not properly laid out. What did the authors learn about the model that would help them improve it, for instance?\n\n# Update\n\nThe authors updated the manuscript and addressed my questions. For this reason, I am willing to increase my score.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper describes an equivariant neural network (ET) with attention mechanisms that is applied to the prediction of molecular properties. The ET performs competitive with previous approaches on commonly used benchmark data.",
            "main_review": "The paper is well structured and the proposed method is described clearly. The use of the modified attention mechanism could be better motivated. Often, attention is used to capture non-local interactions for which convolutions are not suitable (e.g. Unke et al 2021). Here, the attention is forced to be local by a cutoff function and more reminiscent of factorized tensor layers.\n\nET improves on the QM9 dataset for 5 of 12 chemical properties. It does not improve however on the energies-related properties (U0, U, G, H), while improvng on energies on the MD17 dataset. This might indicate overfitting. ET uses more than 10x the number of parameters as NequIP or PaiNN. Therefore, it does not become clear whether the improved performance stems from the ET architecture or the size of the model. This should be demonstrated in the ablation studies with reduced number of layers, feature dimensions and/or RBFs.\n\nThe analysis of attention scores is interesting, however, it does not become clear from the paper how to interpret the results, e.g. in Figs 3 and 4. In the current form, the shown results are not too useful to \"shed light into the black box predictor\". The analysis of attention to displaced hydrogen is aligned with what one would expect from equilibrium vs out-of-equilibrium data, but could probably also have been achieved by a sensitivity analysis. The paper could be improved by extracting patterns from the attention scores that, ideally, go beyond pairwise interactions.\n\nThe demonstrated computational efficiency is impressive, in particular given the model size. Is there an explanation why the computation time is comparable to the much smaller model except implementation issues? Do the benchmarks include the computation of interatomic distances with scalable neighbor lists, or is the implementation considering all pairwise interactions? A fast evaluation is particularly important for molecular dynamics simulations. Therefore, having a comparison for MD17 molecules with PaiNN and and NequIP would be very relevant.\n\nFurther comments:\n- p. 1: Pfau et al and Hermann et al have proposed neural network approaches to VMC, not coupled cluster\n- p. 1: the original publication of Behler-Parinello should be cited (PRL, 2007) when referring to first work in this field\n- p. 2: when using gated equivariant blocks as in PaiNN, Weiler et al 2018 should also be cited who introduced equivariant gated non-linearities",
            "summary_of_the_review": "The presented approach shows iterative improvements over previous work. Unfortunately, at the current stage it is not clear whether those are due to the ET architecture or the increased model size. The analysis of attention scores is promising, but still lacks the necessary interpretation.\n\nUpdate:\nBased on the response of the authors, I have raised my score.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper proposed equivariant transformers --- a neural network based algorithm to predict properties of molecules. The architecture is built upon the traditional transformer architecture, combined with some modifications specific to molecular property prediction tasks, such as exponential normal radial basis functions, SiLU activation function and design of update layers. It is shown that the proposed model have good performance on QM9, MD17 and ANI-1 dataset. Ablation studies analyze the attention weight and give some insight about how the model works.",
            "main_review": "Strengths: The experiment results of this paper looks very promising. In QM9 dataset, the proposed model outperform other models in many targets. Especially for dipole moment and electronic spatial extent, the proposed model is much better. For MD17 dataset, the proposed model outperform all the baselines on all the targets. For ANI-1 dataset, the result is also much better. \n\nWeakness: It will be better to include more discussion about the components of the model, especially the components not in regular transformers. Readers might be curious about \n(1) What are the motivations of these components. \n(2) Will the performance drop if we replace the components by others? Which components are the most important ones?\n\nAlso, there are some other famous baselines such as EGNN [1], DeepPotential [2]. It will be better to include the results in the table.\n\n[1] Satorras, Victor Garcia, Emiel Hoogeboom, and Max Welling. \"E (n) equivariant graph neural networks.\" arXiv preprint arXiv:2102.09844 (2021).\n[2] End-to-end symmetry preserving inter-atomic potential energy model for finite and extended systems\nL Zhang, J Han, H Wang, WA Saidi, R Car - arXiv preprint arXiv:1805.09003, 2018",
            "summary_of_the_review": "Overall, this paper is a good paper with strong experiment results on various datasets. The components of the model look very interesting. Possible improvement include more discussion on model components and comparison with more baseline algorithms.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}