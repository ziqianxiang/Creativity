{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "The paper contributes to the literature on federated learning by introducing a hybrid local SGD (HL-SGD) method. HL-SGD is motivated by the setups where edge devices are grouped into clusters with fast connections within the cluster, but slower connection between the devices and the server. HL-SGD uses hybrid updates: decentralized updates within the clusters and federated averaging steps between the clusters and the server. \n\nInitially, the reviews expressed concerns regarding comparison to prior work, empirical results, and privacy of the proposed scheme. However, the authors adequately addressed all of the concerns, added relevant discussions and results to the paper, and a consensus was reached that the paper should be accepted."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper introduces a new hybrid scheme combining local and decentralized SGD by considering a network with one central node (server) and multiple worker nodes (devices) grouped into clusters with fast intra-cluster communication but slow device to server communication. The authors show that in such settings replacing local SGD with intra-cluster averaging of iterates after each SGD update can lead to faster convergence. Theoretical analysis explores the tradeoff between intra-cluster connectivity, device sampling rate, and convergence rate, while experiments on CIFAR10 and FEMNIST with non-iid splits shows that the proposed approach yields higher test accuracy than local SGD for a given runtime/number of rounds.",
            "main_review": "Strengths:\n\n1. The paper adds a new dimension to the federated learning setup by combining local and decentralized SGD approaches and showing that doing so can give more accurate models than those obtained with just local SGD.\n\n2. The theoretical results are intuitive and the convergence analysis explores and highlights the important tradeoffs between intra-cluster connectivity, device sampling rate and convergence rate.\n\nWeaknesses:\n\n1. My main concern is that the evaluation is a bit unfair to local SGD. It seems fairly apparent that averaging models within a cluster after every iteration will outperform local SGD where models are only averaged after $\\tau$ iterations. Moreover while D2D communication over LAN or across nearby devices may be cheap it is potentially more expensive than averaging computations in a single node or a datacenter network. I also find considering a single choice of $W_k$ in experiments to be a bit limiting (since it seems like the mixing matrix really depends on the setting). Therefore I would suggest the following performing the following additional experiments:\n\na) D2D communication after multiple SGD updates (the analysis for this may be complicated but it would be instructive to see the empirical performance).\n\nb) Experiments with varying $W_k$ (also do mention the value of $\\rho_{\\max}$ corresponding to the choice of $W_k$)\n\n2. A couple of claims rely on being able to tune the intra-cluster communication parameters -  $W_k$ (as in Corollary 1) and $c_{\\text{d2d}}$ (as in the results in Fig. 3) - to obtain improved performance. Is it really possible to tune these parameters in real world settings? I would suggest adding a citation to support this.",
            "summary_of_the_review": "The paper definitely adds to the discussion in Federated Learning and by introducing an interesting new setting of hybrid local SGD. Therefore I am leaning towards accepting it. However while the analysis captures the important tradeoffs, the experiments are a bit limited and seem slightly unfair to the local SGD baseline. If these can be rectified (following my suggestions above or otherwise) I would be completely convinced about accepting it\n\nComments after Rebuttal: Thank you for adequately responding to my queries. I am satisfied with the revisions made to the paper and have updated my score to reflect the same.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper introduces HL-SGD, a method to design a hybrid federated learning method to leverage a hybrid high-speed D2D and low-speed D2S network and speed up the communication of federated learning applications. Both theoretical analysis and empirical results are provided to show the effectiveness of HL-SGD.",
            "main_review": "The paper proposes to use a hybrid model aggregation to conduct federated learning leveraging both high-speed D2D network and low-speed D2S network. \n\nPros:\n1. The idea proposed in the paper is easy to follow, and the research direction of enhancing the communication efficiency of federated learning is potentially impactful. \n2. Both theoretical analysis and empirical results are provided to justify the effectiveness of HL-SGD. \nCons: \n1. The main concern I have for the paper is that the proposed idea lacks novelty. Hierarchical federated learning has been studied extensively in the literature [1,6] as well as hierarchical local SGD [3]. Why is HL-SGD fundamentally novel? \n2. The theoretical analysis also seems to make sense, but it would be valid to show the comparison among the convergence rates among this work and the prior methods. \n3. The scale of the experiments is too small. What are the sizes of the CNNs used for training EMNIST and CIFAR-10 in the experiments? I won’t assume the authors are using large CNNs, but for small-scale models, why will train them to incur communication bottlenecks? \n4. Apart from communication efficiency, another big motivation for federated learning is data privacy. Does HL-SGD come with any privacy guarantee? If not, can it be compatible with existing privacy-preserving methods, e.g., Secure Aggregation [4]? \nMinor comments:\nTypo: “no-iid” —> “non-iid”\n\nMissing references: [4-5]\n\n[1] https://arxiv.org/pdf/1905.06641.pdf\n\n[2] https://arxiv.org/pdf/1909.02362.pdf\n\n[3] https://arxiv.org/abs/2007.13819\n\n[4] https://eprint.iacr.org/2017/281.pdf\n\n[5] https://arxiv.org/abs/2107.06917\n\n[6] https://arxiv.org/pdf/2103.10481.pdf",
            "summary_of_the_review": "Overall I think the proposed method is a natural extension from the previous work. And the novelty is quite marginal. The experimental evaluation can be largely improved.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a new Federated learning algorithm which can take advantage of the fast D2D (Device-to-Device) connections among the devices. In particular, it is shown that the proposed algorithm has better performance in terms of convergence rate and accuracy compared to the existing algorithms such as 'Local SGD'-based FL which rely only on the slow D2S connections for model updating. The convergence of the proposed algorithm is theoretically characterized in terms of the D2D connectivity (topology). Numerical results are provided to show the improved performance compared to the local sgd-based FL.\n",
            "main_review": "Strength:\nThe proposed algorithm takes good advantage of the fast local connectivity among the nodes to improve the convergence. It is a novel contribution. \n\nDetailed convergence analysis of the proposed algorithm is provided. \n\nWeakness:\nWhile the Theoretical results emphasize the role of the local topology in each cluster, the corresponding numerical analysis to understand the impact of topology is missing. It would be interesting to see how the connectivity parameter $\\rho$ impacts the results in Figure 1 to 5, instead of just taking a ring topology. ",
            "summary_of_the_review": "The proposed algorithm is novel and the the performance is rigorously characterized. The improvement compared to existing algorithms is significant and hence I believe it should be accepted for publication. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper proposes a new method to address the heterogeneous communication setting in a hierarchical distributed system.  The system consists of a central server and several disjoint clusters, and each cluster is a local network connection between multiple edge devices. The communications are heterogeneous in the sense that in cluster device to device (D2D) communication are much faster than the device to server (D2S) communication. The proposes algorithm is a hybrid method where decentralized method is applied within clusters (on edge devices) and federated average is applied between cluster and server. Convergence analysis is provided showing the benefit of variance reduction using decentralization within cluster. Empirical studies are conducted to demonstrate the improvement under synthetic federated learning environment.",
            "main_review": "The presentation of the paper is very clear and easy to follow. The strength of the paper lies in the theoretical analysis, where combining decentralization and federated average is not straightforward at all. The theoretical result is a bit hard to digest, further clarification and comparison with existing method would make it more readable. The experimental section is also less convincing. My detailed comments are as follows. \n\n1 Theoretical discussion:  it is not clear from the current presentation how to choose the parameter $\\tau$ (number of decentralized updates per rounds) Ideally, it would need to balance (i) the D2S communication cost and the D2D communication cost (ii) intra-cluster convergence and outer-loop (round) convergence. In other words, is there any tradeoff between improving the heterogeneous communication versus having better convergence rate. For the discussion on $\\rho_{max}$, how it scales with $n$ in a non-extreme situation. The current discussion mostly focus on it being 0 (fully connected) or 1 (no connection), it would be good to discuss its scaling with $n$ in a less trivial situation.\n\n2 Empirical study:  the experiments are performed with a fixed $\\tau$. However, the local SGD may have a different regime on the dependency of $\\tau$. I encourage the authors to provide more comparison on how $\\tau$ influence the performance in figure, for instance with $\\tau =5, 10, 20$. The ablation study in Figure 4& 5 is also not convincing as it seems no difference. In particular, changing $\\rho$ does not lead to any performance or run time change. This might suggest that the local problem is too simple that be efficiently solved even the connectivity is bad.  \n\n3 Privacy concern: the lack of discussion on the privacy issue is a major omission in the paper. The decentralized communication within cluster requires client to exchange information with others, raising potential concerns on leaking informations. I believe this need to be carefully addressed as privacy is one of the major difference in Federated Learning compared to traditional distributed training.\n\nOverall, the theoretical study has merit while the empirical study is less convincing,   ",
            "summary_of_the_review": "The theoretical study has merit while the empirical study is less convincing,   ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}