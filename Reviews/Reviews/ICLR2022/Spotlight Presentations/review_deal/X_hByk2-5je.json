{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "The paper revisits lossless compression using deep architecture. In contrast to main stream approaches, it suggests to make use of probabilistic circuits, introducing a novel class of tractable lossless compression models. Overall, the reviews agree that this is an interesting direction and a novel approach. I fully agree. Actually, I like that the paper is not just saying well, we could use a probabilistic circuit for ensure tractability but also shows that there is still a benefit of different variable orderings for encoding and decoding. In any case, adding probabilistic circuits to the \"compression family\" is valuable and also paves the way to novel hybrid approaches, combining neural networks and probabilistic circuits. I have enjoyed reading the paper, reviews, and discussion."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "Probabilistic circuits are a formalism, developed relatively recently, for describing multivariate probability distributions. PCs are represented using directed acyclic graphs (DAGs), with an operational semantics, and it is relatively straightforward to deduce which operations (marginalization, maximisation, estimation of moments, etc.) are tractible based on the structure of the DAG and the locations of input variables.\n\nThis paper studies the application of PCs to lossless compression, identifying the specific type of marginalisation which is necessary for auto-regressive, arithmetic coding-style compression, and the necessary conditions a PC must satisfy in order for this marginalisation to be tractible. In section 3, a procedure is described for particularly efficient marginalisation by sharing computation.\n\nExperiments show that the approach is competitive in terms of compression rate with other recent 'neural compression' works, and results are presented showing that PCs can be much faster than some existing methods, although I have serious concerns about these (see below).",
            "main_review": "Overall, I felt that whilst this was a reasonably well written paper highlighting the interesting concept of PCs, that I was not previously aware of, the methodological contribution wasn't clear enough and that the experiments were presented in a way which was potentially misleading.\n\nMy main issues with the methodological contribution (specifically section 3.2), are that I felt it was relatively difficult to understand (i.e. the presentation could have been clearer), and that as a result I don't know how significant the contribution is. On a high level, it seemed like a couple of observations were made:\n\n(a) For many models, it is important to decode/encode data in an order which 'fits' the model, leading to computational efficiency.\n(b) Work sharing between encode/decode steps is necessary for efficiency.\n\nTo someone, such as myself, who is an expert on neural lossless compression, these are both obvious points. The O(log D |p|) computational efficiency didn't seem particularly radical to me either. However, I'm unfamiliar with the PCs literature, so maybe these are significant observations to that community, and it's possible that I missed something important, so please clarify.\n\nBefore I discuss the experiments, another significant issue with the overall framing was the statement in the second paragraph of the introduction that the methods presented here are 'tractible', with the implication being that other neural compression methods are not (in some sense). I understand where this came from, since tractibility is a key property in the PCs literature, but I still felt this could be misleading, since neural compression and decompression with existing methods, whilst they may have different performance characteristics to PCs, certainly _are_ tractible.\n\nI had two main issues with the experiments. The first relates to the presentation of the timings of the method. I think it's easy to be misled, and to mislead, with timings in machine learning (and the emerging field of neural lossless compression in particular), and my issue with timings in this work is that the paper is presented as though a significant breakthrough has been made in terms of runtime, but the PC method is only compared to slow implementations of existing methods, which were not optimized for speed. One example of a faster implementation of a neural compression method is BB-ANS with a small VAE (from Townsend et al., 2019), implemented at https://github.com/j-towns/craystack. Running the example code there on a CPU, the compression/decompression for the binarized MNIST test set (a slightly easier task than raw MNIST) is 3.26s/2.82s, an order of magnitude faster than the PC timings in the paper. For extra context, on the same machine gzip takes 0.22s to compress and 0.06s to decompress the raw MNIST test set, so in my opinion neither the Craystack BB-ANS implementation nor the 15s encode and 44s decode of the PCs implementation should be considered fast. This context should have been made clearer, rather than trying to imply that a genuine breakthrough had been made in runtime.\n\nMy other serious issue with the presentation of the experiments were the vague claims of \"state-of-the-art performance on natural image datasets\", in the abstract and at the very end of section 2, discussed in more detail in section 5. It's not at all clear on what task the authors are claiming state of the art performance. At the end of sec 5, the suggestive statement is made that compression/decompression of natural images \"can be done easily\" with the implemented method. Have the authors actually implemented this? If compression wasn't implemented then this should be stated and the authors should explain why not, if it was implemented this should be clearly stated. Unfortunately, even if compression was implemented, a claim of state of the art performance would still be incorrect, because the bitrates achieved in a recent paper by Zhang et al. (https://arxiv.org/abs/2109.02639) are significantly better.\n\nSome more minor points and suggestions:\n - At the beginning of sec 3.2, why does \\pi need to be 'random'? Don't you just mean to say that \\pi is \"some ordering\" (i.e. no need to suggest that it is a random variable).\n - In section 4.1 I think it's best to consistently use the word 'latent' rather than 'hidden', since this is the standard terminology in the deep generative modelling community.\n - It might be a good idea to combine the different related work sections in one place.\n - In 'related work' near the bottom of page 8, there's a slight grammatical error: replace \"grow PC structures to fit better the data\" -> \"grow PC structures to better fit the data\".",
            "summary_of_the_review": "I felt that whilst this was a reasonably well written paper highlighting the interesting concept of PCs, which I was not previously aware of, the methodological contribution wasn't clear enough and that the experiments were presented in a way which was potentially misleading.\n\nEDIT: Score changed to 6, see comment below.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper proposes a new probabilistic model named PC for efficient encoding and decoding. The authors claim that PC greatly reduces the time complexity for inference. Experiments show that PC achieves 5-20x faster than SOTA neural compression algorithms, and performs SOTA results on MNIST datasets.",
            "main_review": "Strengths:\nThis paper provides a new probabilistic modelling framework named PC for lossless compression. This framework is new in AI lossless compression community and achieves desired compression ratio and compression bandwidth in MNIST. By incorporating PC with explicit generative models in which the prior can be replaced with PC, the model achieves SOTA compression ratio in real-color images.\n\nWeaknesses:\nThis work is somewhat hard to follow in the following aspects:\n1.\tIn the abstract, the authors highlighted the drawback of VAEs such that “bits-back coding brings poor single-sample compression rates”. However, the proposed PC seems have no relevant with bits-back coding. I wonder what is the advantage of PC over VAEs in terms of “bits-back coding”?\n2.\tThe authors claim that the complexity of PC is O(log D |p|) where |p| is number of neural network units. But I am not able to figure out how to incorporate neural networks with PC in O(log D |p|). In particular, consider flow models with PC, it just seems to replace the prior with PC, to what follows, the complexity seems to be O(log D + |p|). Moreover, for auto-regressive models, it seems that PC cannot reduce the complexity to O(log D |p|) but remains O(log D |p|). More discussions on how to incorporate generative models with PC with O(log D |p|) complexity is encouraged.\n3.\tThe PC framework is hard to follow. In Definition 2-4, it is better to give definitions on “scope of variable”, and examples on “Smoothness”, “Decomposability” and “SD”. For Fig. 1, detailed probability corresponding to the figure is recommended.\n4.\tFor Sec. 5, the compression bandwidth of PC+IDF compared with IDF is recommended.\n",
            "summary_of_the_review": "The idea of PC for lossless compression is new and interesting. However, following the main contribution of PC and the main idea of PC is somewhat difficult. Giving simple examples on PC for clarification is recommended.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The manuscript addresses the question of using deep generative models in lossless compression. As highlighted by the authors, the main issue here is related to the cost of computing probabilistic queries or, in some cases such as GANs, the inability to compute them.\nThus, this paper suggest using Probabilistic Circuits (PCs) for lossless compression. PCs allow for tractable probabilistic inference, which enables efficient compression.\n\nExperimental results are favourable in two ways. First, PCs are faster for compression, achieving results from 5 to 20 times faster than competitor neural networks. Second, PCs achieve competitive compression rates on various datasets.",
            "main_review": "A strength of this paper is its clever use of PCs in compression. First, it highlights the importance of tractability in lossless compression, which is an unexplored perspective in related works. Then, PCs are presented as a class of models with tractable inference, while still being expressive enough. This balance between expressiveness and computational cost is key this manuscript's main contribution.\n\nIt is worth clarifying the significance of the technique for improving marginal inference from O(D.|p|) to O(log(D).|p|). This algorithm has relevant similarities with Partial Propagation described in [1]. Thus, a comment on differences here could highlight the novelty in Algorithm 2.\n\nSection 4 provides a practical way for scaling up the use of PCs in compression. However, it is not clear from the manuscript the contributions made in the scaling up process. That is, if there was any new required technique developed for applying Hidden Chow-Liu Tree to the specific problem of compression.\n\nExperimental results are encouraging and convincing.\n\n\nMinor comments:\n* There is a typo with \"p(i.e.,\" in Section 2\n* Theorem 1 significance is not clear from the manuscript and it seems incremental at a first glance. While there is no questioning of its practical implications, a formal treatment could highlight its broader applications.\n\n\n[1] C.J. Butz, J.S. Oliveira, A. dos Santos, A.L. Teixeira, P. Poupart, A. Kalra, An Empirical Study of Methods for SPN Learning and Inference, Ninth International Conference on Probabilistic Graphical Models (PGM), 49--60, 2018.",
            "summary_of_the_review": "The manuscript provides a class of tractable models for lossless data compression. These models are well defined and algorithms are shown. Parts of the work need clarifying for novelty and significance. However, experiment results with faster and competitive compression rates are very convincing.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper showcases an application of Probabilistic Circuits to lossless compression, and achieves competitive compression performance to state of the art method. The PCs output a marginal distribution over the data, which the authors then use to compress with arithmetic coding or other methods. This has several advantages, one being that bits-back coding is not needed, enabling single-sample compression.\n\n",
            "main_review": "Strengths:\nThe contributions of the paper are: (i) reducing the complexity with respect to D from linear to sub-linear and (ii) integration with existing neural compressors. The authors use Hidden Chow-Liu Tree (HCLT) , a PC model initially proposed for simple density estimation tasks containing binary features, to scale up to achieve state-of-the-art performance on various image datasets.  Furthermore since HCLTs cannot be easily vectorized, the authors implemented customized GPU kernels for parameter learning and marginal query computationbased on Juice.jl, an open-source Julia package. This way the authors were able to get significant improvement in the performance.\n\nWeaknesses:\n The baselines BitSwap and IDF, which are definitely NOT the state of the art in compression efficiency and compute time. The comparison that is missing is with respect to 'Improving Lossless Compression Rates via Monte Carlo Bits-Back Coding' (Ruan, Ullrich, Severo, et al.). In that paper, the authors claim to compress all 10,000 MNIST images in less than 100s (Figure 7). Comparison with BitSwap (and possibly IDF) should be done with caution. In BitSwap, the entropy coder runs on CPU while the model can be run on GPU or CPU. It's not clear what the experimental setup was here for BitSwap, PC, and IDF experiments. Exactly what setup the authors claim SOTA should also be clarified (e.g. model on GPU, compression on CPU).\n\n\nComments regarding writing\nPage 4: \n- Not clear what happens when 2 different input units have the same variable. This is exactly the case of Figure 1, where there are 2 input nodes for each of X1, X2, and X3.\n\nPage 5:\n- If the inputs are descendants of the same child unit, then doesn't this imply the opposite? That is, you definitely need to multiply them based on equation 2? It would be easier if the authors followed along with a concise and concrete example.\n- The example in Figure 1 does not help, as the distance from n1 and n2 to the root node is too short for the reader to properly follow along. \n\n",
            "summary_of_the_review": "Overall the work is interesting and timely and I am leaning towards an accept if the authors can convincingly address the points above.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}