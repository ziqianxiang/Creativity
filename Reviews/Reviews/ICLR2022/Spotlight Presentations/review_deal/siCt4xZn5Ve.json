{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "All the reviewers agree that this paper made a solid contribution of understanding the algorithmic regularization of SGD noise  (in particular the label noise for regression) after reaching zero loss. The framework is novel and has the potential to extend to other settings."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper proposes a new mathematical framework for explaining implicit regularization of SGD after attaining zero training loss. Technical novelty includes a different time scaling of the SGD that leads to the Katzenberger process, and then applying weak convergence of the Katzenberger process to obtain an elegant characterization of the SGD dynamics on zero loss manifold. This new technical device leads to a new proof of the sample complexity bound on the overparametrized linear model with label noise regularization.",
            "main_review": "Pros: The analysis using $\\eta^{-2}$ scaling to separate the fast and slow motion of SGD dynamics is novel, and this mathematical framework seems appropriate for analyzing the implicit regularization of SGD in the manifold of zero training loss.\n\nMajor comments:\n- Previous papers, e.g. Ghorbani et al. (2019) [Sect. 4.1], Gur-Ari et al (2018), Li et al. (2020), empirically show that there is significant alignment in top eigenspace between the Hessian and gradient covariance matrix using various deep neural network architectures and image datasets. If this alignment were true, I assume that Eq. (13) could describe the implicit regularization of SGD for general deep neural network. It would be very interesting if Eq. (13) can be used to explain the empirical observation of the spectrum of Hessian around minimizer found by SGD, i.e. a large bulk of nearly 0 eigenvalues with some large outliers. It seems this can be answered by a closer look of the right-hand-side of Eq. (13) and see how the gradient of $\\Phi$ interacts with the gradient of tr(Hessian).\n- The definition of the manifold $\\Gamma$ is a bit confusing. At first it is defined at the beginning of p.5 that looks general and is not specific to a model, but then in Theorem B.9 in the appendix, definition of $\\Gamma$ in Eq. (15) specializes on overparametrized linear models. Does Theorem 4.6 only apply to settings in Sect. 6? Clarification in appropriate places may be needed.\n- Given that the ground truth parameter $w^\\ast$ is $\\kappa$-sparse in Sect. 6, could some other types of regularization beside label noise better explore the manifold $\\Gamma$ and recover $w^\\ast$ with a lower sample complexity than $\\kappa\\log{d}$?  For example, if one is allowed to introduce a deterministic trend in the right-hand-side of Eq. (13) that can somehow lead to sparsity without leaving the manifold $\\Gamma$, e.g. directional pruning (https://arxiv.org/abs/2006.09358), would it help reduce the sample complexity?\n\nMinor comments:\n- Typo: “Polyak-ojasiewicz”\n- Fifth line below Eq. (6), no $n$ in $\\lim_{n\\rightarrow\\infty}{X\\left(t\\right)}$, and something seems to be wrong in that sentence\n- The $\\dagger$ is undefined in Definition 4.4 where it firstly appears\n- Cor 5.2, $\\Sigma=c\\nabla^2 L$, no trace\n- Are $\\sigma$ and $\\Sigma$ the same thing in Eq. (8)? I assume the operator $\\sigma$ in Sect. 4.1 is the same as the $\\Sigma$ in later sections, but it is best to unify them\n\nReferences:\n- Ghorbani, B., Krishnan S. and Xiao, Y. (2019). An Investigation into Neural Net Optimization via Hessian Eigenvalue Density. ICML\n- Guy Gur-Ari, Daniel A. Roberts and Ethan Dyer (2018). Gradient Descent Happens in a Tiny Subspace. Arxiv: 1812.04754\n- Xinyan Li, Qilong Gu, Yingxue Zhou, Tiancong Chen and Arindam Banerjee (2020). Hessian based analysis of SGD for Deep Nets: Dynamics and Generalization. Arxiv: 1907.10732.\n\n\n",
            "summary_of_the_review": "This paper provides a new mathematical framework for analyzing the implicit regularization of SGD after attaining zero training loss. The framework is novel, and it leads to a simpler analysis of sample complexity under noisy label regularization. I am in favor of accepting this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose an SDE approximation to study the implicit bias of SGD with infinitesimal learning rate. They show that given any noise covariance structure, after $\\eta^{-2}$ steps, the SGD converges to an SGD on a certain manifold of local minimizer as the learning rate $\\eta\\rightarrow 0$. This recovers and strengthens some existing results in the literature. In particular, they show a sample complexity gap between label noise SGD and GD in the kernel regime for an overparametrized linear model, justifying the generalization benefit of SGD.",
            "main_review": "Strength. The results are novel and the analysis is non-trivial. It introduces new ideas to the field by adapting the ideas from Katzenberger (1991). It allows the authors to obtain a global analysis of the implicit bias valid for $\\eta^{-2}$ steps, as well as any arbitrary noise covariance structure.\n\nWeakness. Some model assumptions, setups and the conditions in the main results might need more explanations.",
            "summary_of_the_review": "(1) It would be nice if the authors can explain more and justify why you can use the SDE approximation in equation (2). $\\eta$ is the stepsize, and I can understand the stochastic modified equation in the literature, but somehow I don't see why SGD can be approximated by equation (2). \n\n(2) In the drift term in equation (3), should it be $\\nabla L$?\n\n(3) In the paragraph after equation (3), you wrote that hopefully we can simplify the dynamics Equation (3) via choosing suitable $\\Phi$. What I don't understand is that on the surface, it seems equation (3) is even simpler than equation (4) or (5). So what do you mean by simpler?\n\n(4) On page 5, in the paragraph before Theorem 4.1., what is $\\phi$?\n\n(5) I find the assumptions in Theorem 4.6 quite strong. You need to assume that the SDE in equation (10) has a strong solution. Ideally, the assumption should be on the loss function $L$ instead of the equation (10). Another strong solution is that $Y$ never leaves $\\Gamma$. This in my opinion is also quite strong. $Y(t)$ is an SDE with a tangent noise term, and because of the Brownian noise, I just don't see how this assumption can be satisfied. Even if you have degenerate noise in some direction, e.g. in the case of underdamped Langevin diffusion, it is still supported on the entire Euclidean space. If you have to assume such strong solutions for Theorem 4.6., it might be helpful to construct some toy examples to demonstrate that the limiting $Y$ is non-trivial and meaningful at least for some toy examples.\n\n(6) I also find Remark 4.7. a bit puzzling. In Remark 4.7., you mentioned that the convergence in distribution result in Theorem 4.6. (at the time $T$) implies the convergence for the sample path on $[0,T]$. I don't understand this. Shouldn't it be the case that the latter implies the former?",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper provide a global analysis of the implicit bias of SGD after it achieves zero training loss by adapting ideas from Katzenberger (1991). Moreover, based on this analysis, they show that label noise SGD can avoid the kernel regime, i.e., its sample complexity is significantly less than $O(d)$ in the overparameterized sparse linear regression model.",
            "main_review": "This paper provides a very strong result for the implicit bias of SGD after it achieves zero training loss, i.e., capturing its behavior once it is in the manifold of the global minimizers. Based on Katzenberger (1991)’s result, they capture the global behavior, and their result covers the case that the stepsize $\\eta\\to 0$. In detail, via a projection operator, they kill the noise in the normal space and derive the limiting SDE via showing that the SGD will converge in distribution to this SDE. For each term of this SDE, they provide convincing explanation. Based on this result, they derive the limiting Flow for label noise. Furthermore, they study the overparameterized sparse linear regression model, showing that, in contrast to GD, SGD with label noise can recover the ground truth efficiently provided almost any initialization.\n\nOverall, this paper is very solid, and its technique, though is adapted from Katzenberger (1991), shall have its own value to the community to understand the behavior and property of SGD. Its application to demonstrate the power of label noise SGD is also very interesting. \n\nHowever, I have a few confusions about the label noise SGD. In the paragraph after the equation 12, the author mentioned that “Suppose the model can achieve the global minimum of the loss ...” Can you demonstrate that why the ground truth should be the global minimizer in the label noise setting? On the other hand, it seems that this analysis heavily relies on the label noise. Is it possible to extend this result to the mini-batch SGD?",
            "summary_of_the_review": "This paper has very good theory and is well-structured. Also, its application to label noise SGD is also very interesting.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors of the paper under review carry a general analysis of the effect of the noise near the manifold of global minimum $\\Gamma = [x, L(x) = 0]$. More precisely they show that in the limit of infinitesimal step-sizes, a time rescaled effective dynamics follows a diffusion in the manifold $\\Gamma$ that mixes effects of the noise and the gradient flow. Leveraging this, they characterise explicitly the limiting point recovered by SGD on a sparse problem trained with label noise on a overparametrised linear network of depth 2.",
            "main_review": "First, I would like to thank the authors for the paper, the intuitive explanations, the clear writing and the proper referencing of the very nice Katzenberger article. \n\nBoth as a reviewer and a curious researcher I really enjoyed reading this article. I truly think it tackles a very nice question about the effect of the noise of SGD in minima selection. Remarkably, the paper introduces the very mathematical object that lacks in the Blanc et al. paper. Even if the limiting diffusion, in all its generality, is difficult to interpret, it shows properly some already known intuition in the case of SGD for machine learning problems with label noise. Finally, to give an explicit example where the limiting dynamics can be analysed thoroughly, the authors show optimality of label noise SGD for overparametrised linear network: what a nice result and application ! \n\nObviously, as one can read, I am very enthusiastic about this paper, but let me try to help the authors with a few notes:\n\nI would say that sometimes the writing is a little bit too direct and should be tempered a bit: the authors study the impact of the noise locally near $\\Gamma$ with an effective dynamics. To « trap » the dynamics in $\\Gamma$, the consider a regime for which the step-sizes go to zero (and along with it the effect of the noise itself). Hence I see this result as a limiting one: surely in practice, the dynamics is not exactly the one depicted: a first phase where the loss goes to zero and a second phase where the dynamics diffuses in $\\Gamma$. Of course, it does not diminish the nice impression that the paper gives me, but it should be clearer that to clearly separate the two phases the authors consider a limiting regime.\n\nEven if this work is already pretty complete:\n- I would like to read more interpretations of the limiting process of Eq. (10). What happens, e.g., for other neural networks than the one depicted in Section 6 ?\n- What happens if one does not add artificially label noise, e.g. with the model of Section 6 ? I expect that the analysis collapses because the noise itself is killed in $\\Gamma$. If not how is the dynamics changed ?\n- Can you give more intuition on the effective dynamics of equation (17) ? How are killed the directions for which the sparsest predictor is zero ? \n\nFor the modelling of machine learning-SGD with an SDE, authors should add the nice work of S.Wojtowytsch [ https://arxiv.org/abs/2106.02588 ]. Here, it is explained that the nature of SGD noise for machine learning problem is very degenerate. Another nice reference for your problem is the nice work of Pesme et al. [ https://arxiv.org/abs/2106.09524 ] on the implicit bias of the same model as the one studied in Section 6. It should also be pointed out that Lemma 6.3 is well known for these models: despite the fact that Woodworth et al., 2020; Azulay et al., 2021 assume convergence other works show it directly, e.g. in Pesme et al.. Hence I cannot consider it as a contribution (yet, it is in my opinion negligible w.r.t. the main message of the paper!).",
            "summary_of_the_review": "As already expressed, I am very enthusiastic about the paper as they introduce the perfect mathematic tool and scale that characterizes nicely the effect of noise near the interpolation manifold. Furthermore they apply flawlessly and successfully their vision to a nice problem. In a word: Congrats ! ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}