{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "In this paper, the authors extend the FLAMBE to the infinite-horizon MDP and largely improved the sample complexity of the representation learning in FLAMBE. Meanwhile, the authors also consider the offline representation learning with the same framework. Although there is still some computational issue in MLE for the linear MDP, the paper completes a solid step towards making linear MDP for practice. The paper could be impactful for the RL community. \n\nAs the reviewers suggested, there are still several minors to be addressed:\n\n- The extension of the proposed algorithm for finite-horizon MDP should be added. \n- The directly comparison between the sample complexity of FLAMBE and the proposed algorithm in infinite-horizon MDP is not appropriate. The authors should clarify the difference here. \n- The organization of the proof is not clear. As reviewer suggested, the one-step back trick should be emphasized for better significance of the submission."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper studies representation learning in low rank MDPs in both online and offline setting. This paper proposes REP-UCB algorithm, which improves the sample complexity of FLAMBE significantly. In addition, REP-UCB is much simpler than FLAMBE.  For the offline setting, the REP-LCB algorithm also achieves polynomial sample complexity with partial coverage. In particular, the coverage is measured by the ground-truth feature.",
            "main_review": "This paper is well-motivated: representation learning is a very important question in RL, both empirically and theoretically. This paper is also well-written, and the technical part of this paper is easy-to-follow.\n\nOn the plus side:\n-\tThe REP-UCB algorithm is conceptually very simple: it alternates between learning a model (using MLE) and finding an optimistic policy. In terms of the computation complexity, the REP-UCB algorithm is oracle efficient, using the same computation oracle as FLAMBE.\n-\tThis paper significantly reduces the sota sample complexity in the online setting to a reasonable level. Although technically speaking, FLAMBE focuses on reward-free exploration, where this paper focus on a standard online setting.\n-\tFor the offline setting, this paper achieves polynomial sample complexity with partial coverage, where the coverage is measured by the ground-truth feature. In contrast, previous results in the offline setting assumes a known representation, or requires a coverage over all hypothesis. \n\nOn the minus side, I have the following questions/concerns:\n-\tThe sample complexity in this paper is measured by the number of episodes N. In each episode, the algorithm collects one (s,a,s’) tuple from distribution d^{\\pi}_{P^*} with uniformly random action. However, after taking a random action, the distribution of the state changes. Does it mean that, in order to collect one transition tuple, the algorithm needs to interact with the environment for multiple steps? How does this mechanism affect the sample complexity?\n-\t-\tIn the proof of Lemma 12, the last equation block in Page 18: Could you elaborate on the first inequality (the MLE guarantee)? If I understand correctly, the inequality is doing a change of measure (from \\hat{P} to P^\\star). But the MLE guarantee only upper bounds the expected one norm squared. So a naïve Jensen gives an $\\sqrt{\\zeta_n}$ upper bound, instead of \\zeta_n stated in the paper. Then $n\\sqrt{\\zeta_n}$ is on the order of \\sqrt{n}, and I expect that this bound is too loose for later steps.\n\nAdditional questions/remarks:\n-\tDid the authors try regret minimization in the online setting? It seems to me that the random action in data collection creates some technical difficulty.\n-\tIn Page 14, the second displayed equation in the proof of Lemma 8: it should be ${\\hat{\\Sigma}_{n,\\phi}^{-1}}$.\n\n======= after rebuttal =======\nThe authors' response addressed my concerns. I'll raise my score according.",
            "summary_of_the_review": "This paper is well motivated, well written, and the results are impressive. But I don’t fully understand some of the derivations (see main review). So at this point, I can only suggest a weak accept with low confidence. If my concerns were addressed, I’m happy to raise my score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies the representation learning for linear MDP. The authors provide an online algorithm and its offline counterpart. Theoretical analysis justifies that the proposed algorithms are sample efficient.",
            "main_review": "Strengths:\n\nThis paper improves the sample complexity bound of [FLAMBE](https://arxiv.org/abs/2006.10814) by doing the exploration, exploitation, and model selection at the same time. This interplay is much more practical and interesting than the previous explore and commit version. Also, in the offline algorithm (Rep-LCB), the proposed relative condition number provides some insight into the coverage of the offline dataset. It also makes sense by comparing the result of the offline algorithm with the policy covered by the offline dataset.\n\nWeakness:\n\nFirst, I'm concerned with the actual sample complexity of the algorithm. The authors calculate the sample complexity by counting $N$, which is the total episodes. However, it takes $poly((1 - \\gamma)^{-1})$ in expectation to sample state $s$ from distribution $d_{P^*}^{\\pi}$. Also, [FLAMBE](https://arxiv.org/abs/2006.10814) considered a time-inhomogeneous model for finite-time horizon setting, while here a time-homogeneous, infinite-time horizon setting is considered. I wonder if it is fair to directly convert the $H^{22}$ to $(1 - \\gamma)^{22}$ here.\n\nSecond, the realizability assumption (Assumption 2) sounds wired to me. Why we need **any** combination of $\\mu$ and $\\phi$ is a (signed) measure (i.e. $\\int_{s'} \\mu^\\top(s')\\phi(s, a)\\mathrm d(s') = 1, \\forall \\mu, \\phi, s, a$)? This assumption sounds too restrictive to me, and it has never appeared in the previous literature (like [FLAMBE](https://arxiv.org/abs/2006.10814)) to the best of my knowledge. It would be better if the authors can provide some examples satisfying that assumption.\n\nThird, though the previous literature made the same Maximum Likelihood Oracle as in Definition 3 here, I'm wondering if this optimization oracle can be solved efficiently for an arbitrarily large state space. It would be beneficial if the authors can provide some optimization solutions for this oracle.",
            "summary_of_the_review": "This paper studies the representation learning in linear MDP and significantly improves the previous sample complexity. However, as mentioned above, I'm concerned with the following aspects:\n- Sample complexity of the algorithm and fairness comparing two algorithms in different settings (time-inhomogeneous v.s. time-homogeneous)\n- Assumption 2 sounds too restrictive for me.\n- Definition 3, though appeared in the literature, I wonder if it can be solved efficiently\n\nBased on the concerns above, I lean towards rejecting this paper but I'm open to further discussions.\n\n-- post rebuttal\nI've updated my score since the authors clearly addressed my concerns.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper studies low-rank episodic MDPs when the reward is deterministic and the reward function is known. The paper proposes a method that collects data, uses the data to estimate the low-rank MDP, uses this estimate, confidence bound around it, to come up with the policy to be used for the next episode. \n\nThe algorithm provides PAC style bound on how many episodes are needed to learn epsilon optimal policy.\n\nThe authors extend the scope of this work to offline low-rank MDP where a logged data set is given where an RL algorithm is needed to learn and optimize to come up with a new policy.\n \n\n\n\n\n\n",
            "main_review": "The paper is well written. \n\nThe paper provides a few tangible novel definitions such as partial coverage in offline RL.  \n\nHowever, the paper needs further work to be ready. \n\n1- The first major drawback of this paper is that the online algorithm just uses one sample per episode, no matter how long is the episode. I doubt anyone would implement this algorithm in practice and the insight is limited. I understand it makes the theory cleaner, but the authors are encouraged to provide a full study. \nAlso, it is not clear how this algorithm might be extended to undiscounted finite-horizon MDPs, a subset of episodic MDPs. \n\n2- It seems the work assumes M is finite. While extensin to infinite set might be straightforward, including it in the paper is necessary. In that case, is the MLE part efficient?\nNote that is it quite a limiting setting since state-space seems to be large.\n\n3-  The condition that for any mu and phi \\int_s' mu(s') phi(s,a)= 1 is very strong and very limiting. The authors are encouraged to relax it. \n\n\nI might have been wrong in the above statements.\n\nThe paper studies an important problem, but some details are still left to be developed and the authors are encouraged to address them. \n\n\n",
            "summary_of_the_review": "The paper is well written,\nAddress an important problem.\nHowever, there are pieces of this work that are still missing to complete the work. \n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper consider the representation learning in low-rank MDPs under infinite-horizon settings. The authors provide the improved sample complexity and statistical error for the online and offline scenarios correspondingly, based on an observation called the almost optimism/pessimism at the initial state distribution.",
            "main_review": "The almost optimism/pessimism seems not surprising to me. The key observation, as far as I understand, is that we can maintain the data distribution for the online setting via certain stop criteria in infinite horizon scenarios. That is to say, when we terminate the episode with probability $1-\\gamma$ at each step, the state distribution we sample is exactly the stationary state distribution. If we uniform sample the action at the state we terminates, by standard generalization arguments (e.g. Lemma 12), we only need to pay an additional $|\\mathcal{A}|$ factors, which will be fine. Also, such stationary distribution connects the value function with the sampling distribution of dataset, which is helpful for the theoretical derivation (e.g. in Lemma 9).\n\nAlthough the overall idea is interesting, I have several concerns on this paper, both from the high-level perspective and the technical details.\n\n1. If I understand correctly, the proposed algorithm can only be worked on the infinite horizon cases, as it heavily relies on the observation that when we terminate the episode with probability $1-\\gamma$ at each step, we can return the sample of state following the stationary distribution. As far as I know, this does not hold for any real ''episodic’’ case, where we always finish at a fixed number of step. I would like to ask, is it possible to construct a similar algorithm for the episodic setting that terminates at a fixed number of step, following the idea proposed in the paper, as the authors argue they follow the exactly same setup as Flambe in Table 1, but the basic setting already differs. And if no such generalization, I will suggest the authors revising such claim and making the application of this trick and the potential limitation of this trick more explicitly.\n\n2. I would also want to ask the question in the following technical way: how can we track the state visitation probability other than the policy cover methods in Flambe if we consider the fixed termination setting? I feel things can be even worse when the transitions can depend on the specific level $h\\in[H]$ where $H$ is the length of each episode (which is the exact setting of Flambe). Does this paper provide new insights on that? Or is this paper only provide a cheap way for the policy cover, utilizing the structure of infinite horizon setting?\n\n3. The proof part is not well organized and can be hard for the potential readers to understand. There are also some typos in the derivation, most of them are easy to fix. I want to name some typos that make me extremely confused at the first glance. At the end of Page 15, the authors claim to replace $\\pi_n$ with $\\bar{\\pi}_n$ via importance sampling. However, $d$ is the stationary distribution, whose density ratio cannot be directly bounded by $|\\mathcal{A}|$. Such transformation is needed for the application of ellipsoid potential lemma, and I guess here $d^{\\bar{\\pi}_n}$ is not the stationary distribution of $\\bar{\\pi}_n$, but the stationary distribution of $d^{\\pi_n}(s) \\times \\text{Uniform}(a)$. Overall, the $\\bar{\\pi}_n$ is never defined. I hope the authors can go through the proof again to eliminate all of these ambiguities.\n\n---------------\nUpdate after the rebuttal: I check the proof again and understand the generalization to the time-inhomogeneous setting now. The one-step back trick can be conducted at any time step $h$ conditioned on the visiting distribution at $h-1$, and with recursion we can go back to the initial distribution $d_0$, which I now feel is the most significant contribution to the community. In this way, we don't need to construct point-wise upper bound like UCBVI and LSVI, which can be beneficial for the learning of low-rank MDP (as if I understand correctly, Flambe, in some sense, aims at providing point-wise upper bound so that they need to make the policy cover with the ellipsoid planner). I have adjusted my score accordingly.\n",
            "summary_of_the_review": "The authors don’t properly address the difference between the newly proposed method and Flambe, and I feel the proposed method, although delicate, has so many restrictions that the authors don’t properly address. Meanwhile, the proof is not well-polished. I hope the authors can refine the writing during the rebuttal and make the intuition much more clear, compared with the current highlight of analysis in Section 4.2, that implicitly use several convenient property of the infinite horizon setting.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}