{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "All reviewers believe that this paper is valuable, and the authors have made a significant, careful contribution.\n\nSome suggestions from the area chair:\n- \"in causality\" is not a standard technical term and also not non-technical idiomatic English, so it should be explained the first time it is used.\n- The authors should briefly cite and discuss research on so-called positive and unlabeled (PU) learning. This seems like the special case where there is exactly one known class and one novel class. The distinction between sampling in causality and labeling in causality appears in the PU literature, though not under this name.\n- The authors could also mention the obvious but surprising point that if data are generated by two clusters, then a classifier can be learned using exactly one labeled example--not even one from each class.\n- I have read the reference EJ A’Court Smith. Discovery of remains of plants and insects. _Nature_, 1874 and I fail to see its relevance. It is only one paragraph. Work from the 1800s should not be cited merely to suggest a veneer of scholarliness.\n- The writing uses italics for emphasis much too often."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "Learning to discover novel class is a very challenging task and a new research topic in recent years. In this task, a known-class dataset can be used to help cluster novel classes. “Novel” means that there are no overlaps between novel classes and known classes. This setting looks ill-defined since it is not clear why we need known classes to help cluster novel classes. However, this paper answers this question based on a novel concept: K-epsilon separation and points out when this setting is ill-defined/well-defined. Based on this contribution, this paper is above the acceptance borderline. \n\nNevertheless, the presentation should be polished. Although I understand the setting and the contribution in the end, the presentation flow is not smooth. Some typos make me struggle when I read this paper. Besides, the connection between sampling process and Assumption (D) should be discussed deeply. I find that they might not the same thing. \n",
            "main_review": "Pros:\n\n1.  The analysis of solvability is very interesting. Since we only need to cluster novel classes, it seems that introducing additional data is useless. In this paper, the authors answer this question and point when L2DNC is a meaningless problem. This point might affect the development of the L2DNC. Confirming the boundary of solvability of L2DNC is important since we can naturally do the clustering task with novel classes themselves.\n\n2. Considering few-observation situations make L2DNC methods be used in more practical scenarios.\n\n3. The proposed method can improve the performance with large margins, which is important to the field. CATA is also a new sampler to the meta-learning field (ablation studies verify the effectiveness of CATA as well).\n\n4. Interacting with meta-learning provides more possibilities to improve the performance according to SOTA meta-learning methods. The meta discovery is a general framework to address L2DNCL.\n\nCons:\n\n1. I feel struggling when I first read this paper. Figure 1(a) misleads me and lets me think that L2DNC aims to detect novel classes. You should explain L2DNC at first in the caption of Figure 1.\n\n2. I cannot follow the benefits of the sampling process (Y->X). The difference between X->Y and Y->X should be discussed in detail in introduction (it is important to make readers understand them in the introduction).\n\n3. K-epsilon-separation r.v. is a very interesting concept. However, I am not sure why we need \\tau=\\epsilon rather than \\tau<\\epsilon. The latter seems more natural.\n\n4. There is a typo in (2), it should be \\mathbb{P}_{\\pi(X)} rather than \\mathbb{P}_{X}, which makes me very confused when I read (2).\n\n5. From two theorems, Assumption (D) is indeed an important assumption for L2DNC problem. However, the connection between (D) and Y->X is not very clear. I find that (D) is not the sufficient and necessary condition of Y->X. More discussions are needed here. You cannot simply say that Y->X means (D). From my experience, (D) contains more situations. So, what are these situations?\n\n6. The condition used in Theorem 2 should be explained in natural language. \n\n7. Directly introducing meta-learning (using “few”) is not good. At least, you should introduce meta-learning based on (D) or Y->X.\n\n8. Expect for CATA, are there new components in MM or MP?  \n\n9. Experiments look nice, but I am interesting to know why MP can have such good performance in OmniGolt.\n",
            "summary_of_the_review": "The current version is a borderline paper because some important parts seem unclear to me. The presentation should be polished, and the connections among sections should be strengthened. I expect a better version after the discussion period.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper introduces meta-discovery as a way to adapt meta-learning\nstrategies to the problem of learning to discover novel classes. It\nalso presents a formalization of the problem that helps shading light\non the conditions under which it is learnable.\n\nThe authors additionally introduce a novel sampling strategy aimed at\nsampling data having the same \"dominant\" view to help the following\nclustering procedure.\n\nThe experimental evaluation shows advantages over existing recent\ncompetitors when the number of examples for unseen classes is small.",
            "main_review": "Learning to discover novel classes (L2DNC) is a challenging task,\nwhere the learnability conditions are not entirely clear. The work\ncontributes to formalize some intuitive notions of learnability in\nterms of epsilon-separation and consistent transformation sets,\nlisting the assumptions that are needed for L2DNC to be learnable.\n\nThe need for a shared transformation/representation, that is one of\nthe results of the formalization, is sensible but clearly not\nnovel. On the other hand, the author introduce a multi-view learning\nthat intuitively makes sense as a way to help clustering to focus on\nthe correct semantic features, but its connection with the formal\ntreatment is less clear. \n\nResults are reasonably well-discussed, and include both few-data (the\nsetting of the paper) and large-data results (where performance\ndifference shrink, as expected) are well as ablation studies.\n\nMinor points:\n\nWhy \"X is K-epsilon-separation\" ? should it be K-epsilon-separable or something?\n\n",
            "summary_of_the_review": "A useful formalization of the conditions under which L2DNC works.\n\nAn intuitively sensible multi-view based meta discovery algorithm\nshown to outperform existing alternatives in the low-data regime.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper considers the problem of novel class discovery (NCD). Different from the original setting of NCD, this paper reconsiders the assumptions behind NCD and defines a new yet more practical setting, which can significantly reduce the number of unlabeled data needed for training novel classes. In addition, this paper presents a meta-learning-based approach to address the new setting, which achieves consistent improvements on four public datasets. They also provide a theory to reveal why we need to assume that known and novel classes should share semantic features.",
            "main_review": "Pros:\n\n+ This paper is well-written and easy to follow. The motivation of the new setting of NCD is clear.\n\n+ This paper considers the problem of NCD in a new perspective and rigorously formulates the NCD setting. \n\n+ A theoretical analysis is provided to demonstrate when the NCD setting can be solved. In addition, they reveal that NCD can be solved if known and novel classes are related. They also show a result to see how NCD fails if one key assumption is not satisfied. The authors give detailed algorithms and the source code, which can help readers better understand and reproduce the proposed method.\n\n+ Extensive experiments are provided to verify the effectiveness of the proposed method. Also, all the results include the standard deviation, which is important for reflecting the real improvements on the CIFAR and SVHN datasets.\n\nCons:\n\n- It is interesting to see a formulation of NCD problem. In this formulation, we can know when the labeled data can help cluster novel-class data. However, the connection between the theory and solution is not very strong. It is better to move some parts in the Appendix to the main context. Using meta-learning to define L2DNCL is also an interesting part. \n\n- In Definition 1, why do you need to set an equation here? Is it better to consider an inequality? It is like that the separation is less and \\epsilon? A similar question follows in Definition 2.\n\n- It is not appropriate to say “Impossibility Theorem for Previous Setting”. I admit that previous studies do not consider NCD based on some basic assumptions, but they still propose many working algorithms for NCD. Thus, I recommend deleting the “Previous Setting”. You can add some explanations below Theorem 2. \n\n- The pipeline of meta-learning should be introduced clearly. The connection between Sections 3 and 4 is somehow weak. I understand that meta-learning is suitable for few-shot situations. However, it is not good to use this as the motivation of your method. It is not easy to know how to use the output of Algorithm 2. I found that L2DNCL is demonstrated in Appendix, which is much clear. You can consider swapping some descriptions from Appendix to the main context.\n\n- When training the pairwise loss, did you use different outputs for the labeled data and unlabeled data? That is, using outputs with C_l-dim for the labeled data while outputs with C_u-dim for the unlabeled data.\n\n- How to sample the training data without the CATA? I did not see the trivial sampling procedure for MEDI.\n\n- How about include the results of K-means in all figures? I did not get the point why you need Table 1.",
            "summary_of_the_review": "Overall, this is an interesting paper, which reformates the problem of NCD to a more realistic setting. In addition, this paper builds a connection between meta-learning and NCD and presents an effective approach to address the introduced problem. Extensive experiments are provided to verify the effectiveness of the proposed method. Some statements and implementation details are not very clear, I would like to see the response of the authors.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "The authors provide a formal definition of the L2DNC task, give proofs of useful insight into the problem, and empirically demonstrate novel methods. The authors use several baselines and ablation to demonstrate the value of the proposed methods. The theoretical insights suggest that common high level features are necessary to solve the L2DNC task, which motivates the proposed approaches based on low dimensional non-linear projection of the data. Similarly the clustering-centric definition of the problem motivates the similarity learning methods presented.",
            "main_review": "Overall, I found this paper very interesting. The authors provide a definition and insight into a problem that is often approach with little rigor or formalization. Further, the formalization gives good motivation for the proposed methods MM and MP. However, CATA is less well explained.\n\nMy understanding is that CATA learns a low-dimension projection and set of orthogonal classifiers. It then assigns each training point to a group defined by which of the orthogonal classifiers was most strongly activated by the observation. Meta learning inner tasks are then sampled from each group. It is unclear to me what exactly this aims to accomplish. I think the authors should explain the task-sampling problem in meta learning more completely. \n\nTypo on page 6. \"For $z_i = \\pi_{mm}(x_i)$ and $z_i = \\pi_{mm}(x_j)$\" should be $z_i = \\pi_{mm}(x_i)$ and $z_j = \\pi_{mm}(x_j)$\".\n\n",
            "summary_of_the_review": "The theoretical framing of the L2DNC problem is valuable, as it can help us understand what is necessary to solve this problem effectively. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}