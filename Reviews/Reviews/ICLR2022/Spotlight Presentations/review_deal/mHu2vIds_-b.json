{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper integrates model ensembles with randomized smoothing to improve the certified accuracy. The methodology is motivated theoretically by showing the effect of model ensemble on reducing the variance of smooth classifiers. Moreover, it proposes an adaptive sampling algorithm to reduce the computation required for certifying with randomized smoothing. Extensive experiments were conducted on CIFAR10 and ImageNet datasets.\n\nThe strengths of the paper are as follows:\n+ In terms of significance of the topic, the problem tackled in the paper is significant and highly relevant. \n+ The motivation of using model ensemble is clearly illustrated via a figure and well justified with theoretical analysis. \n+ Algorithmically, the paper proposes Adaptive Sampling and K-consensus algorithms to reduce the computational cost, making the method more practical.\n+ Experimentally, the paper exhibits competitive results against several frameworks for training smooth classifiers and on several datasets."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper integrates model ensembles with randomized smoothing to improve the certified accuracy. The methodology is motivated theoretically by showing the effect of model ensemble on reducing the variance of smooth classifiers. Moreover, it proposes an adaptive sampling algorithm to reduce the computation required for certifying with randomized smoothing. Extensive experiments were conducted on CIFAR10 and ImageNet datasets.",
            "main_review": "This paper has several strengths.\n\n- The problem that this work is tackling is both of interest and importance.\n- The theoretical analysis presented in section 5 along with figure 2 potentially motivates the use of model ensemble.\n- The Adaptive Sampling proposed in section 6 algorithm is practical.\n- The experiments conducted in this work are extensive. They cover several frameworks for training smooth classifiers and several datasets. In most of the setups, model ensemble resulted in improvements in the certified accuracy.\n\nHowever, there are several concerns that need to be addressed:\n\n- The main claim about the time reduction that CertifyADP provides is based on an unfair comparison with Certify. The amount of information that both algorithms provide is not the same. CertifyADP tells whether an instance is certified with a pre-chosen radius $r$ or not, however it gives no hint about the actual radius that the instance is certified with, which is the output of Certify. I am not trying to lower the importance of CertifyADP since in many practical situations, it is useful to know whether an instance is certified with a given radius or not, but the comparison against Certify needs to be fair.\n\n- The writing of the paper can be significantly improved (especially section 5). \n\n\nHere are few comments/suggestions:\n\n- typo: In page 4 third paragraph, \"$y_p$ to be zero-mean\".\n\n-  While the analysis in section 5 are interesting, consider shrinking it to include more experiments in the main paper such as Table 15. It is interesting to see that the ensemble of models trained by different frameworks outperforms individual models. \n\n- Consider switching the locations of figures 5 and 6 since figure 6 is mentioned in the text before figure 5.\n\n- The definitions of SampleRF, TimeRF, and KCR are in the last page in the paper. However, they are important to understand table 3 and the \"Computational Overhead Reduction\" paragraph.\n\n- Figures 2 and 3 need more elaborative discussion.",
            "summary_of_the_review": "This work has several merits as pointed out in the main review. The key concern is the unfair comparison between Certify and CertifyADP presented in this work. Moreover, the writing of this paper can be vastly enhanced for better exposure. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "In this paper, the authors propose using the aggregation of an ensemble of similar models as the base classifier in the randomized smoothing (RS). They show that the use of ensembles helps reduce the variance of the base classifier under noisy inputs, thus, improving the performance of RS. Both theoretical arguments and numerical experiments are included to support their idea. Further, the authors provide practical algorithms that significantly reduce the computational costs of their method.\n",
            "main_review": "Strengths:\n1. From both theoretical and experimental perspectives, the authors demonstrate the effectiveness of combining randomized smoothing with ensembles in building certified robust classifiers.\n2. Though the theoretical arguments include many assumptions, the authors carried out experiments to validate them.\n3. They proposed Adaptive Sampling and K-consensus algorithms to reduce the computational cost, making their method more practical.\n\nWeaknesses/Concerns:\n\nWhile the empirical evidence are sufficient to support the effectiveness of using ensemble to boost the performance of RS, I have some concerns about the assumptions in the theoretical arguments. \n\nIt assumes that the \"correlation between the logits of different classifiers has a similar structure but smaller magnitude than the correlation between logits of one classifier\". Since different classifiers only differ in random seed for training, they should share behaviors in common. However, it is quite possible that such commonness is captured by $\\mathbb{E}_l[f^l(x)]$, reflecting the underlying task, network architecture, dependence on $x$, etc., while the deviations of different models $\\delta^i$ and $\\delta^j$ are uncorrelated.\n\nIn fact, as shown in Figure 11(a) in the appendix, the off-diagonal blocks in the Delta subfigure are almost identical to the ones in the True covariance subfigure. It would be possible to reconstruct an argument about the variance reduction solely based on the assumption that $Cov(y^i_c, y^j_c)$ are near zero.",
            "summary_of_the_review": "The underlying topic of constructing a certified robust classifier is important. The main idea of the paper is sound and is supported by extensive experiment investigations. Further, the authors proposed algorithms to reduce the computational cost, making their method not only workable but also practical. There also includes empirical studies to validate (to some extent) the assumptions in their theoretical arguments. Hence, I would like to support the publication of the paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The paper aims to boost randomized smoothing (RS). Specifically, the paper demonstrates that an ensemble of diverse base models can enhance RS both theoretically and empirically. The key insight is that reducing variance of ensembles over the introduced perturbations can lead to more consistent classifications for inputs. The paper also introduce two simple  yet effective techniques to speed up the certification. Extensive experiments are conducted to thoroughly evaluate the proposed boosted RS.\n",
            "main_review": "Strengths\n\n+Important research problem\n\n+Well written and well-organized\n\n+Interesting analysis on why ensemble works\n\n+Extensive evaluation\n\nWeaknesses\n\n-Evaluation only on the L2 certification\n\n-Missing related work",
            "summary_of_the_review": "Certifying an ensemble of models is faster than certifying an individual model through the proposed two strategies. However, training an ensemble of models requires far more computational resources and time than training an individual model. Although the authors mainly leverage the pretrained models, but in practical applications this may be infeasible. I would suggest the authors discuss the issues of training an ensemble of models.   \n\nThe authors say that “…substantially increased certifiable radii for samples close to the decision boundary..” in abstract, but no results to validate this claim. I would like to see the results for those samples having a small margin.  \n\nThe authors assume that y_c is the clean part and y_p is the perturbation/noisy part. How to define y_p and y_c when evaluating the real image datasets? Or y_p and y_c are just for analysis? \n\nWhat’s the impact of $\\alpha$ and $\\beta$? \n\nIn table 1, why ensemble is less effective than MACER when radius r is small? \n\n“Even when using only K = 2, we already obtain 81% and 70% of the ACR improvement obtainable by always evaluating the full ensembles (k = 10 and k = 50) “ => How do you calculate the numbers 81% and 70%? \n\n In Table 3, what do SampleRF, KRC, and TimeRF mean or short for? \n\nMy another concern is that the experimental results are only for certifying L2 perturbation.  There are several papers that design RS to certify $L_0$, $L_1$, $L_\\infty$ perturbation.  For example, \n\n$L_0$: Wang et al. “Certified robustness of graph neural networks against adversarial structural perturbation via Randomized Smoothing”\n\n$L_1$: Lee et al., “Tight Certificates of Adversarial Robustness for Randomly Smoothed Classifiers”, NeurIPS 2019\n\n$L_\\infty$: Yang et al., Randomized Smoothing of All Shapes and Sizes\n\nI am also really interested in whether the proposed ensemble is also effective for certifying $L_0$, $L_1$, or/and $L_\\infty$ perturbation. \n\nThe following papers also derive certified robustness based on randomized smoothing: \n\nJia et al., “Certified Robustness for Top-k Predictions against Adversarial Perturbations via Randomized Smoothing” \n\nZhang et al., “Black-Box Certification with Randomized Smoothing: A Functional Optimization Based Framework”\n\nMohapatra et al., “Higher-Order Certification for Randomized Smoothing”\n\nKumar et al., “Certifying Confidence via Randomized Smoothing”\n\nKumar et al., “Curse of Dimensionality on Randomized Smoothing for Certifiable Robustness”\n\nFischer  et al., “Certified Defense to Image Transformations via Randomized Smoothing”",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}