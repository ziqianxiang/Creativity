{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "Thanks for your submission to ICLR.\n\nThree of the four reviewers are ultimately (particularly after discussion) very enthusiastic about the paper, and feel that their concerns have been adequately addressed.  The fourth reviewer has not updated his/her score but has indicated that their concerns were at least somewhat addressed.  I took a look at their review and agree that the authors have addressed these concerns sufficiently.  I am happy to recommend this paper for acceptance at this point.  Note that I really appreciate the time and effort that the authors went into adding additional results and clarifications for the reviewers."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper introduces a method of aligning the support of one distribution to another. This is important as for domain adaptation problems, distributional alignment can come with a lot of problems (such as moving the mass of one class to another when there are differences in representation of different classes).",
            "main_review": "Strengths:\nOverall a great paper. Very easy to follow, the algorithm seem theoretically grounded and simple and very well motivated. It's a very important domain, and the story is pretty clear in that regard. Uses well-known tools (e.g., GAN discriminator), so should be easy to implement. Experiments are well-motivated and support the approach.\n\nWeaknesses (not really) / comments:\nOne thing that would be good in this work is to see this applied to unsupervised domain translation (e.g., converting SVHN to MNIST). There are a number of works that attempt to address this problem (e.g., Binkowski 2019), but they come with a great deal of complexity. Have you tried this approach?\n\nI feel like d(.,.) could have been clarified better earlier, at least to provide some examples to avoid the reader wondering about what it was until the experiment section.\n\nBinkowski 2019: Batch weight for domain adaptation with mass shift",
            "summary_of_the_review": "The paper is very clear, well motivated, the approach is simple, and problem is important.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "Motivated by label shift between source and target domains, the paper proposes a novel support alignment task with a novel support divergence.  They provide theoretical results for these and build up theory that leads to a simple adversarial algorithm for support alignment using a discriminator and nearest neighbor algorithms in 1D.\nThe paper provides extensive empirical results comparing to common domain adaptation methods.\n",
            "main_review": "**Strengths:**\n- Proposes a novel support (pseudo-)divergence (not a true divergence because D = 0 means the supports match but possibly the distributions do not.) to measure the difference in supports between distributions.  This is an extension of the Hausdorff distance.\n\n- Theoretically proves that alignment of the support of the optimal discriminator implies alignment of the distributions and gives several insights about alternatives.\n\n- Place the support alignment goal and objectives within related relaxed and full alignment objectives.\n\n- Extensive empirical experiments under the label shift setting.\n\n**Weakness:**\n- My main concern is that one solution to support alignment is distribution alignment.  And more importantly, the algorithm (as shown by the mini-batch part) seems to naturally collapse to distribution alignment (as evidenced by the mini-batch issue).  However, the label shift application suggests that a better solution is to align the supports but NOT align the whole distribution (even though that is one solution). Thus, it seems an explicit regularization or something should be added to the objective rather than an ad-hoc history buffer.  One suggestion might be a transportation cost regularization to the transformation (see below).\n\n- Another issue is that the support alignment method is only evaluated using the extrinsic task of domain adaptation.  The paper does not include results on evaluating the support alignment explicitly.  Even a simulated experiment may help to evaluate the algorithm with respect to support alignment directly.\n\n**Minor Weaknesses:**\n- The history buffer idea seems to be entirely heuristic.  Can you explain why this might be important theoretically? Or why without it there may be a collapse to distribution alignment?  My intuition is that the objective will naturally move towards shrinking these values towards 1/2 as mentioned below in comments but maybe there is something else going on.\n\n- The idea of \"constraints\" being an issue for distribution alignment seems a little odd (as in the first figure).  The solution seems to be to increase the model capacity rather than align the support of the distribution. This seems to be a fairly weak high-level motivation for support alignment (at least in the introduction). The shift in label distributions is a stronger motivation in my view and should be the primary example.\n\n- Proposition 2.2 is a bit confusing to understand.  Maybe a little illustration would be helpful to show the densities of $\\tau_p$ and $\\tau_q$ and the resulting ratio of densities for $\\tau$ along with the $x$ and $x'$ that was mentioned in the explanation.\n\n- The paper should make a clearer distinction between \"continuous\" support alignment (which seems to be the overall benefit) and discrete support alignment (which seems to be the case in the algorithm which uses samples).\n\n\n**Other comments or questions**\n- It is interesting that full distribution alignment corresponds to the case when the optimal discriminator is just 1/2.  Thus, it seems that concentrating all mass near 0.5 is really distribution alignment, while \"spreading\" out the mass of the $\\tau$ predicted probability distributions (except for 0 and 1) means that the supports are aligned.  Is this a correct intuition/interpretation?  Is this also why mini-batch training actually finds this solution (as it is the most stable solution maybe)---i.e., we can always improve the objective by shrinking towards 1/2, which is the full distribution alignment case?\n\n- If the above interpretation is correct, then it would seem that you could actually enforce support alignment but NOT distribution alignment by requiring $\\tau$ to have high entropy.\n\n- It seems that another way to avoid this \"collapsing\" phenomena would be to add a transportation cost term to the transformations.  Thus, we would seek to move the points as little as possible to align the support of the distributions. This would also be much simpler and more grounded than the history idea. Have you considered this idea?\n\n- Proposition 4.3 seems quite strong.  Is statement 1 related to max sliced wasserstein distance (i.e., joint Wasserstein is 0 if and only if max sliced Wasserstein is 0)? \n\n- The result tables should be shown after the description of the methods, metrics, etc.\n\n- I might suggest rewriting with label shift as the core issue as this seems to be the most realisitic example and iis strongly supported by your experiments.  Other settings are not demonstrated by your experiments and are not well-motivated.\n\n",
            "summary_of_the_review": "Overall, the paper presents novel ideas and tasks and places the work well within existing literature.  This could have significant impact on unsupervised domain alignment and more generally on thinking about alignment problems. The main weakness is the potential for the proposed algorithm to collapse to standard distribution alignment and the lack of explicit evaluation of support alignment.\n",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors propose symmetric support difference (SSD) divergence to align supports of distributions. The authors further propose to use the optimal log-loss discriminator to project supports into 1-dimensional space for SSD. The authors also illustrate the advantages of the proposed methods for domain adaptation.",
            "main_review": "+ Strengths:\n\nIt seems that the authors propose a \"new\" divergence to align supports of distributions, and empirically show its advantages in domain adaption.\n\n+ Weakness:\n\nThe proposed SSD is closely related to the Chamfer divergence, if not the same (see ArXiv1612.00603).\n\n+ Detailed comments:\n\n1. Could the authors discuss the differences between the proposed SSD with the Chamfer divergence? (see ArXiv1612.00603, ArXiv2102.04014)? \n\n2. It seems unclear about the motivation of using the 1d-projection? Is it better to consider the original space? What are the advantages of using the 1d-projection?\n\n3. I also wonder about the advantages of using the 1d-projection by the optimal log-loss discriminator. Why is it favored over other projects? (is it suboptimal/optimal in some sense?)\n\n4. The authors emphasize considering the alignment for the supports of distributions instead of comparing distributions. Are there any reasons to consider the expectation instead of average in equation (1) for SSD? [In case, only supports are important, I wonder why SSD  needs their \"weights\" in its definition?]\n\n5. For experiments, I think the proposed method is closely related to optimal transport-based approaches for domain adaption. However, it seems that there are no such baselines yet (e.g., using standard OT, using sliced-Wasserstein, or their variants for domain adaption).\n\n",
            "summary_of_the_review": "It seems that the proposed method (SSD) is closely related to Chamfer divergence if not the same (however, there are no discussions about their relationship yet). For experiments, in my opinion, the proposed approach is similar to optimal transport. Therefore, the authors should compare the proposed approach with other optimal transport-based approaches for domain adaption.",
            "correctness": "3: Some of the paperâ€™s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The authors propose a weak alignment of distributions consisting in equalizing their supports. They justify this choice by the recent advancements in Domain Adaptation that show the drawbacks of a strict alignment of marginal distributions. A distance between supports is defined as a modification of the Hausdorff distance between sets. The support divergence is further proven to be a limit case of a previously proposed weak Wasserstein distance requiring only the boundedness of the density ratio, thus defining a continuum of relaxations of divergences.  For all of the divergences in this continuum, a theoretical result shows the possibility of reducing the original problem of computing the divergence to unidimensional distributions, obtained via a domain discriminator minimizing a Jensen-Shannon divergence. The theoretical findings are used to propose an algorithm that is tested in the class imbalanced domain adaptation setting, highlighting the interest of the approach.",
            "main_review": "# Strengths\n* Rreducing divergence measures between multi-dimensional distributions to unidimensional ones, whether it concerns the support alignment (Theorem 2.1) or the Wasserstein type divergences (Proposition 4.3).\n* The interest in the minimum accuracy per class, which is a metric that is rarely considered in the domain adaptation literature to the best of my knowledge, but that is important in real-world applications.\n\n# Weaknesses\n\n* There is no conclusion summarizing the main findings of the paper at least.\n\n* Experimental part:\n\n    * It is hard to tell how much of the success can be attributed to the developments in the theory, especially that apart from VADA, other methods do not use a conditional entropy loss in their original implementations, whereas this latter loss is added with a coefficient $\\lambda_{\\text{ent}} = 0.1$. Also, the expression of this unsupervised loss should be mentioned in the supplementary material (or at least a reference to its expression, e.g. from [1]). I am not advocating for the idea that an algorithm should surpass all of the state-of-the-art methods, but it is instructive for the community to see if the proposed theory offers the same order of performance.\n    * The domain discriminator has a single update per 1 update of feature extractor and classifier. This does not go in line with the theoretical findings that only concern the optimal discriminator $f^*(x)$, i.e. after convergence. Such a detail needs to be mentioned in the main paper.\n\n# Other comments\n* In Proposition 2.3, a counter-example is provided concerning the equivalence between aligning the supports in the original spaces and their 1D push-forward counterparts induced by a domain discriminator. While the establishment of the result concerning the JS divergence discriminator relies on densities, the provided counter-example concerns discrete distributions. It would be better to provide a counter-example with distributions having densities.\n* Appendix A: it should be $f$ rather than $f_W^*$ in the operand of the $\\sup_{f:Lip(f)\\leq 1}$ operator. \n## Typos:\n* Before Remark 2.1.1: \"techincal\" --> \"technical\"\n* Paragraph **Setup**: \"optimizied\" --> \"optimized\"\n\n# Questions\n* In Proposition B.1, after slicing the distributions by $\\theta \\in \\mathbb S^1$, are the supports of the push-forward distributions equal? Because $f^\\theta \\\\#q$ will have no supports on a segment of length $2$, centered at zero.\n* I am not sure if I understand the notation $q^Y(y) \\propto \\sigma(y)^{-\\alpha}$. Does it imply that for example, we can have $q^Y(1) \\propto K^{-\\alpha}$ ?\n* Provided that I understood correctly the previous point, when $\\alpha$ is very large, it means that some class is almost absent, leading to the partial DA setting. Did you test the approach in this regime?\n* Did you notice any acceleration of the algorithm via using the strongly convex squared distance rather than the absolute one ?\n* When you mention \"all alignment methods\", do you refer to only **ASA-sq** and **ASA-abs**, or to all of the other approaches as well? If the latter case holds, why not use the default parameters of the methods (e.g. $\\lambda_{\\text{align}} = 0.01, 1$ respectively for VADA and DANN, instead of the fixed value 0.1) ?\n* Is it possible to establish theoretical guarantees on the convergence of the empirical support divergence to its true counterpart? and can this aspect be related to the observation that large buffer sizes imply a drop in performance?\n\n# References\n[1] Shu, R., Bui, H. H., Narui, H., & Ermon, S. (2018). A dirt-t approach to unsupervised domain adaptation. *arXiv preprint arXiv:1802.08735*.",
            "summary_of_the_review": "The theory established in the paper is interesting for the community, especially with the general result of reducing the comparison between distributions to a uni-dimensional one. However, there are some gaps between the theory and the proposed algorithm, especially concerning the use of the unsupervised conditional entropy loss on the target domain.\n\n# Post-rebuttal \nAs a result of the authors's detailed response, I update my review score to 8.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}