{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This manuscript introduces a theoretical framework to analyze the sim2real transfer gap of policies learned via domain randomization algorithms. This work focusses on understanding the success of existing domain randomization algorithms through providing a theoretical analysis. The theoretical sim2real gap analysis requires two critical components: *uniform sampling* and *use of memory*\n\n**Strengths**\nAll reviewers agree that this manuscript provides a strong theoretical analysis for an important problem (understanding sim2real gap)\nwell written manuscript, and well motivated\nIntuitive understanding for theoretical analysis is provided\n\n\n**Weaknesses**\nanalysis is limited to sim2real transfer without fine-tuning in the real world\nthe manuscript doesn't provide a novel experimental evaluation\nlack of take-aways\n\n**Rebuttal**\nThe authors acknowledge the limitation of not addressing fine-tuning, but also point out that several papers have performed sim2real transfer without fine-tuning. \nThe authors address the lack of novel experimental evaluation by arguing that the theoretical analysis can be directly linked to existing algorithms for which empirical evaluations have already been performed. I agree with the authors that in that context it seems of little value to redo those experiments. However, I also believe that those links could be made even clearer in the manuscript and I would encourage the authors to do so. Furthermore, while the authors do provide intuitive take-aways for domain randomization algorithms, it would be helpful if those take-aways were more clearly linked to existing algorithms as well (given that there is no experimental evaluation of this). \n\n**Summary**\nThis manuscript provides a theoretical framework for analyzing the sim2real gap and using that framework provides bounds on the sim2real gap. All reviewers agree this is a strong theoretical analysis. Some take-aways on what makes domain randomization algorithms successful are provided by the provided sim2real-gap analysis (memory use, uniform sampling). Thus I recommend accept."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper is a theoretical analysis of domain randomization in the context of latent MDPs. The paper provides bounds on a definition of the sim-to-real gap, defined as the difference between the optimal value function and a \"domain randomization oracle\" value function. In this context the oracle is defined as a history dependent policy which uses the first steps in the environment to improve its belief on the latent variable and then optimally behaves according to this belief. \n\nProvided bounds can be applied in three different contexts: finite domain randomization (i.e. domain randomization on a finite number of domains) satisfying a separation conditions (i.e. a requirement on the existence of a state-action pair which leads to sufficiently different next states), finite domain randomization without the separation condition and infinite domain randomization. \n\nBounds are expressed in terms of D (i.e. a bound on number of time steps required to reach any other state in the state space) and of H (i.e. the number of steps in an episode). In the case of finite simulators with separation the domain gap is O(D log^3(H)). In the case of finite simulators without separation the domain gap is O(D H^(1/2) log^(1/2)(H)). In the case of infinite domain randomization the gap is approximatively O(D H^(1/2) log^(1/2)(H)). \n\nBounds are provided with assumptions and most of these assumptions are justified by proving that removing these assumptions opens the possibility to counterexamples which do not meet the provided bounds. ",
            "main_review": "The paper is well organized and easy to read. Most of the mathematical concepts do come with an intuitive description of their meaning and this allows the reader to get a grasp on the importance of the theoretical results. \n\nThe focus of the paper is very relevant to the ML and RL community. Domain randomization is emerging as a fundamental training procedure and theoretical guarantees on its performance are a necessary requirement for application in the real world. The submitted paper not only provides this bounds but also express them in terms of quantities (e.g. the value function gap) which are relevant for downstream real-world applications. \n\nThe main limitation of the paper is in assuming no fine-tuning with real-world samples. The concept of \"domain randomization oracle\" implicitly assumes the ability to eventually find the global optimal policy exploiting history and memory. Somehow this ability shares some commonalities with fine-tuning and therefore it seems possible to extend the author's approach to the more relevant scenario of domain-randomization with fine-tuning. \n\nAnother limitation of the paper is related to the format required by the conference. The submission is limited to 9 pages which the authors comply to. However, the proofs of all theorems are in the supplementary material which, as a reviewer, shouldn't be necessary to get most of the paper. This is in this case not really the case because I believe proofs are a necessary component to understand the quality and relevance of the paper. \n\nADDITIONAL COMMENTS\n- Page 5. \"The agent does not know explicitly which MDP is sampled, but **she** is allowed to interact with this MDP M for one entire episode.\". -> it\n\n- Page 5. \"We consider the ideal scenario that the domain randomization algorithm eventually **find** the globally optimal policy of this LMDP,\". -> finds\n\n",
            "summary_of_the_review": "An important and relevant paper which addresses the problem of computing bounds for the value function of a policy trained with domain randomization and tested on one instance of the latent MDP distribution sampled during training with domain randomization. The paper doesn't have limiting assumptions besides the absence of analyzing how results extend to the fine-tuning case. \n\nAs a reviewer, checking the correctness of the paper wasn't easy because all relevant proofs are not in the main paper (9 pages) but mostly in the supplementary material (14 pages). The proof overview (section 6) isn't enough to check the paper correctness. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a theoretical framework for reasoning and analyzing domain-randomization techniques. Its approach models a simulator instance as a Markov Decision Process for which the parameters of the MDP correspond to tunable simulator parameters. In this designed setting, the work proves sharp bounds on the gap between an optimal policy's value in the domain-randomized and the real-world setting. The work also analyzes the conditions under which sim-to-real transfer can be successful in the considered theoretical setting.",
            "main_review": "Overall I believe that the main strength of this work is providing a theoretical analysis of sim-to-real deployment via domain randomization. Sim-to-real is of particular importance for enabling safe training in simulation. Theoretically analyzing and understanding this setting has the potential to provide safety guarantees and inform the design of new algorithms. While the analysis provided by this work is interesting, in the current stage I have three major concerns. First, its motivation and contribution seem to be detached from sim-to-real practice. Second, the assumptions (if I understood them correctly) seem to be very restrictive, and finally there is no evaluation whatsoever (e.g. to quantify a gap between the derived bounds and any realistic sim-to-real setting)\n\n# Motivation and Contribution\nIt is a very interesting approach to analyse sim-to-real transfer by interpreting simulators as a class of MDPs with different transition models. However, in the way this work is currently phrased, I am not sure if it provides any practically useful results. A theory work does not necessarily have to be at a stage that it is useful but I would have expected a clear technical discussion of the papers limitations in order to make use of such results in any real-world system. Currently, the link to practical implementations is provided to some extent in Sec 3.2. However, this section feels very detached from the rest of the work. It does introduce how domain randomization works in practice but does not discuss how the results of this work relate to it.\n\n# Assumptions\nThe initial discussion speaks of domain randomization in general. However, the assumptions seem to be very restrictive:\n\n* While I believe that assumption 1 is common, it would be interesting to see for which simulator real-world simulator it holds. In the case where the state-space is continuous with continuous support, the expectation $E(T(s'|\\mathcal{M}, \\pi, s))$ can easily become infinite for many interesting system classes?\n* There is a similar issue with assumption 2. As I understand it, it makes it very hard for the simulator instances in $\\mathcal{U}$ to be configurable in a continuous way because this assumption basically enforces to have a minimum distance between any two simulator instances. I wonder where this happens in practice.\n\nAgain, in theoretical work, it may be okay to have such restrictive assumptions. However, the authors should spell out which simulator & environment combinations currently satisfy such assumptions. And if there are none (which may be okay for an otherwise interesting theoretical work), there should be at least a discussion on what are the main limiting factors of the assumption compared to practical settings.\n\n# Evaluation\nI believe that the absence of any experiments is also problematic. Theoretical papers do not necessarily always need to have experiments (some theoretical contributions cannot be evaluated or may not require experimental evaluation). However, in the present case, the work aspires to address a practically relevant setting and seeing the performance evaluated seems quite relevant given that Domain Randomization already has an extensive empirical body of work. Even some of the pure theoretical results can be nicely illustrated with numerical experiments such as demonstrating cases in which derived bounds are particularly good or particularly bad.",
            "summary_of_the_review": "In summary, I believe that this work addresses an interesting problem. Its main shortcomings are the lack of discussion linking the particular contribution and assumptions to particular use-cases as well as the absence of any experimental demonstrations.  Addressing these shortcomings will result in an interesting contribution.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The goal of this paper is formalize and analyze the technique of domain randomization in robotics. The authors frame domain randomization as training over a set of \"plausible MDPs\", only one of which is the MDP that will be used at test time. The authors then analyze the best possible performance of models for this problem under 3 different settings: when the set of plausible MDPs is finite with and without a separation condition, and when the set of plausible MDPs is infinite. The authors find algorithms that achieve a performance gap that beats the worst case performance gap of $O(H)$ in all three settings ($O(log^3H)$ and $O(\\sqrt{H})$).",
            "main_review": "I found this paper very well written and easy to follow. Each assumption that the authors make - for instance that the MDPs are communicating or that the value functions are smooth - is justified by showing that there exist MDPs where the best case transfer performance is $O(H)$ in which case not much can be said. The bounds themselves are also intuitive and explain (to some extent) why zero-shot transfer is possible when training with domain randomization. \n\nOne suggestion that I had: the authors mention in the abstract and introduction that their results explain why policies trained with domain randomization should have memory. I think it might be worth mentioning in the body of the paper that the way in which Algorithms 1/3/4 use memory is somewhat different from parameterizing a network by an RNN. In these algorithms, memory is simply used to isolate the test time MDP, after which point the optimal policy (which is memoryless) for that MDP is used. In contrast, people in general would train a single RNN over the distribution of MDPs and then test the RNN on the test-time MDP. It's possible that the RNN internally learns to switch from using memory to becoming memoryless once enough observations have passed to internally determine the MDP, but it might be worth including some discussion of how Algorithms 1/3/4 compare to standard training schemes for applying domain randomization.\n\nCurrently, I feel that the bounds derived in the paper largely justify standard practices (e.g. parameterizing policies with memory when training with domain randomization, or why domain randomization should be used in the first place if one wants to transfer a policy trained in simulation to the real world), but don't suggest new practices. In the conclusion, the authors say they hope their analysis will lead to the design of more efficient algorithms for sim-to-real transfer, but I think the paper could benefit from more discussion of which of their results the believe could be the most applicable to improving how domain randomization is used in practical settings.",
            "summary_of_the_review": "This paper formalizes the common practice of \"domain randomization\" and develops algorithms and bounds for the performance of domain randomization depending on if the set of randomized MDPs is finite (with or without separability) or infinite. The central result is that the performance gap is $\\tilde{O}(\\sqrt{H})$ in all cases. The formalization of domain randomization is accurate and the analysis of the problem setting is thorough.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a theoretical framework for analyzing the sim-to-real gap in the context of domain randomization. The paper defines formally the sim-to-real gap and the domain randomization method. Next, it analyzes the upper bound of sim-to-real gap in three different scenarios: finite simulation classes with and without separation conditions and infinite simulation classes. The paper provides constructive arguments to prove the upper bounds.\n\nIn my opinion, this paper has made multiple contributions. I am most impressed by its theoretical framework that formally defines the sim-to-real problem and its proof of the upper bound of the sim-to-real gap based on the constructive arguments described in the paper.\n",
            "main_review": "Let me start by admitting I am a sim-to-real practitioner, not a theorist. Therefore, it is likely that I am not fully aware of other related works that develop a similar theoretical framework. I have found the paper itself very inspiring, and both the theoretical formulation of the sim-to-real gap and the high-level idea behind the proof overview make very good sense to me. Still, I have a few comments, and it would be good if the authors could clarify them:\n\n1. There is another possible definition of sim-to-real gaps: the performance difference between a policy $\\pi$ (in particular, $\\pi^*_{DR}$) in the simulation environment and in the real world:\n\nGap($\\pi,\\mathcal{U}$) = $V_{\\mathcal{M}^*,1}^{\\pi}(s_1) -  \\mathbb{E} \\_{\\mathcal{M}\\sim\\nu} V_{\\mathcal{M},1}^{\\pi}(s_1)$.\n\nWhile I agree Eqn. (1) is perhaps a more interesting definition of sim-to-real gaps, Eqn. (1) is fairly difficult to measure in practice. Could the authors comment on these two definitions?\n\n2. It looks like the stepwise reward is assumed to be within 0 and 1. I guess the whole framework is also applicable to unbounded stepwise rewards, e.g., by remapping them to [0, 1] via a sigmoid function. Does this affect your main results?\n\n3. Comparing Sec. 5.1 and Sec. 5.2 seems to imply that narrowing the sim-to-real gap is more difficult for problems without separation condition, but I have a feeling that this is not always the case. Consider an extreme scenario where all $\\mathcal{M}$ in $\\mathcal{U}$ are very similar to each other (and therefore they are all similar to $\\mathcal{M}^*$. Such a $\\mathcal{U}$ is not a separated MDP set, but I would expect the sim-to-real gap to be much smaller than the gap given for the separated MDP set (Theorem 1). In particular, if $\\mathcal{U}$ simply copies $\\mathcal{M}^*$ multiple times, the sim-to-real gap would be 0. It looks like assuming only a few pairs of non-separated MDPs in $\\mathcal{U}$ may make the problem harder, but if all MDPs are non-separated, the problem seems to be much easier.\n\n4. The constructive argument for proving Theorem 1 seems interesting and smart, but I have a few questions about its technical details:\n\n* Alg. 1, line 7 (the condition in the if-statement): why not simply say “if $\\Pi P\\_{\\mathcal{M}_1}(s’|s_0,a_0)\\geq \\Pi P\\_{\\mathcal{M}_2}(s’|s_0,a_0)$”? This would also avoid the need for treating $P\\_{\\mathcal{M}_2}=0$ in Sec. D.1. Also, this if-condition does not seem to be symmetric: if there exists $P\\_{\\mathcal{M}_1}=0$ and $P\\_{\\mathcal{M}_2}=0$, it always chooses to eliminate $\\mathcal{M}_2$. Does it affect your proof? (Looks like it does not.)\n\n* Eqn. (41): are $s,a$ referring to $s_0,a_0$ specifically, or are they arbitrary states and actions? It looks like they should be $s_0,a_0$, according to the sentence immediately after Eqn. (41).\n\n* What is $\\tilde{\\mathcal{M}}$ in Eqn. (42) and Eqn. (49)? I was expecting $\\mathcal{M}_1$ there.\n\n* Eqn. (49): I do not see why $H$ appears on the right-hand side. It would make more sense to me if the right-hand side is $-2\\alpha S n_0$.\n\n* I cannot see how Eqn. (51) is derived from Eqn. (50) after trying to plug into Eqn. (50) the provided definitions of $\\alpha$, $\\delta_0$, and $n_0$. In particular, are $c$ in Eqn. (50) and $c_0$ in the definition of $n_0$ two different numbers? Also, it seems that $\\delta$ in the numerator of $n_0$ between Eqn. (50) and Eqn. (51) should be replaced with $\\delta_0$ so that it matches the definition of $n_0$ in Lemma 5.\n\nA few minor comments:\n\n1. Eqn. (1): why does Gap need to take $\\mathcal{U}$ as input if all it needs is $\\mathcal{M}^*$?\n\n2. What is $d$ in Theorem 3?\n\n3. The notation $PV$ is defined in Sec. 3.1 but does not seem to be used elsewhere in the main paper.\n\n4. How do we check if Assumption 3 is satisfied in practice?\n",
            "summary_of_the_review": "I appreciate the whole theoretical framework this paper establishes for studying domain randomization techniques for sim-to-real problems. I think defining the problem formally and properly is a good contribution.\n\nThe high-level proof overview makes good sense to me. I also manually checked the proof for finite simulator class (but not very carefully) in the appendix. The whole proof seems legit despite some minor issues, which I tend to think is relatively easy to fix.\n\nThe construction of base policies (Algorithms in the appendix) may also be interesting both for sim-to-real theorists and practitioners.\n\nOverall, I recommend strong acceptance assuming that all the equations in the main paper and in the appendix are correct or are relatively easy to fix.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}