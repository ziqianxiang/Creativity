{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "The paper describes a framework that unifies several previous lines under hindsight information matching.  Within that framework, the paper also describes variants of the decision transformer (DT) called categorical DT and unsupervised DT.  The rebuttal was quite effective and the reviewers confirmed that their concerns are addressed.  The revised version of the paper is significantly improved and consists of an important contribution that should interested many researchers.  Well done!"
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper presents variants of the Decision Transformer that can condition on desired state or feature distributions instead of scalars.\nAlong the way, the authors also describe an organization of hindsight relabeling methods proposed in the last few years.",
            "main_review": "This paper offers three main contributions:\n1. It consolidates a number of existing techniques, including TDMs, Learning from Play, GCSL, and Decision Transformer (DT), into the same umbrella of \"hindsight information matching\" algorithms in the spirit of hindsight relabeling.\n2. It describes a variant of DT, called Categorical DT, that conditions on a desired future state or feature distribution (as opposed to scalar cumulant).\n3. It offers a version of the Categorical DT that includes an unsupervised learning component, with the goal being to learn representations $\\phi$ such that matching the first moment of $\\phi$ is equivalent to matching the distribution of raw features.\n\n**Hindsight information matching**\n\nI did appreciate the exercise of mapping out the space of hindsight algorithms that resulted in Table 1. I am not sure this includes a new insight, since hindsight is commonly acknowledged as a primary motivation for algorithms in this space. For example, from the Decision Transformer paper:\n> Thus, by combining the tools of sequence modeling with **hindsight return information**\n\nand from the Trajectory Transformer paper:\n> ...our method also resembles recently proposed work on goal relabeling [**HER**, GCSL]\nand reward-conditioning [UDRL, RCP] to reinterpret all past experience as useful demonstrations with\nproper contextualization.\n\nand from the GCSL paper:\n> By generating demonstrations using **hindsight relabelling**, we are able to apply goal-conditioned imitation learning primitives...\n\nThat being said, I think it is valuable to aggregate trends like these in a way that makes it easier for researchers to understand how a number of related algorithms fit together, so even if there is nothing technically new from this exposition I do think it is a useful contribution.\n\n**Categorical DT**\n\nI was somewhat less convinced of the other two contributions.\nWhile in principle the ability to match a target _distribution_ should enable substantially more flexible behavior than scalar return-conditioning, this did not seem to be adequately explored or evaluated.\nOne low-level issue is the datasets used for evaluation: all experiments use the `medium-expert` D4RL datasets from what I can tell, which are mixtures of only two policies.\nThis severely limits the types of behaviors that can be extracted from the datasets, and calls into question even some of the more impressive-looking generalization results.\n\nFor example, in Figure 4 we see that the model is able to produce a bimodal x-velocity distribution given a bimodal input (halfcheetah synthesized 6, top right).\nSomewhat suspiciously, however, the modes are in the wrong places.\nThe reason the modes land where they do is because these are the modes of the two policies in the dataset: the medium policy gets an average of 5~6 reward per step, and the expert policy gets an average of ~12 reward per step.\n\nWhile this is possibly the best that one could hope for given the dataset, it sets the bar fairly low.\nSince there are only two behaviors in the dataset, it is only possible to recover different mixtures of two qualitatively different things.\nYou could reduce the entire distributional conditioning to a single parameter that acts as a mixing coefficient between the two policies.\nIn general, this sort of idea would be better evaluated in datasets with higher entropy, to better show the types of behaviors that are recoverable from distribution conditioning that would not be recoverable via standard return conditioning.\n\n**Unsupervised DT**\n\nUnfortunately I could not find quite enough detail about either the method or evaluation here to understand what was going on.\nI think I understand how a representation learning objective could also be used for distribution matching purposes, but aside from that high-level link this addition did come off as a bit tacked-on to the end of another paper.\nMost of the design choices are relegated to the appendix, which is sometimes reasonable given space constraints, but then even the appendix does not provide a self-contained description but mostly references out to other papers, such as:\n> In addition, for contrastive loss, we adopt CURL objective (Srinivas et al., 2020) for state input while removing data augmentation.\n\nMore detail could be useful here -- the data augmentation is important to contrastive methods like CURL, so I am not entirely sure what objective is being used.\n\nThe results are somewhat terse as well:\n> We found that generally SL loss with respect to $\\phi$ could ruin representation learning, as results for SL + UL and SL demonstrate unsuccessful learning , and using contrastive loss is worse than using AE loss (see Figure 15 in Appendix).\n\nIf three of the four variants of Unsupervised DT are meant to serve as either ablations or negative results, could you write more about what we should learn from them?\nI think it could be interesting that the unsupervised-only version works so much better than versions with the supervised loss, but without a more thorough investigation into why this happens it is hard to contextualize.\n\n**Minor**\n1. \"policy that whose future state rollouts\" --> \"policy whose future...\"\n1. Footnote 7: \"prameterized\" --> \"parameterized\"\n1. \" To compute $I_z^\\phi(\\tau_t:T)$ efficiently for all timesteps $t$ given a trajectory $\\tau_1:T$ , we use similar recursive Bellman-like computation inspired by Bellemare et al. (2017).\" Could you write the recursion to make the description more self-contained?\n1. \"CDT don’t match the targets\" --> \"CDT doesn't...\"\n1. \"unsuccessful learning ,\": remove space before comma\n\n**Post-rebuttal update**\n\nThanks to the authors for their responses. I have updated my score based on the conversation below.",
            "summary_of_the_review": "This paper presents a great and potentially very useful idea, but does not evaluate it in situations where the idea could reasonably be expected to do something more interesting than the original DT.\nBetter motivating the types of behaviors that could be expressed and produced via more flexible conditioning, and seeing that through by testing the method on higher-entropy datasets, would make this a much stronger contribution.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work discusses many prior methods under a Hindsight Information Matching (HIM) framework, where the methods can be interpreted as  trying to minimizing the KL divergence between the achieved statistic and some target statistic for a particular choice of statistic. The authors propose to replace the return-to-go conditioning in Decision Transformer with a distribution over some specified state features, which they name Distributional Decision Transformer (DDT). This work also proposes an unsupervised variant, Unsupervised DT (UDT), which does not need certain state features to be specified and can be interpreted as performing offline state-marginal-matching. The authors show in various MuJoCo settings (HalfCheetah, Hopper, Walker2d) that the proposed methods can learn to imitate target information statistics in offline RL and imitation learning settings.",
            "main_review": "Strengths:\n1. Novel work tackling offline state-marginal-matching (SMM) problem. Like other works learning RL skills from offline datasets, this can learn from more signal from fixed data than reward alone. From a learning standpoint, this can aid in learning much more interesting models, especially in the consideration of zero-shot or few-shot learning like GPT or BERT.\n2. By conditioning on the state distribution of a new trajectory, DDT can serve as an imitator at test-time, similar to how language models/GPT can imitate prompts.\n3. Similar to (2) but slightly distinct, this work extends DT to consider a distribution of rewards/returns, which is also novel itself.\n4. Results show fairly strong conditioning performance across the settings considered; in particular I think Section 6.2.2 with an unrealistic target distribution and Section 6.3 with Unsupervised DT are interesting.\n5. Proposed Hindsight Information Matching framework is an interesting way to interpret past work. While I'm not sure about the novelty exactly (i.e. log-probability, squared-error loss are well-known to connect to KL divergence), the work discusses many prior algorithms.\n6. Investigates various representation learning approaches in the context of DTs.\n\nWeaknesses:\n1. While the promise of state-marginal-matching and language modeling is to extract a lot of signal from a dataset, namely trajectory statistics here, much of the experimental analysis focuses on (a) learning from a single feature (as far as I can tell) and (b) x-velocity in particular. On many of these environments, the reward is actually just the x-velocity shifted by some small amount, so it is almost identical to conditioning on the reward. Unsupervised DT lifts the restriction of (a) but is still evaluated on (b). A simple addition would be to evaluate on height; a more substantial addition would be different environments, see (4).\n2. The proposed method seems very similar to one-shot imitation learning (OSIL) [1], where a model is trained to imitate a new trajectory at test time end-to-end, bypassing the statistics step. [2] tackles OSIL with transformers. While DDT/UDT may be more interpretable and/or powerful than OSIL, this is not compared to or discussed in the text.\n3. In the contributions, it is written that the work \"proposed the first benchmark for offline state-marginal matching\". Table 2 is the only quantitative result shown in the paper, however the results seem hard to interpret, and I'm not sure the Wasserstein distance is the right choice of metric here. A proper quantitative metric is important for a benchmark. Adding qualitative comparisons to the main text for better clarity could be helpful. I am also not sure exactly how Deterministic DT works (what is it conditioned on?), but it is probably also a weak baseline. If conditioning on the reward is equivalent to the x-velocity, then Deterministic DT would be a strong baseline. Otherwise, other approaches like OSIL or conditioning on the mean of the target feature would be better.\n4. I think the various representation learning methods investigated for Unsupervised DT are interesting, but they are not really shown or discussed in the main text, so it is hard to think of them as a main investigation of the paper. If the insights can be distilled into a paragraph/table/figure in the main text, I think it would be helpful.\n5. In the introduction, it is written that UDT can \"imitate any test trajectory\". I would strongly recommend rewording this claim, as it is practically impossible to evaluate this claim in continuous settings.\n6. It is not clear to me that the test-time target distributions proposed in Section 6.2 are interesting problems to study and especially to benchmark on. Manipulation tasks, or Atari and other rich environments, could generate more interesting heldout trajectories which are plausible and interesting, but differing in the statistics compared to the training set.\n\nOverall, I am optimistic about this work, and interested in how we can maximally extract training signal from fixed datasets for reinforcement learning. In particular, I think a model which can effectively perform SMM may yield strong finetuning behaviors, although this could be left for future work. Learning to perform SMM offline may also aid in downstream online exploration, which is an important research direction. While I think the experiments and discussion are somewhat weak, I think some relatively simple adjustments (1-5 in weaknesses above) would improve the paper; if they are added, I will change my score to a weak accept. If the authors are interested in continuing this work, I think (6) would make the paper significantly stronger.\n\n=======\n\nMiscellaneous\n\n1. I wasn't fully sure what \"hindsight BC algorithms always work with samples $(\\tau_i, z^{\\tau_i}$\" was supposed to mean; I think hindsight RL methods do this as well? I might be misunderstanding something.\n2. The tables in general have a small font which makes them hard to read, especially Tables 2, 8, and 9 are overloaded. It would be much easier to read with less decimal places or multiplied by 100 or some other choice; an aggregated mean in a single column would also make it easier to interpret. The figure font is also a little small at times.\n3. Under Equation 1 in the definition of $p_t^\\pi(s)$, the $t$ in the product should be redefined to some other $t'$ to not clash with the $t$ representing the time of marginal distribution.\n4. In Equation 5, I think the expectation is missing $s \\sim p_z^\\pi$. (It is included in Equations 4 and 6, for instance)\n5. For the pseudocode in the appendix, I would strongly recommend avoiding the use of red/green due to colorblindness issues.\n\n=======\n\n[1] Yan Duan et al. 2017. \"One-Shot Imitation Learning\"\n\n[2] Sudeep Dasari and Abhinav Gupta 2020. \"Transformers for One-Shot Visual Imitation\"",
            "summary_of_the_review": "The paper is interesting and presents novel methods which may in the future be important ideas leading to new work. Since the results are somewhat weak right now and there are minor clarity concerns, I recommend weak reject.\n\n=====\n\nAfter rebuttal (11/18):\n\nI think the updates made by the authors during the rebuttal period are significant, adding writing clarity, experiments, and the Bidirectional DT. As I wrote below in my response: \"While the scope of experiments is still limited (as also pointed out by Reviewer Ffah), I think the experiments are thorough and the paper contains interesting insights that will be valuable for future investigation, especially given the early state of literature on transformers for RL; I will thus increase my score from 5 to 8.\"",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes an extension of Decision Transformer (DT) that can work with distributions of features. A number of prior hindsight-based or context-dependent methods are shown to be special cases of a generic scheme called Hindsight Information Matching (HIM), where arbitrary statistics of future trajectories can be used for conditioning. Therefore, the proposed Distributional Decision Transformer (DDT) can be seen as a practical implementation of the HIM algorithm. Experiments on D4RL medium-expert data for HalfCheetah, Hopper, and Walker2D investigate generalization of DDT. Additionally, another extension of DT called Unsupervised Decision Transformer (UDT) is proposed and evaluated that learns the features and rewards in unsupervised manner.",
            "main_review": "Strength:\na) unifying framework is developed and prior methods are nicely shown to be special cases\nb) distributional and unsupervised versions of decision transformer are proposed\nc) generalization for 1D distribution shifts is evaluated (i.e., distribution over x-component of velocity is shifted)\n\nWeaknesses:\n1) The paper is somewhat chaotic. It proposes 2 different extensions of decision transformer (DDT and UDT), furthermore introducing two variants of DDT (Categorical (CDT) and Gaussian (GDT)) and two variants of UDT (auto-encoder (AE) and contrastive loss) together with 4 settings (SL+UL, SL, UL, UL-Fix), at the same time it describes offline state-marginal matching and inverse-RL imitation learning and develops a hindsight information matching (HIM) framework. It feels there are enough ideas for several papers. Perhaps because of the scope, the evaluation is rather sketchy. Essentiall, the experiments only show that the basic implementation works, and that generalization w.r.t. to 1D velocity change seems to perform as expected. Focusing on just one variant, e.g., CDT, but providing a more thorough evaluation would be more beneficial and provide a clearer focus for the paper.\n2) Gaussian Decision Transformer is announced but not described. Introduction refers to Sec. 5, which in turn refers to Appendix D, but the latter only covers Categorical Decision Transformer.\n3) The way in which bold numbers are highlighted in tables may be misleading. In the majority of cases, results for DT and DDT lie within 1 standard deviation from each other. It would be more appropriate to either highlight both in bold or not highlight anything.\n4) Only comparison to DT is provided. Comparison to baseline distributional algorithms should be added.",
            "summary_of_the_review": "The paper is not ready for publication. The unifying perspective presented in the paper is very interesting, as well as the proposed distributional and unsupervised extensions of decision transformer. However, the evaluation is not sufficient. There are too many variations of the algorithm considered and the experiments are not expressive enough to illuminate them. Generalization only studied in a very limited scenario. No comparison to distributional RL is provided. As a result, the paper is trying to cover too many things at once and it is recommended to restrict the scope and provide more thorough analysis.\n\n====\n\nAfter rebuttal\n\nThe authors addressed my concerns with evaluations and baselines. Furthermore, the connection to distributional RL was clarified. The updated paper is 80% new. With the improvements made by the authors, I have no further issues and raise my score to \"8 accept, good paper\".",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}