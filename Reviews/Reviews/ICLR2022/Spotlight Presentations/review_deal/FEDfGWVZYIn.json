{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "The paper proposes an approach and specific training algorithm to defend against membership inference attacks (MIA) in machine learning models. Existing MIA attacks are relatively simple and rely on the test loss distribution at the query point and therefore the proposed algorithm sets a positive target mean training loss value and applies gradient ascent if the average loss of current training batch is smaller than it (in addition to the standard gradient descent step). The submission gives extensive experimental results demonstrating advantage over existing defense methods on several benchmarks. The primary limitation of the work is that it defends only against rather naive existing attacks which do not examine the model (but rely only on the loss functions)."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "To defend the common membership inference attacks, this paper proposes a relaxed loss with a more achievable learning target to reduce the attack accuracy and also help with the generalization gap of learning models.  Extensive evaluations on five datasets with diverse modalities show that the proposed method can achieve higher membership inference protection.",
            "main_review": "Strengths:\nThis paper proposed a relax loss to defend privacy leakage, since the authors find that membership privacy risks can be reduced by narrowing the gap between the loss distributions. Also the gradient descent and gradient ascent are used alternatively to balance the member and non-member loss distributions. \n\nWeakness:\n1. The authors give an analysis for \"RelaxLoss increases the variance of the training loss distribution\" in appendix A. In Theorem A.1, the authors show that the variance of loss distribution increases after a gradient ascent step, with the condition $\\text{Cov}(l, |\\Delta l|)>0$. My questions here are why we need the absolute operator. The variance of loss distribution after a gradient ascent step should be $\\text{Var} (l+\\Delta l)$.  In this case, $\\Delta l$ can be negative, and how to make sure the condition $\\text{Cov}(l, |\\Delta l|)>0$ still holds. Also, the assumption of \"each sample within a batch exhibits the same gradient\" seems too strong and impractical. The common assumptions may be: each sample within a batch exhibits the same norm of gradient. \n\n2. In Figure 5, why does 'w/o gradient ascent' only have a single point? It should be a curve just like 'ours' and 'w/o posterior flattening'.\n\n#############after authors' response#############\nThanks for the responses from the authors. I will raise my score to 8.\n",
            "summary_of_the_review": "This paper proposes a relaxed loss to narrow the loss gap and reduce the distinguishability between the training and testing loss distributions, and further prevent the privacy leakage. However, I have some questions about the theoretical analysis, i.e., why the \"RelaxLoss increases the variance of the training loss distribution\". ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "The study tackles the problem of defense against membership inference attack, with a focus on (1) decreasing the performance of the attack, (2) maintaining the classifier’s performance, (3) assuming the blindness towards the attack model. They achieve (1) by closing the distance between the train and test distributions and maintain the utility of the model by flattening the posterior scores of the non-target classes. Their method is shown to be computationally efficient (i.e., the additional computational cost is negligible), close to optimal in maintaining the trade-off between the utility and attack performance, and effective in the face of attack’s countermeasures.",
            "main_review": "Positive aspects:\n-\tThe proposed method outperforms similar defense strategies while preserving the accuracy of the classification model on test data.\n-\tEven with the full knowledge of the attacker on the defense mechanism, the defense algorithm is capable of reducing the attacker’s accuracy considerably.\n-\tTheir defense strategy is computationally efficient (in comparison with other similar methods) and blind to the attack algorithm.\n\nNegative aspects:\n-\tThe paper lacks intuition behind some assumptions and choices made in designing the algorithm (more below).\n-\tThe authors did not discuss limitations of the method (more below).\n-\tSome minor errors that can be easily fixed (see Minor Comments).\n\n---------------------------------------------------\nMajor Comments:\n-\tIn the set of defender assumptions in Section 3, the necessity for considering the first assumption is not clear. In practice, additional unlabeled data is normally available and if they can help with building a stronger defense mechanism, why should we ignore them? This assumption lacks justification.\n-\tI find the theoretical and intuitive justification behind the “posterior flattening” step to be lacking. By discarding the learned information of the network about the non-ground-truth classes and manually replacing the values, my intuition is that there is a good chance we are overfitting (i.e., we can't expect good performance on the test set). The poor testing results in Table 1 for purchase100 data, which has the largest number of training samples, might be hinting at this overfitting. So, it seems the method has its own limitations with regards to the training sample size. I believe authors should discuss this limitation (and any other possible ones) and address them.\n-\tFigure 4 would be more informative if the comparison was made against the best performing baseline instead.\n-----------------------------------------------------\nMinor Comments:\n-\tIn “Notations” (section 3), the sentences are confusing. There are no x, y , p in f(.;theta), and the notation for “1” is not consistent.\n-\tSimilarly, in \"Attacker’s Assumptions”, the definition of m and z should precede that of S.\n-\tIf the attack has full access to the model (as mentioned in Attacker’s Assumptions), it means the attacker has full knowledge on the classifier’s architecture and parameters. So, shouldn’t it be A(z,f(.;theta)) instead of A(z,theta)?\n-\tFigure 1-c, the (E,Val) pairs should be linked to the corresponding distribution.\n",
            "summary_of_the_review": "The approach is clear, and the extensive experiments performed on variety of datasets against reliable baselines proves the merit of the proposed defense algorithm. However, the intuition behind some assumptions and steps in the algorithm is not clear and authors should certainly discuss the limitations of their method.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a new training algorithm to defend against membership inference attacks (MIA) in machine learning models. Motivated by the connection between MIA success and difference between training and test loss distributions, the proposed algorithm sets a positive target mean training loss value and applies gradient ascent if the average loss of current training batch is smaller than it. Furthermore, to avoid hurting model accuracy, the proposed algorithm also flattens the probabilities among incorrect labels during training steps. Extensive results on multiple datasets along with several defense baselines validate the effectiveness of the proposed defense idea.",
            "main_review": "[strengths]\n1. The two techniques – relaxed target loss with gradient ascent and confidence flattening are well-motivated in Section 4 and 5.\n2. The paper thoroughly evaluates the defense method on 5 datasets, and compares it with 8 defense baselines, making the experimental results very convincing.\n3. The consideration of adaptive attacks in Section 6.4 is necessary and important.\n\n[weaknesses]\n1. Section 6.4 only considers adaptive attacks for NN attack, can you run adaptive attacks for other attack methods as well and report the best attack success among all methods in Table 2 (or put results in another new table)?\n2. A small presentation issue: subfigures in Figure 3 are too small. Maybe put some of them in Appendix and enlarge the main subfigures?\n",
            "summary_of_the_review": "Overall, I like this paper. The used techniques are well-motivated. Also, the experimental evaluation is very thorough to include several datasets, multiple existing defense baselines, and include adaptive attacks. So, I recommend accepting this paper.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}