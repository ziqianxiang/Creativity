{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "All reviewers suggest acceptance of this paper, which reports the relationship between perceptual distances, data distributions, and contemporary unsupervised machine learning methods.  I believe this paper will be of broad interest to different communities at ICLR."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "Interesting (but debatable) observations between image prior, image quality, and perceptual distances.",
            "main_review": "Strengths:\n\nThe authors investigate a very interesting and important problem that lies in the intersection between perception and statistics.\n\nWeaknesses:\n\nFor Observation 1:  $\\frac{D_p(\\mathbf{x}_1, \\mathbf{x}_2)}{\\Vert \\mathbf{x}_2 - \\mathbf{x}_1\\Vert_2}$ is symmetric. Therefore, it should be correlated with both $p(\\mathbf{x}_1)$ and $p(\\mathbf{x}_2)$. One subtlety is one image (e.g., $\\mathbf{x}_2$) is a \"distorted\" version of the other (e.g., $\\mathbf{x}_1$). In other words, $p(\\mathbf{x}_1)$ is the distribution over undistorted data and $p(\\mathbf{x}_2)$ is the distribution over distorted data. Does it make sense that $\\frac{D_p(\\mathbf{x}_1, \\mathbf{x}_2)}{\\Vert \\mathbf{x}_2 - \\mathbf{x}_1\\Vert_2}$ is correlated with two different distributions. Note that although Observation 1 assumes $\\Vert \\mathbf{x}_2 - \\mathbf{x}_1\\Vert_2$ is small, $p(\\mathbf{x}_2)$ can be a distribution over very perceptually noisy data.\n\n\nFor Observation 2: This observation relies on the idea of score matching, which is derived under a particular type of distortion -  additive white Gaussian noise. Does it generalize to other types of non-additive, non-linear distortions, e.g., JPEG compression, Gaussian blurring, etc.\n\nFor Observation 3: The reviewer believes this observation will largely depend on the capacity (i.e., information bottleneck) of $f$. One extreme counterexample is that $f$ has no capacity and is a constant function.\n\nFor Observation 4: $p(\\mathbf{x}) = p(e(\\mathbf{x}))\\left\\vert\\frac{\\partial e}{\\partial \\mathbf{x}}(\\mathbf{x})\\right\\vert$ holds only for bijective mappings with identical input and output dimensions. This is not the case for compression. In addition, comments on Objective 3 can also be applied here.\n\nFor Observation 5: The reviewer respectively disagrees with this observation. For $D_\\mathrm{in}(\\mathbf{x}_1,\\mathbf{x}_2) = \\Vert e(\\mathbf{x}_1) - e(\\mathbf{x}_2)\\Vert_2$, it is not hard to find, for example using gradient-based optimization, a distorted image $\\mathbf{x}_2$ with the same compressed representation as the original image, $\\mathbf{x}_1$ of perfect quality, that is, $e(\\mathbf{x}_1) = e(\\mathbf{x}_2)$. This image may not appear in TID 2013, on which the authors justify Observtation 5.\n\nMoreover, it remains to be seen whether PixelCNN++ is a good generative model to approximate $p(\\mathbf{x})$, and how the approximation error affects the five observations.\n\nA final concern is that there is a lack of experiments/discussions on how useful these observations are for designing better generative models and/or better perceptual distances.\n\n\n",
            "summary_of_the_review": "See above for detailed comments.",
            "correctness": "2: Several of the paper’s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "\nThe work presented in this paper aims to analyze the relationships between the probability distribution of the data, perceptual\ndistances, and unsupervised machine learning. Perceptual sensitivity is correlated with the probability of an image \nin its close neighborhood. The paper also explores the relation between distances induced by autoencoders and the\nprobability distribution of the training data, as well as how these induced distances\nare correlated with human perception. At the end, the paper specifies that perceptual distances do not\nalways lead to noticeable gains in performance over Euclidean distance in common\nimage processing tasks.",
            "main_review": "Strengths:\nThe paper was nice to read. Overall there are six (6) observations and 3 Eqns in main body of the paper, which formed the core part of the work, while 13 more in Appendices were supporting as proofs and extra information.\nSpecific contributions are: \n(i) Pixel-CNN++ based image likelihoods are correlated with human psycho-physical distances;\n(ii) Distances induced by AEs are correlated with Prob(Trng data), as well as with human perception.\n(ii) double counting effect in perceptual distance based loss function\nOverall, as nice 3-way relationship being empirically explored.\n\n\nCons:\nLet me start with some generic queries first:\nI would like a comment from authors- these 6 observations are like soft theories (ala Perception theories), \nor they can be raised to the level of Results/Lemmas (in applied maths )?\nWhat can be the overall conclusion for theorists and experimental application domain experts, from these 6 (+ 3) contributions ? \nHow can one exploit them for better design of DL architectures/algos. to solve certain problems ?\nI am trying to get an overall picture, if possible, of what is the gain in technical know-how or analytics this paper has produced.\n\nThe words Perceptual and psycho-physical distances - are they meaning the same ? A reference would be ideal.\n\nYou have used AE as your target application, what about other deep-CNN models - transformers, LSTM, GAN based models for generation or discrimination?\n\nWhat does one need to do to counter this double-counting effect ? Modify loss function, dataset samples or  architecture ?\nIt appears that you have identified a tricky fallacy, but the remedy has not been explored ? Or, have I missed something.\nYou mention using training without image data - using uniform random noise as input- is this not same as various GAN-based models ?\n\nEqn (2) appears to be a special/modified form of Vapnik's theory of empirical risk minimization (ERM), applied to VAE ? What is the value addition then?\n\nEqn (3) - Ex is not defined ? Or I missed - although appears to be the trivial  - Empirical mean ? (I assume).\n\nAny particular reason of Using  softplus (yes, recently popular) rather than softmax ? Pixel-CNN+ also uses softmax. \n\nsec. 5.2, 1st para, pp 7 - this part of the (long) sentence may need rewording:\n......minimizing a loss weighted by the \"likelihood the samples\" belong to the Student-t,....\n\nSame within Sec. 6, 1st para, pp 8:\n......by construction, for perceptual distances we are , considering that the human visual system has....\nNow, this evolution process has been over a very long time, say more than few centuries  - and with no empirical results feasible/available involving ancestors of humans - my opinion is that:\n - this hypothesis used by visual perceptions scientists may be kept aside, in the context of this paper.\n\nThe last sentence in Conclusion section, pp 9, says:\n....both machine learning and biological perception is informed by the distribution of natural images.\n\nWell as scientists and engineers, should we not be more interested in the gap between the two, rather than the commonality ?\nThat would help us to bridge the gap and build more efficient, robust, autonomous, intelligent perception-machines, I believe, \nas our main target to design.\n\n\nWell the term \"Appendices\" is missing as a sub-heading before Appendix A - left me initially wondering if these were supplementary materials or Appendices of the paper. or extension.\nI am not happy with the purple background used in figures 11-13. Quite difficult to isolate out the central part (hoping that is significant?)  visually - eye-stressing I must admit.\nFigs. 18-19: traditional Image quality experts provide a zoomed-in part of pics to highlight the distortions. The overall pictures here look bland, and quite difficult to identify regions/parts of images with good vs bad quality (as evaluated often using SSIM or PSNR) - mostly, the pair in last row, \nThis is not the case for samples in Fig. 16.\n\nSec. E- Entropy limited AE: is this your idea, or a variant of any used elsewhere  ?\nDid not see a reference - hence checking.\n\nI enjoyed reading the paper, which had a nice flow, although a bit long (as per expectations in a Conf.) \ndue to supplementary/Appendices.\n\n",
            "summary_of_the_review": "\nThe paper has provided sufficient and substantial evidence of the observations made of the 3-way connections between - \nlearned or hand-crafted image representations (autoencoders and perceptual models), the distribution of\nnatural images, and human perception.\n\nThis paper may initiate good discussions/work along the domains overlapping visual perception and ML/DL scientists .\nIn terms of technical know-how, although the paper takes a small step, but does not really contribute to a big\ngain in analytics and new formulations/algorithms.\n\nAlthough there are some unanswered questions and issues, the paper appears quite interesting.\nMinor improvements would be necessary for it to get accepted.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The authors present few relation between statistical learning and perceptual distances. Specifically, they explain that\n\n1. perceptual distances correlate with image likelihoods;\n2. auto-encoder latent space induced distances of natural images are correlated with training data probability and human perception; \n3. perceptual distances are redundant with Euclidean distances in latent space.\n",
            "main_review": "#### Strengths :\n\n* Interesting topic at the cross-road of perception and machine/deep learning (ML/DL)\n* Good review of the literature\n* Well written and understandable\n\n#### Weaknesses :\n\n* No major contributions\n* List of observations that are already known in the ML/DL community\n* More like a review paper \n\n### Detailed comments\n\nOverall reading this paper is interesting as it gathers two/three fields: ML/DL, neurosciences and psychophysics. However, I don't really see any contribution to any of these fields. \n\nFor ML/DL: most of the observations are already known. No one in the ML/DL community would be surprised to hear that distances between images are correlated with their distribution. Gans, normalizing flows or NeuralODEs are approaches that takle the same goal: mimicking the dataset distribution using a latent space representation. \n\nNeurosciences: No data, nor models are provided.\n\nPsychophysics: No new data is presented. Nothing new is presented about the relation between perceptual distance and human perception. \n\nI fail to understand why the authors introduces so many distances D_s, D_r, D_e and D_in and why it is interesting. In addition, why is it insteresting to state all those observations (that are variant from each others and somehow all result from the efficient coding hypothesis) ? \n\nTo finish, on a better note. I think, the authors are working on an interesting topic and I encourage them to pursue. However, I also think this paper is too preliminary. To me the most interesting contribution (which is only a small part of the paper) is the training of auto-encoders with noise and a perceptual similarity distance. Really, is it useful for training with less data ?\n",
            "summary_of_the_review": "I think it is more a review paper and there are not important enough contributions. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        },
        {
            "summary_of_the_paper": "This paper presents a mainly theoretical explication of the relationship between natural image statistics and perceptual distances for small image distortions. The paper presents a number of observations linking distances in natural images, autoencoders, and perceptual similarity for humans. The paper finally explores some implications of these observations, including impressive-seeming results on training with no data using perceptual distances as regularizers.\n",
            "main_review": "While the paper is dense, I found it nevertheless to be well structured and the figures to be generally helpful. While at first I thought that the result might be trivial (the human visual system compresses natural input, so it makes sense that autoencoders and perceptual distances are related), I believe that the paper draws out the value of making these connections explicit --- though I am not an expert in the theoretical side of this field. I have minor comments.\n\n- The observations presented are limited to small image differences $\\delta$. I would be interested to know (1) if the theory can be extended to estimate bounds on $\\delta$ in some meaningful dimension and (2) the authors' speculations on whether the assumption of perceptual distances as metric spaces breaks down when $\\delta$ is large (see e.g. Tversky, 1977, *Features of Similarity*), as in the suprathreshold regime comparing different types of image manipulations.\n\n- Could the \"double-counting\" effect be mitigated by reweighting the relative contributions of perceptual loss and training?\n\n- Figure 3: why is the RMSE correlation with MOS (dashed line) flat as a function of bpp? \n\n- Figure 4: there seems to be quite some kernel smoothing happening here, making it unclear how much data this actually represents.\n\n- P. 8 typo \"for perceptual distances we are ,\"\n\n- A relevant reference for the introduction linking efficient coding and compressed unsupervised representations is: Storrs, K. R., Anderson, B. L., & Fleming, R. W. (2021). Unsupervised learning predicts human perception and misperception of gloss. *Nature Human Behaviour.*",
            "summary_of_the_review": "A theoretical explanation of the link between natural image statistics, autoencoders and perceptual distances, with some seemingly important implications. This is not exactly my field so I may be missing relevant background.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        }
    ]
}