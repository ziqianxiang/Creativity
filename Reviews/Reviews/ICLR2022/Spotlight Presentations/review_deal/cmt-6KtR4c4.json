{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper is about unsupervised translation between programming languages. The main positive is that it introduces the idea of using a form of unit test generation and execution behavior within a programming language back-translation setup, and it puts together together a number of pieces in an interesting way: text-to-text transformers, unit test generation, execution and code coverage. Results show a substantial improvement. The main weaknesses are that there are some caveats that need to be made, such as the (heuristic, not learned) way that test cases are translated across languages is not fully general, and that limits the applicability. There are also some cases where I find that the authors are stretching claims a bit beyond what experiments support, e.g., in the response to zd7L about applicability to COBOL.\n\nAll-in-all, though, it's a good implementation of an idea that should have a lasting place in this line of work, so it's worth accepting."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "This paper is aimed at using the automatic generation of unit tests as a means to perform transpilation (i.e., the translation of code from one language to another). The authors demonstrate their technique from Java to Python and from Python to C++. They claim that their approach outperforms prior methods by +16% and +24% over prior state-of-the-art.\n",
            "main_review": "Overall, I like this paper. The idea of using unit tests for transpilation is, although not formally guaranteed, a reasonable approach for transpilation in my opinion.\n\nSome of the language in the paper is a little off-putting as it seems to be disconnected (in my opinion) from deeply understanding the space of natural language translation (unstructured ambiguous languages) vs. programming language translation (structured unambiguous languages). It's my hope that if this paper is accepted, that the authors will polish the writing slightly.\n\nFor example: \"translation systems are not as effective for source code as for natural languages.\" This is a weird claim. The work that Alvin Cheung (Berkeley) and JRK (MIT) for Maaz Ahmad (UW Washington / Adobe Research) is wildly effective and has automatically transpiled ~300 functions with a geometric mean performance improvement of 3.36x. Moreover, this approach is *formally verified* unlike the authors approach. In addition, it's already being used in production quality code (Adobe Photoshop v.21).\n\nThere are many other such examples that seem to demonstrate the authors perhaps need to do a bit deeper literature review before making such bizarre claims. For example, the word \"transpilation\" never even appears in the paper, yet the whole paper is about transpilation. Do the authors not know about the field of \"transpilation\" and that they are working in this space?\n\nhttps://en.wikipedia.org/wiki/Source-to-source_compiler\n\nThese minor issues aside, I actually like the approach and am favorable toward the paper.\n",
            "summary_of_the_review": "TL;LD: it's an important emerging area. It's a reasonable (although not formal) way to perform transpilation. Using self-supervision is likely one of the most important aspects of the system because 99% of the code that we have today is currently unlabeled. To my knowledge, only a handful of labeled datasets exist (e.g., POJ-104, Google Code Jam, IBM/MIT's Project CodeNet, Microsoft's CodeXGLUE, etc.)\n\nWeak accept.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "- This work presents a way to fine-tune code translation models by providing weak indirect supervision from automated test case generation tools\n- The proposed approach has 3 main components\n   - Pre-trained models for code translation: the authors use Transcoder and DOBF for this.\n   - Automated test case generation tool: the authors use EvoSuite, an evolutionary computing based automated test case generation tool for Java, and\n   - Dataset: the authors collect approximately 330k Java code samples from Github with permissive licenses\n- The proposed approach then proceeds as follows:\n   - It starts by creating unit tests for Java code samples using EvoSuite\n   - It uses the generated expected input / output pairs to create unit tests for other languages -- Python and C++\n   - The method then translates code samples from one language to another and then validates the generated translation using the unit tests created in the step above\n   - In this manner, the approach collects parallel translation data samples that are later used for fine-tuning the model\n- The authors propose 2 variants of the method: offline and online\n   - In the offline variant, the approach utilizes all the parallel code samples collected to fine-tune the model, while\n   - In the online variant, the approach maintains a cache which is updated with new examples generated from the model\n- Models fine-tuned on parallel data created in this manner show substantial increase in performance over TransCoder and DOBF models.",
            "main_review": "**Strengths:**\n- A simple method is proposed to fine-tune existing code translation models without the need to manually curate parallel data\n- The method does not require any changes to the training procedure of the model, and can be independently applied in the fine-tuning stage\n- The method can potentially be applied to fine-tune a model for characteristics other than correctness. for ex, runtime efficiency, code style, etc.\n- Results show significant improvement over the baseline TransCoder and DOBF models\n\n\n**Weaknesses:**\n- If ground truth parallel data were available for this task, then fine-tuning a model on this data would naturally result in gains in performance. Since this parallel data is extremely scarce, the authors propose to utilize an automated test generation suite to build this parallel data from the model translations itself. I have 2 concerns with this proposal:\n   1. It requires an automated unit test generation tool to be available for at least one of the languages translated. Unit tests generation is an active area of research and tools are available only for limited sets of languages and primarily for modern languages such as Java, Python, etc. An important premise of automated code translation -- which the authors mention in their introduction and ethical concerns section -- is about modernizing legacy languages such as COBOL, where automated test generation systems might not be available at all. The proposed method therefore, in my opinion, is bottlenecked on the availability of automated unit test generation tools, and is not applicable when they are not available.\n   2. Automated unit test generation tools can themselves generate incorrect test cases -- test cases testing the incorrect method, incorrectly testing the method resulting in false positives or false negatives. AthenaTest [1], for instance, works better than EvoSuite and generates correct unit tests for the intended method only 16.21% of the time as per [1]. 26.71% of the times the test case generated by AthenaTest test for the wrong behavior. Other factors such as dynamic typing (as in Python) [2] or the use of more complicated data structures (such as Generics in Java [3]) further affects the effectiveness of these test generation systems. It would have been nice to get some insights into the quality of test cases generated and how this affects the performance of the translation model. This effect, I believe, would be especially pronounced in cases where the code translated is a piece of more complicated codebase and not standalone functions that can be compiled in isolation.\n\n- The authors utilize EvoSuite more as a test data generation tool than a test case generation tool. EvoSuite generates test cases for Java, while this work trains models for Java, Python and C++. Therefore, if I understand correctly, the way test cases are created for Python and C++ is by using the expected input/output pairs and assert statements generated for corresponding Java code, and putting these in a template for the other languages. I'm not completely convinced that this is an ideal strategy that can generalize to a broader set of languages or for more complicated pieces of code. For example: if the source code implements a Java class and the generated test operates on this class and their methods, how would this be translated to Python test without changing the procedure for Python test case generation? Or, how would this procedure work when there isn't a direct mapping of data types and constructs between the languages. This further reduces the applicability of this approach.\n\n- If I understand correctly, the evaluation used in this work includes a reference implementation and a translated implementation, which are tested to generate the same output given the same input? This creates problems as authors note in Figure 4, where the gold translation is equivalent to the source code only on a small domain, while the model generated translation, though correct and better is incorrectly marked wrong. This, to me, indicates an issue with the utilized test set where gold implementations might not be correct translations, thereby affecting the scores. Contemporary works in code generation and translation [4,5] have utilized expected input / output pairs to test the performance of the code generation system, and I wonder why the authors here chose to compare the performance against a reference implementation and not against input-output pairs directly. Additionally, we don't know how prevalent the problem of incorrect reference translations is in the utilized test set.\n\n\n**Other concerns/Suggestions:**\n- The authors mention that they collected data from GitHub with permissive licenses. Can the authors please share the list of licenses used for this data collection step? Having this information in the appendix will provide a reference point for future work in this space on which licenses might be acceptable to use when working with GitHub data.\n\n\n[1] Tufano, Michele, et al. \"Unit Test Case Generation with Transformers.\" arXiv preprint arXiv:2009.05617 (2020).\n\n[2] Lukasczyk, Stephan, Florian Kroiß, and Gordon Fraser. \"Automated Unit Test Generation for Python.\" International Symposium on Search Based Software Engineering. Springer, Cham, 2020.\n\n[3] Vogl, Sebastian, et al. \"EVOSUITE at the SBST 2021 Tool Competition.\" 2021 IEEE/ACM 14th International Workshop on Search-Based Software Testing (SBST). IEEE, 2021.\n\n[4] Hendrycks, Dan, et al. \"Measuring Coding Challenge Competence With APPS.\" arXiv preprint arXiv:2105.09938 (2021).\n\n[5] Chen, Mark, et al. \"Evaluating large language models trained on code.\" arXiv preprint arXiv:2107.03374 (2021).",
            "summary_of_the_review": "The proposed method achieve significant improvements over the baseline TransCoder and the DOBF models. However, I found the following weaknesses, resulting in my recommendation.\n\n1. Bottleneck on the availability of test generation tools thus limiting the number of languages it can be applied to.\n2. Quality of generated tests dips on more complicated code segments, and we do not know what effect that has on the performance. The method is trained on Java functions that can be executed in isolation.\n3. Method to generate test cases for translated Python and C++ codes seems to be dependent on expected input/output pairs, and it's not clear how the proposed method would work for more complicated code segments, or for languages with no direct mapping of data types and constructs.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper proposes a technique to generate parallel training data for machine translation between programming languages. This data is generated based on specifics of the programming languages and the availability of programming tools in these languages.\n\nUsing this additional data, the paper shows that machine translation using state-of-the-art transformers between programming languages significantly improves over prior data collection methods with back-translation.\n",
            "main_review": "The paper is well written, easy to follow and presents an interesting and novel idea. Technically, the work touches many parts - “static” text-to-text transformer, executing code, checking tests passing, observing and optimising test coverage. The paper has quite some effort into it in getting precise results using different decoding strategies and has ablation studies on these factors. What would have been more interesting though is how well would it work with differences in the test generation/acceptance strategies.\n\nThe experiment about dropping mutation scores is the most interesting one from this perspective. If I understand correctly, it seems to have no huge effect on the results (other things like the cache are also ablated?). Probably a related question is if computing mutation scores is needed or one can replace it with much cheaper code coverage metrics.\n\nOne of the biggest limitations of the approach seems to be around how unit tests are translated between the languages. The paper downplays this step in its expositions by simply saying that simple scripts can translate unit tests. While I agree that unit tests tend to be inputs and outputs for which translation should be easier, this is probably limiting the scope of what can possibly be translated. The authors could include an appendix or discussion on how many of the tests could be translated successfully and what incorrect translations look like - e.g. tests that perform a sequence of APIs or expect exceptions. A related question is if tests could be translated with some of the baseline models such as Transcoder/DOBF and if such an idea was considered and was/wasn’t successful.\n\nIt is not completely clear if the data filtering techniques (to only include functions for which tests can be generated) also play a role in the selection of the evaluation data. I would be happy to get a confirmation of this from the authors.\n",
            "summary_of_the_review": "The idea of the paper is specific to translation between programming languages and may be of interest to a subset of the ICLR community. There are interesting ideas and overall good evaluation of the paper.\n",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "The paper introduces a pipeline to generate parallel data corpus for code-to-code translation instrumented with unit tests, starting with monolingual data in Java programming language. This allows to improve data quality over noisy backtranslation approach and subsequently improve the model accuracy finetuned on these data. The approach is evaluated on the code-to-code translation against TransCoder baseline.\n\nThe authors also introduce two methods (online and offline) to train on these data, substantially improving upon the state of the art in unsupervised code translation, and reducing the error rate.",
            "main_review": "The paper is well written and easy to understand. It provides a very nice overview of related work and the history of software testing. The core methods is very practical and easy to reproduce.\n\nUsing unit tests to improve quality of parallel datasets generated via backtranslation is not entirely new in the ML for software engineering domain. There were attempts to use it for bug patching applications, see for instance, DeepDebug (https://arxiv.org/abs/2105.09352) paper. It is also frequently used during inference, as a postprocessing step, and evaluation. \n\nNevertheless, a technique that would work in a multilingual code-to-code translation setting is non-trivial and novel. Generating a high-quality parallel corpus starting with monolingual data is non-trivial, it relies on a pre-trained translation model for the target task, and translating unit tests generated by EvoSuite from Java to target languages using heuristics and mapping of types. Using mutation scores to select test suites is also novel.\n\nAuthors claim that the unit tests suites can be used to test program semantics in any programming languages, provided there is a way to establish the mapping between types and parameters. While this is true in most cases, have you observed and analyzed some corner cases, for instance, when an object needs to be instantiated in the body of the unit test method prior to assertions. Instantiating such variables may rely on imported classes or require translating extensive surrounding code context other than the unit test itself. How often does it happen, are these cases ignored?\n\nStarting with 333k Java functions, authors obtain 103k high-quality test case suites with EvoSuites which are then iteratively used to generate parallel training corpus. Have you analyzed the failed cases: \n•\tDoes the model successfully generates data for small methods only (e.g. a single statement), and majority of failed cases are long methods? How does it handle test cases with multiple asserts?\n•\tWhen does EvoSuite fail to generate high-quality test suites? I wonder if EvoSuite scaffolding is an issue\n\nMaybe I am missing something, but why would generated test set mismatch to TransCoder tests set be an issue? Are you generating tests sets for evaluation as well? Does this change the baseline (published) TransCoder computational accuracy numbers?\n\nMinor:\n\nFor multiple published papers the authors cite corresponding arXiv pre-prints. \nFor instance:\n1. Miltiadis Allamanis, Marc Brockschmidt, and M. Khademi. Learning to represent programs with graphs. ArXiv, abs/1711.00740, 2018 -> Published at ICLR 2018\n2. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. -> published at ICLR 2015\n… etc\n\n\n\"maximize a criteria such as code coverage or mutation score\" -> \"maximize criteria such as code coverage or mutation score\"\n\n“Otherwise, the mutant is said to be survived” -> “Otherwise, the mutant is said to have survived”\n\nA1 caption (right), should perhaps be changed to “A generated unit test suite”",
            "summary_of_the_review": "A well written paper that introduces a practical method to generate a supervised parallel corpus for code-to-code translation. The findings are  generally well supported by evaluation, but some additional tests could strengthen the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        }
    ]
}