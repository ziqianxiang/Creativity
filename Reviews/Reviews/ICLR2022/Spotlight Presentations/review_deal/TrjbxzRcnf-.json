{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "This paper studies the problem of dealing with long contexts within a Transformer architecture.\nThe key contribution is a  kNN memory module that works in concert with  a Transformer by integrating upper layers with additional retrieved context.\n\nThe idea is simple  but the execution is good.  While the  idea is reminiscent of other recent work on this topic, and novelty is somewhat borderline, it is practically useful.\nOverall, though ambivalent, my recommendation is that the paper should probably be accepted"
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The paper plugs a kNN memory module into a Transformer for long-distance knowledge. Specifically, the bottom layers are still Transformer blocks, then a kNN-augmented attention layer is used to store the processed (key, value) pairs into external memory, and another Transformer block is put at the top layer for aggregation. The LM experiments are conducted on webtext (C4), math papers (arXiv), books (PG-19), code (Github), as well as formal theorems (Isabelle).",
            "main_review": "Strengths:\n\n- Integrating kNN module into Transformer is an interesting modeling technique, which would be potentially useful for language generation especially when the context is quite long.\n\n- The kNN layer is integrated into the attention layer, while previous work usually integrates it into FFN blocks.\n\n- The LM experiments are conducted on several corpora.\n\nWeaknesses:\n\n- The method is not compared with previous models in the experiments, such as Transformer-XL, Compressive Transformers, and other efficient Transformers for LM.\n\n- There is no ablation study in terms of modeling.\n\n- I didn't find clear comparisons of training speed and GPU memory usage between different models. It is valuable for the readers to have a sense of what kind of resource is needed to train or host such models.\n\n- Is the application restricted to sequence decoding? Can we finetune the proposed model as BERT on downstream tasks, and how about the performance?\n\n- The above question is related to generalization. Such kind of kNN method seems to suffer from too much memorization instead of generalization. If the model cannot generalize well, it would not be good at zero- or few-shot learning. I agree that language modeling is a meaningful task, but it is more like a testbed for modeling techniques that improve generalization. Otherwise, the shortcut of memorizing context would hide the real challenge. \n\n- The code seems unavailable. It is unclear how many engineering efforts are required to reproduce the results. The paper writing ignores many details that are helpful for reproducing the method. Even the code would be released, the standalone paper should be informative enough.\n\n- The kNN layer is densely triggered for each position, which seems quite heavy. How about using a gate to determine whether we need to perform kNN?\n\n- Is there any evaluation about the performance of the learned kNN search? For example, we could conduct some human evaluations to see whether the retrieved knowledge makes sense for prediction.\n\n- The model is only evaluated on the language modeling task. More tasks would be helpful to indicate whether the proposed method is general enough in terms of data distribution.\n",
            "summary_of_the_review": "I am okay to accept the submission. But it can be improved as mentioned in the above review (e.g., including ablation studies, more tasks, comparisons with previous models, and analysis).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This work proposes to deal with long-range dependencies in sequences using a transformer language model augmented with memory. The memory comes in the form of a k-Nearest Neighbor (kNN). The memory stores the last M  (Key, Values) from the previous-to-last layer, and proposes k neighbors from the memory per element in the sequence to attend to the last layer, in addition to the current ones in the sequence. The last layer is trained to utilize the memories. The memory is non-differentiable, but it is still possible to be updated, and used an \"input\" to a layer in the network. The authors test their model with math paper from arXiv (in Latex), PG-19, C4, Github code, and Isabelle (theorem proving). \nThe experiments measure performance of the language models with perplexity. The context (input) length is typically fixed to a relatively small number (512). The paper studies effects of memory vs no-memory,  different sizes of memory, and fine-tuning by training on small memory first followed by an increase in memory size. An alternative with 2 k-NN memories for the last 2 layers was tested too. Further, a few examples show how the memory is most usefully used (mostly for predicting rare tokens). The experiments show improvements in perplexity when using memory, and some saturation points.",
            "main_review": "The paper proposes a novel method to connect a k-NN based memory with transformers for language modeling. The idea seems interesting and somewhat novel and evaluated empirically. The previous work is nicely identified, although the work of Rae et al. (ICLR 2020) could be described there as well. \n\nStrong points:\n- The memory seems to be usefully working and having a positive effect.\n- Practically, seems to work.\n- Relevant work to this community.\n\nWeak points:\n- Section 3 could see more formalization and better description on what are the input for searching the kNN, how the model is trained (maybe, an algorithm figure).\n- Experimentation could be greatly improved: no hyperparameter search or reasoning behind the selection, no baselines with other memory architectures showing differences either performance or specific use-cases (beyond plain transformers).\n- Memory sizes are too big for the data selected. It is unclear why such big memories are still improving the model.\n- The results on PG-19 doesn't seem to replicate previous work (original paper shows 33.6 for Compressive transformer, whereas this work shows ~19 for plain transformers). \n- Results are very close for each memory size and there are no error measurements (e.g., std dev). It is hard to conclude anything about memory sizes and their effectiveness. \n- Hard to replicate: several proposed datasets are downloaded from the web, it will be hard to download the same files.\n- No details on the specific hardware and software used.\n\nMinor:\n- What do you refer with \"16 attention heads of size 64\"?\n- Describe the content of the figures in the caption (e.g., what is the architecture about?)",
            "summary_of_the_review": "This is an empirical work that aims at improving the processing of long-range dependencies in sequential data. \nI've updated my score  recommendation is to accept the paper given that the additional experiments and standard deviations are finalized and included in the paper. I would recommend to further include a word-level tokenization of the PG-19 dataset (and maybe others) to make them comparable to previous work [Rae et al.] and more challenging to the model. This would allow readers understand whether such benefits are indeed beyond the compressive transformer or other work. Also, as the authors are using character-level language modeling, it would be much better to use the bits-per-character metric instead of perplexity. \n\nMy recommendation is to reject this work given the poor quality of the experimental section (see above and below). Also, there will be some challenges to reproduce this work with respect to the datasets that should be addressed as well. \n\nQuestions:\n- Please, could you clarify how did you choose the hyperparameters and justify them?\n- What is being used as search in k-NN? The output of the layer connected to the memory? Are the keys used in the search or both key/values?\n- Why did you choose k=128 in k-NN? What would happen if these value changes?\n- Why do you use one layer of memory, if two improve the results?\n- Could you add std dev. values to all your experiments? \n- Please explain why does the arXiv dataset have such a low perplexity?\n- How many sequences are used for training, validation and testing?\n\n==================================\n\nUpdate:\n\nI've updated my score based on the rebuttal discussion with the authors, and the additional elements presented. The new score is based on the existing merit, however, the contributions are limited and the experimental results require further work. The additional experiments could strongly benefit the understandings of the benefits of this method. For example, finalizing the experiment with 2K vs 500 + 1.5K memory with transformer and transformer-XL, would add lots of value. The presented results are difficult to compare with previous work (other than the included transformer-XL) due to the selected tokenizer. Therefore, including other models, e.g., compressive transformer, is missing. This could have been avoided by reproducing the experiments from those works. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
            ],
            "details_of_ethics_concerns": "Some of the experiments in the paper were implemented on data downloaded online. \nThis data includes scientific papers from arXiv, code from github, and mathematical proofs from another repository. It is unclear if the authors have rights to apply their work on these papers. \nTaking code for a dataset requires proper licenses, and same goes for arXiv papers that may have different licenses (different CC types, etc).\nThe authors don't mention how they deal with copyrights. \n\n\n-----------------\nUpdate:\n\nThanks to the authors for clarifying how they deal with these aspects.\n\n\n ",
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "This paper presents a memory-based Transformer where a memory is used to leverage the external inputs for scenarios that have long inputs like documents or codes. The key contribution is that it describes how external contexts are integrated into the representations of the current inputs. By introducing such external contexts with the memory mechanism, perplexity of LM can be improved on every dataset studied in the paper.",
            "main_review": "Pros:\n- propose a non-differentiable cache mechanism to reuse keys and values coming from prior training steps.\n- comprehensive experiments are conducted to evaluate the effectiveness of the proposed memory-based method.\nCons:\n- there are some benchmarks that require models to be able to handle long inputs, such as document-level QA datasets and document-level retrieval. Besides the LM task, it could be better if more document-level downstream tasks could be evaluated to verify the usefulness of the proposed approach.\n",
            "summary_of_the_review": "This work is highly related to those scenarios, such as document-level QA/retrieval and code completion, where the models should consider and leverage long inputs. The key contribution, based on my understanding, is the joint KNN/local attention part, where both external context and local context are combined. In general, the paper is well written and all details are clear to me. One thing could be further improved is that I suggest to include more downstream tasks to verify the effectiveness of the proposed method.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
        },
        {
            "summary_of_the_paper": "\nTransformer language models attend to the previous N tokens, and so do not have any access or knowledge of the tokens that appeared more than N tokens ago. Extending N would allow language models to have more knowledge which might increase performance. But there’s a tight limit as to how far we can naively extend N, since memory usage grows quadratically with N, and so in SoTA transformer LMs N is limited to a few thousand tokens. \n\nThis paper shows a novel, simple and intuitive method for extending the number of tokens a transformer can access at every timestep. The authors augment the model with a kNN datastore, so that in the top layer, the model attends to the local context using normal attention but then also attends to up to 262k previous tokens by using an approximate nearest-neighbors datastore. This is basically equivalent to approximately attending over those previous tokens. No gradients are passed to the representations in the datastore in any way, making this easy to implement and relatively efficient to train. \n\nThe authors present results on multiple language modeling datasets that have long-range dependencies. On most of the datasets the gains from using the proposed method are quite small (<=.1 perplexity), and it’s not clear that they are statistically significant, but on the C4 dataset the model reduces perplexity by 2 which I believe to be a strong result. I believe that this idea is promising and that giving it a stage at ICLR will push the community to try further improving upon it, hopefully leading to larger gains. \n\n",
            "main_review": "Strengths:\n\n1. The method proposed is novel, intuitive and simple to implement. This is a huge advantage in the language modeling space. We’ve seen many memory-augmented transformers in the past few years, and most of them have failed to have significant impact because they are too complicated to replicate or take too many resources to train. \n2. The results on the C4 dataset seem significant and show that  memory augmentation could really improve transformer language modeling. \n3. The analysis is super interesting, and it shows the potential of this simple approximate attention method to find very relevant contexts to retrieve from the recent past, and how these retrievals aid in improving the LM. \n4. The paper is well-written and easy to understand. \n\nWeaknesses:\n\n1. The authors do not compare their model to any previously published model. Throughout the paper the authors only compare their model to baselines that they designed and trained. Doing this for some of the datasets is fine, but not even having one dataset in which you compare to strong, previously published models makes it harder to convince the reader that you have found something interesting. You did show results for PG19, which has been used before in LMing papers, so it’s not clear why you haven’t compared to previously published models on that dataset, such as the compressive transformer? The numbers in the compressive transformer are very different from the numbers your models obtain, so I’m assuming that you preprocessed that dataset differently and so have a different vocabulary, making those results incomparable? Could you re-train your model using the same settings as the compressive transformer so that we could compare the results of your method and theirs?\n2. The baseline has just 6 layers, which is significantly smaller than the current transformer LMs that are being discussed by the community. I’m definitely not expecting to see GPT-3 sized models in every research paper (or even close to that) but I think 16 layers would make more sense for such an exploration. I’m afraid the gains shown here would not transfer to larger models. On the other hand, larger models might produce better representations resulting in even bigger gains. \n3. The authors do not show standard deviation numbers for any of the baselines on the various datasets. This makes understanding the results on each dataset, for the different models, harder. For example, on the arxiv dataset the authors talk about a drop from 2.927 to 2.877 perplexity between two different models, but is a 0.05 perplexity difference really statistically significant here? \n4. The authors don’t provide any speed or memory usage statistics. Of course, it’s totally fine if this model runs slower or uses more memory than the baseline- I expect such preliminary investigations to potentially be nonoptimal, with later work making it an actually viable model. I would not penalize this paper for having slow runtime or using too much memory. But I think it's important to discuss these things. \n\n\nMisc:\n\n1.  Although the perplexity improvements on many of the datasets are quite small (<=.1 perplexity) this could be a non-issue, and more of a problem with the perplexity metric than with the model. The perplexity metric, and most of the data that we train our LMs on, heavily incentivizes models to get short term dependencies right. Getting the model to look back into the past is both hard and barely incentivized, and so these small initial gains that are attained in this paper might be improved when we find a better loss function that incentivizes looking back more, or when we find data that has more long-term dependencies and less short-term ones. \n2. The way I see it, there are two different ways to augment the transformer with a memory module. The memory module could be ‘global’ or ‘local’. Local memory transformers have a memory module that can only access the recent past. So models like the compressive transformer or Transformer-XL are local memory transformers. On the other hand, models like the kNN-LM or REALM are ‘global’ memory models, since they can access the entire datastore for every prediction, without any kind of limitation that is dependent on the current timestep and the recent previous timesteps.  \nThe model proposed here is clearly a ‘local’ memory transformer, since it performs approximate attention over the recent past. I believe that that is part of what makes this model work, and I think the paper might benefit from a discussion of this and maybe even an empirical test. Does this model improve performance on WikiText-103, where all the articles are in a shuffled ordering? I don’t think it would, and that’s totally fine, but I think readers would benefit from a discussion of this property of the model.  \n3. I think it could be helpful if the authors had more comparisons where their model and the baseline can access the same amount of tokens for every prediction. For example, if you train one of the memory models with 2k tokens of memory and 512 tokens of local context it would be interesting to compare it to the baseline model trained on 2.5k tokens. Of course, since your model performs approximate attention we do expect to see worse perplexity values, but it would be very insightful to see precisely how much using this approximate attention method degrades performance.  \n4. For section 3.2, consider referencing [CrossBatch] which discusses this distributional shift in section 3.2 of their paper, in the vision domain. \n\nReferences:\n\n[Compressive Transformer] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Timothy P. Lillicrap. “Compressive Transformers for Long-Range Sequence Modelling” \n\n[REALM] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang. “REALM: Retrieval-Augmented Language Model Pre-Training”\n\n[CrossBatch] Xun Wang, Haozhi Zhang, Weilin Huang, Matthew R. Scott. “Cross-Batch Memory for Embedding Learning” \n",
            "summary_of_the_review": "This paper introduces a new method for extending the transformer model with a simple datastore. The insert and query operations into this datastore do not have to be trained which is a big advantage for efficiency and simplifies the implementation. Results are shown on multiple datasets, with the model showing big gains on one and smaller gains on the others. I believe the idea presented is reproducible, and extensible and I’m looking forward to seeing how this research direction develops. The analysis presented is insightful and could guide further research. I believe this paper should be accepted. ",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}