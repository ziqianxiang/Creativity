{
    "Decision": {
        "title": "Paper Decision",
        "decision": "Accept (Spotlight)",
        "comment": "A novel method is described that uses RL to search for a rule set which predicts multiple relations at once for KBC-like problems.  The rules can include latent predicates, which reduces the complexity of individual rules, similar to Cropper & Muggleton's (2015) meta-interpretive learning framework, which is usual for rule-learning systems.  Another novel aspect is use of a cache memory for rules.\n\nPros\n - the idea of using RL instead of carefully-designed discrete search for symbolic learning systems is a very nice novel idea\n - the experimental results are strong\n\nCons\n - the benchmarks are synthetic (although GraphLog does at least include noise)\n - although the Cropper and Muggleton work is cited the relationship between the search spaces of the two systems is not discussed - this should be corrected in the final version."
    },
    "Reviews": [
        {
            "summary_of_the_paper": "The author investigates the rule learning problem and  proposes R5, a reinforcement learning  based framework to reason over relational graph data and explicitly mines underlying compositional logical rules from observations. It mainly contains two components: RL agent to model rule learning as a sequential decision-making problem and a dynamic rule memory module to maintain and score candidate rules during the training of the reasoning agent.\n",
            "main_review": "\nStrengths  \n\nS1 The dynamic rule memory module is interesting.\n\nS2 Detailed ablation study and case study.\n\nS2 The paper is well written and well organized.\n\nWeaknesses\n\nW1 RL agent aims to seek long-term and maximum overall reward to achieve an optimal solution. However, this paper aims to divide the long-term decision into independent short-term decision. Therefore, I don’t see why RL should be used in this scenario.\n\nW2 Too much reliance on heuristics. For example, it utilizes statistical features, such as the number of occurrences to represent relation pairs. \n\nW3 How to recover the logical rule is unclear to me. Should we enumerate over the whole rule space to select the rules with the highest score?\n\nW4 How do the authors handle uncertainty during the deduction is unclear. It is possible for a two relation sequence to have multi heads. For example, given two relation sequences hasAunt and hasSister, both hasMother and HasAunt can be the head. How can the proposed solution solve the multi head scenario?\n\nW5 The author should discuss more about how to choose the value of n (number of unknown relations). Intuitively, a small n may reduce the expressiveness of the model while a big n may cause computational inefficiency. Choosing the value of n could be a tricky problem.\n\nW6 Lack of some representative baselines. For example, NeuralLP, RNNLogic are missing.\n\nW7 Datasets include too few relations.To show the scalability of the proposed model, it will be better to conduct experiments on datasets with more relations e.g., FB15k.",
            "summary_of_the_review": "The author investigates an interesting problem, logical rule learning. It also proposes an interesting idea to decompose the long path into small pieces to recurrently learn rules. Although the idea is interesting, the model and the experiments are less convincing as pointed out in the weaknesses. It will be better for the author to clearly illustrate the reasoning why they choose the  RL model to learn rules and how they handle uncertainty during the deduction. They also need to include more baselines and larger datasets to make the experiments more convincing.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This work explores a rule learning approach (R5) for 2 relation prediction tasks (CLUTRR and GraphLog). The proposed approach starts by finding connecting paths between the two query entities. Then it recursively merge relation pairs until it consists of a single relation output. The merging process is controlled by a policy/value network trained with episodic rewards. The rules are induced whenever the output relation matches the gold relation -- in a fashion similar to that of curriculum learning. \n\nExperiments show that R5 generalizes better than previous approaches such as Graph Attention Networks (GAN) and Conditional Theorem Provers (CTPs) especially when generalizing to paths longer than those from the training set.\n",
            "main_review": "In general the proposed approach is reasonable and effective for the targeted 2 tasks. The introduction of new relations, and application of RL are interesting. However, the task and model design seem to be very specific to a type of task where the relevant part of a graph (to the prediction) is restricted to a path, which can be recursively derived from the target relation. The separation of path finding and reasoning also seems to be an artifact from the specific task assumption.\n\n\nClarity related questions: \nIt would be helpful to formally define the relation prediction task.\n\nIt would be helpful to describe the path sampling procedure.\n\nHow is the model updated given the loss function l?\n\nThe state representation has one entry per relation pair. Will that lead to information loss when a relation pair appears multiple times in the path?\n\n\"recurrent reasoning agent enables us to predict ... efficiently\" -- it is more efficient than what approach? and why?\n\nTable 4 and 5: given that the rule recalls of R5 are almost 100% why the accuracy much lower than 100%?\n\n",
            "summary_of_the_review": "An interesting approach with good results for a limited task setup.",
            "correctness": "3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
        },
        {
            "summary_of_the_paper": "This paper presents a novel method for rule induction. The main idea is to apply\nreinforcement learning in the task of relational pathfinding within a finite\nHerbrand base. The reinforcement learner uses MCTS to find the best routes to\nestablish the path in between the two arguments of training examples, while the\nuseful actions (which are length-two relational paths) are maintained in a hash\ntable as the induced ruleset. Experimental results has shown the effectiveness\nof the proposed method.",
            "main_review": "Program induction is a hard problem, especially for large-scale tasks in noisy\ndomains. This problem can be formulated as a Relational Pathfinding task [1], which\nhas been studied for many years in the area of Inductive Logic Programming. In\nStatistical Relational Learning there is also a missing related work that\nsearches for grounded relational paths first and then applies machine learning\nto extract first-order logical rules [2].\n\nCompared to previous works in this direction, the main contribution of this\nwork is applying reinforcement learning in the relational pathfinding problem. The\nproposed approach works surprisingly well in the experiments, and the combination of \nneural-guided heuristic learning for MCTS and the symbolic rewarding method perform good\nin the relational pathfinding problem.\n\nThe main weakness of relational pathfinding is the grounding problem, i.e., for\neach training example, the algorithm needs to search for ground paths before\ntraining. Besides, the pathfinding algorithm may suffer from incompleteness if\nthe hypothesis has some complex structures, e.g., if there are some useful\nrelations do not showing in a path (node \"z\" in p(x,y)<-q(x,y),r(x,z)), or if\nthere exists higher-order primitive predicates or triadic predicates. Another\npossible weakness is that reinforcement learning usually requires large numbers\nof training epochs, especially when learning the heuristics for program\ninduction.\n\n### Some minor issues:\n- \"unknown relations\" should be called \"invented predicates/relations\", following the\n  convention in the area of program induction;\n- The algorithm implicitly assumes all rules are using the same template, which\n  is a dyadic chain meta-rule [3], which could result in incompleteness [4]\n  (i.e., some hypotheses can never be learned), I think this should be made\n  clear in this paper.\n- Following my last point, the proposed method seems can only learn dyadic chain\n  rules in datalog (e.g., every rule in the target hypothesis have the form of\n  \"p(x,y)<-q(x,z), r(z,y)\", and the language is function-free). Is it possible to extend it to complex domains?\n- The learned policy can not be transferred to other tasks (e.g., adapt from one\n  task to another task, or from one target relation to another one).\n\n### References:\n1. Bradley L. Richards and Raymond J. Mooney. Learning relations by pathfinding.\n   In Proceedings of the 10th National Conference on Artificial Intelligence,\n   pages 50–55, San Jose, CA, July 1992.\n2. Wang-Zhou Dai and Zhi-Hua Zhou. Statistical unfolded logic learning. In:\n   Proceedings of the 7th Asian Conference on Machine Learning (ACML’15), Hong\n   Kong, 2015, JMLR: W&CP 45, pp. 349-361.\n3. S.H. Muggleton, D. Lin, N. Pahlavi, and A. Tamaddoni-Nezhad.\n   Meta-interpretive learning: application to grammatical inference. Machine\n   Learning, 94:25-49, 2014.\n4. Andrew Cropper and Stephen H. Muggleton. Logical minimisation of meta-rules\n   within meta-interpretive learning. In Proceedings of the 24th International\n   Conference on Inductive Logic Programming, pages 65-78.\n   Springer-Verlag, 2015. LNAI 9046.\n",
            "summary_of_the_review": "This paper presents a novel algorithm to solve program induction formulated as a\nrelational pathfinding problem. The proposed approach outperforms existing rule\ninduction methods in several benchmark datasets. It has missed several important\nreferences and needs further clarification, so I would like to recommend to\naccept if the authors can fix those problems.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold",
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
        }
    ]
}