{
    "Decision": {
        "decision": "Accept (Spotlight)",
        "comment": "All reviewers unanimously accept the paper.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper introduces a novel  meta path planning algorithm  that utilizes neural network module that improves the data-efficiency for iterated path planning problems.\n\nThe authors address a relevant issue and the experiments make sense given the research question.  I particular like the 3 ablation studies that the authors include, which makes the empirical analysis very thorough.\n\nWriting and Clarity:\nThe introduction is written quite well. Section II&III is written quite technical and dense. This can be very hard to understand for non-experts. However these section are  important to understand the  rest paper. Finally, these two sections should be integrated (preliminaries, quite literally, should be at the beginning). Section 5 \n\n\nAdditional Questions:\n1. Philosophically, how does the self-improvement for iterative planning problems not contradict the no-free lunch theorem? What kind of repeated structure do we assume here (because it seems as in Fig. 1 both the obstacles as well as the goal state change randomly)\n2. As you employ a neural network to do value iteration how does the wall-clock time compare to the baselines?  I do not mean the environment time-ticks (that you checked for using the number of collision checks), but actual compute time. \n3. How sensitive is the proposed solution to parameter initialization?  Did you find much variation in changing hyper-parameters, such as network topology, learning rate et cetera?\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #1",
            "review": "Summary:\n\nMotion-planning in high dimensional spaces is challenging due to the curse of dimensionality. Sampling-based motion planners like PRM, PRM*, RRT, RRT*, BIT* etc have been the go-to solution family. But often these algorithms solve every planning problem tabula rasa. This work combines learning with sampling-based planning such that the parent-sampling and expansion steps instead of being done by common heuristics are learnt in an online manner. Also the resulting exploration-exploitation problem is naturally dealt via using a UCB-style contextual bandit algorithm. Since the number of parents are always varying the common trick of 'describe the action choices with respect to the environment' is adopted so that varying number of actions (states to be sampled from) can be naturally incorporated. \n\nThe other significant aspect of this paper is that there is a self-improving component (Algorithm 3) where a dataset is built up every time step, of environments where either an expansion with RRT or the learnt expansion policy is attempted with the policy being invoked more as time goes on and it trains more. If the process succeeds in finding a path to the goal then this example is added to a dataset and the dataset used to update the policy and associated value function to guide it towards the feasible paths found in the tree. \n\nComments:\n\n\n- Algorithm 3: \"Reconstruct optimal path\". These paths are not really optimal for the problem. They are optimal in the tree T that is built so far for example U. But for the problem they are feasible and if RRT* were to be run asymptotically then perhaps near-optimal. The accompanying text should be updated accordingly so that there isn't confusion.\n\n- Here is my main concern with Algorithm 3: For equation 6  where the policy and value functions are updated, the policy is inevitably going to suffer from covariate shift. This is because the algorithm is essentially doing behavior cloning (BC) with respect to the feasible paths found on the planning examples. Since we are inherently in a sequential setting (non-iid) where the states visited by the policy are a direct result of its own decisions the error bound will be quadratic in the horizon (path-length) for equation 6. This phenomenon has been well-understood in imitation learning literature and algorithms like DAgger, AggreVate or online versions like AggreVateD, LOLS already address these problems in a principled manner. Equation 6 should ideally be replaced with an inner DAgger/AggreVateD like loop (with an RRT* dynamic oracle) for stable learning of policy and value function. I am happy to be convinced that covariate shift and resulting quadratic mistake-bound problems are not present here.\n\n- Application of imitation learning to both self-improvement style path planning and leveraging experience in planning has been done before: See \"Learning to Search via Retrospective Imitation\nJialin Song, Ravi Lanka, Albert Zhao, Aadyot Bhatnagar, Yisong Yue, Masahiro Ono, 2018\" (this is unpublished it seems so it is unfair of me to mention this perhaps but I wanted to give an example of how to use dynamic oracles for stable imitation in planning.) and \"Data-driven Planning via Imitation Learning\nSanjiban Choudhury, Mohak Bhardwaj, Sankalp Arora, Ashish Kapoorâ€ , Gireeja Ranade, Sebastian Scherer and Debadeepta Dey\", IJRR 2018. At least the last paper should be cited and discussed in related work.\n\n- Also would be curious how the authors would situate methods which are non-learning based but leverage experience in planning (example E-Graphs: Bootstrapping Planning with Experience Graphs, Phillips et al, RSS 2012) via graphs discovered in other problems directly. Perhaps a discussion in related work is warranted?\n\nUpdate: After rebuttal updating to Accept.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposes an approach to learn how to plan in continuous spaces using neural\nnets to learn a value function and a policy for scoring and sampling next-step candidates\nin a stochastic tree search.  The networks are updated as more planning tasks\nare executed, producing more data for the policy and value function, leading to gradually\nbetter plans compared to a number of baselines on benchmarks introduced by the authors.\n\nThis is a very interesting paper, although I did not always found it easy to read,\nmaybe too densely packed for comfort. My main concerns are clarity of the exposition (especially\nof the neural net architecture (sec 4.2) and that the comparisons are exclusively done\non benchmarks introduced by the authors rather than on benchmarks on which the baseline\nmethods had been previously been optimized, which may introduce a bias in favour of the\nproposed approach.\n\nClarifications\n\nBefore eqn 2, I don't understand why U includes both S_free and map, although the map specifies the free space and thus S_free seems redundant.\n\nIn sec 3 (page 3), the authors introduce a new notation s_init which seems to be the same as s_0 in the previous sections (or is it?).\n\nSection 4.2 was really difficult for me to parse and is too compressed (so is the rest of the paper but this one was worse).\n\nFigures were too small (esp. fig 4 and fig 7) for me to read from the printed paper.\n\nThe term 'meta self-improving learning' seems inappropriate. I did not see  how this was a form of meta-learning. Unless I missed something I suggest to change the terminology.\n\nOther Concerns\n\nI have a concern regarding the way r_t(s) is estimated (page 4) by kernel interpolation of the rewards. I fear that it will not generalize properly when trying to extrapolate, especially in high dimensions (since the claim of the paper is that the proposed algorithms is meant for 'high dimensional' states).\n\nIn addition, the experiments are actually performed in  rather low-dimensional settings (compared to working on problems with perceptual inputs, for example).\n\n"
        }
    ]
}