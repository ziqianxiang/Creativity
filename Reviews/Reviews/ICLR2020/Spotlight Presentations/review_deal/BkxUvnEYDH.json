{
    "Decision": {
        "decision": "Accept (Spotlight)",
        "comment": "This paper provides a fascinating hybridization approach to incorporating programs as priors over policies which are then refined using deep RL. The reviewers were, at the end of the discussion, all in favour of acceptance (with the majority strongly in favour). An excellent paper I hope to see included in the conference.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "This paper provides a method for instructing an agent using programs as input instructions, so that the agent should learn to contextually execute this program in a specified environment, learning to generalise as needed from perception, and to satisfy concerns that in the language of planning would be called monitoring and execution. The authors provide a method that breaks this problem down into one of interpreting the program (which is crafted separately as a compiler that benefits from a DSL), learning to identify contextual features and then adapting and applying the policy.\n\nThe arguments in this paper are well made but the paper would benefit from better clarifying several points:\n\n1. To start at the very beginning, the authors begin in the first page by giving the impression that the agent has gone directly from an NL instruction and otherwise uninterpreted sensory input to a solution, in the spirit of typical end to end systems, whereas what the authors are proposing is a very different and more pragmatic approach wherein the interpretation of the task is handled prior to learning, so that learning is only applied to smaller subproblems. This could be made clearer in the introduction.\n\n2. In particular, it was unclear how the DSL comes about and what restrictions it places on the problem. The DSL will clearly have an influence because a very different task from MineCraft, say, robot manipulation, would have quite different needs of sensor-driven control and hence the information flows (specifically, the separation between goal identification, context perception and motion execution) would be different. What one puts into the DSL will significantly influence how well the overall framework performs (e.g., the ability to crisply ask is_there is powerful). Have the authors systematically explored this axis of design? Can we hear more in the setup about this?\n\n3. The influence of the domain is once again seen in the modulation mechanism for the goal and the way in which affine transformations enable generalisation. This is of course sensible in 2D spatial navigation but may be less straight forward in other decision making contexts. The authors have been clear enough about what they have done, but I would have found it interesting to understand how much we should expect this particular concept to stretch and where its limitations become more apparent - perhaps in the discussion.\n\nOverall, this is good work and the writing is clear with suitable references. I would note that the authors are revisiting concerns well studied in the planning literature. While the authors do acknowledge HAMs and so on from the HRL literature, they'd make the paper stronger by also tracing the roots of some of these ideas into the rest of planning.\n\nI'll end this review by asking about the relationships between NL instructions and the formal programs. In some domains, the number of realisable programs that map to an ambiguous NL instruction can be large. Equally, not all lay users can write good programs. So, it is worth noting this gap and making clear that this paper does not really address this bit of the overall problem.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #2",
            "review": "This paper presents a reinforcement learning agent that learns to execute tasks specified in a form of programs with an architecture consisting of three modules. The (fixed) interpreter module interprets the program, by issuing queries to a (pre-trained) vision module and giving goals to a policy module that executes them in the environment. The paper also introduces a policy modulation technique, with the goal of modulating the current state with the expected (symbolic) goal. The model is evaluated on a 2D approximation of Minecraft, where it outperforms a set of baselines. In addition, the authors modified the program dataset and re-expressed it in terms of natural language and showed that the baselines perform better with programs than with the natural language instructions.\n\n\nThough I think the general idea of the paper is worth exploring, I find the concrete contribution of the paper a bit thin to the point that I am hesitant to recommend this paper for acceptance. Allow me to explain my objections:\n\nFirst and foremost, this work is very close to work by Denil et al where the execute (in a differentiable fashion) programs in an RL agent. Their work does not have a discrete interpreter per-se, but it does have a differentiable execution of commands. The major difference between these two works would be that Denil et al do not have the vision module (they do mention learning from pixels as future work).\nHowever, that is not entirely true. The model presented here uses a pretrained vision module, which by itself is not a problem and is used in related work [1], but this vision module does not operate on visual input but the symbolic representation of the map. A crucial thing here is that if all of a sudden we want to include a new object on the map, the model won’t be able to use some learned similarity since it would require introducing a new object (slice of the input), as it would should it have been trained on pixels. So technically speaking, this is not a vision module but a symbolic state processing module.\n\nThen, the modulation mentioned in the paper does not seem particularly novel. Sure, the exact architecture of the model is probably unique, but the idea of modulating a state with a goal is not and has been seen in other work such as [2] and [3] among others. The paper does not mention why, for example, this modulation technique is useful and why any other similar architecture would not be as successful, nor does it mention related modulation techniques in other work.\n\nA big issue I have with the evaluation in the paper is that I do not see the benefit of having the experiments with natural language at all. The focal point of the paper are agents able to execute tasks in the form of programs. The (though manually) generated natural language instructions from those same programs cannot even be used by the proposed agent as there is no natural language interpreter, so they are a dangling part of the paper which is there just to showcase that programs should be easier for baselines to learn to execute than natural language (the hypothesis would be that a simpler and more structured/formal language is easier to learn than the natural language). Hence the seq-LSTM results on language which are a tad lower than the results on programs are expected, though the performance of transformers is the opposite---they are better on language, and that is something that is puzzling and left unexplained, as well as the unexpectedly low performance of Tree-RNNs. One would expect them to perform a bit better than LSTMs, but that might be contingent on the size of the dataset more than the structure of the inputs. However, none of these curious findings have been explained.\nMoreover, the comparison to baselines is not particularly fair as these baselines had to learn the symbolic state interpretation, whereas your model did not. You could have provided the same to the baselines for a better comparison.\n\nIn addition to that, 5.4.2 goes into detail of analysing the baselines, and ignoring the proposed model. Why didn’t you include the same statistics for your agent in Figure 5 and said subsubsection?\n\nThe paper is missing some notable related work:\n- S.R.K. Branavan’s work (all but one cited language-instructed agent papers are post 2017) as well as [4]\n- object-oriented and hierarchical RL\n- [5], where they train the neural programmer interpreter from the final reward only, which brings NPI close to this work\n\nQuestions:\n- Figure 5 - There’s a mark for Tree-RNN, but Tree-RNNs are not in any figure. Why didn’t you plot the performance of your model?\n- The setup naturally induces a curriculum - how does it do that if the programs are randomly sampled?\n- You say that your model learns to comprehending the program flow. I’m not sure I would agree with that because what your model learns is to execute single commands. From what it seems, the interpreter is the only part of the model (which is fixed btw) which sees the control flow, whereas the policy just executes singular commands. Did you mean something else by that statement?\n\nMinor issues:\n- You say twice interpreter (i.e. compiler). Given that they’re not the same, and you’re using an interpreter, I suggest omitting the word compiler.\n- Figure 2 is lacking detail. There is no difference between r and i (both being positive integers) other than their descriptions, - operators agent[_], env and is_there lack parameters (non-terminal nodes), and where’s river, bridge, etc?\n\n[1] COBRA: Data-Efficient Model-Based RL through Unsupervised Object Discovery and Curiosity-Driven Exploration\n[2] FeUdal Networks for Hierarchical Reinforcement Learning\n[3] Universal value function approximators\n[4] Vogel et al Learning to follow navigational directions\n[5] Improving the Universality and Learnability of Neural Programmer-Interpreters with Combinator Abstraction",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "Update: I thank the reviewers for their extensive rebuttal and revision of the paper addressing all of my concerns. I have increased my score.\n\nSummary\nThis paper investigates an important direction: How can RL agents make use of high-level instructions and task decompositions formalized as programs? The authors propose a model for a program guided agent that, conditioned on a program, interprets the program, executes it to query a perception module and subsequently proposes subgoals to a low-level action module. The method outperforms LSTM and Transformer baselines on a Minecraft-like task and generalizes to programs larger than the one seen during training.\n\nStrengths\nContribution in the important direction of training RL agents with instructions and prior knowledge, here in the form of programs\nClearly written paper with good illustrations of the model\nGood performance on generalization task of acting in environments where the programmatic instructions are longer than those seen during training\n\nWeaknesses\nOne of the contributions of the paper is a modulation mechanism (Section 4.3) on the state features that incorporates a goal-conditioned policy. However, a very related approach has been proposed by Bahdanau, Dzmitry, et al. \"Learning to Understand Goal Specifications by Modelling Reward.\" ICLR 2019. They introduced FILM layers that modulate the layers in a ConvNet conditioned on a goal representation. This should be discussed and compared to in the paper.\nI am surprised there is no comparison to other work that conditions on programs or hierarchical RL approaches. For example, the authors mention various works in Section 2, but fail to compare to them or at least explain why a comparison would not be possible.\nAnother point of criticism is that the authors do not use an existing environment, but instead a Minecraft-inspired one similar to Andreas et al, Oh et al. and Sohn et al. This makes a comparison to prior work hard and I would like to understand in what way previous environments were inadequate for the research carried out here.\nOne aspect that I found most interesting in this paper is that the authors also let annotators map the given programs into natural language form. However, there is no discussion of these results. Similarly, there are interesting qualitative analyses in the appendix of the paper that I only stumbled upon by chance. I believe these should be referenced and a short summary should be integrated into the main part of the paper. I would particularly like to see a discussion of limitations already in the main part of the paper.\n\nMinor Comments\np1: I like the motivation of cooking recipes for work on program conditioned learning. There is in fact a paper (probably multiple) from the NLP community that I think could be cited here. The one that comes to my mind is: Malmaud, Jonathan, et al. \"Cooking with semantics.\" Proceedings of the ACL 2014 Workshop on Semantic Parsing. 2014.\np1: I agree with the argument that programs might be favored over natural language to specify goals as they are unambiguous. However, I think this can also be seen as a drawback. Natural language allows us to very efficiently share information, maybe sometimes information that is only disambiguated through observations in the environment. Another advantage is that natural language for instructing learning agents (like people) is abundant on the web, while programs are not.\np2: \"that leverages grammar\" -> \"that leverages a grammar\"\np2: \"we propose to utilize an precise\" -> \"we propose to utilize a precise\"\np2: For learning from video demonstrations, an important prior work is Aytar, Yusuf, et al. \"Playing hard exploration games by watching youtube.\" Advances in Neural Information Processing Systems. 2018.\np3: A deep learning program synthesis work prior to the ones mentioned here is Bošnjak, Matko, et al. \"Programming with a differentiable forth interpreter.\" Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.\np5: Would it make sense to also compare to a purely hand-crafted programmatic policy? I am missing a justification why learning is strictly necessary in the environment considered in this work.\np6 Section 4.4.1: I believe the explanation of the perception module would benefit from a concrete example.\nQuestions to Authors",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}