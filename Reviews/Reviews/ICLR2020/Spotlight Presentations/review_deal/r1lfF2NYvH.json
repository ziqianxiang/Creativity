{
    "Decision": {
        "decision": "Accept (Spotlight)",
        "comment": "This paper proposes a graph embedding method for the whole graph under both unsupervised and semi-supervised setting. It can extract a fixed length graph-level representation with good generalization capability. All reviewers provided unanimous rating of weak accept. The reviewers praise the paper is well written and is value to different fields dealing with graph learning. There are some discussions on the novelty of the approach, which was better clarified after the response from the authors. Overall this paper presents a new effort in the active topic of graph representation learning with potential large impact to multiple fields. Therefore, the ACs recommend it to be an oral paper.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The paper presents a new graph representation learning method for the whole graph under both unsupervised and semi-supervised setting. Different from existing ones using graph kernel, or graph2vec, the proposed InfoGraph is able to extract graph-level representation with fixed-length features that are generalized well. Basically, InfoGraph is parameterized by graph neural networks, but guided by mutual information loss. Experiments on both unsupervised and semi-supervised experiments on popular benchmarks demonstrate the effectiveness of InfoGraph and InfoGraph*\n\n*  The paper is well written and easy to follow, and the research problem is of great value in different fields.\n\n* In general, the novelty of this paper is ok, but itâ€™s partially based on Deep InfoMax (DIM) published recently. This may undermine the novelty of this paper somehow.\n\n* Authors change the fonts in equation from italic to non-italic in Eq. (6), please make sure to use one format throughout the paper.\n\n* Why Jensen-Shannon MI estimator is used in Eq. (5) instead of other estimators for MI, and any more explanations or motivations here?\n\n* Eq. (7) and (8) in facts jointly optimize between two networks \\phi and \\varphi, but little details about the optimization have been exposed. Also, we are not sure if the loss from the two models can converge finally. Better to show some qualitative results and analysis.\n\n* READOUT seems to play a critical role in building the global representation, however, it is unclear if other READOUT function will work well, and why the current one is feasible. Please explain.\n\n* In semi-supervised setting, it seems InfoGraph* is comparable to the SOTA MeanTeachers model on 12 targets. I am not sure if this can reflect the true performance of the proposed model. Maybe another dataset will be able to highlight the superiority.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "The paper presents an unsupervised method for graph embedding. The authors seek to obtain graph representations by maximizing the mutual information between graph-level and patch-level\nrepresentations. They also consider a semi-supervised task when the Mutual Information-based criterion has an additional term which quantifies a classification error, obtained when constructing a classifier based on the obtained graph representations. \n\nDespite having good experimental results, the proposed approach is rather a mix of previous works and hence not novel. \n\nIn particular, the main building block of the embedding algorithm, the target functional based on mutual information, was borrowed from Deep Graph Informax paper. The differences, listed by the authors, are only of technical nature. Advantage of using it for unlabeled data is poorly motivated: why we can learn smth useful when maximizing the mutual information between graph-level and patch-level representations obtained via GNN? What if patch-level representations are not sufficiently characteristic to have anything in common with the graph?\n\nThere is no discussion of [1], which uses CBOW framework, has theoretical properties, and produces good results in experiments. There is no comparison with GNN models such as [2]. \n\nMinor comments: please, correct fonts - they are different in formulas 6,7 and 5\n\nI would be more interested to see explanation of the obtained results for each particular dataset (e.g. why MUTAG has 89% accuracy and PTC 61%); what so different about dataset and whether we reached a limit on most commonly used datasets. \n\n[1] Anonymous Walk Embeddings? ICML 2018, Ivanov et. al. \n[2] How Powerful are Graph Neural Networks? ICLR 2019, Xu et. al.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "In this paper, the authors propose a graph-level representation, which extends the existing node-level representation learning mechanism. Besides, both unsupervised and semi-supervised learning are leveraged for InfoGraph and InfoGraph*, receptively. The authors naturally apply Deep Graph Infomax, a contrastive representation learning method, for the whole graph level instead of the previous node embedding learning. The experiments on graph classification and molecular property prediction indicate the effectiveness, even compared to the supervised methods.\n\nThe learned graph-level representation looks good to me. Instead of some heuristics based graph-level pooling, the proposed method automatically figure out the best way to produce the fixed-length feature vector in a data-driven approach. Such motivation is pretty reasonable and natural for me. Also, the usage of both unsupervised and semi-supervised learning procedure is well-motivated. Overall speaking, the paper is well written. The definitions of problems, the details of methods, and the settings of experiments are clear to me.\n\nI have some questions and suggestions for the authors:\n1. The overall writing looks good to me. Besides, It could be much better if the authors could provide more and better illustrations for the method. Both figures 1 and 2 are not that informative to me, to be honest. I know it could hard to visualize the graph-level representation, but it worth it. There are many steps and equations in the paper, and the illustration could play an essential role in putting all the steps together to demonstrate the big picture.\n\n2. The authors carefully discuss the difference between this submission and concurrent work (Information Maximizing Graph Network) or existing work (Deep Graph Infomax), which helps a lot for the reader to understand the literature better. It could be much better if this submission could be more self-contained. For example, for the semi-supervised learning setting, the authors let the reader read some external papers. I am suggesting that a small section in the appendix could make life much more comfortable."
        }
    ]
}