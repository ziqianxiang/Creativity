{
    "Decision": {
        "decision": "Accept (Spotlight)",
        "comment": "The paper introduces the concept of overfitting in meta learning and proposes some solutions to address this problem. Overall, this is a good paper. It would be good if the authors could relate this work to meta learning approaches, which are based on hierarchical (Bayesian) modeling for learning a task embedding.\n\n[1] Hausman et al. (ICLR 2018): Learning an Embedding Space for Transferable Robot Skills \nhttps://openreview.net/pdf?id=rk07ZXZRb\n[2] Saemundsson et al. (UAI 2018): Meta Reinforcement Learning with Latent Variable Gaussian Processes\nhttp://auai.org/uai2018/proceedings/papers/235.pdf\n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #2",
            "review": "This paper analyses a pitfall of current meta-learning algorithms, where the task can be inferred from the meta-training data alone, leaving the task-training data unused. Such a meta-learner would generalise well on the meta-training tasks, but will fail to generalise on new tasks at test time. This kind of overfitting is formalised as the memorization problem. This problem is implicitly resolved in current meta-learning algorithms by constructing mutually-exclusive meta-training tasks, which is not easy to construct in all scenarios. The paper introduces an information-theoretic meta-regularizer which forces information extraction from the task data (D) by restricting information flow from meta-parameters (\\theta) and input (x^*). Experimental evaluation with one gradient based and one contextual meta-learning method, on non-mutually-exclusive tasks bring out the mettle of the proposed regulariser. \n\n+ves:\n+ The characterization of the memorization problem and the proposed regularizer are novel contributions. \n+ The paper motivates the problem well, before proposing the methodology.\n+ The paper is well-organised and the experimental setting is designed in a thoughtful manner.\n+ The results are promising.\n\nConcerns:\n- The key hypothesis that the proposed meta-regularization method is based on - is that a model with memorization tends to be more complex. What is the basis for such an assumption? This is an important one for the work at the outset.\n\n- Would the proposed regularizer help if mutually-exclusive meta-training tasks are available, as it forces the model to extract maximum information from task training data (D)? The paper does not comment on this, and this would have been useful to know.\n\n- How much is the training overhead (in terms of time) incurred while adding the regularizer to the baseline methods (MAML and CNP)? The paper does not talk about this additional complexity.\n\n- Evidently, the most important results in the Experiments section are the ones in Sec 6.3. However, the results do not clearly distinguish whether the meta-regularization was performed on the activations or weights here (earlier subsections do talk about this). This makes it difficult to make a conclusive inference on what aspect of the methodology actually helped here.\n\n- There have been recent efforts that have attempted addressing overfitting in meta-learning. The paper mentions these efforts in Sec 5, and states that these have been used for existing settings where tasks are mutually exclusive. It would have been useful to include at least one of these methods in the experiments to see how the proposed regularization differs from them in practice.\n\nMinor issues:\n- The abstract says: “This causes the meta-learner to decide what should be learned from data and what must be inferred from the input.” - what is the difference between data and input?\n- There are some minor typos in the work, which would benefit from a proofread. E.g: Sec 6.3 “neigbhor” -> “neighbor”\n\n===== POST-REBUTTAL COMMENTS =========\nI thank the authors for the response, the clarifications and the updated manuscript. I am happy to upgrade my rating to Accept. \n\nThe concerns regarding the ‘higher complexity of memorized models’ has been addressed convincingly in the narrative, and the visualization of weights for models with and without using the proposed regularizer makes the argument cogent. \n\nMentioning the number of  gradient steps used to obtain the results on mutually-exclusive meta-training tasks (Figure 9) in the narrative would help. Was the same number of steps used for MAML and MR-MAML experiments? The optimal \\beta value would be very important if we want to use MR-MAML in situations where mutual-exclusivity of tasks is not known a-priori (as alluded to in the rebuttal). \n\nI would encourage the authors to include training overhead (in terms of time) in the paper, even if it is minimal, as it would clear concerns of the reader. I would also highly encourage the authors to release the code as this would help easy reproducibility of the results. \n\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #3",
            "review": "\nSummary:\n\nIn this paper, the authors propose a new method to alleviate the effect of meta over-fitting. The designed method is based on the information-theoretic meta-regularization objective. Experiments demonstrate the effectiveness of the proposed model.\n\nStrong Points:\n\n+ The authors aim to alleviate the effect of meta over-fitting. In this paper, they mainly focus on alleviating the effect of brute-force memorization in the meta-training process. The problem is important in the meta-learning field.\n\n+ The motivation for this paper is clear. The authors try to maximize the mutual information between x*, \\theta and \\bar{y}^*, D. \n\n+ Experiments on both sinusoid regression, pose regression and image classification show that MR-MAML outperforms MAML and MR-CNP outperforms CNP. \n\nWeak Points:\n\n- My first concern is about the novelty of the proposed model. The framework and the derivations are straightforward. I think the problem is very important, however, the technical contribution may not enough to be accepted. It is better for the authors to clarify their contributions.\n\n- It will be more helpful if the authors can describe the algorithm of the meta-testing process in Appendix A.1. In the meta-testing process, do we need to sample \\theta from q(\\theta|\\tau)? If so, is the accuracy calculated by the averaged value of tasks with sampled weight?\n\n- I am a little curious about the results in Table 5. The results of MAML and TAML is quite high. It would be better if the authors explain more.\n\nAfter rebuttal\nI think the authors' response and the revised paper address most of my concerns. I raise my score to 6.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "################################################################################\nSummary:\n\nThis paper illustrates, identifies, and formally defines a memorization problem in meta-learning -- the model can simply memorize meta-training tasks and ignore meta-training train sets. The paper proposes to optimize the mutual information between testing predictions and the training data (given input and meta model), and upper bound it by imposing a information bottleneck between output and input+model. Unlike related work, this paper specifically is able to generalize to meta-test even when the meta-train dataset is not made confusing enough (i.e. even when model can learn well from test data in meta-train alone), making it applicable to use cases where it is hard to make the dataset confusing.\n\n################################################################################\nDecision\n\nI vote for accepting this paper, since as far as I know this paper gives a novel insight to the overfitting problem in meta learning, and has formulated the problem formally with theoretical insight, and given a working solution with strong experiment results and clean comparative studies. Despite somewhat narrow experiments and sometimes confusing writing, the paper should provide new insight to meta-learning.\n\n################################################################################\nPros:\n! DISCLAIMER: I am not an expert in this field, so take my novelty judgements with a grain of salt.\n+ Novel view into meta-learning's overfitting problem\n(1) Large models can simply memorize which input data corresponds to which task, and memorize the meta-training tasks, without being able to generalize into meta-test tasks.\n(2) Formulates this into a low mutual information between meta-train training data and predictions.\n+ Easy to implement and quite widely applicable as a regularization loss addon to multiple existing meta-learning methods\n+ Impact-wise, the paper takes meta-learning further from memorization, making methods more capable of operating on less-carefully designed, more natural datasets (rather than permutation of datasets)\n+ Experiments are clean with ablation studies and hyperparameter sensitivity tests, and method performs well across real and toy datasets\n+ Motivation part is easy to read\n\n################################################################################\nCons:\n- I'm still skeptical of the novelty of the paper, since the conclusions are, in hindsight, very straightforward.\n- Sometimes sentences are very confusing to readers.\n(1) The term \"mutually-exclusive\" is confusing because the view-point example the paper gives seems to be mutually exclusive (each task has its own kind of data, hence \"exclusive\"). It is unclear whether the task data is exclusive, or the task function is exclusive, and not straightforward to see its relationship with memorization. Consider renaming it to e.g. \"mutually-confusing\" or \"mutually-contradictory mapping\".\n(2) Can you please rename \"information-theoretic meta-regularizer\" to \"meta-regularizer using information theory\"? It is hard to read for non-native speakers.\n(3) Paragraph under definition 1 is confusing and has redundancies.\n(4) Section 4.1, not very clear how the logic goes from the decomposition to adding the upper bound to the loss, and how the other term comes in.\n- For the motivation, it is better to give examples of use cases when it is impossible to make meta-train \"mutually-exclusive\". I'm sure even in the patient example you can shuffle classes or input dimensions.\n- An experiment comparing to others in mutually-exclusive datasets would be nice to have, in order to judge how much this compensates a badly-designed meta-learning dataset.\n\n################################################################################\nImprovements:\n- Clarify each point in the \"Cons\" section.\n- Please also clarify if all methods in all experiments are hyperparameter-tuned separately, i.e. that the experiments are not favoring the MR-* model in any way. (the paper only clarifies it in one of the experiments)\n- For future work, does recent developments in mutual information modeling (e.g. MINE https://arxiv.org/abs/1801.04062) help this method in any way? e.g. try increasing mutual information between some representation of the meta-train training data and some feature vectors before the prediction?\n- First parenthesis in Section 4.1 has a misplaced space\n\n\n\n################################################################################\nPost rebuttal\n################################################################################\nIt seems that reviewers agree that the contributions are novel (regardless of whether each reviewer thinks it is trivial). So that addresses my main concern. I think the contributions are novel enough since it gives theoretical guidance as well. Other concerns are mostly addressed by the rebuttal. I will keep my rating.\n\nAlthough I do urge the authors to reconsider the name choice \"mutual-exclusive tasks\" since it is not very informative and quite confusing to readers.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}