{
    "Decision": {
        "decision": "Accept (Spotlight)",
        "comment": "This paper provides a new algorithm for learning fair representation for two different fairness criteria--accuracy parity and equalized odds. The reviewers agree that the paper provides novel techniques, although the experiments may appear to be a bit weak. Overall, this paper gives new contributions to the fair representation learning literature.\n\nThe authors should consider citing and discussing the relationship with the following work:\nA Reductions Approach to Fair Classification., ICML 2018",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #3",
            "review": "Update:\n\nThanks for providing additional results on the Adult dataset. I have increased my score. However I'd be nice to also see the balanced accuracy (i.e. sum of TPRs for each class divided by 2) results and compare to baseline trained with oversampling or re-weighted loss.\n\nI would suggest authors to add more extensive comparisons to other methods using Adult dataset. There are quite a lot of papers in the fairness literature that experiment with the Adult dataset. They focus on different metrics and there is probably no method that is uniformly the best. Your paper demonstrated that your approach can succeed in achieving accuracy parity, but it would be good to also show tradeoffs with other metrics (in addition to DP). I was able to do some back-of-the-envelop calculations and your results seem fine, but a clear comparison would be good.\n\nBelow are some examples of the papers studying Adult dataset from the fairness angle:\n[1] Mitigating unwanted biases with adversarial learning. Zhang et al., 2018. (already cited, but no comparison)\n[2] Whatâ€™s in a Name? Reducing Bias in Bios without Access to Protected Attributes. Romanov et al., 2019.\n[3] Learning fair predictors with Sensitive Subspace Robustness. Yurochkin et al., 2019.\n\n\n---------------------------------------------------------------------------------------------------------------------------------------------------------\n\nThis paper proposes an adversarial representation learning approach. The key difference with prior work is that the objective function is built around balanced error rates, one for classes, that is eventually used for classification, and two adversarial for predicting each of the protected attributes. Authors argue that proposed approach can simultaneously achieve accuracy parity and equalized odds.\n\nThe notion of accuracy parity does not seem to be very meaningful. For example, predicting uniformly at random seems like an intuitively fair classifier with EO gap 0 and DP gap 0. However it will not necessarily have error gap of 0 (i.e. satisfy accuracy parity), making me wonder if the notion of accuracy parity makes much sense.\n\nI am not really sure what is the Err0 + Err1 metric used in Figures 1 and 2. Is it not normalized and can vary between 0 and 2? In which case it seems counterintuitive for performance quantification. If it is normalized, then results on COMPAS do not make sense. Err0 + Err1 of all methods is above 60%, which is worse than predicting uniformly at random.\n\nPlease report your TPRs for classes grouped by protected attribute when reporting the results in the context of group fairness. Further for Adult dataset, reporting balanced TPR as a measure of accuracy seems to make more sense given the class imbalance.\n\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary: Authors extend on work that attempts to learn fair data representation (features) and propose an algorithm (which is a modification of a loss essentially) and show that it allows to achieve accuracy and equalized odds parity, and show that while achieving equalized odds they don't hurt demographic parity. The experiments demonstrate utility of the algorithm for balanced datasets (without sacrificing performance to fairness)\n\nDisclaimer: I am completely out of this area\nBut it is an easy read and an interesting angle. Authors show that simple modification of the loss makes it more fair (in a sense).  The experiments are somewhat thin.\n\nMin max problem in Section 3 - I think it requires more intro for people outside. I assume you use an architecture that extracts the representation and has two head (outputs) - h and h'. For a modified loss, do you have 3 outputs - h, h' and h''? A pic with an architecture would be really helpful\n\nProbably more datasets are required to have a more convincing empirical story\n\nSection 3.4: Even though you can't directly optimize for BER, there are ways that can work, instead of just replacing it with CE, for example this https://arxiv.org/pdf/1608.04802.pdf\n\nOne critique is based on 3.4 I don't understand  how can this be extended to multiple axis of intersecting groups- e.g. not just mutually exclusive race values, but also gender for example. "
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #2",
            "review": "This paper focuses on learning representations which can simultaneously achieve equalized odds and accuracy parity without impacting demographic parity. The authors show both theoretically and empirically that the proposed algorithm show better utility-fairness tradeoff on balanced datasets. This is indeed a useful result. Overall, I liked the presentation of the paper including motivation and background of other methods. Theory is sound, and experiments are sufficient.   Therefore, I do not have any major concern. Some of minor concerns are mentioned below:\n\n1. I see some ambiguity in the definition of the classifiers. h is defined to be deterministic in section 2; whereas, later \\hat{Y}, which I believe is treated at h(g(x)) for some h, is taken to be randomized. Also, the theorems are mentioned with respect to h making them specific to deterministic classifiers according to the current definition.\n\n2. I am not sure why the last statement of the second last paragraph on page 2 is true. Can you please explain? There should be an additional condition on distribution D, which is not clear at that moment in the paper.\n\n3. I am wondering if the utility can be maintained for imbalanced datasets by taking two parameters \\lambda_1 and \\lambda_2 for BER_{D^0} and BER_{D^1} in equation 2. Did the authors check when we have different regularization parameters for both terms? If yes, then what was the conclusion?\n\n4. Please write theorems as fully independent statements. \"Assume the conditions in Proposition 3.2\" is probably not the right way to start a theorem. Other way is to mention the conditions separately and then use it throughout the paper.\n\n5. Can you please write or elaborate the final optimization problem after section 3.4?\n\n6. How did the authors construct the optimal classifiers in the experiments for real data? Can you provide some details?\n\nTypo: \"sensitive attribute A, then the second term\" --> remove then\n\n----  After Rebuttal ---\n\nI thank the authors for providing response to my questions. At this point, I am going to keep the same score.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}