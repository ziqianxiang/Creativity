{
    "Decision": {
        "decision": "Accept (Spotlight)",
        "comment": "This paper presents an idea for interpolating between two points in the decision-space of a black-box classifier in the image-space, while producing plausible images along the interpolation path. The presentation is clear and the experiments support the premise of the model.\nWhile the proposed technique can be used to help understanding how a classifier works, I have strong reservations in calling the generated samples \"explanations\". In particular, there is no reason for the true explanation of how the classifier works to lie in the manifold of plausible images. This constraint is more of a feature to please humans rather than to explain the geometry of the decision boundary.\nI believe this paper will be well-received and I suggested acceptance, but I believe it will be of limited usefulness for robust understanding of the decision boundary of classifiers.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "The paper presents a method for explaining the output of black box classification of images.  The method generates  gradual perturbation of outputs in response to gradually perturbed input queries. The rationale is that, by looking at these, humans can interpret the classification mechanics.\n\nThe presentation is clear. The coverage of prior work is sufficient (although references should point to the published work instead of arxiv entries, when the former is available).\n\nOne question that is not addressed is how efficient is this method, in terms of computational cost. This is a method that increases the amount of input data (through perturbation). What is the minimum amount of input data that needs to be perturbed in this way, before the method can become human interpretable? \n\nAlso, ideally any work on human interpretability of ML should be evaluated on humans. If not, it is an approximation, and it should be presented and reasoned as such (with a discussion of limitations and caveats, for instance)."
        },
        {
            "rating": "8: Accept",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Here are the claims I could find in the intro:\n\"Given a query input to a black-box, we aim at explaining the outcome by providing plausible and progressive variations to the query that can result in a change to the output\"\n > This is well supported as the model generates these and it is very reasonable that it can.\n\"the counterfactually generated samples are realistic-looking\"\n> The images seem to support this.\n\"the method can be used to detect bias in training of the predictor\"\n> Section 4.4 makes it really clear that, at least in the described setting, it works.\n\nI think the idea could be presented in a better way. The general concept of exaggerating a feature that represented a class seems novel and exciting. Just based on the novelty of that alone I think this is worth accepting. I would imagine there would be a cleaner way of achieving all this but maybe it is all necessary.\n\nI don't understand Figure 1a. I don't think this helps to illustrate the point. M_z seems to just be a bottleneck but the writing makes it seem like it is more.\n\nSection 4.2 is a bit hard to read. It is not clear for me what is the goal of this section.\n\nSection 4.4 seems very similar to the idea in this work https://arxiv.org/abs/1805.08841 which studied how bias in CycleGANs can be seen when you vary the bias which I think should be cited here.\n\nTypos:\n\"our application of interested\"\n"
        }
    ]
}