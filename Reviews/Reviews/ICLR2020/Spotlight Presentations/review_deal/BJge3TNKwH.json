{
    "Decision": {
        "decision": "Accept (Spotlight)",
        "comment": "The paper addresses an important problem (preventing catastrophic forgetting in continual learning) through a novel approach based on the sliced Kramer distance. The paper provides a novel and interesting conceptual contribution and is well written. Experiments could have been more extensive but this is very nice work and deserves publication.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "[Summary]\nThis paper proposes a new method for overcoming catastrophic forgetting in continual learning, based on distribution-based regularization using the sliced Cramer distance, i.e. Sliced Cramer Preservation (SCP). Unlike previous work on catastrophic forgetting, this paper tackles unsupervised learning scenarios as well as supervised learning. They evaluate the proposed SCP on permutated MNIST, sequential learning in autoencoder task, and sequential learning for segmentation. \n\n[Pros]\n- This paper tackles unsupervised learning scenarios beyond the classification on benchmark datasets.\n- This paper employes sliced Cramer distance with theoretical justification.\n- The analysis on EwC and MAS in terms of geometric view\n- Experimental results look promising.\n\n[Cons]  \n- Even if MAS[1] describes the details on synaptic concept and Hebbian rule, many readers might be not familiar with synaptic or neuro-science terms. So, in prelimiary session, more explanation can help readers to understand.\n- In addition to permute-MNIST, it is required to be evaluated on more conventional tasks such as MNIST->SVHN or CIFAR-10, 100 datasets. Also, more recent work should be compared such as IMM [2] and PGMA [3] for supervised learning scenarios.\n- All graph result figures can be improved for enhancing legibility. In Figure 6, In particular, the subtitle of Summer case confuses me.\n \n[1] Aljundi et al. Memory Aware Synapses: Learning what (not) to forget, ECCV 2018.\n[2] Lee et al. Overcoming Catastrophic Forgetting by Incremental Moment Matching, NIPS 2017.\n[3] Hu et al. Overcoming Catastrophic Forgetting for Continual Learning via Model Adaptation, ICLR 2019."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "I think the paper is written quite well, and the approach makes a lot of sense. I think the idea of replacing generalizing the KL to sliced Cramer distance is quite interested. The authors put in some effort in explaining why they propose this alternative distance between distributions. \n\nI think overall this is a great paper, very informative. If I need to nitpick, I think the experimental section relies heavily on MNIST (e.g. permuted MNIST, auto-encoding MNIST), which I think is not a hard enough task. It is though widely used in the continual learning community, though maybe it should not anymore. But I think given the reliance of the field on these datasets it makes sense, plus I think the strength of the paper is not in the empirical evaluation but rather in the derivation of the method. \n\nI think while the authors put quite a bit of effort in explaining the difference between KL and Cramer distance, I would have appreciated a even more detailed exposition. I think the difference between these metrics is not well understood by the majority in the community. "
        }
    ]
}