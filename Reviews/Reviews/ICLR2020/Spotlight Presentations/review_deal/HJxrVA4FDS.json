{
    "Decision": {
        "decision": "Accept (Spotlight)",
        "comment": "All the reviewers recommend acceptance. The reviews found the paper to be interesting with substantial insights. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This is an interesting paper. It seeks to disentangle the need for top-down and horizontal connections for grouping tasks using (a) a new synthetic dataset that seeks to evaluate one over the other, and (b) a new recurrent neural network model.\n\n\nI think this is an interesting scientific question that is worth answering. It is especially useful given the context of what we know about the visual cortex in the brain, which is the presence of a large number of horizontal and top-down connections.\nI think this paper takes a good first step in understanding this. I liked the problem setup and the approach of looking at accuracy at a given sample complexity rather than accuracy alone. I also liked the fact that the authors used both deeper networks and state-of-the-art baselines and corrected for the parameter count. In these respects the paper is novel and thought-provoking.\n\nI would like the evaluation to be stronger, however. I would like to see the following experiments: \n(1) How do variants of the architecture (deeper/shallower) perform under the same settings? Do the conclusions change with network depth?\n(2) How do these results generalize to real-world segmentation datasets? Are both top-down and horizontal connections needed for e.g., PASCAL/COCO?\n\nSince this is not a vision conference, I am giving this a weak accept, but I would really like to see at least an evaluation of (2)."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "This paper proposed a dataset and designed a relevant network structure to analyze the function of horizontal and top-down connections for perceptual grouping. The used two datasets smartly isolate the requirements for exploiting Gestalt cues and object-based strategies. Appendix A detailed describes the cABC dataset, and the control experiments in Appendix B further validate the designing of the cABC. The proposed network flexibly integrates three types of connections and successfully solves both two challenges. The visualization results in Figure 4, S8 and S9 are insightful and also validate the intuitions. Overall the paper is clearly written and easy to follow.\n\n\n\nAlthough the use of different types of connections in the proposed network is clear, I think the author should conduct more analysis for the standard networks and datasets.\n\nIn section 3, the author claims that the two challenges will cause a high computational burden for feedforward models like ResNets. However, in Figure 5, the results show that ResNet-152 and ResNet-50 could easily solve the Pathfinder. For cABC, ResNet-18 could solve and ResNet-50 and ResNet-152 will fail while the author illustrates this is due to ResNet-50 and ResNet-152 overfit to cABC. Those phenomena are quite misaligning with the arguments. \n\n- Can we find a suitable ResNet structure that could solve both two challenges? \n- Can we increase the depth of U-Net and make it solve both two challenges? \n- Can we add more data or increase the difficulty for cABC and prevent the overfitting for ResNet-50 and ResNet-152? \n- Can we keep increasing the difficulties of cABC and straining ResNet-18 and UNet? Does that difficulty level would also strain TD+H-CNN?  \n- We could also perform similar experiments on Pathfinder that increasing its difficulty for ResNet-50 and ResNet-152. \n\nAlso, if we train the TD+H-CNN on standard image datasets such as cifar-10 or ImageNet, what about the recurrent activities and the final acc?\n\n=========================================================\nAfter Rebuttal:\n\nI thank the author for the response. \n\nI hope the comments are useful for preparing a future version of this work when you have enough time.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "rating": "8: Accept",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The article tries to examine existing hypotheses from the neuroscience and perception literature by using neural networks as a computational model of the brain. Namely, the authors assess the efficiency of different strategies for solving two visual challenges, one of which is novel. The authors also evaluate the level of consistency between the performance of humans and different types of neural architectures.\n\nI believe that the quality of this work is above the acceptance threshold. The results seem to clearly support the claims. The experiments are well-designed and an adequate number of baselines are provided. However, it should be mentioned that the conclusions are by no means surprising.\n\nThe following sentence should probably be fixed:\n> \".. models that not learn overfit the training set.\"\n\nSome questions (answering is optional):\n -  In the second paragraph of the introduction, the authors state that the two feedback mechanisms should be iterative. Can the authors provide elaborate as to why these strategies should be inherently iterative and simply applying the same model a small/finite amount of times is not enough?\n - The authors claim that the relatively low performance of ResNet-18 and U-Net on Pathfinder is due to a higher computational burden, yet the reason for the poor performance of ResNet-50 and ResNet-152  on cABC is the result of overfitting. Is there any evidence to support this distinction or are the authors simply arguing this because it is the most plausible explanation?"
        }
    ]
}