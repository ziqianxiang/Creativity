{
    "Decision": {
        "decision": "Accept (Spotlight)",
        "comment": "The paper extends LISTA by introducing gain gates and overshoot gates, which respectively address underestimation of code components and compensation of small step size of LISTA. The authors theoretically analyze these extensions and backup the effectiveness of their proposed algorithm with encouraging empirical results. All reviewers are highly positive on the contributions of this paper, and appreciate the rigorous theory which is further supported by convincing experiments. All three reviewers recommended accept.\n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "8: Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "This paper proposed two novel gates to improve the convergence speed of the learned ISTA (LISTA) algorithm for sparse coding. The first gate is designed to address the problem that the output of the LISTA algorithm usually has a lower magnitude compared against the ground-truth. To address this, the paper proposed a gain gate to increase the magnitude of a layer in the LISTA algorithm. The second gate is designed to further improve the convergence with a technique similar to the momentum but with a time-varying coefficient. \n\nThis paper provides rigorous theoretical justifications for the observations that motivate the two gates in two propositions. It also provides theoretical guarantees for the first gain gate under practical assumptions. \n\nThorough synthetic experiments are conducted to empirically validate the proposals as well as the theorems. The effectiveness of the gates is also proved in a real-world computer vision task, photometric stereo. \n\nGiven the thoroughness of the theoretical and experimental justifications, I strongly recommend the acceptance of the paper.\n\nDespite the strength of the paper, I have the following suggestions and questions regarding the two gates proposed by the paper:\n\n1. The second proposed gate, the overshoot gate, lacks theoretical justification and is not as well studied as the gain gate. The experiments for this gate are not included in the main text but got placed in the appendix. I suggest squeezing some of them into the main text to show a complete picture about the overshoot gate. If there are more theoretical results about the behavior of the overshoot gate, the author should include it in the paper; otherwise, the author should clearly states this limitation in the main text of the paper.\n\n2. The author proposed two gates to address two different issues of the original LISTA algorithm. However, the two gates are only applied seperately to modify the LISTA algorithm; they have not been integrated into a single concise algorithm. It would strengthen the paper if the author could provide a further discussion on how the two gates can be combined to address the two issues in one algorithm.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #1",
            "review": "Summary:\n\nThis paper is focused on solving sparse coding problems using LISTA-type\nnetworks. It discusses the weakness of the ``no false positive'' assumption in\nprevious works and the weakness results in underestimated code components. The\nauthors propose a ``gain gating function'' to mitigate the weakness. Moreover,\nthe paper incorporates another ``overshoot'' gating function inspired by\nmomentum-based methods. Both contributions are supported with theoretical and\nempirical results. Numerical experiments show that the proposed model is\nsuperior to previous works especially in cases with high measurement noises or\nill-composed basis matrix. The paper is well written and easy to follow, and the\nempirical results are impressive.\n\nI really like the relaxation of ``no false positive'' assumption as in real world\napplication, learning-based algorithms may find it difficult to keep satisfying\nthe assumption while maintaining good empirical performance. And this relaxation\ncontributes to guarantee the convergence in a more real scenario.\n\nI think in (Chen et al., 2018), the support selection technique serves to\nmitigate the problem of underestimated code components as it bypasses the\nthresholding function for codes with large magnitudes. The empirical results\nalso show that in many cases LISTA with support selection has comparable\nperformance. I don't know if LISTA with support selection also suffers from\nthis underestimation problem severely.\n\nQuestions:\n\n1. The theorems in Section 3.1 hold without overshooting mechanism, i.e. eta==1.\nProp. 2 says that for one step of iteration, the optimal updates requires\novershoot. When the gain and overshoot mechanisms are combined, however, will\nthe theoretical convergence still hold?\n\n2. I don't have a very good understanding of why GLISTA performs so well given\nill-composed basis matrix compared to previous works. Could there be some\n(intuitive) explanation as to this based on the theoretical results?\n\n3. More details about the training process should be included. For example, I\ncan guess the loss function used for the training. But it is confusing not\nmentioning it at all in the paper. Some might think the objective funciton in\neqn (2) but the convergence analysis is with respect to the ground truth sparse\nvector. Also, the training scheme and some hyperparameter selection should also\nbe stated at least in Appendix.\n\n\nOverall I hold very positive attitude towards this paper due to its theoretical\ncontributions and good empirical results.\n\n================================\nUpdate: the authors addressed my questions well and I will keep my positive decision on this paper.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #2",
            "review": "1. Summary\nThe authors propose extensions to LISTA with the goal of addressing underestimation (by introducing “gain gates”) and including momentum (by introducing “overshoot gates”). The authors provide theoretical analysis for each step of their LISTA augmentations, showing that it improves convergence rate. Their theoretical statements are empirically validated and then a numerical comparison is performed between the most interesting LISTA variants. Their proposed GLISTA performs favorably, especially for networks of depth greater than 10.\n2. Decision and arguments\nThe theory given is quite comprehensive and seems solid. Moreover it’s motivated by and helps real problems, namely L1’s well-known underestimation and LISTA’s lack of momentum. On top of that, the numerical results not only show your GLISTA outperforming others, but that by adding gates to other LISTA variants you can get better results. On the other hand (see details in the Questions section), there is some critical information missing from the empirical section which makes the results non-repeatable. \nTraining is not described, i.e. hyperparameter searches and stopping criteria. A couple of the plots validating theory are difficult to understand. The datasets used are synthesized and tiny, only 1000 samples (training/testing/validation sets are not described) and 250x500 dimensions. That means your network has many more parameters than data samples unless I am mistaken.\nSo I have given weak reject because the paper has strong theory but weak experiments making it hard to trust the conclusions. I really want to hear back about the questions raised below.\n3. Questions \na) Just before eqn 12 you say that gains greater than 1 can be more appropriate. Am I understanding this correctly: *before* shrinkage you want to apply a gain on code elements that are “truly” nonzero, in order to cancel out the imminent L1 penalization? And you will learn to predict those code elements via parameters Lambda?\nb) Just before Section 4, you describe the difference between yours and Moreau & Bruna’s momentum taps. If I understand correctly they have a matrix called W_m^(k) which is multiplied onto the previous iterate—but for each layer / time unit (k), the matrix may vary by learning (note that in the main body of their paper they don’t explicitly use ^(k) notation, but it is explained to be the same as the other LISTA parameter matrices which vary with time, and is made explicit in the appendix). So I think their momentum is in fact time-varying. Moreover it is certainly dependent on the previous iterate, using the function f(z^(k-1)) = W_m^(k) * z^(k-1). Did you mean something else by that? In what sense do you have a higher capacity—do you have more parameters?\nc) If I understand correctly, in Figure 5a, 5b, for each point you have trained independently an N-layer GLISTA. Then you observe properties of your learned parameters.\nFor 5a: how do you compute the “output” of the gain gates? What is the input that gives this output?\nFor 5b: which ‘t’ is used to calculate ||W(t)D-I+U(t)A||? Or did you just train a single 15-layer GLISTA, and the x-axis is just ‘t’?\nd) Could you tell us more about the hyperparameter tests for every method? How do we know it was a fair comparison? There are no error bars, but from experience I know that training LISTA type networks can be a pain. What algorithms did you use? Stopping criteria? Some plots start at zero layers (4c, 5b, 6a-c) and some start at 1 layer… \ne) Interesting that in your real-data example, the sparse vector e has more non-zeros than zeros, is that really sparse? What dictionary A did you use, then? By your own description this task does not fit the sparse coding model you have analyzed…. Am I misunderstanding?\n4. Additional feedback/ minor comments\na) Add reference for DOA estimation application\nb) You should put something like “we prove this in the supplement” for props and theorems. Otherwise to readers less familiar with the literature it will seem like you forgot to put a reference.\nc) It seems unnecessary to put ||epsilon||<= 0 , instead of epsilon=0.\nd) Basically I think you should explain what your gate is before providing proofs about it. It would be useful to see a plot of the function g and/or kappa so provide some intuition about what you are doing to the architecture (nonlinear? Linear? Threshold? Etc.). Capital “Lambda” is not defined until after theorems are provided about the functions g, kappa, f. In fact it seems like kappa is only useful for the sake of proofs. The first time through, I over-read the statement “All the learnable parameters are thus collected as Lambda…” just before Section 3.1.1 because I was thinking “Why isn’t Lambda in the definitions of f/g/kappa??”. Anyway it makes perfect sense when you define it clearly as after Eqn 18.\ne) At the beginning of Section 3.2 you have a clause “the over-shoot gates act more like on the output”, which doesn’t make sense.\nf) Figures should be approximately self-explanatory—but “Ratio of lower components” is not explained in the caption of Fig 4b. Although I greatly appreciate that you have made the actual plot lines/markers very easy to see! Should these results be averaged over many training attempts with error bars?\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}