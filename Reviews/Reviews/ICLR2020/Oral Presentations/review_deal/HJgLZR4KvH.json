{
    "Decision": {
        "decision": "Accept (Talk)",
        "comment": "This is a very interesting paper on unsupervised skill learning based on the predictability of skill effects, with the incorporation of these ideas into model-based RL.\n\nThis is a clear accept, based on the clarity of the ideas presented and the writing, as well as the thorough and convincing experiments.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper introduces an unsupervised learning algorithm Dynamics-Aware Discovery of Skills (DADS) for learning low-level “skills” that can be leveraged for model-predictive control. The skills are learned by maximizing the mutual information between the next state s’ and the current skill z conditioned on the current state s. Maximizing this objective corresponds to maximizing the diversity of transitions produced in the environment, while making the skill z be informative about the next state s’. The idea is that using this objective leads to learning a diverse set of skills that are predictive of the environment. The skills z correspond to a set of action sequences, which are represented by a distribution \\pi(a|s,z). Because the above objective is intractable to compute (because it relies on the true dynamics p(s’|s,a)), it is variationally lower bounded using the approximate dynamics q_{\\phi}(s’|s,z), which represents the transition dynamics when using a certain skill and this variational lower bound is optimized to produce the optimal q_{\\phi}(s’|s,z) and \\pi(a|s,z).\n\nIn the second phase, model predictive control is used to do planning for a new test environment where we have access to the reward function. This corresponds to simulating multiple trajectories using the learned transition dynamics and skill function, computing the reward of each trajectory according to the reward function, executing the first action of the most optimal trajectory and repeating. It is mentioned that planning is done in the latent skill space, which enables easier longer-horizon planning.\n\nExperiments are performed to show that: (1) the learned skills exhibit low-variance behavior (which means that the skills have predictable behavior when used for model predictive control); (2) Model predictive control performs favorably compared to other relevant baselines.\n\nOverall, I feel this is a very well-motivated and interesting submission with very thorough experiments."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors try to incorporate intermediate-level skills into model-based RL, which is an essential problem in the RL field. The algorithm works well even in the case of high-dimensional state and action spaces. Their contributions are in four aspects:\n(1)they propose an unsupervised RL framework for learning intermediate-level representations, i.e. skills, based on maximizing the mutual information between the future state and current skill given the current state. This procedure is well-motivated and the mathematics is easy to follow.\n(2)they reformulate model predictive control (MPC) in the latent skill space.\n(3)their method is compatible with the idea of continuous skill spaces, which seems to give rise to more diverse trajectories and hence offers greater utility.\n(4)their method yields low-variance behavior while maintaining enough diversity.\n\nIt’s an accept for me. On one hand, using model-free unsupervised RL methods to learn intermediate-level skills is not a new idea. on the other, approaching this problem via mutual information is, as far as I know, new to this field. Although, the novelty of this approach remains undecided, this method seems to work well enough compared to model-based, model-free and hierarchical RL methods. Their analysis from the perspectives of continuous skill space and skill variance also seem to hold.\n\nNevertheless, the study would benefit from more comparison with other methods using intermediate-level primitives (apart from DIAYN). Moreover it would be interesting to show this method works in scenarios apart from locomotion. I wonder how well the approximation p(z|s) \\approx p(z) works in non-locomotion tasks. Otherwise, the authors should mention this method is somewhat limited to locomotion tasks in the main text. \n\nOthers:\nTypo:\n1.Page 3: “maximally informative about about …” remove the redundant “about”;\n2.Page 8, first line in section 6.3 “is to be enable use of planning algorithms…” may be changed to “is to take advantage of planning algorithms”."
        },
        {
            "rating": "8: Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper proposes a novel approach to learn a continuous set of skills (where a skill is associated with a latent vector and the skill policy network takes that vector as an extra input) by pure unsupervised exploration using as intrinsic reward a proxy for the mutual information  between next states and the skill (given the previous state). These skills can be used in a model-based planning (model-predictive control) with zero 'supervised' training data (for which the rewards are given), but using calls to the reward function to evaluate candidate sequences of skills and actions. The proposed approach is convincingly compared in several ways to both previous model-based approaches and model-free approaches.\n\nThis is a very interesting approach, and although I can already think of ways to improve, it seems like an exciting step in the right direction to develop more autonomous and sample efficient learning systems. I suggest to rate this submission as 'accept'.\n\nRegarding the comparison to model-free RL: although it is true that no task-specific training is needed, a possibly large number of calls to the reward functions are needed during planning. It would be good to compare those numbers with the number of rewarded trajectories needed for the model-free approaches.\n\nMy main concern with the proposed method is how it would scale when the state-space becomes substantially larger (than the 2 dimensions x and y used in the experiments). The reason I am concerned is that the proposed method uses brutal sampling to search for good trajectories in z-space and action-space. It looks like the curse of dimensionality will quickly make this approach unfeasible. Also, it would be nice to have the learning system discover the important dimensions in which to plan (the x and y in the experiments), rather than having to provide them by hand.\n\nA minor concern is the following: is it possible that the optimization could end up discovering a large number of highly predictable (and diverse) but useless skills?\n\nIn the related work section, 1st paragraph, in the list of citations, it might be good to also include the work on maximizing mutual information between representation of the next state and representation of the skill (Thomas et al, arXiv:1802.09484).\n\nThe definition of Delta (page 9) is strange: it is said that Delta should be minimized but Delta is defined as proportional to the rewards (which should be maximized). Maybe a sign is missing. Also, why not simply define the rewards as being normalized in the first place, so that the metric IS the accumulated reward rather than this unusual normalized version of it."
        }
    ]
}