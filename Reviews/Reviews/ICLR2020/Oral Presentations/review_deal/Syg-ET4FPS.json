{
    "Decision": {
        "decision": "Accept (Talk)",
        "comment": "The paper extends posterior sampling to the multi-agent RL setting, and develops a novel algorithm with convergence guarantees to a Nash Equilibrium strategy in two-player zero sum games. Reviewers raised several questions, many of which were well addressed by the authors and which helped further clarify the approach and contribution of the paper. The paper is timely in that novel connections between Game Theory and RL are being explored in fruitful ways, and the paper provides valuable new insights and directions for future research.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #4",
            "review": "Review for \"Posterior Sampling for Multi-Agent Reinforcement Learning\".\n\nThe paper proposes a sample-efficient way to compute a Nash equilibrium of an extensive form game. The algorithm works by maintaining a probability distribution over the chance player / reward pair (i.e. an environment model).\n\nI give a weak recommendation to accept the paper. Although I haven't checked the proofs in detail, the premise seems to be sound - the authors extend model-based exploration results from MDPs to games. The essence of the argument seems to be that the model of the chance player becomes close to d^\\star quickly enough to get a sub-linear bound.\n\nThe main complaints I have about the paper concern clarity.\n\n1. The paper is very densely written. This isn't necessarily bad, but it makes the paper a bit hard to understand. It would benefit the manuscript greatly to provide a figure which shows how the algorithm works for a small toy game. There is space left in the paper, so even a one-page figure would fit in. The figure should show all the major quantities: d, \\sigma, u.\n\n2. The meaning of the quantity \\mathcal{G}_T^i should be more thoroughly described, given it is important in the proof. \n\n3. You define a game with N players, but the algorithm works with 2.\n\n4. Do you really need all the notations in section 2.1? Why not just define the ones used in the algorithm?\n\n5. Can you discuss how large the constants \\xi can become in practice? The definition of \\xi^i seems to be different on page 10 and in Theorem 1 - please disambiguate.\n\nI ask the authors to add a figure and address the issues above. \n\nI am not an expert in this sub-field so I may have missed aspects of the paper.\n\nMinor points:\n- In Figure 1, please say that \"default\" is your algorithm.\n- \"optimal in the face of uncertainty\" => \"optimism in the face of uncertainty\" ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #2",
            "review": "PSRL\n------\n\nThis work considers the task of finding a Nash equilibrium in a two-player zero-sum imperfect information game, where some aspects of the game are not known to the agents (specifically, the chance node probabilities, and the reward function). \n\nThe authors propose a method based on PSRL, i.e. at each iteration a set of game parameters are sampled from the posterior of the distribution. Then, CFR is applied in the inner loop; but instead of finding the NE strategy, one player finds the strategy that basically maximizes the reward deviation between two games sampled from the posterior, given that the opponent is playing Nash in the first game.\n\nThe authors prove convergence bounds for their algorithm, and demonstrate its performance on Leduc Hold'em with game parameters randomly chosen from a Dirichlet distribution.\n\n-------------------------\n\nI agree with the authors that standard CFR suffers from the requirement that the full game is known, so it doesn't work well in its standard form when the environment is not known. There *are* other regret minimizers that do work in the model-free setting ([1], [2], [3]), none of which are discussed by the authors.\n\nI also find the proposed setting somewhat unconvincing. The authors are considering a situation where the environment is unknown, but it's not the RL setting because you still must control both (opposing!) agents. Of course, you can't actually find a Nash Equilibrium if you can't control the other player (because they might just never explore part of the game tree). But you would want your algorithm to be a regret minimizer regardless of your partner's strategy. Is this true of the proposed algorithm?\n\nThe proposed interaction strategy described in Eq. 5 and 6 is clever: basically, each player explores a part of the tree that maximizes the difference in payoffs between the two sampled games. This seems a bit inefficient thouggh, why doesn't the agent just find the BR to \\sigma_{-i} under \\tilde{d} given that the opponent plays the NE under d? I don't see why you would want to explore parameters that have a high reward uncertainty if there's a different strategy that does better than the whole confidence interval. It's like UCB: you should play a strategy not with the highest uncertainty, but with the highest optimistic payoff.\n\nI think the work could be substantially improved by comparing against model-free baselines, e.g. fictitious self-play [2], CFR with outcome sampling [1]. The current work deosn't provide any evidence of what benefits the Bayesian approach provides over model-free regret minimization. Especially since the Bayesian approach presumably does not scale as well due to the requirement of maintaining beliefs over all possible games, and the requirement that a correct prior is provided. I would be curious to see Bayesian and model-free appraoches compared in games of different sizes to see how the different methods scale.\n\nNits:\n- such as the private pokers in poker games (private cards)\n- h_1, h_1 \\in H^C, are independent\n- The reference to Neil 2018 should be \"Neil Burch\", not \"Burch Neil\"\n- And if you're going to say \"we can directly apply the technique from [100-page PhD thesis]\", please mention the page number.\n\n\n\n[1] Lanctot, Marc, et al. \"Monte Carlo sampling for regret minimization in extensive games.\" Advances in neural information processing systems. 2009.\n[2] Heinrich, Johannes, Marc Lanctot, and David Silver. \"Fictitious self-play in extensive-form games.\" International Conference on Machine Learning. 2015.\n[3] Srinivasan, Sriram, et al. \"Actor-critic policy optimization in partially observable multiagent environments.\" Advances in Neural Information Processing Systems. 2018.\n\n\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "rating": "8: Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Posterior sampling for multi-agent reinforcement learning: solving extensive games with imperfect information\n================================================================\n\n\nThis paper investigates the use of Thompson sampling in multi-agent reinforcement learning.\nThey present a natural extension of the PSRL algorithm paired with counterfactual regret minimization, rather than expected reward maximization.\nThey provide support for this algorithm's efficacy through a theorem that proves polynomial learning rates, together with empirical evaluation where this approach is competitive with state of the art.\n\n\nThere are several things to like about this paper:\n- This paper is definitely \"groundbreaking\" in that it makes a true extension to the existing literature: PSRL has been relatively well-studied in single-agent RL but never (to my knowledge) in the multi-agent setting.\n- The extensions from single agent to multi-agent are natural, but also non-trivial, and it seems like this is a genuinely novel piece of work that can be interesting to both side (exploration and multi-agent).\n- The general structure of the paper and presentation is good.\n- The support from the theorem is great, and also the empirical evaluation is convincing.\n\nThere are a few places where the paper might be improved:\n- It might be helpful to draw the connection to Thompson sampling more explicitly at the start. PSRL is really an application of Thompson sampling principle, but it is important that it doesn't happen every step but instead on a longer timescale. It might be helpful to cite \"a tutorial on thompson sampling\" Russo et al.\n- Do you think there are promising avenues towards PSRL with generalization (rather than tabular)? It feels like actually this should carry over naturally... so maybe you should mention this?\n- I'm not really an expert on the novelty / impressiveness of this algorithm in the multi-agent setting so cannot fully comment on that.\n\n\nOverall I think this is a really interesting paper that should be of interest to the ICLR community.\nI can't say this with full confidence (especially with respect to the multi-agent side) but I do think it's something that probably would add value to the conference!\n"
        }
    ]
}