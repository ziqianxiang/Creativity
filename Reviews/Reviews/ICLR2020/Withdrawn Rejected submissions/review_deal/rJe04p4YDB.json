{
    "Decision": {
        "decision": "Reject",
        "comment": "Authors propose a new method of semi-supervised learning and provide empirical results. Reviewers found the presentation of the method confusing and poorly motivated. Despite the rebuttal, reviewers still did not find clarity on how or why the method works as well as it does.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The paper proposes Coaching for semi-supervised learning. The teacher generates pseudo labels for unlabeled data and the performance of the student on labeled data is used as a reward to train the teacher. The empirical results are very impressive.\n\nOverall, the paper is clear and easy to follow. The idea looks interesting to me. However, I find that there are some weaknesses.\n\nFirst, the method is not well-motivated in the Introduction section.\n\nSecond, the derivation of the teacher's update rule is incorrect. In Eq. (11), $g_S^{(t)}(\\hat y_{unl})$ is a vector, $\\frac{\\partial \\ell(x_{unl}, \\hat y_{unl}; \\theta_T)}{\\partial \\theta_T}$ is also a vector. What do you mean by multiplying two vectors? The left side of Eq. (11) is a matrix while the shape does not match on the right side.  It is incorrect to get Eq. (3) from Eq. (11), especially the transposed $g_S^{(t)}$. Where does the transpose come from?\n\nAnd $g_T^{(t)}$  in Eq.(3) is inconsistent with line 6 in Algorithm 1, where $\\eta h^{(t)}$ is not included in $g_T^{(t)}$.\n\nFor experiments, I wonder what is the performance of pure Coaching without RandomAugment and the consistency loss/UDA in Table 1. I think this is a fair comparison with the baselines like Mean Teacher and VAT. I suggest adding this to the ablation study as well.\n\nAnd I expect more explanations on the \"additional implementation details\". For example, why is it correct to use cosine distance instead of the dot product? In this case, is it a valid gradient? Same for other tricks. We need to understand why it works apart from adding a bunch of tricks together. And I doubt whether the improvement is due to these tricks or the method itself.\n\nI would be willing to increase the score if all the concerns are addressed in the authors' response.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #2",
            "review": "This paper studies the teacher-student models in semi-supervised learning. Unlike previous methods in which only the student will learn from the teacher, this paper proposes a method to let the teacher learn from the student by reinforcement learning. Experimental results demonstrate the proposal’s performance.\n\nThe paper achieves some good empirical results compared to other baselines. However, the proposed method is implemented with many tricks listed on Page 4, and with data augmentation techniques, which may not be used in previous methods. Additionally, the paper is weak in technology. There is no clear explanation of why the proposed method works except a metaphor for sports coaches. I vote for a clear rejection of the paper.\n\nFirst, the paper is weak in experiments. It works hard to achieve a good experimental result, through many tricks listed in the “Additional Implementation Details” in Page 4, and through the data augmentation used in Page 5. However, these tricks to improve the performance may not be used in previous methods, as the paper does not run experiments on baselines under the same setting, but use the results reported in Oliver et al. (2018). Additionally, the paper only uses one number of labeled data for each data set, it makes readers doubt that the proposed method only works under this number of labeled data. \n\nThe paper fails to clearly state why we need to let the teacher learn from the student. Actually, I doubt if this is necessary. Given the strong learning capacity of neural networks, the proposed method will easily be overfitting. Assume we have a very weak student network at the beginning, then by training in the way proposed in the paper, the teacher network will have all labeled data classified correct, and all unlabeled data classified into the same labels as the student network. I cannot see from the simple proposal why such overfitting can be avoided.\n\nThe paper is weak in both technology and experiments. It is also poorly written without clearly stating the motivation for this problem. I would vote for a reject for the paper. \n\n-------------------------------------------\nThe rebuttal has cleared some of my concerns. However, it is still not clear why the proposed method work and how does it prevents overfitting. The paper also needs more experimental results to confirm its effectiveness. I will increase my score a little bit, but would not vote for an accept this time. But I believe with further revision, the paper may be worth publishing in the future, if the questions in all reviews can be addressed.  ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #1",
            "review": "This paper provides a simple but novel coaching method for teacher-student based semi-supervised learning framework. The coaching method consists of a two-stage update: first update Student network according to the pseudo label produced by Teacher network on the unlabeled dataset; second update Teacher network according to the Student network's performance on the labeled dataset. The authors propose a novel policy gradient update for the Teacher network. The authors evaluate the coaching method on several different semi-supervised learning dataset CIFAR-10, SVHN and ImageNet. The comparison with baselines are thorough. I appreciate that the authors explain the design of the experiments and tuning in details.\n\nI vote for acceptance but I still have some questions. I would be willing to increase my score if the authors address my questions in the rebuttal.\n1. The motivation of coaching is not accurate. In the second sentence of Introduction, the authors mention \"Although coaches do not play as well as the players\". However, we are training neural networks. There is no such thing as \"coaches do not play as well as the players\". They are the same parametric neural networks. (The authors also use the same architectures for both teacher and student neural networks.) I would remove this analogy sentence.\n2. The authors mention that their coaching method beats the fully supervised learning on the CIFAR-10. I am not convinced by this result. The author explained that this is due to the less overfitting in the student network. However, besides coaching, there are many other regularization method we can use to avoid overfitting. Using far less labels in training strictly reduces the amount of information we have. There is a simple test the authors can do. We can use the full labelled data set and use sampled results from Teacher network to train a Student network on it. We can perform coaching on this regime. This coaching on full dataset should do better than coaching on partially-labeled dataset (4000 labels).\n3. The state of art for CIFAR-10 is 99% now [1]. It would be nice to see the performance of author's approach applies to the state-of-art network/structure.\n\nReferences:\n[1] Huang, Yanping, et al. \"Gpipe: Efficient training of giant neural networks using pipeline parallelism.\" arXiv preprint arXiv:1811.06965 (2018).\n\n-----\nThe authors' response does not answer the questions about motivation and why the method works, which is also questioned by the two other reviewers. For example, why the authors need to have two networks is unclear; I guess that true labels should be more helpful to guide a student network to learn than a teacher network. Even though the authors seem to have state-of-art semi-supervised learning results, some extra explanations are needed. ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        }
    ]
}