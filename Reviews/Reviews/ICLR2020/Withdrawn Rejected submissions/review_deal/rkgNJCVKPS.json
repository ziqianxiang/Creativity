{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Summary\n\nThe authors present a framework for visualization and interpretation of medical images classified with a DenseNet: the framework is tested on MURA, for abnormality detection in musculoskeletal radiograph images, allowing to do feature extraction and visualize CAMs. The authors claim that the framework helps clinicians to build trustability in the model and identify hidden features extraction failures.\n\nContent\n\nAlthough the work is interesting, it doesn't bring any novelties, the methods used (feature extraction and CAM) are well known and the paper does not offer any quantitative measurements to back-up its claims.\nThe paper is well written, in my opinion, it is a good paper for a conference/journal at the interface of healthcare and Deep Learning in a more applied field. However, most of the content is too detailed for the conference it aims for: Whole paragraph on how radiography works, which is not important here to explain the results,  too many details on the structure of a DenseNet which is a well-known model in the community, even most of the proposed framework could have been written more concisely. \nThe results are exclusively qualitative, moreover, the resulting images are shown in the appendix which elongates the paper. With more concise writing and a better formating for the figures (not showing all the features for example), the authors could have fit the results with the explanations. As for quantitative experiments, I would encourage the authors to have a look at Evaluating Feature Importance Estimates (Hooker et al) and other experiments in Sanity Checks for Saliency Maps (Adebayo et al). The last paper even gives good explanations on the \"edge detector\" resemblance of CNN, which could help the authors to explain some of their results (the frame in fig 10 for example).\nIt is unclear why CAM was used here and not GradCam or Grad-CAM++ (Chattopadhyay et al) or even more recently Smooth Grad-CAM++ (Omeiza et al.). Even saliency map approaches such as Guided backprop would have been interesting to see in this work. \nThe paper brings results on a DenseNet trained on ImageNet, but it would also have been interesting to see the resulting visualizations with a pre-trained model on an X-ray dataset (for example ChestXray from NIH).\nThe part of how this framework could help clinicians make their analysis faster could be detailed: The framework generates features for each blocks, which seems a lot to analyze in the end, and even if those features offer a good interpretation of where the network is looking and why it is wrong, the authors do not offer a solution as to change this in order to improve the accuracy.\n\nTypo\n\n- in Discussion, first paragraph, last line, \"it will then zoomed\" -> \"it will then be zoomed\"?\n\nConclusion\n\nIn the end, the paper is interesting and I believe the framework would be relevant in a hospital, with maybe more features such as different models, methods of interpretability, etc. However, it doesn't offer any novelty in the Deep Learning field and does not fit for a conference such as ICLR."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper is an application of a CNN architecture to classification problems in radiology. The author(s) simply use DenseNet in this work. They also address the visualization of the output of the convolutional layers (visualization of layer wise activation using matplotlib in python) to provide interpretations for what the network is doing as it classifies images. \n\nFrom a method perspective, I fail to see any contribution of this work. Looking at the output of layer activation is not new, certainly not in the way it is presented in this work. Given the focus of this conference, I am not sure that this work is appropriate here. There are no real contributions to AI or Machine Learning proposed in this paper, not from a method perspective and also not from an application perspective. \n\nAlso, while the authors do report on experiments performed on an interesting corpus of images, I find the evaluation quite shallow. I concede that it is difficult to measure or estimate interpretability as it is mostly a qualitative concept. However, the paper could benefited at least from a small user study. At this point, I recommend a rejection for ICLR 2020. "
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Paper summary\nThis paper aims to make model predictions of DenseNet more interpretable in order to aid the decision making process in clinical radiology. To do that, the authors suggest to visualize the layer-wise activations of the network, as well as the class activation map for a binary prediction task. The authors suggest that these visualisations be given to a radiologist in order to speed up, and improve reliability of the decision making process. To support their claim, the authors train the DenseNet on an abnormality detection task on the MURA data set under 4 different training conditions (data subset, pre-trained vs. not pre-trained). The authors then visualise an exemplary case study in figure 4 which they interpret in the results section (with additional cases in the Appendix). The authors conclude that these visualisations have been “insightful to the authors” and that this work has revealed “a close resemblance of a trained DenseNet [...] to the approach of a human”.\n\nReview\nI vote to reject this paper. Unfortunately, I am having difficulties in identifying either novelty or usefulness in the presented approach that would support an ICLR publication.\n\nOn Novelty\nThe paper uses two ingredients to make DenseNet interpretable: (1) Layer activation maps, and (2) Class activation maps. Both of these ingredients are common approaches to do qualitative analysis of deep learning models. The authors don’t claim novelty on these aspects, however since these are the major ingredients in their method it is important to comment on this. Both visualisation techniques belong into the standard toolbox in deep learning, applying them to DenseNet is not a novelty in itself. \n\nOn Usefulness\nEven though there might be no technical novelty, one could expect some practical usefulness in the presented approach. Either, (1) to build better models in the future, or (2) to aid - as the authors envisage - clinical practitioners. I will focus on the example presented in figure 4 to discuss these points. For this example, the authors argue that the “feature mechanism followed an intuitive hierarchical path”, and that “at the end of the network, most of the feature maps were activated close to where the fracture was located”. I disagree with the authors. The only thing I can see with these feature maps is that they become coarser the deeper we go in the network, but that is simply owed to the model architecture. I cannot identify the fracture location from the feature maps in block 3 and 4. I believe that the authors judgement on the feature map is a result of their knowledge of the fracture location. But that is not a realistic situation in the clinical setting. Another point why I believe feature visualisation is not useful in the clinical practice is a point which the authors argue themselves: work fatigue (see the first sentence in the abstract). The original goal of the authors was to reduce overload for radiologist by giving them interpretable information. But what they suggest here is to increase the number of images that a radiologist has to look at from a couple of images to several 10s or even 100s of images. In my opinion, this would put more mental overload on radiologist even under the assumption that the feature maps give useful insights - which I still disagree with.\n\nImprovements\nIf the authors believe that feature maps visualisations can help to make models more interpretable, this is what I would suggest the authors to show:\n\n(1) Show that radiologists can actually improve in performance (speed, accuracy) when using CAM and feature activation maps. Run a study with radiologists that use the tools that you are suggesting.\n(2) Use insights from looking at the feature maps to motivate and train a different model architecture. Does the model behaviour change? Can you improve performance? Do features become more interpretable?\n"
        }
    ]
}