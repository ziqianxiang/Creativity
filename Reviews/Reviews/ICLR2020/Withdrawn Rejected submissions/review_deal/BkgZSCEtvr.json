{
    "Decision": {
        "decision": "Reject",
        "comment": "Novelty of the proposed model is low. Experimental results are weak.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "This paper proposes a new variant of invertible flow-based model for graph structured data. \nSpecifically, the authors proposed a continuous normalizing flow model for graph generation, first in the graph generation literature. \nThe authors claim that the free-form model architecture of neural ODE formulation of continuous flow is advantageous against standard discrete flow models. Experimental results show that the proposed model is superior to recent models in image puzzling and layout generation datasets. \n\nOverall, manuscript is well organized. \nThe combination of continuous flows and graph structured data is new in the literature (as far as I know). Proposed formulation seems natural and reasonable. I find no fatal flows in the formulation. In experiments, the proposed model achieved good scores against recent GNN works. \n\nConcerning the existing invertible flow-based models for graph structured data, Madhawa’s work is one of the first attempts in the literature. I think the paper below should be refereed appropriately:  \nMadhawa+, “Graph NVP”, arXiv: 1905.11600, 2019. \n\nThe way of incorporating relational structure into flows are very similar to the GraphNVP and Graph Normalizing Flow: using neighboring nodes’ hidden vectors as parameters (or input to parameter inference networks). \nIn addition, I found no special tricks or theoretical considerations to achieve the continuous flow for graphs. Based on these points, I think technical contribution of this paper is somewhat limited. \n\nI cannot find information about the specific chosen forms of f-hat and g in Eq.(10) within the manuscript. Are the choices of f-hat and g are crucial for performance? It is preferable if the authors can present any experimental validations concerning this issue. \n\nMy main concerns are in the experimental section. \n\nI’m not fully convinced in the necessity of the continuous normalizing flows for the experimental tasks. None of the experimental tasks have `````'' intrinsic continuous time dynamics over graph-structured data (Sec. 2)''. Then, what is the rationale to adopt Continuous graph flow for these tasks? \n\nOne reason to adopt continuous flows is that the ODE formulation allows users to choose free-form model architecture, yielding more complicated mappings to capture delicate variable distributions. \nI expect some assessments are made concerning this issue. My suggestion is (i) to test the discretized model of the proposed Continuous Graph Flow and see how the discretization deteriorate the performance, and (ii) to test several choices of f-hat and g (Eq.10) to show the necessity of ODE formulation, accommodating free-form model architecture.  \n\nIn the current manuscript, Graph Normalizing Flow (GNF) is the closest competitor. However, GNF is not tested in the puzzle and the scene graph experiments. Why is that?\n\nI’d like to hear opinions of the authors concerning these issues, and hope some discussions are included in the manuscript. \n\n\nSummary\n+ continuous normalizing flow is first applied to graph structure data\n+ manuscript is well organized\n+- natural and reasonable formulation. But at the same time, technological advancement is limited. \n- Less convinced to adopt continuous flow for graph-structured data without no intrinsic continuous dynamics. \n- Necessity or advantages of ``continuous’’ flows are not well assessed in the experiments. Please consider some additional assessments suggested in the review comment. \n- GNF, the closest competitor, is omitted in the 2d and the 3rd experiments. No explanations about this. "
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The paper presents a method to do Continuous Normalizing Flow for graphs by defining the density evolution via ODEs over shared variables. Continous message passing scheme introduced can leverage the free-form model architectures which allow flexibility in handling variable data dimensions. The results of three different generation tasks indicate improved performance over the baseline methods. \n\nNovelty: The basic methodological contribution of the paper seems somewhat limited. The main idea of the paper follows directly from NeuralODE (Chen et al., 2018) and FFJord (Grathwohl et al., 2019). Nevertheless, I do agree that adapting the NeuralODEs and continuous flows for unstructured data is challenging and deserves its own analysis. However, I find the experiment section to be not very clear (See specific comments below).  \n\nRecommendation: Weak Reject\n\nConcerns: \n\n- The experiment on Graph Generation is not clear to me. What is the motivation of using the dual of the graph? What is the input to the full-connected layers? Is the input dimension N^2 (the number of edges)? Please clarify. It would be helpful if the implementation details in Appendix A.1 can be formalized in the main paper since this is the main contribution of the paper. Overall the experiment section in the main paper lacks sufficient details to comment on the efficacy of the method.\n       \n- NeuralODEs involve iterative numerical solvers which can be very slow. It seems that the proposed method involves simultaneously solving ODEs that linearly scale with the number of nodes (Equation 11)? I think there should be some analysis of how the proposed method scales with the graph size. I am not sure if NeuralODE would perform well when the number of nodes is large.\n\n- Compared to other baseline methods (see You et al. 2018), the datasets used for analysis are smaller versions of the original datasets. There are other datasets that can be used for experiments on graph generation:  See Grid, Protein "
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposes continuous graph flow (CGF), a flow-based generative model for graphs.  Based on the idea to build continuous-time flows with neural ODEs, the node features are transformed into Gaussian random variables via message passing. The log-likelihood can be approximated stochastically.\n\nI find it hard to assess the novelty of this work because 1) the algorithm looks like a trivial application of continuous-time normalizing flow to graph data using message passing algorithm, and 2) the concurrent work graph normalizing flow (GNF, Liu et al., 2019).  I failed to find any algorithmic innovation more than a mere application of continuous-time flow to graph data. The very idea of a flow-based model for graphs using the message passing algorithm may be considered as a contribution, but this is also blurred because of the concurrent work GNF. \n\nThe experiments look interesting, but there are some minor concerns. For the unsupervised graph generation task, CGF is shown to perform better than GNF, but I think the comparison here may not be fair because the results for GNF seem to be obtained from the paper uses a different way to construct initial node features to be fed into message passing. Specifically, according to the GNF paper, they inject an i.i.d. Gaussian noise as initial node features, but this work uses different schemes based on the dual graph (btw I think the term dual graph may be misleading. There already exists a common term called dual-graph with different definition). So I think to compare the expressive power of flows, it would be fair to start from a common scheme to build initial node features. GNF should also be compared to CGF for the other experiments. Seems like the baselines presented in Table 2 and Table 3 are quite dumb baselines not well suited for the tasks considered here. \n\nIn page 2, the authors stated that \"GNF involves partitioning of data dimensions into two halves and employs coupling layers to couple them back. This leads to several constraints on function forms and model architectures...\". I think GNF can be improved using more advanced transformation other than affine coupling, such as recently proposed mixture logistic CDF [1] or neural spline flow [2].  Continuous-time flow may still have an advantage in memory usage, but at least in terms of expressive power, I think it is not clear whether CGF is particularly better.\n\n[1] Ho et al., Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design, 2019\n[2] Durkan et al., Neural Spline Flows, 2019"
        }
    ]
}