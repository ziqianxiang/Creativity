{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper introduces PC-NAS, a new method for architecture search addressing the problem of shared weights in architecture search. \n\nThe authors test the algorithm on ImageNet and show that they are comparable or better than other published methods and also test the model on COCO which is a detection benchmark and show that it holds for the detection and person re-identification tasks.\n\nTHe paper is motivated well by identifying a core problem of weight sharing in architecture search and providing a bayesian point of view and a solution for the problem. But, as a few other folks pointed out it is important to do thorough comparisons and it looks like when authors compare the current method to other methods there are other changes to the pipeline that could give boost to the accuracy. So, the claim of achieving state of the art is disputable.\n\nIt will be good to decouple the bells and whistles and compare methods on an even footing. If authors can comment on that I still think it is a valuable idea and even if there is no significant delta the work can be a useful contribution."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Let me start by saying I am not an expert in architecture search, and putting lowest confidence accordingly.  Nevertheless, I have concerns about this paper.    While the basic idea of progressive search is reasonable,  it is not tightly coupled to the Bayesian argument the authors use to motivate it.  This argument makes some unnatural assumptions (e.g. layers are independent); and the authors do not give any rigorous analysis, either theoretical or empirical,  of how their progressive search method mollifies the posterior fading problem they discuss.  IMO, the whole Bayesian setup makes the motivation unnecessarily complex, and is math for maths sake, rather than for clarification.  The authors do try to show that the final results are better (which isn't the same thing as saying that the method is fixing posterior fading); but I don't think these experiments are convincing.  In particular, the authors seem to use different regularizers than there competitors, and it is not clear where from where the performance gains come."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper builds upon one-shot neural architecture search (NAS), an established technique that consists of the following stages: 1) training a supergraph that can be viewed as a search space (i.e. a one-shot model that includes multiple overlapping candidate architectures), 2) ranking candidate architectures extracted from the trained one-shot model based on their validation accuracy, and 3) training the top candidates from scratch and evaluating them on the test set. The authors point out that the validation set accuracies produced by the one-shot model in step 2) are less predictive of the accuracies produced by stand-alone model training as the number of candidate architectures grows. They name this phenomenon posterior fading. To mitigate this effect, they propose Posterior Convergent NAS (PC-NAS), a method that trains the supergraph in multiple intervals (one interval per mixop/layer). In each interval, a limited pool of candidates (P=5) is selected via a process similar to beam search. The supergraph is trained based on the losses produced by this small number of candidates, thus limiting posterior fading. Experiments are conducted on ImageNet classification, COCO object detection and Market-1501 person re-identification.\n\nA mathematical derivation of the effect of number of architectures in the supergraph on the divergence of  distribution of one-shot estimated parameters as compared to the distribution obtained by training individual models is included. The derivation is a bit sloppy and contains unrealistic assumptions such as that the parameters in different layers/operations are independent. \n\nThe authors conduct experimental comparisons with prior related works using similar search spaces, as well as ablation of the novel components of their approach.\n\nBetter performance compared to related work is obtained, but an additional SE layer is used in the current work, which may account for a large part of the gains. If more operators are available in the search space, prior work is clearly outperformed (Table 1).\n\n\nAblations of the novel components: layer-wise beam search with pool P for finding top P candidates in a super-graph (Table 3, row 2) and additionally, re-training a shrinking space of models (the full algorithm, Table 3, row 1) , compared to another search method EA (Table 3, row 3) are presented. I don’t know whether EA is a strong search baseline. \nThe additional re-training helps 77.1 → 78.1 , but an important question is not answered:\nIf only beam search without re-training is done, would there be more gains from evaluating more models while using the same computational resources for the search? Since re-training takes 100 additional epochs and training is more expensive than evaluation, additional models (larger P, larger S for Algorithm 1) could have been used.\n\nIf the authors  can provide more convincing evidence that the iterative re-training provides a better search complexity / accuracy trade-off compared to established search baselines and the proposed approach without re-training, as well as clean up the writing and improve the derivation to drop the independence assumption, the paper could be accepted.\n\n-------\n\nSome examples of sloppy math:\n\nIn Section 3.2 \np_alone(\\theta_{m_k}|..) = p_alone(\\theta_{m_k}|..). p_alone(\\hat{\\theta_{m_l}) | ..) \nI believe this should be proportional instead of equal\n\nIn section 3.3.\nPartial model pool : l-1 selected operators but listing o_1 to .. o_l\n\nThere are also many grammatical and language use mistakes.\n\n"
        }
    ]
}