{
    "Decision": {
        "decision": "Reject",
        "comment": "The authors propose a way to produce uncertainty measures in graph neural networks. However, the reviewers find that the methods proposed lack novelty and are incremental additions to prior work.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "title": "Official Blind Review #3",
            "review": "This paper studies the effect of various uncertainties in bayesian GNNs. They study various uncertainty models such as leatoric, epistemic, vacuity and dissonance.  They study multiple uncertainty types in both\ndeep learning (DL) and belief/evidence theory domains. They treat the predictions\nof a Bayesian GNN (BGNN) as nodes’ multinomial subjective opinions in a graph\nbased on Dirichlet distributions where each belief mass is a belief probability of\neach class. By collecting evidence from the given labels of training nodes, the\nBGNN model is designed for accurately predicting probabilities of each class and\ndetecting out-of-distribution. They show that their proposed Bayesian GNN outperforms state-of-the-art counterparts in terms of the accuracy of node classification prediction and out-of-distribution detection based on six real\nnetwork datasets.\n\nThe main issue with this paper is the motivation of the framework. While the technical details seem correct, the writing is unclear in many places in the manuscript. From an experimental perspective, ablation experiments need to be added. Also, the main motivation of this work is that it is modeling the uncertainty in model predictions. However, I do not see this verified anywhere experimentally.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #1",
            "review": "This paper proposes to model various uncertainty measures in Graph Convolutional Networks (GCN) by Bayesian MC Dropout. Compared to existing Bayesian GCN methods, this work stands out in two aspects: 1) in terms of prediction, it considers multiple uncertainty measures including aleatoric, epistemic, vacuity and dissonance (see paper for definitions); 2) in terms of generative modeling, the GCN first predicts the parameters of a Dirichlet distribution, and then the class probabilities are sampled from the Dirichlet. Training/inference roughly follows MC Dropout, with two additional priors/teachers: 1) the prediction task is guided by a deterministic teacher network (via KL(model || teacher)), and 2) the Dirichlet parameters are guided by a kernel-based prior (via KL(model || prior)). Experiments on six datasets showed superior performance in terms of the end prediction task, as well as better uncertainty modeling in terms of out-of-distribution detection.\n\nPros:\n1. This model considers uncertainties in multiple dimensions.\n2. Better predictive performance and OOD detection ability on 6 real datasets.\n\nCons:\n1. Adding an additional layer of the Dirichlet is not well motivated.\n2. Needs ablation studies on modeling choices, e.g., how much does the graph kernel prior help.\n3. In table 2, needs to compare with traditional Bayesian GCN (such as Zhang et al 2018). Besides, is GCN-Drop in table 4 Zhang et al 2018?\n4. In table 2, seems that knowledge distillation helps, since GCN gets similar performance to BGCN, but BGCN-T outperforms. A natural baseline is GCN w/ knowledge distillation.\n5. In table 4, the baseline GCN-Drop gets better uncertainty estimates than the proposed approach in terms of aleatoric, epistemic, entropy which can be evaluated for GCN-Drop. I wonder if it is possible to develop a measure for vacuity and dissonance for GCN-Drop as well. But anyway this table contradicts the motivation for adding an additional layer of the Dirichlet.\n\nMinor details:\n1. Is teacher jointly trained with the model or is it pretrained? And what's teacher's network architecture? Is it much larger? I cannot understand \"choose two graph convolutional layers in which the first layer is 16hidden units for GCN and 64 hidden units for GAT, and removed a softmax layer\".\n\nOverall, this is a very technical work, but the modeling choices need to be better justified/motivated compared to existing works on GCN with MC Dropout. I am inclined to reject this paper.\n\n\n-------updates after reading rebuttal----\nThanks for the response! I have no further questions.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors proposed a Bayesian graph neural network framework for node classification. The proposed models outperformed the baselines in six node classification tasks. The main contribution is to evaluate various uncertainty measures for the uncertainty analysis of Bayesian graph neural networks. The authors show that vacuity and aleatoric measure are important to detect out-of-distribution and the dissonance uncertainty plays a key role for improving performance.\n\n** Introduction/Conclusion/Contribution\n- Applying MC dropout to graph neural networks is a bit old idea [1-5], it cannot be considered as the “contribution” of this study.\n- The authors should emphasize more on what’s the advancements over existing studies.\n\n** Methodology\n- My understanding is the authors proposed to use multiple uncertainties (vacuity, dissonance, aleatoric, epstemic). Vacuity and dissonance measures can also be implemented with other Bayesian graph neural networks.\n- Ablation study is need to demonstrate the usefulness of each component in the objective function (equation 11). \n- Can “BGAT-T” be considered as the proposed method? It doesn't use the proposed GNN framework but just an extension of original GAT with MC dropout and knowledge distillation.\n\n** Experiments\n- Please provide the details of hyperparameter settings, e.g. optimizer, batch size, learning rates, …\n- The authors should perform experiments with varying numbers of labels per category (e.g. 5, 10, 20), because different methods show different behaviors under label scarcity.\n- There are a number of recently published papers that address node classification based on Bayesian graph neural networks, e.g., see references. They should be used as baselines if available.\n- I think the authors should evaluate BGAT-T with Co.Physics, Ama.Computer, and Ama.Photo datasets, because this method was overall superior to other methods on the first three datasets. \n- Experiments are insufficient in the uncertainty analysis (section 5). The authors can evaluate the performance of BGAT as well as other Bayesian graph neural networks in terms of uncertainty quantification performance. \n- Also, in order to evaluate uncertainty quantification performance, I would suggest to look at the trade-off between classification accuracy and classification rejection based on the uncertainty, like accuracy-rejection curve in [6] \n\n** misc\n- Check the following sentence of subsection 3.6. “our key contribution is that the proposed Bayesian GNN model is capable of estimating various uncertainty types to predict existing GNNs.”\n\nReferences\n[1] Ryu, S., Kwon, Y., & Kim, W. Y. (2019). Uncertainty quantification of molecular property prediction with Bayesian neural networks. arXiv preprint arXiv:1903.08375.\n[2] Zhang, Y., & Lee, A. A. (2019). Bayesian semi-supervised learning for uncertainty-calibrated prediction of molecular properties and active learning. arXiv preprint arXiv:1902.00925.\n[3] Pal, S., Regol, F. L. O. R. E. N. C. E., & Coates, M. A. R. K. (2019). Bayesian graph convolutional neural networks using non-parametric graph learning. In Representation Learning on Graphs and Manifolds Workshop, Int. Conf. Learning Representations.\n[4] Akita, H., Nakago, K., Komatsu, T., Sugawara, Y., Maeda, S. I., Baba, Y., & Kashima, H. (2018, December). Bayesgrad: Explaining predictions of graph convolutional networks. In International Conference on Neural Information Processing (pp. 81-92). Springer, Cham.\n[5] Zhang, Y., Pal, S., Coates, M., & Ustebay, D. (2019, July). Bayesian graph convolutional neural networks for semi-supervised classification. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 33, pp. 5829-5836).\n[6] Nadeem, M. S. A., Zucker, J. D., & Hanczar, B. (2009, March). Accuracy-rejection curves (ARCs) for comparing classification methods with a reject option. In Machine Learning in Systems Biology (pp. 65-81).\n"
        }
    ]
}