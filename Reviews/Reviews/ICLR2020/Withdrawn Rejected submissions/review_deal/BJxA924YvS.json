{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper connects knowledge distillation and label smoothing regularization. With two sets of well designed experiments, train large network with small teacher or poorly trained teacher, the authors claim knowledge distillation has a strong regularization effect. The authors then show that both knowledge distillation and label smoothing loss can be written as linear combination of supervised learning loss and KL divergence loss. Finally, two “teacher free” methods are proposed, use the same network as student and teacher, and a variant of label smoothing.  \n\nI am generally on the fence for this paper. I think it makes sense to view knowledge distillation as a regularizer. It is not a surprising finding but I have not seen it in previous papers. On the other hand, there are multiple ways to interpret knowledge distillation, including categorical relationship, loss smoothing, and regularizer. It is hard to convince me regularization is the main drive because in original KD, even without the supervised part H(q, p), it still works reasonably well. It looks to me this paper is more about label smoothing/regularization than knowledge distillation. But since label smoothing has been shown to be an effective regularizer in previous works, it is difficult to say how much practical value is there. \n\nAbout the two proposed teacher free methods, the first one looks like Born Again Network, the second looks like can be achieved by tuning the label smoothing parameter \\alpha. \n\nA relatively minor issue is how the hyper-parameters (\\alpha and there should be a temperature parameter T) are decided for knowledge distillation.\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This submission takes a critical look at knowledge distillation (KD) and asks the important question whether or not it’s just plain regularization in disguise. It shows some interesting evidence to support this and suggests simpler KD variants based on this insight.\n\nI think it’s a great premise, but I will have to land on Weak Reject because of weaknesses in the suggested Tf-KD versions. I think the Tf_KD_reg is just label smoothing, which removes the primary novelty of the paper. I think this is a grave error, but I do not go below a weak reject since I do like the premise and the observations in the first half and think they hold some merit on their own.\n\nPremise:  \n\nThe premise of the paper to link KD with LSR and the experiments showing evidence that unintuitive or even poor KD can be effective are great. I think it’s a great starting point and I would like to see more in this direction. I am not surprised by this conclusion either, so it is important to point out.\n\nExperimental thoroughness:\n\nThere are plenty of experiments, many models and datasets, with repeated runs and error bars. High marks for this. However, as I will point out shortly, I don’t think the hyper-parameters were necessarily chosen carefully.\n\nTf-KD_reg:\n\nThe main contribution is Tf-KD_reg, so let's take a closer look. Using an “almost correct” teacher looks identical to label smoothing to me. I provide some mathematical arguments that this is true.\n\nEq. (2) establishes what you get if you blend two distributions. Let’s denote this q’_alpha, since we may want to change the mixing ratio alpha. So, if we define u here as another version of Eq. (1), but taking q’_beta, the results will be exactly q’_{alpha*beta}. What this shows is that if we do an alpha-convex blend with q’_beta, then it’s equivalent to doing a alpha*beta-convex blend with the uniform distribution. With this established, note that Eq. (8) can be written as q’_{(1 - a)*K/(K-1)}. So, when that is plugged in, it simply turns into blending directly with a uniform distribution (i.e. label smoothing) with mixing ratio alpha*(1-a)*K/(K-1). Label smoothing has one parameter alpha, while this teacher-free formulation has alpha and “a” - it’s simply an over-parameterized version of label smoothing. Adding temperature “tau” to the mix is even further over-parameterization. The fact that LSR does worse probably just means that the tables are a comparison between two different smoothing strengths of LSR - one which is better. Try changing the alpha in LSR with a factor (1-a)*K/(K-1) and see if you now get the same results. Please let me know if there is a difference I’m failing to see. I may have rushed the derivations, but I think it's intuitively clear that they are at least very similar, if not identical.\n\nTf-KD_self:\n\nI disagree that we should call this “teacher free”, since it does have a teacher that needs to be pre-trained prior to training the student. The only novelty is that the student and the teacher share the same network. This is a very modest novelty, but I agree it has some benefits, especially when integrating this technique into a deep learning framework, where it would be as easy as a flip of a switch."
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "First off, the title needs changing as it doesn't make much grammatical sense! \"Revisiting\" instead of \"Revisit\" would be much better. The paper is well written, so I'm surprised the title isn't :)\n\nIn this work, the authors examine the effects of using knowledge distillation in two unusual scenarios. Firstly, when the student network is fully-trained but is weaker than the teacher, and secondly, when the teacher network is only partially trained. In both cases, the student benefits, implying that \"dark knowledge\" between class labels is a nonsense, and all it's really doing is label smoothing.\n\nI must admit, I really like these experiments, and I'm broadly a fan of this paper. I do have a few comments.\n\nIt's important to note that the Hinton et al. paper builds heavily on an earlier work (https://arxiv.org/abs/1312.6184v5), which I would recommend citing. Also, I don't like the use of \"surprising\" in the prose, but that's just down to taste.\n\nSection 2 is good, but it don't understand why there is a separate list of student networks and teacher networks. Why not just put every combination of them? Error bars are good. For Table 1 why does ResNetXt get ResNet18 as a student when the other networks didn't?\n\nTable 10 in the appendix is concerning. I don't see why there are different temperatures and alphas in use. At least with temperature, there is only one case where it isn't set to 20 (why is it 6 for ResNeXt29-ResNet18?). Why does alpha differ so heavily across experiments? This creates the (hopefully unintended) image that your results have been cherry picked. I would strongly recommend either using the same alpha/temp for each experiment, or providing a strong justification why it differs between networks.\n\nThe comparison between label smoothing and KD is neat.\n\nThe first method suggested (Tf-KDself) is *exactly* what the authors use in Born-again nets (https://arxiv.org/abs/1805.04770). I notice that this paper has been cited briefly in the small related work section at the end, but this really should feature earlier as it's not a great look to pass off an older method as something new (although this may be unintentional).\n\nThe difference between the virtual teacher approach and LSR is so subtle that I'm surprised there is much difference in results. The only comparison that I can find is the table in Figure 6 but this is missing error bars (I am using error bars and STDs interchangeably). I would like to see a comparison with error bars (although this could be on say, CIFAR-100 if ImageNet is too expensive).\n\nOn a related note, the table in Figure 4 could benefit form error bars.\n\nPros:\n- Experiments in Section 2 are novel and interesting, as far as I am aware\n- Experiments are conducted across different datasets and networks, suggesting generalisation\n- Paper is well written (apart from the title. Please change!)\n\nCons:\n- The use of different hyperparameters across experiments is concerning\n- Tf-KDself is just a rebranding of Born-Again-Neural networks\n- It is not clear how/why the virtual teacher surpasses label smoothing\n\nI propose the paper should receive a weak accept. It is well written (but please change the title), interesting, and experimentally satisfying. However, I would caveat this by strongly recommending that the authors address my concerns regarding hyperparameter changes. It would be a real shame if this was a case of cherry-picking experiments to suit a hypothesis.\n"
        }
    ]
}