{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper aims to tackle few-shot learning by revisiting the simple finetuning baseline, which is supposed to suffer from overfitting when there are only few-shot training samples. They empirically show that in the few-shot learning scenario, finetuning prefers a low learning rate and adaptive gradient optimizers. They also show that finetuning the whole network benefits more in the case of cross-domain few-shot tasks.\n\nThe paper should be rejected because\n1) its writing lacks clarity. The overall structure of the paper requires a significant revision. The authors mix experiments, \nbaselines and their proposed technics in the experiment section, making it difficult for me to find where the proposed approach is. I was expecting an approach section which explains a) technics they proposed when fine-tuning the network,  b) how the technics solve the issue of overfitting. The presentation of the experimental results is bad too. For example, it is hard to interpret so many numbers in Table 1. What should be compared in this Table? Why many results of different methods are identical in Table 1? Why does fine-tuning works better with VGG16 than with ResNet? In section 3.7, the authors finally start to explain their technics but that's too late. I am confused about which results are achieved with those finetuning technics.\n\n2) little novelty. I can see little novelty in this paper. The normalization technic is from Qi et al. 2018. Using lower learning and Adam optimizer are obviously not new either. \n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposes a few-shot learning method based on fine-tunning, and the experiments are verified on both the standard low-resolution images and high-resolution images. With carefully ablation studies, the authors find some reasonable ways to fine-tune a few-shot learning model.\n\n1. The novelty of the paper:\nIt is a bit hard to evaluate the novelty of this paper. Learning a few-shot model based on a pre-trained model has been investigated in some papers, e.g., [1-4]. By fine-tuning the whole network based on the pre-trained backbone, most methods get improvements. The authors propose to \"revisit\" the fine-tune, it is more welcome to include some discussions that why fine-tune can facilitate the few-shot learning (understand fine-tune from a novel way), and when fine-tune will/will not help.\n\n2. About the method:\nIt is not very clear for the method part. After pre-training the network, it seems the method is fine-tuned with both seen and unseen/novel classifiers. Does it require the meta-train instances? Is it learning in a transductive way? It is better to write the objective function in section 3.4.\n\n3. Experiments:\n3.1 The authors provide comprehensive experiments to investigate the method, e.g., with different network architectures. How will the number of instances in the target few-shot task influence the fine-tune method? For example, when there are more ways and shots in a task.\n3.2 Dataset: It is interesting to test how the number of classes in the meta-train set influences the ability of the fine-tune based model. For example, will the fine-tune methods get better performance given a larger meta-train set (e.g., on tieredimagenet)?\n\n[1] Task dependent adaptive metric for Improved Few-Shot Learning\n[2] Meta-Learning with Latent Embedding Optimization\n[3] Learning Embedding Adaptation for Few-Shot Learning\n[4] Few-Shot Image Recognition by Predicting Parameters from Activations\n[5] On First-Order Meta-Learning Algorithms"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "\n\nThis paper studies fine-tuning methods in few-shot learning. It considers several fine-tuning choices and concludes that fine-tuning is helpful for few-shot and cross-domain few-shot tasks. The results are competitive with those meta-learning based methods.\n\nEven fine-tuning is not a completely novel thing in transfer learning, it is indeed under-explored in few-shot classification. A solid study of baselines approaches in few-shot learning would be very helpful to the whole community.\n\nHowever, I have some serious concerns with the experiments:\n\n1. The experiments lack fair comparisons to achieve a solid conclusion. In Table 4 (the major Table to draw conclusion of this paper), Fine-tune (Ours) uses VGG-16 as backbone, while Baseline/Baseline++/MatchingNet/ProtoNet are using 4-layer ConvNet, which is not a fair comparison. As an empirical study paper, solid ablation studies are needed to get a convincing conclusion that could be helpful to the community. Therefore, it is better to have results for methods with the same backbone, and conduct experiments on more datasets (tieredImageNet, ImageNet).\n\nIn recent years different few-shot methods are proposed with many different backbones, even some of them are not clearly specified, this could be one of the main reasons that baselines are not fairly evaluated. Getting a fair comparison through solid experiments is the value of a empirical study.\n\n2. The novelty of this paper over Chen et al. seems unclear, both in the introduction and the whole paper. Chen et al.  has done very similar experiments as this paper, and the conclusion is also extremely similar. Is this paper improving over Chen et al., by using low learning rate, Adam, and fine-tuning the whole network? I think those is too trivial for an ICLR publication. It's basically network training engineering. The \n\n3. \"Note that we do not compare the performance of our method with the state-of-the-art algorithm in the high-resolution single-domain and cross-domain tasks because the performances for these tasks are not reported in the corresponding papers.\" I think this can be addressed by using their source code or reimplementing them. As an empirical study paper, it is important to include more recent state-of-the-art methods to convince people. Although I guess the conclusion might not change much but it's necessary.\n\n4. Minor issues:\n1) 2.1 Notation should be improved.\n2) The use of word \"retraining\" is confusing, is it equivalent to \"fine-tuning\"?\n3) The captions in tables could be less redundant.\n\n\nOverall, the paper did not compare baselines with common few-shot methods with the same backbone, and on more datasets to make the conclusion solid. Also in my opinion the novelty over Chen et al. is either unclear (or a bit trivial if the network engineering part is the novelty). I vote for rejection in its current form.\n"
        }
    ]
}