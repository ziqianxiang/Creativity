{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper presents an extension of FID for conditional generation settings. While it's an important problem to address, the reviewers were concerned about the novelty and advantage of the proposed method over the existing methods. The evaluation is reported on toy datasets, and the significance is limited.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposes a variant of the use of Frechet Inception Distance (FID) for the evaluation and benchmarking of conditional GAN models. FID is a popular measure for comparing image distributions in the Inception v3 feature space, in terms of the means and variances of multivariate Gaussians fit to data samples from each distribution. The authors argue that FID is ill-suited for use with cGANs, in that they do not explicitly take into account conditional consistency or intra-conditioning diversity. The main contribution and basic idea of this paper is to create joint image-conditioning distributions from the image embedding and the conditioning embedding in the Inception embedding, and then to combine them (by default, through vector concatenation). These joint image-conditioning distributions are then fed to FID as per standard usage on image distributions alone. The authors refer to their approach as FJD (Frechet Joint Distance).\n\nAlthough the authors propose FJD as a new technique, it should more properly be regarded as the direct use of FID on joint distributions. The main practical contribution of the paper thus reduces to the notion of concatenating the learned conditioning representation with the image representation. As a research contribution, this is in itself not very substantial. However, in their experimentation the authors do take care to show through examples the effect of their joint image-conditioning approach in assessing image quality, conditional consistency, and intra-conditioning diversity, under a variety of conditionings.\n\nThere are some issues that are not adequately addressed:\n\n1) In all experimental cases FJD scores and FID scores correlate well, which undercuts the argument that FJD is superior to FID in assessing the performance of cGAN models. Can situations be experimentally demonstrated where that is not the case? In particular, what happens when the fit in image representation is dramatically better / worse than that of the conditioning representation? This situation is interesting, but not considered in this paper.\n\n2) The effect of the dimensionality of the learned representations is not addressed. When concatenating vectors to produce a joint image-conditioning representation, the dimensionality increases, which would tend to produce larger FJD values than their corresponding FID values. The experimental results of this paper seem to confirm this. However, is\nit really valid then to declare FJD as being somehow more sensitive to the conditioning simply by virtue of obtaining larger values and larger spreads than FID? It should be remembered that FID and FJD are *not* unitless measures.\n\nOverall, in its current state the paper appears to be below the acceptance threshold.\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "Summary\nThis paper mention that there are some critical drawbacks existing in IS (Inception Score) and FID (Fréchet Inception Distance) which are two popular metrics to measure image generation quality. However, IS and FID scores are initially designed for measuring unconditional distribution, which fails to capture the conditional consistency of conditional distribution. Thus, the authors propose to concatenate conditioned embedding h(y) with image feature vector f(x) to extend the FID metric. The authors also implement the method on a toy dataset to show the sensitivity of FJD on conditional consistency and several popular cGAN models to show the efficiency of FJD on real data.\nPaper Strengths\n  1. The method is intuitive and easy to implement.\n\nPaper Weaknesses\n1. Although this paper shows the problem of FID for capturing the conditional consistency sprightly with the toy dataset, however, this problem does not obviously show up on real data. Basically, FID can also give a good comparison of the different model as FJD"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "Summary:\nThis paper extends the Fréchet Inception distance (FID) to the conditional distribution. To this end, the authors use an additional embedding for the condition variables (class, image, text), and concatenate to the data embedding. The proposed metric, name the Fréchet joint distance (FJD), captures three desired properties of conditional generative models: sample quality, conditional consistency, and sample diversity. The authors demonstrate that the proposed metric indeed captures the properties using a synthetic (dSprite) dataset, and shows reasonable values for real datasets.\n\nPros:\n- FJD is an intuitive extension of FID for conditional generative models.\n- FJD can be applied to various types of conditions (e.g., image and text), which cannot be done by prior work (e.g., [1]).\n- The paper is easy to read and experimental details are clearly stated.\n\nCons:\n\n1. FJD is a straightforward extension of FID.\n\nFJD simply follows the FID formula but concatenates the condition embedding to the original data embedding. It is a straightforward extension of FID and suffers from the design choice problems due to the concatenation, as stated below.\n\n2. FJD requires many design choices and not theoretically justified.\n\nAs FJD requires an additional embedding function h, balancing parameter \\alpha, and merging function g, it raises a burden of design choices. While the authors give some suggestions, they are not theoretically justified. Also, one may use the statistical distances [2] between data distribution p_data(x,c) and model distribution p_g(x,c) to evaluate conditional generative models in a principled way, e.g., measure the KL-divergence using the density ratios [3]. The advantage of FJD over such metrics is unclear, as stated below.\n\n3. The advantage over the prior work is not clear.\n\nFJD and FID show the same trend in all reported experiments (Table 2, 3, 4), hence the advantage of FJD is unclear. Also, one may measure the FID score on conditional distributions, i.e., \\sum_c FID( p_data(x|c), p_g(x|c) ). It also captures the desired three properties and would be a strong baseline for FJD. Besides, while the authors aim to design a single metric to stand the models in a line, identifying the trade-offs of models may also be useful. For example, Improved PRD [4] provides the precision-recall trade-offs of generative models, which provides some insights for the models.\n\n\n[1] Ravuri and Vinyals. Classification Accuracy Score for Conditional Generative Models. NeurIPS 2019.\n[2] https://en.wikipedia.org/wiki/Statistical_distance\n[3] Uehara et al. Generative Adversarial Nets from a Density Ratio Estimation Perspective. arXiv 2016.\n[4] Kynkäänniemi et al. Improved Precision and Recall Metric for Assessing Generative Models. NeurIPS 2019."
        }
    ]
}