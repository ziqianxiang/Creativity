{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposes to train a classifier on top of edge detection (and, optionally, GAN-based texture filling-in) and claims to achieve robustness to adversarial examples, domain shifts and backdoor attacks using this approach.\n\nStrengths:\n+ The rationale to make nets rely more on shape features is justified\n+ The approach is relatively straightforward and easy to follow\n\nWeaknesses:\n- Solid tests for adversarial robustness are missing\n- Thresholding in the \"robust\" Canny seems to perform gradient masking\n- Use of GAN after edge detection is questionable (data processing inequality)\n\nDetailed comments:\n\n1.) My main concern with the paper is that – like many other papers claiming adversarial robustness – it does not perform a solid evaluation using a range of strong attacks. Specifically, the thresholding procedure in the \"robust\" Canny algorithm effectively performs gradient masking, as it sets the gradient to zero wherever the Canny operator is below threshold on the unperturbed image. Thus, gradient-based attacks like the ones used by the authors are not effective. Such manipulation, however, does not imply robustness! At the very least, the authors have to apply decision-based attacks like the boundary attack [1], which is trivial to apply [2].\n\n2.) It's not clear to me what purpose the GAN-based texture inpainting is meant to serve. If the GAN can learn to fill in textures that help discriminating classes, then the classifier should be able to use the same features in the edge image that the GAN uses (this statement follows from the data processing inequality).\n\n\n[1] Brendel, Rauber, Bethge, ICLR 2018: https://arxiv.org/abs/1712.04248\n[2] https://github.com/bethgelab/foolbox"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper aims at improving the robustness of CNN models for image and edge detection. They introduce a new edge detection approach (Robust Canny) which applies gradient thresholding to perform noise reduction on adversarial images. They also propose EdgeGANRob which is a variation of the previously proposed model (EdgeNetRob). Specifically, they train a GANS model following the work pix2pix to reconstruct the original content of the image which is then used to fill in the edges detected by EdgeGANRob. They finally evaluate their model on Fashion MNIST and CelebA, and show that it is more robust to different sources of image attacks like adversarial examples, distribution shifting, and backdoor attacks (inserting watermark patterns in the original image).\n\nAlthough the paper has some interesting contributions, I am still not convinced that the experimental results support the claims made in the paper:\n\nLooking at Table 3, it seems like EdgeGANRob is doing worse than EdgeNetRob specially under RadialKernel settings (for Fashion MNIST). CelebA has the same story: PAR and EdgeNetRob are outperforming others in some cases. A discussion explaining why these failures happen is missing from the paper. It does make sense to me that performance degrades under huge shifts in color since EdgeGANRob depends on the content of the image as well as the edges and shape.\n\nIn Table 4, although EdgeGANRob is doing great at detecting poisoned images, it comes at a cost of 4 to 5 percent drop in clean accuracy. An analysis of the results an understanding of what images EdgeGANRob is mispredicting compared to the Spectral Signature method would be helpful.\n\nOne of the key ideas of this work lies in untangling image content from its structure. Although authors make several references to previous works using edge features as a means of representing the structure, there is more to an image's style than just boundaries indicating sharp transitions in color (as shown in several works on style transfer learning such as https://arxiv.org/pdf/1705.04058.pdf). I'm curious to know if authors have done (or planning to do) any work using more features in their classification before inpainting them with texture information.\n\nOverall I like the work and think it has some potential to improve if the above concerns are addressed.\n\n\nMinor edit:\nIn the introduction on page 2, the line \"Using Robust Canny is able to EdgeNetRob dramatically improve the robustness of EdgeGANRob.\" should be changed to something like \"Using Robust Canny dramatically improves the robustness of EdgeGANRob.\"\n "
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "Paper summary: The authors propose to improve the robustness of neural networks by encouraging them to rely more on shape information (as opposed to texture). Specifically, the approach involves training classifiers on processed versions of the input image---either just the output of a (robust) edge detector in EdgeNetRob, or a combination of this with GAN-based infilling in EdgeGANRob. The authors evaluate these approaches in terms of clean and robust accuracy under the threat models of l-infinity adversarial attacks, backdoor attacks, and specific forms of distribution shift on the Fashion MNIST and (binary) CelebA classification tasks.\n\n\nComments: I find the high-level motivation of the paper compelling. As discussed in the paper, it has been found that there is a mismatch in the kind of features that deep networks rely on for classification, when compared to humans [Geirhos et al, 2019; Ilyas et al., 2019]. Moreover, it has also been shown that encouraging networks to rely more on shape cues may boost robustness to certain corruptions [Geirhos et al., 2019]. This paper aims to put forth a general framework to encourage networks to rely more on global structure in the input images.\n \nMy main concerns with the paper are as follows:\n \nA. The individual components of the approach are poorly motivated. \n \n1. In particular, it is not obvious that preprocessing using edge detection is enough to suppress adversarial perturbations/watermarks/distribution shift in general (more on this in B). The authors claim that edges are robust features, but do not back this up with sufficient evidence.\n\n2. The images generated post GAN-based infilling seem to have similar texture (and thus local structure) to the original input images. The authors do not provide sufficient explanation for why classifiers trained on this generated data would be any more robust (or more broadly, reliant on global structure) than classifiers trained on the original inputs.\n \nB. The evaluation of the proposed approach is not sufficiently convincing. \n \n1. Datasets: Experiments are performed only on Fashion MNIST and *binary* CelebA, which are a) not standard datasets in robustness literature and b) simple tasks. In particular, the fact that images produced by EdgeNetRob (basically the output of an edge detector) get comparable clean accuracy to training on the actual data (cf. Table 2) suggests that the classification task is just not hard enough. For instance, on typical deep learning datasets such as ImageNet, it seems improbable that we can get good accuracy using just edge information. Thus, the authors need to evaluate their approach on standard (harder) tasks such as CIFAR-10 or ImageNet classification.\n\n2. Choice of watermarks/distribution shift: What was the motivation behind the specific attacks considered in the backdoor attack section, and in the distribution shift sections? To me, these seem like specific attacks under which the proposed approach is more likely to succeed. \n- For instance, in the backdoor attack literature, the pattern is often small but somewhat perceptible [Gu et al, 2017: arXiv:1708.06733] and such a pattern would easily bypass the edge detector in EdgeGANRob. Thus, the performance reported in the paper seems somewhat specific to the attack considered (an imperceptible watermark) and would probably not generalize to patterns that are typically considered in the literature. \n- Similarly, the kinds of distribution shift that are considered are such that the edge information is preserved and thus may not provide a clear picture of the performance of the proposed defense. I would be curious to see how this approach performs in the face of other corruptions such as JPEG compression, fog, snow, etc. from the benchmark in Hendrycks and Dietterich [2019].\n\n3. Adversarial evaluation: The authors should report accuracy under black-box attacks and also white-box adversarial accuracy as a function of epsilon (e.g., for the Fashion MNIST model, plot adversarial accuracy as a function of the attacker eps from 0/256-256/256). In the Appendix, the authors mention images are scaled to [-1, 1], whereas in the robustness literature it is usually [0, 1]. This matters when you consider the eps used for attack and compare to other work: are all the networks in the paper including the adversarially trained ones also trained with images in this range?\n\n\nThus, while the problem the paper tries to tackle is an important and interesting one, I am not yet convinced about the effectiveness of the proposed approach. To make the empirical evaluation more rigorous and convincing, I think the authors should: \n\n(1) Repeat the adversarial robustness experiments on more commonly-used datasets such as ImageNet and CIFAR, and also evaluate robustness of their models more thoroughly, using techniques like black-box attacks.\n(2) Repeat the poisoning experiments with watermark patterns which are more standard in the literature.\n(3) Evaluate their approach on other forms of distribution shift such as the common corruptions benchmark from Hendrycks and Dietterich.\n\nAs of now, I am recommending rejection, but I would be willing to reconsider my score if the authors performed the above-mentioned experiments. \n"
        }
    ]
}