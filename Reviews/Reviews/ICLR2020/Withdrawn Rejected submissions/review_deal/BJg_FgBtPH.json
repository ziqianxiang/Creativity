{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposes a new suite of automatic evaluation metrics to measure the coherency, fluency and diversity of open-domain dialogue generation systems.  The authors leverage a strong language model (GPT-2) to assess fluency and coherency and use n-gram entropy of responses to rephrasals of the query input to measure diversity.  While there has been prior work in using language models to measure fluency, the authors demonstrate higher correlations than previous work, likely owing to the low perplexity of the language model used.  The proposed method to measure diversity is novel and addresses an outstanding issue with prior work: it allows one to measure diversity conditioned on the input query/prompt. The authors show that this metric correlates strongly with human judgments of diversity.\n  \n== Decision ==  \n\nWhile I like the ideas and execution in this paper, the paper is missing some key experiments needed to substantiate its claims and to validate the utility of the proposed metrics. I am leaning towards rejecting this paper.\n  \nMy biggest concern is that the method is only evaluated on a single system: a good evaluation metric (automatic or not) must have high correlation with human judgments for a variety of systems. Prior work, e.g. Chaganty et al. (2018) shows large variation in instance-level correlation depending on the target system. Do you have a sense for how these metrics behave on other models?\n  \nWhile the context coherence metric has relatively high correlation with human judgment, the plots in Figure 1 suggest that much of this correlation really comes from the fact that low scoring examples are indeed badly rated by humans, while higher scoring output has human scores across the spectrum. Novikova et al. (2017) and Chaganty et al.  (2018) observe this behavior as well and conclude that automatic metrics are great at identifying _bad_ generations, but are poor at distinguishing between good ones. How well does the context coherence metric correlate for examples that score greater than 0.5?\n  \nWhat instructions were annotators provided for each of these tasks?  Were they primed / provided instructions on what to rate as a 1 vs 5?  How much context is shown to annotators when evaluating context coherence: the previous utterance or the whole conversation history?\n  \nWhy is the human variance the highest for the baseline \"dataset\" in Table 6? If the claim is that most sets of responses are diverse if you do not control for the query set, then shouldn't most human responses agree on high diversity scores (reducing variance)?\n  \nCan you provide some examples of the diversity sets generated by the different methods (CTG in particular)? This is one of the papers core contributions, but it is hard to evaluate how good a technique it is without understanding how similar the context sets really are.\n  \nIt appears that the CTG method to generated controlled query sets consistently performs better than WS on Spearman correlation -- do you have an understanding of why this is the case?\n  \n== Additional comments ==  \n\nI highly recommend using violin plots to provide a more accurate picture of the distribution of examples at each human score (see Chaganty et al. (2018))\nSome additional references for using language models to evaluate fluency of generation include GLEU (Mutton et al., 2007) and ROUGE-LM (Kann et al., 2018).\n  \nThe use of \"dataset\" to describe the different controlled query sets was confusing to me -- I would choose a different word to describe them.\n  \n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "In this paper, the authors tackle the problem of automatic dialogue evaluation. They construct a method that relies on the recent GPT-2 language model, and divide their evaluation into three components: context coherence, fluency, and diversity. \n\nI really like that the authors are investigating automatic dialogue evaluation; I think it’s an important and under-studied problem. The paper is fairly well-written, and the three components of evaluation make sense. I like the proposed idea of measuring diversity via augmenting queries from the source dataset (although it requires access to the dialogue model parameters, this is usually feasible). \n\nUltimately, though, I’m not convinced by the paper’s empirical evaluation. Specifically, the paper only uses a single dialogue model (a simple seq-to-seq model with attention) for calculating their correlations, with different top-k sampling levels. This means that there is no indication of whether the proposed methods will apply to other dialogue models (an important criteria of an evaluation metric) or whether there is any model-level correlation between humans and these metrics. Further, the number of human ratings in the test set is quite small (200, 200, 500 respectively), and there are no error bars shown in the paper, making statistical significance a concern. \n\nI also have more foundational concerns with using purely unreferenced evaluation metrics in dialogue (i.e. metrics that aren’t trained on a dataset of (context, response, score) triples). We want to develop automated metrics that will be useful in driving progress in dialogue systems. However, unreferenced metrics have the same information available to them as the dialogue systems which they are evaluating. Thus, the only way that these unreferenced metrics will produce useful data is if the model it uses has been trained on significantly more data (or has much more capacity). My guess is that, in this paper, the correlation results are strong because the language model used for calculating the metrics (GPT-2 + fine-tuning) is much larger / has been trained on much more data than the dialogue response model (seq-to-seq + attention). I suspect the results would be different if comparisons were done on several GPT-2 based dialogue models. \n\nAnother way to think about it: if this metric were to be widely adopted by the dialogue community, one could simply create a dialogue generation model that uses GPT-2 + fine-tuning (and indeed, some researchers have already done so), and this would give maximum scores on these metrics (which are mostly the log-likelihood under GPT-2). \n\nGiven the above concerns, I cannot recommend acceptance in its current form, however I encourage the authors to continue working on this problem. \n\nOther questions:\n- How did the authors choose the 5th percentile for truncating the conditional log likelihood? \n- Given that context coherence is calculated via the log-likelihood under GPT-2, it should also capture things like response fluency. How much information is the response fluency (the unconditional log likelihood) really adding? \n- While the performance of each metric is measured independently, it would be interesting to see how well they worked in combination. \n\n\nSmall fixes:\nImpracticable -> impractical\nBLUE -> BLEU\nTable 1: this table is taken from Liu et al. (2016) without citation, please add the source\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors offer a suite of three automated metrics for evaluating generated responses from open-domain dialogue models. The metrics are claimed to be measures of coherence, fluency, and diversity, and are shown to correlate positively with human evaluations along the same axes. The authors plan to open-source the code for making evaluations, in a step towards standardizing the way dialogue models are measured and compared. \n\nI recommend the authors revise and resubmit to a conference like COLING, where such evaluation work will be a bit more welcome than a conference on learning representations.\n\nThe authors provide extensive analysis correlating their proposed metrics with human evaluations. The tables and graphs are informative and concise, and relay the information effectively. I found the comparison of inter-evaluation  between their metrics and human evaluators to be particularly helpful, as it did provide some better evidence. If spearman is that much higher than pearson though, the authors may have some issues with ties? It would be something to double check. The extensive graphs of correlations were also nice, but some of these graphs could be relegated to the appendix to assist with length issues (The authors barely extend to 9, which means they are supposed to have a harsher review). \n\nThe authors have a fundamental flaw that they don't seem to address: that their metrics can be pretty easily gamed by anyone that does some sort of gpt-2 fine tuning as their proposed model. In this way, their evaluation heavily depends on the ability to have a language model that is more powerful and stronger than the model it is evaluating. In fact, even gpt-2 big is far from producing extremely high quality generations, and so the authors would find themselves hitting that ceiling very easily. (The fact that they use the 12-layer GPT2 makes this even more problematic).\n\nThe authors also have a bit of an issue in that their coherence metric is not really fundamentally different than ppl of neural language models. The wordnet-substitute weighted ngram model is different, but really just harkens back to pre-deep learning language modeling.\n\nThe work would be made significantly more robust by having a *large* number of models being compared, rather than just one model evaluated on a number of utterances. Really the authors have only shown that they can evaluate *THIS ONE SEQ2SEQ MODEL*, and NOT general generations. I would not trust papers which would use this as their evaluation metric.\n\nMissing citation: probably CIDEr. They could also dig up an earlier reference for language modeling than Bengio 2003.\n\n\nNits\n- “BLEU” is mispelled as “BLUE” throughout the paper; “Softmax” is both capitalized and not.\n- The overall grammar of the paper is lacking, and impacts the legibility of the paper\n- It would be good to have tables on the same page as they are mentioned in the text.\n"
        }
    ]
}