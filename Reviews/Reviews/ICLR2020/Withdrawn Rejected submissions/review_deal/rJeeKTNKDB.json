{
    "Decision": {
        "decision": "Reject",
        "comment": "Two reviewers are negative on this paper while the other reviewer is slightly positive. Overall, the paper does not make the bar of ICLR. A reject is recommended.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper proposed a hierarchical graph-to-graph translation method to modify compounds to improve the biochemical properties. The authors proposed to generate the new molecular in a autoregressive manner. To improve the performance of the model, the input molecular is encoded into different resolutions including atom, attachment, and substructure layer. The paper is well written and the figures in the paper also enable the paper easy to read. In the experiments, the authors compare the proposed method with serval state-of-the-art methods. The results well analyzed and the ablation study is provided. Overall, this is a good paper considering its technical contribution and writing.\n\nHowever, there are some small issues should be addressed:\n\n1. There are some typos in the paper. For example, in the topological prediction section in page 3,  \"the hidden representation of $S_k$ learned be the decoder \" -> learned be encoder.\n\n2. In the training, the authors apply teacher forcing to the generation process with depth-first order. Why do you use depth-first order not any other orders?  \n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "The authors present a heirarchical graph-to-graph translation method for generating novel organic molecules.\nWorking from the model of Jin et al. (2019), the authors introduce a three step heirarchy - the model first determines where a substructure should be generated, what is the substructure, then the attachments to the existing molecule.\nAll steps of this uses embeddings generated from a message passing network - these embeddings are input into a few bilinear attention layers to obtain the heirarchical generation scheme.\nThe model is trained with molecular pairs (X, Y), and a VAE loss - a hidden z vector controls the way to modify X to improve its properties.\nThe encoder is just a MLP over the difference between sum of embeddings at a atom level and at the substructure level.\nThe model is evaluated on accuracy and diversity, in both conditional and unconditional settings.\nThe experiments show a small improvement over previous SOTA algorithms.\n\nThis is a borderline paper, and I'm leaning towards a weak reject, because I don't believe the model is well motivated enough:\n- Sec 3.1 it's unclear how the substructures are generated - they provide a lot of inductive bias for the algorithm. \n  Are they automatically generated or built from a database of substructures?\n- Variational decoding does not seem well motivated enough - would a stochastic decoding procedure not work as well as having a latent vector that essentially adds noise to the training?\n- The experiments seem interesting and comprehensive - it seems that the model learns to exploit the biases and increase logP, as well as showing the ability to conditionally turn off DRD2-active properties of the molecules.\n\nSome questions:\n- Why not use a Transformer instead of an LSTM or GRU? The cell naturally acts over sets of neighbors and transformers are a natural model to tackle this problem.\n- Sec 3.1 Topological Prediction, the attention is over c_{X}^{S} but the text claims it should be over c_{X}^{G}? Is ^G the attention substructure?\n- Sec 3.1 Attachment Layer MPN: the A_i seem to be a tuple (S_i, {v_j}). The set of attaching atoms is limited to 2 right? It might be more clear to simply enumerate them here if so.\n- Sec 3.1 Substructure Tree: Since tree decompositions are not unique, does this work use the different tree decompositions and DFS traversals as data augmentations?\n- Table 2b: What is a \"two-layer\" and \"one layer\" encoder? Is it the size of the MLP or the removal of the attachment MPNs?\n- Ablation study: Since the Attachment Layer has all the substructure information, this ablation should ideally make sure the models all have a similar number of parameters, and the decrease in performance isn't due to the decrease in parameters.\n\nNits:\n- Sec 3.1 \"bi-linear\" should not have a dash, bilinear is one word.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper developed a hierarchical graph-to-graph translation model to generate molecular graphs using chemical substructures as building blocks. In contrast to previous work, the proposed model is fully autoregressive and learns coherent multi-resolution representations. The experimental results show that the proposed method outperforms previous models.\n\nA few comments: \n\n1.The novelty\n- The method seems to be almost the same as the previous junction tree based formulation.  The paper includes a straightforward hierarchical extension and provides limited novelty with respect to deep learning.  \n- Can the method be used for other types of graph generation? \n\n2. Some minor wording issues\n- For instance, in the abstract, \" In particular, we realize coherent multi-resolution representations ..\" What does this mean? \n\n3. The main claim : \" ... our graph decoder is fully autoregressive..\"  why is this a merit? \n\n4.  The paper provided results from multiple molecular optimization tasks. The results and analysis seem comprehensive. The model was shown to significantly outperform baseline methods in discovering molecules with desired properties. The model runs faster during decoding and can perform conditional translation. \n "
        }
    ]
}