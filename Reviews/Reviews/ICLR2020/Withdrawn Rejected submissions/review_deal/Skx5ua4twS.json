{
    "Decision": "",
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #643",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper uses a distillation method on the biaffine dependency parser to reduce the model parameters and parsing time. \n\nTwo obvious drawbacks:\n1.1 The method used is not novel. Knowledge distillation is absolutely not a new idea and has been widely used in NLP tasks. There is no comparison with existing work about distillation. It seems that this work directly borrows the methodology.\n\n1.2 Although speed of the parser can be increased by distilling, accuracy of the parser is injured too much that nearly 1 point on UAS compared with Biaffine baseline.\n\nModeling:\n2.1 For the technical part, this work only consider minimizing the loss between teacher and student distributions. In other words, it only takes advantage of “what the teacher is capable of”, just like in previous literatures. However, “what the teacher is not capable of” also matters. Maybe the authors can think about in this aspect, e.g., introducing a contrastive loss to penalize the wrong predictions of the teacher model, to improve the technical contribution.\n\n\nEvaluations:\n3.1 This paper only tests the distilling method on Biaffine parser which is not sufficient. It would be better to try other types of dependency parsers.\n\n3.2 The authors did not use FP32 or FP16 in the model training and inference stages. \n1. Train the model in full precision mode and inference in FP32 or FP16 mode,\n2. Training and inference in FP32 or FP16 mode.\nI expect to see such a baseline comparison.\n\n3.3 Compared to directly reducing the hidden layer dimension, there are other things such as reducing the number of hidden layers, reducing the size of the input embedding, maintaining or even expanding the hidden layer state (the size of input embedding is the main part of the model parameters), and layer parameter sharing. \nI expect to see the above mentioned empirical studies.\n"
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper applies model distillation to the Biaffine dependency parser (Dozat & Manning, 2016), showing that parsing speed and model size can be substantially improved without significant loss in performance. The paper is generally clearly written (though see comments below), and the results are generally convincing. On the other hand, there is not a lot of content, and not much novelty. Overall I am slightly positive towards this paper, mostly because of the important topic and the clean experimentation, in particular with multiple languages.\n\nComments:\n1. While the writing is fairly clear, the tables and graphs can be improved:\n- the upper part of table 1 is misleading. As the authors rightfully note, the speed is not comparable between different reported numbers due to architecture differences, etc. I would recommend showing only the two bottom parts, or reproduce some of the other papers on the same architecture. \n- The fonts are very small in all graphs, making them hard to understand.\n- Figure 3 is confusing (especially given the small font). It might be preferable to make the baseline parser the baseline numbers on which the proposed approach and the small model are compared against, instead of making the proposed method the baseline. \n\n2. I would appreciate more details about how the model was compressed to X%. The authors mention reducing the number of dimensions of each layer, but more details would be helpful.\n\n3. As the authors rightfully notice, their favorable results in Figure 2 are almost exclusively attributed to improved performance on Tamil. Adding standard deviations to Figure 2 would help appreciate the observed trends. Moreover, while experimenting with 8 languages is beyond standard practice in NLP, it seems adding more languages, in particular some with small datasets such as Tamil, would help better appreciate the nature of the proposed approach."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper proposes to increase the efficiency of dependency parsers, in terms of both number of parameters and runtime speed, through knowledge distillation. More concretely, this paper distills the graph-based parser of Dozat and Manning (2017), into the same model architecture, albeit with fewer parameters than the original model. \n\nThe distillation objective is done locally on the distribution of the possible heads and corresponding arc labels for each token, using the standard distillation loss (i.e. minimising the KL divergence between the teacher's and the student's probability distributions). This distillation loss is then interpolated with the standard cross-entropy loss with respect to the gold labels. On a subset of the UD corpus, the approach leads to distilled parsers that are 1.21-2.26x as fast as the teacher model at test-time, with minimal performance loss (~1% in UAS/LAS decrease when using the fastest student model), and achieve slightly better accuracy than the teacher model when using the biggest student (80% of the teacher's parameters).\n\nOverall, despite the encouraging empirical findings and depth of analysis, I have several major concerns regarding the work, as listed below. Given these concerns, I am recommending a \"Weak Reject\" ahead of the authors' response.\n\n1. The paper fails to compare with, or even mention, highly relevant prior work on distilling dependency parsers (Kuncoro et al., 2016; Liu et al., 2018; full reference below). These two papers featured methods to extend the distillation objective to structured objects (trees in this case). In contrast, this paper is methodologically much more straightforward as it only performs distillation at the local word level, thus disregarding the global structural dependencies. \n\n2. Related to Point 1 above, the technical contributions of this paper are limited. The methods are very straightforward and are not particularly creative applications of knowledge distillation to dependency parsing, while the findings are also unsurprising. As knowledge distillation has been shown to work on various problems, it is not surprising that distillation also works for dependency parsing. The decoding speed-up is also relatively minor on GPU (1.21x, even when using the fastest, least accurate student model).\n\n3. Despite the reduction in the number of overall parameters, I am not entirely convinced of the \"greener\" aspect of the approach. The main reason is that training the distilled parser is a two-stage process, where one must first train the carbon-expensive teacher model, and then repeat the training process for the smaller student model. Naturally this two-stage training process results in more carbon footprint at training time, and training neural networks are known to be computationally expensive as they require multiple epochs on the same dataset. It is unclear whether the decreased carbon footprint by using the smaller model at decoding time offsets the increased carbon footprint at training time. In short, smaller number of parameters are not the whole story when it comes to carbon footprint.\n\nMinor questions and suggestions:\n1. I suggest using bold or underline to highlight the best and fastest parsers on Table 1, which would make the table easier to read.\n\n2. \"Bucilu et al., (2006)\" in Page 3 should be \"Bucila et al., (2006)\". The corresponding entry on the References should also be modified accordingly.\n\n3. Are the losses weighted equally in Eq. (3), or were there any interpolation factors used to balance the four loss terms?\n\n4. The Dozat and Manning entry on the reference should be in ICLR 2017 rather than ICLR 2016 (the ArXiv version was published in 2016).\n\nReferences:\n\nTimothy Dozat and Christopher D Manning. Deep biaffine attention for neural dependency parsing. In Proc. of ICLR 2017.\n\nAdhiguna Kuncoro, Miguel Ballesteros, Lingpeng Kong, Chris Dyer, and Noah A Smith. Distilling an ensemble of greedy dependency parsers into one MST parser. In Proc. of EMNLP 2016.\n\nYijia Liu, Wanxiang Che, Huaipeng Zhao, Bing Qin, and Ting Liu. Distilling knowledge for search-based structured prediction. In Proc. of ACL 2018.\n\n"
        }
    ]
}