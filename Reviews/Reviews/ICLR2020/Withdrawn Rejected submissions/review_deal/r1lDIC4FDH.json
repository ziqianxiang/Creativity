{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary\n\nThis paper proposed a memory-augmented neural network inspired by the human cognitive process. Specifically, the paper has 3 model contributions: A modification to LSTM cell, a 2 layer LSTM architecture where the second layer is updated only when the mask in the lower layer is ON, an external memory which is accessed in a sparse way, again only when the mask is ON. Authors show that the proposed architecture is better than LSTM and NTM is a series of synthetic tasks.\n\nMy comments:\n\nOverall it is an interesting idea. However, there are several issues that need to be addressed before the paper is ready for publication. I will accept this paper only if all these changes (suggested below) are made in the revision.\n\n1. Lipton et al. 2015 is not the reference for LSTM! Please give the correct citation.\n2. All the citations are wrongly formatted. Please use citet, citep appropriately.\n3. There are several related works that are not discussed in the paper. The idea of using a mask in the first layer to update second layer is very similar to Chung et al. 2016 (Hierarchical Multiscale Recurrent Neural Networks). Also the sparsity in memory access has already been explored in Rae et al. 2016 (SAM, Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes), Gulcehre et al. 2016 (D-NTM, Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes), and Gulcehre et al. 2017 (TARDIS, Memory Augmented Neural Networks with Wormhole Connections). Even though you don’t compare with them, you should at least discuss the relations.\n4. In section 2.1, you spend more space explaining chrono-initialization than explaining your contribution. You mention your contribution only in the last few lines. I expect more discussion on why you choose to modify the LSTM cell.\n5. Is it right to say that if you remove the mask, then the entire model is exactly the same as NTM? I would like to see more experimental analyses on the sparsity constraint in the mask. What happens if the constraint is not very strict?\n6. Section 2.3 is too verbal. I cannot implement the model by reading this description. Please make it more precise.\n7. The paper will get more clarity by adding the pseudocode of the model operation.\n8. In section 3.1, isn’t d_i(n) = n-i ?\n9. Section 3.1 is a general discussion of the advantage of using external memory. This does not help to differentiate your model from NTM, D-NTM, or TARDIS.\n10. Why not multiple runs for standard LSTM? The comparison is not fair. I would like to see same number of runs for both models.\n11. Table 1 is not precise. What is the metric for each column? Is higher the better or lower the better? I can guess all this. But it has to be explicit in the table. Also, it does not mention the length of the copy task.\n12. Why is ADD task results missing in table 4?\n13. Are the authors willing to release the code to reproduce their results?\n\nMinor comments:\n\n1. Page 1: last para: fix “modal model”\n2. “neural turing machine” should be “neural Turing machine” .\n3. Lines before section 2.2: “fitter” or “filter”?\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Contributions:\nThis paper describes modifications to the MANN setup for sequence learning. The modifications include a variant of the LSTM controller which includes constrained outputs, and a trainable mask which is use to compute a boolean for each timestep indicating whether or not to perform costly updates to an external memory at that timestep. When trained on a certain problem, the mask becomes sparse (encouraged by L1 regularisation) to mean that far fewer memory updates are performed, with corresponding runtime benefits and keeping approximately the same performance.\n\nMotivation: I find the entire idea of learning an (apparently) non-input dependent mask of which timesteps to do memory access on very questionable - a) this seems to impose a fixed sequence length, which means the many domains on which RNNs are applied to arbitrary sequences (language, reinforcement learning, ...) are effectively excluded. b) even if this was not the case - why does it make sense to learn this as part of the trainable weights? Surely the sensible thing, which would match human cognition, would be that depending on the current input, plus previous state, we decide whether to perform an expensive memory update. If you decided ahead of time \"It's good to perform memory updates on timesteps 1, 5, 7 and 11\" you have surely done nothing except overfit to the specific frequency characteristics of your data - in my view this severely limits the applicability of RNNs, whereas the paper reads like this is an across the board improvement - \"the two components are demonstrated to behave better than their counterparts, and the model with an external memory improves further and outperforms the neural turing machine with less memory access.\" I strongly feel the paper needs to address this limitation. If the entire argument was that \"if you run MANNs on fixed sequence length problems, this technique could be good\" that would be quite different.\n\nThe authors do cite a good variety of recent relevant work, although some papers which would be good baselines are cited but not actually compared against. In particular I feel Cached Uniform Writing (Le et al 2019) is doing something very similar to this, but without aspects of this paper that I feel are problematic (see below).\n\n\nDecision: reject. The paper is not clear or easy to read, and the underlying technique does not have obvious utility to me (see above about fixed length mask). The experiments on Add, MNIST and pMNIST do not sufficiently convince me that this is practically useful - the easiest things to add for a resubmission would be something like Penn Treebank, even if RNNs are no longer SOTA on that dataset it would still be good to see the uplift of this method compared to regular LSTMs. I suspect the fact that PTB is inherently an ongong sequence of characters instead of fixed size chunks (like MNIST etc) would mean this method no longer makes much sense.\n\n\n\nDiscussion:\n\nThe access to the external memory in section 3.2 is too concise - the DNC is cited but the description sounds like a cut-down DNC, without the link matrix or any kind of location based addressing, meaning the memory used is possibly even less capable than the original NTM.\n\nThe section on theoretical advantages of SMANN is quite unclear. To take one example, \"The network is better in memory capacity when the mean path length spanning all time steps is shorter\" - I feel the memory capacity of a MANN is just how many scalars it can store (ie a function of memory size / word size, to use NTM terminology). It feels like the argument here is that without the SMANN variants, the memory capacity _you can learn to make use of with BPTT_ is better when the mean path length is shorter - but that's a totally different point. A diagram would really help to illustrate the concepts of path length and the other forms introduced - in particular the $[d]_{RNN}$ notation is a bit strange. I assume this just means \"evaluate the term inside the brackets in the context of an RNN\" but this is not clearly explained.\n\n\n\n\"network architecture should be refined so as to simulate human cognitive process more precisely\" - this is highly debateable, it would be fine to try and argue this but the paper presents it as if it is a fact that all practitioners would agree on.\n\n\nEquation 1 - It wasn't clear to me on first reading that this constrained output LSTM (differing in the equation for $h_t$) is one of the contributions of the paper - from reading the abstract it could be interpreted as the authors are using this existing model variant. Either way the description right at the end of section 2.1 which starts and ends with \"thus we use a double tanh function ... assign weights on the rest ones\" is very unclear to me. Skipping to section 4.1 which is advertised to contain insights about this variant - the section contains some results which are unclear (are you doing 5 runs *each* for regular LSTM and your variant? If so, it is not clear which dotted line corresponds to which model. If not, then are there different numbers of runs per model? It should be simple to just run 5 for each, and only plot 2 lines with error bars), but there is no insight as to _why_ this is sensible variation to the model. Ideas like initializing the forget gate bias in a certain way are backed up with explanations about letting gradients flow through time easier - why should constrained the output of an LSTM (by introducing more trainable variables & computation) be a good thing? If the results in figure 2 showed a huge gap between the models that could mean no explanation is needed, but on the right hand side of the graphs the difference between the bold lines is marginal.\n\n\nMinor points:\n\nFigure 1: I think it would be more consistent with other work to show external memory as containing rows, not columns. Do the red/green/purple colouring of the circular nodes have any meaning that could not already be gathered from the diagram? It's not super obvious why when memory access is enabled, both the controller cores are red, but when it's not enabled they are different colours. It might also make thngs clearer to have read/write arrows coming from the later timestep where access is enabled.\n\nTop of page 5: \"Bengio et al. Bengio et al. (1994)\" - as far as I can tell there is only one Bengio paper from 1994 in your references.\n\n\"In the add task, the networks are feed with\" -> \"In the add task, the networks are fed with\"\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper presents a memory-augmented network for long-term sequence modeling. The major novelty is a new constrained LSTM structure with a learned binary mask, which is particularly designed to solve the problem of intensive memory access of previous MANNs.\n\n1. It is reasonable to learn a sparse binary gate to reduce the frequency of memory access, but the authors did not further demonstrate its benefit through experiments. I suggest comparing the computational cost of the proposed model with that of the previous MANNs.\n\n2. I find the formulation in Eq. (1) not fully motivated. May I ask for what reason the authors used the trainable vector u and the diagonal matrix D?\n\n3. One contribution is this work is to use the chrono-initializer in the constrained LSTM. I suggest the authors add a reference when it is mentioned for the first time.\n\n4. Some related work in memory-augmented LSTMs was not mentioned or compared with, e.g. RMC [Santoro et al. 2018] and E3D-LSTM [Wang et al. 2019].\n[Santoro et al. 2018] Relational recurrent neural networks.\n[Wang et al. 2019] E3D-LSTM: A Model for Video Prediction and Beyond.\n\n5. Section 2.3 can be more self-contained to include more details of the memory read and write processes."
        }
    ]
}