{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "Summary:\nThis paper proposed an ensemble method (SEERL) for model-free RL algorithms. The main advantage of the SEERL is that the training cost is small compared to other ensemble methods with multiple neural networks. SEERL proposed a dynamic learning rate schedule to generate multiple variants of policies during the training of a single instance.  The proposed method is applicable to both discrete action space tasks and continuous action space tasks.  \n\nDetailed comments:\n\n*Methodology:\nIt is not clear whether Eq 2 represents the training loss of the proposed method. Eq 3 is an indicator function, which has zero gradients almost everywhere.  The only useful term is the KL divergence term if the policies are trained with Eq 2. Does the author mean L(s, a) = L'(s, a) if the \"total error\" is greater than the threshold?\n\nThe motivation of using Eq 2 as training loss is not clearly justified or empirically demonstrated in the ablation study. What is the reason for using a threshold to select which policy to train? Is this loss shown to be more effective than just training the policies with the typical actor-critic loss?\n\nOne of the contributions claimed is the optimization framework for selecting the best policy. However, the ensemble approaches for both discrete action space (Sec 4.3.1) and continuous action space (Sec 4.3.2) are heuristic approaches. It is not clear how to justify those ensemble approaches are optimizing any objectives. \n\nTheoretically, the actor-critic/policy gradient approach is an on-policy algorithm, which means the sampled trajectory via behavior policy cannot be readily used for training current learning policy, without using techniques such as importance sampling. It is compatible with SAC since off-policy training can be directly applied, while the discussion on this issue has not been discussed. \n\n*Experiments:\nAccording to the reported results, the performance of SEERL is volatile. It fluctuates a lot comparing to the baseline approach.\n\nThe reported results do not seem to be sufficient to support the effectiveness of the proposed method. The baselines such as DDPG/TRPO/A2C are not state-of-the-art in terms of sample-efficiency. The comparison with SAC is valid.\n\n*Minor:\nEq 5. The definition of advantage function, missing a bracket, etc.\nParagraph 1, Section 5, citations of the baselines."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "### Summary ###\n\nThis paper focuses on training an ensemble of policies in the reinforcement learning setting. Specifically, the author proposes a framework to train a collection of policies without the extra computation cost and sample complexity, and a method of selecting the best policies for ensemble.\n\nMost existing ensemble methods in reinforcement learning suffers from increased sample complexity or computation cost that scales with the number of agents being trained. The authors propose a method that uses cyclic learning rate to obtain multiple policies in a single training run. Specifically, the authors adjust the learning rate in a cyclic fashion, and save the policies each time the learning rate reaches its minima to obtain a collection of policies.\n\nThe authors then propose a way of selecting the top M policies for ensemble. The authors formulate the policy selection into a constrained optimization problem, where the cost function consists of a policy loss term that encourages the selection of well performing policies, and a divergence term that encourages the selection of diverse policies. After selecting the top policies, the authors use various ensemble techniques to get the final action from the policies.\n\nThe authors evaluate the proposed algorithm on the domain of simulated robot locomotion in MuJoCo and Atari game. The authors observe improved performance on some environments compared to the baseline.\n\n\n### Review ###\n\nOverall I think this paper presents an interesting idea in training an ensemble of polices without extra cost. The idea is very well presented and authors include many empirical evidence to support the proposed method. However I do find a number of shortcomings that need to be addressed.\n\nPro:\n1. The idea for this paper is really well presented. The structure of the paper is well organized and  the authors include informative illustrations to help demonstrate the idea.\n\n2. The authors conduct a wide range of empirical experiments to support the effect of the idea. The authors test the idea on environments with both continuous action spaces and discrete action spaces. In the appendix, the authors include a fairly comprehensive evaluations of the proposed algorithm in 39 Atari environments and 6 MuJoCo environments. The authors also evaluate the proposed ensemble mechanism on top of multiple baseline algorithms.\n\nCon:\n1. I am not convinced about the benefit of the ensemble method as in many experiments presented in the paper, the baselines outperform the proposed method. In 7 of the 39 Atari environments, the baseline outperforms the ensemble method, and in 20 of the 39 environments, the baseline seems to be on par with the ensemble method. Therefore, it is not convincing that the proposed ensemble method really has an advantage over baselines.\n\n2. In many of the experiments, the baseline algorithms do not seem to be well-tuned and do not match the performance from the original papers. For example, in the MuJoCo experiments where the authors use SAC[1] as baseline, performance on 5 of the 6 environments has been reported in the SAC paper[1] and none of SAC baselines in this paper matches the performance in the SAC paper. In fact, the gap of performance is pretty large. Therefore, I would not trust the comparisons before the baseline algorithms are properly tuned and re-evaluated.\n\n3. For the Atari experiments, it would be better if the authors could compare to more recent baselines such as IQN[2] and Rainbow[3]. These algorithms perform significantly better than A2C so it is important to include these results.\n\n4. The choice of the optimization objective (equation 2) for ranking the policies seems a little arbitrary to me. Especially in many RL algorithms, the policy gradient loss for the policy and Bellman error for the value function are not normalized and therefore cannot be simply added together. It would help if the authors could provide some justifications or theoretical analysis to this ranking rule.\n\nThe idea in the paper is well presented and the authors include comprehensive experiment results. However, due to the mismatch between baseline algorithm performance presented in this paper and those presented in other papers, I’m not convinced about the validity of many empirical results in this paper. Therefore, I would not recommend acceptance before these problems are addressed. \n\n\n\nReferences\n\n[1] Haarnoja, Tuomas, et al. \"Soft actor-critic algorithms and applications.\" arXiv preprint arXiv:1812.05905 (2018).\n\n[2] Dabney, Will, et al. \"Implicit quantile networks for distributional reinforcement learning.\" arXiv preprint arXiv:1806.06923 (2018).\n\n[3] Hessel, Matteo, et al. \"Rainbow: Combining improvements in deep reinforcement learning.\" Thirty-Second AAAI Conference on Artificial Intelligence. 2018.\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The paper presents an ensemble method for reinforcement learning. Multiple policies conforming the ensemble are obtained by checkpointing a training policy at different times. The training policy is optimized by a “single-thread” training procedure and a particular learning rate scheduling. The learning rate is cyclic (increasing and decreasing) and it is shown to be beneficial for obtaining high performance. The multiple policies from the ensemble are further filtered by a selection criteria during evaluation. Finally, the selected policies are used for action selection. The authors perform experiments showing the performance of their algorithm compared to some baselines.\n\nMy decision would be to reject this paper. The main reasons for this are: 1) the main contribution of the paper (page 4)  is poorly explained and lacks motivation. 2) The experiments although extensive in number of environments (for training) are poorly evaluated.\n\nSupporting arguments: \n\nEquation 2 needs more motivation. Why the authors have selected specifically such objective function? They could have chosen any other arbitrary loss function that enables ensemble diversity in a different way. Why the use of the L(s,a) and L’(s,a)? They also introduce several parameters here that might make difficult to tune the algorithm e.g. beta, M, threshold. How do the authors compute P(s) ?  \n\nThe experiments are poorly evaluated in the following sense: \n\n- The authors provide in Figure 3 the results from evaluation of SEERL against the chosen baselines. However, in this Figure only 2 environments are evaluated. All the remaining Figures focus on the training performance (including the ones in the appendix) or unrelated to performance (Figure 4) .   Evaluation is really where one can see if a method is better or worse compared to another method. For example, see original DQN paper (Mnih 2015) to see a sensible evaluation procedure. Since  evaluation is the most important experimental analysis, the authors should devote their efforts in presenting a strong evaluation, even more, since their method highly relies on evaluation given that the ensemble consensus is only executed during evaluation (see Algorithm 1).\nAlthough many training results have been shown (see appendix) not many evaluation results are present. I believe this is crucial to show if their method is truly better than the proposed baselines.\n\n- Looking at training results SEERL seems to be highly unstable depending on hyperparameters and random seeds. For example, Figure 5(a) shows how sensitive the performance is when changing the number of policies in the ensemble. Similarly, in Figure 5 (b) one can see how much the performance varies by  slight changes in the initial learning rate. Furthermore, the training results in Figure 6 and 7 present plateaus in performance (as if the training procedure would have get stuck) and extreme drops in performance. Additionally, Figure 8, 9 and 10 show that SEERL is roughly under-performant in 6 out of 11 environments. \n\n- I believe the authors should also compare to standard baselines (e.g.  simple A2C / TRPO / PPO / SAC without ensemble).\n\n- Figure 2 (b) has no axis scale and therefore is of no use and cannot be understood.\n\n- How are the authors computing the average over P(s) in Equation (2)?\n\n\n\nMinor comments:\n\n- “However, value function ensembles from different algorithms trained independently could degrade performance as they tend to converge to different fixed points and thereby have different bias and variance”. This is only true if the objective is different even if the algorithms are the same.\n\n- The formatting of the paper seems to not follow the template since there seems to be no space between paragraphs (other ICLR papers do have such space).\n\n- The notation in Preliminaries is abused. P refers to transition dynamics but it is also used in P(A) for probability. Also, the \\epsilon is used instead of \\in. The log in equation 5 should be \\log. The advantage function in (5) is not defined.\n"
        }
    ]
}