{
    "Decision": {
        "decision": "Reject",
        "comment": "The submission proposes to train a model to modify objects in an image using language (the modified image is the effect of an action). The model combines CNN, RNN, Relation Nets and GAN and is trained and evaluated on synthetic data, with some examples of results on real images.\n\nThe paper received relatively low scores (1 reject and 2 weak rejects).  The authors did not provide any responses to the reviews and did not revise their submission.  Thus there was no reviewer discussion and the scores remained unchanged.\n\nThe reviewers all agreed that the submission addressed an interesting task, but there was no special insight in how the components were put together, and the work was limited in the experimental results.  Comparisons against additional baselines (AE, VAE), and ablation studies or examinations of how the components can be varied is needed.\n\nThe paper is currently too weak to be accepted at ICLR.  The authors are encouraged to improve their evaluation and resubmit to an appropriate venue.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "1. The paper aims to train a model to move objects in an image using language. For instance, an image with a red cube and blue ball needs to be turned into an image  with a red cube and red ball if asked to \"replace the red cube with a blue ball\". The task itself is interesting as it aims to modify system behavior through language. \n\nThe approach the authors  take is to encode an image with a CNN, encode the sentence with an RNN and use both representations to reconstruct  the frame (via a relational network and decoder) that solves this task. This process as described was already done in (Santoro 2017). The idea of using a CNN feature map and LSTM embedding to solve spatial reasoning tasks is not new. \n\nThe main contribution is to add a discriminative loss to turn the problem into a \"is this solution correct or not.\" This is interesting but does not perform much better than the baseline of not using the GAN loss (as suggested by the results in Table 2). This suggests that the GAN term is not adding as much value as the authors claim.\n\n2. Reject\n- Reason 1: The results in Table 2 show that the GAN does slightly better (0.0134 vs 0.0144) in RMSE against the non-GAN version. This improvement does not seem statistically significant enough to warrant the added GAN complexity.\n- Reason 2: Other baselines need to be considered, AE, VAE or other variations.\n- Reason 3: No ablations on the impact of the parameters to eq 1.\n\n3. To improve the paper I suggest adding other baselines such as VAE, AE. In addition, consider using more negative samples instead of the single negative image."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposes an architecture for generating images with objects manipulated according to user-specified or conditional instructions. I find the domain very interesting and do believe that tasks like these are critical for learning human-like cognitive capabilities.\n\nThis paper is also very clear and easy to follow and understand what the authors have done.\n\nBut I do feel like this work could use more polishing. There are four components that are used in this work, CVAE, LSTM, RN, and GANs. It seems that those components are all taken straight out of the shelf and combined. It would be interesting to see what subtle changes were important in a combined system to further increase performance.\nFor example, why is RN only before decoding, could RN possibly help the decoder as well?\n\nWhat are some of the most frequent failure cases?\nThe qualitative results look reasonable, and Iâ€™m quite surprised that only 10K images were used for training. Improvements in which areas would lead to perfect results?\n\nWould better performance be obtained if every module were to be trained separately first, rather than the proposed end-to-end approach?\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a model that takes an image and a sentence as input, where the sentence is an instruction to manipulate objects in the scene, and outputs another image which shows the scene after manipulation. The model is an integration of CNN, RNN, Relation Nets, and GAN. The results are mostly on synthetic data, though the authors also included some results on real images toward the end.\n\nThis paper, despite studying an interesting problem, is limited in terms of its technical innovations and experimental results. My recommendation is a clear reject.\n\nThe model is simply an integration of multiple standard neural nets. To me, it's unclear how the system can inspire future research. Such an integration of neural nets won't generalize well. The authors have to pre-process real images in a very specific way for limited sim-to-real transfer. It's unclear how the model can work on more complex images, nor to mention scenes or sentences (or actions) beyond those available during training. \n\nThe experimental setup is very simple. The model is tested on scenes with a clean background and a few geometric primitives. There are only four actions involved. There are no comparisons with published, SOTA methods. All experiments are with the ablated model itself. Considering all this, I believe this paper cannot be accepted to a top conference such as ICLR.\n\n"
        }
    ]
}