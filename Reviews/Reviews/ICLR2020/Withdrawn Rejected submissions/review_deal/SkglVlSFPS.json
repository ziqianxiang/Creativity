{
    "Decision": {
        "decision": "Reject",
        "comment": "The authors study planning problems with sparse rewards.                                                                           \nThey propose a tree search algorithm together with an ensemble of value                                                            \nfunctions to guide exploration in this setting.                                                                                    \nThe value predictions from the ensemble are combined in a risk sensitive way,                                                      \ntherefore biasing the search towards states with high uncertainty in value                                                         \nprediction.                                                                                                                        \nThe approach is applied to several grid-world environments.                                                                        \n                                                                                                                                   \nThe reviewers mostly criticized the presentation of the material, in particular                                                    \nthat the paper provided insufficient details on the proposed                                                                       \nmethod. Furthermore, the comparison to model-free RL methods was deemed somewhat                                                   \nlacking, as the proposed algorithm has access to the ground truth model.                                                           \nThe authors improved the manuscript in the rebuttal.                                                                               \n                                                                                                                                   \nBased on the reviews and my own reading I think that the paper in it's current                                                     \nform is below acceptance threshold. However, with further improved presentation                                                    \nand baselines for the experiments, this has potential to be an important contribution.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Uncertainty-Sensitive Learning and Planning with Ensembles\n=====================================================\n\nThis paper investigates the use of uncertainty-aware estimates in solving planning problems (RL with access to simulator).\nThe proposed algorithm combines a learned model-free value estimate with MCTS planning.\nAn ensemble of neural networks is used to model posterior uncertainty in the value estimate and drive efficient exploration.\n\n\nThere are several things to like about this paper:\n- The paper takes on several core issues in RL/planning research, most notably the synthesis of dealing with model-based and model-free uncertainty in RL.\n- The general flavour of the paper + algorithm seems to be reasonable. The proposal to use ensemble uncertainty estimates to drive model-based MCTS is interesting, natural, and I think it's a good one.\n- The proposed structure of the paper is quite nice, there is mostly a linear and logical progression of complexity in the experiments. This is nice to see clear benefits of the approach on the simplest possible settings and build up from there.\n- The effort to open source code + implementation details is laudable.\n\nHowever, there are several places where this paper falls short:\n- In general, the claims and results of the paper are far too vague to be fully understood and replicated. Take the main algorithm 1, it really seems like more of a \"sketch\" of a very general family of algorithms, rather than a specific description of a clear algorithm.\n- This vagueness is spread throughout the plots and figures as well... note that Figure 1 has no indication of how many steps have been evaluated, and Figure 2 has no indication for what value K > 0 was actually used. The clarity does not improve in Sections 3.2 and 3.3 where quite inconsistent performance metrics and presentations are presented.\n- Generally, the writing could be tightened quite a lot. In particular I would encourage you to think about whether each statement you make is clearly supported by some theorem, experiment or plot in your paper. For example, on page 3 \"We found this mechanism to be beneficial... see Section 3.3\" but then it's not clear exactly what statement shows that particular part of the mechanism was helpful, versus other issues associated with ensemble learning. There are more than a few typos... the on(e) in Osband... akin to ??... might be obtained by choosing from (the) ensemble...\n- It would be very helpful to clarify that the agent is given access to a simulator... so that this is not exactly the typical RL setting of sequential decision making. This should appear early in the paper.\n- The code that is released with the paper is also quite confusing, it is not structured with a clear README and includes many sections of dead/commented code. I was hoping the code might rescue some of the clarity, but I think that still needs work.\n\nOverall, I do think there is some interesting material here...\nIt's an important problem, and the core building blocks of combining model, value and uncertainty for better exploration is interesting.\nHowever, I just think the actual paper is not clear enough on the details.\nMy belief is that going through this paper very methodically and carefully to make sure that every single detail + claim is rigorously supported would help this paper immensely.\nFor that reason I have to say that I think it's a \"reject\" in its current form."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #4",
            "review": "The authors propose to combine planning methods like MCTS with an ensemble of value functions to a) estimate the value of leaf nodes of the search tree and b) use the ensemble estimate of uncertainty to guide exploration during MCTS search. \nThe MCTS rollouts are also used as optimization targets for the value function.\n\nI believe this is a clear reject. On the one hand, the paper needs signficiantly more work on the writing and clarity. On the other hand I have several worries on the method and evaluation side. \n\nRegarding the presentation of the paper:\nOverall, the paper seems quite rushed. This is not a strong reason for rejection but should be improved in a future version. For example, punctuation and sentence structure is often wrong, the paper has only slighlty over 7 pages, a citation is undefined on p.7 and images and whitespace is formatted wrongly on occasion (e.g. top of page 6).\nMore importantly, on the content side, the experimental section is sufficiently clear and well written, however, the method description needs more detail and background information. The paper relies on several prior works which are referred to but not described (E.g. MCTS , the sampling mechanism by Osband et al. which they are using but not describing, the 'mask' from Osband et al which they are using but not describing).\nFurthermore, the algorithm itself is not described in sufficient detail:\n- How does the 'soft-penalization' work?\n- How exactly does the mechanism \"similar in fashing to\" Thomson sampling work?\n- Are you learning a model or do you have access to the true transition function?\n\nRegarding the method:\nI can't say anything definitive about the method as I'm not entirely clear how exactly it works. However, I have several worries that might need addressing:\n- It seems to me that the method relies on access to the _true_ transition and reward function and not on a learned model. This is a big difference to much of the prior work they compare against. This also makes the comparison against any pure model free method like PPO much less meaningful.\n- Similarly, manually avoiding dead-ends and loops is a very strong assumption \n- Also, being able to distinguish and use a fixed ratio of \"solved\" and \"unsolved\" episodes is a strong assumption. \n- The one main contribution seems to be a new way of how \\phi_a(x) is defined. Their particular choice needs a clearer motivation. Furthermore, if there is more contribution and differences to prior work, highlighting them more would help the reader understand the contribution. \n- As the work makes several strong assumptions regarding the environment and access to the model, significantly more work (e.g. ablation studies) is needed to clearly show which assumption and feature of the algorithm is important for performance (and ideally also why). For example (but that's just a first idea): To understand the impact of their choice of \\phi vs. their planning architecture, it would be be interesting to maybe train PPO using an exploration bonus based on \\phi. This would allow disentangling the contribution of: Access to the true model, \"discrete-environment-tricks\" like penalizing dead-ends, and exploration incentivication of \\phi. \n\nEdit:\nThank you for your response and the updated manuscript, which reads considerably better.\nI also agree with your point regarding the strength of assumption regarding \"solved\" and \"unsolved\" episodes.\n\nConsequently, I will raise my score to a \"weak reject\" to express that I think this is promising work.\n\nI do believe that ablation studies would add a lot to the paper as they would allow one to see which of the (many) added components help how much, for example between the selection function $\\phi$ and the various penalizations used. ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposes an approach blending model-based, model-free methods and utilizing risk-sensitivity information in ensembles as part of the value estimation and exploration process. The exploration is based on risk- sensitivity measures such as moments and relative majority vote. There is a lot of work currently trying to marry the model free with model based approaches for integrated planning and learning as the authors have mentioned in the related work section of the paper and also called out similar methods and techniques. The authors have provided evidence via experiments in three environments and shown good results of using this blended approach. Code is also provided for others to further carry out explorations in this research area.\n"
        }
    ]
}