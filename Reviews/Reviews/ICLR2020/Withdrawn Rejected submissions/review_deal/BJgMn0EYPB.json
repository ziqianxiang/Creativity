{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposes to detect adversarial attacks by checking consistency conditions coming from a reconstruction model. It shows some success in detecting them (around 50% detection rate at a 5% false positive rate on SVHN). It shows that many of the attacked images that evade detection are also misclassified by humans, although given that the allowed perturbation norm was large it isn't completely clear what to conclude from this.\n\nOverall assessment: There are some interesting ideas in the paper, and the adversarial evaluation is more thorough than most papers in the literature (although there are still a couple evaluation issues). There are several minor issues and a couple moderate issues that together make me hesitant to accept the paper in its current form, but I am open to revising my recommendation if these issues can be addressed.\n\nImportant issues:\n1. It is misleading to say that you introduce the notion of deflected attacks. Deflected attacks is the same as using a detection method which has been extensively considered, so much so that Carlini et al. wrote a 2017 paper bypassing ten different detection methods: https://arxiv.org/abs/1705.07263. In fact, many detection methods have been broken including the Defense-Gan method (https://arxiv.org/abs/1805.06605), which is fairly similar to the method proposed in this paper. it is certainly important to discuss this literature, and also to check the various ways of fooling detection methods to see if any of them break your approach.\n\n2. It is not meaningful to evaluate against white-box attacks or attacks that are not defense-aware. Consequently, most of the numbers presented in the paper are not meaningful and are misleading to include. The only meaningful numbers are those against the defense-aware, white-box attack. These other graphs should be removed and instead more extensive experiments with the current defense-aware attack and other strong attacks should be conducted.\n\n3. Some important sanity checks are not included. While the number of PGD iterations (200) is large enough in standard settings, your optimization problem may be more difficult due to the combination of four loss functions, and consequently you should test what happens if you increase the number of iterations and verify that the effectiveness of the attacker actually plateaus. More broadly it is important to verify that your optimizer is working well. For instance checking that you consistently hit zero reconstruction error in settings where this is known to be possible, and that attack effectiveness is 100% when the allowed ell-infinity norm is large.\n\nMore minor comments on the attack:\n-I also could not assess whether the step size was appropriately chosen because I couldn't interpret the units--does 0.01 mean 0.01 / 255 or 0.01 / 1? The former seems too small since we can't even reach the boundary of the ell_infinity ball in that case while the latter seems somewhat large.\n-Why do alternating gradient steps on the two losses instead of a single gradient step on the sum of the losses?\n\nPoint 3 is particularly important. It would be hard for me to accept the paper until I was convinced that the proposed defense-aware attack optimizer actually worked well enough to provide meaningful estimates of robustness.\n\nMinor comments:\n-Cisse et al. (2017) is not a certifiably robust model."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Authors combine different adv detection schemes to detect adv examples. In cases that their approach has failed they perform a human study and conclude that those examples are misclassified by humans as well thus deflecting adv attacks. \n\nAlthough combining existing adversarial detection methods are interesting but I think the paper lacks novelty. Also the work is motivated by stating that stronger attacks can break the defenses that are not certified. But isn't it the approach taken in this paper? How do we know other attacks would lead to the same empirical results as reported in the paper?  "
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "\nSummary\n========\nThis paper proposes a detection mechanism for adversarial examples based on capsule networks. The defense uses three distinct detection criteria to distinguish between benign and adversarial examples.\nExperiments on SVHN and CIFAR10 show that evading detection requires large perturbations that may change the class semantics of the attacked image.\nThe evaluation could be improved though, and does not sufficiently convince me of the validity of the proposed defense. For this reason, I am leaning towards rejection.\n\nComments\n==========\nThe detection mechanisms proposed in this paper are fairly natural and easy to understand. The authors also seem to have done an effort to consider adaptive attacks on their detection mechanism. Still, I think that the current experiments are not sufficiently convincing that this defense is indeed hard to attack. Addressing the comments below may help the authors in presenting a more compelling argument in favor of their defense.\n\n- The notion of \"deflected\" adversarial example proposed in this paper is simply the standard definition of a robust classifier. If a classifier is robust to attacks, then fooling that classifier should require making perturbations that change the input's underlying class. I think it's unnecessary to introduce some new terminology for this.\n\n- I have a hard time imagining that the perturbations in Figure 1 are indeed limited to eps=16/255 in the infinity norm. The changes are much too salient for such a small per-pixel perturbation. Some pixels have clearly changed from black to white or vice-versa.\n\n- The newly proposed cycle-consistent loss is natural but never evaluated. How much better do your model and defense perform thanks to this new loss? The ablation study in Section 5.3 does show the effect of the cycle-consistency detector, but not whether the loss improved the performance of this detector.\n\n- The assumption that the white-box adversary does not know the threshold used by the GTF defense is not realistic. An adversary could always try different thresholds and see which one performs best.\n\n- The multi-objective loss in (5) appears to be the right way to create a defense-aware attack. However, it is never discussed how the weights \\alpha_i are chosen. Optimizing these multi-objective losses is often fairly challenging, so a lot of care should be taken in choosing the right weights through extensive hyper-parameter searches. According to Figure 4, it seems that a single detector is often much more powerful than the two others. So an attack that only targets that detector might already do very well.\n\n-  The dual-stage optimization described in Section 4 does not seem optimal. Why not incorporate the misclassification objective into the loss in (5) as well?\nWith the proposed dual-stage method, why is the success rate of CC-PGD in Table 1 so low? If you first optimize for an example that fools the model, you should then at least aim to reduce the loss in (5) while maintaining the misclassification.\n\n- Experiments with hard-label (\"gradient-free\") attacks would be worthwhile to guard against gradient masking effects. Similarly, since this defense is attack agnostic, it would be good to evaluate against a spatial attack (e.g., rotations and translations) as the search space of these attacks can be brute-forced. \n\n- I couldn't find any mention of the clean accuracy of the networks you trained. \n\n- In Table 1, the success rate of white-box PGD on CIFAR10 (49.3%) is extremely low. This attack should succeed with 100% success rate at eps=8/255 if implemented correctly."
        }
    ]
}