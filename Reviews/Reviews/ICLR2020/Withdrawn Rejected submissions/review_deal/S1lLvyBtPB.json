{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "I read the paper and found the approach of treating the adversarial detection problem as anomalous pattern detection quite interesting. The authors do a good job of explaining the relative work, the methods their work builds upon. The analysis provided on various layers (Table 1) is quite insightful. \nHowever, I am not convinced by the results and found them hard to compare. The datasets used are also MNIST or Fashion MNIST and doesn't really capture the real case / challenging scenario for the anomaly detection. "
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "* Summary *\nThe paper proposes a new unsupervised anomaly detection method using the discrepancy between the activations of a deep convolutional autoencoder (AE) for clean (in-distribution) samples and noisy (out-distribution) samples. The authors propose a Subset scanning method which searches for a subset of node activations that are higher than expected, in order to classify new inputs are anomalous or clean. Furthermore, the authors propose the ability to visualize the set of anomalous nodes in the AE output error space to provide an explanation for the decision of the anomaly detector. They also extend the idea of AE reconstruction error based anomaly detection to incorporate hidden layers of the AE. The focus is on detecting anomalies generated via adversarial attacks, rather than on out-of-distribution samples coming from different classes.\n\nPros:\n1. The paper is well written, and provides sufficient background on the topic.\n2. The idea of using a subset scanning approach for activations in the hidden layer is interesting.\n3. The motivation and background for the subset scanning approach are well explained.\n\nCons: \n1. The experimental section lacks in depth, and tests only a very restricted scenario of anomaly detection. The method is sold as working on \"any pre-trained, off-the-shelf autoencoder network\", yet the evaluation is only on very simple datasets MNIST and Fashion-MNIST, which have no background, centered, single-scale images. Evaluations on harder datasets which require more sophisticated autoencoder architectures would have been interesting to showcase the strength of the method. If e.g. geometrical transformations are applied to images (translations, rotations, etc.), I strongly assume that the subset scanning method would have difficulty, because the set of highly activated neurons would most likely differ, especially in early layers.\n2. The comparison is only done to two not exactly state-of-the- art methods. Again, if the method can be applied to pre-trained models, it would have been interesting to apply the method to existing state-of-the-art models, or at least multiple variations of autoencoder architectures.\n3. The authors focus entirely on adversarially generated samples as anomalies. Although this is an interesting and difficult scenario, this is not properly motivated, and there should also be an evaluation of other anomaly cases, e.g. detection of out-of-distribution data coming from different classes or different datasets.\n4. The focus is on detecting adversarial examples from a single attack model, then the performance of the proposed unsupervised method should be compared to supervised detectors of adversarial attacks, e.g. as described in Metzen et al. (2017) \"On detecting adversarial attacks\" and follow-up papers.\n\nMinor issues:\n1. Many of the referenced figures are in the appendix, and figures are not presented in the order in which they are cited.\n2. There are quite a few typos in the manuscript.\n3. Fig. 5 is confusing to understand. Why should we expect the clean images to have anomalous nodes along the contours of the digit? \n\nOverall I think the authors propose an interesting idea but not enough convincing evidence that their method is actually improving unsupervised anomaly detection in its general setting. In its current form the paper is not fit for publication at ICLR, but after addressing the weak points and a more thorough experimental evaluation this could become a good paper."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper explores the idea of using subset scanning [7] over the hidden activations of an autoencoder (AE) for the task of unsupervised anomaly detection (AD). The proposed method combines the standard reconstruction error with subset scanning scores and does not require any modification/retraining of the standard AE. The paper further inspects how units of the AE that detect anomalous patterns from such subset scanning scores might be used for anomaly visualization. Finally, the paper concludes with experiments on MNIST and Fashion-MNIST that indicate an improvement in detection performance over the standard AE reconstruction error and OC-SVM baselines.\n\nThis paper should be rejected in my opinion due to the following four main reasons: \n(i) The technical quality of the paper is poor and the main idea not well explained;\n(ii) The experimental evaluation considers rather simple datasets (MNIST, Fashion-MNIST) and only includes two baselines (vanilla AE, OC-SVM), but not any major competitors ;\n(iii) The work is not well placed in the literature and major related work is missing;\n(iv) The overall presentation is poor;\n\n(i) I find that subset scanning [7], seemingly the main component the approach, is not well defined and explained in the paper. Section 3 introduces the method with an example that to me reads like an explanation of p-hacking (Section 3.1, second paragraph). Inconsistent notation and the use of undefined expressions (see examples in comments below) make it hard to follow and understand what exactly is being done. After reading, I would somewhat summarize the idea as “conducting multiple hypothesis testing over the hidden network activation statistics of an autoencoder for unsupervised anomaly detection”. This idea might be valid and indeed lead to significant improvements over the baseline of just using the AE reconstruction error. However, the specific method proposed is not rigorously defined and clearly explained in this manuscript. The explanation of the method further leaves important questions unanswered: (ia) Why use the Berk-Jones over the Kolmogorov-Smirnov test? (ib) How is the Linear Time Subset Scanning (LTSS) property exactly defined and why is it satisfied for AEs?\n\n(ii) I find the experimental evaluation in the paper not convincing. Only two baselines (vanilla AE using reconstruction error and OC-SVM) are considered, but there are many deep competitors (VAEs, GAN-based AD, Deep SVDD, etc.) [1, 10, 4, 9, 5, 6] that have significantly improved detection performance over the AE baseline. MNIST and Fashion-MNIST are also rather simple datasets. Adding more complex datasets like CIFAR-10 or MVTec [2] would be insightful (also see experimental evaluations in [9, 5, 6]).\n\n(iii) The paper does not properly place the work into context with existing literature. Major works on deep anomaly detection and out-of-distribution detection are not included [1, 10, 4, 9, 5, 6, 8, 3].\n\n(iv) The overall presentation of the paper is poor. The writing is wordy, terms are imprecisely and inconsistently used, and the text contains many grammatical errors. Figures are unpolished and references are scattered over the paper. See comments below for examples. Going over the 8 page limit is not justified in my opinion.\n\nI listed some suggestions and ideas to improve this work below.\n\n\n####################\n*Additional Feedback*\n\n*Ideas for Improvement*\n1. Explain your method rigorously and clearly. I believe that using the activation statistics of AEs (via testing or otherwise) should lead to an improvement over the reconstruction error baseline.\n2. Plots as in Figure 9 for the activation distributions of normal vs. anomalous samples would be insightful. In which layers may anomalies be detected?\n3. It might be interesting to specifically look into different activation functions and choose/design specific suitable tests. Might using some form of normalization (e.g. batch normalization) be beneficial?\n4. Going further into the direction of deep AD interpretability and improving over the reconstruction heatmap baseline might be worthwhile since this is an important problem with only little prior work.\n5. Include major related works as outlined above and clearly state the contribution of your work in context of the existing literature.\n\n*Specific comments*\n6. Imprecise use of terms and language:\n• Abstract: “large outliers” What should large outliers be exactly? Be specific.\n• Section 1: “... improve the anomaly score of current autoencoders ...” You improve the anomaly detection performance. A score is just a function.\n• Section 1: “... reconstruction error space ...” The reconstruction error finally is a one-dimensional loss. Be specific what you mean.\n• Figure 1, caption: “... the highest mutual information exchange ...” Exchange?\n• Section 3 on subset scanning jumps between subsets of data samples and subsets of network activations.\n7. Inconsistent notation:\n• Section 1, second paragraph: Jumping between x and w for samples.\n• Function $\\phi$ in Eq. 4 not defined.\n• Section 2.2, definition of BIM: Clip function not explained. Readers might guess, but rather explain your notation and definitions.\n8. Writing is repetitive and redundant:\n• Section 1, first paragraph: Repetitions of applications with the same references already (“... human annotation errors ...”).\n• The autoencoder is introduced and explained three times: Section 1, Section 2.1, Section 4.\n9. Abstract: “In this paper, we proposed ...” » “In this paper, we propose ...” Simple present is the primary writing tense. Many more examples like this in the main text.\n10. Closely place Figures where they are referenced in the text and vice versa. The overall first reference in the Introduction, is to Figure 4 (b) on page 9.\n11. Section 1, second paragraph: “Further detail on the autoencoder architecture and training setup for the experiments can be found in the Section A.4” A reference like this only should first appear in the experimental section.\n12. Section 1: “This is formally quantified as the subset with the highest score according to a non-parametric scan statistic.” Passive voice is bad writing style. Write in active voice. Many more examples like this in the main text. Section 2.2: “Several attack models have been used to target classifiers in this study, ...” etc.\n13. Section 2.1: “... to address the shortcoming of conventional autoencoders in the presence of anomalies samples during training.” Name them. Be specific!\n14. Section 3: “Treating the detection problem as a subset scan has desirable statistical properties.” No reference or explanation. Be specific!\n\n\n####################\n*References*\n[1] J. An and S. Cho. Variational autoencoder based anomaly detection using reconstruction probability. Technical report, SNU Data Mining Center, Seoul, South Korea, 2015.\n[2] P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger. Mvtec ad–a comprehensive real-world dataset for unsupervised anomaly detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9592–9600, 2019.\n[3] R. Chalapathy and S. Chawla. Deep learning for anomaly detection: A survey. arXiv preprint arXiv:1901.03407, 2019.\n[4] H. Choi, E. Jang, and A. A. Alemi. Waic, but why? generative ensembles for robust anomaly detection. arXiv preprint arXiv:1810.01392, 2018.\n[5] I. Golan and R. El-Yaniv. Deep anomaly detection using geometric transformations. In NIPS, 2018.\n[6] D. Hendrycks, M. Mazeika, and T. G. Dietterich. Deep anomaly detection with outlier exposure. In ICLR, 2019.\n[7] E. McFowland, S. Speakman, and D. B. Neill. Fast generalized subset scan for anomalous pattern detection. The Journal of Machine Learning Research, 14(1):1533–1561, 2013.\n[8] E. Nalisnick, A. Matsukawa, Y. W. Teh, D. Gorur, and B. Lakshminarayanan. Do deep generative models know what they don’t know? In ICLR, 2018.\n[9] L. Ruff, R. A. Vandermeulen, N. Görnitz, L. Deecke, S. A. Siddiqui, A. Binder, E. Müller, and M. Kloft. Deep one-class classification. In International Conference on Machine Learning, pages 4393–4402, 2018.\n[10] T. Schlegl, P. Seeböck, S. M. Waldstein, U. Schmidt-Erfurth, and G. Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In Proceedings International Conference on Information Processing in Medical Imaging, pages 146–157. Springer, 2017."
        }
    ]
}