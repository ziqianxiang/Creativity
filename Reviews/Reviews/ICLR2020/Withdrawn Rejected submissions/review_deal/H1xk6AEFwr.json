{
    "Decision": "",
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "Based on the observation that encoder-decoder models can generate pathological output (e.g., reproduce noisy training data, repeated statements, contradictory statements, filler statements) and are prone to adversarial attacks (e.g., Wallace, et al., 2019], the authors propose curating clusters of prepared statements and directly classifying into this ‘response class’ space using a ULMFit [Howard & Ruder, 2018] like model. The steps to generating clusters is (1) k(=10) nearest neighbor based on standard (non-contextualized) embeddings to filter pairwise matchings, (2) generation of a similarity matrix of candidate responses using pre-trained BERT, (3) run agglomerative clustering up to a (manually determined) threshold, and (4) manual curation of remaining clusters. Using this method, they generated 187 response groups generated in ~3 hours based on 700k rounds of doctor-patient dialogue— which is then used for training. Evaluation is conducted primarily with a user study of comparing a language-model based decoder (I think AWD-LSTM crossed with [Serban, et al., 2015]) and showing that doctors prefer the ‘classification-based’ answers to the ‘decoder-based’ answers when there are differences. Secondly, they compare classification with different input encodings for the classification problem. Finally, they conduct some preliminary experiments to quantify cluster-size tradeoffs wrt accuracy, time, etc.\n\nAt a high-level, the basic premise makes sense — manual curation of responses, especially in potentially sensitive domains (e.g., medical) can lead to better responses. However, to compare pre-selected sentences to an ‘pure’ encoder-decoder model is literally the two extremes of this setting and not a nearly complete comparison. In commercial settings, slot-and-frame systems are still widely used where curated utterances are produced modulo slots that could be thought of as delexicaled expressions that are bound with a value during inference. Modern research systems generally don’t rely on a strict encoder-decoder model, but have some notion of state-tracking, knowledge base lookup, and a context-sensitive generation step — or similar variants that would mitigate some of the decoder issues that motivate this work (e.g., some combination of [Wen, et al., EACL17] (policy network to response decoder), [Wu, et al. ACL19] (for state generator ), and [Lei, et al., ACL18] (wrt to generation). Furthermore, the motivating example of this work (including in the title) was multi-domain and there is no evidence of this work doing multi-domain DST, clustering transferring, etc. Honestly, it isn’t even clear if the method treats the system as a dialogue since it is basically a classifier based on a set of previous utterances — more in the lines of sequential QA based on the application. Thus, I think the core underpinnings of this work need to be more clearly motivated and contextualized for it to be meaningfully evaluated. The basic idea may have some commercial value, but the basic premise isn’t well-developed enough (or sufficiently contextualized) as a research contribution. Additionally, without releasing a dataset, etc., it would be difficult for others to reproduce this work. Based on these reasons, I recommend rejection in its current form. Below are some additional comments to potentially address as this work is pushed forward.\n\n— How exactly did the doctors annotate the results. How many doctors? What was the annotator agreement? How were the results presented?\n— LSTMs can be conceptually generative or discriminative (e.g., https://arxiv.org/abs/1703.01898). The distinction is really that this work is contrasting with a (compositional) language model decoder as opposed to a classifier based decoder (as both are using LM-based encoders). The tradeoff would obviously be sparsity of responses, out-of-vocabulary questions, diversity of responses, etc. All of these aspect should be evaluated.\n— Neither method has an automatic mechanism to ensure high-quality responses; one is generation and one is classification; thus, just a sparse space of answers as opposed to a structured space dictated by the combinatorics of the language. The responses in the submission are just manually curated. One could think of validation models based on distance from the canned responses that might have suitable advantages.\n— Once the equivalence class is selected, it wasn’t clear which response is selected. Is there one for the entire equivalence class? Is one selected randomly?\n— With respect to being able to make changes post-training, this can also be done with slot-and-frame and other copy-augmentation/delexicalization methods.\n— Ranking models and [Wan & Chen, 2018] are indeed conceptually (and even methodologically) very similar (as the authors are aware). A closer comparison is warranted.\n— The authors touch on this in the last section, but the tradeoffs between number of clusters, quality of clusters, classification accuracy, etc. is somewhat sensitive. In principle, one could put everything in one class and get perfect ‘classification’, but would be pretty worthless as a generator. This needs to be further studied to validate the method is viable (even in a single domain).\n— The idea of using canned responses isn’t entirely new beyond what the authors recognized in the paper (e.g., [Didericksen, et al., Collaboration-based User Simulation for Goal-oriented Dialog Systems, ConvAI NeurIPS WS, 2017] — even if in a different setting and not clustered)\n\nIn any case, there are some ideas here worth salvaging, but the current study is insufficiently contextualized/contrasted wrt the spectrum of conversational systems, doesn’t really support what the motivation set out to do, doesn’t make a sufficiently convincing case that the recommended approach is viable in practice, and would be difficult to follow-up without public datasets, etc. Finally, I don’t think the scope of potential impact (if well-executed) makes ICLR the ideal venue. In its current state, this work belongs in a dialogue systems workshop where attendees may have interest in more practical settings.\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper proposes to use a fixed set of response classes and build a classification model leveraging advances in pre-training strategies to pick a response from the bank to return rather than using a generative model. The main idea is to have  a stronger quality control over the generated responses while trading from the flexibility. Authors evaluate their proposed approach compared to generative models through expert human evaluations, where they show the responses generated by the proposed approach is worse than the doctor’s 12% of the time, whereas this number is 18% for generative models. While the paper’s objective of getting more control on the quality is an interesting direction, it is not clear how the proposed solution is particularly novel or significant because it seems to execute a pipeline of existing methods to first cluster the responses that appear in training data, and then train a classifier on the created response classes. More precisely, it is not clear what exactly is claimed as the central contribution of the paper. Overall, this paper needs further work and improvements to support that the proposed method is applicable in the general multi-domain dialog scenario and superior to generative models.\n\nHere are some of my questions and concerns about the paper:\n\n# The applicability of this approach seems limited to be considered as a general approach for dialogue response generation especially because it relies on several hand-crafted steps and human experts (for Step 5) to finalize response classes that can lead to a better performance than generative models. So, it not clear how applicable/costly the proposed method would be for open-domain or multi-domain dialogue. It would be useful to include experiments and comparisons on some other dialogue datasets.\n\n# My another major concern regarding applicability is that it is hard to imagine many realistic scenarios where a few hundred predefined responses would be enough to cover the complexity and diversity of person-to-person dialogues (even in patient-doctor conversations). One interesting study to report would be to do an additional analysis over the test data by cheating to compute an upper bound on the BLEU score: Compare the ground-truth response with the response classes to find the best matching class, and compute the BLEU score by using the best matching response class for the ground-truth response in test data. This would give some idea on the applicability of the proposed method and its generated response classes in the scenario where the classifier is assumed to be perfect.\n\n# I am also curious about the development cycle of this approach. What is the criterion to decide on the final set of response classes and their quality? More precisely, is human evaluation the only way of testing the quality of response classes and hence the proposed model? And if so, how would one be able to adapt this approach to their own problem or domain without intermediate automatic evaluations? It would be useful to include a brief discussion on this.\n\n# Including a discussion on the final response classes as well as some examples of these in the form of (centroid, a few other responses from this class) would be helpful. It would also be helpful to see some qualitative examples of actual response generations comparing the proposed approach and the generative baseline.\n\n# Details of the human evaluation (#raters, rater agreement, etc.) along with its design choices should be discussed in more detail. In terms of its design for example, why are categories (a) and (c) treated differently? Or, is there a reason why the authors did not ask raters to select between discriminative response and generative response, comparing them head-to-head?  Besides, it would still be useful to include automatic evaluation metrics (e.g., BLEU, ROUGE, etc.) for the evaluated models. If nothing, it would help us understand the correlation between BLEU score and human eval in this domain. Can the authors include common automatic evaluation metrics like BLEU and ROUGE?\n\n# A few relevant works like [1*, 2*] are missing from the discussion.\n\n\nMinor comments and questions:\n\n# I am wondering why the patient-doctor data experimented with in the paper is considered multi-domain.\n\n# It is not entirely clear how the discriminative model can generate a higher quality response than the gold response. It would be nice to see an example or a more general scenario when this happens.\n\n# If step 5 for response class generation is not skipped, after the manual merging, which text becomes the centroid? And which text from the response class is used to return in the inference time?\n\n\nRelated work:\n\n* [1*] Exemplar Encoder-Decoder for Neural Conversation Generation, Pandey et al.\n* [2*] Search Engine Guided Neural Machine Translation, Gu et al."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper proposes a way to generate responses during a conversation that uses a classifier to select from a set of high quality, expert-curated responses based on the conversation context.  The proposal is to overcome the problem of frequently seen  low quality, offensive, boring, or self-contradictive output of generative models like seq2seq or even GPT-2.   The method is designed for and tested in a (primary care) medical dialog setting.\n\nThe focus of the work is on how to prepare the responses.  Essentially, the suggestion is to find clusters of like-purposed responses that are not over-lapping.  The clusters are found by, initially, automated stages using various sentence encoding vectors and similarity matching techniques like k-means, BERT, and hierarchical clustering, followed by a pass of manual merging.\n\nIn the experiments, it is shown that the proposed process produced more favorable responses than those produced by a generative model, as judged by human evaluators.\n \nThe method proposed is sound, and the results are convincing.  However,  there have been many years of prior work in (neural or conventional) retrieval based approaches to response generation.   The proposal in this paper is just one small variant from this line of work, so there is little novelty.\n\nFor prior work, see an overview at:  https://basmaboussaha.wordpress.com/2018/03/23/overview-of-generative-and-retrieval-based-dialogue-systems,  and papers like: \"Response Selection with Topic Clues for Retrieval-based Chatbots\", Yu Wu et al., 2017, or \"Multi-level Context Response Matching in Retrieval-Based Dialog Systems\", by Basma El Amel Boussaha et al., 2019.\n\nP.2, paragraph starting with \"Experts ...\".  The last two sentences are not comprehensible, something (quotation marks?) seems missing.\n"
        }
    ]
}