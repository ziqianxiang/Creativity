{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposed an architecture to apply Graph Neural Networks (GNN) into the problem of conversational machine comprehension. The author argued that in conversational machine comprehension, question aware context representation can form a context graph. And a GNN can be used to encode the nearest neighbor version of that graph in order to capture context relevance. Experimental results show weak and mixed gain using the proposed approach. Although GNN might seem to be a reasonable approach to handle graph related structures, I am not convinced that the a context graph is necessary to be built in the first place.  One can build a context representation without the need to construct such a graph and there is no strong evidence to support building such a graph is necessary. Justification on why such a graph is important is critical to understand the motivations of the approach. "
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper introduces a fairly sophisticated model for conversational QA.   The authors describe it as a combination of self-attention, BiLSTM encoders, and graph neural networks.   BiLSTMs are used to encode local contexts, self-attention predicts edges of a graph (using a top-k mechanism) and graph neural networks / GNN perform message passing within this predicted graph. Note that these are gated GNNs, so, in practice, another set of gates is used to control the flow of the information (on top of gates in self-attention). GNNs do not start in each conversation turn from scratch but rather integrate information from the previous turn (using forget gates).  Some improvements over external baselines have been shown on QuAC and CoQA.\n\n I do not find the motivation for the model particularly convincing and the results exciting enough to justify accepting a paper with an over-complicated architecture.  To motivate the approach, the authors attempt to link their model to how humans reason (see the introduction) which I find forced.\n\nOne issue which seems especially confusing is the distinction between self-attention and GNNs in the paper. In practice, they are very similar. GGNNs used in this work, when applied to a fully-connected graph will end up being quite similar to 1-head Transformer. I do not quite see why we need 2 sets of gates: one to select edges, do top-k and then another one to again guide message passing.  The intuition for what it buys you needs to be more clear. I could imagine that this just adds more parameters to the model.\n\nConsidering that this is a long paper, analyses are also limited or anecdotal (see, e.g., \"Interpretability analysis\").\n\nMinor:\n1. The attention scores are computed using the dot product between two ReLU-transformed vectors. Is there a reason to force the attention scores to be non-negative? \n\n2. The passage \"difference between manual graph construction and automatic graph construction is analogous to the difference between the feature engineering + Machine Learning paradigm ...\" is very speculative. (Also, since much of the previous GNN/GCN QA work used gates, you can think of the manual construction as providing candidates for the edges.)\n\nThere some types, e.g., in the intro:\n\"parallel of question turns\" and \"parallel of passage words\" -- typos / unclear (page 1)\n\"improved by the hidden states\" (page 2)\ninterpretability analysis -> qualitative analysis (?)"
        }
    ]
}