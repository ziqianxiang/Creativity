{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper tackles neural response generation with Generative Adversarial Nets (GANs), and to address the training instability problem with GANs, it proposes a local distribution oriented objective. The new objective is combined with the original objective, and used as a hybrid loss for the adversarial training of response generation models, named as LocalGAN. Authors responded with concerns about reviewer 3's comments, and I agree with the authors explanation, so I am disregarding review 3, and am relying on my read through of the latest version of the paper. The other reviewers think the paper has good contributions, however they are not convinced about the clarity of the presentations and made many suggestions (even after the responses from the authors).  I suggest a reject, as the paper should include a clear presentation of the approach and technical formulation (as also suggested by the reviewers).",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "In this paper, the author proposed a model to address the training instability of a GAN model on the NRG task. The authors take advantage of an energy based model to measure the distance between a predicted response and the center of all qualified responses. The training process becomes a hybrid one with the original loss function in the beginning followed by the loss that pulls the response to the center of the response cluster later. In general the paper is well written, with experiments clearly showcased improved training stabilities. However, one major flaw in the experiments in that the authors almost only compared diversity measures such as Dist-1, Dist-2 and Ent4. These measures did not take into consideration the matches between the predicted one and the ground truth. The only relevance measure used in this paper is the  Rel., which the authors defined as the average of embedding distances. Such a measure is by no means an objective measure and can't really demonstrate the effectiveness of the model in terms of generating responses that are close to the ground truth. The authors would need to submit results on one of the widely adopted benchmark metrics (e.g., BLEU, ROGUE) or their equivalents in order to demonstrate the quality of the generated response. And this is the main reason of my rating recommendation. \n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #1",
            "review": "POST-REBUTTAL FEEDBACK\n\nI share the same concerns as that of reviewer 2 in the response to the rebuttal. Hence, my score remains unchanged.\n\n\nSUMMARY OF REVIEW\n\nThis paper motivates the need to \"contextualize\" responses based on the query to bring about stable training in NRG and consequently proposes localGAN to realize this. On the overall, I like the motivation and the proposed approach of this paper. The experimental results also look convincing.\n\nOn the flip side, the technical formulation and theoretical results are not presented rigorously and important technical details are missing, as discussed below. As a result, clarifications from the authors are needed to ensure the correctness of their formulation. The authors also need to improve the presentation and proof of the theoretical results; the correctness has to be checked again. In my opinion, these theoretical results do not improve my current assessment of the paper and can be removed to cut down to 8 pages. If the authors like to keep them, they need to revise them based on my concerns above.\n\nIt would be good to show some sample queries and corresponding \"meaningful\" responses produced by their proposed LocalGAN that are not considered safe responses which are produced by the other tested methods.\n\n\nDETAILED COMMENTS\n\nFor Lemmas 1 and 2 and Theorem 1, the authors need to present them rigorously by specifying the exact math expressions since they have not defined what it means by sufficient, approximates, small enough, and estimate properly. This will also eliminate any discrepancy in their interpretations. For example, the authors have used Taylor series expansion to approximate the expectation of F(q,r) in equation 19 (instead of bounding it). Hence, one can claim that Lemma 2 does not hold and hence Theorem 1 does not hold as well.\n\nIn Section 4.3, the described mechanism is confusing to me: \n\n(a) Are the authors saying that it is performed sequentially from foundation to phase-1, followed by phase-2? Or are the authors saying that these three phases are expected behaviors occurring during the optimization in equation 17? \n\n(b) For the foundation phase, is the DBM pre-trained, that is, prior to optimization in equation 17?\n\n(c) Are there multiple response clusters, that is, one for each q? If so, the second RELU term in the minimizing criterion in equation 17 does not seem to properly reflect this.\n\n(d) How are the response cluster centers r_c exactly determined? The authors vaguely say that they are modeled from training data. Is it one center per response cluster? Are the cluster centers also optimized in equation 17, besides the generator's weights? Can the authors provide the argument under the min operator in equation 17? It is confusing to leave out r_c from the subscript of alpha.\n\nI would have preferred that the authors specify the expression of the evaluation metrics to be self-contained.\n\nIn Fig. 2, how exactly do the authors measure stability? If the entropy rapidly increases like that of LocalGAN and Adver-REGS, are they considered stable?\n\n\nMinor issues\nPage 1: Despite of?\nPage 3: inequation?\nPage 3: Equation 4 and 5?\nEquation 6: The first summation should just be over q, unless there are multiple sets of R_q per q.\ntilde{R}_q is not used in equation 6.\nPage 4: a limited samples?\nPage 5: defined in 3.2?\nPage 5: To be consistent, mathbb should be applied to E.\nPage 9: valid this aspect?\nPage 9: from the the?",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Contributions:\n\nThe main contribution of this paper lies in the proposed LocalGAN for neural response generation. The key observation is that for a given query, there always exists a group of diverse responses that are reasonable, rather than a single ground-truth response. Therefore, the local semantic distribution of responses given a query should be modeled. Besides the original GAN loss, the proposed LocalGAN adds an additional local-distribution-oriented objective, resulting in a hybrid loss for training, which claims to achieve better performance on response generation datasets.  \n\nStrengths:\n\nI think the proposed model contains some good intuitions, that is, the generated responses should be modeled as a local distribution, rather than a single ground-truth output during training. The motivation of this paper is therefore clear. Experimental results in Table 1 seems encouraging. \n\nHowever, I would have to say that the current draft is poorly presented. There are a lot of unclear parts that should be more carefully clarified, with details below.\n\nWeaknesses:\n\n(1) Writing: I think the language in this paper is repetitive, and can be much more precise and concise. Also, there are typos here and there throughout the whole draft. I would suggest the authors doing a careful proofreading before next submission. \n\nMinor: in the line before Eqn. (4), change \"SIMPLY\" to \"simply\". \n\n(2) Clarity: Overall, the presented method is unclear. \n\na) It is not entirely clear what the authors mean by saying \"this paper has given the theoretical proof of the upper-bound of the adversarial training ...\". I am not sure whether Eqn. (6) is totally correct, or at least how useful it is.\nb) The notations throughout the paper is a little bit confusing. The authors should normalize all the notations to be consistent. \nc) It is not clear what Eqn. (3) truly means. What is the value for s? The KL divergence should take two distributions as input, but here, the input are two triplets. \nd) In the line below Eqn. (6), what is \\tilde{R}_q? This is not defined. \ne) The proposed method relies on the use of R_q. However, how to define, or learn R_q is not clear. In the dataset, given a given query q, how do we find R_q?\nf) It is not clear why Deep Boltzmann Machines are needed here. I'd like the authors to more clearly clarify this design. Further, since DBM is used, then how the final model is trained together? Now, the models contains both adversarial learning, and contrastive-divergence-based algorithms for DBM training. This seems make the whole model training more unstable. \ng) Generally, I think Section 3 and Section 4 are hard to follow. Further, I did not see how useful Lemma 1 & 2 and Theorem 1 are. The final objective Eqn. (17) is also confusing.\n\n(3) Experiments: My biggest concern about the experiments is that human evaluation should be conducted, given the subjective nature of the task. This is lacked in the current draft. Only reporting numbers like Table 1 is not convincing. \n\n** This paper provides a link that actually links to a github repo. I am not sure whether this violates the policy of ICLR submissions or not. But at least from my point of review, this link should be anonymized. **"
        }
    ]
}