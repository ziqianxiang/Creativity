{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary:\nThis paper investigates the question of generalization in deep networks. The main contribution is to that show train loss can be predictive of the test loss when you normalize the networks, for over-parameterized network using ReLu activation functions. Authors perform a set of experiments on CIFAR10/CIFAR100 and MNIST to validate their claim.\n\nComments:\nOverall the paper main message is clear, however some points/formatting in the paper could be improved: \n-\tAuthors state in section 2 that they use CIFAR10, MNIST, CIFAR100. While the results on CIFAR100/MNIST are in the appendix, I did not find references/descriptions of those results in the paper main body.\n-\tSection 2 refers to equation 7,8 several times. Those equations are only defined in the appendix. It seems that authors should refer to equation 3 and 4 instead? \n-\tAuthors refers to Figure 8 in section 2 without describing it. It is unclear why Figure 8 is needed here?\n-\tSection 2 claims that the Rademacher complexity in the experiment is found to be small for the normalized network. But it is unclear which data/figure support this claim?\n-\tSection 3 refers to figure 9 which appears unrelated (and figure 9 caption refers to figure 10). I think section 3 should refers to figure 4 instead.\n-\tI did not find any reference to figure 3 in the main text\n-\tIn section 4, I find the following part ‘generalization for the classification error and for the unnormalized cross-entropy is attained for much larger N […] N must be significantly larger than the number of parameters...’ a bit unclear. Could you elaborate given that overparameterized networks can achieve good validation test \n-\tAbstract claims that ‘we show that when the performance is measured appropriately […] training performance is predictive of expected performance’. In practice, the paper assumes that the networks are overparameterized and uses non-linearity which satisfies the positive homogeneity property. While those limitation are clearly stated in the section, it would be nice to make that clear in the abstract/introduction as well\n-\tIn section 2, authors say that ‘most datasets are separable by overparameterization’. It would be nice to elaborate on that or add a citation justifying this claim.\n\nIn addition, while the paper clearly show that the normalized train cross-entropy can predict the performance of the of the normalized test cross-entropy, it is unclear if the normalized train cross-entropy is predictive of the test classification error ?\n\nWhile the paper writing and clarity could be improved, I think the paper makes interesting observation, I therefore lean toward acceptance.\n\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Many papers over the last few years have studied the “generalization puzzle”, in the words of this paper, of how overparameterized deep neural networks have the capacity to completely fit the training data yet still generalize well to a test set. This paper proposes a resolution to this puzzle: assuming a model $f: \\mathcal{X} \\to \\mathbf{R}$ and a classification rule $\\hat{y} = \\mathbf{1}\\{f(x) \\geq 0 \\}$, we can divide $f$ by a large number while maintaining the same classifications (and therefore leaving the 0-1 loss the same). However, dividing $f$ by a large number compresses, say, the logistic loss, with the end result that the train and test logistic losses of the resulting “normalized” networks are quite similar. Therefore, the paper claims that there is no generalization puzzle.\n\nIn my opinion, the paper does not accurately capture the nature of this generalization puzzle. It is easy to train a model that achieves 0 training error but high test error (this is not specific to neural networks; any overparameterized linear model would do, or even consider a model that simply memorizes the training data and predicts a random number otherwise). The central question is why training a deep neural network on real data often results in a model with 0 training error but low test error. Normalizing the neural network by dividing it by a large constant and then showing that the resulting (high) logistic loss on the training set is close to the logistic loss on the test set is expected behavior, and does not address this question."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #1",
            "review": "Summary\n\nThis paper shows that the generalization puzzles introduced by Zhang et al. 2016 are in fact not puzzles at all. The difference in generalization performance from training performance is well explained by the normalized loss on the training set. This paper shows that for networks using the cross-entropy loss with ReLu activations, normalizing the weight matrices with the product of their Frobenius norms scales the training loss so that it more appropriately approximates the test loss. The paper concludes with a recommendation that practitioners use this normalized loss function, giving more accurate assessments of generalization error for early stopping. The paper additionally concludes for theorists that these experiments demonstrate that the training performance can still be considered a proxy for testing performance.\n\nReview\n\nI think that this papers approach to understanding the generalization behavior seen in deep learning is important. By understanding the relationship between the test-time performance and train-time performance, we can apply the classical statistical learning theory results to deep neural networks reliably. I think that the investigation of the training performance and generalization performance as the number of parameters increases is a fine demonstration of the utility of the proposed scaling.\n\nI did not fully understand the motivation to scale the weights specifically by their respective Frobenius norm. The paper mentions that \"Later we will explain the precise motivation for these choices\", but I cannot find any such explanation. It seems that we could choose to scale each weight matrix arbitrarily with positive constants and observe similar results, and perhaps scaling by even larger values (perhaps the squared Frobenius norm) we could asymptotically cause the training loss and testing loss to match. My question to the author is: \"Why use the Frobenius norm to normalize the weights as opposed to some other value?\"\n\nI found that the number of insights contained within the paper were too few and too inconsequential to constitute a full ICLR conference paper. This could simply be due to poor vertical space management, especially with several of the plots in the main paper. The paper additionally heavily referenced plots in the supplementary material in a way that made many of the claims unable to stand alone from the appendix. Figure 4 also does not appear to be referenced in the main body of the paper and the caption for this figure leaves much to be desired. It is unclear to me what this nearly full-page figure is trying to communicate, as the message is not immediately clear from the figure itself.\n\nThe paper directly responds to Zhang et al. 2016, however many of the empirical results are in no way comparable making it difficult to understand how this paper builds upon or further extends the work by Zhang et al. 2016. For instance, using the loss function as a surrogate for classification error while discussing the generalization abilities of the classifiers makes it difficult to understand how well each classifier is actually performing in the empirical results. For instance, the caption of Figure 2 says \"Notice that all points have zero classification error at training\", then continues to make additional claims about classification error in the empirical results; yet I see nothing in Figure 2 demonstrating the classification error of the networks. Because of this, the only conclusions I could draw from Figure 2 are that the suggested normalized loss function better matches in training and testing than the unnormalized loss function; however, this does not allow me to draw any conclusions about the generalization abilities of these networks for classification.\n\nAdditional Comments (do not affect score)\n\nThe captions throughout the paper do not well explain what is shown in the figures. For instance, I did not originally understand the dashed pink lines in Figure 1 until noticing them again in Figure 3 on my first pass.\n\nI generally disagree with this direction of research and reasoning as I find that the assumption that the number of parameters in a deep network is much greater than the number of samples is poor. While this is certainly true when testing deep neural networks on small datasets like MNIST and CIFAR-10, it is not sustainably true for networks deployed in data centers or deep networks used for any sort of lifelong learning setting.\n\n----------\nEdit after reading discussion and rebuttal.\n\nNothing changes in my previous assessment of the work. I believe that this paper has interesting insights, but they are too few and too small. I would like to see a bit more motivation of the placement of this work in the grander scheme of the ML literature; at the moment it feels like a letter directly in response to Zhang 2016 (which is not a bad way to consider scientific research, but without centering the threads of inquiry this is a slippery slope down an unrecoverable rabbit hole).\n\nI am going to lower my score to communicate a stronger signal that I believe the paper has more work to be done before publication, but note that my general opinions about the direction of this research have shifted slightly more positively after considering the other reviews.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}