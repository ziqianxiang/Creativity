{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper applies a density ratio test based on deep learning for anomaly detection. The general idea behind this paper seems solid and the experimental section seems accurate and the results are good. I had a really hard time understanding exactly what it is that the authors proposed that led to the good results and what the modeling message was. There was a good bit of jargon about various architectures and why the exact model of the authors is a good idea but it was not made clear conceptually what the advantage of their approach was. I think there are good ideas in this paper but they are not distilled well.  "
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The paper studies inlier-based outlier detection via density-ratio estimation in a deep learning context. Specifically, they consider the use of KLIEP in conjunction with a deep neural network. One finding is that batch normalisation is harmful for performance of this method. Experiments show consistent gains over a self-supervised baseline.\n\nThe paper is generally well-written, and provides a nice overview of the density-ratio approach to anomaly detection. Its aims are clearly spelled out, and the careful derivation in Sec 3 of the Bregman perspective of inlier-based anomaly detection serves as a clean introduction to the topic.\n\nAt the same time, the technical contribution of the paper is modest. As best I could tell, Sec 3 is entirely devoted to a (nice) review of the existing framework of (Sugiyama et al., 2012b), and contains no new material. Sec 4 seems to be the only novel technical contribution of the paper, namely, the observation that batch normalisation affects the KLIEP method adversely. This is certainly an interesting finding, but it is accompanied by no further critical analysis. In particular, there is no deeper explanation for why batch normalisation might affect KLIEP in this way; whether this finding is consistent for different choices of architecture; whether this finding is consistent for different choices of method (e.g., uLSIF); and so on. By itself, I feel there merely observing that batch norm interacts badly with KLIEP is not sufficiently deep as to carry the paper.\n\nThe experiments compare KLIEP against an unsupervised baseline based on geometric transforms (GTs). This comparison is somehow unfair, since (as I understand) GTs assume one operates in a different problem setting altogether (where there is only a single sample of inliers, and no background contrast sample). Given this, it is surprising that GTs are able to be competitive with the proposed method in several cases; I feel this deserves more comment. It also raises the question of whether one may combine the proposed method with GTs. More such experiments might have made the paper stronger, but in their current form I don't find them too illuminating or surprising.\n\nAnother confusion that arose was that the experiments, as best I can tell, focus entirely on KLIEP. This makes the motivation for Sec 3 unclear -- the section introduces an abstract Bregman divergence view of anomaly detection, but then settles on one specific case of this which can be derived rather directly without this (as was done in the original KLIEP paper, to my knowledge). Had there been some study of different instantiations of the power divergence empirically, e.g., that might better justify the discussion in Sec 3. (I would imagine that for example KLIEP is more susceptible to outliers or label noise, which could motivate using intermediate values of Î±.)\n\nOverall, while I appreciated the paper as an overview of inlier-based anomaly detection, its increment over the current literature appears modest.\n\nMinor comments:\n- \"and it is used as an outlier score\" -> \"and is used as an outlier score\"\n- \"neural network ARCHITECTURES for complex image datasets\"\n- \"Deep SVDD approach ARE the distance of a data point\"\n- why do you use p^* in Sec 3 rather than just p?\n- remove italics from Fig 1 labels.\n- \"let us consider using A CNN to ESTIMATE the density\"\n- it seems fairer to use the GT authors' implementation of their own method?\n- \"cleverly impose a non-negativity constraint\" -> omit the word \"cleverly\"."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "In this paper, authors propose a new density-ratio based anomaly detection algorithm. More specifically, the CNN architecture is employed in the ratio model and is trained by minimizing the Bregman divergence. Through experiments, the proposed approach outperformed existing outlier detection algorithms.\n\nThis paper is to make the model of (Nam & Sugiyama, 2015) deeper. The contribution of this paper is not high enough.\n\nDetailed comments:\n1. As a framework, the novelty of the paper is not high. If authors can propose an architecture that is suited for anomaly detection with density-ratio estimation, the paper can be stronger.\n\n2. The performance of the simple CNN architecture should be reported (Nam & Sugiyama, 2015). Otherwise, we cannot know whether the proposed architecture is good.\n\n3. In related work, Deep SVDD, AnoGAN are introduced. So, it is good to report Deep SVDD and AnoGAN results. Also, it is good to report the kernel based approaches."
        }
    ]
}