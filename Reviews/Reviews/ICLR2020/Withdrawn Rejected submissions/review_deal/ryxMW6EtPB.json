{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposes looking at the duality gap to measure performance. However, the metric is just an upperbound on the true metric of interest, and therefore its value can be ambiguous. \n\nThe reviewers found the paper to be in an unacceptable form and was clearly hastily prepared. They were also skeptical about the novelty of the result as well as the comprehensiveness of the experiments.\n\nThis paper would require extensive revisions before any potential acceptance. Reject   ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposed to use the duality gap sup_f V(f, g*) – inf_g V(f*, g) as a metric for GAN training. It proves that this metric is an upper bound of F-distance. It also proves a generalization bound for this metric. Simulation resultson MNIST, CIFAR10, etc. are reported.\n\n  The contribution of this paper is incremental due to the following reasons.\n\n 1) The duality gap is only an upper bound of the F-distance. This means that if the duality gap is zero then the learned distribution is the true distribution. However, the converse is not necessarily true: even if the algorithm starts with the true distribution, the duality gap may not be zero. Thus the metric is not a proper metric.\n  The proof of the upper bound is straightforward.\n\n  2) Another issue is the gap between the min-max formulation and the real training algorithm. As for GAN, due to the inexact update, it is not really solving the min-max problem. For the proposed metric, it is also impossible to solve sup_f V(f, g*) and inf_g V(f*, g) to reasonable accuracy. Thus what the algorithm is really doing, perhaps, is to optimizing a new loss which is the sum of the original loss and and an extra term. Viewing it as a “duality gap” seems to be far from the practical training. This discrepancy exists for GANs, but it is a bigger issue for the duality gap interpretation. \n\n  3) The simulation is not convincing. The reported FID for CIFAR10 using WGAN-GP is 54.4, which seems to be a bit high. I’m not sure whether it is due to parameter choice or due to weak D/G networks used in the simulation. If the paper cannot compare various architecture, it is more convincing to at least use some standard architecture, like DCGAN. Or at least report the parameter tuning effort made for getting the results. "
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "I vote to reject the paper at this stage, mainly because of the following three points:\n\n1) The motivation is unclear and overall structure of the paper is confusing. It should be better motivated why one should use the duality gap as an upper bound for the \"F-distance\". Minimizing the F-distance as is usually done seems like the more direct and simple approach. Since the results are far from state of the art, a clean and neat presentation of the theoretical advantages and contributions is crucial. \n\n2) The presentation is not professional, hard to follow and the submission overall looks very rushed:\n- In equations, please use \\inf, \\sup, and \\text{...} for text such as distance, data, ... \n- I have trouble understanding the overall idea behind Algorithm 1 and Eq. (22). What is the definition of f^* and g^* in Eq. (22)? Some explanatory text would be valuable.\n- The set F in Definition 3.5 looks odd, as it appears to be recursive and might not be unique.  \n- The writing looks very rushed, and should be improved. For example, I have trouble understanding the sentence \"So the existed algorithms should be heuristic or it can get a bad result even we train the neural networks with lots of datasets.\" in the introduction. \n- The aspect ratio in Fig. 5 should be fixed.\n\n3) The experiments are completely preliminary and not reasonable:\n- The WGAN-GP baseline is very weak, i.e. does not show any reasonable generated images (Fig. 9). There are countless open pytorch implementations on GitHub which out-of-the-box produce much better results. \n- The shown inception scores are far from state-of-the-art. It is unclear, why one should use the proposed duality gap GAN."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper has problems with clarity/polish and experimental design that are sufficiently severe\nto merit rejection by themselves.\n\nRegarding clarity/polish:\nI am generally not super picky about these things, but there does have to be some standard.\nThis paper looks very hastily put together, especially pages 7 and 8.\nThere are many typos and unclear statements.\nJust a few examples:\n\n> Generative Adversarial Networks (GANs) are powerful framework for (in the abstract)\n> be a good metric to evolution the difference (in the abstract)\n> In the past few years, Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) are impactful because it has shown lots of great results for many AI tasks, (first sentence)\n\n> It means that there is no an unanimous metric to represent the difference between the true data distribution and the generated distribution\nWhat does this mean? People have mostly settled on using FID for this.\n \n> It is also difficult to know whether the generated distribution is close to the true distribution, and this is often observed by human eyes.\nIsn't this just restating the point made in the first sentence?\nRegardless, nobody really uses human evaluation anymore - so this is just not correct.\n\n>  It means that if the original generator and discriminator are random, it is difficult to confirm that the generator and discriminator can converge to the ideal conclusion by training with given data.\nBut this paper doesn't propose a way to solve that problem, so it's strange to mention this here in this way.\n\n\nThese issues would maybe be excusable if not for the totally inadequate experimental validation.\nA non-exhaustive list of methodological problems with the (single) experiment:\n\n1. The experiment uses a single run each of the baseline and DG-GAN, when it's well-known that GAN training runs\nhave inter-run variance larger than the difference in score reported in Fig 1 and 2.\n\n2. The models have not been trained for long enough.\n\n3. The architecture of the neural networks used for the Generator and Discriminator is very non-standard, which\nprobably leads to:\n\n4. The scores achieved by the baseline are very far from state of the art, making the comparison mostly useless,\nand rendering the third claim from the introduction (\"We propose an new algorithm with the new metric which demonstrates better results than state-of-the-art algorithms.\") completely untrue.\n\nIn light of these other issues, I haven't checked the proofs.\n"
        }
    ]
}