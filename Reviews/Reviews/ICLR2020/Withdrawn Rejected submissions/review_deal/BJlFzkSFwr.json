{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper addresses the problem of text classification by incorporating classification at both token and sentence levels, with the aim of better learning composition. The paper finds that introducing losses at multiple levels, along with additional auxiliary objectives, generally improves performance over simpler baselines.\n\nThe paper is clearly written, and I like the combination of token-level and sentence-level objectives with the aim of better learning compositional properties of language. I think the architecture described is interesting, with reasonable motivation, so overall I think the paper has potential. However, I don't think the paper in its current form has clear enough takeaways or strong enough results to make a publishable contribution -- in particular, it's not clear that the model in fact advances our ability to capture composition, nor do the results clearly advance the state of the art, and the contextualization with respect to related work is too minimal. \n \nThe main claim of the paper is that this multi-level labeling scheme incentivizes the network to learn better composition functions. However, there is no compelling reason to conclude that composition has necessarily been improved here. The majority of tasks being tested on are not related to composition -- it seems that the token/sentence relationship in the error detection and entity detection tasks simply involves identifying whether one token in the sentence has a positive label, which really has nothing to do with composition. The SST task can plausibly require non-trivial composition to relate between the token-level sentiment and the sentence-level sentiment, but that would typically involve incorporating sentiment at the phrase level along the way, rather than jumping from the token to the sentence level, which seems more likely to focus on individual lexical cues rather than composition. Beyond the choice of tasks, there is also no further analysis/discussion to help in determining whether anything about composition has been improved. \n\nIf the results were strong enough to advance the state of the art in these tasks, the composition takeaways might be less critical, but since the only baselines compared against are simpler versions of this model, it's not clear that the results here advance the state of the art in any way.\n\nAs another point: the contextualization with respect to related literature should also be improved. Currently the related work is described extremely briefly and fairly obliquely, and it is not clear how exactly this work differs from other work that has involved prediction at multiple linguistic levels (or work on composition).\n\nOverall, I think the paper has potential, but it needs a clearer contribution and better contextualization.\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper studies the effects of joint learning of multi-level tasks: sentence classification and sequence labeling using a hybrid architecture of bidirectional LSTM and multi-head attention. They introduce a modification of the multi-head architecture to create a sentence representation and auxiliary objectives to enforce some relationship between the two tasks. The experiments comparing different settings of the models demonstrate that the models jointly learned using the two tasks are somewhat better than individual task-specific models. In addition, the zero-shot model also performs better than the baseline in the sequence labeling task.\n\nThis paper requires some clarification and more experiment results. I am leaning toward rejecting this paper for the following reasons:\n\nIt is unclear on the novelty of this paper’s contribution. It has been shown that multi-task learning objective is helpful. While the majority of recent work has focused on transfer learning (from LM objective) and weakly related tasks, the direction of the multi-level learning task has been explored to some extent, such as Hashimoto et al., 2017 (building from bottom-up) and Williams et al., 2017 (weakly supervised lower-level task). I think there should be more effort into distinguishing their findings from the previous. Additionally, the paper should build on top of a well-known architecture for a greater impact. For example,  using the multi-head attention architecture to perform classification is discussed in Devlin et al. 2018. A comparison with existing models, both theoretically and empirically, will be very helpful to readers.\n\nThe proposed method does not seem well justified by the experiment results. As mentioned previously, the paper lacks a comparison with existing models to justified architectural and training choices. Furthermore, the internal comparisons show that the proposed model has a small improvement from the individual models on both sentence-level and token-level tasks. As for the other auxiliary losses, we see all of their effects combined, rather than individual ones. The ablation analysis would be helpful. Finally, I think the zero-shot experiment is interesting. Unfortunately, we cannot draw a solid conclusion here. The MHAL-zero has an additional signal from the weak supervision from the proposed attention loss which is somewhat related to how the sentence labels are created (a big improvement on CoNLL-2003 and FCE). \n\nClarification Questions:\n1. How does this model work in the case of H != S and S > 2?\n2. How is the LM loss computed? I.e. Which representations are used to predict word distribution?\n3. In the semi-supervision experiment, I am not sure how you get “38% on SST, …” for MHAL-zero. Additionally, can you explain “Using only 30% of data, MHAL already approaches its fully-supervised performance”?\n\nHashimoto, Kazuma, et al. \"A joint many-task model: Growing a neural network for multiple NLP tasks.\" arXiv preprint arXiv:1611.01587 (2016).\n\nWilliams, Adina, Andrew Drozdov, and Samuel R. Bowman. \"Learning to parse from a semantic objective: It works. Is it syntax?\" arXiv preprint arXiv:1709.01121 4 (2017).\n\nDevlin, Jacob, et al. \"Bert: Pre-training of deep bidirectional transformers for language understanding.\" arXiv preprint arXiv:1810.04805 (2018).\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "Short summary:\nThis paper builds a two-level hierarchical text classification model; with token classification at the bottom and sentences classification on top of token classification results. This work is closely related to a previous art (https://arxiv.org/pdf/1811.05949.pdf).  The model setup shares ideas with multi-task learning, in which the two separate tasks could be co-trained.\n\nNovelties:\nIt extends the previous work by:\n1) using separate head for each distinct tag label so that there is an explicit correspondence between attention heads and token labels.\n2) due to 1), the classification extends to multi-labels naturally (instead of binary labels). \n3) a unique regularization term is introduced to enforce distinct representation sub-space for each of the query vector (head-specific). This is formulated in Eqn (13). \n\nWeakness:\n* The novelties introduced above seem minor in my opinion. The importance of using the same number of heads for each distinct label is not tested or studied with ablation. In other words, with the normal setting of multi-head attention (with the head size not correlate with the size of tag label, for example, using fixed number of heads - 4 heads or 8 heads) thus not collapsing the queries into head-specific, would you get similar results? Is it really helpful to obtain a per-head representation to improve model performance on sentence-level classification? I know it probably helps human interpretation when you have per head representation. But beyond that, do you really get quality benefit?\n\n* The experiment results would be more strong, if there are more reference base lines from the recent work. Currently all the models are different variations from this paper, except one trivial \"random\" model (Table 1 and Table 2).\n\n* Another novelty ( bullet 3 above) introduced is the regularization term in Eqn 13. However, this does not have its own experiment. The current experiment (that involves this term) is always combined with two other changes (i.e. MHAL-joint). I will be interested to see experiment results (probably with another model with MHAL-sent + Rq or MHAL-sent+tok + Rq) to see if this additional regularization really helps.\n\n* limitations of this model: \n  - since the number of heads is bound to be equal to the number of tags, the model would be difficult to scale when the tag vocabulary is large. In all the experimental dataset the author has worked with, the tag vocabulary size are all pretty small (from 3 to 6). \n  - It seems we always need a way to  map between token labels and sentence labels. This does not seem practical in a lot of text classification scenarios.\n  - To get the max benefit from this model, we need good quality of token-level labels and sentence-level labels, which also seems not very practical in real NLP tasks. \n"
        }
    ]
}