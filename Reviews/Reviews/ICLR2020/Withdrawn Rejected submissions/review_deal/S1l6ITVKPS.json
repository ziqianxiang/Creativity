{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposes a model that can learn predicates (symbolic relations) from pixels and can be trained end to end.  They show that the relations learned generate a representation that generalizes well, and provide some interpretation of the model.\n\nThough it is reasonable to develop a model with synthetic data, the reviewers did wonder if the findings would generalize to new data from real situations.  The authors argue that a new model should be understood (using synthetic data) before it can reasonably be applied to natural data.  I hope the reviews have shown the authors which areas of the paper need further explanation, and that the use of a synthetic dataset needs to strong justification, or perhaps show some evidence that the method will probably work on real data (e.g. how it could be extended to natural images).",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "The authors propose a new neural network architecture that learns to form propositional representations with an explicitly relational structure from raw pixel data. The authors testified the proposed algorithm using the Relations Game, whose aim is to label an image containing a number of objects as True or False according to whether a given relationship holds among the objects in the image. This paper is well organized. The applied methods are introduced in detail. The authors showed the improvement using the Relations Game."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "This paper presents PrediNet: an architecture explicitly designed to extract representations in the form of three-place predicates (relations). They evaluate the architecture on a visual relational task called the \"Relations Game\" which involves comparing Tetris-like shapes according to their appearance, relative positions, etc.. They show that their architecture leads to useful generalizable representations in the sense that they can be used for new tasks without retraining.\n\nI think this paper contains a number of unusual and interesting ideas but is let down by its presentation. The writing is good, but provides very little intuition for why we should expect this approach to work (aside from its connection to equation 1) - I discuss this in more depth below. The experimental task is interesting (I'm okay with synthetic tasks of this form for unusual new architectures like this), but I'm not sure what it tests that isn't tested in the CLEVR and sort-of -CLEVR datasets which rely on similar relational reasoning to solve. The advantage of those datasets is they are well-established with strong baselines so we can be more certain that a fair comparison is been made. I've voted to reject this paper because I feel its premature in its current form.\n\nExpanding on this - The description of PrediNet covers the basics, but missing detail and intuition for why certain choices are made. For example, \n - why is L flattened for the queries (I think it’s because the query is independent of pixel location, but flattening seems of when L also includes co-ordinates) but not the key, K? \n - Why is the key space shared between heads (this seems more intuitive - keys should have consistent meaning… but if that’s the case, make that intention explicit)? \n - Also, writing the dimensionality of the matrices, would help (e.g. is W_S in R^{m x j} or R^{m x 1} or something else?). \n - What is the meaning of the position features in E_1 and E_2? From the softmax product it seems they should be a weighted sum of the pixels that are addressed - implying that it is the weighted average location?\n - The final representation mostly consists of an h x (j) vector (ignoring positions) containing the output of the comparators. Could you provide some intuition for why we would expect such a representation to be useful for the downstream task? This representation seems to differ substantially from what is used in the baseline methods: i.e. attention-weighted sums of the input features.\n\n ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper presents a network architecture based on the multi-head self-attention module to learn a new form of relational representations. The proposed method is shown to improve data efficiency and generalization ability on a sequence of curriculum learning tasks. \n\n+ The major novelty of this paper is a new attention-based network architecture that aims to discover objects and the relations between them. The idea of explicitly appending the patch positions to the representations is interesting, though I am not sure whether it can be generalized to real data.\n\n- Since the proposed network takes patches of full images as inputs, my major concern is about its effectiveness on high-dimensional images with more realistic objects as in the CLEVR dataset other than 2d-grid objects. It would be better if the authors could extend their method to natural images. \n\n- A closely related work is the NLM model [1], which can perfectly generalized to new tasks. Please compare the proposed model with it.\n\n- Minor: Figures are understandable, but some of them are too small, especially for the graphical legend. As space is an issue, I would suggest removing some plots and increasing the size of the ones provided. \n\n- In Figure 1, is g equal to n? \n\n[1] Neural Logic Machines. Dong et al., ICLR 2019."
        }
    ]
}