{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposes a fractional graph convolutional networks for semi-supervised learning, using a classification function repurposed from previous work, as well as parallelization and weighted combinations of pooling function. This leads to good results on several tasks.\nReviewers had concerns about the part played by each piece, the lack of comparison to recent related work, and asked for better explanation of the rationale of the method and more experimental details. Authors provided explanations and details, and a more thorough set of comparison to other work, showing better performance in some but not all cases.\nHowever, concerns that the proposed innovations are too incremental remain.\nTherefore, we cannot recommend acceptance.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "The paper provides a new model for semi-supervised node classification in directed and undirected graphs. It is based on a novel fractional filter for graph conv networks, which generalizes several previously employed graph semi-supervised learning frameworks, by introducing a fractional hyperparameter (sigma in the paper), using fractional powers of the Laplace operator.\n\nThe relevant previous work in the area seems to be cited, and the paper appropriately embedded in the previous work\n\nEmpirically, the method outperforms several established baseline models (classical and neural) on standard datasets in the node classification task. in particular one with a low number of labeled nodes. A sensitivity analysis is performed to assess the impact of the hyperparameters of the FGCN. \n\nI believe the experimental results could justify an accept, but I would not claim I am an expert in semi-supervised learning on graphs.\n\n\nQuestions:\n\nHow can the architecture be extended to handle edge types?\n\nHow were the hyperparameters of the baselines tuned?\n\nSmall things:\nCould you provide more context around the reliability in parallel systems (eq 11)? It is not clear how this relates to the rest of the paper.\n\nPlease add a citation for gated max average pooling.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a fractional graph convolutional networks for semi-supervised learning. The proposed method used a classification function of a fractional graph semi-supervised learning (GSSL) [De Nigris et al., 2017] as a graph filter. In addition, the authors adopt a parallel system and weighted combination of max and average pool. Experimental results show that the proposed method (FGCN) shows the best accuracy compared to other recent graph-based neural networks for all datasets except one.\n\nThe key approach of the proposed method is to apply a classification function (equation (3)) obtained by solving a GSSL problem to graph convolutional networks. However, this idea is too incremental and applying the classification function to graph filter is very trivial. This works also combines the fractional GSSL with a parallel system and weighted pool. But, it is not clear which contribution actually improves the results. Moreover, the intuition of the fractional approach is not clear too, e.g., how the optimization (equation (2)) is derived?, and some explanations are unnatural to demonstrate the methodology, e.g., equation (4). For these reasons, this paper is under the bar of acceptance.\n\nMain concerns:\n1. What is the intuition of the optimization of GSSL? How is it obtained? And among all fractional methods (e.g., SL, NL, and PR), which one doe achieve the best performance?\n\n2. The FGS filter in equation (7) is the sum of infinite terms. However, in practical, it is impossible to compute the infinite terms. Does this approximate the sum of finite terms? If does, what is the number of truncation?\n\n3. The authors mention that they establish a theoretical guarantee of the parallel system. But, I could not find any theoretical results. It would be better to include the analysis in the paper.\n\n\nMinor concerns:\n1. In page 3, please edit “forulation” -> “formulation”\n2. In equation (8), I think “X+\\alpha \\tilde{L} X” should change to “\\tilde{L} X”\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #1",
            "review": "This paper presents a fractional generalized graph convolutional networks for semi-supervised learning. The authors design a new graph convolutional filter based on Levy Flights, and propose new feature propagation rules on graphs. Experimental results on multiple graph datasets are reported and discussed.\n\nPros.\n1. This paper presents a nice overview of three popular semi-supervised learning methods in Section 3.1, and presents insights regarding these models.\n2. A new graph convolutional filter is proposed. The motivation is clear, and the technical details are easy to follow.\n3. Experiments on five benchmark datasets are conducted. Both undirected and directed graphs are used in experiments.\n\nCons.\n1. The proposed method contains three major components: parallel FGS convolution, pooling, and residual block. Although some justifications are provided for such designs, it is difficult to justify the role of each component. In other words, it is unclear whether the performance gain is from the parallel structure, or the residual block. What would be the model performance without parallel structures? Also, given that FGCN has quite a few layers, the motivation of using residual blocks should be carefully justified. \n2. Many recent methods on graph neural networks are not discussed or included as baselines, such as [a-b].\n[a] GMNN: Graph Markov Neural Networks, ICML 2019\n[b] Large-Scale Learnable Graph Convolutional Networks, KDD 2018\n[c] SPAGAN: Shortest Path Graph Attention Network, IJCAI 2019  \n\n\n-------------------------------------------------\nThe response from authors addressed many of my concerns. The rating has been updated.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}