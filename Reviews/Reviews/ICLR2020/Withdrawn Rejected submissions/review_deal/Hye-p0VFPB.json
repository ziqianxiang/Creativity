{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper presents an energy-efficient architecture for quantized deep neural networks based on decomposable multiplication using MACs. Although the proposed approach is shown to be somehow effective, two reviewers pointed out that the very similar idea was already proposed in the previous work, BitBlade [1]. As the authors did not submit a rebuttal to defend this critical point, Iâ€™d like to recommend rejection. I recommend authors to discuss and clarify the difference from [1] in the future version of the paper. \n\n[1] Sungju Ryu, Hyungjun Kim, Wooseok Yi, Jae-Joon Kim. BitBlade: Area and Energy-Efficient Precision-Scalable Neural Network Accelerator with Bitwise Summation. DAC'2019\n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Summary: \n\nThis paper proposes a novel decomposition strategy for matrix multiplication to aim for less energy consumption. They demonstrate that energy consumption is reduced on several CNNs. It is an interesting work but it is not sure that there is a difference in contribution when compared to previous work.\n\nThe main argument to impact to the score:\n\n    1. A similar idea is addressed in [1]. It is highly recommended to show the difference in contribution.\n    2. In Figure 7,  the performance and energy consumption of systolic arrays of decomposable MAC  is shown. However, the only energy consumption is shown in section 4.2 when the method is applied to DNNs. It is better to show both performance and energy consumption.\n    3. It is inappropriate to quantize VGG16, VGG19, DenseNet-121, DenseNet-169 and MobileNet based on CIFAR10 dataset since all models are designed for ImageNet dataset. It is appropriate to quantize these models on ImageNet dataset.\n\n\n[1]     Sungju Ryu, Hyungjun Kim, Wooseok Yi and Jae-Joon Kim. BitBlade: Area and Energy-Efficient Precision-Scalable Neural Network Accelerator with Bitwise Summation. DAC. 2019.\n\nMinor comments not to impact the score: \n    1. \"Very Deep Convolutional Networks for Large-Scale Image Recognition\" has been accepted by ICLR 2014. It is better not to use arXiv preprint on citations unless there is a reason.\n    2. It is recommended to put citations on CIFAR-10 dataset."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper presents a decomposable MAC unit for low-power computation for matrix-multiplication operation for neural networks.\n\nAlthough I believe I understood the idea of this technique, I strongly believe this paper should be submitted to an architecture of design automation conference instead of ICLR. I am also not in the position to assess the experiments, which were conducted with synthesis of Synopsis 28nm library. The paper discussed systolic array, MAC in detail, without too many algorithmic elements inside, therefore may be a paper not toward the audience of this conference."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes to shorten the shift-addition operations in the straightforward configurable MACs (Sharma et al., 2018), to an addition-shift style. The authors claim that the new design is able to lower the energy consumption in the matrix multiplication. In the experimental analysis, the authors demonstrate the effectiveness of the proposed method.\n\nThis paper should be rejected since it proposes exactly the same architecture with the following published work:\nBitBlade: Area and Energy-Efficient Precision-Scalable Neural Network Accelerator with Bitwise Summation\n\nAlso, the authors do not provide a valid approach for the auto-selection of quantization bits, which is more significant in my opinion."
        }
    ]
}