{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposes performs an empirical study to evaluate CNN-based object classifier for the case where the object of interest is very small relative to the size of the image. Two synthetic databases are used to conduct the experiments, through which the authors made a number of observations and conclusions. The reviewers concern that the databases used are too structured or artificial, and one of the two databases is very small as well. On top of that, only one network architecture is used for evaluation. Furthermore, the conclusion from two databases seem inconsistent as well. The authors provided detailed responses to the reviewers' comments but were not able to change the overall rating of the paper. Given these concerns, as well as no methodological contribution, there are general concerns from all reviewers that the contributions of this work is not sufficient for ICLR. The ACs concur the concerns and the paper can not be accepted at its current state.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The authors present an empirical study to evaluate the performance of CNN-based object classifiers for situations in which the object of interest is very small relative to the size of the image. Two artificial datasets, based on MNIST and histopathological images are introduced to conduct the experiments. Through empirical evaluation the authors conclude that the size of the dataset required for generalization increases rapidly with the inverse of the O2I ratio, that higher capacity models generalize better, and that accounting for the model's receptive field is key.\n\nThe contributions of the study are limited: i) The artificial datasets generate images that are small and O2Is that are big for the applications of interest, e.g., gigapixels images in digital pathology (see Figure 1). ii) The dataset based on MNIST is perhaps too artificial (too structured), once one compares their results relative to nCAMELYON. iii) The authors only consider ResNet-50. iv) The authors do not consider multi-instance learning pooling functions, e.g., noisy or, noisy and or attention. v) The authors do not consider performance as a function of the positive instances in the image (number occurrences of 3 in the proposed nMNIST). vi) There is no methodological contribution."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper presents a testbed framework to investigate the limitations of CNN at the classification of tiny objects and the effects of signal to noise ratio has in the task. The implemented framework will be made available online upon acceptance.\n\nI believe that the question the authors try to answer is very interesting and worth of exploration. However, I am a bit less excited about the achieved results since I consider them not to be sufficient to drive conclusions. The experiments proposed by the authors are run on two different datasets created ad-hoc: the nMNIST and the nCAMELYON, both modifications of MNIST and CAMELYON datasets. Over all the experiments run, the behaviors observed on the two datasets are not the same. The explanation provided by the authors is that since nCAMELYON is very small, results are different from the ones of nMNIST. While I consider this a valid explanation, this still limits the overall conclusions of this paper. Therefore, my main recommendation to the authors would be to identify other datasets for their experiment.\n\nIn the dicussion section, the authors argue that MS COCO is not a good candidate since the background usually contains information that allows to infer the label. I wonder if it would be possible to generate a new dataset, starting from MS COCO, that avoids this problem. Using the example of the paper, I would argue that it is possible to obtain images of outdoors with people where there are no balls. Having such a dataset would make your paper much stronger.\n\nFinally, to some extent, I consider that your problem is strongly linked to anomaly/detection detection. In the end, this is what a needle in a haystack is. How do you position yourselves with respect to the state of the art on this topic?  "
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The submission proposes an analysis of the impact of object size in images when performing classification tasks using neural networks of the BagNet family. The analysis is performed on two datasets, a large resolution cluttered MNIST and a histopathology dataset named nCAMELYON.\n\nThe paper attack interesting questions and links the size of the object in the image (O2I) to the training dataset size required. Also, showing that max-pooling is the only pooling operation that converges for very low O2I (but is the slowest to converge at higher O2I) is interesting and encourages discussion about the training (optimization) process.\n\nIn my opinion, the main issue about the submission is the limited depth in the contributions, analyzing a single family of network architectures (BagNets) over two datasets, one of which is relatively small. The family of R-CNN and its derivatives were especially designed to counter the impact of object size, it would have been interesting to include them in the analysis. Furthermore, limited insights can be carried out for tasks related to classification such as localization and segmentation. Models such as Single Shot Detectors split the image into grids and variable anchor sizes to perform their inference, are they affected to a lesser extent by object size? The limited insights provided to potential future readers prevents me from recommending the submission for acceptance.\n\nOn p. 7 (and fig. 7 (b) ), it is said that the lower performance of the larger receptive field suggests that class-relevant information is contained in the texture. I am not sure about this remark, I would have expected the network to learn to focus on the right regions, provided the receptive field is big enough to see the whole object of interest. Could the decrease in performance be attributed to the increased amount of learnable parameters that ended up too large for the “relatively small nCAMELYON dataset used for training”? (sec. 3.1, Global pooling operations)\n\nFig. 16 seems to suggest that multiple numbers can overlap significantly in nMNIST, yielding potentially confusing images even for humans. I am not sure if this is a desirable characteristic for such dataset.\n\n\nMinor details\n- Please use \\cdot instead of the asterisk operator to denote multiplication (sec. 3, footnote 3, fig. 5 (b-c) and sec. 3.1;\n- Fig. 4 uses two different (and non-linear) values for the x-axes, giving the impression that both curves can be compared, that might be confusing to the reader;\n- Sec 1 and 5 “pixel-level level annotations”: duplicate “level”;\n- p. 6 “1 and , 2”: extraneous comma.\n"
        }
    ]
}