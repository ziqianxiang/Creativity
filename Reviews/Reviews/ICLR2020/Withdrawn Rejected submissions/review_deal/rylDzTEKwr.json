{
    "Decision": {
        "decision": "Reject",
        "comment": "There was a clear consensus amongst reviewers that the paper should not be accepted. This view was not changed by the rebuttal. Thus the paper is rejected. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "In this paper, the authors proposed an end-to-end variational hashing-based collaborative filtering scheme with self-masking to solve the efficiency of large-scale recommender systems. It mainly targets at addressing a problem in existing hashing-based collaborative filtering methods, where each bit is equally weighted in the Hamming distance computing process. To this end, the presented hashing scheme develops a self-masking technique to encode which bits are important to the user. The comparative experiments on several datasets demonstrate the superior performance of the presented hashing method.\n\nThe idea is interesting in the sense that an efficient self-masking technique is proposed to generate user-adaptive hash codes, by differentiating the importance of binary bits, for a fast recommendation system. However, the issues with the experimental results make me inclined to reject this paper.\n\nIn Table 2, the reported results of MF and its variants are lower than those of DCF. These results are very unreasonable. I have also conducted many similar experiments and the obtained results are not consistent with the results this paper reported.\n\nIn addition, in the original paper, CCCF is reported to achieve the superior performance over DCF. However, in this paper, the results are also inconsistent.\n\nTo sum up, the reported results in the paper are not convincing to me. I would doubt that there is no much accuracy improvement compared against the state-of-the-art methods.\n\nFinally, as I understand, using variational hashing to maximize the likelihood of all observed items and users will inevitably increase the training time. The presented experiments, however, include no results on the efficiency comparison.\n\nIf there are no advantages on the recommendation accuracy and efficiency, the motivations of this designed hashing method are vague.\n\nBased on the above reasons, I tend to reject this paper."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The author proposes a variational encoder (VAE) based approach to perform hashing-based collaborative filtering, which focuses on the weighting problem of each hash-code bit by applying the \"self-masking\" technique. This proposed technique modifies the encoded information in hash codes and avoids the additional storage requirements and floating point computation, while preserving the efficiency of bit manipulations thanks to hardware-based acceleration. An end-to-end training is achieved by resorting the well-known discrete gradient estimator, straight-through (ST) estimator. \n\nStrength:\nThe idea of employing the discrete VAE framework to perform hashing-based collaborative filtering is interesting. From the perspective of applications, I think it is somewhat novel.\n\nThe experimental results demonstrate the performance superiorities of the self-masking hashing-based collaborative filtering method. \n\n\n\nWeakness:\n\nI have concerns over using the VAE to model the ratings of each user and item pairs here. Essentially, you are building a VAE for every rating value R_u, i.  Although the parameters are shared, the VAE’s have their own prior. For generative models like VAE, they need to learn from a lot of data, not just one data point. From the perspective of generating data looking similar to the training data, I don’t think your model have learned anything. Only the reconstruction part is important to your model. \n\nThe idea of using discrete VAE for hashing tasks has been explored before, see “NASH: Toward End-to-End Neural Architecture for Generative Semantic Hashing”.  It employed a very similar idea, although it is not used for CF, but for hashing directly. The novelties of the model is limited.\n\nSince ratings are essentially ordinal data, using Gaussian distribution to model the rating data may be not appropriate.\n\nThe whole paper, especially the model, is not presented well. The model is not presented in a rigorous way, and some sentences in this paper are difficult to follow.\n\nThe runtime analysis is not sufficient. In addition to comparing with the methods using hamming distance, we also want to see the advantages of the hamming based method over the real-value based method on speed acceleration.\n\n\nOther question:\n\nTo realize the self-masking role, the paper proposed to use the function f(z_u, z_i)=g(Hamming_self-mask(z_u, z_i)), Equ. 12, and demonstrate the effectiveness of using this function by experiments. Since the necessity of using self-masking is not very convincing, I doubt whether this self-masking function f(z_u, z_i) is indispensable. Maybe, if some other functions that takes z_u and z_i as input are used, better results may be observed.  \n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "In this paper, the authors study a classical (and well-studied) problem called rating prediction from a new perspective, i.e., learning binary vector representations of the users' and items' latent representations for efficiency. In particular, the authors introduce a personalized self-masking shown in Eq.(2) and in Figure 1 (the 'AND' operation) in order to improve the previous hashing-based collaborative filtering methods without increasing the time cost much.\n\nEmpirical studies on four public datasets show the effectiveness of the proposed model, i.e., variational hashing-based collaborative filtering with self-masking (VaHSM-CF).\n\nSome comments/suggestions:\n\n1 The studied problem (i.e., rating prediction) is well studied in the community of recommender systems, and there are many more accurate algorithms than the basic MF algorithm used in the empirical studies. I thus suggest the authors to include more such algorithms though the focus of this paper is for efficiency (but the authors also claim the accuracy of the proposed model).\n\n2 The authors are suggested to give a brief explanation on choosing those baseline methods in the context of other hashing-based CF methods.\n\n3 Some details are missing, e.g., the number of latent dimensions in MF. And some presentation in the parameter setting are not clear, e.g., 'is chosen consistently across all data sets', 'was consistently chosen'. Does 'consistently' means 'exactly the same'?\n\nMinors:\nThere are some typos: 'a user, u, and', 'of e.g., restaurant and shopping malls', etc.\n\n"
        }
    ]
}