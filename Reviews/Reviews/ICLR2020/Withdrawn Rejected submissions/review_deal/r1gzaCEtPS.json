{
    "Decision": "",
    "Reviews": [
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper presents a method that modifies an output of an arbitrary neural decoder to follow weak supervision. This is done through posterior regularization within an amortized structured variational framework. The method is evaluated on two problems: text generation and unsupervised tagging.\n\nThe paper is well written with sufficient details. The experiments are very detailed. I especially appreciate the discussions of the limitations.\n\nNote that the paper is half a page over the limit and should be shortened. That said, I don't see that as being a problem. "
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposes a procedure to control the output of neural decoder generation architectures via weak supervision. The approach employs structured latent variables as control states that can be constrained during training via posterior regularization, thereby encouraging the network to learn problem-specific knowledge without adversely affecting the expressiveness of the underlying architecture.  The paper presents evaluations on two database-to-text generation problems as well as part-of-speech induction, with comparisons against neural sequence prediction baselines.\n\nAs discussed, a consequence of the end-to-end nature of training standard neural decoder architectures is that it is generally difficult to control their output. Several methods have been proposed to address this either by modifying the architecture to impose a specific inductive bias, or by providing the network with auxiliary supervision. In this case, the proposed approach introduces latent structure into the architecture in the form of control states and then constraints the posterior over these latent states with domain-specific distributions. The results demonstrate that this approach outperforms several baseline methods in terms of several automatic metrics on two text generation datasets. Further, ablations show the advantage of posterior regularization. On part-of-speech induction, the method performs similarly to a state-of-the-art baseline in terms of perplexity (slightly worse, actually).\n\nThe problem is interesting and relevant. However, it relies upon hand-crafted constraints over the latent control states that would be difficult to design in many cases, an issue that the paper acknowledges. This, together with the fact that these constraints are task-specific, takes away from the generalizability of the underyling neural decoder architectures.\n\nThe discussion is opaque at times, making the paper a bit of a chore to read. There are also numerous typos/grammatical errors.\n\nADDITIONAL COMMENTS/QUESTIONS\n\n* The intro would benefit from a concrete example of the control one might like to impose on a network's output.\n\n* The database-to-text task is not terribly compelling as the constraints are encouraging the verbatim use of class labels.\n\n* The database-to-text evaluation relies upon automatic metrics. While these are commonly used for generation, it isn't obvious that they are the right metrics by which to assess the ability to control the network's output.\n\n* The description of the Ours+Force ablation is vague. What hard constraints were used and how?\n\n* The synthetic experiment is vague and doesn't contribute much.\n\n* Is there an L missing in the final PRLBO equation?"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper presents a method to instill prior knowledge into natural language processing models by introducing a latent variable which the authors call ‘discrete control states’ that has semantic meaning. To guarantee this, the authors extend the idea of an approximate Bayesian inference approach with posterior regularisation. Posterior regularisation allows the authors to control the meaning of the discrete control states. In section 5, the authors give two examples of posterior constraints that they used in different NLP tasks. In experiments, the authors demonstrate that their model with posterior regularisation consistently outperforms the baseline models. Some samples, drawn from the trained model are presented in the paper.\n\nI have no experience in NLP, and thus cannot properly judge the significance of the results section of this work. In my comments, I will therefore focus on the technical aspects of this work. Overall, I found the paper difficult to read. This is probably owed to the fact that this work encompasses amortized variational inference, conditional random fields, and posterior regularisation in about two pages of writing. It makes me wonder if some of the aspects of this work could be simplified in order to make it more concise.\n\nFurther comments\n- Section 3, last equation. It is not clear where the equality comes from. The authors claim that log p(y|x)  - KL(q(z|x,y)|p(z|x,y)) = -KL(q(z|x,y)|p(y,z|x), where the left-hand side of the equation comes from the PRLBO (as defined in the second equation in that section), and the right-hand side of the equation is the ELBO. I believe this equation should have an inequality with, i.e. the authors are optimizing a lower bound on the PRLBO. Expanding on the steps from the second to the third equation might clarify things.\n- In section 4 the authors explain that they model the approximate posterior q(z|x,y) as a structured conditional random field. I understand that in this setting the partition function can be calculated using dynamic programming methods as described in a footnote (which I can only partially follow). This allows the authors to calculate marginals for the approximate posterior and hence allows the authors to calculate the entropy of q(z|x,y) and the PR term. What is not clear to me is how the authors deal with the first term in equation (1). Is z sampled? How is back-propagation performed in this case, as z is discrete.\n- Sec. 7 baseline models, Model Ours+Force: Could the authors explain how they enforce hard constraints on the posterior?\n- What is the significance of treating z as a stochastic variable? What would happen if you leave out the KL term in the ELBO? What would happen if you use a deterministic auto-encoder only with posterior regularisation?\n- By now, it kind of folk knowledge in the community that in VAEs with a strong enough decoder, the encoder will be ignored. Does you approach suffer from this problem? How can it be counteracted?\nThe authors mention computation \n\nMinor comments\n- Authors need to number their equations\n- The top line of the sum and product symbols in equations are not correctly printed on paper. I printed the paper two times, and both times this was the case.\n"
        }
    ]
}