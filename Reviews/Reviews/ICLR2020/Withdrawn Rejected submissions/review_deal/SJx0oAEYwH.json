{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper proposes a filtration based on the covers of data sets and demonstrates its effectiveness in recommendation systems and explainable machine learning. The paper is theory focused, and the discussion was mainly centered around one very detailed and thorough review. The main concerns raised in the reviews and reiterated at the end of the rebuttal cycle was lack of clarity, relatively incremental contribution, and limited experimental evaluation. Due to my limited knowledge of this particular field, I base my recommendation mostly on R1's assessment and recommend rejecting this submission.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper presents an interesting filtration method to find staple maps, which proves effective in the recommendation system and explainable machine learning. The new framework is built upon a new concept termed cover filtration based on the generalized Steinhaus distance derived from Jaccard distance. A staple path discovery mechanism is then developed using this filtration based on persistent homology. Experiments on Movielense and FashionMNIST has quantitatively and qualitatively demonstrated the effectiveness of the new model. It also showcases the explainable factors with visualized samples from FashionMNIST dataset. In addition, theoretical discussion in the appendix makes the theory part of this work solid and convincing.\n\n* The method introduced in this paper is intuitive and demonstrates with meaningful outcomes.\n\n* The method itself is built upon the SOTA method “Mapper”. Authors may want to clearly state the difference between the new method and Mapper in writing.\n\n* The overall structure of this paper needs some improvement. Authors attempt to introduce a new distance metric as an intermediate level representation. This is not very clear at the beginning; instead, most of the introduction is related to TDA and mapper, which may confuse people not from this particular field.\n\n* What is the meaning of D_St under definition 3.1? Is this the same as d_St or not?\n\n* It seems the restrictions in the experiments are strong, which can be found from a few settings in the experiments. For example, the authors claim that “To reduce computational expenses and noise, we remove all movies with less than 10 ratings and then sample 4000 movies at random from the remaining movies.” It seems the model may not work well in a more general case...\n\n* The pre-processing steps for filtration cover seem verbose on FashionMNIST “The model is evaluated at 93% accuracy on the remaining 10,000 images. We then extract the 10-dimensional predicted probability space and use UMAP (McInnes & Healy, 2018) to reduce the space to 2 dimensions. This 2-dimensional space is taken as the filter function of the Mapper, using a cover consisting of 40 bins along each dimension with 50% overlap between each bin.” Not sure if this method will work well without fine-tuning or feature selection like this.\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "1. Paper summary\n\nThis paper develops a novel filtration for the analysis of based on the\nidea of *covers* of data sets. The filtration employs the notion of\na generalised Jaccard distance to define stable paths within the nerve\nof the cover. These stable paths are then shown to be useful for the\ncreation of 'gentler' transitions for recommendation systems, as well\nas the development of explainable supervised machine learning models.\n\n2. Review summary\n\nI found this paper to be very interesting in terms of its new\nperspective on filtrations and its idea of the introducing a\n'stability' concept inspired by topological persistence.\nHowever, I recommend rejecting the paper in its current form due to the\nfollowing issues:\n\nA. Missing clarity: the paper requires readers to be familiar with the\n   Mapper algorithm and glosses over important details. Several results\n   that are mentioned as core contributions in the main texts are only\n   stated in the supplementary materials.\n\nB. Missing experimental depth: the experiments shown in the paper are\n   interesting, but only scratch the surface. To have a convincing case\n   study, a more quantitative analysis is required. At present, the\n   selection and depiction of results seems ad-hoc. I realise that,\n   when the foremost goal is interpretability, providing a quantitative\n   analysis is not always easily possible. Nevertheless, each data set\n   should be used for more than one case study; ideally, multiple\n   results that are 'surprising' can be reported. For example, are\n   there paths that do not match our intuition? What do they tell us\n   about the data set?\n\nC. Doubts about the technical correctness: the supervised learning\n   example requires an elaborate setup (dimensionality reduction,\n   UMAP for further reductions, setting up Mapper with additional\n   parameters). I am wondering how trustworthy any result obtained\n   from such an analysis can be. I would suggest discussing the\n   parameter selection process in more detail and ideally providing more\n   information about the impact of these choices.\n\nIn the following, I will comment on these issues in more detail.\n\n3. Clarity\n\n- Concepts such as 'cover' should be briefly introduced; already the\n  abstract presumes that readers are familiar with several TDA concepts\n\n- I would shorten the abstract to improve its flow; the paragraphs 'We\n  demonstrate...' and 'For explainable...' could also be added to the\n  introduction to improve the exposition of the paper.\n\n- The introduction already starts with TDA concepts; a brief motivation\n  would make the paper more accessible\n\n- I disagree with the statement that the existence of a metric on the\n  data is an implicit assumption of TDA. My perspective is quite the\n  opposite: TDA is rather flexible *because* of its support for\n  different metrics; Vietoris--Rips complex calculation, for example,\n  only requires a matrix of pairwise distances. Hence, I would rewrite this\n  sentence.\n\n- To add to the previous point: the paper itself introduces a new\n  metric based on a generalised Jaccard distance. I feel that the\n  discussion of 'ill-fit or incomplete' metrics detracts from the\n  main message of the paper.\n\n- The first explanation/definition of Mapper in the introduction is\n  rather complicated ('nerve of a refined pullback cover'); I would\n  suggest rephrasing this.\n\n- The point about the $1$-skeleton warrants more explanation: it is my\n  understanding that the paper uses paths that are defined using this\n  skeleton as well. My first pass of the paper slightly confused me\n  because I figured that the described paths would be generalised over\n  high-dimensional simplices as well. I would suggest making this\n  'restriction' (it is not a proper restriction because paths can be\n  defined for arbitrary simplices, but it is a restriction in terms of\n  the dimensionality) or property clear from the beginning.\n\n- The discussion of hypercube covers is repeated multiple times in the\n  introduction and the related work section; I would suggest mentioning\n  this only once as it does not have to a large bearing on the methods\n  described in the paper anyway.\n\n- Discussing stability of the paths/cover in the introduction is\n  misleading because these aspects are only discussed in the appendix.\n  Same goes for some of the contributions in Section 1.1; the\n  conjectures or connection to other filtrations are only mentioned in\n  the appendix.\n\n  This needs to be rewritten; I would pick a few contributions that the\n  main text can focus on and mention the rest in passing.\n\n  The paper lacks this structure at present and for a conference\n  submission, all main contributions should also be a part of the main\n  text.\n\n- Stability is meant to reflect a property of a cover; this could be\n  clarified by means of extending Figure 1, for example. During my\n  first pass of the paper, some of the subsequent definitions were\n  lacking a clear motivation. I would suggest to _clearly_ state that\n  the goal is to circumvent issues with paths in a cover that only\n  depend on a few data points.\n\n- Moreover, it would be interesting to point out to what extent the\n  method presented here is the _only_ one capable of doing this. It\n  occurred to me that there's a preprint that discusses how to stabilise\n  the calculations of persistent homology (with respect to picking the\n  _same_ creator simplex for a feature, for example):\n\n    Bendich, Paul et al.\n    Stabilizing the unstable output of persistent homology computations\n    https://arxiv.org/abs/1512.01700\n\n  It seems that one of the unique features of the method in this paper\n  is that the definition of path stability aligns exactly with the other\n  stability concept---the paths are thus indeed stable with respect to\n  perturbations of the underlying data set.\n\n  Maybe this could be mentioned as an interesting feature.\n\n- The paragraph 'In Section 4.1 [...]' in the related work section is\n  somewhat redundant; I would suggest putting it at the beginning of the\n  respective experimental section and merging it with the existing\n  description there.\n\n- The definition of the Steinhaus distance strikes me as unnecessary\n  complex; while it is mathematically pleasing to know that arbitrary\n  measures could be used, it seems that this is never exploited anywhere\n  in the paper.\n\n- In the interest of clarity, maybe it would make sense to refer to the\n  distance as a 'generalised Jaccard distance' instead.\n\n- The proof of Theorem 2.5 could be moved to the appendix. Moreover,\n  I think it could be simplified by mentioning that the nerve is\n  constructed from the cover and that weights are assigned based on\n  the number of intersections; then the main result would follow from\n  monotonicity. (the current proof is of course fully correct, I just\n  had to rephrase this result in terms of concepts that I found easier\n  to grasp)\n\n- Concerning the terminology, I find 'a most stable path' to be somewhat\n  confusing at the first read. I now understand what is meant by it, but\n  maybe 'highly stable' or 'maximally stable' would be better.\n\n- In Section 3, the 1-skeleton should be explicitly mentioned.\n\n- Moreover, the notion of 'shortest path', which _could_ also employ\n  distances, should be distinguished from stable paths; if I understand\n  the argumentation correctly, shortest paths are defined in terms of\n  the number of edges, while stable paths are defined in terms of the\n  weights along those edges. I would suggest spelling this out directly\n  to make the concepts non-ambiguous.\n\n- In Definition 3.1, it should be $d_{St}$, not $D_{St}$, I think.\n\n- Definition 3.1 could also be intuitively summarised as 'the largest\n  edge weight along a path', right?\n\n- The algorithm in Figure 2 should include some comments that make the\n  procedure  more understandable. I found the conceptual leap to Pareto\n  frontiers confusing at first; maybe this could be motivated better.\n\n- Figure 3 needs more explanations. In particular the connection to\n  persistence could be elucidated---I get that there's an immediate\n  connection (see the paper above) but this concept needs at least\n  a brief introduction. Moreover, if this connection is relevant, it\n  deserves to be elucidated in the paper explicitly.\n\n- Figure 4 needs additional labels; I expect that 'distance/stability'\n  are shown in the caption. Is this correct?\n\n  In general, I found the path visualisation not so helpful; maybe it\n  would be better to show the full 1-skeleton (or an excerpt) and\n  highlight the corresponding paths?\n\n- The language and motivation in Section 4 is somewhat informal; this is\n  not an issue for me, but I wanted to mention it as a something that\n  could potentially be rephrased.\n\n- The additional application domains should be moved from Section 4 to\n  a discussion section.\n\n- The cover generation strategy could be understood more rapidly if\n  a small illustration was provided. Do I get it correctly that every\n  *cover set* constitutes a film, while the overlap between them is\n  based on whether the same user also rated another film?\n\n- The path selection procedure for the experiment in Section 4.1 needs\n  to be explained formally. I get that it is akin to selecting a point\n  on the 'elbow' of a curve, but this should be briefly explained.\n\n- Figure 5 needs to be referenced more prominently in the text.\n\n- Section 4.2 is glossing over many important details of the definition\n  of Mapper. These are absolutely required, though, and any parameters\n  selected here should also be analysed to learn whether they affect the\n  results.\n\n- I am not sure about the utility of the paths in Figure 7\n\n- Figure 9 needs more explanations; as mentioned below, why not compare\n  this path with a geometry-based path in the embedding? Moreover, why\n  are there not the same number of images for all classes?\n\n- The conclusion needs to be rewritten; the paper does *not* show\n  stability properties in the main text\n\n4. Experiments\n\n- As a general question, I would like to see how stable the\n  interpretations of the different sections are. There is always\n  a degree of stochasticity when training a neural network, so does\n  the output of Mapper stay stable?\n\n- I am not sure about the relevance of experiment in Section 4.1; it\n  does not really provide a link to an explainable model for me, but\n  instead is more along the lines of 'gentle interpolation in a latent\n  space'. I could see that this has its uses, but an evaluation requires\n  more than one example. I would be particularly interested in knowing\n  about cases in which result is unexpected. Do such cases exist?\n\n- Please comment on the stability of the sampling procedure in Section 4.1\n\n- I understand that Section 4.2 wants to show how algorithms like Mapper can\n  act as a middle ground between black-box and white-box models. Is this\n  correct? If so, the chosen example is _not_ sufficiently complex to be\n  compelling. Would it not be equally appropriate to rewrite this\n  experiment in terms of explaining paths in the latent space of\n  autoencoders? In this context, I see more of a need for these paths as\n  an alternative to paths that purely employ latent space geometry.\n\n- Adding to this, the autoencoder context would it also make possible to\n  compare paths generated using Mapper and the proposed as well as the\n  paths generated by the model. Such an experiment would be more\n  compelling, I think.\n\n- In general, what are the implications of understanding a path as in\n  Figure 9? How could the model updated to account for this? If the\n  paper were to include an example of how to *fix* a model based on such\n  information, it would a highly compelling use case.\n\n- Moreover, would it be possible to detect 'problematic' regions such as\n  the one shown in Figure 9 automatically? If so, it would again provide\n  a highly compelling example. Otherwise, I think that the\n  interpretation of a model depends again on humans and are thus\n  restricted based on model complexity.\n\n5. Minor style issues\n\n- Mapper should be capitalised consistently\n- I suggest removing the paragraph 'Organization' as it is redundant\n- 'mod differences' --> 'modulo differences' (?)\n- 'distanceof' --> distance of'\n- 'on undirected graph' --> 'on an undirected graph'\n- 'Principle component analysis' --> 'Principal component analysis'"
        }
    ]
}