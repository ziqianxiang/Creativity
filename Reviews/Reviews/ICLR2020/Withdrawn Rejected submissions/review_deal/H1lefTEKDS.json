{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper provide an extensive set of benchmarks for Deep Model-based RL algorithms.\n\nThis paper contains a large number of algorithms, environments, and empirical results. The reviewers all recognized the need for such a study to provide some clarity, insights, and common standards. The reviewers we concerned about several aspects of the implementation of the effort. (1) All the performance is based on 4 runs, smoothed curves, and default errors (often extensively overlapping). The paper cites Henderson et al, and yet does not follow the advice laid out therein. (2) The results were fairly inconclusive---perhaps to be expected---we didn't learn much (more on this below). (3) The paper has communication issues.\n\nThe overall approach taken was a bit perplexing. Some algorithms we given access to the dynamics. The reward functions were converted to diff. forms, and early stopping in a domain specific way was employed. This all seems like simplifying the problem in different ways so that some methods can be competitive, but it is not at all clear why. If we take the typical full rl problem and limit domain knowledge, many of these approaches cannot be applied and others will fail. Those are the results we want. One could actually view these choices are unfair to more general algorithms---algorithms that need diff rewards pay no price for this assumption. This also leads to funny things, for example, like using position as the reward in mountain car (totally non-standard, and invalid without discounting). The paper claims a method can solve MC, but that is unclear from the graph. The paper motivates the entire enterprise based on the claimed lack of standardization in the literature, but then proceeds to redefine classic control tasks with little discussion or explanation. \n\nThe paper has communication issues. For example, all the domains are use continuous actions (and the others in the response highlight that is their main focus), but this is never stated in the paper. The paper refers to and varies \"environment length\", but this was not defined in the paper and has no obvious meaning. The tasks are presumably discounted but the the value of gamma is not specified anywhere in the paper (could be there, but I searched for a while). Pages of parameter settings in the appendix with many not discussed or their ranges justified.\n\nThis paper is ambitious, but I urge the authors to perhaps limit the scope and do less, and consider a slightly broader audience in both the writing and experiment design.   \n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review": "Summary of the paper\nThe authors benchmark 11 model-based and 4 model-free RL algorithms on 18 environments, based on OpenAI Gym.\nThe performance in each case is averaged across 4 different seeds.  Computation time is also reported.\nFurthermore, the authors analyze the performance hit incurred from adding noise to the observations and to the actions\nFinally, the authors propose to characterize what hinders the performance of model-based methods, ie., what they call the dynamics bottlenecks, the planning horizon dilemma, and the early termination dilemma.\nAs a conclusion, it turns out no clear winner stands out, which motivates further development of model-based approaches.\n\nStrong and weak points\nThis is a very interesting empirical study, especially since\n- it includes a comparison with model-free algorithms,\n- it considers computational aspects and indicates what algorithms can be run in real-time,\n- the authors use open-source software (PyBullet) as simulators, which makes the study more reproducible (although the code has not been shared yet)\n\nBut,\n- 4 seeds averaged across is clearly low, given the well-known variance of RL algorithms. In fact, the std values in the tables prove this point. I understand the benchmark is heavy on computation, however, this would only have delayed the output of the numbers without requiring more work (and admittedly, been even more harmful for the ecology...).\n- Not as a criticism but rather a suggestion, it would have been useful to summarize the table 2 by comparing the algorithms using normalized values (mean = 0, std = 1) averaged across the environments. \n- One the (strongest) weak points for me remains the assumption of the given differentiable reward function, as learning the reward function might be challenging for the model, for instance when it is sparse.\n- It is also surprising that the authors did not benchmark against nor cite (Ha, David, and Jürgen Schmidhuber. “Recurrent world models facilitate policy evolution.” NeurIPS 2018), especially since the code is open-source. \n- I would have made the same remark for (Learning Latent Dynamics for Planning from Pixels,  Hafner et al.) but the paper does not seem to be not peer-reviewed. Edit: the paper is indeed published at ICML 2019. So the remark holds. \n\nQuestion:\n- could the author elaborate on the early termination? it is not precisely defined anywhere and yet, seems to be an important point.\n\nMinor\n- page 3, parentheses around Finn et al. (2017)\n- page 3, ILQG, “is an model”"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper presents a systematic empirical evaluation of model-based RL algorithms on (mostly) continuous control environments from OpenAI Gym, with comparison to popular model-free algorithms. It identifies three challenges for model-based RL, learning the dynamics, selecting the planning horizon, and applying early termination to guide learning.\n\nA systematic comparison of model-based RL algorithms is missing from the literature, and I believe that this paper does a fairly thorough job of providing such a comparison. A wide range of algorithms are selected, and the environments are representative of those commonly used in the literature. The first two challenges identified have been recognized in the literature. For example, Vemula et al. (2019) [1] discuss the planning horizon in random search RL algorithms.\n\nHowever, I would like to see some results on the policy search algorithms such as PILCO in Section 4.5, even if they are on the simpler environments. Currently they are not represented in Table 4. \n\nMinor comment:\n1. There are several instances where the writing should be clarified, e.g. acronyms are not explained before they are used. For example, it would be helpful to the reader to define GT-CEM and GT-RS in Section 3, especially as Table 1 (page 6) comes before the text discussing those two algorithms (page 7). \n2. Table 1 is a bit difficult to parse. Maybe it could be split up, or some algorithms/environments deferred to the appendix.\n\n[1] Vemula, Anirudh, Wen Sun, and J. Bagnell. \"Contrasting Exploration in Parameter and Action Space: A Zeroth-Order Optimization Perspective.\" The 22nd International Conference on Artificial Intelligence and Statistics. 2019.\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper performs an empirical comparison of a number of model-based RL (MBRL) algorithms over 18 benchmarking environments. The authors propose a set of common challenges for MBRL algorithms.\n\nI appreciate the effort put into this evaluation and I do think it helps the community gain a better understanding of these types of algorithms. My main issue with the paper is that I don't find the evaluation thorough enough (for instance, no tabular environments are evaluated) and the writing still needs quite a bit of work. I encourage the authors to continue this line of work and improve on what they have for a future submission!\n\nMore detailed comments:\n- Intro: \"model-free algorithms ... high sample complexity limits largely their application to simulated domains.\" I'm not sure this is a fair criticism. Model-based are also mostly run on simulations, so sample efficiency is not necessarily the cause model-free are only run on simulations. Further, this statement kind of goes against your paper, since all your evaluations are on simulations!\n- Intro: \"2) Planning horizon dilemma: ... increasing planning horizon... can result in performance drops due to the curse of dimensionality...\" and \"similar performance gain[sic] are not yet observed in MBRL algorithms, which limits their effectiveness in complex environments\" goes against the point you made earlier on in the intro about sample efficiency.\n- Preliminaries: \"In stochastic settings, it is common to represent the dynamics with a Gaussian distribution\", this is only for continuous states. It would be nice if you could evaluate tabular environments as well.\n- Sec 4.1: \"we modify the reward funciton so that the gradient... exists...\" which environments were modified and how did they have to be modified?\n- Sec 4.1: You discuss early termination but have not defined what exactly you mean by it.\n- Fig 1: 12 curves is still a lot and really hard to make much sense of. A lot of the colors are very similar.\n- Sec 4.1: \"it takes an impractically long time to train for 1 million time-steps for some of the MBRL algorithms\" why?\n- Table 1 is a *lot* of numbers and colors and is really hard to make much sense of. There are also so many acronyms on the LHS it's difficult to keep track.\n- Table 2: What about memory usage?\n- Sec 4.4: \"Due to the limited exploration in baseline, the performance is sometimes increased after adding noise that encourages exploration.\" Why does this noise not help exploration in the baselines?\n- Sec 4.5: \"This points out that when learning models more data does not result in better performance.\" This seems like it's closely correlated with the particular form chosen for the model parameterization more than anything.\n- Fig 3: y-axis says \"Relative performance\", relative to what?\n- Sec 4.7: \"Early termination...is a standard technique... to prevent the agent from visiting unpromising states or damaging states for real robots.\" I've never seen this used as a justification for this.\n- Sec 4.7: \"Table 1, indicates that early termination does in fact decrease the performance for MBRL algorithms\", and then you say \"We argue that to perform efficient learning in complex environments... early termination is almost necessary.\" Those two statements contradict each other.\n\nMinor comments to improve writing:\n- When using citations as nouns, use \\citep so you get \"success in areas including robotics (Lillicrap et al., 2015)\" as opposed to \"success in areas including robotics Lillicrap et al. (2015)\" (the latter is what you have all throughout the paper).\n- Sec 3.1: s/This class of algorithms learn policies/These class of algorithms learn policies/"
        }
    ]
}