{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper introduces methods for distilling pretrained contextual representation to a static embeddings for the faster use. It proposes subword pooling and context combination, and its contribution is demonstrated with a suite of classic experiments for static word embeddings.\n\nMy score for this paper is weakly rejected because (1) the motivation of the proposed approach is not clear. The paper title is about “distill”, but I can’t find out what the methods try to distill, what is the objective; (2) the goal of the paper is to obtain a static embedding performing like ELMO or BERT. As we know, the power of the contextual embedding is demonstrated by different NLP downstreaming task like MT, QA and a bunch of text classification tasks. In this paper, I don’t find those experiments; (3) this paper compared the results with word2vec and glove, but not clear which version to compare. And there are some re-embedding approaches that improve word2vec and glove such as “Word Re-Embedding via Manifold Dimensionality Retention”, the table in this re-embedding paper shows different version of glove and some of them are better than the proposed methods distilled from BERT.\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This work introduces very simple methods that use contextualized embeddings from large-scale pretrained language models like BERT to distill / generate static Word2Vec-style word embeddings. The motivation for doing so is twofold: (1) static embeddings are faster to use and easier to understand, which makes them more appropriate for some real-time or resource-constrained systems; and (2) we can then use a variety of analysis methods that have been developed for static embeddings to analyze their contextualized counterparts. The proposed method is very simple, just pooling embeddings of a given word type across multiple contexts (if the word is decomposed into subwords in a context, its subword embeddings are averaged), and there is extensive evaluation across different combinations of pooling functions, pretrained base model, and layer to distill. The authors demonstrate that these distilled word embeddings outperform Word2Vec/GloVe on various word similarity/ relatedness tasks and reveal interesting insights into these embeddings. Additionally, they investigate social bias in contextualized representations and reveal inconsistencies in existing techniques for studying social bias. \n\nOverall, I found this paper to be well-written and the bias experiments in particular to be compelling. However, I felt that there were too many obvious questions left unexplored, which I've noted below, and I didn't quite buy some of the explanations of the results. As such, I am a weak reject for now.\n\ncomments:\n- While mean pooling is simple conceptually and to implement, I do wonder if the authors could have gone further with the methodology. A natural continuation is to weight different contexts differently (e.g.,  upweight common contexts and downweight rare ones). Another is to extend the idea to multisense word embeddings (e.g., through some clustering over contexts). \n- One confounding factor that could affect the experimental results is the embedding dimensionality. The GloVe / Word2Vec embeddings are both 300d, while I suppose BERT's are 768d. Of course, it is hard to control for this when using pretrained public models, but perhaps PCA or some other dimensionality reduction technique could have been applied to BERT-derived embeddings before the comparison. As is, it is unclear how much of the improvement comes from the model size. \n- I would have liked to see the performance of the distilled static embeddings measured across a variety of downstream tasks (i.e., by plugging the embeddings into some task-specific model), not just word similarity / relatedness as these are not particularly insightful. Also, if the authors did this experiment, they could measure the change in the distilled embeddings before / after finetuning on the downstream task.\n- Fig 1 & 2 are hard to read and understand. In Fig 2, the entries in the legend don't seem to be explained anywhere. What are \"ADJ\" and \"PROF\"?\n- The result that social bias is not consistent across different layers of BERT is perhaps to be expected in light of other similar results (e.g., the bottom layers of ELMo and the middle layers of BERT seem to contain more syntactic info than others as shown by Hewitt & Manning 2019).\n- I wonder if the inconsistency in layer-wise patterns across different bias measures is not an issue with the bias measures themselves but rather with the assumption that distance metrics (e.g., cosine) are as meaningful with distilled static embeddings as they are with Word2Vec / GloVe. The latter approaches incorporate the inner product between two embeddings into their relatively simple training objectives, while the distilled BERT embeddings are computed from highly nonlinear Transformer layers. Perhaps for some layers these distance metrics are not meaningful? \n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper proposes to compute static (context-independent) word embeddings from dynamic (contextual) models such as CoVe, ELMo, and BERT by pooling subword encodings with sampled sentences in each layer for each word and compares different pooling methods (max, min, mean, last) on word similarity tasks. Further, the authors provide analysis for the social bias in the produced embeddings with existing approaches. \n\nStrength: 1) The paper presents extensive comparisons with different experiment settings; 2) the social bias analysis is very interesting.\n\nWeakness:\n\n1) Novelty:  The techniques to compute static embeddings are quite simple, the authors basically test all combinations of pool methods and different numbers of sampled sentences. The technical contribution is very limited. \n\n2) Presentation: The figures are not very clear. The authors should at least prevent using the same line color for different settings in the same figure (Figure 1). It's hard to tell which combination of pool methods is the best. \n\n3) Experiments: The layers used to compute static embeddings are different in Table 1 and Table 2 for different tasks. It seems that the authors chose the best scores across layers for different tasks. It's better to compute scores using the same layer for different tasks and compare the layers on the averaged score across tasks. Otherwise, there's no take-away on which layer we should use in new tasks that do not have golden labels. The choice of using the floor(X/4)-th layer in the social bias analysis is also a bit of arbitrary. \n "
        }
    ]
}