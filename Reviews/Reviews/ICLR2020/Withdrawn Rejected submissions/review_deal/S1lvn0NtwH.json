{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper presents an understudied bias known to exist in the learning patterns of children, but not present in trained NN models.  This bias is the mutual exclusivity bias: if the child already knows the word for an object, they can recognize that the object is likely not the referent when a new word is introduced.  So that is, the names of objects are mutually exclusive. \n\nThe authors and reviewers had a healthy discussion. In particular, Reviewer 3 would have liked to have seen a new algorithm or model proposed, as well as an analysis of when ME would help or hurt.  I hope these ideas can be incorporated into a future submission of this paper.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #2",
            "review": "This paper targets at studying the mutual exclusive bias which existed in children learning, to help understand whether there exists similar bias in deep networks. \n\nIn general, the whole paper tries to tell a very interesting, and good story. The paper is very well organized and written. However, I have the following concerns.\n\n1, the ME problem is quite similar to the concept ontology, e.g. , a “Dalmatian,” a “dog”, or a “mammal”.  So what’s the key difference? Hierarchical learners can avoid this problem.\n\n2, I would say, the SOTA neural networks fundamentally are just representation learned, i.e., feature representation learning. The learned features could in principle be employed to construct advanced learners, .e.g., hierarchical Bayesian. It’s probably a bit unfair or misleading to claim neural networks suffering from ME bias. \n\n3, In Sec. 3, the first and section paragraph, I can not quite understand how the ME bias theory guide the following synthetic experiments. Please give more explanations.\n\n4, in Sec. 3.1, considering the small number of training instances, whether it is large enough to train the NN/ not overfitting issues?\n\n5. Whether the ME bias mostly attributed to the one-hot representation? If one uses word2vec as the representation in NN, the ME bias will be solved. Actually, this is the standard practice in some learning tasks, e.g., zero-shot learning. \n\n6, the experimental design of Sec. 4.2 is also a bit unfair. It seems to me that the tasks are organized as unbalanced instances of each classes, and asking the common NNs to learn this  task. Of course, this common NNs can not address it.\n\n----\nI read the rebuttal. the authors clarified and answered the questions. I would like to raise the score.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #1",
            "review": "Summary:\n\nThis paper makes an observation that most of the neural network architectures do not learn the mutual exclusivity (ME) bias: if an object has one label, then it does not need another. Authors demonstrate this in both synthetic tasks and real-world tasks like object recognition and machine translation. Authors argue that ME bias could help the model to handle new classes and rare events better.\n\nMy comments:\n\nI very much enjoyed reading this paper. I support accepting this paper. It highlights one of the missing inductive biases in ML and proposes it as a challenge. As the authors also agree, ME bias is missing not just in DNNs. It is the issue of MLE. It would be good to have some non-NN results too. I see this is a challenge for MLE than DNNs.\n\n1. In figure-4 you mention that entropy regularizer helps to keep the initial ME score. Can you elaborate more about the way in which entropy regularizer is used with regular MLE training?\n2. It is not very clear how is the base rate computed in Figure 5. I have a guess. But it is better to explain it in detail.\n3. Section 4.2 need more clarity. For example, what do you mean by classifying the image as “new”? Is “new” a class name? Also, how is P(N|t) computed? Please explain.\n4. Are the authors willing to release the code and data to reproduce the results?\n\nMinor comments:\n\n\n1. Page 3: second para, line 4: “our aim is to study”\n2. Page 5: last line: estimate for -> estimated for\n3. Section 4.2: 3rd line: “the class for the from”\n\n=====================================================\n\nAfter rebuttal: I have read the authors' response and  I stand by my decision.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "*** Increased to weak accept after discussion of merits of ME bias was improved in the paper *** \n\nThis paper investigates whether neural networks exhibit a ‘mutual exclusivity (ME) bias’, whereby novel inputs tend to be associated with previously unseen outputs, an inductive bias that is cited to be present in children when learning to associate new words and objects. Via three different sets of experiments, the authors conclude that, under standard training procedures, neural networks in fact display an anti-ME bias: when faced with a novel input, they tend to assign less probability mass to unobserved outputs than is justified by the incoming data. The authors go on to argue that explicitly designing neural networks to reason by mutual exclusivity could lead to faster and more flexible learning. While the experiments across several domains convincingly show the presence of an anti-ME bias in neural network training, I recommend that the paper be rejected because it falls short in demonstrating to what extent performance could be improved by correcting or reversing this bias. The authors pose an interesting hypothesis, but it would gain a lot of credibility if they could provide an empirical analysis of an algorithm that uses ME reasoning to improve learning in a realistic setting or if they could at least perform some quantitative analysis of the effects of ME bias on task performance for a toy example.\n\nComments / questions:\n* The authors duly acknowledge that ME bias is not necessarily desirable in all circumstances, namely in tasks that feature many-to-one mappings, citing polysemy and synonymy in machine translation as examples. Without an empirical example in a realistic setting, it is hard to judge whether the benefits of introducing an ME bias for better classification of new inputs belonging to new classes can outweigh the negatives via a potential increase in misclassification of those belonging to old classes.\n* For the synthetic dataset one-to-one mapping of one-hot vectors it is clear that an ME bias would incur an advantage for classification on a zero-shot basis, i.e. an increased an accuracy for the first time a new input is observed. Is ME bias considered to be useful here because (i) we care directly about improving this type of zero-shot classification, or is it (ii) because it's implicitly assumed that a better initial prediction will lead to faster learning on these examples?\n    * If it is (i) we care about, then it would be useful to quantify the advantage gained either empirically or analytically. It’s not obvious to me how useful the ME bias would be when there is a large number of classes since the probability assigned to the true novel class with a perfect ME bias would be 1 / (# unseen classes). So at the beginning of training, while there are lots of new classes to benefit from the ME bias, the maximum advantage per example is small, and vice versa at the end of training - how do these conflicting effects balance each other out over the course of training and how do the benefits of ME bias scale with the number of classes? Presumably, whether the benefit scales well will depend on the loss function - in any case, this warrants some analysis/discussion especially given that there are no experiments to test it.\n    * The authors say that “ME can be generalized from applying to 'novel versus familiar’ stimuli to instead handling ‘rare versus frequent’ stimuli” and they cite the fact that neural networks take longer to learn from rare stimuli, suggesting that ME could help for reason (ii) above. It’s not obvious to me that ME bias will provide a significant advantage in a one-shot or few-shot learning setting - i.e. how useful is having made a better initial guess (which is random amongst the unseen classes) after you gain some specific information about the target class? Perhaps the benefit is small relative to that of the one-shot or few-shot information gained - I think this needs to be quantified empirically.\n* It would be interesting to see a more detailed analysis of the predictions of the image classifiers in section 4.2. When a new image is presented, how is the probability mass distributed across previously seen classes versus across unseen ones? From a strategic standpoint, in a scenario where it is difficult to determine whether a new image belongs to a new class or not, could it plausibly be more sensible for the network to make a strong prediction on its best guess out of previously seen classes (that it knows more about) rather than a scattered prediction on the unseen classes? Admittedly this is a difficult question to quantify, but my point is to question whether the anti-ME bias shown in Figure 6 is necessarily suboptimal, given the difficulty of classifying a new image as a new class or not.\n\nMinor comments /questions not affecting review:\n* Is the acronym ME pronounced like the word “me” or is it spelled out “M-E”? If the latter, then all cases of “a ME bias” should be corrected to “an ME bias”.\n* Section 4.2 line 3: “sample the class [for the] from a power law distribution\"",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}