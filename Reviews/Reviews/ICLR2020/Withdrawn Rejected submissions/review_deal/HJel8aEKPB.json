{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper study the effect of Lipschitz regularization in GANs. They first show empirically that the choice of Lipschitz constant doesn't affect too much the training. They confirm this observation by showing that the Lipschitz constant only affect the scale of the output and the gradient of the discriminator. They then show both experimentally and theoretically that the choice of Lipschitz constant also affects the size of the domain. Finally the paper show that instead of choosing the Lipschitz constant we can instead use a parameter $\\alpha$ that scales the output of the discriminator.\n\nSome of the observations are quite interesting, however the organization and the poor writing of the paper makes the contribution unclear. Thus I think this paper should be rejected.\n\nMain argument:\n- According to the author when the loss is linear, the choice of Lipschitz constant has no effect, because it only scales the gradient of each parameters by some constants which the authors argue is corrected by methods such as RMSProp. I still believe this could have an effect at the beginning of training when $E[(k_{SN}^mg)^2]$ has not \"converged\" yet, and could still suffer from vanishing or exploding gradient as mentioned by the author. In particular, this would make the different layer of the discriminator learn at different speeds, and this would scale down or up the gradient of the generator. Thus it might be necessary to slightly adjust the learning rate at the beginning of training, especially the gradient of the generator. Can the author discuss how they chose the learning rates in their experiments ? I'm not sure the empirical observations would hold if the authors were to use different learning rates for different choice of $k_{SN}$.\n- The idea of introducing some hyper-parameter $\\alpha$ to scale the output of the discriminator, is not entirely novel. For example for NS-GAN this is similar to changing the temperature of the sigmoid which is already a known trick to modify the smoothness of the prediction. So I'm not sure how much this is a new contribution.\n- The claim that the loss function should be strictly convex for better convergence but linear to avoid mode collapse, is weakly supported in the paper. In the current state I don't think there is enough evidence in the paper to claim that.\n- It would be nice to run the experiments with different seeds and report the mean and standard deviation.\n\nMinor comment:\n- Notation are a bit confusing in equation 5, should be clarified that this the RMSProp update of the weights for the layer at depth m, if the loss function is linear.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary\nThe present work investigates the mechanism by which Lipschitz regularization influence GAN training. It makes the observation that under scale invariant activation functions and optimizers (for instance, RELU and RMSProp), the value of the Lipschitz constant in GAN variants like WGAN or spectrally normalized GAN only results in a multiplicative constant of the generator output. Therefore, it argues that the role of the commonly used 1-Lipschitz penalty is not to ensure small discriminator gradient, but rather to restrict the relevant regions of the loss function and the size of the gradients. To support this claim, it is shown that in the limit of small Lipschitz constants the size of the domain of attained discriminator outputs shrinks. For large Lipschitz constants, however, the performance of all GAN variants other than WGAN detoriates, which is explained with the fact that the relevant domain of the loss function is not restricted any more.\n\nDecision\nI believe that it is an interesting pursuit to better understand the role that Lipschitz regularization plays in GAN training and I much appreciate the thorough experiments provided by the authors. However, in the present form, key points of their analysis too vague (see examples below), which is why I vote for rejecting the paper\n\nIt is unclear why the paragraph above Hypothesis 1 implies that \"the choice of loss functions contributes little\", as claimeed just before the hypothesis.\n\nHypothesis 1 inadequatly summarizes the introduction of Liptschitz regularization which was largely motivated by the fact that if the data and generated distribution are mutually singular (for instance, because the data is concentrated on a low-dimensional structure), loss of the optimal discriminator does not depend on the choice of generator. In the original WGAN paper, WGAN is motivated as computing an approximate minimizer of the wasserstein distance between the data and the generated measure. As is evident from the Kantorovich duality that changing the Lipschitz constraints on the discriminator only amounts to applying a scaling to the resulting distance.\n\nThe paper sometimes mentions the role of Lipschitz regularization and sometimes the role of the choice of Lipschitz constant. However the interpretation of WGAN as approximating the Wasserstein distance readily shows that the Lipschitz regularization can matter even in cases where the exact choice of Lipschitz constant does not. For instance, it claims \"The equivalency between tuning Lipschitz constants and domain scaling (Eq.11) implies that the performance improvements of various Lipschitz regularized GANs (Table 2) originates from the restriction of the loss function\" Tuning the Lipschitz constant is equivalent to rescaling of the loss function. However, imposing a Lipschitz constraint at all is not equivalent to restricting the output range of the discriminator, since a function of arbitrary small range can have arbitrary large Lipschitz constant. Thus, it can not be concluded that the improvement due to Lipschitz regularization is due to a restriction of the loss function.\n\nSuggestions for improvement\nThe observation that spectral regularization often amounts to rescaling the entire network and that this in turn can be seen as a restriction of the relevant domain of the loss function might still be a useful observation, but as outlined above important parts of the conclusions are unclear in the present form."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper conducts extensive experiments of Lipschitz regularization on GAN models with different loss functions and observes that typical GAN setups are insensitive to the choice of Lipschitz constant. And it claims that Lipschitz regularization just scales the neural network output. What's more important in GAN training is to restrict the domain and interval of attainable gradient values of loss functions, for which the paper proposes so-called ''domain scaling'', because it can avoids the gradient vanishing or exploding problem. \n\nQuestions:\n1. I get confused with the domain scaling (11). If we use very small alpha, the M in Corollary 1.1 will correspondingly become very large (reciprocal relation)? Then how can domain scaling restrict the interval of attainable gradient values?\n\n2. I think the narrative of the paper is not very good and thus confusing about the most significant point of it. There are too much trivial derivation in the main text before Section 4.3 . But it talks little about the proposed domain scaling. I think the authors should spend more time explaining how (11) works."
        }
    ]
}