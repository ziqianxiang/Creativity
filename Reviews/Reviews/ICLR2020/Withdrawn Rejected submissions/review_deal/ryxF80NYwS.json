{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper uses neural amortized inference for clustering processes to automatically tune the number of clusters based on the observed data. The main contribution of the paper is the design of the posterior parametrization based on the DeepSet method.  The reviewers feel that the paper has limited novelty since it mainly follows from existing methodologies. Also, experiments are limited and not all comparisons are made. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "\nIn this paper, the authors consider the neural amortized inference for clustering processes, in which the number of cluster can be automatically adapted based on the observed samples. The proposed algorithm largely follows the standard variational auto-encoder. The major contribution of the paper is the design of the posterior parametrization so that the posterior satisfies the permutation invariant within a cluster, between clusters, and unassigned data, based on the DeepSet method. The model can be incorporated into random communities models. Finally, the authors apply the algorithm for neural spike sorting problem. \n\n\nThe paper is well-organized and easy to follow. However, there are two issues should be addressed:\n\n1, The novelty of the proposed algorithm might not enough. The two major components in this paper, i.e., VAE and DeepSet, are all carefully investigated before. This paper applies the DeepSet parameterization in the VAE framework.\n\n2, The details of the amortized inference training is not clearly explained. It is well-known that gradient through the discrete random variable is quite difficult. How the gradient for the parameter of the proposed model is calculated should be carefully discussed. The REINFORCE gradient in this model, whose support of c can be as large as the number of samples, can be quite huge. \n\n3, In the empirical evaluation, I was curious why the mean-field and MCMC have not been considered in the spike sorting problem.\n\nI am expecting the authors can address my concerns during rebuttal. \n\n=======================================================================\n\nThanks for the responses to clarify my concerns. \n\nThe learning procedure and experiments are clear now.  Indeed, as a purely variational inference paper,  the discrete variable problem is absent, since the model is always *fixed* and not updated. \n\nThe major contribution of the paper becomes the design of the posterior parametrization by DeepSets, which I think still not enough. \n\nI will keep my score.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper presents a neural network based clustering process where the number of clusters is not known a-priori.  Proposed approach requires conjecturing a generative process (where number of clusters/classes is a random variable) and the model learns to uncover the posterior distribution over clusters given the observed samples.\nOverall I think it is a valuable contribution, well written paper with good results.  \nSpecific comments:\na) Even though the model allows for variable number of clusters, I feel there may be a strong dependence between the number of clusters the model can hypothesize and the number of clusters in the training data.  It will be useful to give further insights into this.  For instance, if an MNIST model is trained with only digits 0-5 training data, how well would it perform in detecting all 10 clusters at test time?  Understanding model’s biases based on training data is one area I feel is important and the paper could add to.\nb) The neural clustering process could potentially be viewed as a transductive inference model for classification of test data.  Typically at test time classification is done for each test sample independently, and the clustering process allows one to bring in other similar test samples to help with classification.  Have the authors considered this and have any comments on potential value / feasibility of this?\nc) In the examples presented in Section 2.3, please clarify how training & testing was done.  Specifically what training data was used (all of MNIST training data?), and the test time clustering was done on a subset of MNIST test data?\nd) Use of ‘q’ for a neural-network and ‘q_\\theta’ for posterior distribution is a little confusing, will be better to have different notation for these.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary:\nThis paper introduces a novel deep learning architecture for efficient amortized Bayesian inference over mixture models. Unlike previous approaches to amortized clustering, the proposed method allows us to treat local discrete labels of data points and infer the unbounded number of mixture components, making it more flexible as in the case of Bayesian nonparametrics. It is shown that the resulting algorithm can be parallelized and applied to both conjugate and non-conjugate models. The authors also suggest an extension to models of random communities and a novel approach to neural spike sorting for high-density multielectrode arrays based on the proposed method.\n\nStrengths:\nThe paper is generally well written and the relationship to previous works is well described. Empirical results seem quite convincing, for example, the clustering results presented in Fig. 2 and Fig. 3 clearly show not only the inferred number of clusters, but also the posterior probability which indicates that reasonable samples are assigned higher probability.\n\nWeaknesses:\n- Overall, the idea looks very original and promising, but I find some technical details are not easy to understand under the current form, especially for non-experts in this domain. I would recommend the authors to elaborate a bit more on the proposed architecture and the variable-input soft-max function in Sect. 2.1.\n- On page 8, the authors mention that the NCP is much more efficient compared with MCMC, for example, in the Gaussian 2D example. However, regarding the DPMM clustering model, it is known that MCMC methods are generally slower compared with variational inference, which is computationally faster. I think it would be interesting to add a discussion or comparison with variational inference in terms of computational efficiency.\n- If I understand correctly, the NCP is essentially based on a sequential sampling procedure. The authors claim that the proposed method is easily parallelized using a GPU, but there does not seem to be sufficient details on the GPU-parallelization of sequential sampling.\n\nMinor comments:\nThe size of some figures appears too small, for example Fig. 6 and Fig. 10, which may hinder readability."
        }
    ]
}