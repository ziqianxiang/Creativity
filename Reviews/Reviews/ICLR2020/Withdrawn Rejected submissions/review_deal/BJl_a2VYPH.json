{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper focuses on synonym based substitution defense from adversarial attacks (SEM) and proposes an improved  \nadversarial attack approach (IGA).\nThe topic is important and the proposed idea could make the  \nadversarial attack more meaningful. \n\nUnlike prior work using spelling checkers (map wrongly spelled  \nwords to correct ones) to reduce the effects of character modifying  \nbased attacks, the authors map word synonyms to a unique  \nencoding.\nThe authors claim that the results demonstrate that SEM can significantly  \nimprove the robustness of neural networks and that IGA can achieve better  \nattack performance compared to existing attacks. The work  \nis generally well-explained.\n\n\nBeing limited to word synonym substitution based attacks makes the paper a little bit narrow, but it is fine if the methods  \nare thoroughly investigated. Could you use any typical and existing  \ncluster algorithms with the SEM encoder? Why do you want to invent something  \nnew (since there is no reference for the proposed Algo. 1, I have to  \nassume that the authors did invent it)?  Are there any advantages of  \nthe proposed Algo. 1 over some typical cluster algorithms?\n\n\nDid you fine-tune the word embedding? if embeddings are not fixed  \nduring training, it may change the Syn(w, delta,k), since calculating  \nSyn(w, delta,k) is based on the embeddings, right?\n\nhow does k affect the experiment?\n\nOne concern from Algorithm 1 is that the mapping E you want to learn  \nis biased by the order of how you traverse in W (line 2 in Algo. 1).\nSuppose you traverse W by the order (...,w_1,w_2,w_3,...) ,  if  w_2  \nis in both Syn(w_1,delta,k) and Syn(w_3,delta,k), then  E(w_2) =  \nE(w_1). However if the order is (...,w_3,w_2,w_1,...), then E(w_2) =  \nE(w_3). I wonder whether this matter or not.\n\nHow about the computational costing time for GSA,PWWS, GA, and IGA ? This could  \nmatter if you compared their attack performance with your proposed IGA.\n\n IGA could traverse all synonyms of a word no  \nmatter what value k is. Does this have a similar impact with the cases  \nwe increase k or delta (the distance threshold)? All these cases will  \nmake the search space bigger and may also cost longer time (like more  \niterations), I think.\n\nIn the extreme cases of IGA, if two words are mutually the only  \nsynonym with each other, it could replace each other for many times.  \nThe good thing is that there was a maximum number, lambda. BTW, what  \nis the maximum number lambda?\n\nThe authors mention they randomly sample 200 examples  \nto evaluate the efficiency of the SEM method. The SEM layer is not that time-consuming during training. I  \nguess that the adversarial attack should be time-consuming? In this  \ncase, changing GA to IGA should be more carefully evaluated, it may  \ncost a longer time.\n\n\n\n\nminor comments\n\n1. In Algo. 1, we can remove \"E[w_j'] \\neq NONE\" in line 5, since it  \nwas already mentioned in line 4.\n2. Can you directly report the numbers in Figure 2?\n3. you mention in the introduction  \nthat the extra encoder is called SEM, and then you call the clustering  \nalgorithms also SEM. So should SEM be the encoder for the defense, or  \njust a new cluster algorithm?\n4. The subscripts for w and w' are confusing. In the first equation  \n(the unnamed one), the subscripts for w' (1,2,...k) seem like the order  \nbased on the distance. In the paragraph just below equation (1),  \nthe subscripts for w' or w (i.e., 1,2,...n) denote the word order based  \non the input x. In Algo. 1, the subscripts for w denote the order in  \nthe vocabulary. It may need to be changed in some ways.\n5. In the first paragraph of Sec. 2, did you explain K? I think K  \n(capital one) and k are not related, could you change one of them to  \nanother letter.\n\n\n\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper presents two main contributions. A modification to an existing synonym based adversarial attack algorithm for text based classification models, and a Synonym Encoding Method which aims to increase a language model's robustness to adversarial attacks. \n\nThe paper is well written and easy to follow and the results are thorough.  It falls prey to a pervasive issue with the adversarial literature however. The defense which they propose, namely ensuring that synonyms share the same encoding, reduces the test accuracy of the model. It has been shown that models which under-perform on the test set often have slightly higher adversarial robustness, and so in order to insure that their defense (instead of simply the lower test accuracy) is responsible for the improved robustness, a standard network should be trained to a similar test performance, and then the robustness of this under trained model should be presented.  This would greatly strengthen the work.  \n\nIdeally however they can find a version of their defense which does not impede the test set accuracy. As is argued by Gilmer et al (Motivating the Rules of the Game for Adversarial Example Research, 2018), Robustness should not come at the expense of test accuracy, given that the test set is the only known distribution that the network will be tested on.\n\nthe paper also presents an improved synonym based attack which is conceptually unrelated to the defense which they propose.  This is an important contribution of the work. And it is strengthened by the human evaluation study they present.  \n\nOverall i think this paper is a borderline case. It is clearly written and has worthwhile contributions, but it could be improved if they tested a similarly performing standard network on the adversarial attacks. And it would be greatly improved if they were able to find an implementation of their defense that did not hinder the test performance."
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Summary:\n\nThe paper propose a novel adversarial attack and defense method for NLP models. The defense method proposes to smooth out the feature space such that features within an epsilon distance to the current features has the same output.\n\nComments\n\n1. The paper is clear written and well motivated.\n\n2. It virtual adversarial training methods in  Miyato, T., Dai, A. M., & Goodfellow, I. (2016). Adversarial training methods for semi-supervised text classification. arXiv preprint arXiv:1605.07725 seems to be related. Virtual adversarial training proposes virtual adversarial samples by adding adversarial samples from adding epsilon noise in the feature space. it would be nice to have a comparison to this.\n\n3. It seems to assume that words within a certain distance to the true x is a synonym. I am if there exist such words that are not synonyms and if there is a way of evaluating what are the percentage of words that are. Since this would obviously effect both the defense and the attack methods.\n\n4. I am curious if smoothing out the output spaces (right before the softmax for example) would have a similar effect and  if the authors could compare to this method.\n\n5. It would be interesting to see more results on other commonly used QA systems such as the SQuAD and MS MARCO dataset. Especially MS MARCO since it requires a generative answer (rather than just classification).\n\nMinor comments:\n\n1. S_m on page 2 was used without being defined. I think it is meant to be the loss for the model output, but would be nice to define it before using it.\n\n"
        }
    ]
}