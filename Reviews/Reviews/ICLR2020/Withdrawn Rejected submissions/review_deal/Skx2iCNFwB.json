{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a new framework, named Neural Stochastic Differential Equation (Neural SDE), for regularizing and stabilizing Neural Ordinary Differential Equation (Neural ODE).  For this reason authors consider SDE by adding stochastic term to ODE. They propose three alternatives for stochastic term and provide experimental comparison between all of them and pure Neural ODE. Results of these experiments demonstrate that Neural SDE outperforms Neural ODE. \n\nOverall, proposed concept is quite reasonable but supplied experiments have methodological issues. Therefore, I suppose this work is not a sufficient contribution to neural continuous models and could be rejected.\n\n1. There are several controversial questions, which do not allow me to put a higher score:\nThe information provided to set up the experiment is not enough to reproduce. Learning rate, optimizer, batch size and other parameters are missing. I have conducted experiments for CIFAR10 dataset for baseline Neural ODE model using authors’ network architecture, standard data augmentation and those parameters - optimizer: sgd, learning rate: 0.1, momentum: 0.9, batch size: 128, integration method: euler, amount of steps: 10, with learning rate decay by 10 times on 60, 100, 140 epoch. With this parameters I am able to achieve 85.40% accuracy, while best result for Neural SDE model is 84.55%. Given this, it is very difficult to draw any conclusions about the effectiveness of Neural SDE.\n2. Supplied experiments are various and shows that Neural SDE achieves better test accuracy and is more robust to adversarial and non-adversarial perturbations to the input. Nevertheless, all of the experiments are conducted with relatively small models, which achieve low test quality. One cannot guarantee that obtained results will remain the same for large models. Therefore, judgement about significance of these results gets clouded. Moreover, I suppose this issue to be important because authors state their model as a way to regularize Neural ODE. That means, that the initial model has to be large enough to be able to overfit. \n3. The main advantage of Neural ODE is memory efficiency of the training procedure, which is achieved by adjoint method. Authors suggest their framework as regularizing technique to Neural ODE, but they conduct experiments without adjoint method. Therefore, it is hard to make any conclusions about advantages of proposed model in comparison with others. In order to demonstrate that Neural SDE saves memory efficiency feature authors should conduct experiments with Neural SDE with adjoint if this is possible. \n\nFor making this paper better I would suggest to conduct experiments with stronger baseline networks. Currently accuracies for baseline networks are very low. Also I would like to make some notes which can help to clarify text of the paper:\nIn the end of the second paragraph of introduction authors say that regularization methods are not applicable to Neural ODE, since Neural ODE is deterministic system. But ordinary neural networks are also deterministic, so this idea is not clear. \nIn the part  3.3 ROBUSTNESS OF NEURAL SDE authors say that assumption on function f and G quite natural and is enforced in the original Neural ODE paper. As I could find there is no first assumption about at most linearity in original paper. Maybe authors can add some additional argumentation about this assumption. \nExperiment in section 4.1 demonstrates that Neural SDE achieves better test accuracy than Neural ODE. Given this, authors state that noise injection acts like a regularization and therefore Neural SDE achieves better generalization performance. However, it would be more obvious for readers if train accuracies were also provided.\nPage 6, Definition 3.1 B: it is not clear with respect to which parameter supremum is taken.\nIn the main body of the paper there are a lot of references to formula 25 (from appendix), but almost the same formula 3 is in the main text.\nQuestions for authors:\nDo you have some intuition why Dropout without TTN works worse than ODE for TinyImageNet in experiment session 4.2?\nAnd why different methods work differently for different datasets in experiment session 4.3?\nWhat is the size of ensemble for TTN results?\nDo you search noise parameter by the best validation or test accuracy?\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper presents an extension to Neural ODEs by introducing stochastic regularization techniques that inject noise to the Neural ODE dynamics. The method consists of adding a Gaussian diffusion term to the ODE dynamics to obtain a stochastic differential equation (SDE). Using a black box SDE solver, the paper proposes to train the parameters of the SDE dynamics (drift and diffusion) via back-propagation using autograd, with an alternative method described in the appendix.\n\nThe ideas in this paper are definitely interesting, but I'm more inclined to reject this paper for the following reasons: the theoretical contributions are hard to follow and, while the empirical results are encouraging, the technical contribution is small when compared to the existing literature (Tzenand Raginsky, 2019), (Jia and Benson, 2019), (Rackauckas et. al., 2019). The paper does mention the differences with the other works, but the title and the text make it seem as if the main contribution was proposing the idea of Neural SDEs. The paper would read better if it was clear from the title and introduction what the main contribution is, which appears to be a  study about the robustness of Neural SDEs.\n\nThe paper requires more work to address some issues, including:\n\n* The paper mentions that injecting noise to a Neural ODE is non-trivial, but that depends on what kinds of noise, and how you interpret it. In this paper, the noise is process noise that affects the intermediate activations (but not the parameters of the dynamics). You could also interpret the noise as providing different samples of the unknown dynamics f, which is related to Bayesian Neural Networks. In fact, this gives another baseline: Training a Neural ODE where the parameters of the drift term are perturbed by noise. Similar how dropout is applied on recurrent neural nets (Gal and Ghahramani, 2016), the noise perturbation should be sampled at t=0, and kept fixed until t=H. This should be a trivial change to the Neural ODE code that provides a point of comparison\n\n* The way Bernoulli dropout is modeled in the proposed Neural SDE framework is rather crude. I Can see how the first two moments of the approximation in equation (7) match the first two moments of the Bernoulli distribution, but the distributions have supports that are way too different to claim that eq(7) is a good approximation: The mass of the Gaussian is concentrated around 0, while the mass of the scaled and centered Bernoulli is away from 0. A more appropriate way to model dropout would be using jump diffusion.\n\n* The proof of robustness can be written more clearly: what is it that you set out to prove? Are the assumptions just saying that f + G is bounded and Lipschitz continuous? The way they are written is a bit confusing. How is the main theoretical result, Corollary 3.0.1, connected to the subsequent experiments? What insights do you get from that result?\n\n* The paper claims that stochastic regularization techniques like Dropout do not apply noise at testing time, but this statement ignores the literature on Bayesian Neural Networks (Neal, 1995) (Blundell et al, 2015) (Gal and Ghahramani, 2016), which use multiple parameter samples to produce an ensemble of predictions .\n\n* The paper would be a lot more interesting if there was a comparison (computational cost and performance) between back-propagating through the SDE solver, and using the method proposed in the appendix.\n\n* The writing could benefit from proof-reading by a proficient English speaker."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The paper introduces Neural Stochastic Differential Equations models which is a follow-up of last year paper on Neural Ordinary Differential Equations.  The stochasticity is obtained by adding an injection noise term. \n\nPros: \nAs there is still no published paper that introduces Neural SDE, this paper would be well placed in the literature. Apart from introducing the Neural SDE architecture, the authors also study both empirically and analytically the robustness of suggested models in comparison with Neural ODE models. Overall the paper is clear and well written. \n\nCons: \nThe transfer from 'Ordinary' to 'Stochastic' is achieved by only adding a Dropout (injection noise) term. So the idea seems to be not 'that' big. \nThere is another papers, not cited by the authors, that suggests stochastic versions of Neural ODEs based on ResNets with a specific initialization distribution on the NN parameters (Peluchetti and Favaro, 2019). Their model can be considered as a continuous version of ResNets with applied stochastic gradient descent. The current interpretation of Neural SDE also can be considered, just I find the suggestion of Peluchetti and Favaro to be more natural. \n\nStefano Peluchetti and Stefano Favaro. Neural Stochastic Differential Equations (2019) \n\nMajor suggestions:\nI don't have major suggestions. \n\n\nMinor suggestions: \n\n* In the first and last sentences of subsection “Gaussian noise injection”, Section 3.1, there must be references to equation (3) instead of (25), which is in the appendix. And also further in the main text of the paper there are references to (25). \n\n* In equation (7) the identity matrix must be written in bold.  \n\n* Section 3.2. we can also calculates -> calculate\n\n* There are also some missed articles throughout the paper. \n\n* Appendix: \n- I think it would look better to put Equation (12) in Lemma A.3. \n- After Equation (16), there is another letter epsilon.\n- Appendix D contains only Figure 7 which is the same as Figure 3 (left).\n\n* References: \n- Weinen E. -> full name \n- Some papers have already been published, e.g. Xavier Gastaldi’s paper was at ICLR 2017.\n- Conference names are not homogeneous, such as ‘Advances in Neural …’ and only ‘Neural …’, written with capital and not capital letters. \n"
        }
    ]
}