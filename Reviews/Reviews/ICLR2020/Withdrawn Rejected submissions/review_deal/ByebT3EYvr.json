{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #3",
            "review": "\nThe paper studies \" SD-CFR\"  neural network to approximate  the tabular in CFR.  It builds on  DCFR but  computes average strategy from value network buffer and the results show that SD-CFR is better than DCFR in practice.\n\n\nThis paper is similar to DCFR in terms of , and the written should be improved. For example, the notion of  Leduc poker in section B  is not consistent with section 6.1.\n\n\nMy major concern is that the experiment is done a small action space poker game, which limits the contribution. How about the performance of SD-CFR in the unlimited version which has more actions and the game size is controllable that can compute the exploitability? Furthermore, the experiment should be conducted more thoroughly. For example, to test the robustness??, apply another network architectures instead of using the DCFR version; verify the network has generalization across info sets instead of just remember the value of each Infoset; evaluate the influence of one-hot encodings of cards rather than just the embeddings chosen in the experiment.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "\nSummary of the Paper\n====================\n\nThis paper addresses an important topic on using function approximation method to solve imperfect information games. In this paper, the author proposed a method called Single Deep CFR (SD-CFR), which is similar to Deep CFR but only needs to learn value networks. They compare SD-CFR against Deep CFR on small Leduc Hold'em and a medium-size 5-FHP. Their performance is slightly better than Deep CFR. \n\nHowever, the paper is not well written and many details in motivation, method, and experiments are too unclear to evaluate them. Theorem 2 is false as currently written. The experimental results are not convincing, e.g., the parameters and buffer size in neural networks are larger than the number of infosets. Some parts of this paper are very confusing and incorrect. I list some of them as follows. Many key issues have not been addressed. It requires *major* revisions before being resubmitted.\n\n\nThe main idea of the algorithm is very similar to Deep CFR. The major difference is that SD-CFR inferences average strategy based on a buffer of value networks rather than learning a separate network in Deep CFR. This idea is reasonable because the average strategy in CFR is the weighting average of current strategies on past iterations. \n\nOverall, I believe this idea is just a small incremental work based on Deep CFR and it's hard to say this work is novel enough to be accepted by ICLR.\n\n\n\nComments: \n========\n1. First abstraction in Sec.1, \"Counterfactual Regret Minimization (CFR) is the most successful algorithm for finding approximate Nash equilibria in imperfect information games\". \n\nCFR is only proven to converge on two-players zero-sum (or constant-sum) *perfect recall* imperfect information game(Martin Zinkevich et.al. 2017). \nIt's less rigorous to say CFR is the most successful algorithm for imperfect information games. Typically, CFR is slower than some first-order methods when solving some imperfect information games. However, the author didn't mention this kind of method.\n\n2. \"CFR’s reliance on full game-tree traversals limits its scalability and generality.\" \nMCCFR and hybrid CFR continue resolving in DeepStack don't need to full game-tree traversals.\n\n3. The first sentence in Sec.1, \"In perfect information games, players usually seek to play an optimal deterministic strategy. In contrast, sound policy optimization algorithms for imperfect information games converge towards a Nash equilibrium, a distributional strategy characterized by minimizing the losses against a worst-case opponent\"\nMany reinforcement learning algorithms can learn optimal stochastic strategy profiles for perfect information games.\nDo you mean Nash equilibrium is a stochastic strategy profile? Typically, it can be a deterministic strategy in some games.\nIt could have more than one opponent in imperfect information games, such as six players Texas Hold'em in Pluribus.\n\n4. \"the scalability of such tabular CFR methods is limited since they need to visit a given state to update the policy played in it.\"\nTabular CFR methods are used in DeepStack, Libratus, and Pluribus. They are all successful applications of tabular CFR. why this causality should be true?\n\n5. \"While tabular CFR has to visit a state of the game to update its policy in it, a parameterized policy may be able to play an educated strategy in states it has never seen before.\"\nMCCFR also visits a subset of nodes in game tree and converges to strong approximate Nash Equilibrium, e.g., External sampling in Pluribus.\n\n6. \"Not aggregating into S removes the sampling- and approximation error that Bs and S introduce?\"\nUsing function approximation methods typically leads to approximation error. Do you mean Deep CFR has a sampling error? Why?\n\n7. The conclusion of Theorem 2 requires that the neural network converges to a global minimizer after every update. Finding the globally optimal parameters in a neural network is NP-hard in general, so the assumption in this theorem is false as stated. \n\n8. It's inconsistent in the description for Leduc Hold'em. In Sec.6.1, the authors use J, Q, K but in the Appendix, they use A, B, C. The figure.1 is on standard Leduc Hold'em. The standard Leduc only contains thousands of infosets. However, the buffer size is 1 million (much larger than #infosets). Collecting data over 1500 external sampling traversals will visit almost all the infosets in Leduc. So I think this work is failed to address the generalization of neural networks. Does 396MiB in Figure.1(b) refers to the capacity memory? That seems much larger than the tabular CFR. \n\n9. It's unclear how many infosets in Leduc and 5-FHP.  You have introduced Leduc in Appendix, why not introduce 5-FHP. Such a game is different from the one in Deep CFR. BTW, in FHP, there are many chance infosets, there is no need to store strategies for these nodes. All these issues are important and should be addressed. \n\n10. I notice that the author used similar parameters like Deep CFR. However, a self-consistent paper should contain such important hyperparameters and game settings. The one-on-one performance in Figure.2 is not clear enough. A better figure should contains a significant interval. It's quite confusing for the x-axis in Fig.3 like $10^{1.5}$, $10^{2.5}$.\n\n11. comparing different methods under different embedding size, sampling ratio/observation ratio in each iteration, running time of tabular against neural, neural network SGD update iterations are very necessary. However, this paper doesn't address these issues. It's difficult for me to judge whether SD-CFR is better than Deep CFR in particular settings.\n\n12. The authors only compare their methods against Deep CFR. Some related works, such as NFSP, Double Neural CFR, ED or at least tabular CFR should be added to the comparison.\n\n\n\n\nClarity:\n========\n1. \"This reduces the overall sampling- and approximation error\". What do you mean about sampling- and approximation error?\n2. $\\pi^{\\sigma^t}_{-i}(I)$ is not defined. Maybe, you can add it's defined. Also, expected utility $u^{\\sigma}_i(h)$ is important and I suggest you give a formal definition.\n3. In Section3.1, some latest CFR variants are worth mentioning, such as Lazy-CFR[1], ICFR[2], low-variance MCCFR[3].\n   In Section7, the latest function approximation for imperfect information games [4,5] should be discussed and compared.\n4. [Section 4]\"To mimic the behaviour of linear CFR, we need to ...\", in this section, it seems that you just introduce Noam Brown's work, why use we here? Do you introduce your work here? Confusing.\n5. [Section 4]Eq.3 and Eq.5 are very similar. Si(I, a) is not defined here. I suggest changing Eq.5 to the optimization function of the average strategy network S.\n6. [Section 4]\"Not aggregating into S removes the sampling- and approximation error that Bs and S introduce, respectively\". Very confusing. Do you mean the external sampling and reservoir sampling in Deep CFR will lead to sampling and approximation error? Why is this true?\n7. Sec.4, the regret r in $R^{T}_{i,linear}$ refers to the regret in external sampling MCCFR, it typically has a different definition with the one in Sec.3.\n\n\nTypos and minor errors:\n========\nThere are many confusing descriptions or typos. I list some of them as follows. I would suggest having the paper proofread and revised to correct these errors.\n\n1. \"improve the strategy played in each state.\"  -> use infoset is better.\n2. \"we need to weight the training losses between the predictions D makes and the sampled regret vectors in B with the iteration-number on which a given datapoint was added to the buffer.\" very confusing, could you give a more detailed explanation? BTW, the word \"makes\" here is a typo?\n3. \"In tabular methods, the gain of not needing to keep sigma in memory during training would come at the cost of storing t equally large tables (though potentially on disk) during training and during play.\" very confusing explanation, such as \"the gain of not needing\", \"would come at the cost of ...\", \"during training and during play\"\n4. \"Like in the tabular case, we do need to keep all iteration strategies, but this is much cheaper with Deep CFR as strategies are compressed within small neural networks.\" -> \"Like the tabular case\", \"chiper than Deep CFR because strategies [in whose method?] are ...\"\n5. The definition of I' in I in Eq.6 is confusing, such notation typically means I' is a member of I.\n6. \"While tabular CFR has to visit a state of the game to update its policy in it\" -> \"and update\"\n7. \"player t mod 2 updating his on iteration t\" -> updating his [what]\n8. \"most successful algorithms of the recent past\"\n9. $D^t$ above Eq.4 is $D^t_i$?\n10. \"the computational work needed to train\" -> worker?\n\nCitations\n=========\n\n[1] Lazy-CFR: fast and near optimal regret minimization for extensive games with imperfect information.\n[2] Efficient CFR for Imperfect Information Games with Instant Updates.\n[3] Low-Variance and Zero-Variance Baselines for Extensive-Form Games.\n[4] Computing Approximate Equilibria in Sequential Adversarial Games by Exploitability Descent.\n[5] Double Neural Counterfactual Regret Minimization.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper builds upon recently introduced Deep CFR, a method to solve large imperfect-information games. Deep CFR, which combines counterfactual regret minimization and deep learning, learns value networks and a strategy network. The paper suggests a modification to avoid learning a strategy network, proving that it is theoretically more attractive and showing the beneficial performance in experiments.\n\nCFR and its variants are state-of-art methods for large extensive form imperfect information games. Scalability of CFR is an actual problem, widely discussing in literature. The suggested modification to Deep CFR is a significant contribution. The paper proves theoretically that the modification indeed produces an average strategy. Also, it is proved that Deep CFR is not guaranteed to provide an average strategy because of the finite sample size and approximation error. Experiments show that SD-CFR produces less exploitable strategies than Deep CFR in Leduc Poker and wins one-on-one matches in 5-Flop Hold’em Poker. Experiment in 5-FHF also shows significant decrease in the amount of necessary storage space, because SD-CFR stores all trained value networks, while Deep CFR needs additional space for strategy network dataset.\n\nThat being said, I follow up with some questions/criticism.\n1.\tI would like to see some discussion about scalability of the method. Can it be applied, for example, to HUNL? To 6-players holdem? If not, is it compatible with abstraction-based methods or with depth-limited solving?\n2.\tWhy were one-hot encodings of cards instead of embeddings chosen in experiment 6.1? How does it influence the performance of Deep CFR? In experiment 6.2 the paper reports “The neural architecture is as Brown et al. (2018a).” Does it mean, that embeddings were used? \n3.\tBoth in the paper and in Deep CFR nns were trained from scratch each iteration. Could it be beneficial not to start from scratch? For example, pretrain good card embeddings on a variety of similar tasks in advance (using a lot of data and time, as it is offline computation), and then use these embeddings without changes during SD-CFR (or Deep CFR) training? \n\nMinor comments:\n1.\tIntroduction second paragraph: “To address these two problems, researchers started to augment CFR with neural network function approximation”. Augmenting like in Deep CFR is one alternative to abstraction schemes for large games. Another alternative is depth-limited solving + resolving [1], [2] and DeepStack actually represents this branch of methods.\n2.\t“Now, the iteration-strategy for player i can be derived by …”. It is probably better to say that choosing such strategy minimizes the overall regret (see regret matching).\n3.\tSection 5.1. Instead “we satisfy the linear averaging constraint of equation (5)”, which I found a bit confusing, it is better to say that probability of sampling a particular action is the same as sampling the action from the average strategy (5).\n4.\tTo reduce variance in one-on-one test, AIVAT technique can be applied [3].\n\n[1] Noam Brown, Tuomas Sandholm, and Brandon Amos. Depth-limited solving for imperfect information games. arXiv preprint arXiv:1805.08195, 2018. \n[2] Matej Moravˇc´ık, Martin Schmid, Neil Burch, Viliam Lis`y, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. Science, 356(6337):508–513, 2017.\n[3] Burch, Neil, Martin Schmid, Matej Moravcik, Dustin Morill, and Michael Bowling. \"Aivat: A new variance reduction technique for agent evaluation in imperfect information games.\" In Thirty-Second AAAI Conference on Artificial Intelligence. 2018.\n"
        }
    ]
}