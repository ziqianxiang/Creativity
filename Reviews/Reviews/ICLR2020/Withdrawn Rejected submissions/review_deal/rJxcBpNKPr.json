{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper is board-line but in the end below the standards for ICLR. Firstly this paper could use significant polishing. The text has significant grammar and style issues: incorrect words, phrases and tenses; incomplete sentences; entire sections of the paper containing only lists, etc. The paper is in need of significant editing.\n\nThis of course is not enough to merit rejection, but there are concerns about the contribution of the new method, experiment details, and the topic of study. The results are reported from either a single run or unknown number of runs of the learning system, which is not acceptable even if the we suspect the variance is low. The proposed approach relies on pre-training a feature extractor which in many ways side-steps the forgetting/interference problem rather than what we really need: new algorithms that processes the training data in ways the mitigate interference by learning representations. In general the reviewers found it very difficult to access the fairness of the comparisons dues do differences between how different methods make use of stored data and pre-training. The reviewers highlighted the similarity between the propose approach and recent work in angle of generative modeling / out of distribution (OOD) detection which suggests that the proposed approach has limited utility (as detailed by R1) and that OOD baselines were missing. Finally, the CL problem formulation explored here, where task identifiers are available during training and data is i.i.d, is of limited utility. Its hard to imagine how approaches that learn individual networks for each task could scale to more realistic problem formulations.\n\nAll reviewers agreed the paper's experiments were borderline and the paper has substantial issues. There are too many revisions to be done.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "[update after rebuttal]\n\nI thank the authors for their detailed reply, answers to my questions, and updates to the paper. (I especially appreciate the substantial effort to address all points and improve the manuscript even given the strongly negative rating. I am not personally in favour of \"extremifying\" the rating system the way ICLR did this year; I think it discouraged many authors from working on their rebuttals, and caused many reviewers to have a sense of inertia to keep their bad scores. I'm glad the authors didn't fall into the first camp, and I'm doing my best not to fall into the second! I think you did a very good job of improving the paper).\n\nI think the manuscript has improved substantially; especially the clarifications about (not) using negative samples and additional detail on INNs with citations. \n\nI find some of the sentences still unclear, and strongly suggest having the text read over for grammar/clarity for a camera-ready version.\n\nBut overall I recommend acceptance of this work. \n\n------------\nPaper summary:\nThis paper does continual learning using an invertible neural network (INN) trained to recognize each class. At test time a new example is presented to all of the INNs and the INN whose prediction has the smallest norm is used to predict the class of the test example. They do experiments on MNIST and CIFAR-100\n\nPaper contributions: \n- Review of methods and evaluation settings for continual learning\n- Review of invertible neural networks \n- Experiments comparing the proposed method to several other continual learning methods \n- Examination of memory cost\n- Exploration of feature space of trained INNs\n\nReview summary & decision: \nThe proposed idea of using invertible networks for continual learning sounds interesting, and I think that this could be a good paper. The experimental evaluation is not consistent or complete enough for me to to tell, however, and the core motivation for the idea is not clearly explained. There are also some less critical, but still important aspects of the paper (related work, clarity of explanations, repetitiveness) which lead me to decide this paper is not currently ready for publication.\n\nReasons for decision: \n1. A lot of statements and decisions are made without being explained, or are unclear / innacurate\n      - In CL, the objective is to learn several tasks one after the other\" this is not the objective, it's the problem setting. The objective is classification accuracy (or some other metric) in this setting.\n      - The choice of datasets seems a little odd; why not CIFAR-10 as well?\n      - It's very misleading to say your method is able to learn one class at a time without storing data; unless I've misunderstood this is only after pretraining all of the invertible networks with labelled data.\n       - Why 500 examples per class for CIFAR-100, and how many for MNIST? What criteria was used to arrive at this number?\n      - in the abstract you state the definition of continual learning says you can't have access to the data from previous tasks, but then mention that previous works use samples from previous data. This seems contradictory\n      - I'm not extremely familiar with Li & Hoeim (learning without forgetting), but I don't think it's very accurate to call it distillation regularization\n      - In 4.1 I don't think it's accurate to call this compression; it's just having smaller size layers in between (unless I misunderstood).\n      - I'm not familiar with model superposition; it should be more explained since your claims of having lower memory cost rely on this, as far as I can tell.\n      - The experimental baselines seem inconsistent / not comparable, making it difficult to evaluate what is going on. E.g. some update pretrained features and some do not. Some (including yours) do offline pretraining on a subset of examples, with inconsistent amounts of data, and sometimes this is considered part of training (Fearnet) and sometimes not (yours). \n       - I don't understand the central motivation for using INNs. The only sentence that seems to talk about it is \"this way ... network won't be able to have an output similar to the outputs on data from its training set\", but this is not clear, not enough explanation, and seems somewhat innaccurate. It's not that it's not able, it's just not likely (I think; please correct me if I'm wrong!), and I don't see why this isn't also true of e.g. MLPs (i.e. for MNIST, train 10 MLPs each to classify [class i] vs [all classes except i] - In order to justify the proposed approach, the baseline described above (using MLPs instead of invertible NNs) would be very useful\n 2. The training regime for the INNs is not clear (are they shown negative (other class) examples? How many, how are they sampled? I didn't find any hyperparameters for the INN training in the appendix, even though it says they should be there.    \n 3. The single head setting seems very similar to (or maybe the same as?) open set learning; I think this should be mentioned and works in this area should be reviewed.\n 4. Section 3 is repetitive and unclear. It could be greatly shortened to make space for more experiments.\n 5. It would be nice to see computational cost as well as memory cost; this is important in many settings where continual learning would be deployed.\n\nFeedback/suggestions/nits (not necessarily part of decision assessment): \n1. Cite the definition of continual learning (e.g. with a reference to a textbook or review) \n2. A lot of the writing is unclear, wordy, and/or grammatically incorrect\n    - inconsistent verb tense\n         -  e.g. \"if one would need .... it is required\" should be \"if one would need .... it would require\" I'd suggest rewording this sentence entirely, because it's misleading - it says \"retrain on this new dataset (which sounds like train just on the new data), but I guess you mean retrain on all data including the new data. e.g. \"Updating the model with new data requires retraining on the full dataset (old + new data). However, there are.... method may not be applicable.\" \n         - \"we are reaching\" -> we reach    - frequent use of \"indeed\" when it doesn't make sense\n    - Section 3 repeats the intro, 3.1 and 3.2 are sort of saying the same thing, and are also sort of repetitive of the related work. The related work should all be in the related work section, and this section should just be about what _you_ are doing. Invertible neural networks should be reviewed in something like a 'background' section, which possibly could be in appendix. Right now that section has even more related work in it\n3. Include tsne hyperparameters in appendix \n4. \"appendix\" not \"annexe\" (annexes are on buildings :) ) \n5. when you define continual learning, bold it (not italicize) \n6. \"Y is then Y_i\" -> Y = Y_i\" \n7. Caption for Figure 3 is unclear; I don't understand what I'm supposed to take away from these images. I also don't see a black cross.\n\nQuestions:\n1. What is the motivation for using INNs specifically? (rather than e.g. normal MLPs). \n2. What is the training regime for the INNs?\n3. What is model superposition, is it expensive in computation or something other than memory, and if you don't do that what is the memory cost of your method?",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper tackles continual learning problem with stacks of invertible network blocks. Similar as the ensemble idea, the proposed method learns an invertible network for each new object class. During test, for each class, each learnt network outputs a norm and the one with the smallest norm is the predicted class. The proposed model has shown performance improvements in classification accuracy on MNIST and CIFAR100 datasets in incremental class tasks.\n\nDespite the linear increase of memory usage over number of tasks as authors pointed out in the conclusion, this is an elegant method for continual learning problems in incremental class tasks. The paper is very-well written and easy to follow. To my best knowledge, the formulations in the paper are correct and clear. It seems that there are sufficient details for reproduction. However, I have the following concerns which, I think, may lower the contribution of the paper, unless authors can help clarify.\n\n1. Since authors propose an invertible neural networks-based method for continual learning, instead of only focusing on incremental class, authors should also evaluate the proposed method in other continual learning tasks in object classification, for example, incremental domain and task, as defined in this review paper (https://arxiv.org/pdf/1810.12488.pdf).\n\n2. Though I would assume OVA-INN would outperform state-of-the-art regularization-based methods, it would be convincing to show quantitative results of these methods (e.g. inclusion of Elastic Weight Consolidation in Figure 2).\n\n3. I do not understand the term \"learning type\" in Column 4, Table 1. Please give the definition of this term.\n\n4. Figure 3 is an interesting visualization of the latent representation learnt by the model. What about prototypical networks? It would be great to provide side-by-side comparison with such visualizations for the clusters learnt by prototypical networks the few-shot continual learning settings.\n\n5. In Table 1, how much is the memory cost for nearest prototype method?\n\n6. How much training data has been used over multiple classes?\n\n7. Side note: it is unclear to me how this method can generalize to other tasks, e.g. regression problems and reinforcement learning problems. Authors can briefly discuss how the proposed method can be applied in these scenarios in the future work section."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The central idea of this paper is to converts the classification problem into conditional generative modeling, it trains one normalizing flow (NICE) for each class of the data to perform out of distribution detection (one versus all binary classification). The data are presented one class at a time without being able to look back.\n\nThis paper proves a good improvement over the baseline methods compared to. Especially on CIFAR100, which is considered a difficult task in the regime of continual learning. It is quite evident to me that this approach has a big advantage over the other incremental classification methods.\n\nI give a weak reject to this paper mainly because,\n\n1. For invertible network, the NICE [1] coupling structure is used in this paper, although it is described in this paper as minimizing the distance from zero vector, the loss is basically equivalent to NICE with a Gaussian distribution as the target distribution. This is because normalizing flow maximizes the log p(z) + log(|det(z/x)|), in the volume preserving case the log determinant is a constant, it reduces to maximizing log p(z), which is equal to L2 loss if Gaussian distribution is chosen for p(z).\n\nIn my opinion, the paper could be better presented from the angle of generative modeling / out of distribution (OOD) detection. From this point of view, it would be desirable to include comparisons to the other OOD detection literature. It has been reported that the log probability is not a very good measure for OOD detection [2] because if an input has \"simpler\" patterns it would be classified as in distribution although it is actually OOD, and it has been reported that volume preserving flow (as used in this paper), also suffer from the same problem. There're several works proposing alternatives [3,4]. The good results of this paper may be because of the pretrained feature extractor, using a pretrained feature extractor could be practically useful but has limited insight for future research. Also, using a fixed feature extractor is avoiding the forgetting problem instead of solving it, it wouldn't generalize to more general / realistic continual learning setting.\n\n2. The incremental classification setting in this paper is not a very practical assumption for continual learning. i.e. the task boundary are assumed to be available during training, and within each task the data are i.i.d shuffled. One of the goal of continual learning is to prevent catastrophic interference, training separate network for each task, in my opinion, is avoiding the problem rather than solving it. Although there're many papers in the continual learning regime assumes availability of task boundary, I think this setting doesn't bring too much insight to how we can eventually solve the general case continual learning. This been said, I think this paper could be better presented from the OOD detection point of view.\n\n\n[1] NICE: Non-linear Independent Components Estimation https://arxiv.org/abs/1410.8516\n[2] Do Deep Generative Models Know What They Don't Know? https://arxiv.org/abs/1810.09136 \n[3] WAIC, but Why? Generative Ensembles for Robust Anomaly Detection https://arxiv.org/abs/1810.01392 \n[4] DETECTING OUT-OF-DISTRIBUTION INPUTS TO DEEP GENERATIVE MODELS USING TYPICALITY https://arxiv.org/abs/1906.02994 "
        }
    ]
}