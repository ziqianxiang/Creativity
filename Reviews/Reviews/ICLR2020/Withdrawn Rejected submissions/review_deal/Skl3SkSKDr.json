{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposes a parametrisation of Euclidean distance matrices amenable to be used within a differentiable generative model. The resulting model is used in a WGAN architecture and demonstrated empirically in the generation of molecular structures. \n\nReviewers were positive about the motivation from a specific application area (generation of molecular structures). However, they raised some concerns about the actual significance of the approach. The AC shares these concerns; the methodology essentially amounts to constraining the output of a neural network to be symmetric and positive semidefinite, which is in turn equivalent to producing a non-negative diagonal matrix (corresponding to the eigenvalues). As a result, the AC recommends rejection, and encourages the authors to include simple baselines in the next iteration. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "\t\nThis paper addresses the important problem of molecular structures generation, and more generally of efficient point-cloud distributions learning in d-dimensional space.\nThe paper is written clearly, the pseudo-code is presented in a clear way, striking a good balance between explicit-ness and concision.\n\nAfter my first reading I was disappointed in the experiments, because I read the paper quite quickly at first.  After a second more careful read I understood most of it and was much more enthusiastic. Disclaimer: I am inexperienced in this particular field (GANs and GANs for molecular generation) so I lack literature knowledge, and I may be over-evaluating the quality of performances (compared to recent results).\nBut currently I believe that the authors are a bit too humble about their results (a rather uncommon phenomenon). There is a part of the method that I would like to have clarified (about bond/atom types), but other than this clarification, I strongly recommend the paper to be accepted.\n\nThe paper deals with a couple of distinct, related problems.  One is that of sampling valid (Euclidean) distances matrices (EDM). This is done with algorithm 1.\nThe paper uses these EDM to train a generator G directly in EDM-space, against a Critic network C (the architecture of which is taken from existing literature).\nSome of the output configurations have to be discarded due to incorrect bond types assignment (this is the part that is still obscure to me).\nThe remaining outputs are chemically sensible in terms of bond types/valence/local chemistry/etc, but also, and quite impressively, have reasonable ground state energies !  In this sense, the paper produces samples that may deserve to be added into the QM9 data set (After some data augmentation using DFT or other physics-validated methods of course).\n\n\nI have mainly two requests:\n\n1. The bond type or atom assignment is not clearly explained. I ask further detailed questions below, but overall I think you should attempt to make a pedagogical explanation of how the atom types are assigned/learned, and how some of them are later discarded.\n\n'The  generator produces an additional type vector in a multi-task fashion which is checked against a  constant type reference with a cross-entropy loss.'\nCould you explain this better ? At least in the appendix. Otherwise this very dense sentence remains mysterious. Eq.11 is currently the single occurrence of the H, t, and tref terms.\n\nAbout the validity test using Open Babel. Again, what happens with bond types is not very clear. Are they set in stone at generation time, and then most of these affected types are 'wrong' and are discarded by Open Babel ?\nWhy don't you include this validation at training time (if it's very complicated to do, explain why) ?\nIn any case, please clarify this paragraph, as for now it is cryptic.\n\n2. Part of the paper goal is to achieve learning of point clouds distributions and not about chemistry.  However, discovery of new structures (and their conformations) is in itself a big topic.  I think it would be good to mention a couple of follow ups to your work in the conclusion. For instance, it would seem rather natural to me to include some equivalent of OpenBAbel and Energy-estimates within the learning loop, so that the generator directly generates valid (open-babel wise) and reasonable (energy level-wise) structures.\nBesides the conclusion, you should stress out better the significance of your results with a couple of comments here and there. (I have more precise suggestions below).\n\n\nSome other comments to improve the paper (in clarity or other):\n\nIn algorithm 1, you should precise that G() is your NN-based generator network. At this point of the paper G and z and N_z have never been mentioned. This is a problem when reading the paper for the first time.\nLine 5, you could explicitly precise, 'with (5)'.  Also I am puzzled by the use of the softplus here, but later in page 5 you say g() is softplus for the first three eigenvalues, and set to 0 for all others. Why not already anticipate and say this in section 2 ?\nLine 8, I would have written 'eigenvalues .... of D'  (what is the role of Eq. 1 here?)\n\nAt some point (i.e. around end of section 2), a comment would be welcomed, about whether you may sample the space of EDMs uniformly at random using algorithm 1 (I think you do not, and it is ok, but the question naturally arises and it's not addressed).\n\n´´which transforms a prior distribution into a target distribution´´\nyou could specify that this prior is the Gaussian N(0,1)^Nz in this case, to better connect with algorithm 1.\n\nAbout equation 6: it would be nice to have some intuitive explanation of what the output values of C(x) mean. They are scalars that represent the opinion of C on the molecule x, so they are the probability that the observed molecule is 'a true molecule x' ?  You should recall that for the inexperienced reader.\n\nStill about Eq.6 : could you quickly provide the motivation for demanding C to be L-Lipschit with  $L \\leq 1$ ?\n\nThe drift term Eq.9 seems to be some sort of regularization, maybe it could be mentioned once in the text for completeness?\n\nUsing Mopac Stewart and figure 4: you could insist more on this result. Up to that point I was very dubious about the usefulness of the whole work, because I would have expected many generated samples to be highly unrealistic, i.e. having huge energies, and so being extremely unstable. Instead here you show it is not the case, and even all your energies lie within the range of observed energies !  This is a very strong result, that is not obvious to expect, and is highly valuable (even after keeping only 7.5% of structures, this is still a strong result.)\nI think this test and the corresponding result should be emphasized more.\n\nYou observe new topological types and complain there are not many. I think ~20 new topologies is not a small number (for so few atoms), and you may be rather proud of it.  What is too bad is that you do not have a tool for differentiating between the very similar conformations (that correspond to thermal fluctuations around a given structure) and the conformations that encode a new structure (which does not necessarily means new topology). \nIf you had this tool, you could enrich figure 3 with a curve showing the new structures (not counting conformational variants).\n\nrefs 2017a and 2017b are the same.\n\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Authors of this paper present architecture to produce valid Euclidean distance matrices. A Wasserstein GAN is then constructed by utilizing a permutation invariant critic network based on the architecture. Generating molecular structures in a one-shot fashion is conducted using the produced distance matrices in 3-d embedding.\n\nIn Section 2, the constraint on L makes M symmetric and positive semi-definite. This seems to be equivalent to treating M as a kernel matrix, and D is the pairwise distance between the kernel function induced by M. So, learning a valid Euclidian distance matrix is same as learning a kernel function.\n\nAuthors need to provide the evidence that Equation (5) holds for all function g, especially for the used softplus activation function. As I know, it holds if function g is a polynomial matrix function.\n\nIn Algorithm 1, what is the meaning of step 14? There is no definition or discussion about G and the \\nabla L. The construction function is the key contribution of this paper, which is incorporated the existing SchNet, so it is better to show the advantage of the proposed construction method comparing with SchNet.\n\nAlthough this paper is an application-oriented paper, the comparisons with baseline methods are preferred, such as some simple and straightforward baselines. Due to the lack of comparisons, it is hard to understand the statement given by authors that “ the current performance… is not optimal and can likely be improved by a better hyperparameter selection”.\n\nIn Section 4, authors calculate the similarity of generated molecules by the closest respective matches, which is determined by the maximal atomic distance after assignment of atom identities and superposition. Is this the standard way to compute the similarity between two graph structures?\n"
        },
        {
            "rating": "8: Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper is extremely interesting, solid and very well written. The idea is simple but nonetheless developed in a smart and effective fashion. The underlying theory is solid, even if some choices should have been discussed more deeply (e.g. the chosen loss function). Introduction and references are adequate, and the paper is readable by a quite broad audience, despite the detailed technical sections. The main issue related to the manuscript is very narrow target of the experimental part, limited to the isomers of a given compound - it would have been interesting to check its potentialities in generating more different structures and distance matrices, and thus to compare its effectiveness versus alternative generative approaches."
        }
    ]
}