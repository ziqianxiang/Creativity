{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper analyzes the non-convergence issue in Adam in a simple non-convex case. The authors propose a new adaptive gradient descent algorithm based on exponential long term memory, and analyze its convergence in both convex and non-convex settings. The major weakness of this paper pointed out by many reviewers is its experimental evaluation, ranging from experimental design to missing comparison with strong baseline algorithms. I agree with the reviewers’ evaluation and thus recommend reject.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper points out that existing adaptive methods (especially for the methods designing second-order momentum estimates in a exponentially moving average fashion) do not consider gradient decrease information and this might lead to suboptimal convergences via simple non-cvx toy example. \nBased on this observation, the authors provide a novel optimization algorithm for long-term memory of past gradients by modifying second-order momentum design. Also, they provide aconvex regret analysis and convergence analysis for non-convex optimization. Finally, the authors evaluate their methods on various deep learning problems.\n\nSignificance/Novelty: While there have been many studies on non-convergence of Adam, raising an issue on ignoring the gradient decrease information seems novel. \n\nPros:\n1. The motivating toy example in Section 3 is useful for readers to get intuitions.\n\n2. By introducing long-term memory on past-gradients, the authors fix the Adam's issues and they can also improve the convergence rate in a non-convex optimization (Corollary 4.2).\n\n3. Empirical studies show superiority to original Adam (Section 5).\n\nCons:\nWhile they provide a significant study on Adam's failure and a novel optimization algorithm, I have several concerns:\n1. What is default hyperparameters for AdaX in Algorithm 1? Is it the same as AdaX-W (Algorithm 3) in Appendix? The bias correction term in the line 7 of Algorithm 1 will be very large even with small $\\beta_2$ since it is expoential (For example, (1 + 0.0001)^(100000) ~ 20000 for $\\beta_2$ = 10^(-4)). So, it is not clear that the second momentum estimate of Ada-X is really stable. For this, it would be interesting to see how the trajectories of second-order momentum estimates of Adam, AMSGrad, Ada-X are different. I think this will help to understand Ada-X better.\n\n2. In terms of theory, I think the Lemma 4.1 is inevitable for convergence guarantees in Theorem 4.1 and Theorem 4.2. Although the authors effectively remove log T in the numerator in Corollary 3.2 of Chen et al. (2019) using their lemma 4.1 (I think this is the key point), the assumption that $\\beta_{2t} = \\beta_2 / t$ seems quite strong, and original Adam paper has no such assumptions. For a real deep learning problems such as training ResNet on CIFAR-10, the $\\beta_{2t}$ is almost zero after even one or two epochs where Ada-X behaves like vanilla SGD. Is there no room for relaxing this assumption such as $\\beta_{2t} = \\beta_2 / \\sqrt{t}$? Also, it is not clear how the authors derive Corollary 4.2 from Theorem 4.2 since Theorem 4.2 assumes $\\beta_{2t} = \\beta_2 / t$ while Corollary 4.2 does not.\n\n3. In the experiment, it is not clear that the authors use the same strategy for constructing first-order momentum for Adam with a newly introduced parameter $\\beta_3$. In other words, the authors should use the same policy on constructing the first-order momentum estimate for both Adam and Ada-X. Also, as the authors add an additional hyperparameter $\\beta_3$, the effect of $\\beta_3$ on performance should be discussed at least empirically.\n\n4. There are many studies on fixing poor generalization of adaptive methods (such as AdaBound which the authors cited). In this context, Zaheer et al. (2018, Adaptive methods for non-convex optimization) propose a large epsilon value (numerical stability parameter) such as $\\epsilon = 10^{-3}$ for better generalization. It will be more interesting to see the comparisons in this regime.\n\n5. In my experience with Adam-W (Decoupled weight decay regularization), Adam-W requires a relatively large weight decay parameter $\\lambda$.\nAs an example, DenseNet-BC-100-12 shows a similar validation accuracy with Adam-W $\\lambda = 0.05$ under the learning rate scheduling in (Huang et al. 2016, DenseNet)  as vanilla SGD.\nTherefore, the authors should consider more broader range of weight decay parameters for at least image classification tasks.\n\nMinor:\n1. In eq (2), the domain of x should be mentioned: according to Reddie et al, it is [-1,1].\n2. In both theorem 4.1 and corollary 4.1, $D_{\\infty^2}$ should be $D_{\\infty}^2$?"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #4",
            "review": "This paper proposed a new adaptive gradient descent algorithm with exponential long term memory. The authors analyzed the non-convergence issue in Adam into a simple non-convex case. The authors also presented the convergence of the proposed AdaX in both convex and non-convex settings.\n\n- The proposed algorithm revisited the non-convergence issue in Adam and proposed a new algorithm design to try to address this issue. However, the new algorithm design is a bit strange to me, especially in Line 6 of Algorithm 2, the authors proposed to update v_t by (1+ \\beta_2) v_{t-1} + \\beta_2 g_t^2, where normally people would use (1-\\beta2) and \\beta2 as the coefficients. I am not quite get the intuition of using such as strange design. I wonder if the authors could further explain that.\n\n- The authors also add change \\beta_1 in Line 5 of Algorithm 2 into \\beta_3. And then in theory, the authors again choose \\beta_3 as \\beta_1. It seems that \\beta_3 is not contributing to any theoretical result. It seems to me to just have another parameter to tune in order to get better performances. Can the authors gives more justification on why introducing such a term here? And also show how the different choice of \\beta_3 affects the final result? \n\n- Missing some important references closely related to this paper:\n\nChen, Jinghui, and Quanquan Gu. \"Closing the generalization gap of adaptive gradient methods in training deep neural networks.\" arXiv preprint arXiv:1806.06763 (2018).\nZaheer, Manzil, et al. \"Adaptive methods for nonconvex optimization.\" Advances in Neural Information Processing Systems. 2018.\nZhou, Dongruo, et al. \"On the convergence of adaptive gradient methods for nonconvex optimization.\" arXiv preprint arXiv:1808.05671 (2018).\n\nI would suggest the authors to also compare with the above mentioned baselines to better demonstrate its performances and theoretical results.\n\n- The authors include theoretical analysis in both convex and non-convex settings, which is appreciated, however, the theoretical result seems to show similar convergence guarantees with AMSGrad. I wonder if the authors could provide theoretical justifications on why the proposed method is better than prior arts, probably some sharper convergences or some generalization guarantees?\n\n- In the experiments part, I wonder why the authors did not compare with AMSGrad, RMSProp in later parts such as ImageNet, IoU and RNN parts? I makes no sense to drop them for those experiments. Also, are the authors fully tuned the hyper-parameters for other baselines such as step size and weight decay on SGDM?\n\n================\nafter the rebuttal\n\nI thank the authors for their response but I still feel that the intuition of this paper is not clear enough and comparison with more baselines is needed. Therefore I decided to keep my score unchanged.  ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "In this paper, the authors propose a new adaptive gradient algorithm AdaX, which the authors claim results in better convergence and generalization properties compared to previous adaptive gradient methods. The paper overall is fairly clear, although the writing can be improved in places.\n\nSection 3 is interesting, although calling the section the \"nonconvergence of Adam\" is a bit misleading, since the algorithm does converge to a local minimum.\n\nI have some concerns about the rest of the paper however. I am a bit confused about the changes proposed to Adam that gives rise to the AdaX algorithm. \n\n1. Won't replacing beta2 with 1+beta2 keep increasing the magnitude of the denominator of the algorithm just like AdaGrad does? In that case, is the algorithm expected to work better when having sparse gradients?\n\n2. I also do not quite understand how to interpret using a separate hyperparameter for the momentum term (ie the beta3 hyperparameter that is introduced). How is beta1 and beta3 related? The numerator loses the interpretation of momentum, i.e., averaged past gradients, when using a separate beta3 parameter, and this does not feel like a principled change.\n\nI have a number of questions about the experiments as well, which makes it hard for me to interpret the significance of the empirical results presented:\n\n1. What is the minibatch size used? Do any of the conclusions presented change if the minibatch size is changed?\n\n2. Is the learning rate tuned? The authors mention the initial learning rate used for the experiments, but it is not clear why those values are used? Was the same initial learning rate used for all algorithms?\n\n3. Were the beta1, beta2 and beta3 values tuned for AdaX? What about beta1 and beta2 for Adam? What are the optimal values of these parameters that were observed?\n\n4. How sensitive is performance to the values of these hyperparameters?\n\nOverall I think this work requires quite a bit of work before it is ready for publication, and would benefit from a much more thorough empirical evaluation of the algorithm.\n\n==================================\n\nEdit after rebutall:\nI thank the authors for their response. While the paper has definitely improved in the newer draft, after having read the other reviews and the updated draft, I believe the paper still requires a bit of work before being ready for publication. I am sticking to my score.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper introduces a new step-size adaptation algorithm called AdaX. AdaX \nbuilds on the ideas of the Adam algorithm to address instability and non-convergence issues. \nConvergence of AdaX is proven in both convex and non-convex settings. The paper \nalso provides an empirical comparison of AdaX against its predecessors \n(SGD, RMSProp, Adam, AMSGrad) on a variety of tasks.\n\nI recommend the paper be rejected. I believe the convergence results could be a significant contribution, but the \nquality of the paper is hampered by its experimental design. The paper felt generally unpolished, containing\nfrequent grammatical errors, imprecise language, and uncited statements.\n\nMy main issue with the paper is the experimental design. I am not convinced that we can \ndraw valid conclusions from the experimental results for the following reasons:\n  - The experiments are lacking important details. How many independent runs of the experiment \n    were the experimental results averaged over? All of the experiments have random initial conditions \n    (e.g. initialization of the network), and should be ran multiple times, not just once. \n    There's no error bars in any of the plots, so it's unclear whether AdaX really \n    does provide a statistically significant improvement over the baselines. \n    Similarly, the data in all the tables is quite similar, so without indicating the \n    spread of these estimates its impossible to tell whether these results are significant or not.\n\n  - How were the hyperparameters and step-size schedules chosen? The performance of Adam, AMSGrad, and \n    RMSProp are quite sensitive to their hyperparameters, and the optimal hyperparameters are problem-dependent. \n    Some of the experiments just use the default hyperparameters; this is insufficient when trying to directly \n    compare the performance of these methods, as their performance can vary greatly with different values of\n    these parameters. I'm not convinced that we should be drawing conclusions about the relative \n    performance of these algorithms from any of the experiments for this reason.\n\nOf course, meaningful empirical results are not necessarily characterized by statistically outperforming the baselines. \nWell designed experiments can highlight important ways in which the performances differ, providing the community \nwith a deeper understanding of the methods investigated. I would argue that the experiments in the paper do not \nachieve this either; the experiments do not provide any new intuition or understanding of the methods, showing \nonly the relative performances in terms of learning curves on a somewhat random collection of supervised learning problems. Why were these specific problems chosen? What makes these problems ideal for showcasing the performance of AdaX? If AdaX is an improvement over Adam, why? What exactly is happening with it's effective step-sizes that leads \nto the better performance? Can you show how their step-sizes differ over time? \n\nStatements that need citation or revision:\n  - \"Adaptive optimization algorithms such as RMSProp and Adam... as well as weak performance \n     compared to the first order gradient methods such as SGD\" (Abstract). This needs a citation. \n     Similarly, \"AdaX outperforms various tasks of computer vision and natural language processing and can catch \n     up with SGD\"; as above, I'm unaware of work (other than theoretical) that shows that SGD significantly \n     outperforms Adam in deep neural networks.\n  -  \"In the era of deep learning, SGD ... remains the most effective algorithm in training deep neural \n      networks\" (Introduction). What are you referring to here? Vanilla SGD? Or are you including Adam etc here? \n      As above, this should have a citation. Adam's popularity is largely due to its effectiveness in training \n      deep neural networks.\n  - \"However, Adam has worse performance (i.e. generalization ability in testing stage) compared with SGD\" \n     (Introduction). Citation needed.\n  - In the last paragraph of the Introduction, you introduced AdaX twice: \"To address the above issues, we propose a \n    new adaptive optimization method, termed AdaX, which guarantees convergence...\", and, \"To address \n    the above problems, we introduce a novel AdaX algorithm and theoreetically prove that it converges...\"\n"
        }
    ]
}