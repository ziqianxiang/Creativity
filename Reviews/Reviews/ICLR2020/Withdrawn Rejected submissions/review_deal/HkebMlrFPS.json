{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "A one-line summary of the paper: The paper proposes a sequence-to-embedding based neural approach with non-negative sparse coding loss to learn multiple mode representations of a given phrase or sentence.\n\nOVERALL COMMENT\n------------------------------\nI didn’t think that the paper was very clearly written. While the proposal for obtaining multi-facet embeddings might be important, neither the task nor the experiments were motivated well enough. The use of NNSC seems promising but there are various concerns about the experimental details. I believe the following (detailed comments) should be clarified for a better understanding of the tasks, experiments and the paper in general. I had trouble grasping the major contributions other than the model.\n\nPlease correct me if I am wrong somewhere in my understanding.\n\nOverall, due to a lack of motivation and the below-mentioned concerns, it’s hard for me to envision this paper offering a significant contribution to the community.\n\nDue to the above reasons, I am giving a score of 3. \n\nDETAILED COMMENTS:\n--------------------------------\nConcerns/Typo:\nPage 1: The first paragraph of the introduction mentions examples of completely unsupervised representation learning techniques. BERT uses minor supervision in the form of Next Sentence Prediction.\nPage 1: |E| should be the embedding dimension and not the “number of embedding dimension”\nPage 2: Figure 1: Stop words are shown to be important here, although in the paper you mention that stop words have been removed.\nPage 3: Please clarify the procedure for getting ‘co-occurring words’ from similar sequences. This seems like an important aspect of the paper that hasn’t been focused upon.\nPage 4: Normalization makes the cosine distance between two words become half of their Euclidean distance -> half of square of their Euclidean distance.\nBaselines for sentence and phrase similarity: While token level embeddings obtained using BERT are useful, averaging token embeddings or using [CLS] embeddings without finetuning are generally not strong enough. A stronger baseline would be to use sentence encoders like [2] and/or [3].\nWhile table 1 is important to show the various ‘facets’ learned by the model, the other experiments are not motivated well enough. Why should obtaining multiple facets or learning representations using multiple facets help in better phrase similarity, sentence similarity and so on?\nWriting: The paper is difficult to read. There are multiple sentences with length  > 50  which are very difficult to comprehend. \n\nQuestions:\nPage 3: Is the only difference between the training signals for phrase and sentence, that of static and dynamic window size respectively? Or is there more to it?\nPage 4: Is the process of optimization alternating between optimization of M^{O_t} and parameters or just a one-time optimization of M^{O_t} followed by multiple backpropagations. If it is the former, it would be good but not necessary to show that the algorithm will tend to converge.\nPage 5:  ‘Similar to BERT, we treat the embedding <eos> as sentence representation’.\n-> BERT uses [CLS] embedding for that.\nWhat is the need of a transformer-based decoder since the “latent modes” are neither learned auto-regressively nor decoder based self-attention would be useful since the modes might capture distinct topics. If they capture very similar topics, then one concern might be the utility of having different phrase embeddings in the first place. Since it is a non-auto-regressive decoder, something as simple as a Neural network could work ( or as proposed in [1]). Having an LSTM or any auto-regressive decoder is an overkill.\nPage 7: First paragraph about SC. What is the reconstruction error formulation? In its vanilla L_2, reconstruction error should be symmetric function. What is the need of having both the error functions? Also, the ordering of topics (multi-fact phrase embeddings: columns of F_u) should matter here. If both of the phrases represent similar “facets” but in a different order, then the symmetric distance might become larger than required.\nPage 9: Hypernymy detection: ‘...often reconstruct the embeddings of its hypernym better than the other way around’ -> Please provide some justification as to why that happens? \n\nI am open to updating my scores if the concerns/questions are addressed.  That being said, it is important to majorly revamp the write-up of the paper.\n\nREFERENCES:\n----------------------\n[1] Kim, Yoon. \"Convolutional neural networks for sentence classification.\" arXiv preprint arXiv:1408.5882 (2014).\n[2] Cer, Daniel, et al. \"Universal sentence encoder.\" arXiv preprint arXiv:1803.11175 (2018).\n[3] Conneau, Alexis, et al. \"Supervised learning of universal sentence representations from natural language inference data.\" arXiv preprint arXiv:1705.02364 (2017)."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #5",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper introduces a novel method for training a neural model to generate multi-facet embeddings, given a word, phrase, or sentence. The model is trained in an unsupervised setting based on surrounding words. The method is evaluated on several unsupervised tasks: phrase similarity, sentence similarity, hypernym detection, and extractive summarization.\n\nThe method for learning multi-facet embeddings could be an interesting contribution. However, the necessity of multi-facet embeddings does not seem sufficiently motivated, and the experimental results do not seem to clearly validate their use, relative to single-facet embeddings. The motivations given in the introduction for multi-facet embeddings are:\n1. words can have many senses\nHowever, the experiments do not assess this proposed limitation of single-facet embeddings. Also, it seems recent approaches to generate contextualized word or sub-word embeddings may be sufficient to resolve word sense ambiguity.\n2. Sentences and phrases can have many topics\nHowever, the best performance on phrase similarity tasks is achieved using K=1, i.e. a single-facet embedding.\n3. multi-facet embeddings can capture asymmetric relations like hypernymy\nHowever, the best performance is also achieved using K=1.\n\nGiven that best results for multiple tasks are achieved in the K=1 case, it would seem the proposed method reduces to be quite similar to the original word2vec formulation.\n\nThe paper evaluated the proposed embeddings on a range of tasks, and achieves favorable comparisons to baselines. However, the results are weakened by the fact that most of the tasks seem to lack strong published results to compare with (with an exception for summarization, which compares with, but under-performs Zheng & Lapata 2019). In some other cases, more recent baselines seem to be omitted:\nSection 3.1 - For BiRD, it seems there are stronger baselines in Table 4 of Asaadi et al. 2019?\nTable 4 - There do not seem to be comparisons with other recent work on hypernym detection, such as Le et al. 2019?\n\nThe experimental setup also does not really offer a fair comparison with BERT or similar approaches for generating contextualized language representations, where the learned representations are intended to be fine-tuned on the task of interest. The proposed multi-facet embeddings are not evaluated on target tasks of interest where fine-tuning data is available. This is one limitation of considering only the unsupervised setting, but the limitations of the BERT comparisons are also noted by the authors.\n\nThe writing seemed relatively clear overall, however in section 2.3 the model architecture, and its motivation, are a bit hard to follow. It seems unconventional to refer to the model as using a Transformer decoder, if not training the decoder with masked self-attention and not decoding auto-regressively. This would seemingly be more clearly described as a second Transformer encoder, with an additional attention sub-layer attending to the output of the first Transformer encoder? It may also be helpful to better motivate this modeling decision, if other architectures were not evaluated. There are many possible functions mapping phrases to a set of K vectors. For example, could a single Transformer encoder be used? Is it important to model interactions between the K vectors before the final output layer?\n\nOverall, it is an interesting question whether multi-facet embeddings are a better representation for words, phrases, and/or sentences than commonly used single-facet embeddings. The method proposed is novel, and achieves strong results on a wide range of unsupervised tasks. However, the core claim is weakened by the single-facet version of the proposed method performing best on multiple tasks, and several tasks lacking comparisons with strong previously published baselines. \n\nThe paper could be improved by clarifying the core contribution in the context of the best results being achieved using the K=1 variant of the method, and by comparing across tasks with more previously published results.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This work proposes a self-supervised objective to learn phrase/sentence representation. A text span is first encoded and clustered, and then used to predict neighboring tokens, minimizing mean square error between their representations. Experiments show that the proposed method outperform baselines on unsupervised text similarity tasks. \n\nAlthough the idea is very straightforward, the paper did not clearly present it. The approach appears very similar to many existing works, e.g., the only difference from skip-thought seems to be that here the orders of the surrounding context are ignored. Maybe the authors can justify their technical contribution. As for the experiments, I'm no expert in this specific field, but I would be really surprised if this work is among the most competitive ones, and this paper does not compare to many recent efforts, such as [1], which I found by a quick Google search. Last but not least, the paper does not do enough in explaining why the objective does very well in text similarity tasks, and makes no attempt in evaluating it on other downstream tasks.\n\nI do not recommend that the paper is accepted at its current status.\n\n[1] Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. https://arxiv.org/abs/1908.10084"
        }
    ]
}