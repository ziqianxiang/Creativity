{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "In this paper, the authors studied goal-oriented dialogue agents in the setting of a multi-player text-based fantasy environment. This problem is very interesting and also popular, which is a main direction for game company. But for the writing, I have a feeling that they translate to English by machine, e.g., \"We define the general task of,...\" in the second paragraph in introduction.\n\nThey trained a goal-oriented model with reinforcement learning. Numerically, they compare their method with a strong inverse model baseline."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper explores the goal-oriented dialogue setting with reinforcement learning in a Fantasy Text Adventure Game. To do so, the authors cast the original chit-chat task into a POMDP.\nIn this setting, the observations are the agent's previous utterances,  the other (static) agent's utterances and actions, and the game description. The actions either consist in selecting a topic (sec 4.2) or to select one utterance over a filtered set of human utterances. Finally, they report their results and observe that the RL approaches outperform the supervised learning models.\n\nI have several significant concerns about this paper on several facets, i.e., on the novelty and contribution part, on the algorithm part, and the initial claim.\n\nContribution part: \n  - It is an empirical paper as it assesses how scalable is an approach in a challenging setting. Such papers can be beneficial when they provide insights, extensive experiments, and share the authors' feedback. Unfortunately, those sections are not present in this paper. As a hint, the paper contains 7 pages to explain the method and 1/2 page to list the results and 1/2 page to analyze them, and there are no experimental appendices. Despite unarguable work (the authors perform heavy experiments on a big dataset), there are no simple ablation studies (10, 50, 100 topics? inverse model vs. non-inverse model), no comparison between the two models (e.g. how often the 50 utterances overlap? What are the error made by the models? Are there similar, complementary? Quid of mixture of expert), no qualitative analysis ( errors and/or limit analysis, why actions are harder than emotes?). Besides,  I assume that the authors spend some time fine-tuning the network, but there is no hint or take-away about the training difficulties: what can be generalized to other domains?  I agree that it is impossible to perform *all* those analysis, but I would have like *some* of them\n - Again, I acknowledge the considerable engineering work in this paper. However, it seems that most of this codebase contribution was already published in (Urbanek et al 2019). \n - The paper examines two methods: they either learn to predict the topic or perform topic retrieval. Even if those approaches make sense, they are not novels. For instance, the topic prediction is close to state tracking models where the state would be defined by the topic. There also exists an extensive literature of sentence retrieval, and some relevant works in RL such as (Dulac-Arnold, 2015)\n\nAlgorithm part:\n - This paper focuses on RL, but I have a few concern about the soundness of the experiments. It is also surprising that RL training is summarized in two sentences when it is supposed to be the core contribution of the paper.\n  \nI am particularly concerned with the RL experiments run with n=1. In this scenario, the agent only takes one action before the end of the episode. Thus, there is no planning involved, and applying RL has little interest here. At least, I would have appreciated a note on the impact of exploration.\n\nMore importantly, the core issue is the following: The reward is defined by 1 in case of success, 0 otherwise. If we apply policy gradient for with n=1 (e.g. A2C), and if we ignore the baseline, for now, the setting is strictly equivalent to applying a cross-entropy loss for every new success while discarding failure cases. In other words, the \"RL\" training procedure is equivalent to performing data augmentation with a supervised model.\n\nIn the case of A2C, the loss includes the td-error, an entropy regularizer, and a baseline. Yet, the predicted value function only learns to predict the success ratio as there is no bootstrapping n=1 while computing the td-error. It is thus equivalent to an auxiliary loss. While the baseline makes do penalize negative samples,  the underlying reward definition issue remains, and the current setting is more or less equivalent as performing hidden data augmentation with a supervised training loss.  \n\nNote that this observation does not apply for n=3, and I have no concern regarding those experiments. I am only missing the training hyperparameters.\n\nClaim:\nThe paper states that the paper somehow bridges the gap between the chit-chat setting and the goal-oriented setting. However, they cast the problem into a goal-oriented setting by having a reward for specific emote or action (which terminate the episode.) Even if I acknowledge the paper's motivations, I disagree that the paper provides a new perspective on this issue.\n\nOther remarks:\nFinally, I would have appreciated a sketch of the models, and a table listing the hyperparameters. \nAs the model work as the utterance level, there is no risk of language drift.\n\nIn the end, I recommend paper rejection for the following reason: \n - I feel that the paper is lacking sufficient analysis\n - The RL experiments with n=1 concern me\n - The novelty is extremely limited over the literature and (Urbanek et al 2019). \n - there is no training hyperparameters or experimental take-way which limit the reproducibility\n\nDulac-Arnold, Gabriel, et al. \"Deep reinforcement learning in large discrete action spaces.\" arXiv preprint arXiv:1512.07679 (2015).\n\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper studies a multiagent dialog task in which the goal of the learning agent is to generate natural language actions that elicit a particular action or emote from the other agent. Specifically the paper compares two Reinforcement Learning agents against an Imitation Learning baseline and shows that the RL-agents can achieve higher levels of task completion on seen and unseen dialogs. The experimental setup features a \"base agent\" that is supervised-trained to mimic human-players. The goal for the RL-agent is to produce dialog utterances that cause this base-agent to produce the desired action/emote. Two A2C-based RL agents are presented: the first chooses from a discrete set of 50 topics - then utilizes a pre-trained model to generate dialog based on the chosen topic. The second relies on a supervised-trained (again from Light's human dataset) model to generate 50 candidate dialog utterances, and then chooses amongst these. The baseline agent (dubbed 'Inverse Model') is trained via imitation learning to mimic the human players. All agents leverage recent advances in transformer architectures. Results show that RL agents can improve on this baseline agent both when given 1 and 3-steps of interaction for both action and emote tasks.\n\nOverall, this paper provides two interesting methods to incorporate RL training into large-scale transformer models while still generating complex natural language actions (assuming access to a pre-trained model or dataset of expert demonstrations). Broadly, I think the task is interesting and the sample dialogs show convincingly natural dialogs from both the RL and environment agents. I can see some promise in this as a proof of concept for a learning system that both interacts naturally and still accomplishes a goal, but I have several reservations that can hopefully be addressed:\n\nIt's unclear what the primary contribution(s) and takeaway from this paper is. I see possible aspects of (1) a new benchmark task for RL-based goal-oriented dialog - but no mention is made of release of models, (2) methods for Imitation + RL training of transformer-based policies - but unclear how novel these methods are, (3) proof of concept system for natural-goal-oriented-dialog - but it's not clear how faithfully environment-agent replaces actual humans.\n\nMy largest concern is the use of the baseline (environment) agent to simulate a human user. Due to the difficulty of accurately simulating human dialog (even in situation contexts such as these), it's difficult to distinguish between genuine advances in the quality of goal-oriented dialog versus overfitting against the simulated human (environment agent). I believe this paper would be greatly strengthened by including a human user study which evaluated the various approaches against real humans, or equivalent analysis to convince a reader that the RL agents are not simply optimizing to exploit weaknesses in the environment-agent.\n\nOn this note, little analysis is presented about the fidelity of the environment-agent. The set of 4 examples in Table 3 helps in this direction, but nothing is shown for 3-step dialogs, and there are no examples of failure. Specifically it would be great to see that the environment agent actually generates relevant responses to the action of the RL-agent and doesn't simply produce the dialogs/actions/emotes seen in the human-human dataset. It would be great to have a much more comprehensive appendix addressing this concern.\n\nHorizon of learning is very short. 1-step RL is contextual bandit learning and A2C is likely not the best choice for contextual bandit settings. 3-step is better, but I wonder if the RL-agent is actually generating dialog that conditions upon the utterances of the environment-agent (instead of just trying 3 of the top-scored actions rather than 1). Some 3-step dialog examples would help demonstrate the presence of contextually relevant dialogs.\n\nIn my view, this task is not true multiagent learning since the base-agent (environment agent) is not learning or changing its policy in concert with the RL-agent. This removes all the non-stationarity of true multiagent Reinforcment Learning. For this reason the last sentence of related work strikes me as false (although I would wholehartedly agree that the Light task, as administered to humans, is indeed multiagent).\n\nFinally I was unable to find any mention of the number of training runs performed for RL agents. As RL algorithms can be subject to learning instabilities and policy collapse, it is common practice to analyze the robustness of an RL-agent by performing several independent training runs for each agent. "
        }
    ]
}