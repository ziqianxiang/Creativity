{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This work proposes a new convolution layer with a context gating module, which consists of the Context Encoding Module, the Channel Interacting Module and Gate Decoding Module. The proposed method attempts to adjust the weights of the convolution kernel with context information. Experiments show that this context gated convolution layer can improve the performance of CNN on image classification and action recognition to some extent.\n\nCons:\n1. I don't agree with the first contribution. Previous studies like deformable convolution, attentive convolution, etc., are following the same direction and highly related. In addition, using context information to perform gating is incremental.\n2. I would like to know how the exact inference speed will be influenced with CGC. The computational complexity in terms of big O notation is not enough.\n3. Considering point 1, comparison with more baselines is desired.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a novel and interesting idea of modulating\nconvolutional kernels with global context.  They use depthwise\nfactorized channel-wise interaction to reduce complexity and parameters.\n\nWhile an interesting idea, I found it hard to see how the global\ncontext would usefully modulate the kernels.  While Figure 1\nillustrates the idea, it is not clear how the modulated kernel is\nbetter suited for the task relative to the original kernel.  It would\nbe nice if you could show a simple/toy example where your modulation\nclearly modifies the kernel in a way that is intuitively useful.\nOverall, I believe the motivation for the method could be made clearer\n- Why is this the best way to incorporate global context?\n\nIt is also unclear to me how the proposed network relates to the\nmotivating and cited Neuroscience literature.\n\nIt is nice that the computational complexity is carefully tracked.\nThe CGC network does seem to converge faster but the benefits to\nvalidation accuracy seem modest in Table 1/Fig 3.  It would be\ninteresting to further explore what is improving the convergence speed\n-i.e. to test your supposition that it \"improves the model's\ngeneralization ability and the gating mechanism reduces the norm of\ngradients back-propagated to he convolution kernels ...\".\n\nThe experiments are well described and extensive and some of them do\ngive some more substantial performance improvements, but overall I think the\npaper does not explain the motivation for the algorithm sufficiently for\nICLR publication.\n\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "Different from the attention module that modulates feature maps by incorporating the global context, this paper uses the global context to modulate the weights of convolutional layers and help CNNs capture more discriminative features. To validate its effectiveness, the author conducted a series of experiments on image classification, action recognition, and machine translation.\n\nThis idea is quite straightforward and the experimental results also show it will achieve high performance with fewer parameters compared to the feature maps modulating way. I am wondering are there any intuitive explanations why the weights modulating way is better than the feature modulating way? Or can we combine both the two way and achieve higher performance on large-scale datasets? I would appreciate it if there are some related discussions and experiments.\n\nMy major concerns are the experimental settings. For the action recognition, a lot of papers will test on Charades and Kinetics. The Something-Something dataset may be used primarily for visual understanding tasks. Of course, it is ok that we test the action recognition performance on the Something-Something dataset. But if there is only one video dataset used to report scores, I think the Charades and Kinetics is a better choice. For the machine translation, papers will mainly compare on the wmt en-de, including the cited papers. They reported the scores on both wmt and iwslt. Since the cifar-10 is very small where ablation studies performed, are the results in the table averaged over multiple training runs? And if so, how many? \n\nAlso, I notice the specific module designing is very similar to the module proposed in the Squeeze-Excitation Network. They will both first fuse the spatial information and then fuse information between different channels. The difference is the size of the final weight map, where the size in this paper is equal to the size of convolutional layers, and the size in SENet is 1."
        }
    ]
}