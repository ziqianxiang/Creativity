{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper proposes a method for out-of-distribution (OOD) detection for neural network classifiers.\n\nThe reviewers raised several concerns about novelty, choice of baselines and the experimental evaluation. While the author rebuttal addressed some of these concerns, I think the paper is still not ready for acceptance as is. \n\nI encourage the authors to revise the paper and resubmit to a different venue.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The paper considers the problem of out-of-distribution detection in the context of image and text classification with deep neural networks. The proposed method is based on [1], where cross-entropy between a uniform distribution and the predictive distribution is maximized on the out-of-distribution data during training. The authors propose two simple modifications to the objective. The first modification is to replace cross-entropy between predictive and uniform distributions with an l1-norm. The second modification is to add a separate loss term that encourages the average confidence on training data to be close to the training accuracy. The authors show that these modifications improve results compared to [1] on image and text classification.\n\nThere are a few concerns I have for this paper. The main issue is that I am not sure if the level of novelty is sufficient for ICLR. The contributions of the paper consist of a new loss term and a modification of the other loss term in OE [1]. At the same time, the paper achieves an improvement over OE consistently on all the considered problems. Given the limited novelty, experiments aimed at understanding the proposed modification would strengthen the paper. Right now I am leaning towards rejecting the paper, but if authors add more insight into why the proposed modifications help, or provide a strong rebuttal, I may update my score.\n\nI discuss the other less general concerns below.\n1. I believe the presentation of the method in Section 3 is suboptimal. The authors start with presenting a constrained minimization problem, then convert it to a problem with Lagrange multipliers, then modify the problem in an ad hoc way (adding a square and a norm without any mathematical reason), to get the standard form of loss with regularizers. The presentation would be much cleaner, and wouldn’t lose anything if the authors directly presented the loss with regularizers. Furthermore, in the Lagrange multiplier view the Lagrange multipliers are not hyper-parameters, they are dual variables, and the optimization problem shouldn’t be just with respect to theta, we need to find a stationary point with respect to both lambda and theta.\n\n2. The motivation for changing the distance measure between the predictive distribution on outlier data and the uniform distribution is unclear. The authors state multiple times that KL is not a distance metric, but it isn’t clear why this is important. KL is commonly used as a measure of distance between distributions. One could also use symmetrized KL in order to get a distance metric that is similar to KL. I am not opposed to just using l1-norm because it performs better, but if the switch of distance measures is listed as one of the two main methodological contributions, I believe more insight needs to be provided for why it helps.\n\n3. The motivation for the other loss term which is enforcing calibration of uncertainty on train data is also not very clear. At least in image classification, strong networks typically achieve perfect accuracy (or close to that) on the train set, and then the proposed loss term would basically push the predictive confidence on all training data to 1, which is already enforced by the standard cross-entropy loss. Does outlier exposure prevent the classifier from getting close to 100% accuracy on train? What is the train accuracy for the experiments on CIFAR-10, CIFAR-100 and SVHN?\n\n4.  While the authors report accuracy of out-of-distribution detection in the experiments,  they don’t report the accuracy of the actual classifier on in-distribution data. Is this accuracy similar for the proposed method and OE? Is the accuracy also similar for the proposed method and baseline for the experiment in section 4.4?\n\n5. The method is only being compared to the OE method of [1]. Why is this comparison important, and are there other methods that the authors could compare against? \n\n[1] Deep Anomaly Detection with Outlier Exposure. Dan Hendrycks, Mantas Mazeika, Thomas Dietterich"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This work proposes a new loss function to train the network with Outlier Exposure(OE) [1] which leads to better OOD detection compared to simple loss function that uses KL divergence as the regularizer for OOD detection. The new loss function is the cross entropy plus two more regularizers which are : 1) Average ECE (Expected Calibration Error) function to calibrate the model and 2) absolute difference of the network output to $1/K$ where $K$ is the number of tasks. The second regularizer keeps the softmax output of the network uniform for the OE samples. They show adding these new regularizers to the cross-entropy loss function will improve the Out-distribution detection capability of networks more than OE method proposed in [1] and the baseline proposed in [2].\n\n\nPros:\nThe paper is written clearly and the motivation of designed loss functions are explained well.\n\nCons:\n1- The level of contributions is limited.  \n\n2- The variety of comparison is not enough. The authors did not show how the approach is working in compared to the other OOD methods like ODIN[3] and the proposed method in [4].\n\n3- The experiments are not supporting the idea. First, the paper claims that the KL is not a good regularizer for OOD detection as it is not a distance metric. But there is no experiment or justification in the paper that supports why this claim is true. Then the second contribution claims that the calibration term that is added to the loss function improves the OOD detection as well as calibration in the network, but the experiments are not designed to show the impact of  each regularizer term separately in improving the OOD detection rate.  Figure 2 also does not depict any significant conclusion. It only shows that the new loss function makes the network more calibrated than the naive network. This phenomenon was reported before in [1]. It would be better if the paper investigated the relation between the calibration and OOD detection by designing more specific experiments for calibration section.   \n\nOverall, I think the paper should be rejected as the contributions are limited and are not aligned with the experiments.  \n\nReferences\n[1]Hendrycks, Dan, Mantas Mazeika, and Thomas G. Dietterich. \"Deep Anomaly Detection with Outlier Exposure.\" arXiv preprint arXiv:1812.04606 (2018).\n\n[2] A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks, ICLR2016.\n\n[3] Liang, Shiyu, Yixuan Li, and Rayadurgam Srikant. \"Enhancing the reliability of out-of-distribution image detection in neural networks.\" arXiv preprint arXiv:1706.02690 (2017).\n\n[4] Lee, Kimin, et al. \"A simple unified framework for detecting out-of-distribution samples and adversarial attacks.\" Advances in Neural Information Processing Systems. 2018."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "This paper proposes to tackle the problems of out-of-distribution (OOD) detection and model calibration by adapting the loss function of the Outlier Exposure (OE) technique [1]. In OE, model softmax outputs are encouraged to be uniform for OOD samples, which is enforced through the use of a KL divergence loss function. The first proposed modification in this paper is to replace KL divergence term with an L1 penalty. The second change is the addition of an L2 penalty between the maximum softmax probability and the model accuracy. Experimental results demonstrate that adding these two components increases performance over OE on standard OOD benchmarks for both vision and text domains, and also improves model calibration.\n\nAlthough this paper presents some good quantitative results, I tend towards rejection in its current state. This is mainly due to the limited comparison to alternative methods, and the lack of ablation study. If these were addressed I would consider increasing my score.\n\nThings to improve the paper:\n1) Currently, one of the most commonly used benchmark methods for OOD detection is the Mahalanobis distance based confidence score (MD) [2], which, as far as I am aware, is state-of-the-art among published works. The authors claim that they do not compare to this work because it is a post-training method, and, presumably, the techniques should be doubly effective when combined. However, we do not have any proof that this is actually the case. Therefore, I think it is important to verify that the two techniques are indeed compatible, and if not, then direct comparison with MD would still be necessary.\n\n2) In the case of confidence calibration, there is no comparison made with other calibration techniques, such as temperature scaling [3]. I think it would be good to included these for reference.\n\n3) Since two distinct components are being added to the loss function, I think it is important to include an ablation study to identify how much each component contributes to improvements in OOD detection and confidence calibration.\n\n\nMinor things to improve the paper that did not impact the score:\n4) With regards to the confidence calibration loss, there is similar work by [4] which also optimizes the output of the model to make sure confidence predictions are close to the true accuracy. It may be worth citing if you think it is relevant.\n\n\nAdditional questions:\n5) How are lambda1 and lambda2 tuned? I could not find this information in the paper.\n\n6) How sensitive is model performance to the setting of the lambda hyperparameters? It would be nice to see a plot of lambda versus the OOD detection metrics.\n\n7) Have you evaluated how this method performs at detecting adversarial attacks? I do not think the paper will suffer without these results, but they are certainly relevant and of interest to practitioners in this area.\n\n\nReferences:\n[1] Hendrycks, Dan, Mantas Mazeika, and Thomas G. Dietterich. \"Deep anomaly detection with outlier exposure.\" ICLR (2018).\n\n[2] Lee, Kimin, Kibok Lee, Honglak Lee, and Jinwoo Shin. \"A simple unified framework for detecting out-of-distribution samples and adversarial attacks.\" NeurIPS, (2018).\n\n[3] Guo, Chuan, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. \"On calibration of modern neural networks.\" In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1321-1330. JMLR. org, 2017.\n\n[4] Corbière, Charles, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, and Patrick Pérez. \"Addressing Failure Prediction by Learning Model Confidence.\" NeurIPS (2019).\n\n\n### Post-Rebuttal Comments ###\nI would like to thank the authors for their hard work during the rebuttal period. I think the current version of the paper is much improved over the previous version. The choice to remove the claims about calibration definitely improves the focus of the paper. The addition of the  Mahalanobis distance experiments and the ablation study also significantly strengthen the paper. However, as the other reviewers have pointed out, the novelty of the paper is quite limited since the majority of the gains come from simply swapping the KL-divergence penalty with an L1 penalty. Despite simplicity, this single change yields a significant improvement in performance for the OE algorithm, which is noteworthy. As a result, I will increase my score from a weak reject (3) to a very, very weak accept (more like a 5 than a 6). ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        }
    ]
}