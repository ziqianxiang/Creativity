{
    "Decision": {
        "decision": "Reject",
        "comment": "Three reviewers have scored this paper  as 1/1/3 and they have not increased their rating after the rebuttal and the paper revision. The main criticism revolves around the choice of datasets, missing comparisons with the existing methods, complexity and practical demonstration of speed. Other concerns touch upon a loose bound and a weak motivation regarding the low-rank mechanism in connection to DA. On balance, the authors resolved some issues in the revised manuscripts but reviewers remain unconvinced about plenty other aspects, thus this paper cannot be accepted to ICLR2020.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "In this work, the authors improve the work (Raab & Schleif, 2018) by (1) reducing the computational complexity, (2) neglecting the sample size requirement, and (3) achieving a low-rank projection through Nystrom approximation. More specifically, the feature dimensionality is reduced by using only s biggest eigenvalues and eigenvectors, and the sample size is coordinated through Nystrom approximation. Class-wise sampling is used for the source, and uniform sampling is used for the target. Experimental studies on three datasets have been done.\n\nThis paper should be rejected because (1) the paper lacks important latest references on domain adaptation, (2) the paper misuses the notations that makes the paper is not easy to follow, (3) the algorithm is not well justified either by theory or experiments, and (4) the presentation should be further polished. Here are some detailed comments:\n\n(1)\tA lot of recent deep domain adaptation methods are missing. These deep works achieve the state-of-the-art results on many transfer benchmark tasks. Without comparison with them (the paper does not mention any deep works), it is very unconvincing to conclude the paper makes new contributions to the transfer community. \n(2)\tIn line 2 of page 5, the authors claim that BT assumes S_Z ~ S_X, which is not true. The key idea of BT is to construct new source data using target basis and source eigenvalues. Similarly, the authors make the same claim in section 4.1 for NBT, which is also not valid. \n(3)\tThe notations in eq. (12) and (13) are very misleading. Eq. (12) follows the notations of the first line in page 5, but in the first line below eq. (12), why R_X \\in R^{d \\times s}, S_X^2 \\in R^{s \\times s}? I understand that X_s is the low dimensional X, then X_s = XR_s with R_s is the dimensionality reduction matrix whose size is d \\times s (using biggest s eigenvectors is fine). With this, eq. (13) is incorrect as X should be L_X*S_X*R_X^T, but not L_s*S_s*R_s^T. Moreover, it is also unclear why X is decomposed into product of two matrices in this work, is there any benefit of doing so for transfer purpose? \n(4)\tIn section 4.1, A_Z and A_X have exactly same form with X and Z, i.e., L_Z*S_Z*R_Z^T and L_X*S_X*R_X^T, please clarify. How eq. (14) X_s = \\tide{L_X}*S_X come from? Is it the same as eq. (13)? \n(5)\tThe title highlights low-rank, but it is not very clear how low-rank matters in the proposed method. I do not find contents stating the low-rank property of the proposed algorithm in the main technical sections.  \n(6)\tRegarding section 4.2, what is the benefit of using class-wise sampling for the source? Have you tried to use uniform sampling for both the source and target domains?\n(7)\tThe bound in eq.(16) is not very meaningful as s << n, m, d. Moreover, I am also not convinced by the claim this reduces the distribution differences, please give more theoretical justifications.\n(8)\tRegarding the experimental studies, why not use accuracy as many existing works do? The baselines are all subspace-based papers, and are out-of-the-date. The latest subspace papers, e.g., JGSA and MEDA, should be included. Moreover, deep methods are completely missing, which makes the empirical evaluation much less convincing. The improvements of NBT to BT are very marginal, 0.6. \n(9)\tSome typos and unclear points (please further polish the paper): \n(a)\tThe last sentence of para 2, X and Z should be data, not features/\n(b)\tPara 3, it is unclear what are the implicit alignment and explicit alignment of domain distributions.\n(c)\tThe first sentence in section 2, it should be homogeneous. \n(d)\tPage 5 above eq. (15), it should be S_Z ~ S_X.\n(e)\tSection 4.2 the second line, it should be “in the data matrix”\n(f)\tThe second last line of page 6, it should be inequality (16).\n(g)\tSome references miss page information.\n\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "1.\tThis manuscript is mainly based on the previous work BT (Raab and Schleif, 2018). The novelty seems to be too limited.\n2.\tThe proposed method NBT improves the computional efficiency of BT. The Datasets used in the experiments are not representative. More experiments should be conducted to demonstrate its efficiency on large-scale Datasets, such as VisDA and Offic-Home Dataset.\n3.\tThe precise results of CGCA are not provided, which is unfair. Besides, there is no comparsion between the proposed methods and the state-of-the-art deep learning-based methods.The experimental results seems unconvincing.\n4.\tTypos:\n(1)\t‘Out BT approach has no free parameter…’ in page 14. Here ‘Out’ means ‘Our’?\n(2)\t‘…NBT is fastest at Newsgroup und Image data set. In page 14.’\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper is mainly an improvment of the basis transfer model, by introducing the Nystrom approximation and a NBT model is formulated. This paper lacks of research motivation and solid experimental validation.\nThe authors claim \"it is the fastest domain adaptation algorithm in terms of computational complexity\", which is not very convinced.\nI cannot observe new knowledge in domain adaptation, except the Nystrom approximation technique used.\nThe equation (9) is similar to the representational based transfer model, where the T matrix is just the reconstruction matrix, because X and Z have different number of samples.\nThis paper written is a little poor, and the novelty of NBP is not clearly claimed.\nThe experimental effectiveness is weak and have no improvement compared with BT 2018 in image dataset.\nIn deep learning era, could you discuss what is the value in deep transfer learning or deep domain adapatation?"
        }
    ]
}