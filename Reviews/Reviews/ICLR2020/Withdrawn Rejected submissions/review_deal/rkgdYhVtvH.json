{
    "Decision": {
        "decision": "Reject",
        "comment": "The authors attempt to unify graph convolutional networks and label propagation and propose a model that unifies them. The reviewers liked the idea but felt that more extensive experiments are needed. The impact of labels needs to be specially studied more in-depth.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper connects graph convolutional networks with label propagation. There are a numbder of issues that need to be solved before possible publication.\nThe theoretical part (secion 2) is hard to follow. For example, the authors introduce in 2.1 a mapping M from vertices to labels. Then in Theorem 1, this mapping M indeed maps features of the vertices to labels. But then waht is the meaning of a L2 norms between features? At this point, I had a look in the experiment section to see what features are considered in practice. In section 4.1, it is written for the citation networks: 'each node has a sparse bag-of-words feature vector' or in the coauthor networks: 'Node features represent paper keywords for each author's paper'. How do you relate these claims to your theoretical analysis? Things get even worse in section 2.3, where derivatives with respect to initial feature vector are taken. How do you take a derivative with respect to a bag of words?\nEquation (5) is not clear at all, we need to read the end of this section to understand that A^* is constrained to have the same support as the adjacency graph and computed as a function of the node features. The authors shuold also define clearly y_hat in (5)."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #3",
            "review": "The paper proposed an unified model for Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN). It is shown how it is possible to infer the relationship between LPA and GCN in terms of label or feature smoothing (how label/feature does propagate over the neighbors) and label or feature influence over the other nodes. The results are given in terms of two theorems (whose proofs are in an appendix) which essentially state that the total label influence of nodes with a particular label “l” on a specific node is proportional to the probability that node is labelled as “l” by LPA. In practice, LPA acts as a regularizer to learn transformation matrices and edge weights simultaneously in GCN. By means of a simple joint loss (eq.8), the regularized training show that transductive learning with the joint model surpasses GCN/GNN baselines. \nWhile the proof of the first theorem is reasonable and seems correct (Taylor+ Schwarz inequality + L-Lip), the second left me a little puzzled, in particular wrt eq 16 and 17. Is it possible to add a graphical explanation or idea of the proof? Why “y” has to be reset at each iteration?\nUnfortunately, the improvement brought by the framework is marginal, even if it seems general. \nThe other limitation is that we have transductive training, but the authors are well aware about this.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "This paper introduces a unified model which combines label propagation algorithm (LPA) and graph convolutional networks (GCNs) for node classification. The motivation of this combination is supported by two analysis on the feature/label smoothing and feature/label influence. The proposed GCN-LPA framework utilizeds LPA to adjust the edge weight A* through the label information. Then, this edge weight A* is used to transfer the knowledge from label information to feature information for enhancing the representation learning in GCN. An end-to-end  solution is proposed by treating the LPA process as regularization. Overall, the idea of unifying GCNs and LAP in an end-to-end fashion is interesting. \n\nOne major concern is that from the experiment, it is unclear how much the LPA impacts the node classification. It will be more convincing if the performance comparison under different percentage of labeled samples (during LPA) is provided. \n"
        }
    ]
}