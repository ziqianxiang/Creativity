{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "In this paper authors propose a framework for learning in multi-agent systems. Main contribution of the proposed framework is that it learns the communication structure between the agents, which improves the perfomance. In order to impose a structure authors use a graph neural network, which enables the agents to share information and rewards. They empirically show that their method is able to outperform Multi-agent Deep Deterministic Policy Gradient (MADDPG), Trust Region Policy Optimization (TRPO) and Deep Deterministic Policy Gradient (DDPG) on Waterworld, Multi-Walker and Multi-Ant tasks.\n\nI would recommend the paper to be accepted since the authors came up with an innovative solution to the difficult problem of how to coordinate in a multi-agent setting although they have a limited experimental setup. In order to improve the exposition I would recommend the authors provide more evidence along the lines of how the proposed method deals with indefinite number of agents problem. How does the performance change as more agents added to the task. They chose 4 in their experiments why that number was chosen? How does the gap between baseline methods vs. SMAL change as the number of agents increase?"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a new way to perform multi-agent reinforcement learning via using a graph neural network like method to pass messages from agents. They show performance improvements over baseline techniques such as TRPO, DDPG, and also multi-agent algorithm MADDPG.\n\nThe problem is very interesting and I do like the experimental domains such as multi-walker. However, the experimental results lack comparison against a simple baseline and it is unclear how the proposed SMAL algorithm would perform against basic multi-agent RL techniques.\n\nThere are also various inaccurate or debatable claims in the paper.\nIn the first paragraph, the authors state that single-agent RL learns to fulfill goals without modeling the behaviors of others. This is clearly not the case as the most basic model-free approaches such as DQN agents may indeed learn to model other’s behaviors inside their policy networks.\n\nA basic centralized multi-agent system would have action spaces N times A where A is the action space of a single agent. Learning would take observations from all agents and act jointly and in a coordinated manner (both centralized actor and centralized critic). This system however is not scalable to a large number of agents. However, it is still a good baseline to compare against. For example. How would a centralized multi-agent DDPG system work on multi-walker? e.g. with dimensions 4*24 ? I think it would be surprising if SMAL would be better than this baseline, what are authors thoughts/views on this?\n\nFirst paragraph of page 2 suggest that SMAL is able to learn +ve and -ve weights to suggest whether an agents are collaborators or competitors. However, from sections 4.1.1 and 4.1.2, it appears that those bounds on v_ij are actually set by hand.\n\nPositive/negatives weights on an edge is hardly a good indicator of collaboration/competition. I do not see a relationship between the two. Typically, collaboration or competition arise out of the reward function, e.g. whether it is a zero-sum game or a general-sum game.\n\nIn addition, the claim in 4.1.2 that it is ok to zero-out messages from ‘competitors’ are dubious. Why would it not be beneficial to infer additional information from messages of other adversarial agents?\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The paper applies ideas from message passing over a global graph between agents to perform multi-agent reinforcement learning. The algorithm SMAL is demonstrated on a suite of continuous control domains.\n\nThe paper is imprecise, unpolished and doesn't reflect the state-of-art fairly. It does not support as significant contribution. The current approach conditions the policy on the messages from other agents and therefore is a centralized controller. Experiments make comparison to methods that either support decentralized execution (MADDPG) or are single-agent RL techniques (TRPO) which is unfair. No comparison to other methods like [1,2] that learn to communicate between agents or even the introduced environments in those works. The claim that \"messages and rewards are both necessary for collaboration\" is unjustified from the experiments given the environment comes from [3] which did not allow any communication between agents and showed scaling to 10 agents compared to 4 agents in the current work.\n\n[1]: https://arxiv.org/abs/1605.07736\n[2]: https://arxiv.org/abs/1812.09755\n[3]: https://link.springer.com/chapter/10.1007/978-3-319-71682-4_5"
        }
    ]
}