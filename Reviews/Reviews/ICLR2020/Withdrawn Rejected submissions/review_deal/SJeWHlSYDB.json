{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper studies spread divergence between distributions, which may exist in settings where the divergence between said distributions does not. The reviewers feel this work does not have sufficient technical novelty to merit acceptance at this time.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper proposes a new divergence, called spread divergence, to distinguish probability models. The approach is motivated from the concern that traditional divergence such as f-divergence or KL divergence may not always exist, in which the spread divergence may be a substitute. Some empirical supports are provided for the proposed method. Below I will summarize my concerns.\n\n1. The spread divergence is proven no larger than the traditional divergence, so the paper claims this as an advantage of using spread divergence. My question is that if KL or f-divergence of two probability models is infinity, which means they distinguish the models very well, whether a new method is necessary (though it may provide a finite value). \n\n2. As a new method, it would be useful to thoroughly compare it with the traditional ones. There is a lack theoretical comparison with the KL or f-divergence. Some numerical examples are provided but seem not enough. For instance, the applications focus on the situations where likelihood is not defined, i.e., data are deterministic w/o observation noise. It is interesting to see other examples where likelihood is defined and how traditional methods perform.\n\n3. Kernel based spread divergence has been a major focus of this paper. It is interesting to see which kernel maximizes the spread divergence. Section 3.2 considers Gaussian kernel. Is this an optimal option?\n\n4. Section 5 compares EM and spread EM based on one experiment and claims the latter has smaller error.   Does the same conclusion holds true in other examples?\n\nI believe the motivation of this paper is interesting. This would be a stronger paper if more theoretical and empirical analysis can be added."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "I think the paper must be Desk-rejected as the identity of the authors was revealed. \nThis thing aside, the paper is an interesting contribution. The concept of spread divergence can be valuable in many context. The presentation is thorough and the theoretical part is correct. On the other hand, the examples are quite diverse and include a standard model (ICA) as well as modern deep generative models. Thus, it represents a valuable contribution worth of publication, if we ignore the identity revelation aspect. "
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper introduced a way to modify densities such that their support agrees and that the Kullback-Leibler divergence can be computed without diverging. Proof of concept of using the spread KL divergence to ICA and Deep Generative Models ($\\delta$-VAE) are reported based on the study of spread MLE.\n\nComments:\n\nIn Sec 1, mention that f should be strictly convex at 1. Also mention \nJensen-Shannon divergence, a KL symmetrization, which is always finite \nand used in GAN analysis.\n\nIn Sec 2, you can also choose to dilute the densities with a mixture: \n(1-\\epsilon)p+\\epsilon noise.\nExplain why spread is better than that? Does spreading introduce \nspurious modes?, does it change distribution sufficiency? \n(Fisher-Neymann thm)\n\nIn Formula 4, there is an error: missing denominator of \\sigma^2. See \nAppendix D too.\n\nIn footnote 4, page 8, missing a 1/2 factor in from of TV (that is upper \nbounded by 1 and not 2)\n\nKL is relative entropy= cross-entropy minus entropy. What about spread KL?\nIn general, what statistical properties are kept by using the spread? \n(or its convolution subcase?)\n\nIs spreading a trick that introduces a hyperparameter that can then be \noptimized for retaining discriminatory power, or is there\nsome deeper statistical theory to motivate it. I think spread MLE should \nbe further explored and detailed to other scenarii.\n\nSpreading can be done with convolution and in general by Eq.3:\n\nThen what is the theoretical interpretation of doing non-convolutional \nspreading?\n\n\nA drawback is that optimization on the spread noise hyperparameter is \nnecessary (Fig 3b is indeed much better than Fig 3a).\nIs there any first principles that can guide this optimization rather \nthan black-box optimization?\n\nOverall, it is a nice work but further statistical guiding principles \nor/and new ML applications of spread divergences/MLE will strengthen the \nwork.\nThe connection, if any, with Jensen-Shannon divergence shall be stated \nand explored.\n\nMinor comments:\n\nIn the abstract, state KL divergence instead of divergence because \nJensen-Shannon divergence exists always.\n\n\nTypos:\np. 6 boumd->bound\nBibliography : Cramir->Cramer, and various upper cases missing (eg. \nwasserstein ->Wasserstein)\n"
        }
    ]
}