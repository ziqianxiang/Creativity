{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Summary: The paper presents an approach to generate 3D training data to improve the performance. At the core of this work is the use of hybrid gradients, i.e., a combination of analytical gradients (task-specific network) and approximate gradients (graphics renderer). The benefits (computational and generalization) of the proposed approach is shown using three benchmarks for depth and surface-normal estimation: (1). MIT-Berkeley dataset; (2). NYU-v2 depth dataset; and (3). Basel Face. \n\nPros: \n\n+ well-done evaluation and comparisons with prior art! The authors demonstrate the various design decisions and improvements over existing works on different datasets.\n\n+ intuitive combination of analytical gradients and approximate renderers to combine the synthesizer and task-specific network.\n\n\nConcerns: \n\n- A goal for the use of synthetic data is to enable an extensively large amount of data to train parametric models, which may otherwise not be available because of the constraints on capturing and creating real data. It is, therefore, important as how we can automatically create a humungous diverse synthetic data. However, the proposed approach comes no-where close to do it. There is an upper bound to what we can achieve using the proposed method, and it by no-means come close to what we can make just by using a few samples from real data. Here are more specific instances for my reasoning: \n\n(1). Section-5.2 and Figure-6:  Only a little modification (camera position and object rotation) is possible in terms of placement of objects in a scene. This constraint does not enable us to create something far different from what already existed in synthetic data.\n\n(2). Table-2: The use of hybrid gradients enable smarter ways to do augmentation and leads to better performance than prior art. However, the performance is far lower than what one could achieve with few samples from real data (using only 750 training examples images from NYU-v2 dataset) even when trained from scratch. Here are the numbers of this setup: Mean: 21.2; Median: 13.4; <11.25: 44.2; <22.5: 66.6; and <30: 74. \n\nIt is by no means clear as to how the proposed approach can ever come close to this performance, and make the current plan unuseful for any practical purposes. It will be useful to understand what future directions can be pursued to close this gap. Additionally, the authors can add experiments that use the network trained on synthetic data as an initialization and fine-tune it for real data distribution. Does it lead to better performance?\n\n- Figure-4: please mention the delta difference at the time of convergence of hybrid gradient with other approaches. It gives a sense of how far do we get in terms of computational efficiency.\n\nMinor Concerns: \n\n-inconsistent citation for PCFG in Sec-1 and Sec-4.\n\n- typos and grammatical errors here and there."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This work proposes an approach to automatically adapt the generation of synthetic 3D training data such that a downstream neural network performs well on real data. The idea is to adaptively modify the data generation during training such that the network performs optimally on a validation set. The authors propose a gradient-based optimization pipeline, where they unroll the intermingled data generation and training process and compute approximate gradients for the non-differentiable rendering module using random search.\n\nSimilar approaches have been proposed before, the main idea that sets it apart is random sampling in black-box modules in order to get an estimate for the gradient and to combine this with analytic gradients wherever they are available. I'm a bit surprised that this \"hybrid gradient\" hasn't been used before, but a quick literature search seems to confirm this claim.\nMy main concern with the approach is limited scalability. Random search gets harder for higher dimensions. The scalability issue seems to surface already in this work: The face model experiment would feature 400 rendering parameters, but the authors chose to do dimensionality reduction (I assume in order to make the problem tractable with random search). One would argue that any simulation that can transfer to real data reasonably well will likely need orders of magnitude more parameters than what is handled here. The observation is also in line with the other experiments: The grammar for NYUv2 features camera pose and the poses of a handful of shapes and the CSG experiment is very simplistic.\n\nThe claim that none of the networks has seen a single training image (except for validation) in the NYUv2 experiment is misleading. Clearly, the validation loss provided an error signal throughout the (meta-)training of the approach (c.f Figure 2). In general, the validation set will leak information into the training in this approach. It is thus not surprising that some of the baselines which can't leverage this information (e.g. random \\beta) will perform worse. \n\nFinally, it doesn't seem to be a good idea to only train on synthetic data (when doing computer vision). I'm missing more realistic experiments and comparisons. How would the approach fare against a modern monocular depth estimation model (for example MegaDepth)? Why only evaluate the normals and not also evaluate the depth maps? Most importantly: Can the model ensure that synthetic data provides value beyond, or in combination with, real data?\n\n\nSome parts of the paper are unclear:\n\n- For how many steps is the training loop unrolled? Can you discuss any limitations that the unrolling imposes in practice. E.g. would limited memory be an issue with larger networks and more unrolling steps?\n- It is not clear to me what the \"Basic Random Search\" baseline in Table 2 and Table 4 is exactly. \n- NYUv2: What does it mean to initialize the network using the original model in Zhang et al. 2017? Is that the pre-trained network, or the network architecture initialized with random weights?\n- Section A.2. mentions using RMSProp for the intrinsic image dataset. Was RMSProp only used on this dataset? If so, why?\n\n\nProblems with citations:\n\n- Citing Mania et. al. 2019 for the random search procedure seems like a misattribution. Random search as a DFO method is very old [1], and getting an estimate of the gradient via random sampling has been explored in multiple other work [2,3].  \n- Missing citation: [4] is close to the presented work and should be cited and adequately discussed.  \n\n\nTypos: Page 3, 2nd paragraph \"the generated the\"\n\nSummary: The paper proposes a simple, (to the best of my knowledge) novel technical trick, but it is not clear if the approach would provide value in practical settings.\n\n[1] J. Matyas. Random optimization. Automation and Remote control, 1965\n[2] A. D. Flaxman, A.T. Kalai, and B.H. Mcmahan, Online convex optimization in the bandit\nsetting: gradient descent without a gradient, ACM-SIAM Symposium\non Discrete Algorithms, 2005\n[3] Y. Nesterov and V. Spokoiny. Random gradient-free minimization of convex functions. Foundations of\nComputational Mathematics 2017.\n[4] N. Ruiz, S. Schulter, and M. Chandraker. Learning To Simulate, ICLR 2019"
        },
        {
            "rating": "8: Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper presents a method for optimizing synthetically-generated scenes for transferability to real scenes. The synthetic scene is parameterized by a vector of parameters (beta), and the Authors propose a method to optimize the evaluation loss with respect to beta by gradient descent. The key contribution is a method for calculating the gradient of the evaluation loss with respect to beta, which is done half-analytically (by differentiating the evaluation loss with respect to the training images using unrolled SGD) and half-numerically (differentiating the training images with respect to beta by sampling in the beta space).\n\nThe idea is conceptually simple and neat, but practically it seems fairly complicated to chain all these derivatives together to obtain an accurate-enough estimate of the gradient for gradient-descent.\nIt is very impressive IMO that in the end of the day this loop can indeed be closed, and synthetic scenes can be automatically optimized for transferability to real data, as the results prove.\n\nGiven the great importance of sim-to-real in computer vision and robotics, the neatness of the method, the strong results, and the clear presentation, I am rating this paper \"accept\". At the same time, I admit that I have little experience myself with sim-to-real problems and generating simulated environments, and it is thus harder for me to assess the novelty and the comparison of this approach to others.\n\n"
        }
    ]
}