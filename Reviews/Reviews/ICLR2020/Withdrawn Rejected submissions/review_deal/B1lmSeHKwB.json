{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "To better tackling zero-shot learning problem, this paper utilized the attribute information of seen and unseen classes to bridge the visual features to semantic space (attribute space). In order to bridge the visual to semantic features, the authors proposed soft-labeling to compute the similarities between seen and unseen classes in attribute space.\n\nIt is interesting to use soft labeling for GZSL. The soft labeling enables the classifier to learn unseen classes through samples from seen classes.\n\nIn terms of writing, native speakers are needed to carefully proofread the submission. There are many grammar mistakes, e.g. missing articles (a/an/the). And some sentences are kinda contextually redundant.\n\nMy major concerns are as follows:\n1. In terms of the object function in Eqn. (3), why \\|W\\|_F and \\|W\\| are both used? What do you want to balance here?\n\n2. It is expected to see the influence of the soft labeling. To show that, analysis on \\alpha, which balances the hard and soft labelings, is needed.\n\n3. It's not clear how many training/test images are there from each dataset. Also, how to tune the hyper-parameters, \\lambda and \\gamma, was not mentioned.\n\n4. The experimental results are not consistently better than baselines on all the datasets/settings. For some settings, the performance of the proposed SZSL is quite worse than a few others."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #5",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Summary\n========\nThis paper proposes a simple method for generalized zero-shot learning where the objective at test time is to classify between seen and unseen classes, where for the seen ones there were both images and attributes available at training time whereas for the unseen ones there were only attributes available at training time. Their proposal is to first embed image features x into a “semantic space” g(x). There, multiplication with a fixed “attribute matrix” A yields “logits” for classifying x into each of C classes, where C is the union of seen and unseen ones. Their main contribution is a mechanism to mitigate the fact that the training images x will never belong to unseen classes, and therefore the learned semantic space may “overfit” to capturing features that are useful for classifying seen classes but not necessarily unseen ones. \n\nSpecifically, they propose a soft labeling method where each seen image is assumed to belong (to some small extent) to unseen classes whose attributes are similar to its semantic representation, instead of belonging (in a hard manner) only to its ground-truth seen class. The soft label of each example is a vector whose length is the total number of seen and unseen classes as usual, but with more than one non-zero entry. Specifically, it will have exactly one non-zero entry corresponding to a seen class (the ground truth class) and at least one non-zero entry corresponding to unseen class(es). The non-zero unseen entries sum up to q, which is a hyperparameter, and the non-zero seen entry therefore has value 1-q. Their overall objective constitutes minimizing two losses in a weighted fashion (governed by another hyperparameter). The first loss is the standard (hard) cross-entropy, and the second is another cross entropy using the soft labels instead.\n\nExperimentally, they obtain strong performance on standard benchmarks, especially in terms of accuracy on unseen classes when compared to previous methods. However, they observe a trade-off in performance on seen and unseen classes.\n\nReview\n======\nA) Limitation: requires knowing the unseen classes in advance\nThe authors discuss that the proposed soft labels approach is a supervised regularizer that is more effective than the previously-used unsupervised ones. However, it also has an important limitation that is not discussed: the proposed method is transductive in that it requires knowing in advance (i.e. during training time) the attributes of each unseen class. It therefore is not able to handle truly new classes at test time, e.g. in an online fashion.\nNotice that other methods don’t have the same limitation. For example, consider having trained a generative model that given a set of attributes characterizing a new class can generate images of that class. Then, no matter what classes we are given at test time (as long as they are expressible in terms of the same set of known attributes), we will be able to generate a training set for it. As far as I understand, CRnet does not suffer from that limitation either.\nIt might be that in the current benchmarks the unseen class attributes are indeed available in advance, but this in general should not be expected. It should be noted that the proposal is a solution to a restricted version of the problem.\n\nB) Trading off performance on seen classes for performance on unseen classes\nThe ideal objective would use soft labels to obtain some training signal for related unseen classes of each training example, but without sacrificing the ability to classify that training example into its true ground truth (seen) class. In other words, soft labels should allow to improve on unseen classes in addition to (not instead of) improving on seen classes. However, the formulation in Equation 6 says that an example belongs to some extent to unseen classes *at the expense of* belonging to its ground truth seen class. This naturally degrades the performance on seen classes (indeed their results are competitive in terms of accuracy on unseen classes but not the best in terms of accuracy on seen classes), and gives rise to trade-offs regulated by the value of q.\nA more natural formulation would be to treat this as a multi-label classification problem where there is more than one “correct” class for each output: one is the ground-truth (seen) class, and the other(s) is/are the similar unseen class(es), in a soft manner. Perhaps having a formulation that captures this desideratum would also alleviate the need for having both L_hard and L_soft as weighted terms of the overall loss.\n\nC) Insufficient description of related work. \nA lot of works are cited but they are not described sufficiently. High-level statements like “some methods utilize embedding techniques” and “others use semantic similarity between seen and unseen classes” aren’t sufficient in my opinion to get a sense of what those methods did and how the proposed one relates to them.\n\nD) At the end of page 3, just above section 3.3, in the definition of q, should i also be marginalized out? Otherwise q is dependent on a particular seen class i?\n\nE) Aside from being non-trainable, there is no description of the matrix A. Is each attribute vector a_k associated with a particular class a binary vector, i.e. each entry is a 1 iff that attribute is present for the particular class? Are attributes instead numerical values, e.g. number of legs in animals? It would be good to state any assumptions. \n\nF) In Equations 6 and 7, L^{soft}(x) is a function of x, but x does not appear on the right hand side of those equations.\n\nG) The analysis of the effect of \\tau is not too insightful - this behavior is what is expected of a temperature parameter. It would have been more useful in my opinion to use that space to discuss the effect of \\alpha, which balanced L_soft and L_hard in the overall objective.\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "Summary of the paper: In this paper, the authors proposed a simple method for GZSL through the inductive transfer of attribute semantics through training with soft-labeling. Experimentally the proposed method appears to achieve state-of-art performance on a widely used benchmark.\n\nKey Contributions:\nThe paper demonstrate that SoA performance is achievable by carefully shaping a loss function that encourages learning the discriminator and transferring from seen to unseen classes.\n\nComments:\nThe proposed method is simple both conceptually and in implementation, yet achieves SoA performance. This is a pleasantly surprising result as this method deviates from the current landscape of multi-part models. I have read (but have not run) the code attached to the submission and the results do seem reproducible.\nThe model used by the authors is similar to Romera-Paredes (2015), but is used on GZSL rather than ZSL and trained differently. Additionally, soft-labeling on GZSL has been previously explored by Wu et al (2018). I would like to see the authors highlight the connections and differences to these prior works.\nThe usefulness of the equations in section 3 appears marginal. The rather lengthy exposition distracts away from a simple method that can be presented more elegantly. However, this is not a major concern.\nWith the simplicity of the proposed methods, there are missed opportunities in exploring how visual features are mapped into attributes. Due to the unusual nature of the proposed method, visualizations will make the results more convincing.\nMy main concern with the paper is the poor writing quality. Grammatical errors and ill-drawn sentences are distracting readers away from the ideas being presented. Some variables in equations are used before their definition. The paper is not suited for publication in its current state.\n\nConclusion:\nI find this paper difficult to judge in the dimension of impactfulness as it presents an effective yet simple method that is of wide interest to the ZSL community, but the lack of any theoretical analysis or methodological contributions renders the paper quite dry. The writing also requires improvement to reach publication quality. I am willing to raise the rating depending on further iterations of the manuscript.\n\nReferences:\nBernardino Romera-Paredes, Philip H. S. Torr (2015) An embarrassingly simple approach to zero-shot learning. https://arxiv.org/pdf/1806.08503.pdf\n\nFan Wu, Kai Tian, Jihong Guan, Shuigeng Zhou. (2018) Global Semantic Consistency for Zero-Shot Learning. https://arxiv.org/pdf/1806.08503.pdf\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The paper addresses the problem of generalized zero shot learning, a multi-class classification problem in which the test example can be from a class that is unseen before as well as from a seen class. Side information about labels is given in terms of attributes. The proposed model, augments training labels, with imputed soft labels for unseen classes.  The label imputation for each label takes into account the similarity between the unseen label and seen labels in terms of attributes. Then a classification model on the new label set is learned, with a multilayer perceptron. The model is evaluated on five ZSL datasets and the results are in favor of the paper for some of the experiments.\n\nClarity and soundness:\nI find this paper hard to read, mainly since the specifics of the method is Section 3.2 and 3.3 are not motivated or justified enough.\nIn particular: seems like the imputed value of a soft label in Equation (1) takes into account only the closeness of the unseen label to seen labels (in terms of attributes), but not that if the specific seen label is active or inactive for the data point. How is that reasonable or justified? \n\nSignificance:\n- It is not that either the idea or technicalities of the method, or the experimental evaluation reveal a strong significance of the method. \n -About the problem setting, the attributes for unseen labels must be present at training time. A more general and useful setting, can be one in which such attributes for unseen classes can become available only later at the inference time. \n\n\nDecision: Reject. \n\nMinor:\n\n- It is not clear what \\bar{y}_k^s is  in (6). Is it the same as z_k^s?\n- What norm of matrix W is used in the last term of (3)?\n- It is useful for reader to state early on in the paper that the classification setting considered in the paper is multiclass as opposed to multilabel. Currently it takes until training strategy in Section 3 for this to be clearly mentioned.\n- \"It is not difficult to see that this lack of training samples biases the learning process towards seen classes only\".  Is this with the assumption that unseen labels are set to zero at training time? \n- \"architecture, We\" "
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Summary:\nThis paper proposes an interesting generalized zero-shot learning (GZSL) method called soft-labeled ZSL (SZSL) which trains the network with seen data over both seen and unseen class labels via soft-labeling. The network should output probabilities of samples on both seen and unseen classes during training when only seen class samples are available: for seen classes, the target probabilities are easy to get based on the ground-truth labels; for unseen classes, their desired outputs can be estimated based on the semantic similarities between seen and unseen classes (by means of attributes). They rescale the probabilities on seen and unseen classes to make them sum to 1. Overall, the idea is simple and easy to implement. Experiments on conventional ZSL benchmark datasets are conducted and the results demonstrate that SZSL can obtain on-par or better results. Further analysis on the choice of some hyperparameters is also performed.\n\n+Strengths:\n1. The paper is well organized and easy to understand with very few grammar or spelling mistakes. The method is very clearly explained with the companion of the illustration figure. Besides, the paper provides a very thorough review of the existing works in the introduction part (both classic and latest methods).\n2. The proposed method is simple but effective, which performs better or on-par with SOTAs on ZSL benchmark datasets under the evaluation of the harmonic average of accuracies.\n\n-Weaknesses:\n1. While the framework is relatively simple, there are too many hyperparameters and the selection of gamma and lambda are not mentioned in the implementation detail. How about the sensitivity of the framework to the hyperparameters? Which combination produces the best results in Table 2 since so many combinations exists.\n2. The hyperparameter q, which denotes the sum of probabilities attributed to unseen classes, is an important factor in the proposed method. The author showed the accuracy curves under different q in Figure 2, and the optimal performance is obtained with much different q on the 4 datasets. First, did the author choose the optimal q for different datasets when reporting the results in Table 2? I was wondering for a new GZSL task in the real world (we know the labels of unseen classes), we cannot do a grid search over q beforehand. In this scenario, how to choose the q based on experience from the authors?\n3. The proposed approach has some limitations in terms of novelty. It seems that the proposed approach can only perform GZSL task. Can it apply to ZSL? If yes, how to adapt to the ZSL task? The novelty of the paper mainly lies in assigning soft labels to the training samples by class similarities. The similar idea has been studied in some previous approaches [r1][r2]. Such relevant works should be discussed and compared with the proposed method.\n[r1] Sung et al. Learning to compare: Relation network for few-shot learning. In IEEE CVPR 2018.\n[r2] Jiang et al. Transferable Contrastive Network for Generalized Zero-Shot Learning. In IEEE ICCV 2019.\n4. In Eq. (3), there are 2 regularization terms which is not very common. Is it a typo or for some special consideration? Please clarify.\n5. On AwA1, CUB and SUN, the performance of SZSL via the harmonic average of accuracies is lower than COSMO and CRnet. It is not a problem since COSMO and CRnet may be complex compared to SZSL as illustrated by the paper. I was wondering if quantitative comparisons of the complexity (such as the number of weights) can be given.\n6. In Sec.4.3, the paper offers some explanation why the accuracy curves show different properties on different datasets. The blue dotted curves (accuracy on seen classes) seems to drop monotonically on AwA1 and aPY, but first increase and then decrease on CUB and SUN. Further analysis should be done.\n7. In Eq. (2), is the similarity score calculated via cosine similarity?\n\nMinor issues:\n1. Typo: pairwsie -> pairwise (2nd paragraph in 4.3).\n2. The spacing between items in the references should be larger for better view.\n3. In A.1 Line 3, ZSL/GZSl -> ZSL/GZSL\n\n\n"
        }
    ]
}