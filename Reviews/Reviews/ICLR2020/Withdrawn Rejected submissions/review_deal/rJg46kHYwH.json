{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper presents an interesting method for creating adversarial examples using a GAN.  Reviewers are concerned that ImageNet Results, while successfully evading a classifier, do not appear to be natural images.  Furthermore, the attacks are demonstrated on fairly weak baseline classifiers that are known to be easily broken.  They attack Resnet50 (without adv training), for which Lp-bounded attacks empirically seem to produce more convincing images.  For MNIST, they attack Wong and Kolter’s \"certifiable\" defense, which is empirically much weaker than an adversarially trained network, and also weaker than more recent certifiable baselines.\n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "======== update ========\nI have read the authors' response and it has addressed most of my concerns. I am glad to see the authors' experiments on online adversarial training. \n\nHowever, there is one additional concern that I didn't realize previously. Currently the performance of adversarial training is measured in \"success rates\". However it seems to me this success rates were not computed using human evaluation (since the authors claim once the classifier is finished training, the attack success rate can be larger than 99%). I would have changed my score to 8 if either 1) some adversarial images from the generator were included after finishing adversarial training or 2) success rates using human evaluation is reported. Unfortunately, I only realized this after the author rebuttal period, and the authors didn't have the chance to address this.\n\nThat being said, I feel this paper still presents interesting contribution to the field. I am still largely in favor of the acceptance of this paper, and will remain my rating of 6 for now. If this paper gets accepted, I strongly encourage the authors to address the concern I mentioned above in their camera ready.\n\n\n======== original reviews ========\n\nThis paper proposes a novel method on generating unrestricted adversarial examples by finetuning GANs. The authors have conducted comprehensive experiments on evaluating the advantages of their approach. They demonstrated that their attack is harder to mitigate using adversarial training, produces unrestricted adversarial examples faster than existing methods, and can generate some unrestricted adversarial examples for complex high-dimensional datasets such as ImageNet.\n\nI feel although the approach is straightforward, the authors have done a good job in motivating the method and have demonstrated its advantages via a good cohort of experiments. I like how the authors motivated finetuning in section 3.2, and I am glad that the authors have conducted ablative experiments to support their arguments in section 4.4. The experiments on adversarial training are especially interesting, since previous work hasn't considered this straightforward defense against unrestricted adversarial attacks. I am also glad that the authors can generate unrestricted adversarial examples for data as complicated as ImageNet images using the latent technique in GANs. Although still not perfect, some of the unrestricted adversarial examples on ImageNet are surprisingly good to the sense that they may be used as practical attacks.\n\nThe writing is great, and it is a pleasure to read this paper. \n\nI do have some suggestions and questions for further improvement of the paper, and I strongly recommend the authors to address those before publication.\n\n- Section 3 is lacking an explicit form of the combined objective function. Currently some loss functions such as $l_ordinary$, $l_targeted$, $l_d$ and $l_finetune$ are only defined in Figure 1 but not in the main text. It is not clear their explicit mathematical form.\n\n- In section 3.2, it is better to also mention the ablative study you did later in section 4.4. \n\n- In section 4.1, the authors showed nearest neighbors to some of the unrestricted adversarial examples they generated. It is more convincing to have some quantitive results of that. For example, what is the average minimum distance to training data for a group of 10000 unrestricted adversarial examples? In addition, what is the distance function used in computing nearest neighbors? Did you use Euclidean distance? If so, it would be better to also have results using distances computed in the feature space of a pre-trained convolutional network.\n\n- In section 4.2, the adversarial training was done by alternating two phases of training rounds. I am wondering whether this makes the classifier harder to adapt to the newly generated unrestricted adversarial examples? Can you use some procedure more similar to traditional adversarial training, i.e., the attacker and the classifier are learned together at each step? \n\n- Song et al. require 100-500 iterations to generate an adversarial example, whereas your approach only need one iteration. Why is your approach 400 to 2000x more efficient? What is the additional reason that speeds up your approach?\n\n- In section 4.5 line 1, the word \"replies\" was repeated twice.\n\n\n\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper presents a GAN architecture that generates realistic adversarial\ninputs that fool a targeted classifier.  Adversarial inputs are unrestricted:\nthey may be any realistic images that humans will often classify as real\nexamples of the intended class, whereas the target model misclassifies them.\nThe novelty is that they finetune the generator itself during training, the\nmethod can be applied to a variety of GAN architectures, and the method is fast.\n\nTricks used to successfully train the GAN are clearly described, and the\nexperimental evaluation was of good scope, covering a good selection of\nexperiments.  I particularly enjoyed the short Section 4.2 and Fig 7a+b, where\nthey show that a local defense can always be fooled somewhere else along\nthe input manifold of that class.\n\nWhile the modifications to existing solutions may at first seem minor, they\nhave significant impact in applicability, effectiveness and speed of generating\nunrestricted adversarial images.  So I think this paper can be accepted.\n\nI had a bit of avoidable confusion in the introductory sections.  Figure 1\ndescribing the GAN is never referred to.  It includes components not exactly\nagreeing with my naive expectations from surrounding text.  Are any Fig. 1 features\noptional?  It would help to highlight the novel elements in Fig. 1.  Or does\nFig.1 correspond perhaps to the combined GAN elements in Section 4 (\"In our\nexperiments, we combine three ...\").  My uncertainty was really relieved only\nby the time I got to Related Work and Appendix E :(\n\nThe main claims seemed well supported by experiments, apart from claim 3\n(applicability to \"any\" checkpointed GAN codebase). Might the scope of their\napproach also be clarified by clearly identifying required and optional GAN\ncomponents in Fig. 1?\n\n---- misc comments ----\nSome sentences were long and difficult to parse:\n- 4.1: \"Our method generates..., else ....\"  Perhaps make the else clause a second sentence.\n- 4.2: \"Image quality as measured ....\" length and references made this difficult to read. Can you\nrewrite as separate shorter sentences?\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #3",
            "review": "The paper proposes using GANs to generate unrestricted adversarial examples. They seek to generate examples that are adversarial for a specific classifier, and they do so by using class-conditional GANs and a fine-tuning loss. The fine-tuning loss consists of both the ordinary GAN loss (to fool the discriminator) as well as an adversarial loss (which rewards the GAN for generating examples misclassified by the specific classifier). The authors perform various experiments on their generated examples to check for realism and how adversarial the generated images are.\n\nI would reject this paper for two key reasons. First, I feel that the contributions are not significant enough (in comparison to the prior work of Song et. al). Second, I feel that some of the methods (and some of the writing) are not too principled.\n\nIn my opinion, unrestricted adversarial examples are significant if they can be made to be realistic. If our current deep learning models often mislabeled very realistic images, that would properly expose a big failure mode of our current models. However, if our machine learning models perform poorly on images that look fake/generated 40% of the time (which is what the authors state) and don’t look too realistic to humans, it is less worrying.\n\nIn comparison to Song et. al, the authors state that their methods result in very similar results in terms of realism and how adversarial their images are (arguably, Song et. al actually produces better results in terms of being adversarial). In my opinion, the authors’ claimed improvements are not significant enough, because I think realism should be the primary metric to evaluate this field. Improving speed of generation is nice, and being able to bypass a simple adversarial training procedure is interesting but not significant unless this insight is expanded upon. The results on MNIST in Fig. 5 and Fig. 6 are not too convincing, as simpler attacks that generate (arguably) more realistic images like translations and rotations [1] or L1/L2 attacks [2] (since the networks are trained for L_inf robustness) can also degrade accuracy. Finally, I can also think of another reasonable baseline that I would have liked to see the authors compare their method against. Because the authors want to attack a specific network, they could have (1) generated realistic images using a pre-trained GAN (2) used a norm-bounded attack on the specific classifier and the generated GAN images. These images could be even more realistic if the norm-bound of the attack is fairly small, and would still be able to attack specific classifiers.\n\nFinally, I am confused by the comparison to a not-fine-tuned GAN in Fig. 14/Fig. 15 and would appreciate a clarification so that I can understand the results. For example, what does it mean for intended true label = 9, target label = 0 to have 90% success in Fig. 15? Does this mean that when you try to generate a 9 with the GAN, the classifier misclassifies it as a 0 90% of the time? In particular, I’m struggling to understand what the target label is for the case of the not-fine-tuned GAN.\n\nSecondly, I feel that there are many instances in the paper where the methods used are not explained in a principled way. For example, one of the key parts of this work is the fine-tuning loss function. Why does the loss function involve multiplying the ordinary GAN loss (with some additional transformation applied to it which seems unnecessary) with the adversarial loss? It seems most reasonable add the adversarial loss and the ordinary GAN loss (without the additional transformation). Is the stochastic loss selection procedure necessary? If all these peculiarities of the method are necessary, it seems that the success of this method is quite brittle.\n\nAdditional feedback:\n\n- In the intro, I think citing [3] in addition to Xu et. al is more appropriate.\n- You should refer to Figure 1 somewhere in the text of your work\n- In section 3.2, you can use “cosine similarity” to describe what you are doing faster.\n- When you talk about “global optima of realistic adversarial examples” and “local optimal of unrealistic adversarial examples,” it sounds weird. I would try to reword this because I don’t think you are trying to make a precise mathematical statement but it sounds like one when you write it this way.\n- In Table 1, I would format the numbers better to be vertically aligned\n- You should provide a citation for MixTrain on page 5\n\n[1] https://arxiv.org/abs/1712.02779,\n[2] https://arxiv.org/abs/1905.01034\n[3] https://arxiv.org/abs/1802.00420",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        }
    ]
}