{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The authors propose a modification of a CNN architecture where an LSTM processes the activations of each layer to provide a per-layer attention mask, that is subsequently applied to the activations before passing them to the next CNN layer.\n\nWhile the idea might have some merit, the paper in its current state is clearly not ready for acceptance at this conference. The writing must be improved. The paper fails to convey a clear description of the idea and of the experimental results, to the point it is difficult to evaluate the proposed method. Please have your submission proof-read for English style and grammar issues.\n \nDetailed feedback:\n\n1) At the end of Sec 2 the authors state that there are only a few works to exploit RNN to directly enhance the performance on vision tasks that don’t require sequential decisions. This is not accurate, in fact there are many examples in the literature, such as:\n \n     a) CNN-RNN: A Unified Framework for Multi-label Image Classification by Wang, et Al.\n     b) ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks by Visin, et Al.\n     c) Regional Latent Semantic Dependencies by Zhang, et Al.\n \n2) The content of the “Context: simplified representation of feature volume” section is obscure to me. What are max and average pooling applied to? Each layer? The last? Does the length of this “context” representation depend on the architecture or is it fixed? How is this context used?\n \n3) Model complexity: the manuscript reports that the model complexity is independent of the network depth. This seems inaccurate though: to my best understanding some of the implementations of the context function would indeed require additional per-layer capacity. Also, please also include considerations on the increase in time complexity.\n\n4) The authors find that their IA+RLA variant works worse than the base RLA model alone, but leave further investigation as future work. I find this (and the authors’ intuition on why this happens) unsatisfactory. Unless the IA+RLA variant is properly explored in the experimental section, it should be removed entirely from the paper. There is no point in introducing a variant that in my opinion had no particular merit in the first place, works worse than the base model and is not explored in depth in the experiments.\n\n5) Introduction: the sentence “[deep neural networks] successes have been limited to domains where large amounts of labeled data are available” is incorrect. Indeed, neural networks have been used successfully in many domains where labelled data is scarce, such as the medical images domain for example. Please remove the sentence.\n\n\nMinor:\n- The caption of Figure 1 is not sufficient to understand the figure, nor the model. Please improve it.\n- The experiment section should start with the main experiments and end with the ablation study, rather than the opposite. Also, the datasets should be introduced before the experiments on such datasets, not after.\n-  Please use bold for all the experiments in a reasonable range (e.g., 0.5% or 1%) from the best in the tables. A 0.1% difference (MS Coco experiment) is statistically insignificant, and should be acknowledged as such."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper (Recurrent Layer Attention Network) describes a method for aggregating information over layers, using a combination of RNNs \"controlling\" the scaling of feature volumes from CNN activations. The experiments and ablation studies in this paper are detailed, and show the workings of the proposed mechanism. Given comments on the ICLR submission page, codifying the relationship between DIANet and this work inside the paper itself would be beneficial. While the baselines chosen are sensible, it would be of great interest to see this method applied to a nearer-to-SOTA architecture (likely a ResNet variant) as well, given the plethora of open source implementations to choose from for the benchmarks, and the direct applicability of the proposed method. The same holds for the instance segmentation experiments, although I do understand the cost of running experiments in that domain - the proposed method should be easily applied to *any* segmentation architecture utilizing a ResNet trunk. \"Recurrent Layer Attention network achieves significant performance enhancement\" seems a bit of an over-reach given the existing experiments, the more modest claims in the introduction \"RLA network achieves similar or superior results\" make more sense. \n\nThe primary issue with this paper is flawed grammar and generally difficult-to-read \"flow\" of the writing itself. Rewriting with a careful eye to these problems has potential to raise my score - as it stands I cannot recommend the paper for acceptance, even though the core work, experiments, and architecture all seem promising to me. More work on the writing portion of this paper would greatly improve things, along with experiments pushing the edge of performance based on recent open source codebases."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes an attention mechanism to improve the performance of convolutional networks. Rather than the more common purely layer-local attention from Transformer-style models, the authors here use a separate parallel stream to carry attention information across the layers. This is an interesting an novel idea, but unfortunately the paper is let down by being extremely unreadable: Due to a combination of many grammatical errors, and what appears to be plain sloppy writing, it's virtually impossible to follow the main ideas. E.g. phrasing the attention stream as a a recurrent network is either extremely confusing or plainly incorrect: There is no time series input, so is the implied recurrence over processing steps, similar to Neural Turing Machines or the more recent Universal Transformer model? Are the weights even shared across layers? Or is the only thing that makes it \"recurrent\" that the structure of an LSTM cell is used? \n\nOverall, this seems like a nice extension of the ideas behind Squeeze-and-Excite networks, i.e. extracting global features from convolutional feature maps and using them for an attention-style reweighing. The results are not earth-shattering, but outperforming S&E indicates to me that the authors are onto something with the proposed attention mechanism. While the manuscript is certainly far from publishable in its current form, I would welcome to see a revised version submitted to a different forum.\n\n\nA couple of examples just from the abstract (I am not commenting on the rest of the paper): \n- \"attention module\" -> \"attention modules\"\n- \"Main goal of\" -> \"The Main goal of\"\n- ‘Recurrent Layer Attention network,’ -> ‘Recurrent Layer Attention network’,\n- \"concurrently propagating\" -> \"concurrently propagate\"\n- \"of proposed\" -> \"of the proposed\"\n- \"scaling coefficients(i.e., layer attention)\" -> \"scaling coefficients (i.e., layer attention)\"\n- \"Recurrent Layer Attention network\" -> \"The Recurrent Layer Attention network\"\nNote that these issues are just cosmetic (minor grammar issues and sloppiness), it gets far worse in the main text and many sentences are just not understandable at all to me. \n"
        }
    ]
}