{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a multitask learning-based model for unsupervised text style transfer by adding a syntactic prediction task to the existing framework of Lample et al. (ICLR 2018, 2019). The authors essentially modify the backtranslation part of the Lample training procedure by forcing the model to reconstruct both the input tokens and input syntactic labels (here, POS tags). They also propose a consistency constraint to align the generated syntactic labels with the words. Both human and automatic evaluations demonstrate the effectiveness of their model (called SAST). The overall idea of the paper (using not only surface forms of word but also syntactic information for style transfer without parallel data) is very interesting. However, the paper strikes me as a relatively minor modification over the Lample et al. framework, and the ablation study shows that removing the syntax information / consistency loss does not significantly affect the model. As such, I am a weak reject.\n\ncomments:\n- A more detailed comparison with the Lample papers would be nice. From what I've understood, it appears the same apart from (1) reconstructing POS tags along with words during autoencoding and backtranslation; (2) the encoder is pretrained here whereas Lample train it online; (3) a consistency loss is added to align words and POS tags. \n- Following the previous comment, why did the authors find it necessary to pretrain the encoder here? \n- There is a lack of detailed analysis of why the consistency constraint helps. What kind of errors are made without this constraint? Did you observe a lot of \"incompatible outputs\"? In general, explaining this section clearly with examples would make it more compelling; as is, it feels like an unnecessary addition (and the ablation study doesn't really help motivate it).\n- The authors mention \"our preliminary experiments did not show further improvements by feeding the syntactic label as input\". It would be nice to have more details about these experiments. How were the POS tag labels fed to the model? Similar results in NMT (e.g., Aharoni & Goldberg ACL 2017) have shown that this is an effective approach, so it would be useful to know if it doesn't work in style transfer. \n- Why just POS tags? There was potential here to use many different types of syntactic labels, such as linearized parses (the Aharoni & Goldberg paper above), chunked parses (Akoury et al., ACL 2019), or dependency relations, all of which seem more informative than POS tags. \n- there were minor typos / grammatical errors throughout. some examples:\n    - 4.2 last line \"we the\"    \n    - Table 4 downward arrow for perplexity\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper presents a multi-task learning approach to predict both generated tokens and syntactic labels, in an attempt to apply better style transfer to the original text. The model framework is based on previous text style transfer architectures, and the novelty of this work mostly lies in predicting (and leveraging) additional word-level syntactic labels (POS-tags) to further improve the results. The intuition makes sense to me, as a proper text style transfer can possibly benefit from a better understanding of the syntactic annotation of the text. However, it is not clear whether a well-pretrained language model like BERT or GPT-2 can already capture such subtle information, and whether this fact renders less merit of explicitly modeling the syntactic structure. In general, the paper is properly written and pedagogical. Specific comments:\n\n1. A major concern is that this paper misses an important reference: \"Structured Content Preservation for Unsupervised Text Style Transfer\", which first uses POS-tag information for text style transfer. From this point of view, using syntactic knowledge is not very novel. Although the ways to use POS tags are different, I would at least mention and discuss the differences, and possibly comparing with them. \n\n2. Is the pre-training stage of self-reconstruction important? How much performance will be affected by the pre-training part? \n\n3. Since there is no ground truth label for \\tile{x} and \\tile{l}, adding L_cons on them may bring unexpected prediction errors. For example, if prediction work \\tile{x}_k is wrong, the syntactic label \\tile{l} could also be wrong. Simply forcing the model to predict a wrong syntactic label is probably questionable. This might be the reason why the coefficient of consistency loss in Equation 12 is small (0.005) in the experiment. Additionally, the ablation study shows the impacts with adding consistency loss is relatively small. It's not very convincing that the consistency loss do help a lot. Why don't the authors place the consistency loss on \\tile{x}_b and \\tile{l}_b? "
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The authors introduce an unsupervised text style transfer model that adds syntactic features (part-of-speech tags) to the word-level output targets and input. The authors use a fairly simple neural architecture (as far as text style transfer goes), and achieve state-of-the-art style transfer, compared to many other models on automatic metrics and human evaluations. The overall idea (adding syntax information to models) is an interesting one to explore.\n\nThe empirical results and evaluation show that the overall model is strong; however, I would like to see the authors' main claim (that adding syntax is helpful) better explored. The proposed model differs from other style transfer models is a number of ways (architecture and training losses), so it's unclear how much of the gains come from syntax. The authors include one (very helpful) ablation in Table 4 (row 2) that shows that removing syntax entirely from their model hurts performance. However, I think it'd be helpful to explore the role of syntax in more depth, i.e.:\n1) Is syntax helpful because it is an additional prediction target/auxiliary loss? Or it is helpful because it is an additional input feature to the model? Or both? If it is only one or the other, it may be possible to simplify the model and/or improve other model architectures by making a simple modification. (For example, what happens when you add syntax as an auxiliary prediction target to another model?)\n2) In what kinds of cases syntax is helpful/hurtful? What sorts of errors does having syntax help with?\n3) Do other kinds of syntax-related annotations help?\n4) How do humans rate the fluency of the no-syntax version of the proposed model (i.e., the evaluation in Table 2)? It would be interesting e.g. if style and content ratings stayed the same, but fluency ratings went down (or if something else happened)\n\nA second major concern that I have is that syntax may not be helpful for style transfer when using pre-trained language models (i.e., as in XLM from from \"Cross-Lingual Language Model Pretraining\"). XLM showed that the initialization is very important for strong unsupervised results with back-translation and denoting auto-encoding/self-reconstruction. Thus, in the long run, it seems that unsupervised style transfer will move towards using pre-trained language models, which already learn a great deal about syntax implicitly. So I am concerned that the gains that the authors report now will not hold in the future. Ideally, I would like to see a comparison of style transfer results between XLM and XLM+Syntax.\n\nQuestions/Other Notes:\n* Consistency Reward in Sec. 3.3.4: I might be missing something, but why not use supervised learning to train the label predictor to predict the labels from an external syntax annotation tool (rather than using REINFORCE)? In general I found this subsection a bit confusing and hard to follow\n* It would be nice to add XLM to the related work section, as the model architecture and losses seem similar (except without syntax). Other than that, the related work is written very clearly!\n* It would be helpful to add some discussion/citations about where syntax has been shown useful in related contexts (I felt that mostly unsupervised style transfer work was discussed)"
        }
    ]
}