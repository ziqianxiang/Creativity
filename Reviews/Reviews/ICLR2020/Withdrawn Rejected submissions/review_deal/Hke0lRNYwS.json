{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposes to reintroduce bipartite attractor networks and update them using ideas from modern deep net architectures. \n\nAfter some discussions, all three reviewers felt that the paper did not meet the ICLR bar, in part because of an insufficiency of quantitative results, and in part because the extension was considered pretty straightforward and the results unsurprising, and hence it did not meet the novelty bar. I therefore recommend rejection. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "This paper presents an attractor network (AN) approach for pattern interpretation and completion. The authors propose a convolutional bipartite architecture consisting of visible (input and output) and hidden layers with weight constraints and squared and energy-based losses. To prevent vanishing/exploding gradients, temporal-difference method and leaky sigmoid activation function are exploited. Training is done by stochastic gradient descent. In experimental validation, the proposed model is able to reconstruct missing pixels in the images for bar task and supervised MNIST. And, in OMNIGLOT and CIFAR-10 experiments, the proposed approach outperforms its variants, and in super-resolution results, it outperforms the baselines. \n\nThe approach looks interesting but the motivation of using AN might not be well justified in the current presentation. Interpretation was emphasized in the paper, but it was not clear to me what induce interpretability in the model and how to interpret experimental results. Also, even though the proposed approach shows promising results in experiments, there were no baseline or only its variants as the baseline, except super-resolution task. \n\n\nDetailed comments:\nDo you have any explanation why \\lambda =1 in TD(\\lambda) works best among others?\nIt was not clear to me how the proposed approach uses recurrent networks. \nIn supervised MNIST experiments, it was claimed that it achieved the state-of-the-art results. But figure 8 might not be enough to justify the claim and could you provide more evidence? \n\n\nI am not quite familiar with this area and my understanding might be limited. But, to the best of my judgement, this paper has an interesting idea but is not yet ready to be published in ICLR. \n\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "\nThis paper studies and evaluates variations on attractor networks (which have recurrence and converge to a fixed point) trained end-to-end by SGD on image completion or super-resolution tasks. Several loss functions are discussed and compared.\n\nThe only quantitative results against published state-of-the-art results (on  the super-resolution task) show that the proposed approach wins on some measures and tasks and loses on others (in fact it wins 25% of the time, within a pool of 4 methods...). While the authors claim superior performance against other recent attractor networks, no quantitative comparison was found in the paper. There were quantitative comparisons against the \"convolutional denoising VAE\" but that model was not previously proposed by other authors to be competitive nor was it defined properly (and it is not clear from its name and the description whether this is really a VAE,  since VAEs are generative models and are not trained to denoise). Overall, I am concerned that the experimental  part of  this paper does not warrant the conclusions of superiority. That being said, I like the approach and I believe that these issues can be fixed but I'd like to see those revisions before accepting the paper.\n\nThe experiments on toy problems like the Bar Task are not necessary. Better use the place for real comparisons against published state-of-the-art benchmark baselines. Otherwise (like with your own implementation of the CD-VAE) it is plausible that the comparisons will be biased (one usually works harder on our own method than on improving someone else's architecture for our task). \n\nSimilarly, the experiments on MNIST are not very informative and you might want to push them to appendices. Also, a more interesting comparison would have been with the convolutional version, where convnets do a LOT better than 1.5% error.\n\nThe comparison against Liao et al is not fair because they don't use convolutions, which we know can make a huge difference.\n\nIn addition to the experimental results  issues, there are several places in the paper which are unclear or possibly wrong. See minor points below.\n\nOne theoretical issue which bothers me is with the \"loss\" L_{\\Delta E}. There is no guarantee that it is lower-bounded, i.e. it can diverge which would not be appropriate for sure.\n\nThe training procedure which ends up being used is TD(1) but that procedure (in the context of the setup of the paper) has not been explained at all. It is really necessary to clarify that.\n\nIt should be clarified that the parameters are tuned by gradient descent on the loss (using backprop through time), or not?\n\nMinor points\n\nThe last paragraph of section 2 seems strange to me. Many ANs are also EBMs so the critique of EBMs (as  if this did not apply to ANs) probably needs to be removed or reconsidered. Also the last sentence of the paragraph is clearly wrong,  since EBMs can easily handle inputs (by opposition to the outputs which are to be predicted or constructed), e.g. see conditional Boltzmann machines or the ANs in the style of Equilibrium Propagation - Scellier et al 2017 (which by the way should probably be mentioned in the review, along with follow-up work).\n\nThe 'bipartite' structure is not clear. What are the two \"parts\" when you have more than 2 layers? Clarify.\nNote that the proposed sweep is inefficient compared to the one proposed for Deep Boltzmann Machines (Salakhutdinov et al 2009) which alternates between odd and even layers. The latter is also more biologically plausible than a feedforward-feedback sweep (consider that the differences in update times of different layers are very different depending on their depth, with your proposed scheme).\n\nDefine CBAN.\n\nPlease clarify how many iterations are needed for convergence in the various experiments. My own experience with ANs suggests that convergence can be quite slow, which is problematic both in terms of computational cost (compared to standard feedforward methods with reasonable depth) and in terms of memory. Hence the sentence in the conclusion stating that the computational cost of CBANs is no greater than DNNs is not clearly true until you demonstrate those aspects quantitatively.\n\n\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "title": "Official Blind Review #3",
            "review": "The paper argues for the use of attractive networks (AN) for the tasks that involve learning from noisy data. Attractor networks are recurrent in nature and use energy minimization dynamics. As motivation, the authors point to studies that give evidence for the usefulness of recurrence for visual tasks. The experiments presented show that the proposed model produces better quality images than a VAE based baseline.\n\nThe main contribution of the paper appears to the scaling of the AN based architectures to large-capacity models. The authors describe several components that are aimed at improving the gradient flow information and training stability. Firstly, a procedure to constraint the convolutional filters as required for ANs is described which allows CNNs based architectures to be used. An energy-based loss function is also described.\n\nTo tackle the vanishing/exploding gradients during training the authors propose to use leaky sigmoid activation. This is applied at the pre and post-convergence stage. How critical is the leaky sigmoid? Ablation studies for the activation choice and the loss choice need to be analyzed in more detail. \n\nThe authors should make clear the novel contributions, it can be a bit hard to extract this from the descriptions of these components. The point that the proposed model achieves state of the art results for associative memory models needs to elaborate in the text. CD-VAE seems like a weak baseline for the denoising task presented. \n\nIt will be useful to get more insights into the learning dynamics of CBAN and the ablated versions apart from the denoising performance. \n\nMinor:\n\nCBAN is not defined in the text. \n\nIt might be easier to follow the main body by providing all the details of a couple of tasks/datasets in the main body and moving other tasks such as super-resolution to the appendix.\n\nUpdate after rebuttal: I agree with the other reviewers that the paper is not ready for publication. My score would be between 4 and 5 after the rebuttal. \n\nThe various techniques described to make the architecture work is useful.  But the experimental validation is still an issue as pointed by Reviewer 1 and 2. I do not quite agree with the authors that CD-VAE is a strong baseline.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}