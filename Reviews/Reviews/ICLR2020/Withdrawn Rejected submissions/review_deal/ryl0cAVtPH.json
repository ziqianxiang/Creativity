{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper addresses the question of why warm starting could result in worse generalization ability than training from scratch.  The reviewers agree that increasing the circumstances in which warm starting could be applied is of interest, in particular to reduce training time and computational resources.  However, the reviewers were unanimous in their opinion that the paper is not suitable for publication at ICLR in its current form.  Concerns included that the analysis was not sufficiently focused and the experiments too small scale.  As the analysis component of the paper was considered to be limited, the experimental results were insufficient on the balance to push the paper to an acceptable state.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The author proposes different approaches to the problem of  \"warm-started\" neural networks. Models trained from scratch on the whole dataset have better performance than \"warm-started\" models, which are trained with weights initialized from training using part of the available data.  The authors change hyperparameters like batch size and learning rate and demonstrate a tradeoff between generalization performance of the model and time, required for its training. We can also see that the choice of hyperparameters, necessary for the best performance, levels benefit in time from \"warm starting.\"\n\nThe core idea of the paper is the investigation of various possible causes of difficulty of \"warm start\" to reduce training time without damaging a generalization performance. The authors investigate the dependence of this effect with gradient values, regularization, part of \"warm started\" layers, noising weights, catastrophic forgetting, and so on. \n\nThe authors describe different problems where this research can be useful and tries to shed light on the causes of this problem, but the solution is not found. This article can be helpful for future researchers as they continue research in this direction from a warm start. However, it is hard to judge how valuable this contribution is.\n\nAlso, see a few minor comments:\n1. Maybe it should be useful to include in Table 1 results for models trained using only 50% of data.\n2. Typo: NVIDA -> NVIDIA (p. 6)\n3. It seems that the problem lies in the area of the complex learning landscape for optimization of Neural Networks as we end up in the worse local optimum if we use a warm start. Maybe one should attack the problem with this direction, as there are several papers that investigate how the loss function landscape looks like e.g. [1] \n4. It seems that the behavior becomes worse if we increase the complexity of the model. Investigation of the dependence of severity of warm start effect on e.g. number of layers in the network can be useful. Also, it can be possible to gain some theoretical insights and provable results while dealing with simpler models. [1.] H. Li, Zh. Xu et. al. Visua\n5. It might be useful to research the dependence of \"warm start\" effect from the portion of data on which the model was pre-trained. As I understand situations, where 80-90% of data are used in the first round of training, are more common.\n6. The authors provide us with graphics of accuracy on training dataset for \"warm started\" and trained from the scratch models. We can see that both models reach 100% train accuracy, and the authors state that it is impossible to spot the \"warm start\" problem on the training dataset because the metrics are equal. Maybe it could be useful to show graphics of loss function. Despite of similar 100% train accuracy, final losses might be different(\"warm start\" loss > from scratch loss). It may mean that in case of warm start we reach a local minimum, but not the best one. \n7. It might be a good idea to add to Table 2 results for not-regularized model in order to compare results with and without regularization and figure out what effect on \"warm started\" models regularization have.\n\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #2",
            "review": "\nThis paper conducted an empirical study on why training with warm starting has worse generalization ability than learning from scratch. The paper is interesting, however, it has something unclear to me, as explained below.\n\n\n1)\tThe scale and diversity of the study can be improved. Only three models and three datasets were examined, which might not be representative enough. For example, the popular Transformer model, the large-scale datasets like ImageNet, the language understanding and machine translation tasks, etc. were not included in the study. This may make the study less relevant to many important tasks and domains.\n\n2)\tThe interesting and highlight part of the paper is that it studies many different factors and aspects, including the influence of batch size, learning rate, regularization, moment, denoising, etc. However, I kind of feel that the experimental setting has some fatal problems, which makes the experimental results not convincing. The authors partitioned the dataset into two halves, using the first half for pre-training, and then use the whole datasets for continued training. Although the partitioning is random, given the limited size of the datasets, such a treatment will change the underlying distribution (frequency of the samples during training). The first half of the dataset plays a more important role in training: it was used during pre-training, and also used in training. So somehow the first half was used twice, or at least used more than once,  depending on the numbers of epochs in pre-training and training. This distribution change may make the training a little biased, and at least it is not a fair comparison with learning from scratch (the latter will not have such bias in data). So for a fair comparison, one needs to add some baselines to understand the influence of data frequency change. Without the understanding from this angle, the study may be mis-leading.\n\n\n**I read the author rebuttal, however, I still think the experiments are not comprehensive and the use of data partitions in the experiments are problematic (nothing to do with realistic or not, just for fair comparison with learning from scratch). So I would not change my assessment.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper examines the problem of warm-starting the training of neural networks. In particular, a generalization gap arises when the network is trained on the full training set from the start versus being warm-started, where the network is initially (partially) trained on a subset of the training set, then switched to the full training set. This problem is practical, as it is often preferable to train online while data is collected to make up-to-date predictions for tasks (such as in online advertising or recommendation systems), but it has been found that retraining is necessary in order to obtain optimal performance. The paper also mentions active learning, domain shift, and transfer learning as two other relevant and important problems.\n\nThe paper attempts to investigate this phenomena from a few different avenues: (1) simple experiments segmenting the training set into two different subsets, then training to completion or partial training on the first subset before switching to training on the full set; (2) looking at the gradients of warm-started models; (3) adding regularization; (4) warm-starting all layers, then training only last layer; (5) perturbing the warm-started parameters. \n\nStrengths:\n\nI believe very strongly in the practical impact of the problems presented in this paper. These indeed are challenging problems that are relevant to industry that have not been given sufficient attention in the academic literature. I appreciate the initial experimentation on this subject, and the clear demonstration of the problem through simple experiments. The paper is also well-written.\n\nWeaknesses:\n\nSome questions I had include:\n\n- Why is the Pearson correlation between parameters of the neural network a good way of measuring the correlation to their initialization?\n\n- Why is it surprising that the magnitude of the gradients of the \"new\" data is higher than at a random initialization?\n\n- Why does this phenomena occur even though the data is sampled from the same distribution? \n\n- Does this work have any relationship with work on generalization such as:\n[1] Recht, Benjamin, et al. \"Do CIFAR-10 classifiers generalize to CIFAR-10?.\" arXiv preprint arXiv:1806.00451 (2018).\n[2] Recht, Benjamin, et al. \"Do ImageNet Classifiers Generalize to ImageNet?.\" arXiv preprint arXiv:1902.10811 (2019).\netc.\n\nAlthough I like the topic of this paper, the investigation seems too preliminary at this point. There is no clear hypothesis towards answering the problems proposed in the paper. There is also no analysis, which places the burden on the numerical experiments to demonstrate something interesting, and the experiments seem sparse and small-scale. For these reasons, I am inclined to reject this paper at this time, but I strongly encourage further exploration into the topic. \n\nSome potential questions or directions could include:\n\n1. What if only a single epoch of training is used on 50% of the data? Does the gap appear in that setting? I ask because one would expect that a single epoch of training on 50% of the data, then training on new data would be equivalent to training on the full dataset from the start.\n\n2. How does this gap change with respect to the (relative) amount of new data introduced into the problem? For example, if one were to only add a single datapoint to the training set, would one still observe this behavior? Could one potentially add data more incrementally (rather than half of the training set) and potentially mitigate this drastic change in the problem?\n\n3. There are optimization algorithms specifically designed for stochastic optimization (with a fixed distribution) versus for online optimization (online gradient, Adagrad). Is the online optimization framework perhaps more \"realistic\" than the stochastic optimization framework in these streaming/warm-starting settings?"
        }
    ]
}