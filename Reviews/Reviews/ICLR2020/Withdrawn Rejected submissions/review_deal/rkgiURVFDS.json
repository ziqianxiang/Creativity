{
    "Decision": {
        "decision": "Reject",
        "comment": "The authors develop a certified defense for label-flipping attacks (where an adversary can flip labels of a small number of training set samples) based on the randomized smoothing technique developed for certified defenses to adversarial perturbations of the input. The framework applies to least-squares classifiers acting on pretrained features learned by a deep network. The authors show that the resulting framework can obtain significant improvements in certified accuracy against targeted label flipping attacks for each test example.\n\nWhile the paper makes some interesting contributions, the reviewers had the following shared concerns regarding the paper:\n1) Reality of threat model: The threat model assumes that the adversary has access to the model and all of the training data (so as to choose which labels to flip), which is very unlikely in practice. \n2) Limitation to least squares on pre-trained features: The only practical instantiation of the framework presented in the paper is on least squares classifiers acting on pre-trained features learned by a deep network.\n\nIn the rebuttal phase, the authors clarified some of the more minor concerns raised by the reviewers, but the above concerns remained.\n\nOverall, I feel that this paper is borderline - If the authors extend the applicability of the framework (for example relaxing the restriction on pre-training the deep features) and motivating the threat model more strongly, this could be an interesting paper.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary:\n\nThis paper proposes a certifiable defense against data poisoning attacks by using a randomized smoothing approach. An adversary in such a setting is permitted to flip any r labels from a dataset of size n. The smoothing procedure (stated roughly) is to train on a dataset with \"noisy\" or \"smoothed\" labels, obtained by flipping each label with some probability q. The authors obtain a lower bound on r in terms of q. Directly using this technique requires training multiple classifiers on multiple noisy datasets. To show that this method is useful, the authors study the effectiveness of this model against a classifier that performs linear regression on a pre-trained feature extractor. \n\nThe authors provide a succinct summary of current research concerning randomized smoothing. The novelty in this paper is that it considers randomized smoothing defenses for data poisoning (label flipping) attacks, as opposed to perturbation based attacks. While the paper was an enjoyable read, I recommend rejecting the paper due to the following shortcomings that:\n\n(1) The paper is essentially studying (a variant of) linear regression. This was really not obvious from the title or the abstract. It meant that as a reader, I had high expectations but was let down upon reading section 4.1.\n\n(2) What prior work exists for data poisoning attacks against linear regression or logistic regression? How does the contribution in this paper fit in that context?  Adding some discussion along these lines to the paper seems necessary.\n\n(3) There is no explanation why the authors chose the specific datasets that they studied."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary.\n\nThe paper investigates data poisoning type of attack. In such attacks, an adversary can alter/flip the labels of some of the training examples. The paper proposed a new approach towards certified robustness against this type of attack. In particular, the new classifier will output a prediction along with a certificate in which the prediction would not change if certain number of labels in the training data were flipped.\n\nThe authors use randomized smoothing on a binary linear classifier with logistic loss and deduce a radius of certification that is a function of the probability of flipping labels in the training data and the probabilistic separation p.\n\n\nMajor concerns.\n\n1) I find the application of certification to data poisoning type adversarial attacks rather limited and is not of a major interest to the verification/certification community. Certification arose as a major issue since models implemented in practice can be fooled if subjected to noise at testing time. However, the proposed certification is on the flip rate of the labels in the training data. Since this is fixed, certification in this context is not of a major interest. The only potential relevance of such a problem is upon training models in a federated approach (online learning) even then, one can argue that the portion of the data that the adversary has access to is a small portion to the complete dataset. It feels to be that certification was somehow forced into data poisoning type of adversarial attacks although they jointly do not make much sense.\n\n\n\n While I do appreciate the work from authors, I am not convinced that certification in this context makes significant sense and has interest to only small group of researchers.\n\n\n2) There are some serious limitation in the work specifically that the bounds are derived for a binary classifier. While the authors did indeed discuss the multi-class case, there are no experiments beyond the binary classification. The dataset sets where the sentiment analysis of IMDB, MNIST1/7 and the dog fish classes from ImageNet. Experiments on multi-class case is essential here for practical reasons.\n\n3) May the authors clarify some few things in the experiments for me.\n\nIf I understand Figures 1,2,3 correctly, then what the authors do is that they train networks over different number of label flipping (shown as a percentage on the top of the figure). Then for each test example, they compute the maximum radius given in Eq 10 for multiple qs. If the number of flipping in the dataset is less than r (less than the maximum radius per sample) then the sample is certified.\n\n\tQuestion. I do not understand why is an example considered certifiable when it is both correctly classified and unaffected under at least r flips? In page 7 of the experiments section, the authors say \"we plot the fraction of the test set that was both correctly classified and certified to not change under at least r flips\".\n\n\tQuestion. Can the authors comment on the expected performance of the network upon comparing certified accuracy with networks having q as probability flip rate and when the training data flip rate percentage is exactly q. Will the certified accuracy for q that matches the flip rate in the dataset be better than the ones with q that does not match the training flip rate. For instance, in Figure 1, note that the percentage in the flip rate of the training dataset ranges from 0 to 14%, however the certified accuracy was for q that ranges from 30 to 47.5. Will the certified accuracy at a given percentage of the data be the highest for q that match the flip rate? \n\n\n\tQuestion. Since the classifier is linear (since features are fixed), can the authors comment perhaps on the relation between p and q?\n\n\n\n\n\nMinor comments.\n\n1) The authors need to define what p is. The authors can correct me if I'm wrong, p is a lower bound on the separation of the probability for some class, i.e.  Prop(f(x) = 1) >= p. This is similar to Cohen et al. 19. This was not defined and come as a surprise in Eq 3.\n\n2) Moreover, in the work of Cohen et al. the smoothed classifier g is smoothed in probability which is unlike definition 1. Perhaps definition 1 fits more the framework of Lecuyer et al 19 as they showed that if an algorithm A is differentially private (probability distribution do not change much under database perturbations), the expectation over the algorithm is also differentially private.\n\n3) In the solution to the ridge regression, \\alpha is a function of the tested sample, e.g. eq 11. This can be confusing and I advise authors to consider adding a superscript \\alpha^j indicating that this is a function of the test sample j. Similarly, in the algorithm \\alpha in Eq 7 and bullet point 1.\n\n4) In example 3, the paper cited is a concurrent submission to ICLR20 and not 19."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper leverages the randomized smoothing technique, on labels of images during training, to counter the adversarial label-flipping attack. The authors proposed a strategy to build classifiers that are certifiably robust against a strong variant of label-flipping attack that can target each test example independently. The resulting classifier can make a prediction and includes certification for each test point. On a simple binary classification problem, the proposed defensive significantly improves accuracy towards the label-flipping attack. I have the following questions about this work:\n\n1. Can the authors provide training and testing CPU time and memory consumption, and compare them with the training without defense?\n\n2. Can the authors generalize the proposed adversarial defense to the multi-class classification problem?\n\n3. Can the authors also test the algorithms on the MNIST and CIFAR10 dataset with multiple different selected pairs?\n\n4. Besides randomized smoothing on the input images, recently Wang et al showed that randomize the deep nets can\nalso improve the deep nets and they gave it a nice theoretical intepretation. Here is the reference: Bao Wang, Binjie Yuan, Zuoqiang Shi, Stanley J. Osher. ResNets Ensemble via the Feynman-Kac Formalism to Improve Natural and Robust Accuracies, arXiv:1811.10745, NeurIPS, 2019\n\nOverall, this paper studies an interesting and important certified adversarial defense against label-flipping attack problems with a focus on certification on each test data, but more experimental verification is needed. Please address the above questions in rebuttal."
        }
    ]
}