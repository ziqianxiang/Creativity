{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "The authors propose a very interesting solution to training better GANs. The authors the explanation of previous work that training GANs is hard due to the proximity of real/fake samples and propose a novel way to augment the training sample by considering some fake samples as real during training. The experiment on synthetic data are illuminating and the experiments on real data show some benefit of this approach.\n\ncomments/questions\n1. I'm not very sure I fully buy the stability part (Fig 5), the objective itself is changed in such a way to be more stable (unless that is removed from the graph/objective).\n\n2. Have the authors considered the inverse, i.e. instead of considering fake examples as real, how about just throwing away those samples ?"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper tries to stability problem when training GANs. They first show theoretically that if the generator distribution is not exactly equal to the empirical real data distribution, then the optimal discriminator is not a constant $\\frac{1}{2}$. They then argue that when the generator produces samples almost identical to real samples this leads to gradient exploding or vanishing and causes mode collapse. They then show that if we penalize the discriminator to give similar scores for data points and generated points which are close to those data points, this reduces the norm of the gradient of the generator. They argue that since it's hard to find \"close\" points, a good way to achieve the same results is by using a zero-centered gradient penalty instead. Another problem the author mention is that the exploding gradient can lead to assigning more fake points to a single real points. They show that this issue can be mitigated by considering that fake datapoints (which are close to some real datapoints) are real with some probability. In practice they propose to replace some of the real samples in a batch by the generated samples with the lowest discriminator score. They show empirically that this lead to improved performance on a variety of datasets.\n\nI lean slightly towards rejecting this paper. I think the novelty in this paper is quite small, the work should be better contrasted with existing methods to stabilize GANs and the contributions made clearer. I also think proposition 2 and proposition 3 are a bit vague and I'm not convinced they solve the problem of exploding gradients.\n\nMain argument:\n- Proposition 1 is not very novel, it's already known that when the support of the distributions don't match the standard GAN objective is ill defined see [1] and Arjovsky et al (2017).\n- The discussion about why this leads to gradient exploding and vanishing is a bit vague. Several paper discuss it more clearly for example [1], and already proposed a fix. Can the author contrast better their works with [1] and WGAN ?\n- I find proposition 2 and 3 a bit vague, indeed a regularization can reduce the norm of the gradient but if move the samples even closer, wouldn't the norm of the gradient still explode. Thus the regularization is just slowing down the explosion of the gradient.\n- In the experiments how did you choose of $M_0$ and $M_1$, how does the choice of $M_0$ and $M_1$ influence the performance ? It would be nice to a have a comparative study for different values of $M_0$ and $M_1$.\n- It would be nice to run the experiments with different seeds and report the standard deviation.\n\n\nMinor comments:\n- It's not clear which methods GAN-0GP-sample,  GAN-0GP-interpolation and GAN-0GP-sample-with-our-method refers to ?\n- There is some typos in the paper.\n\nReferences:\n[1] Arjovsky et al. \"Towards principled methods for training generative adversarial networks\" arXiv (2017)"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary:\nThe paper proposes mixing real samples, which a GAN discriminator is trained on, with some fake samples. The motivation is to reduce the norm of the gradient which the generator receives from the discriminator, especially at fake samples close to real ones. The paper provides an analysis of why reducing such gradient can stabilize training and avoid mode collapse, and shows some empirical results supporting that.\n\nComments:\n - The writing and structure of the paper needs some improvement. \n       *  Some sections are too verbose, and arguments are not easy to follow.\n       *  Notation is not very clear. For example, it is not clear what's the difference between; m, M1 and M0 in Table 1\n       *  The list of contributions constitutes mainly a list of observations/conjectures. The last point is probably the main solid contribution.\n - The proposed method is not particularly interesting, and while most of the paper is dedicated to motivate it, I don't think the arguments are solid enough to convince the reader.\n - While it is great that the paper experiments with multiple datasets, including CIFAR-10, CIFAR-100 and Imagenet, the paper mainly reports a single plot for the proposed approach compared to a single baseline. It is well known that GANs are highly sensitive to choice of hyper-parameters, so the paper should emphasize more that those results are consistent across an accepted range of hyper-parameters and baselines.\n"
        }
    ]
}