{
    "Decision": {
        "decision": "Reject",
        "comment": "This was a borderline paper, but in the end two of the reviewers remain unconvinced by this paper in its current form, and the last reviewer is not willing to argue for acceptance. The first reviewer's comments were taken seriously in making a decision on this paper. As such, it is my suggestion that the authors revise the paper in its current form, and resubmit, addressing some of the first reviewers comments, such as discussion of utility of the methodology, and to improve the exposition such that less knowledgable reviewers understand the material presented better. The comments that the first reviewer makes about lack of motivation for parts of the presented methodology is reflected in the other reviewers comments, and I'm convinced that the authors can address this issue and make this a really awesome submission at a future conference.\n\nOn a different note, I think the authors should be congratulated on making their results reproducible. That is definitely something the field needs to see more of.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #2",
            "review": "This paper builds upon recent work on detecting and correcting for label shift.\nThey explore both the BBSE algorithm analyzed in Detecting and Correcting for Label Shift (2018)\nand another approach based on EM where the predictive posteriors and test set label distributions\nare iteratively computed, each an update based on the estimate of the other.\n\nCrucially, while the former method requires only that the confusion matrix be invertible,\nthe  latter method only appears valid under strong assumptions including the calibration fo the classifier.\nThus the authors propose an approach for “bias-corrected calibration”\nand shows that bias-corrected calibration can improve the performance of BBSE and EM.\nThe method is crucial for EM and with it, the results seem to show that EM,\nin the large sample (8000 examples) regime and with good initial classifiers\n (on the relatively easy CIFAR10 task with a strong baseline)\nthat EM outperforms BBSE.\n\nThe paper is easy to follow an the authors should also be credited for releasing \ncode anonymously with which we could reproduce their results. \n\nI have as few specific concerns/questions about the paper that I would like the authors to address: \n\n * They consider JS divergence as a metric for evaluation. But they don’t consider other metrics\n  like the error in weight estimates which is considered in most of the prior work \n * They don’t compare their results with regularizations suggested on top of BBSE, particularly Azizzadenesheli et. al. https://arxiv.org/abs/1903.09734. \n * They compare methods for particularly limited ranges of Dirichlet shift (\\alpha=0.1,1.0). \n  * What happens when the \\alpha increases to have less severe shifts? \nOptimizing ELBO with EM can lead to local convergence to the likelihood function when the likelihood is not unimodal. \n *  Is this likelihood function unimodal?  Does the EM approach converges to MLE under some appropriate initialization and assumptions? \n  \nA small presentation note: many of the papers are reporting the same metric and ought to be grouped as a large table, not as many tables. Also every table should state clearly what it is reporting in the caption, not just referring to earlier tables.\n\n=======Update\nI have read the rebuttal and appreciate that the authors took the time to establish the concavity of the likelihood function for EM. Overall this paper makes an interesting contribution in establishing the usefulness of the likelihood formulation (here optimized by EM) of label shift estimation and its apparent benefits over BBSE in some settings. I am happy to keep my score despite apparent disagreement from the other reviewers. I must say that some other reviews were disappointingly lacking in thoroughness.  \n \nThe paper still leaves open some serious questions, e.g. --- why is this bias correction heuristic so effective vis-a-vis EM and is this explained by its performance at the calibration task itself? The original temperature scaling paper reported a similar heuristic yet didn't see such a benefit wrt their metrics. Why is it so useful here? \n\nStill, while this paper can be improved in some key ways, it does make an interesting contribution.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This work conducted exhaustive experiments for a label shift problem with EM and BBSE over CIFAR10/100 and retinopathy detection datasets. In addition, as for model calibration, they also considered temperature and vector scaling and introduced intermediaries between those, namely, NBVS and TBVS.\n\nOverall, the paper is well organized and provides rigorous experiments with conclusions based on the empirical observations. As stated in the paper, the main contribution of this work is to explore the impact of calibration. \n\nHowever, it is still doubtful whether those empirical results can be generalized as there is no analytical and thoughtful discussions. Further, the label shift was simulated by means of dirichlet shift on both datasets. It would be great to apply the method on real cases. "
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "The paper considers the label shift problem and investigates the role of the calibration in improving the results of two different domain adaptation solutions including EM and BBSE. They show with having better estimation of conditional distribution in the source domain the final label distribution obtained by EM and BBSE in the target domain is more accurate. They conduct the experiments on three different datasets i.e. CIFAR10, CIFAR100 and Kaggel Diabetic Retinopathy as the proof of concept.\n\nOverall, I think the paper should be rejected as it suffers from not high enough level of novelty in proposed method. The paper used different variants of famous Platt Scaling family approaches as the calibration methods to improve the estimation of conditional distribution of the training data $p(y|x)$ and showed the positive influence of that on label shift approaches like EM and BBSE, which is not enough contribution. \n\nThe paper is well-written and point the interesting problem. But the way of reporting the results are not clear enough. It would be better that the accuracy is reported for the baseline, EM,  BBSE-Soft and BBSE-hard at the same table with mean and std values. The tables some how should show the impact of using the calibration to improve significantly the final label shift accuracy. But in this way of reporting the results it is not clear how big calibration can improve the final accuracy. Comparing to other baselines like RLLS [1] is also missing in this table.\n\nPointing out that ECE is not a good metric to measure the calibration is also reported before in the related research works [2]. \n\n\nReferences\n[1] Azizzadenesheli, Kamyar, et al. \"Regularized learning for domain adaptation under label shifts.\" arXiv preprint arXiv:1903.09734 (2019).\n[2] Vaicenavicius, Juozas, et al. \"Evaluating model calibration in classification.\" arXiv preprint arXiv:1902.06977 (2019).\n"
        }
    ]
}