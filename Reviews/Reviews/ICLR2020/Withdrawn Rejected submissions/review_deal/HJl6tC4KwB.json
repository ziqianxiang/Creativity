{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The paper presents a method to manipulate images using natural language input. The method uses a deep network to parse the input image, input text, and a generative adversarial network to produce the output image. The architecture is sensible, and close to prior work. The loss function is standard and almost identical to (Li et al., 2019).\n\nStrength:\n + Good results\n + Good experiments and ablation\n\nAreas of improvement:\n - Highlight contributions\n - Compare to Li et al\n\nThe paper is well written, easy to understand. The results look very good, and the model performs well in the quantitative comparison. The experiments are extensive and contain a detailed ablation of the presented method.\n\nThe paper could be stronger if the authors could highlight the contributions more. A large part of the technical section is either directly copied or slightly modified from prior work. The interesting new contributions (e.g. DCM) are only fully described in supplemental material. I'd recommend the authors to focus more on the contributions and talk about them in the main paper.\n\nSince large parts of the paper are derived from (Li et al., 2019), it would be nice to directly compare to it. I understand that Li etal do not use a reference image, but a comparison would help show the presented work improves upon the baselines."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposed a very effective approach to modify an input image according to natural language description. The method is modified based on ControlGAN with two novel modules. The 1st is Co-Attention Module (CoA) which is well-motivated and the ablation study clearly shows the effect of this module. The second is the Detail Correction Module (DCM), which is only briefly described in the main paper with the most part described in the supplementary material. The effect of DCM is briefly shown in Fig. 8. \n\nThe qualitative and quantitative results a\nre very impressive compared to SISGAN and TAGAN. In the ablation study, a few examples also show the ControlGAN backbone architecture without either CoA or DCM generates results worse than the proposed method. However, I wonder what's the \"quantitative\" comparison with ControlGAN backbone architecture without either CoA or DCM, or even both modules. \n\nIn Fig.7, why oncat., Matched and Concat., Given generate structurally very different images? Is this typically happening? In Fig. 8, without CoA means CoA is removed from both the main architecture and DCM? In yes, what is used instead of CoA?\n\nIn general, the paper is tackling a hard and interesting task and the results are very impressive. My only concern is how \"quantitative\" improvement is from CoA or DCM as compared to the backbone ControlGAN. Hence, I recommend for weak accept."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "# Summary #\nThe paper works on text-guided image manipulation. The authors proposed two modules, co-attention + detailed correlation, to better exploit the text information and maintain the semantic consistency on the synthesized image. The qualitative results look impressive.\n\n# Strength #\nThe qualitative results are quite impressive. The authors also point out the deficiencies of existing methods, suggesting future improvements.\n\n# Weakness/comments #\n1. The technical part of the paper is poorly organized/described. First, there is no general introduction to the pipeline. (The authors just refer to ControlGAN and show the overall framework in the appendix.) This makes it hard to understand the overall training objective, i.e., where/what are the generator and discriminator? There seems to be no introduction about the text features in Sect 3 and no text features are involved in the co-attention module. More importantly, for the two proposed modules, there are insufficient descriptions about what they are actually doing and why they will perform better than baseline methods.\n\n2. Co-attention mechanisms have been studied in other tasks that involve vision and language, such as visual question answering and captioning. However, the authors fail to refer to them. For example,\n\nJ. Lu et al., \"Hierarchical Question-Image Co-Attention for Visual Question Answering,\" NIPS 2016\nZ. Yu et al., \"Multi-Modal Factorized Bilinear Pooling With Co-Attention Learning for Visual Question Answering,\" ICCV 2017\n\n3. Since the main goal here is to change the attributes (or local detail) but maintain the global layout, the authors should discuss style transfer, which has very similar input (one image for content, and one image for style) and goal. The main difference is that, here, the style is given by text, not by a reference image.\n\n4. The authors should discuss the task (image manipulation) more. For example, if the task considers action attributes (from a standing horse to a running horse, or from a smiling face to a sad face), and if the proposed model can handle it.\n\n5. There is no description of the dataset. Note that, to train the model the authors need triplets of (I, S', I'), which are not directly provided in COCO and CUB. There is no description of the evaluation metric: how the similarity is computed?\n\n# For Rebuttal # \nSee the above comments.\n\nWhile the paper gives impressive qualitative results, the paper needs significant modification, especially on the technical part (e.g., Sect 3.). The current version is more like a technical report without any insights into what the proposed modules are actually doing."
        }
    ]
}