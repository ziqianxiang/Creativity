{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors investigate how to use GAN for estimating the underlying density function of images. Generator network of GAN and its derivative are used for obtaining the density of data, and the authors analyze which is the point with high probability density and which is not.\n\nThe derivation of the probability density using Generator gradient seems to be okay, but this work highly depends on the belief that GAN produces the images from the same density function as the true one. Based upon the results, the authors claim that the result is uninterpretable (ex. the high probability of “1” in MNIST). It looks for me that simply the generator is not the one that makes data same as those from the true density function. Or even if the generator is okay, the gradient of the generator may not provide the gradient of true generator. Either of these can ruin the experimental results, but it was not confirmed that any of these malfunctions did not happen. Rather, the weird results are simply assumed to be correct.\n\nThe authors can provide results using synthetic data first from known distributions and confirm that their algorithm works as expected. The result that “1” in MNIST has higher density than others could be correct, but it should be confirmed as well. For example, the variations of “1” could be less than others. Maybe the authors have derived the useful equation, but unfortunately, the paper did not reach the threshold because of the presentation of results.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors try to estimate the probability distribution of the image with the help of GAN. Intriguingly, they develop a proper approximation to the PDFs in the latent space and train a simple network to predict the distribution. The experiments seem enough for the sanity check. \n\nThe word \"density\" seems a little bit unfamiliar to me. This concept adds to the difficulty to read the paper fluently. For example, \"We perform sanity checks to ensure that GANs do indeed produce reasonable densities on images.\" How can a GAN \"produce\" density? I'd better say the generated images of a GAN conform to a specific distribution. There are lots of places in the paper should be carefully revised.\n\nI have some disagreements on the theoretical analysis part in Section 3. The authors made a strong assumption that G is injective and hence has the following to come. While I disagree that G is injective, a very straightforward phenomenon to support my claim is the common mode collapse that happens nearly in all GAN models. Therefore, training the regressor could be pretty much problematic.\n\nMoreover, I'm pretty curious about whether it is possible to measure the difference between some similar datasets (for example, some classes are the same). The experiment proves that data points in MNIST can be outliers for CIFAR, but MINIST and CIFAR basically are very different from each other. How about data points in two very similar datasets such as CIFAR and ImageNet? Should their distribution be the same or similar? I'll pretty grateful if the authors could give me some insights into this question.\n\nUp to the deadline of the review process, I haven't seen the anonymous github repository and the supplementary materials. Did I miss something here?\n\nConsidering the drawbacks mentioned above, I tend to reject this paper because it has theoretical flaws."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper attempts to estimate densities corresponding to GAN models, and then study those density estimates to understand properties of natural image distributions. Understanding high-dimensional image distributions, and also understanding the distributions that GANs learn, is clearly an important topic worthy of study.\n\nUnfortunately, the actual quantity that the paper estimates is unclear to me, but it is certainly *not* the probability density.\n\nFirst of all, it was unambiguously proved by Arjovsky and Bottou (ICLR 2017, \"Towards Principled Methods for Training Generative Adversarial Networks\") that typical GANs, including presumably the ones studied here, *do not admit a probability density at all* with respect to Lebesgue measure. This is not merely a technicality: you're trying to estimate something which *does not exist*.\n\nBut, okay, per Arjovsky and Bottou the GAN distribution is supported on a (probably finite) union of manifolds (of possibly varying dimension). Then there exists a Radon-Nikodym derivative of the distribution with respect to some measure on its support. One has to be careful in picking the base measure for a union of manifolds of different dimensions, but suppose we make some reasonable choice, and call that the probability \"density.\"\n\nThe quantity P_d(x) given by (2) is still *not that density,* because G is *not* injective for typical GAN generators. Empirically, if you try to numerically invert x = G(z), you can find distinct z yielding essentially the same output image (e.g. https://arxiv.org/abs/1802.05701).\n\nBut, okay, pretend that P_d is the true density on the output \"manifold\" (which may not actually be a single manifold, but ignore that). As you note, points off the manifold should have density zero. Now, you don't say exactly how the regressor is trained, but presumably it's something like mean-squared error of log-\"density\" values. (Incidentally, this is something that should be in an in-PDF appendix as well as in the git repository you decided not to anonymize for submission.) Because you never give the regressor any examples with density zero, its behavior in extrapolating away from the output \"manifold\" is going to be essentially *arbitrary* based on the regressor's implicit priors.\n\nThis makes experiments like Figure 4 (right) and Figure 5 (left) *extremely* difficult to interpret. They're based on the arbitrary extrapolation behavior of a network whose training procedure we don't even know; even if we did, they presumably have very little connection to P_d, which isn't very connected to the true density on the output \"manifold,\" which isn't even *defined* for these out-of-manifold images.\n\nThe proposal of section 6 to instead estimate densities based on densities in Z is also problematic. It's easiest to see this if, instead of taking the latents to be distributed as Gaussians, you take the similarly or perhaps even more popular choice of taking the latent distribution to be uniform over a hypercube. Then literally every point would have the same \"density\" under your model. Images which the GAN cannot produce and hence should have a density of 0 will still probably be assigned that same density value by the density regressor. So your procedure would do...literally nothing. Doing it on a Gaussian prior means the outcomes are not uniform like this, but there is no reason to think they are at all meaningful: the same factors which alter the uniform latent density to the non-uniform output \"density\" will alter the Gaussian latent density in the same way.\n\n----------\n\nIgnoring all of this, the paper has I think two main takeaways:\n\n1. Likelihoods of high-dimensional distributions are unintuitive to interpret; \"simple\" images will probably show up as far more likely than \"complex\" ones, even if the \"complex\" ones are far more \"typical.\"\n\n2. Relatedly, generative models on images can assign very high likelihoods to out-of-sample images.\n\nBoth points are I think probably true, even if I don't at all trust this paper's demonstration of them, and very important to bear in mind when thinking about likelihoods for image-type distributions.\n\nPoint 1 is I think reasonable to expect by analogy to the high-dimensional Gaussian you brought up. I'm not aware of previous work explicitly highlighting it, but even if you had convincingly demonstrated it, I wouldn't have been super surprised.\n\nFor point 2, I again don't really trust your demonstration. I do trust it from Nalisnick et al. (2018)'s evaluation on models that actually have densities that can be straightforwardly-evaluated. But although you cite that paper as an arXiv submission, it was in fact published at ICLR 2019, which makes it a hard sell to call \"parallel to our work\" in an ICLR 2020 submission."
        }
    ]
}