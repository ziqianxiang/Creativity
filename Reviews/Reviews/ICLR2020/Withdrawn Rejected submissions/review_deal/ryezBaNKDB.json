{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors propose a new \"higher-order\" convolution operation in which the parameters of a convolution are learned by a second function (e.g. another convolution operation). The idea is interesting and certainly more general than a standard convolution (if the second function is the identity we are back to a standard convolution). The proposal is motivated in the context of action recognition, where the authors argue that standard convolutions cannot handle the classification of actions that differ by small details (e.g. push an object from right to left vs. push an object from left to right). The proposal was evaluated on four data sets and attained results close or above the state-of-the-art.\n\nAs I mentioned in the summary, I like the overall idea, but I am not fully convinced that other models (such as 3D convolutions) would fail in these tasks. For instance, it would be interesting to see what is the total number of parameters of the models utilised in the experiments. On the other hand, is it nice to see that the proposal attains comparable results even when just trained with RGB inputs (that might mean that the model handles temporal information better).\n\nPros:\n- Simple and interesting idea that increases the representational power of convolutional networks.\n- The method was evaluated on four data sets and attains a reasonable performance.\n\nCons:\n- Experiments do not include an analysis of the performance of the model in the hard to differentiate actions (which is the main motivation for the proposal).\n- Writing of the method is quite convoluted and hard to follow.\n\nQuestions/Issues:\n- What is the number of parameters of the models utilised in the experiments?\n- From the abstract, the authors assume the readers know what they mean by \"higher-order\". However, it is only clear much later in the paper.\n- The authors claim that the architecture is necessary for recognition of actions that differ by small details. It would be nice to see an ablation study comparing the accuracy of the proposed on some of these hard to classify actions against one of the related methods.\n- Captions of tables could be more descriptive."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The paper presents a method for video activity understanding. The paper needs revisions to the method section as it is currently unclear. At a high level, it seems that the proposed method is using weights to generate weights that are used on the input. This is what is meant by \"higher-order.\" The method is evaluated on many datasets, however the state-of-the-art comparisons are missing several works, and the claim of outperforming state-of-the-art is not correct.\n\n\nComments:\n\nThe claim in the introduction that \"It would be difficult for conventional convolutions which recognize fixed patterns...\" does not seem to be supported with any evidence. I'm not sure that this is true, and no experimental or theoretical support is given for this claim.\n\nThe writing of Section 3 (Method) could use significant revision. It is quite hard to follow currently, and I'm not sure I fully understand all the details the method. At a high level, it seems to use weights to generate weights that are applied to the video input.\n\nIf my understanding is correct, then this paper is missing related works on neural networks generating weights (currently none are cited). \n\nI'm not sure that calling this second-order, or higher-order, is an appropriate name. Higher/second-order has a well-defined meaning in mathematics, and implies that this method is taking second-order derivatives, which from the writing, this method does not seem to be doing.\n\nThere is no discussion on the runtime or computational cost of this approach. It seems that it would be more expensive in inference and training. How much more expensive is the proposed H-block compared to a standard convolutional layer?\n\nExperiments:\nTable 2, something-something, is missing state-of-the-art results:\n\"Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification\", ECCV'18 (48.2)\n\"MARS: Motion-Augmented RGB Stream for Action Recognition\", CVPR'19 (53.0)\n\nTable 3, Kinetics-400, is missing some state-of-the-art numbers:\n\"Representation Flow for Action Recognition\", CVPR'19 (77.9)\n\n\nTable 5, Charades:\n\"Evolving Space-Time Neural Architectures for Videos\", arXiv: 1811.10636 (38.1)\nSlowFast: 45.2\n\n\n\nMinor comments:\n\n\"It is more complicated to classify pull and push since it is an XOR operation on the relative positions of the hand and the object resulting from the handâ€™s movements.\" What is meant by \"an XOR operation\"? I think there is a better way to describe that push/pull depend on the motion direction and hand/object relative location.\n\nThe notation is not standard, e.g., conv is f(X;theta). A more standard form would be X * theta. \n\nx_p'' is undefined (middle of page 2)\n\nPage 4: \"d by the simple convolutional model of . \" Of what?\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper attempts to tackle an obvious flaw in current video understanding approaches, spatio-temporal reasoning. In achieving that, it proposes a new 3D convolutional block that convolves the input with its context. This two-iterations of convolutions are referred to as higher-order reasoning. In doing this, the proposal is based on the assumption that relevant context is present around the object (in line with traditional context in images). This is in contrast with recent approaches that search for relevant context anywhere in the image (e.g. video action transformer, 2019 and non-local neural networks, 2018). \n\nThe paper has two critical issues for concern:\nThe first is lacking to properly place the paper in context with state-of-the-art or alternative approaches. The related work is surprisingly short - either missing or briefly mentioning prior work without a discussion of similarities/differences. Importantly:\n*) current approaches that use non-neighbouring context (Video Action Transformer, 2019) without a CRF are not noted. Older approaches (non-local neural network) are noted in related work under \"CRF-based\" but the authors do not expand on why these methods are not suitable to solve the problem at hand, or the fundamental distinctions between such approaches.\n*) Other approaches that attempt exactly the same purpose of higher-order context convolutions (Attend and Interact: Higher-Order Object Interactions for Video Understanding, 2018) are not mentioned altogether. This gives the incorrect impression that such higher-order context is novel and previously unexplored. In fact the paper (Attend and Interact) achieves iterative context attention through a recurrent network. Equation 4 in the noted paper seems to be very similar to the equations in the introduction of this paper.\n*) Temporal-relational reasoning paper is another paper that discusses higher-order dependencies, though only for temporal dependencies rather than spatio-temporal. This paper is also ignored in the related work though referred to in some of the experiments. [note: the reference has missing authors and incorrect order of authors]\n*) Another paper with, again, the same motivation is: Temporal Reasoning Graph for Activity Recognition, though this is more recent (Aug 2019) so I can understand why it had been missed. However, it would be nice to understand how the proposed H block differs from the proposed architecture in Temporal Reasoning Graph.\n\nWhile the paper offers a number of ablation and cross-dataset results, the authors do not report results on any \"unseen test\" subsets. For example, in reporting on Something-Something V2, other papers combine their validation accuracy with the test accuracy on the unseen test set. This avoids overfitting or reporting best performance on the validation set. Similarly on Charades, results on the validation set rather than the actual test set are reported. This is slightly concerning as it's easier to prove higher performance when the ground-truth is within hands. Pairing validation/test results would be the expectation for fair analysis to other published papers.\n\nIn addition to the above major concerns, the paper would benefit from better write-up. It's confusing to read at times and the same concept is repeated multiple times, with others briefly rushed over. The paper also refers multiple times to the advantages of having less parameters than alternative CNNs, but it is not clear how the number of parameters of the proposed solution differs from other state of the art.\n\nMy rating is based on the concern that it is not possible to make conclusions on the contribution of this paper, theoretically (compared to published works) or experimentally (on unseen test sets), from the submitted manuscript as is."
        }
    ]
}