{
    "Decision": "",
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "\n\n## Summary\n\nThis paper address how self-imitation learning (SIL) can guide to find better solutions in multi-agent reinforcement learning (MARL) problems. The main contribution is to extend NFSP (Heinrich & Silver 2016) by SIL (Oh et al., 2016), which gives the proposed approach named NFSIP (Nerual Fictitious Self Imitation and Play). Since the problem being targetted is a multi-agent setting, the notion of \"good\" experience is slightly modified to take both of single agent's reward and global optimization objective denoted by social welfare into account. The proposed approach achieves a significant improvement over baseline MARL approaches in three multi-agent benchmark environments.\n\n## Decision\n\nFrom this reviewer's point of view, this paper is a clear reject. The paper is not clearly written and is incompletely written in its current form, hence not enough quality to be recommended to acceptance. Moreover, experimental results and analysis are limited and not enough. It is also difficult to say the novelty and significance are strong. I think this paper's perspective of applying efficient exploration techniques in a multi-agent RL setting is interesting, and it has some advantages and promising results of improvements, but a better execution of the idea would be required.\n\n## Supporting Arguments\n\nWriting and Clarity:\n\n* In general, the paper is not straightforward to follow. The paper is not structured very well, please see more comments below (i.e. feedback for improvement).\n* Figures are not clear, and very difficult to understand; besides, no caption or detailed explanation accompanying is provided. \n* No related work.\n\nMethods and Experiments:\n\n* The use of social welfare (the objective for the entire system) seems not well-motivated and principled. The main idea is that per-agent SIL objective is used, but only when the social welfare is above some threshold. A downside is that this threshold needs to be gradually tuned as training progresses, which can be hacky and would require a non-trivial tuning. It would be more preferrable to have more principled approach that takes both of per-agent and global objective into account in a self-imitation learning like fashion. In experiments, what is the effect of thresholding at global social welfareness? How was the schedule chosen?\n* Experimental analysis are quite limited, and the paper is lacking a detailed analysis; including ablative studies, a study of individual design choices, or qualitative examples/analyses of the learned behavior, to name a few.\n* Many details of the algorithm is missing. For example, what base RL learner (actor-critic) algorithms is used? How is sampling from prioritized experience replay buffer done?\n* AC-SIL and NFSIP outperform other baselines, which would indicate that the main benefit of tackling sparse-reward environments via exploration comes from SIL objectives. However, the details of AC-SIL and COMA-SIL are missing; how these algorithms were obtained? How are they different from the full approach (NF-SIP)? Also, it is questionable that they were fairly compared.\n\n## Feedback for Improvement\n\n* The authors should address the reviewer's concern about clarity and experiments. Besides, in terms of structure and organization of the paper, a vast majority of the paper should be improved.\n* Experimental results (learning curves) are difficult to parse as all lines are drawn in achromatic colors.\n* Citation was not placed properly; e.g. Ficitious play (FP) (Heinrich & Silver 2016), which is a very direct background work, is not clearly cited. (e.g. very first paragraph of Section 3, or the method description in Section 4).\n* The paper should have conclusion. Also, abstract should be written in the abstract, not as a section.\n* All equations should have numbers and the terms/concepts used in the paper should be introduced formally.\n* Finally, the title, which sounds too broad, seems not the best choice that summarizes this work's main idea and contribution.\n\n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "In this paper, the authors propose a technique NFSIP to augment state-of-the-art Neural Fictitious Self Play (NFSP) with self-imitation learning, and show that this approach can achieve performance improvement compared to other Multi-agent RL baselines on 3 different problem domains in literature. \n\nThe key idea of this paper is applying the self-imitation loop at the end of each episode s.t. the agent will not forget learning from good experience. The idea is simple, but the experimental results show that for the tasks (e.g. V2) that require collaborations from multiple agents, the proposed approach outperforms other baselines including NFSP on both firefighting and search & rescue domains. \n\nQuestions:\n1. it seems that for simpler task V1, the result of NFSIP are very close to NFSP in Fig 3, 4. How to explain this phenomenon?\n2. why there are some baselines not having the same number of iterations in Fig 3, 4? e.g. COMA_SIL in Firefighting v2 seem only have results until 70k iterations. Also, for the task Box Pushing, NFSIP doesn't seem to have advantages?\n\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "===== Summary ===== \nThe paper proposes a new multi-agent reinforcement learning method for solving cooperative tasks that combines Neural Fictitious Self-Play (NFSP) and Self Imitation Learning (SIL). The resulting method is called Neural Fictitious Self Imitation and Play or NFSIP. NFSP and NFSIP are based on the idea of self-fictitious play where an agent selects best responses to other agents’ average strategies. In NFSP, the average strategy of other players is approximated using a neural network that learns to imitate its own past behaviour, which is a suitable substitute since the agent only plays against itself.The best response is approximated using reinforcement learning by minimizing the Q-learning loss using a separate neural network. NFSPI uses the same ideas, but adds another SIL training loop at the end of each episode that learns based only on past states and action that resulted in higher cumulative reward than expected. Additionally, NFSIP selectively trains on those samples that improved the social welfare of all the agents. Thus, the SIL training loop considers only state-action pairs that resulted in higher than expected cumulative reward and higher social welfare for all the agents. FInally, the paper presents empirical demonstrations of the performance of NFSIP compared to other popular multi-agent reinforcement learning algorithms.\n\nContributions:\n1. The paper demonstrates a successful combination of NFSP and SIL for multi agent reinforcement learning problems.\n2. The paper shows that the performance of NFSIP can be competitive to the performance of previously proposed methods. \n\n===== Decision =====\nThe paper shows an interesting combination of NFSP and SIL that could be an improvement over previously proposed multi-agent reinforcement learning algorithms. However, the algorithm is not well motivated, the main ideas are not clearly presented, and the empirical evaluations have very low significance since they are not presented alongside any error measure such as confidence intervals or standard errors. I believe the paper could reach a point where it would make a good contribution to the field, but it should not be published in its current form. Thus, I think the paper should be rejected. \n\n===== Detailed comments about decision ===== \n\n== Motivation ==\nOvercoming environments with sparse rewards seems like the wrong thing to emphasize for this algorithm. One of the main justifications for NFSIP is that the self imitation learning part is useful for learning in environments with sparse reward. However, this has little justification based on the original paper by Oh, Guo, Singh & Lee (2018). In Oh et. al. (2018) the purpose of self-imitation is to exploit past good experiences, which might help in hard exploration problems. However, the algorithm relies on the agent first finding the sources of high reward and then exploiting those experiences that lead to such sources of reward. Hence, it is possible that in environments where random policies hardly ever lead to the most reward, SIL will not help improve performance. In fact, in Oh et. al. (2018) paper demonstrates this in their Key+Apple Environment where A2C + SIL gets stuck in a suboptimal policy. \n\nI still think it is possible to motivate the algorithm by arguing about improved exploration. However, I would change the focus from sparse rewards to scarce good experiences. The problem in multi-agent reinforcement learning is that it might be possible to encounter reward, but because the proportion of such “good” experiences is so small compared to irrelevant experiences, the agents might never benefit from the “good” experiences. Thus, SIL might be a simple and effective way to overcome this issue by giving a larger weight to “good” experiences. \n\n== Clarity == \nThe paper is very difficult to read because the order in which ideas are introduced and the lack of proper definitions for key terms and notation. \n\nThese are a few examples of ideas that are introduced in an order that makes it confusing to the reader to understand the main contributions of the paper:\n\n1. Fictitious self play is a key term of NFSIP; however, it is not defined until after the algorithm has been introduced. This is confusing to the reader because it is not intuitive why should one use such a policy. It is not until Section 4.1, after the algorithm has been introduced, that some justification is provided for using such policy. Even then, the purpose of selecting actions in such a way is not completely clear and, overall, the concept of self-fictitious play and generalized weakened fictitious play are not explained well. \n\n2. Similarly, the loss functions for the self-imitation learning loop are not presented until Section 4.1 and are not justified until Section 4.2. I suggest that the paper should introduce all the main ideas before presenting the main algorithm.\n\n3. It is never mentioned that all the agents use the same parameters for the policy and value network until the second last paragraph of the paper.\n\nThese are a few examples of key terms and notation that are not properly defined.\n\n1. In the introduction, in the second last paragraph, last sentence, it is mentioned that NFSP is supplemented with “supervised reinforcement based policy averaging to ensure good policies remain relevant;” however, the term “supervised reinforcement based policy averaging” is never defined in the paper. \n\n2. Social welfare is a key element of the algorithm. In the SIL loop, an observation are stored only if they resulted in greater than or equal social welfare as before. However, this term is never formally defined. \n\n3. Most of the notation in the whole paper is never formally defined. One would expect the paper would have a section where the multi-agent reinforcement learning loop (i.e., the agents-environment interaction) is defined with each of its elements. \n\n== Empirical Demonstrations ==\nThe empirical evaluations presented in Section 6 are missing a lot of the necessary details for a reader to be able to validate the significance of the results. First, it is never mentioned how many runs were used to obtain the results. Second, there are no error bars in the plots or any mention of error measures such as confidence intervals or standard errors in the text. Finally, no information is provided about how hyperparameters were selected for the main algorithm or for the baselines. It seems that the results are based on only one hyperparameter setting, but no information is provided about how that hyperparameter setting was selected. \n\nI would also suggest including an ablation study to provide some understanding about how each part of the algorithm contributes to the final performance. This could also indicate the authors about avenues for future work. \n\n== Proper Acknowledgement ==\nIn Section 3 of the paper, first paragraph, the paper mentions that “the average strategies converge to a Nash equilibrium for zero-sum games, potential games and identical interests games.” These results should be cited from another paper or papers. Based on the background material covered by Heinrich & Silver (2016), I believe that the appropriate sources should be Robinson (1951) and Monderer and Shapley (1996). However, not being an expert in the field, I would suggest the authors to make a more thorough literature survey in order to acknowledge the original authors of these results. \n\nIn Section 4.1, third paragraph, the weighted log-likelihood loss is presented as an original idea of the paper. However, this is the same loss used in Oh et. al. (2018) Equation 2 and it should be presented as such by citing the original authors within the same paragraph where the loss is presented. \n\n===== Increasing My Overall Score =====\nI would consider increasing my overall score to a weak accept only if:\n\n1. Proper acknowledgement was given to the previous work that lead up to this paper.\n\n2. Statistically significant results that supports the improved performance of the algorithm over the baselines are presented. This would include the number of runs used for the results, measures of error (confidence intervals, margin of errors, or standard errors), and a description about how the hyperparameters were selected. \n\n3. Key terms and notation were defined in the paper. \n\n===== References: =====\nHeinrich, J., & Silver, D. (2016). Deep Reinforcement Learning from Self-Play in Imperfect-Information Games. Retrieved 15 October 2019, from https://arxiv.org/abs/1603.01121\n\nMonderer, D. and Shapley, L. S. (1996). Fictitious play property for games with identical interests. Journal of economic theory, 68(1):258–265.\n\nOh, J., Guo, Y., Singh, S., & Lee, H. (2018). Self-Imitation Learning. Retrieved 15 October 2019, from https://arxiv.org/abs/1806.05635\n\nRobinson, J. (1951). An iterative method of solving a game. Annals of Mathematics, pages 296–301.\n"
        }
    ]
}