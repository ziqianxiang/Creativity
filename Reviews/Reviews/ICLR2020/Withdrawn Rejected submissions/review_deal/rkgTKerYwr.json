{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper works on the problem of learning a representation for a \"network\" (as far as I can understand, a graph). It is realized by applying a graph network-based autoencoder with three losses to preserve the network structure, attributes, and labels. The main technical contribution of this paper is to learn the multi-task learning weight together in the training (section 4.6 and algorithm 1).\n\nAs far as I can understand from algorithm 1, the joint training is applying gradient descent to the loss weight to *minimize* the total loss. I am very confused about why it works: will all the loss weight converge to 0 so that the final loss is minimized to 0? Or will it just learn higher weight for easier tasks and lower weight for hard tasks?\n\nAlso, as raised by the open review, the paper does not compare to the baseline method, making the contribution unclear.\n\nI am not working on this field and I fail to understand the paper in 2 hours. Please point out if my above questions are invalid. My current rating is weak reject.\n\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposed a joint network embedding model to incorporate the information of network structures, node attributes and labels simultaneously. Specifically, three auto-encoders, which are responsible for the three kinds of information, respectively, are developed, and are then combined in an intuitive but direct way to consider the mutual influences among different types of information. The weights of combining different types of losses are claimed to be learned from data, although it is achieved in the most direct way. \n\nStrengths: \n\nThe paper proposed an auto-encoder based framework, which can handle the information of network structure, node attributes and node labels simultaneously\n\nWeaknesses: \n\nThe novelty of this paper is very limited. The proposed model seems to be constructed by simply combining several existing methods mechanically. For example, the authors follow the work of GCN (Kipf & Welling, 2016) and use Eq. (4) for network structure embedding. However, the adjacency matrix (structural information) is used to convolute node features (attribute information) in GCN, it is not convincing that the authors apply this method by just removing the node feature matrix.\n\nThe number of baselines is not enough, and lacking baselines with only structural information.\n\nThe experimental results are not satisfactory. In the experiments of link prediction and attribute inference, the baseline `CAN` performs better than the proposed model in most cases.\n\nIn the experiment of efficiency evaluation, the experimental environment is not explained and the number of iterations of compared models are different, which is not rigorous enough.\n\nQuestions to the Authors:\n\nIn Eq. (4), (9) and (12), why the parameters of the network (W^((0) ),b^((0) ),W^((1) ),b^((1) )) are the same? Are these formulas wrong? If they are correct, then what is the difference between H_x and H_y? A similar problem exists in Eq. (6), (10), (13).\nIn Eq. (9) and (12), why the attribute encoder and label encoder do not utilize attribute matrix X and label matrix Y respectively, why they only encode the structural information?\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes a unified auto-encoder framework for learning labeled attributed network embedding in a multi-task setting.\n\nThe recommendation is a weak reject. The main argument is:\n- The key contribution is to learn the loss weighting layer together with the network parameters.\n- It is not clear how the weight is updated exactly as it is not described in the text and also not much information can be found in Algorithm 1 other than it is updated by the gradient of the corresponding loss. Also it is not clear if it is different or very different from related works say [1].\n[1] GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks (ICML’18)\n- The paper is well written and the empirical evaluation is carefully carried out. Also, it claims that this is the first work to learn the multi-task weights for labelled attributed network embedding. Yet the original contribution seems limited. \n\n\nSome specific comments:\n“We define an adoptive loss weighting layer which simultaneously …”\n-> \nWe define an *adaptive* loss weighting layer which simultaneously …”\n\nSection 5.8\n“Training the model with different loss weights can effect the performance in multiple tasks.”\n->\n“Training the model with different loss weights can *affect* the performance in multiple tasks.”"
        }
    ]
}