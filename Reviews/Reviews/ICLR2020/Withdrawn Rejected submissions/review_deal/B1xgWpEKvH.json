{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper proposes to make two modifications in hard negative mining procedure for descriptor learning:\n - instead of selecting hardest samples in a minibatch it proposes to sample proportionally to distance between descriptors\n - gradient wrt model parameters is weighted inverse proportionally to the distance\nThe authors attempt to analyze the method theoretically and evaluate on two descriptor learning benchmarks, UBC phototour 2011 and HPatches 2017.\n\nI propose reject mainly for the following reasons:\n1) dataset selection and quality of experimental evaluation\n2) the writing (presentation of the results)\n\nRegarding (1), my main concern is on statistical significance of presented experiments. The authors conduct each experiment once and do not take into account variance coming from neural net training. For example, in table 2 all numbers seem to be very close so it is not possible to conclude anything. Same is true for other experiments, one way to improve would be to do 10-fold cross validation or at least train 5 network with different random seed and provide mean +- std of the results.\n\nThen, I think UBC results are not indicative of performance since the dataset is small and very old at this point, so I disagree with the claim that improvement of ~0.1% is of significant margin. Improvement on HPatches seems to be slightly more significant, but it is not clear wether the authors used comparable networks in terms of number of parameters in the network and descriptor size.\n\nRegarding (2), the way the paper is written is very confusing and should be improved. The attempts to have a theoretical derivation of the proposed method are welcome, but should not precede the actual explanation of the method. I suggest to explain clearly the modifications to sampling strategy, and only then relate them to informativeness intuition.\n\nFurther, the improvement over HardNet is not clear. The two proposed modifications are not tested separately. To have a proper ablation study, the authors could sample negatives proportionally to descriptor distance and remove gradient weighting, and vice versa, to sample hardest-in-batch and only add gradient weighting.\n\nI also disagree that hard negative mining is \"widely used in object detection and other vision tasks\" (quoting the authors). It used to be common in metric learning for face recognition, as the authors cite FaceNet 2015, but has been long abandoned in favor of SphereFace-like approaches. Same is true for object detection, where Fast R-CNN and Mask R-CNN without HNM are widely used."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposes a novel patch sampling method to construct effective mini-batch for training networks based on a triplet loss.\nIn the proposed method, matching pairs of patches are drawn as positive samples stochastically according to the “informativeness” which is defined by means of the derivatives of loss.\nThe authors provide theoretical justification for the novel criterion as well as the practically useful formulation only dependent on the descriptor distance.\nThe experimental results on the patch matching tasks using several benchmark datasets demonstrate the effectiveness of the proposed method in comparison with the other learning-based descriptors.\n\nThe proposed sampling criterion contributes to effectively training the patch descriptors and I like the simple method that is well founded on the theoretical backgrounds.\nThough, there are some ambiguous points to be clarified by the authors.\n\n* The concept of “class” is unclear. How is it defined in this framework? Generally speaking, a local patch is not necessarily connected to the object classes but rather related to the lower-level image structures, such as edges and curvatures. Thus, it seems to be difficult to assign/define specific class labels for patches in the framework of learning patch descriptor in contrast to object classification. And, through the theoretical derivation for the method, the class categories seem not to be necessary. Clarify what kind (level) of class is assumed to establish the proposed sampling method. \n\n* The hyper parameter \\lambda in Eq.(13) might be difficult to tune since we don’t have prior knowledge about the loss value. Is it possible to robustly set \\lambda by certain value, say 10, across various datasets and networks?; in the experiment, \\lambda=10 exhibits favorable performance. And, it would be better to show the values of \\alpha in Eq.(12) throughout the training by \\lambda=10.\n\n* In the experiment, is the angle loss Eq.(15) applied to the HardNet? For fair comparison, the network should be trained based on the same loss. In addition, the authors should show the performance results of lambda=0 to clarify the effectiveness of lambda>0.\n\n* Fig.1 is poorly presented in terms both of the coloring (markers) and image quality; it is hard to see even on the screen.\n\n* Show the comparison regarding training time. Although the method contributes to faster convergence, it requires extra computational cost in computing the descriptor distances via forward-pass of the network."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper presents a method for adaptively selecting the positive samples in triplet loss. Based on the assumption that the norm of the gradient of the loss correlates with the informativeness of each sample, the authors theoretically motivate and propose a method to adaptively select the positive examples. On an evaluation on two datasets, the authors show some mild improvements over the HardNet baseline.\n\nOverall, the suggested method seems reasonable, but the evaluation seems weak.\n\n* The reported results are far from beating the baseline with a \"significant margin\" as the authors claim.\n\n* It's unclear if the results are reported over multiple runs or a single run. Since this isn't reported, a single run is insufficient to suggest how this method actually works. For example, what is the variance among different trainings?\n\n* Arguably, the demonstration of this method in two datasets is somewhat weak too. Adding more datasets can strengthen this work.\n\n* The authors argue in Appendix E that AdaSample converges faster. However, this is computed in terms of number of epochs not computation time. How does this method compare in terms of computation time? AdaSample adds some additional computational complexity over HardNet.\n\n* One could imagine that the negative samples of the triplet loss can also be sampled in a similar way. Instead the authors opt for the \"hardest-in-batch\" negatives. Why is such a decision made? Does AdaSample _not_ work for negative samples?\n\nOverall, I find this work interesting and I would like to see it eventually published. However, at this moment, it is unclear to me (and to any reader, I assume) if the additional complexity introduced by this method actually yields performance improvements.\n"
        }
    ]
}