{
    "Decision": {
        "decision": "Reject",
        "comment": "The authors propose using a noisy channel formulation which allows them to combine a sentence level target-source translation model with a language model trained over target side document-level information. They use reranking of a 50-best list generated by a standard Transformer model for forward translation and show reasonably strong results.  The reviewers were concerned about the efficiency of this approach and the limited novelty as compared to the sentence-level noisy channel research Yu et al. 2017. The authors responded in depth, adding results with another baseline which includes backtranslated data. I feel that although this paper is interesting, it is not compelling enough for inclusion in ICLR. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary:\nThe paper describes a noisy channel approach for document-level translation, which does not rely on parallel documents to train. The approach relies on a sentence-level translation model (from target-to-source languages) and a document level language model (on target language), each is trained separately. For decoding, the paper relies on another proposal model (i.e., a sentence level translation model from source to target) and performs beam-search weighted by a linear combination of the scores of all three models. Experiments show strong results on two standard translation benchmarks.\n\nComments:\n-  The proposed approach is strongly based on the neural noisy channel model of Yu et al. 2017 but mainly extends it to context aware translation. While the paper is referenced, I believe more emphasis should be put on the differences of the proposed approach\n-  It seems that the Document Transformer uses parallel-documents to train, so I am wondering if you can still claim that your approach does not require parallel documents.\n-  In general, I think the paper is well written and results are compelling.\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper presents a simple approach for document-level machine translation. The idea is to use a language model on the target side and a reverse translation model to choose the best document-level translation. This is theoretically justified by Bayes’ rule and the assumption that the sentences are conditionally independent. The authors implement this idea using a reranking model that rescores 50 candidate translations generated by a standard Transformer model for forward translation. \n\nThis is interesting work and the experimental results demonstrate the effectiveness of the approach. However, I am concerned about the (missing) comparison between the proposed approach and the approach that combines backtranslation and a document-level translator (e.g.  Doc-transformer). It seems to me that one could backtranslate a large monolingual corpus and use the resulting parallel documents as additional training data for a document-level translation model. How does the proposed approach compare to such a backtranslation approach?\n\nAnother concern is the speed of translation. It seems to me that the computational cost required for generating 50 candidates and reranking them is quite high. I would like to see some experimental results on the actual speed of translation. The aforementioned backtranslation approach should not have this problem, which also makes me unsure about the usefulness of the proposed approach in practice."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "** Paper summary **\nIn this paper, the authors propose a new re-ranking mechanism leveraging document-level information. Let X and Y denote two languages for ease of reference. The authors focus on X->Y translation and Y->X is a model used for re-ranking. Specifically,\n(1)\tTwo translation models X->Y and Y->X are trained, where X->Y is a document Transformer and Y->X is a sentence transformer.\n(2)\tTrain a language model P(Y) on document-level corpus (rather than sentence-level LM).\n(3)\tGiven a document with $I$ sentences (x^1, …, x^I), translate each source sentence $x^i$ to K candidates.\n(4)\tUsing beam search guided by Eqn.(4) to find optimal translation paths, which is a combination of X->Y translation, Y->X translation, document-level language model and the number of words.\nThe authors work on NIST Chinese-to-English translation and WMT’19 Zh->En translation to verify the proposed algorithm.\n\n** Novelty **\nThe novelty is limited. Compared to the paper “the Neural Noisy Channel” (Yu et. al, 2017), the authors use document Transformer and document-level LM for re-ranking, which is of limited novelty. \n\n** Details **\n1.\tSome baselines are missing from this paper: (A) dual inference baseline [ref1]; (B) X->Y is sentence-level transformer and LM is sentence-level LM, i.e., (Yu et. al, 2017), where P(Y|X) and P(X|Y) are sentence-level translation models.\n2.\tIn Table 1, the improvement of doc-reranker is not very significant compared to sent-reranker, ranging from 0.21 to 0.66. \n3.  In Table 4, “Channel + LM” and \"Proposal + Channel + LM\" achieved almost the same results. Does it mean that the \"proposal\" component do not work?\n4.\tMany models are used in this framework. I am not sure whether simple re-ranking or ensemble can outperform this baseline, e.g., 2 Zh->En + 1 En->Zh\n\n[ref1] Dual Inference for Machine Learning, Yingce Xia, Jiang Bian, Tao Qin, Nenghai Yu, Tie-Yan Liu, IJCAI’17"
        }
    ]
}