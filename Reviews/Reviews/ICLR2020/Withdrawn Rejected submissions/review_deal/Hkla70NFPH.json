{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper asks whether the choice of using English as the pivot language for training/generating cross-lingual word embeddings (WE) is empirically well motivated. The paper shows experimentally that choosing English as the hub leads to poor performance on tasks like lexicon induction. Another contribution is the triangulation method for providing dictionaries for all pairs of languages in a Lexicon Induction evaluation dataset (such as the one used in MUSE). Guidelines for how to choose hubs for training Bilingual WE/Multilingual WE are also provided. The finding that a hub language that is neither source nor target leads to good performance appears interesting. \n\nIntuitively, one can argue that the answer to the main research question of the paper is \"no\", because of reasons like linguistic distance, morphology differences etc., that are also noted in the introduction of the paper. A similar research question was asked and addressed in the recent paper by Alaux et al. (ICLR 2019). The paper should at least cite this work and discuss some of the key differences to make it clear what the contribution is. As as result, the current version of the paper makes a limited contribution. \n\nThe quality of the dictionaries obtained through the triangulation method is also not assessed. While the authors argued that doing so will be impractical for all language pairs, surely it can be done for some of them. This will inspire more confidence in the evaluations and the triangulation method.\n\nThe experimental section also can be improved. For instance, limiting to lexicon induction for drawing conclusions about word embeddings is error-prone, as argued in several papers (e.g. Glavas ACL2019, Ruder JAIR 2019 etc.). The experiments should be expanded to more tasks.\nThe experimental comparison is also limited to MUSE, and other popular methods like VecMap are omitted. Expanding here will also strengthen the claims of the paper."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper critically examines projection-based methods for learning cross-lingual word embeddings, analysing the particular aspect of choosing a more appropriate hub language in both bilingual and multilingual learning setups. Through a series of experiments, the paper shows that depending on English as the hub language often leads to suboptimal bilingual lexicon induction (BLI) performance for a range of learning conditions with different languages. While the study is well executed in general, and it is good to see some reasonable high-level intuitions empirically validated, I believe that the novelty of the work remains limited, as all the key contributions have been known (or taken for granted) before.\n\n- Contribution 1: the choice of the hub. It is very intuitive to assume that English won't be the best hub for many multilingual setups, so the only contribution of the paper is validating this for the BLI task. Note that Heyman et al. (NAACL 2019) also did a preliminary study on the importance of choosing a hub language when doing MWE learning, so this paper is not the first study that focused on that problem. Furthermore, similar claims have been verified by a recent work of Alaux et al. (ICLR 2019), which is not even cited.\n-- Regarding the work of Heyman et al., the authors decided not to compare to that model because \"the order in which the languages are added is an additional hyperparameter that would explode the experimental space\". However, I believe that some knowledge from typological databases such as URIEL (used in this paper) can also guide the choice of language order for that model which would prevent the experimental explosion.\n\n- Contribution 2: general guidelines for choosing a hub language. The guidelines are well-known from prior work (multilingual systems are preferred over bilingual ones, typological information guides the selection of the hub), and they are again just empirically processed in an extended and more focused way. The results reported are as expected.\n-- I would like to see another experiment when it comes to choosing the appropriate hub language which would increase the novelty of the work. How important is the quality of the starting monolingual embedding space? That is, do the guidelines change if we e.g. have a low-quality English space trained on a smaller corpus compared to an English space trained on a very large corpus? On a more general note, imagine that you have detected language X as the best hub for the current setup? If we obtain (in a controlled envirnoment) a lower-quality embedding space for the hub language X, will X still remain the optimal hub choice for the same setup? What will happen there?\n-- Although the paper states that the general guidelines how to choose a hub language are offered, the paper fails to clearly stress how to do this in practice. I would like to see more explicit guidelines here. In addition, it would be interesting to measure how much we lose if we use the second or third best hub language instead of the optimal one. Is that really a problem or not?\n\n- Contribution 3: there has been more work on collecting parallel data, especially for lower-resource languages recently, such as the WikiMatrix project or JW300. Extracting training and test data from such resources is trivial and only a byproduct of that work. Therefore, I don't see it as a major contribution. Furthermore, using the PanLex dictionaries to get suitable data sets for resource-lean languages has also gained traction recently, and there are other sources that can be used.\n\nAlso, the work focuses only on the BLI evaluation, which as discussed by e.g. Ammar et al. (2016) or Heyman et al. (2019), or Glavas et al. (2019) should not be used as a single evaluation task to really establish meaningful comparisons between different embeddings. In order to fully understand the implications of choosing different hub languages in different settings, more experiments are neeeded related to downstream evaluation. It would definitely be very interesting to measure how the choice of the hub language can affect multilingual parser transfer or cross-lingual classification/NL inference systems, etc. I believe that those experiments are a must-have for the empirical paper liket this one. \n\nIn summary, while this study has merit as it empirically validates some very intuitive hypotheses, I do not see it making a great impact on the field of cross-lingual word representation learning (especially due to its limited novelty, see above), and the paper might be a better fit for a more NLP-focused conference.\n\nOther questions:\n- Why have the authors limited their choice of methods only to MUSE, since more robust and powerful methods have appeared in the meantime? Why are VecMap (Artetxe et al., ACL 2018) or RCSLS (Joulin et al., EMNLP 2019) not used as (stronger) baseline models?"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper explores the importance of the pivot language when learning multilingual word embeddings. While previous work has been severely English-centric, the paper introduces a new dataset covering all combinations of 49 languages, derived automatically from the MUSE dataset through triangulation, and analyze the performance of two existing mapping methods in it for different choices of the pivot language.\n\nI think that the paper is well written and technically sound, and both the new dataset and the analysis done are valuable contributions. However, I am not convinced that it has enough novelty and substance for the venue: a similar point was already made by Alaux et al. (ICLR'18), although the authors do not seem to be aware of it, the dataset was derived automatically from an existing one using an error-prone (and rather trivial) procedure, the analysis is somewhat limited (e.g. covering only two mapping methods) and, while still valuable, the results do not seem very significant to me. More concretely:\n\n- The main point that the paper tries to sell is that the choice of the pivot language is important, and always picking English, as previous work did, is suboptimal. I have been actively working on this area, so when I read the title and the abstract I was excited to learn more about this finding. However, as I read through the paper, my excitement went down, and I was left with the feeling that the pivot language was not that important after all. In Table 2 and 3, the difference between English and the best configuration is around 1 point, which is rather small for this task. To make things worse, even that difference is questionable, as the best configuration is chosen in the test set (more on this below).\n\n- I think that picking the best configuration in the test set, and comparing that to one of the specific configurations, is methodologically problematic. To make an analogy, let's say that I want to claim that the random seed is important when learning cross-lingual word embeddings with a given method, and picking 5468721, as some implementation does by default, may be suboptimal. So I make multiple runs with different random seeds and show that, indeed, 5468721 is rarely the optimal random seed. This would not surprise anybody, as the same thing applies to pretty much any work in ML. So, coming back to your work, is the choice of the pivot language anyhow different from a random seed? I do believe that there is something more than just randomness in your results, but unless I am missing something, you are not controlling this factor, and the differences you report look too small to ignore it.\n\n- The whole idea of the paper is actually not very novel. Alaux et al. (ICLR'2019, https://openreview.net/forum?id=HJe62s09tX) also showed that mapping all embeddings into English was suboptimal, and proposed an alternative approach. The authors do not seem to be aware of this work, as they do not even cite it. I am not saying that there is nothing new or interesting in the paper, but this is clearly a weakness, as the work done is not put in context appropriately.\n\n- The evaluation is limited to two methods, and some influential systems, like VecMap, are left out. I understand that asking to evaluate dozens of methods would be not be reasonable, but I think that the limited scope of the analysis is a weakness if you want to make general claims on the limitations of previous work.\n\n- Somewhat ironically, the proposed dataset is automatically derived from the MUSE one in an English-centric way. The authors use a simple triangulation approach for that. This is error-prone, as the authors acknowledge and try to mitigate with a post-processing that detects and removes some wrong entries. To make things worse, previous work (Kementchedjhieva et al., EMNLP'2019) has also found issues in the original MUSE dataset, which your derived one will unavoidably inherit. For that reason, I am not entirely convinced about the value of the resulting dataset.\n\n- The paper focuses exclusively in Bilingual Lexicon Induction (BLI), which is common practice in this research area but has been recently questionned. Glavas et al. (ACL'2019) showed that BLI-centric evaluation was problematic and, as mentioned before, Kementchedjhieva et al. (EMNLP'2019) found issues in the MUSE dataset, which you automatically extend. This does not invalidate all your work, but I do see it as a weakness.\n\n\nOther details that did not influence my score but I think the authors should address:\n\n- The term \"pivot\" seems more appropriate than \"hub\". \"Hub\" is usually used with a different meaning in the context of cross-lingual word embeddings (in relation to the \"hubness problem\"), and \"pivot\" seems more standard for what you mean.\n\n- Iterative refinement methods are named \"MUSE-like\", which seems unfair, as this technique was not introduced by MUSE. Why not just say \"iterative refinement\" (or \"self-learning\")? Otherwise you should credit the original authors that proposed this idea, which I believe are Artetxe et al. (ACL'2018).\n\n- \"We hypothesize that learning mappings for both language spaces of interest (hence rotating both spaces) allows for a more flexible alignment which leads to better downstream performance [...] Note that this contradicts the mathematical intuition discussed in Section 2 according to which a model learning a single mapping (keeping another word embedding space fixed) is as expressive as a model that learns two mappings for each of the languages.\" Assuming that the mappings are orthogonal, this is not just a \"mathematical intuition\", it is a provable fact, so it does not make sense to propose a hypothesis that contradicts it. If you find a surprising pattern that you cannot explain, you should acknowledge it, but proposing a hypothesis that is known to be false does not make any sense.\n\n- This is somewhat subjective, but I dislike the term \"European language\" as it is used in the paper. It does not make much sense from a linguistic point of view (Turkish is not an Indoeuropean language), and it is even questionable from a geographical point of view, which should not be very relevant, anyway (e.g. English, Spanish and Portuguese have more speakers outside of Europe than within Europe, and most of Turkey is actually in Asia)."
        }
    ]
}