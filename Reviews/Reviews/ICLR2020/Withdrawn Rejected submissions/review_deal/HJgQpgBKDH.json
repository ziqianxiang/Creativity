{
    "Decision": "",
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper proposes a new approach for handling the setting where the user is given (a) a small number, m, of clean labeled data points (i.e. assumed to be correctly labeled); and (b) a larger number, M >> m, of noisily labeled data points (note that while the paper refers to this as the weakly supervised setting, in this reviewer's opinion, that nomenclature usually refers to when just (b) is present).  The authors then propose a \"meta learning\" approach in which a \"meta model\"- the \"label correction network (LCN)\"- is trained on the clean labeled dataset to take as input a weakly labeled data point, (x,y'), and output the estimated correct label y_c, and then a \"main model\" is trained on these labels (the weakly labeled dataset with the labels from the LCN).\n\nThis paper follows a fairly standard approach for implementing the above procedure, so the bulk of the claims rest on the empirical evaluation in the experiments section.  Unfortunately, the experiments section leaves a lot of questions unanswered, and unfortunately does not support the claims in the paper well enough to merit acceptance in this reviewer's opinion.\n\nTo start, the paper considers three noise models for the weak labels: (A) uniform noise, (B) class-conditional noise, and (C) data-dependent noise.  (A) and (B) are commonly considered models, while (C) is a somewhat new one considered in this paper. \n\nit's important to note that in the majority of the settings, the proposed method seems to really only do better than other approaches in setting (C). In fact, while the authors claim that (A) and (B) \"are similar to one another\", and therefore relegate (B) to the appendix, an inspection of the appendix makes it clear that in setting (B), the proposed approach actually does worse relative to baselines (e.g. see Table 4 vs. Table 5)... regardless, the overall conclusion seems to be that the proposed approach really only does better in setting (C).\n\nThis then raises two questions: (1) is there any bias coming from the fact that the noisy labels in (C) are being generated from the same or similar model class as is used in the LCN to correct the label noise, as appears to be the case?  And (2) is the proposed approach actually doing anything useful, or is the conclusion just that training a model on a small amount of weakly labeled data does pretty well?  Going through these concerns in more detail:\n\nRe: (1): To generate the noisy labels in setting (C) (the \"WEAK\" setting in the paper), the authors use a set of \"weak classifiers\".  To start, there should be additional detail on what these classifiers are (see note about general lack of clarity in experiments section below).  However, from the provided code, it seems like the same pre-trained models are used to generate the weak labels as are used in the LCN to correct for this noise.  This seems to inject a significant bias, which would unduly advantage the proposed method in the one setting where it appears to do better than prior approaches.  Regardless, this is a concern that should have been addressed.\n\nRe (2): A second, more significant question is whether the large weakly-labeled dataset, and the main model trained on it, actually provides value over the LCN model trained on the cleanly labeled data?  I.e., is the proposed approach actually doing anything more valuable than just training a standard supervised model (e.g. the LCN) on clean data would do?  In more detail: the method works best in settings where the LCN is trained on clean data (1-10% of the main dataset according to Appendix tables, which is a significant amount of labeled data for a pre-trained model like that used for the LCN); however, is this just because this LCN model is being trained well enough to be a good discriminator?  Or is the proposed setting- where the LCN is then used to correct the labels of the larger weak dataset, and then a model is trained on this dataset- actually providing additional value?  Figure 5(a) seems to indicate that indeed the LCN model trained on a small amount of clean data might just do fine on its own... either way this is a key point that needs to be ablated / explored\n\nMore broadly, there are many unclear points in the experiments section that make the results hard to interpret.  For example the explanations around both Tables 2 and 3 are confusing and underspecified.  Table 3 is particularly poorly explained: what SSL (\"Semi-supervised learning\" I presume?) methods are used?  These are never mentioned?  What noise model is being used (I assume (A), \"UNIF\", since I see a \"p=0.6\", but the paper never says)?  Why are the number of clean labeled data points seemingly so arbitrary (ranging from 60 to 2.5k)?  What is \"MAE\"?  Etc.\n\nIn summary: this paper leaves open some key questions as to the value of the proposed approach, and the experimental setup in general, that would need to be answered more thoroughly for this paper to merit acceptance."
        },
        {
            "rating": "8: Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper presents an improvement to the task of transfer learning by being deliberate about which channels from the base model are most relevant to the new task at hand. It does this by apply attentive feature selection (AFS) to select channels or features that align well with the down stream task and attentive feature distillation (AFD) to pass on these features to the student network. In the process they do channel pruning there by decreasing the size of the network and enabling faster inference speeds. Their major argument is that plain transfer learning is redundant and wasteful and careful attention applied to selection of the features and channels to be transfered can lead to smaller faster models which in several cases presented in the paper provide superior performance.\n\nPaper is clear and concise and experimentally sound showing a real contribution to the body of knowledge in transfer learning and pruning."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposes to learn via meta-learning to correct noisy labels using a small trusted set of examples with correct labels. Inspired by the work of Ren et al, 2018, which proposes a method to learn to re-weight examples via meta-learning, the authors of this work instead propose to learn to remove label noise, but in a similar fashion. They use a finite-differences approximation to compute the second-order gradient in meta-learning and a look-ahead SGD to update the weights of base classifier after applying several update steps to the meta-learner.\n\nNovelty\nThe idea is novel, well motivated and technically sound. \n\nPerformance:\nThe method does not seem to perform better than the existing approaches of GLC or L2R or Forward unequivocally in all situations. For example even for the simpler UNIF noise, the proposed method performs poorly versus the existing approaches particularly when less trusted data is available. This is to expected as their method relies on learning a CNN to correct the labels, which in turn is likely to be more susceptible to overfitting with inadequate trusted data. This limits the broad applicability of the proposed approach and narrows its contribution.\n\nExperiments:\nI would also like to see the authors results include some real-world image datasets beyond the toy MNIST dataset. For example Hendrycks et al. at the very least included the CIFAR dataset besides MNIST, even though that is also not ideal because of how small the CIFAR dataset. Without these results it is unclear as to whether the proposed method is even applicable to real-world images or not.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a meta label correction approach for learning with noisy labels. They view the label correction procedure as a meta-process and then train a supervised model to fit the corrected labels generated by the meta-model. Some experiments and comparisons with SOTA are given, together with an ablation study.\n\nPros:\n-This paper made an interesting contribution to use a meta-model for label correction and then train a supervised main model using the correction labels via a bi-level optimization problem.\n\nCons:\n-The problem setting is a bit unclear to me. Weakly supervised learning and learning with noisy labels seem to be mixed up in this paper. Weakly supervised learning aims to learn from weak supervision, i.e., incomplete/ inexact/ inaccurate supervision (see more details in “A brief introduction to weakly supervised learning, National Science Review, 2018”), which is a more general concept. This paper only discussed learning from inaccurate supervision, thus stating learning with weak supervision may be a bit confusing.\n\n-In the introduction part, the authors reviewed two lines of prior works: re-weighting training instances and label correction. The literature review seems to be insufficient since there is another important research line that uses the sample selection bias trick. See “Jiang, L., Zhou, Z., Leung, T., Li, L., and Fei-Fei, L. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In ICML, 2018” and “Han, B., Yao, Q., Yu, X., Niu, G., Xu, M., Hu, W., Tsang, I., and Sugiyama, M. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In NeurIPS, 2018”.\n\n-The strategy proposed in Sec 3.1 to convert a classifier to a label correction network seems a little suspicious to me. Can any further explanation or theoretical intuitions be given on how the hyperparameter lambda works? This would be very useful and make the paper more convincing.\n\nOverall the paper proposed an interesting idea to deal with noisy labels in a meta label correction way, but some unclear parts need to be clarified and improved.\n\nFurther details:\n-Sec 2.2: there is a small mistake of the citation.\n\n-Sec 3.3: using a k-step look ahead SGD update as an estimate to the optimal model is good to me. But here k is set to 1500 and then decreased to 500, it seems to be too large and inefficient since the paper L2R also uses a similar trick for optimization and they only use k=1. How about the computation time?\n\n-In the experiments, MLC seems to be inferior in the UNIF case when using very small clean data and slightly outperforms baseline methods when using larger clean data. So how many clean data are used in tuning the baseline methods, since for L2R they only used 1000 clean data for all the experiments, and for Forward and GLC, they are also sensitive to the number of clean data used."
        }
    ]
}