{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper proposes a hierarchical structure for decomposing the image enhancement problem into multiple steps that can be better learned separately. While the method in the paper is intriguing I think it needs further study. The experiment analysis and comparison to baselines on a different image resolution seem a little unfair. As well, given the current writing in the paper, many of the components of the method do not appear to be overly novel. \n\nMore detailed comments:\n-\tWhile the quantitative improvement for the method is important, it is difficult for the reader to understand how significant these quantitative improvements are, relative to image improvement. It could help to include more image examples in the appendix for the paper. The user study experiment does help with this analysis. If it was also performed on the weekly supervised case it would support the improvements provided by the method even more.\n-\tThe paper proposes a perception-based division. How is this perception-based division motivated?\n-\tIn the abstract, there is a confusing sentence? What does the \"dimension-based division\" do? It should be explained a little better before it is introduced in the abstract.\n-\tFor the divide part of the method, things are split into a hierarchy. Will this learning system be combined into a system that will be able to train the levels of the hierarchy together? What is the final loss function that is used to train the network are equations 4 and 5 added together? It might be better to include an algorithm to help understand how the system is trained.\n-\tThe sentence “On one hand, learning additive map is encouraged by the success of residual learning on image colorization and super-resolution et cetera.” Needs a reference.\n-\tFOr equation 1 where the additive and multiplicative advancements are described has a chained version of this been considered instead of the addition of these two maps? This might align more with the methods used in computer graphics to create renderings.\n-\tI find the application to weekly supervised learning very interesting. If more results can be shown on this problem it would help show the impressiveness of the method.\n-\tThe use of cycle GAN for the reconstruction loss does not appear to be very novel. Is its use in some way unique as to make it more novel?\n-\tA reference for equation 5 would be helpful to the reader. Currently, it is a bit unclear where this equation comes from?\n-\tAn ablation for equation 6 is important if the authors are claiming this as an important feature to add.\n-\tUp to the end of section 2, it still seems like this work is a combination of recently used methods. They are combined in a way to make use of an interesting hierarchy but this does not seem to be very novel. For example, the use of multi-scale design on a U-Net feels like a small improvement over current methods. If the authors can further explain the complexity of this combination or the large improvement it has on the results it would help convince the reader of the novelty of this combination.\n-\tIt is noted in the paper that the experiment studies a higher resolution image enhancement data set that the baseline methods were not originally evaluated on. While it is good progress to show that the proposed method in the paper works well on this higher resolution dataset it is important to compare the proposed method to the baseline methods on the lower resolution dataset the baselines were originally evaluated on. This will allow the reader to have a more fair comparison of the improvements of the proposed method.\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review": "This paper proposes a GAN-based approach for high-resolution image enhancement. The contributions are of this work are mainly on a few network architectural designs of the generator and discriminator, and an improved loss function. Some qualitative and quantitative results are presented to demonstrate the effectiveness of the propose method. However, the overall novelty is incremental and experimental results are not solid.\n\nPros:\n\n+ Improved performance on benchmark dataset\n\nOriginality: The novelty of this work is limited. All proposed modules (additive and multiplicative connection, two-stream of low- and high-frequency inputs, and AdaSWGAN), as authors cited in the paper, are directly borrowed from previous work. The AdaSWGAN looks like a simple combination of AdaWGAN and SWGAN. While no one has integrated those in one framework for image enhancement before, I do not find principled designs or intuitions from authors on why those modules work well for this specific task.\n\nQuality: The image enhancement is formed as a one-to-one mapping in this work. However, for a given low-quality input, there can be multiple possibilities for the enhanced output. While there are already a few im2im translation that work on multi-modal, the authors need to take this into account when formulating the problem. Regarding the experiments, it is better to compare with more methods such as CLHE, NPEA, and FLLF, as did in [Chen et al., 2018].\n\nSignificance: The task of image enhancement is still a challenging problem, due to less paired data and the difficulty of disentangling different aesthetic factors. In addition, it is a one-to-many problem. While this work does not concern those topics, except for the performance improvement in Table 1, its significance is greatly reduced.\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The authors propose a divide-and-conquer style adversarial learning algorithm for weakly supervised learning of high resolution image enhancement. Their work is primarily motivated by the limitations of modern mobile cameras in producing images with adequate dynamic range, texture sharpness, and exposure. They also point out limitations of recent work in image enhancement; e.g. supervised approaches suffer from difficulties in data collection (must have paired low quality, high quality images) and previous applications of GANs have suffered from their inability to simultaneously enhance multiple \"perceptual\" qualities of the input images (i.e. texture, color, light, etc). Furthermore, many recent approaches rely on downsampling or patching strategies in order to deal with the computational demands of high resolution images. The authors aim to address these limitations by 1) introducing a \"divide and conquer\" approach in the generator/discriminator networks, thereby jointly optimizing a range of image qualities, 2) adopting the CycleGAN objective for weakly-supervised learning of low/high image sets, and 3) proposing a multi-scale architecture that is able to transfer learned features on downsampled low-resolution images to higher resolution images and improve the overall quality of results. They perform an empirical study using several datasets comparing PSNR and SSIM scores with previously proposed methods. They also provide Mechanical Turk human evaluation results for a small sample of images from two of the datasets.\n\nThe fundamental idea of divide-and-conquer for image enhancement, as well as the multi-scale architecture, are both interesting contributions to the field. The authors also demonstrate fairly promising results on several datasets. However, due to the fact that this work ambitiously introduces a wide range of architectural changes, it suffers from a general lack of thoroughness in empirical support for all of the proposed ideas. For example, while the authors provide reasonable and convincing support for the divide and conquer approach through an ablation study, they fail to provide adequate justification for other proposals, such as the \"adaptive\" sliced Wasserstein GAN (dubbed AdaSWGAN). The only empirical evidence given for this change is an unconvincing qualitative comparison on the 25-Gaussians dataset, in which the alleged improvement over previous methods is unclear. Additionally, despite the fact that the proposed multi-scale architecture is solving a problem suspiciously similar to super-resolution, the authors do not compare their method to similar multi-scale, super-resolution methods such as ESR-GAN. The authors also do not provide any runtime analysis (theoretical or empirical), which seems fairly critical given the apparent complexity of their method.\n\nGiven these concerns, as well as some others which I will detail per-section below, I am giving this paper an overall score of \"weak reject\" for lack of clarity and rigor. If the authors are able to address some of my questions and concerns, I am willing to increase this score, as I do think that some of their proposed ideas are valuable contributions.\n\nSection 1: Introduction\n\n1. \"Besides, due to the computational complexity and unstable adversarial training, the existing image enhancement methods treat high-resolution images using either the downscaled version or patch-wise processing strategies, neither of which are optimal\"\n\nUnless I have misunderstood something about the authors' multi-scale architecture, it seems that their method (as applied in the experiment) also suffers from this limitation. In the appendix, they say that the high-resolution enhancer uses images of size 1024x512 resolution, which appears to be smaller (and oddly has a different aspect ratio) than the resolution of their training data. They note that it is \"still feasible\" to train their enhancer on higher-resolution images \"if [they] have more computational resources.\" But the same would also apply to other methods, since the whole reason for downscaling/patching is to lessen these computational constraints.\n\n2. It is not clear how figure 1 and figure 2 relate to one another. The generator does not perform frequency-based or dimension-based division, so it seems that this \"recombining\" step is sort of implicit in the loss function learned by the discriminator. This should be clarified.\n\nSection 2: Proposed Method\n\n1. Please clarify the x*s_x^-1 term in equation (1); specifically, what is the meaning of the -1 exponent? Is it simply 1/s_x? If so, why is the reciprocal necessary?\n\n2. The mathematical meaning of global concatenation is not clear from the figure or the text. Please clarify.\n\n3. There are no citations or theoretical justication for the section on conv + average pooling with regards to capturing meaningful global features. Specifically the claim \"this design enables our model to enhance full resolution images while it is trained on downscaled images\" needs empirical and/or theoretical support. This seems more like conjecture than fact.\n\n4. Organization of section 2.2 is weird. It's supposed to be about \"frequency based division\" but most of the section is dedicated to discussing CycleGANs. Perhaps these topics should be split into separate sections.\n\n5. For freqency separation, is a Fourier transform used? Or just the application of Gaussian kernels and grayscale components as described in the last paragraph? Additionally, how are the Gaussian kernels parameterized?\n\n6. The integral in equation (5) is missing a differential, presmuably d-theta. The theta \\in S^{n-1} notation doesn't really make sense for integrals, since the space is (I think) continuous. If not continuous, this should be a summation.\n\n7. As mentioned before, the motivation for an adaptive lambda parameter in the Wasserstein objective is not clear. Does this architecture not converge or perform worse without it?\n\n8. The adaptive lambda also trades one hyperparameter for another, i.e. the bound tau. How is this term chosen? And what is the effect on performance?\n\n9. The 25-Gaussians experiment is does not provide adequate support for the necessity of AdaSWGAN. The source paper for SWGAN provides a far more comprehensive comparison as well as quantitative results, rather than a single visualization. Furthermore, there is no evidence given that AdaSWGAN has better performance/convergence on real data.\n\n10. Typo in second to last paragraph: Stifel -> Stiefel\n\nSection 3: Multiscale Network and Training\n\n1. Please clarify whether or not the given low/high resolution image sizes in the appendix are used by the DACAL architecture in all experiments.\n\n2. This section should be condensed to the main points. Some of the details and tangential discussions can be moved to the appendix.\n\nSection 4: Experiment\n\n1. Please clarify the low/high image resolutions for the MIT-Adobe FiveK datasets and whether or not the images used for the high resolution DACAL are downscaled.\n\n2. How are SSIM_f and PSNR_f calculatd on full resolution images if DACAL (and the other methods) operate on downscaled version of the images? Is there an upsampling operation applied in post processing?\n\n3. Why is DPED in the weakly supervised section if it contains paired images? Is that not a supervised task?\n\n4. Space permitting, please provide a qualitative example from one of the supervised tasks that shows the enhanced output of DACAL vs. the ground truth high resolution image.\n\n5. Is the CycleGAN architecture still used for supervised learning tasks? Or just the two-stream U-Net?\n\n6. Style suggestion: replace \"our DACAL\" with just \"DACAL\" throughout this section. This is better technical writing practice.\n\n7. As mentioned before, please provide some assessments of computation costs and runtime (theoretical or empirical) for DACAL vs. the other methods. If DACAL requires significantly more compute time or memory, this is an important trade-off to consider.\n\nUpdate 1: Fixed typos"
        }
    ]
}