{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "This paper investigates the relation between distributional robust optimization and chance-constrained optimization. The authors show relations between these two problems for strategies with and without boundedness through a thorough theoretical analysis. They furthermore validate their insights in synthetic and real-data experiments. \n \nThe paper appears sound and solid to me -- to the extend I can assess this. I have doubts on the  relevance of the paper to ICLR though. The paper appears to be only relevant to a very small subgroup of ICLR attendees and more suited for EC type conferences.\n\nCould the authors please comment on the relevance of the paper regarding topics of relevance at ICLR? How would one utilize the presented results in practice?"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper provides an interpretation of distributionally robust optimization(DRO) approximately as a change-constrained optimization(CCO). The paper is theoretical sound and provides interesting point of view to handle the distributional optimization problem. It also provides some numerical simulation to support the theory.\n\nWhile the method itself is interesting, the main concern to accept this paper is that it does not show enough connection to machine learning community. The whole paper spend a lots of space to do the mathematical derivation, but does not draw connection to a specific type of machine learning problem they want to solve. The only example it mentioned is in the introduction of portfolio management, which seems to be more important problem in operation research community. To be accepted as a machine learning conference paper, I think the paper should apply this fundamental statistical optimization problem into a machine learning problem, such as adversarial learning.\n\nMinor mistake:\n1. Abstraction line 9, the period '.' should be replace by a comma ','.\n2. In problem setting, the line 3 of paragraph of $\\phi-$divergence, there is typo on $0\\phi(\\frac{0}{0})$ and $\\phi(\\frac{a}{0})$, which does not make sense in the context.\n\nIn sum, I think the paper should be rejected unless it makes more connection to machine learning community."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper studies the relationship between Distributionally Robust Optimisation (DRO) and Chance-Constrained Optimisation (CCO). In particular, the paper finds that with a particular cost function and a series of approximations applied to both DRO and CCO, the two optimisation problems start to resemble each other. This enables the authors to relate the hyperparameters of CCO and DRO. The paper then concludes by a series of toy and real-world experiments demonstrating this relationship.\n\nI am currently leaning towards rejection. The main reasons are:\n\n(1) The paper does not seem to be a good fit with the aims of the conference. Instead of using standard statistical/ML terminology, it seems to be written for an audience with quantitative finance background. While some of the employed techniques could be of interest to the broader ML community, I am afraid most readers of the ICLR proceedings will not appreciate these insights due to the unusual terminology and framing of this paper which obfuscates the underlying theory. Relatedly, the authors claim that the main benefit of their discovered relationship between DRO and CCO is that it “opens a door for financial practitioners to interpret the radius parameter [of DRO] through parameters [of CCO] that are directly linked to investment  performance.” While this statement may be appreciated by readers of a quantitative finance journal, it only left me wondering why not use CCO directly and not bother with the DRO relationship, if the “investment performance” interpretation is already embedded within CCO?!\n\n(2) I am not sure whether the amount of the contributions relevant to the ICLR community goes far enough beyond those made in the cited Gotoh et al. (2018) paper.\n\n(3) The paper is difficult to read with a significant number of grammatical errors (especially within the first two pages) and in contrast to its relative preciseness within the proofs section in the appendix, contains a noticeably high number of confusing or imprecise statements within the main body. \n\n\nMajor comments:\n\n- I would have liked to see more discussion of the o(eta_2^(n - 1)) from Lemma 3.2 and of the kappa(epsilon) approximations from sect.3.2. From your experiments in sect.4, it seems like the error introduced by the approximations can be quite substantial. If this is true, I am not sure how to interpret the results in sect.3.3. For example, one of the claims that you emphasise is that the approximation of CCO achieves better results than the approximation of DRO when both have finite optimal value (1st bullet point in thm.3.6). However the original formulations of both the DRO and CCO problems are intended to provide an additional set of guarantees about the obtained solution beyond how good is the value they achieve (robustness and low probability of negative return respectively) which AFAICT are not guaranteed to be satisfied by their approximate versions derived in this paper. Can you please clarify whether the approximate \nversions of DRO and CCO satisfy the original constraints and/or whether you can quantify by how much these can be violated?\n\n- On a related note, can you please clarify what do you mean by “optimal value” in sect.3.3? In particular, is it x^T mu for both (9) and (10), or do you take the value of the whole expression inside the curly braces in (9)?\n\n- Could you please provide some citations or discuss why you have chosen the particular set-up of experiments in sections 4.1 and 4.2?\n\n\nMinor comments:\n\n- In the abstract, you say “Based on this non-robust reformulation, we then show that when a\nboundedness constraint is added to the investment strategy. The DRO problem\ncan be cast as a chance-constrained optimization (CCO) problem without distributional uncertainties.” Wasn’t this supposed to be a single sentence with a comma in the middle?\n\n- Can you please provide citation for the first sentence in the sect.1? Also, in the second sentence, by “true” distribution, do you mean population distribution?\n\n- In the 4th sentence in sect.1, I’d recommend replacing “it”; e.g., you can say “However, the estimates are often inadequate due to an insufficient number of available data points.”\n\n- Last sentence of par.1 in sect.1, “under a scarcity of data” -> “when data is scarce”.\n\n- Can you please provide a citation for the first sentence in par.2 of sect.1? Also can you please reword the very next sentence (particularly the phrase “contains distribution as uncertainty” the meaning of which was unclear to me until sect.3.1)?\n\n- In par.2 of sect.1, within the statement starting with “The second way is the moment-based approach ...”, I would suggest moving the words “for instance” before the citations (i.e., “was studied for instance in …”).\n\n- There are many more places which are not entirely grammatical. To shorten my review, I will not include further comments related to grammar, and would instead strongly recommend that you proof-read the paper again please, particularly the abstract and sect.1.\n\n- In the 2nd sentence of par.3 in sect.1, you say “As a result, optimal solutions become meaningless ...“ while speaking about the moment-based and geometric approaches. Can you provide citation please? While I generally agree with the sentiment that we would like to ensure that the true distribution is within the ambiguity set with high probability, this is AFAIK not guaranteed even by the statistical approaches without further assumptions on the underlying population distribution (which allow to derive an appropriate setting of the diameter rho).\n\n- By the end of the introduction, the reader still did not learn what you mean by the “non-robust interpretation”. Please consider either explaining this within sect.1 or removing all mentions within sect.1.\n\n- In the “Notations” paragraph within sect.2, does “nominal probability distribution” mean the same as what is commonly called the “empirical distribution”, or rather just any distribution  (not necessarily the empirical one) that you will use as the center of the ambiguity set?\n\n- In the “phi-divergence” paragraph within sect.2, please consider restating the expressions which include division by zero in terms of limits. Also, for the benefit of the reader, I would add that for the Radon-Nikodym derivative to exist, Q must be absolutely continuous w.r.t. P, and that if it exists then Q(A) = int_A dQ/dP dP for any measurable set A.\n\n- Please reconsider use of contractions throughout the paper (e.g., “It’s defined by a convex …” -> “It is defined by a convex …” in the “phi-divergence” paragraph within sect.2).\n\n- In eq.1, it is somewhat strange to writhe P(dt) given the variable “t” does not appear anywhere in the integral. Please consider either writing int phi((dP / dQ)(t)) P(dt) or simply int phi(dP / dQ) dP.\n\n- On the penultimate line of p.2, you cite Glasserman and Xu (2014) for the name “alpha-divergence”. I find this somewhat strange as alpha-divergences (or Renyi alpha-divergences) date back much further---see [1, 2]. On a related note, some of the other quoted examples like Hellinger distance and xi-squared distance are (up to a scaling factor) members of the alpha-divergence family.\n\n- In the “Motivation” section, you say that the CRRA utility function is ln(x^T r) when eta = 1; however, you did not constrain the vectors x and r to always have positive inner product (only constraint is x, r in R^n). Can you clarify please?\n\n- At multiple places, you use max and min instead of sup and inf but it is not entirely clear to me the optimum exists in all those cases. Some examples are the definition of the objective max_x E_r [f(x, r)] in sect.2 (with f an arbitrary measurable function), or of max_x min_P instead of sup_x inf_P in eq.2 (which AFAICT allows substitution of any phi-divergence?!). Can you please either switch to sup and inf or provide a justification for using the min and max operators where the existence of an optimum is not obvious?\n\n- Below eq.2 in the sentence starting with citation of Pardo (2005), is N supposed to be n? Also, I am assuming phi^{(2)} stands for the second derivative of phi (?), which should be probably better to state explicitly. Also, AFAICT you have only assumed “phi” is convex which implies differentiable almost everywhere, but does not imply phi is twice differentiable. Is this discussion restricted to the two choices for the phi function from tab.1? Please clarify.\n\n- Also in the paragraph below eq.2, you use the term “center distribution” likely to what you were referring to before as the “nominal distribution”. Perhaps it would make it easier for the reader if you chose to stick with single name?!\n\n- Please consider explicitly stating delta > 0, epsilon > 0 (or similar according to the intended use) somewhere around eq.3.\n\n- In the paragraph under eq.3, I cannot see where the abbreviation “VaR” was introduced?!\n\n- In the proof of thm.3.1, it would have been nicer if you explicitly commented on incorporating the L >= 0 constraint (which you eventually introduce with the convex conjugate of phi), and why dividing by eta_2 is ok in the second unnumbered equation after A.2. A subtler point is that by introducing L, you are implicitly assuming P is absolutely continuous w.r.t. P_0 which is in principle implied by the fact you are taking infinum over a phi-divergence ball of size rho (which only contains distributions that satisfy this condition), but never discussed explicitly.\n\n\nReferences:\n\n[1] A. Rényi, “On measures of entropy and information,” Fourth Berkeley symposium on mathematical statistics and probability, vol. 1, 1961.\n\n[2]  S. Amari, Differential-Geometrical Methods in Statistics. New York: Springer, 1985."
        }
    ]
}