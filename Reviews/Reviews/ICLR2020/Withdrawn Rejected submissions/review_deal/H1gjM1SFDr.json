{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "In this paper, the authors propose a new subspace clustering algorithm that extends L0 sparse subspace clustering (Yang et al., 2016). The main difference is that the proposed algorithm supports the noisy data situation, which is achieved by minimizing a distance-based similarity measure and L0 norm. The authors prove that the proposed L0 SSC has subspace detection property. Next, analyses on the correctness of deterministic case, randomized case and the scenario of dimensionality reduced data are given in the paper.\n\nIn Table 1 and Table 2, the experiments show that the proposed model outperforms baselines on three datasets when evaluated by accuracy and normalized mutual information.\n\nIn this paper, the authors proposes a natural extension to Eq. (1) that allows the clustering model to optimized on noisy (real) data. The paper has a comprehensive analysis for the model, which is a big plus for me. However, my major concern is in the empirical evaluation, especially I'm observing that the comparison with (Yang et al., 2016) is not presented. Addressing  this question may change my judgement.\n\n1) With a quick scan of Yang's paper (L0-sparse subspace clustering), I see they are also performing experiments on same datasets. I can't understand why not also compare with their results in Table 1 and Table 2. Showing the advantage of the proposed model over L0-SSC and AL0-SSC in Yang's paper can be important for readers to use your model. (For example, if the quality of clusters are similar, then you can put focus on optimization speed).\n\n2) You mentioned that you propose a noisy version of L0-SSC, which I believe referring to Eq. (2). (Please correct me if I'm wrong.) However, Eq. (2) in your paper is almost identical with Eq. (7) in Yang's paper.\n\nMinor question and comments:\n\n1) I searched for the number of clusters you used for the experiments, but can't find it. How many clusters did you use? How does this number affect the results?\n\n2) You can just merge Table 1 and Table 2. Then you can add more contents to experiment section and conclusion."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper studies the L0-Sparse Subspace Clustering (L0-SSC) of high dimensional noisy data. The goal of L0-SSC is to cluster the data according to their low-dimensional subspace structure by minimizing the number of the nonzero elements of a matrix Z such that X = XZ for data matrix X.\n\nCompared to the existing work of L0-SSC, this paper proposes noisy L0-SSC to handle noisy data. Moreover, it proposes Noisy-DR-L0-SSC to improve the efficiency of noisy L0-SSC by projecting the high-dimensional data onto a low-dimensional space by random projection and then performing noisy L0-SSC on the obtained low-dimensional data. The correctness of both noisy L0-SSC and Noisy-DR-L0-SSC is proved by showing that the subspace detection property holds. The idea of using random projection to improve the efficiency of noisy L0-SSC seems interesting, and the experimental results support the correctness of noisy L0-SSC and Noisy-DR-L0-SSC by demonstrating that the proposed methods achieve the promising result. Some questions are as below:\n\n1 Please include mean and deviation in the results of Noisy-DR-L0-SSC in Table 1 and Table 2. Because random projection is used for dimension reduction, mean and deviation are expected in the results to avoid performance fluctuation due to randomness. \n\n2 Can the authors show the performance of Noisy-DR-L0-SSC with different p-values (i.e. different dimensions after random projection)?\n\n3 Can one try sparse random projection in the framework of Noisy-DR-L0-SSC so as to further improve its efficiency? For example, does the correctness of Noisy-DR-L0-SSC still hold if we use OSNAP (Jelani Nelson and Huy L. Nguyen. OSNAP: Faster numerical linear algebra algorithms via sparser subspace embeddings. FOCS 2013)? If does, the contribution would be more sufficient.\n\n4 L0 minimization problem is an NP-hard problem, so in general case, the Proximal Gradient Descent (PGD) in Section 5 only renders an approximate solution to the optimization problem of noisy L0-SSC and Noisy-DR-L0-SSC. It could be better if the authors can provide a bound between the approximate solution by PGD and the optimal solution so that there is a guarantee that the gap between the approximate solution and the optimal solution is bounded. \n\n5 Since the correctness of noisy L0-SSC and Noisy-DR-L0-SSC are proved in terms of the subspace detection property, it could be better if a measure directly related to the subspace detection property is used to measure the performance of the proposed methods.\n\n6 In the randomized analysis of noisy L0-SSC, the assumption is adopted, i.e., data in subspace are i.i.d. isotropic samples. In Remark 5, this assumption is regarded to equivalent to the assumption of data are i.i.d. samples uniformly distributed on the unit sphere up to a scaling factor. The scaling factor, i.e. square root of d_k, can be easily removed in the current analysis of noisy L0-SSC so that almost the same correctness of noisy L0-SSC can still be obtained under the assumption that data are i.i.d. samples uniformly distributed on the unit sphere. The assumption of the uniform distribution on the unit sphere has been widely used, e.g. in L1-SSC Soltanolkotabi & Cands (2012) and noisy L1-SSC Wang & Xu (2013). Slightly reformatting the current results of L0-SSC under randomized models and presenting the correctness of L0-SSC under the assumption of the uniform distribution on the unit sphere can make a comparison to existing works easier. \n\n7 C_{p,p_0} in equation (31) is defined in the appendix (equation (55)). Please define it before it is used.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper propose noisy  -SSC and Noisy-DR- -SSC to handle noisy data and dimensionality reduced data so as to improve the robustness of sparse subspace clustering methods. The authors conduct a complete and detailed theoretical analysis, and the results provide theoretical guarantee on the correctness in terms of SDP on noisy data. \nPositive:\n(1) Under both deterministic model and randomized models, this paper presents noisy  -SSC , which enhances  -SSC by theoretical guarantee on the correctness of clustering on noisy data.\n(2)  Moreover, this paper presents Noisy-DR- -SSC, the method improves the efficiency of the basic algorithm, and provably recovers the underlying subspace structure in the original data from the dimensionality reduced data under deterministic model. \nNegative:\n(1)\tThe authors conduct a complete and detailed theoretical analysis, but the description of the algorithm is not clear and the optimization process is not detailed enough.  It is very difficult to understand it.\n(2)\tThis paper proposes a noise model based on Yang et.al (2016), the objective function and the optimization process are already available, therefore, its innovation is not clear in this paper. \n(3)\tThe results of experiments demonstrate the effectiveness of noisy  -SSC and Noisy-DR- -SSC, but this paper not only lacks the description of experimental data, but also lacks the large-scale experimental data. In addition, the experiment is inadequate in this paper, it is suggested to add experiments to analyze the influence of noise ratio on the experimental results. It is hoped that the author will give the parameter analysis and convergence analysis of the algorithm in an experimental way.\n"
        }
    ]
}