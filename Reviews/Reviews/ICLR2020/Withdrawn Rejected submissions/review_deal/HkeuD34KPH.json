{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper proposes to improve sequential recommendation by extending SASRec (from prior work) by adding user embedding with SSE regularization.  The authors show that the proposed method outperforms several baselines on five datasets.\n\nThe paper received two weak accepts and one reject.  Reviewers expressed concerns about the limited/scattered technical contribution.  Reviewers were also concerned about the quality of the experiment results and need to compare against more baselines.  After examining some related work, the AC agrees with the reviewers that there is also many recent relevant work such as BERT4Rec that should be cited and discussed.  It would make the paper stronger if the authors can demonstrate that adding the user embedding to another method such as BERT4Rec can improve the performance of that model.  Regarding R3's concerns about the comparison against HGN, the authors indicates there are differences in the length of sequences considered and that some method may work better for shorter sequences while their method works better for longer sequences.  These details seems important to include in the paper. \n\nIn the AC's opinion, the paper quality is borderline and the work is of limited interest to the ICLR community.  Such would would be more appreciated in the recommender systems community.  The authors are encouraged to improve the paper with improved discussion of more recent work such as BERT4Rec, add comparisons against these more recent work, incorporate various suggestions from the reviewers, and resubmit to an appropriate venue.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposes SSE-PT for sequential recommendation, which is an extension of previous work SASRec by adding user embedding with SSE  regularization [Wu et al. 2019] . They further extend SSE-PT to SSE-PT++ to handle longer sequence. Experiments on five datasets show that the SSE-PT and SSE-PT++ outperform several baseline approaches.\n\nDetailed comments:\n\n1)\tThe technical contribution seems to be scattered: user embedding is introduced, effect of different types of regularization is studied and sampling based approach is added to address long sequence. It could be better if the author could make clear what the major contribution of this paper is. Also, SSE [Wu et al. 2019] is existing technique and simply applying it to sequential recommendation is a bit incremental.\n\n2)    In addition to SASRec, there are some other transformer based model (e.g., [1]) for sequential recommendation and the paper discuss how the proposed method differ from them.\n\n3)\tIn SSE-PT++, would sampling start index v based on the recency (e.g., with exponential decay) make more sense than uniform probability?\n\n4） Overall, experiments look comprehensive: The baseline methods include both non-deep-learning methods and recent deep learning based methods for sequential recommendation; ablation study is conducted; case study is performed on MovieLens to show how the attention weights differ from SASRec; running time is compared against baselines and sensitivity analysis on hyper-parameters are also provided. \n\n\nTo summarize, the paper is a bit incremental/scattered in terms of technical contribution but the execution of this paper looks solid. I would give a “weak accept” to this paper given the reasons listed above.\n\n\n[1] F. Sun et. al. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "The manuscript proposes SSE-PT, a sequential recommendation model based on transformer and stochastic shared embedding (SST). Experiments on several datasets show that SSE-PT outperforms a number of baseline methods. Some analytical results are also provided. Overall, I think this work is not suitable for ICLR due to following reasons. \n\nThe novelty of this work is limited. This work is based on SASREC [W Kang, ICDM2018] and uses transformer to encode user-item interactions in sequential manner. The difference is that this work adds user embedding in bottom layer and utilizes SSE for regularization as well as designs SSE-PT++ by sampling. To me, there is little extension or novelty. \n\nThe experiment results are not convincing. Most of results are copied from [W Kang, ICDM2018] except HGN in Table 1. Table 1 shows SASREC is much better than HGN [C Ma, KDD2019]. However, I checked the results in HGN paper and found HGN is much better than SASREC. Even though datasets are different, most of them are from Amazon data. I was not convinced by this result due to the large difference. In addition, I did not understand why the authors change evaluation metrics in Table 3, i.e., from NDCG/Recall@10 to NDCG/Recall@5. I found SSE-PT without regularization and with different regularizations are much worse than the best result, which makes me concern about the effectiveness of personalized transformer. I did not see ablation study or discussion about this. \n\nUpdate: I have considered author rebuttal. I appreciate the extensive hyper-parameter sensitivity and ablation study in the paper, while these cannot be a key factor in evaluating paper as most of them can be done easily. I main concerns still lie in the novelty and experimental results. I still think this work is not suitable for ICLR and I keep my score. ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review": "In this paper, the authors study an important recommendation problem, i.e., sequential recommendation, and design a novel and improved model called SSE-PT (Stochastic Shared Embedding - Personalized Transformer). Specifically, the authors mainly follow the previous works of the Transformer model and the stochastic shared embedding (SSE) regularization technique. For the part of the personalized transfer (PT), the authors introduce the user embedding for each user $i$, i.e., $u_i$, shown in Eq.(2) and illustrated in Figure 1. For the part of regularization, the authors find that the SSE technique works well in terms of avoid overfiting in context of other regularization techniques.\n\nExtensive empirical studies on five datasets show the effectiveness of the proposed approach compared with other related methods.\n\nOverall, the paper is very well presented, in particular of the introduction and discussion about the related works, and the analysis of the experimental results. \n\nMy major concern is that the technical novelty is somehow limited in terms of the two closely related works of Transformer and stochastic shared embedding (SSE). I thus recommend weak acceptance.\n\nSome suggestion: Some important baseline methods may be included to make the results more convincing, e.g., Fossil, MARank, and/or BERT4Rec.\n\nSome minors:\nTypo: in the paragraph below Eq.(3): user $l$ -> user $i$\nTypo: FPMF, PFMC in different places\n"
        }
    ]
}