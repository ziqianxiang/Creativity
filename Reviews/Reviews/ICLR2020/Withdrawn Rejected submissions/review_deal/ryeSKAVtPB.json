{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposed using a technique called adversarial distillation for crafting adversarial examples targeting the top-k predictions instead of the commonly used top-1 prediction in the literature. The authors generalized the Carlini-Wagner (CW) attack loss to the top-k setting and use it as a baseline comparative method.  Given the same number of iterations in the attacking algorithm, the experimental results on ImageNet show either better attack success rate or lower distortion measured in L1, L2 and Linfinity norms.\n\nAlthough studying adversarial attacks beyond top-1 prediction is interesting, I have several concerns about this submission.\n\n1. Novelty and contributions: In terms of technical contributions, generalizing CW loss from top-1 prediction to top-k prediction is rather straightforward. What is most interesting from the submission is that using the proposed adversarial distillation technique + top-k CW attack loss seems to make the attack more effective. But the authors did not provide a satisfactory explanation on why adversarial distillation can give a better attack. Most importantly, what can the research community learn from this attack? Will adversarial training with it gives more robust model?  The contributions seem limited without discussing and providing new insights in improving adversarial robustness.\n\n2. Lack of justification for the smaller perturbations: Since the proposed attack is basically the top-k CW loss + the adversarial distillation regularization loss, it is counterintuitive to see the perturbation norms of CW attack are consistently larger than the proposed attack, even for the L2 norm. Intuitively, adding an additional regularizer to the objective function will incur a trade-off. In this case, larger perturbation norm of the proposed should be expected. Please clarify and justify why the proposed approach will produce smaller norms.\n\n3. Are attack iterations sufficient? Based on Table 2, it's apparent that by increasing the number of iterations per each binary search, the performance of CW attack and the proposed attack becomes much closer. Can the authors provide more results with increased iterations and more binary search steps? I wonder there may be no performance gaps (or CW attack can perform better) if we increase the number of iterations and binary search steps. \n\n4. In page 2, the statement \"But, the three untargeted attack approaches are much better in terms of pushing the GT labels since they are usually move against the GT label explicitly in the optimization, but their perturbation\nenergies are usually much larger.\" is unclear to me. Are the authors suggesting untargeted attacks will have larger perturbations than targeted attacks? If so, the conclusion does not make sense."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The authors consider the problem of generating adversarial attacks on image classifiers. Going beyond the standard setting (where the goal is to flip the output label for any given input into an incorrect one), the authors propose to design attacks such that the output top-k predicted labels are randomly selected and ordered (and exclude the correct label).\n\nTwo attack strategies are proposed: one being an extension of the Carlini-Wagner (CW) attack, and one being an adaptation of the distillation approach of Hinton et al that \"hides\" information in the probability distribution corresponding to lower-ranked logits.\n\nThe paper makes an interesting first step in a new direction within the area of adversarial attacks. Unfortunately the paper is very confusingly written and requires a thorough revision, which I fear may be beyond the scope of the rebuttal period.\n\nFor one, neither attack algorithm is clearly described, leaving the reader to guess most of the details; clear pseudocode descriptions would help (since they are new after all). In particular, the \"knowledge-based adversarial distillation framework\" remains completely mysterious to this reviewer. P^AD seems to be...hand designed by manipulating Glove embeddings of category labels?? [If so, then what should one do if there is a completely new application where the labels do not have regular meaning?]\n\nMoreover, the experimental setup is confusing. In multiple places (starting from page 4 near Eq (5)) the authors refer to compute budgets such as \"9 x 1000\", which this reviewer interprets to be 9 steps of binary search over lambda with 1000 gradient steps over the relevant loss function. But how can this be a reasonable measuring stick when (a) the range of the binary search is never defined, and (b) the quality of the optimization not only depends on the number of steps, but also on other quantities such as the stepsize? Moreover, why should binary search over lambda be a good strategy at all? Since all the results are based on this setup I do not know how to evaluate the strength of the proposed results. "
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1245",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposes to learn top-k attacks, instead of commonly used top-1, where top-k predicted labels of an adversarial example are k (randomly) selected and ordered labels (the ground-truth label is excluded). The authors propose two attack methods for this problem: (i) extend CW attack (baseline)  (ii) use KL divergence between a manually designed logit layer adversarial distribution and the logit layer output of the current sample as the new loss replacing the loss in the objective function of the extended CW attack. They evaluate their proposed approach on RESNET-50 and DENSENET-121 on a subset of ImageNet under different k and targets.\n\nMain weaknesses of the paper:\n- Motivation is not clear. The author claims that the problem to be novel but does not give convincing arguments why do we want to study ordered top-k attacks.\n- Evaluation is far from being comprehensive. I’d like to see some results on other datasets.\n\nOther issues:\n- Using cosine distance between glove embeddings of two label names to estimate semantic similarity between the two classes (i.e. s(a,b) in Eq(9)) does not seem to be appropriate since there is no empirical evidence that even though two label names are semantically similar, this does not mean that their visual images are similar.\n- I’d like to see more intuitions from the authors on why AD performs better than the extended CW-attack.\n- l2 energy is not explicitly defined.\n\nThings to Improve that did not affect the score:\nThe paper has lots of typos/grammar errors. To just name a few:\n- 2nd to last line “by large margin” should be “by a large margin”\n- The abbreviation of ASR is directly used without being explicitly defined.\n- 3rd to last line on page 2 “diffoculty” should be “difficult”.\n"
        }
    ]
}