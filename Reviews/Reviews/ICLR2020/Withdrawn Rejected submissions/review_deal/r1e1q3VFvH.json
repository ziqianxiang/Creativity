{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Summary: In order to quantize and compress DNNs the paper proposes to use a pipeline consisting of 3 steps. First it uses partial L2 regularization to entirely prune some small weights. Second (most important novel contribution here) it introduces  a mean square error quantization regularization term into the loss being minimized *and* the weight of this regularization term is also learned via gradient descent. Third, it uses lossless compression to further compress the memory footprint of the model. Experimental results when applied on ResNet-18, MobileNet V1 and ShuffleNet models indicate that the proposed method does a decent job of compressing the models without losing too much accuracy despite various levels of pruning/quantization. \n\nEvaluation/suggestions\nUnfortunately the paper does not bother mentioning (in previous work section) or comparing against several recent state of the art results from earlier this year which all seem to do even better. Note that they dont use lossless compression or pruning but those are not the major contributions of this paper and can be easily added to those methods as well. I suggest describing in related works section and a detailed comparison against at least the following (these are the latest , building on a large literature that goes back many years; I picked them since they already achieve better results than those provided here so I feel the authors may benefit from this). \nhttps://arxiv.org/pdf/1902.08153.pdf \nhttps://arxiv.org/pdf/1903.08066.pdf \nhttps://arxiv.org/pdf/1905.11452.pdf \nhttps://arxiv.org/pdf/1905.13082.pdf\n\nNovelty: The novelty of this paper is not very small, but not as much as the authors seem to indicate by ignoring the above recent literature. For example the work on \"differentiable quantization\" above also optimizes quantization as part of the gradient descent. I work still feel that the present paper is novel enough to be interesting but the authors really need to carefully explain their contributions in context. \n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper presents a regularizer that can quantize weights to certain bits. To further reduce the dis-match between float weights and quantized weights, a method that can adaptively change lambda is proposed. \n\nThe proposed method is simple and intuitive (while the method is incremental, I am fine with the novelty). My main concerns are on experiments:\n\nQ1. Why many other methods are not compared in experiments?\n- For example, \"Loss-aware weight quantization of deep networks. ICLR 2017\", \"ProxQuant: Quantized Neural Networks via Proximal Operators. ICLR 2019\"\n\nQ2. Why not perform comparison on more advanced networks?\n- EffcientNet, i.e., \"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. ICML 2019\" can achieve much higher accuracy than MobileNet and SuffleNet. \n  EffcientNet Top-1 accuracy 80.3%.\n  Experiments on these networks are more meaningful.\n - Another architecture can be considered is \"Darts: Differentiable architecture search\", with 73% accuracy and is also higher than MobileNet and SuffleNet. "
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper presents an approach for quantizing neural networks, in order to make them more efficient. Quantization is difficult because training a quantized net can lead to gradient mismatches that impair accuracy. In this paper, the authors circumvent this difficulty using a three step compression approach that relies on retraining with regularization. First, they prune the synaptic connections of a pre-trained un-compressed network after retraining with an additional L_2 regularizer. Second, they retrain the network with a regularizer that uses the mean-squared error between the weights and their nearest quantized value. Third, they re-train the network with a regularizer that uses the mean squared error between the activations and their nearest quantized value. In some cases they also then use a final compression algorithm to render the quantized network as compressed as possible. They show that the accuracy losses on ImageNet associated with this technique are not too bad, and result in comparable or even better accuracy when compared to the TensorFlow 8-bit quantization, even when using less than 8-bits. \n\nUltimately, this is a good paper. It was easy to read and the ideas were sound. The major thing it is missing, in my opinion, is some analysis, particularly:\n\n1) How bad is the impact on the gradient calculations of this technique compared to other approaches? This is ultimately a key claim in their paper, that they do a better job of following the gradient. But it is not directly demonstrated, only indirectly shown via good accuracy. An explicit comparison in the bias on gradient calculations between their technique and other techniques for quantization would be ideal. One possibility is to do something like show the angle between the weight updates given and the true gradient, for the different approaches. A direct demonstration of this central claim is an important missing factor in this paper.\n\n2) Can any analytical guarantees, or even estimates, be given on the accuracy drop in the event of the gradient being well-followed? It would be nice to know how badly the quantization would affect performance even in the best case scenario, for comparison sake.\n\n\nMinor items:\n\n- Figure 2 doesnâ€™t add much. It could be removed to make space.\n\n- It would be easier to see what is going on in Figure 5d if the y-axis range were reduced and the peak clipped. As it stands, it looks like all the weights are zeroed out.\n\n- Why is Figure 6 presented as a bar graph rather than a table? "
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Summary: The authors propose to compress deep neural networks with learnable regularization. Instead of treating the coefficient of the regularization as a fixed hyper-parameter, the proposed method learns the coefficient by adding additional term into loss function. Moreover, the authors apply the proposed method to network quantization and pruning. Experimental results on ImageNet demonstrate the effectiveness of the proposed method. However, the novelty and contribution of this paper are limited. More explanation and experimental results are required. My detailed comments are as follows.\n\nStrengths:\n1.\tThe authors propose a learnable regularization method to learn the coefficient of the regularization. \n\n2.\tThe authors apply the learnable regularization to network quantization, which reduces the mismatch between the full-precision weights and quantized weights.\n\n3.\tThe authors apply the learnable regularization to network pruning, which encourages the weights below the threshold to move towards zero.\n\nQuestions and points needed to be improved:\n1.\tThe novelty and contribution of this paper are limited. The proposed method only introduces an additional term into loss function to control the coefficient of the regularization. The key idea of the proposed method is similar to gradually increasing the coefficient manually during training. However, the authors do not compare the proposed method with such baselines. Moreover, the proposed learnable regularization introduces additional hyperparameter $\\alpha$, which contradicts the motivation of learnable regularization. \n\n2.\tSome method details are missing. For example, how to set the initial values of $\\lambda$ and $\\delta$? This is very important at the beginning of quantization.\n\n3.\tIn Table 1, why the performance of the full-precision ResNet-18 (Top-1 Accuracy: 68.1%) is worse than the one (Top-1 Accuracy: 69.6%) in LQ-Net [1].\n\n4.\t$\\alpha$ in Equation (5) plays an important role in controlling the regularization term. However, the authors do not provide any ablative study on $\\alpha$.\n\n5.\tExperimental results are not sufficient. More recently quantization methods should be considered in the comparison, including LQ-Net [1], Distillation [3], and QIL [4]. \n\n6.\tIn Table 4, the comparison with DoReFa-Net [2] is unreasonable. Compared with the fixed scaling factor in DoReFa-Net, the proposed method learns the scaling factor during training, which can improve the performance of the quantized model. \n\n7.\tQuantization results of SR models on Set-14 are not sufficient. More results on Set5, BSDS100, Urban100, and Manga109 are required.\n\n8.\tFigure 4 should be clear. The legend in the figure is too small to read. \n\n9.\tFigure 7 is confusing. Activations quantization does not change the network size. However, the size of the network with 8-bit activations quantization is larger than the one without activations quantization.\n\nReferences:\n[1]\tZhang, Dongqing, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. \"Lq-nets: Learned quantization for highly accurate and compact deep neural networks.\" In Proceedings of the European Conference on Computer Vision (ECCV), pp. 365-382. 2018.\n[2]\tZhou, Shuchang, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. \"Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients.\" arXiv preprint arXiv:1606.06160 (2016).\n[3]\tZhuang, Bohan, Chunhua Shen, Mingkui Tan, Lingqiao Liu, and Ian Reid. \"Towards effective low-bitwidth convolutional neural networks.\" In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7920-7928. 2018.\n[4]\tJung, Sangil, Changyong Son, Seohyung Lee, Jinwoo Son, Jae-Joon Han, Youngjun Kwak, Sung Ju Hwang, and Changkyu Choi. \"Learning to quantize deep networks by optimizing quantization intervals with task loss.\" In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4350-4359. 2019.\n"
        }
    ]
}