{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary:\nThe paper proposes an explanation method by interpolating examples in the latent space of a generative model that can sample from the data distribution. In particular they interpolate between two samples (called litigation examples) that have the right labels according to the blackbox. The explanations primarily consist of the examples sampled along the interpolation path. The main argument is that by optimizing the encoder and decoder w.r.t. litigation examples, the gradient of logit estimates change is the sharpest. \n\nI have several concerns with this approach. First it is unclear what the explanation is supposed to represent in the first place. \n\n1. For instance, what insight about the classifier should we obtain from these interpolations? \n2. Only one classifier is shown for the celebA data so it is hard to compare different black-boxes in the evaluation. \n3. Is it fair to change the explanation paths (by optimizing the VAEs) to reflect explanations that more closely reflect \" a human standpoint\" as opposed to finding the shortest path that would change the label even if it has no semantic clarity for the human? What kind of bias are we inducing and are we suggesting the classifier is more reliable than it actually is by using a PATH-Auto Encoder as opposed to a vanilla VAE without the litigation examples? \n4. The related work is incomplete. This whole domain is by now well studied under counterfactual explanations, or contrastive examples - see: \na) https://arxiv.org/abs/1802.07623\nb) https://openreview.net/forum?id=HyNmRiCqtm\nc) https://arxiv.org/abs/1806.08867\nd) https://arxiv.org/abs/1711.00399\n\n5. Baseline comparisons: Comparing different VAEs is not sufficient to justify why this explanation method is a good choice. Comparisons with adversarial examples, any of the counterfactual explanation methods above and a quantitative measure of comparing across them seems crucial for a fair assessment of the contribution."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper addresses how to extract human-explainable reasons for particular\noutputs from a black-box classifier that provides class probability outputs.\nAutoencoders provide a smooth (stochastic) path between contested to\nuncontested input. The autoencoders and path are optimized together to find an\nexemplary sequence of inputs in latent space that quickly change the output\nclass from the contested to uncontested label.  Localizing the classification\nchange region to a few inputs can help humans demonstrate or explain why a\nblackbox neural net made a contested classification.  The input space path\nwhere meaning rapidly changes once are called \"semantic paths\".\n\nThe mathematical presentation is quite general, finding semantic paths by\nminimizing a line integral of a Lagrangian.  Some of the papers most\ninteresting sections are in the appendix, where several possible Lagrangians\nare presented.  The main body of the paper presents 2 simple examples (more in\nappendices) showing examples where one can find a few images that visually\nallow one to \"explain\" the reason for a misclassification.\n\nSections 1--4 frame the problem and its potential legal scenario nicely.\nSections 5--6 present the Lagrangian formalism, other loss function components\nand the optimization procedure quite nicely.\n\nHowever, the experimental results section, 7, was difficult for me to follow.\nI did not understand the point about random litigation end pairs trying to\narrive at \"this specific path\".  Fig. 4 was not really understandable.  Which\nalgorithms corresponds to \"our saliency maps\" (vs. traditional \"saliency\"\nmethods)? And names of traditional saliency methods appear nowhere else in the\npaper, so need to be identified and merit at least a reference (perhaps Adebayo?).\n\nWhile the problem setting, general approach and theory is interesting and seems\nsound, the Results section must be clear for this paper to be accepted.\n\n---- Misc comments: ------\n\nI read, but did not verify proofs line by line.\n\nThe authors punt on how one might determine x_T --- some discussion, even if\nonly in the appendix, might be nice.\n\nThe paper describes stochastically generated images. For legal purposes can I\nselect a single unsampled preimage? Are VAE results significantly different\nfrom plain AE? Considering plain AE might also remove some complications\nfrom presenting the core idea of calculating a path-based loss.\n\nFigures displaying the autoencoder outputs might be improved by also providing\nthe original t=0 and t=1 images.  Fig. 2, for example, shows different t=1\nsevens.  I wondered which one most closely resembled the original.\n\n- \"the roll\" --> the role\n- Monotonous means boring, dull, tedious. So I would find some other way to\n  describe your \"close to monotonic\" functions, in Sec. 6 and the appendix D.4\n- Grad CAMP --> Grad-CAM\n- \"a randomized layers\"\n- Sec 8: \"are also provided\" --> \"have been performed\" (?)\n- A. Appendix seems empty.\n\nThe point in section 8 about finding a path along the input data manifold is\ngeneral, so think whether it might be better in Section 4 where the contrast\nwith off-manifold paths in adversarial training is made.\n\nI found appendices D.3--D.5 with examples of path loss functions quite interesting.\n\n\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Summary:\nThis paper proposes to explain how a classifier makes decision by interpolating and sampling paths in latent space by auto-encoding frameworks. They propose a method providing series of examples highlighting semantic differences between decisions. They formalize the notion of a semantic stochastic path and introduce semantic Lagrangians. Experiments are conducted on MNIST and CelebA, with probability paths showing auto-encoding explanatory examples.\n\n\nMy main concern with the paper is that it does not sufficiently address why it is important to generate auto-encoding examples in the way they do. I'm not convinced that these path examples are enough explanatory or represent the decision of the black-box model with sufficient fidelity. I think the paper would be clearer if the importance and applicability of their methods were explicated. I'd suggest rejecting for now, though be open to changing this assessment.\n\nI find the experimental results hard to interpret and support the claims. Below is a non-exhaustive list of examples.\nFigure 2 & 6: The paper gives interpretations on different patterns presented in the figures. How can one access the fidelity of the plausible explanations?\nFigure 5: I suppose 'interpolation' is for the proposed method. How does it imply that 'the proposed saliency map decorrelates with the randomized version' compared with other lines?"
        }
    ]
}