{
    "Decision": "",
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "1. What is the specific question/problem tackled by the paper?\n\nThis paper proposes to improve the soft attention mechanism of Transformer models by making them more interpretable. The authors motivate this work by citing the work of [1] where it is shown that simply interpreting a higher attention weight to be \"more important\" doesn't hold up for biLSTM models with attention. To address this problem, this paper proposes a discrete/hard attention mechanism by using the Gumbel-Softmax trick to propagate gradients through the hard decisions. This model is called the Single Stream Discrete Transformer model. The authors then propose an extension to this model that combines regular soft attention together with the hard attention model which is dubbed the Two Stream Discrete Transformer model. The authors report results on two tasks 1) machine translation on the WMT and IWSLT datasets 2) language modeling on a synthetic/toy dataset. On the translation datasets results are shown to be slightly worse compared to the soft attention model.\n\n2. Is the approach well motivated, including being well-placed in the literature?\n\nI am not convinced by the motivation of this work. The authors motivate their work by saying that soft attention is not interpretable, however I do not see any empirical evidence or analysis of their proposed hard attention mechanism as being more interpretable. Moreover, the work they cite to argue that soft-attention is not interpretable [1] bases their analysis on a biLSTM model and not a Transformer. Moreover, the work of [1] simply computes adversarial attention weights to fool the model -  such adversarial methods can also be extended to hard/discrete attention weights (see e.g. the following work [2] which showed how to attack discretized models which are not any more robust than continuous models).\nI am also a bit confused about the motivation behind the Two Stream Discrete Transformer - what is the point of the hard attention if you also use regular soft attention along with it?\n\nIn my view the only reason to motivate sparse attention models would be the following: 1) attention over long sequences where full attention does not scale (this is the motivation behind [3] for example) and 2) better generalization than full/soft attention. Clearly the work doesn't tackle 1) and the experimental results do not show better generalization than full/soft attention.  \n  \n3. Does the paper support the claims? This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous.\n\nThe paper does not support the claim that using hard attention is more interpretable, since I don't see any such empirical evidence presented in the paper. Moreover, their hard attention model will also be susceptible to the gradient based attack of [1] using ideas from [2]. Their experimental results are also limited and do not support either the model being more interpretable, or generalizing better or being able to scale better than soft/full attention.\n\n\nTypos & corrections (my comments delineated by *):\n- we can even the guarantee that any hidden state does not depend on input elements not being directly attended to. *extra the*\n- The structure of hard attention ensures that the receptive field is defined by the recursion r(i, l) = r(i, l − 1) ∪ r(z^(l)_i, l − 1) where r(i, 0) = {i} and z^(l)_i is the hard sample taken at layer l for position i. *This should be r(i, l) = r(i, l − 1) ∪ r(z^(l)_i, l)* \n- The first layer representations are set to the corresponding word embedding, g^(0)_i = e_g(x_i) and h^(0)_i = e_h(x_i). *What is e_g() and e_h()? They don't seem to be defined anywhere. Are they learned parameters?*\n \nMissing citations:\nThe works of [3, 4, 5] are relevant and should be cited.\n\nReferences:\n[1] Attention is not Explanation by Jain et al (https://arxiv.org/abs/1902.10186)\n[2] Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples\n by Athalye et al (https://arxiv.org/abs/1802.00420)\n[3] Generating Long Sequences with Sparse Transformers by Child et al (https://arxiv.org/abs/1904.10509)\n[4] Look Harder: A Neural Machine Translation Model with Hard Attention by Indurthi et al (https://www.aclweb.org/anthology/P19-1290/)\n[5] Sparse and Constrained Attention for Neural Machine Translation by Malaviya et al (https://www.aclweb.org/anthology/P18-2059/)\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes a Discrete Transformer. There are three main contributions (1) a discrete and stochastic Gumbel-softmax based attention module (2) a two-stream syntactic and semantic transformer and (3) sparsity regularization.\n\nThis paper makes some valid technical contributions but the key question I find myself asking is “why?” Performance results do not outperform regular Transformers and at best perform marginally/comparable. In this case, is there any reason to use the proposed method over the existing methods? \n\nThe motivation for each component proposed is quite unclear and vague. \n\nThe authors enumerate several use-cases (to check for bias?) in the conclusion. Assuming that these use-cases are valid, this is completely under-explored in this paper and should have fallen within the scope of the paper and not as cursory remarks in the concluding statements.\n\nPerhaps the most confusing thing is why is there a need to separate into syntactic and semantic streams? What do these terms mean in this context? I am aware that the authors cite Russin et al. but it is rather unclear how and why they incorporate these intuitions/motivation into their work. I think this work can be more self-contained instead of dropping a reference.\n\nAs of now, I am fairly unconvinced with the premise of this work. There are occasional mentions of interpretability but no resolution or any concrete statements to address if the proposed model is supposed to ameliorate the issue at hand. \n\nToo many things are going on in this paper, for reasons unclear except to maintain performance and to prevent degradation. There seems to be also nothing tangible to be gained.\n\nI would love to hear from the authors and will be glad to increase my score, even significantly, to a positive one if the authors can actually provide a convincing and compelling argument for their model architecture. As of now, the technical exposition is extremely confusing to readers. \n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "\nSummary: \nThis paper presents two modifications to the standard transformer architecture with the goal of improving interpretability while retaining performance in NLP tasks. The first modification is to switch from soft attention to hard attention using a Gumbel-Softmax, which forces the model to attend only to important parts of the input without spreading small amounts of attention throughout. The second modification is to split the model to have two separate streams -- semantic and syntactic -- where the syntactic stream uses a soft attention mask to define the distribution that the hard attention in the semantic stream is sampled from. Hard attention also allows the size of the receptive field to be used as a regularizer, which can further encourage sparsity in the attention map, as shown in the paper. Experiments are performed on a synthetic language dataset and on machine translation. The proposed model achieves similar performance to baselines despite the use of hard attention, and the embeddings extracted from the model perform better than the soft-attention transformer embeddings on a chunking task.\n\nOverall: Reject.\nOverall, I feel that this paper splits the gap between interpretability and a novel contribution and fails to truly satisfy either. If the goal is better interpretability, then there should be a stronger focus on that and proper comparisons with other methods for interpretability. Conversely, if the goal is to provide a new and effective contribution, then more experimental evaluation and more of a contribution is required. In either case, there should be better empirical comparison of other hard-attention mechanisms. I also found the paper somewhat unclear and poorly motivated, particularly the later sections. I still am unclear why the split into the two streams occurred or if it is useful beyond increasing the number of parameters in the model. I do like the use of hard attention and the Gumbel-Softmax for interpretability purposes, however.\n\nClarity: Fairly poor. Notation is lacking at times, and a number of steps and concepts are not well motivated and not well described.\nSignificance: Low. I do not think that the contributions of this paper will have a significant impact.\n\nDetailed questions and comments:\nSection 2.\n- A quick internet search shows a number of other papers that use Gumbel-Softmax-based hard attention, but there is no mention of these papers in the related work (e.g., [1]). Given that the use of the Gumbel-softmax for hard attention is introduced as a novelty here, the comparison to previous work should be more detailed and comprehensive.\n\nSection 4.\n- What are p(y|x) and p(y, z|x) referring to? What are y, x, z here?\n\nSection 5.\n- What is the motivation for splitting into these two streams? \n- Doesn’t this just (roughly) double the number of parameters in the model? \n- Why do different embeddings need to be used for the two streams?\n- Doesn’t the use of soft attention in the syntactic stream negate the interpretability benefits claimed before for hard attention?\n- How does the semantic stream consist of a “fixed” network if the syntactic stream specifies what to attend to? Why would the generated syntactic distribution be fixed?\n- Does the syntactic stream encode syntactic information? What about the architecture implies that it will?\n- Does the semantic stream encode semantic information? What about the architecture implies that it will?\n\nSection 6.\n- As mentioned above, I think this paper would be greatly improved by focusing on either interpretability or novelty. Personally, I would choose the former. What is necessary for both, however, is to compare against other hard attention methods, whereas this paper simply shows ablations of the proposed model and comparisons to a single transformer baseline. How do I know that using Gumbel-Softmax hard attention is more interpretable or more performant than other types of hard attention?\n- Why would the single-stream model perform better on the IWSLT dataset?\n- What do the transformer and single-stream discrete transformer t-SNE embeddings look like?\n- Given the lack of keys and queries in the semantic stream, it’s not particularly surprising that it underperforms when used as an embedding layer; however, it seems like it must learn some amount of information about syntax to be able to get 83% on this task.\n- Please include more examples of words and their nearest neighbors in the appendix.\n\n[1] Fast Decoding in Sequence Models Using Discrete Latent Variables. Kaiser et al. 2018.\n"
        }
    ]
}