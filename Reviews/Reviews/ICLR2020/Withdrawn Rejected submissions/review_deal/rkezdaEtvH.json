{
    "Decision": {
        "decision": "Reject",
        "comment": "While there was some support for the ideas presented in this paper, it was on the borderline, and ultimately did not make the cut for publication at ICLR.\n\nConcerns were raised as to the significance of the contribution, beyond that of past work.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #4",
            "review": "The paper investigates hyperbolic discounting as a more biologically plausible alternative to exponential discounting in reinforcement learning. First, it formulates a notion of hazard in MDPs as constant exponential discounting and shows that hyperbolic discounting is consistent with uncertainty over the hazard rate. The paper then shows how value functions learned with exponential discounting can be used to approximate value functions with other forms of discounting. Specifically, the paper shows in section 4 how exponentially-discounted value functions can be used to approximate hyperbolically discounted value functions. The paper then presents experiments on a small MDP and Atari 2600 games, showing that learning discounted action values with many different discount rates as an auxiliary task improves performance on most Atari games.\n\nOverall, I very slightly tend towards accepting this paper for publication. While the idea of hyperbolic discounting didn't seem to pay off in terms of performance on the Atari 2600 games, the idea still has some merit as being more biologically plausible than exponential discounting, and may perform better on a more suitable environment. In addition, the discovery that learning many action-value functions with different exponential discounting rates as auxiliary tasks improves performance is quite interesting.\n\nNotes:\n- The footnote on the first page seems to just trail off. It would be better to explicitly state what the takeaway is. I'm guessing humans and animals in general would prefer \\$1M now in the first scenario, but \\$1.1M in the second scenario?\n- The idea of 1/(1-gamma) being an \"effective horizon\" seems questionable. For gamma=0.9 the \"effective horizon\" (aka expected number of timesteps before termination) would be 1/(1-.9)=10 timesteps. However, 0.9^(10)=~0.3486 means ~34% of the probability density is still to the right of that number, so calling 1/(1-gamma) an \"effective horizon\" seems to violate what we normally mean by \"horizon\".\n- A single hazard rate for the entire environment seems counterintuitive. Some states in life are definitely more hazardous than others, which suggests state-dependent discounting might be an interesting idea to explore.\n- Many people in the field think we shouldn't even be doing discounting at all (section 10.4 of Reinforcement Learning by Sutton and Barto), only total reward for episodic problems and average reward for continuing problems.\n- Pathworld experiments seem like they don't show anything useful; the hyperbolic discounting matches the true hazard behaviour of the environment (i.e., the hazard rate is drawn from a distribution) so it performs better than exponential discounting. However, if the environment had a fixed hazard rate then exponential discounting would perform better.\n- Using only 3 random seeds does not seem like enough for the experiments. Also there is no measure of standard error or statistical significance on the graphs.\n- Introducing a hyperparameter that increases computation and memory does not really seem like it's solving the problem, and certainly not \"efficiently\" as claimed in the abstract.\n- How was the value of the new hyperparameter set for the experiments? Were several values tried? More importantly, how can a good value for the new hyperparameter be chosen without prior knowledge of the environment?\n- Atari 2600 games don't really seem like a great environment to showcase the benefits of hyperbolic discounting.\n- It would be interesting to see a comparison with existing auxiliary task approaches, but due to space constraints should probably be left for future work.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper argues that hyperbolic and other non-exponential discounting mechanisms have been more utilized by humans and animals for value preferences than exponential discounting as widely used in RL literature. The authors claim that hyperbolic discounting mechanisms are especially preferred in the setting of maintaining uncertainty over the prior belief of the hazard rate in the environment and propose an efficient approximation of the Q function with hyperbolic and other non-exponential discounting mechanisms as a weighted sum of Q-functions with the standard exponential discounting factor. The paper shows empirical evidence that hyperbolic discounting function can more accurately estimate the value in a vanilla Pathworld environment and also demonstrate that the approximated multi-horizon Q functions can improve performance on ALE, which is largely attributed to learning over multi-horizons as an auxiliary task.\n\nOverall, this paper is an extension of the prior work Kurth-Nelson & Redish (2009). It seems to me that the difference between this paper and Kurth-Nelson & Redish (2009) is that in this paper, the approximated Q-value with hyperbolic discounting function is a weighted sum over each Q-values using exponential discounting factor gamma, while in Kurth-Nelson & Redish (2009), the Q-value is estimated by sampling one Q-value based on the distribution of the gamma. Another difference as claimed in the paper is that the authors also present an approximation of other non-hyperbolic discounting functions. Those extensions seem a bit incremental and are not well supported by the experimental results in the paper. The authors didn’t compare their method to Kurth-Nelson & Redish (2009) in both Pathworld and Atari 2600, which seems insufficient to demonstrate the point that the extension is useful. Moreover, the authors didn’t conduct experiments on non-hyperbolic discounting functions, which makes the claim that approximating non-hyperbolic discounting functions is one of the paper’s contributions unsubstantiated empirically.\n\nIn Section 6, the experiments in Atari 2600 show that Hyper-Rainbow and Multi-Rainbow have similar performance, and the authors claim that such results imply that learning over multi-horizons as an auxiliary task is more useful than choosing different discounting schemes. This seems to contradict the whole point of the paper that the hyperbolic discounting mechanism is biologically more reasonable than exponential discounting functions. The authors then claim that learning Q-values with multiple discounting factors is also one of the paper’s contributions, which makes the paper look like a list of tricks to get RL to work in the multi-task setting rather than a unifying framework of the hyperbolic discounting scheme. I think the authors might need to pick a deep RL domain similar to Pathworld, i.e. an environment with risk increasing as the length of the trajectory increases, but in high-dimensional state space and continuous action space, to better demonstrate the effectiveness of the method."
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper studied the discounting factor of RL under the uncertain hazard and non-trivial intertemporal decisions, by proposing a hyperbolic discounting strategy. The authors show the equivalence between the hyperbolic discount function and the temporal difference learning techniques in RL. Experimental results show the performance of the proposed method by comparing it with state-of-the-art. In general,  the text is well written and easy to follow. However,  I am not familiar with the context of hyperbolic computation.  I do not know any related works or what to expect from the results. I could not find anything wrong with this paper, but also do not have any intelligent questions to ask. Therefore, I am not sure about the technical contribution of this paper to the area. \n"
        }
    ]
}