{
    "Decision": {
        "decision": "Reject",
        "comment": "I agree with the reviewers that this paper has serious limitations in the experimental evaluation.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposes a method called \"counterfactual regularization\" whereby the dynamics/transition model is encourage to not have degeneracies where the actions don't influence the state transitions.  Concretely, this is done by, for every state, computing the maximum deviation of Transition model under a different action than the action taken in the history, and encourage that deviation to be as large as possible.  If that maximum deviation is 0, then all actions lead to the same next state. Empirical results show reasonable improvements in the StarIntruders task.\n\nMy biggest complaint (and the only one barring me from supporting acceptance) is that I don't see the body of results as scientifically solid.  Example of additional results that I would find much more convincing are:\n\n-- Experiments on more than one environment.  Currently, this paper should be judged solely for its empirical improvements, because there is little formal analysis or rigorous derivations.  But it's hard to judge that based on only one experiment.\n\n-- Deeper investigation into the effects of counterfactual regularization, including its interaction with learning disentangled representations.  Right now, there is no investigation, just a numerical score of reward attained.  This does not lead to much scientific insight.\n\n-- Exploration of the limitations of the approach.  Personally, I think this approach is reasonable for video games with a few discrete actions, but quickly runs into problems for more complex action spaces."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper considers regularization based on \"counterfactual\" trajectories.\nNamely, it suggests two losses, action-control and disentanglement regularization.\nIt experimentally evaluates the benefits of such regularization in the StarIntruders environment.\n\nThe paper is well written and explained.\n\nIssues:\n\n1) Authors evaluated the two suggested regularizations in separate. \nI would like to also see numbers from a combination of these.\n\n2) I think the related work is missing a large line of work on \"auxiliary tasks\".\nIt seems to me that this paper would exactly fit within that scope?\n\n3) My main issue is the evaluation.\nThe evaluation is done on a in-house game and compares to very few methods.\nFor a paper that has very little theory and thus most of the value is in the empirical evaluation, I think that is a problem.\nIf authors opted for example for Space Invaders (they do say it is similar) or simply more games, one would have many more existing numbers to compare against.\n\nMinor issues:\n\n1) The first regularization - action control regularization is motivated by the idea that there is always an action that changes the state. While true for most environments, this does not hold in general.\n\nSummary:\n\nOverall, this paper has potential but I don not believe is good enough - I suggest a reject.\nThe main problem is that the idea is relatively simple, there is no theory and thus the crucial piece of the paper has to be the empirical evaluation.\nAnd the evaluation only compares to a single method with no regularization, no auxiliary tasks and reports only experiments on a single game."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #2",
            "review": "The paper presents regularization techniques for model based reinforcement learning which attempt to build counterfactual reasoning into the model. In particular, they present auxiliary loss terms which can be used in \"what if\" scenarios where the actual state is unknown. Given certain assumptions, they show that this added regularization can improve generalization to unseen problem settings. Specifically they propose two forms of regularization: (1) enforcing that for different actions the predicted next state should be different (action-control) and (2) enforcing that when certain parts of the low dimensional state are perturbed, over a model rollout the perturbation should only affect the perturbed parts of the state, essentially encouraging the latent space features to be independent (disentanglement). \n\nOverall the idea is well motivated - incorporating counterfactual reasoning into model based RL has potential to to improve generalization. Also, while the assumptions needed for the regularization to be correct are not always true, they do seem to hold in many cases. Lastly, the results do seem to indicate that generalization is slightly improved when using the proposed forms of regularization.\n\nMy criticisms are:\n\n(1) As mentioned in the paper Action-Control assumes that at every single timestep the agent has potential to change the state. However there may be settings where the agent can always change state, but only a small component of the state. In these cases the states should be quite similar. For example a robot only moving a single object when the state consists of many objects. Also as mentioned in the paper Disentanglement will not work in stochastic environments. One concern I have is that since different environments can violate the assumptions to varying degrees, it seems like actually using the regularization and picking the correct hyperparameter to weight it will be very challenging. \n\n(2) The current results are only demonstrated in a single, custom environment. Additionally performance is shown on only 2 test tasks, and in all cases in Table 2 it is unclear how to interpret the reward. Does this performance constitute completing the task? What is the best possible cumulative reward in this case? The performance improvement seems small, but it is difficult to judge without knowing the details of the task.\n\nI think the paper would be significantly improved by (1) adding experiments in more environments, especially standard model based RL environments where the performance of many existing methods is known and (2) adding comparisons to other forms of model regularization, for example using an ensemble of models. My current rating is Weak Accept.\n\nSome other questions:\n- In Table 2 does MPC amount to PlaNet?\n- How sensitive are the current numbers to planning parameters (horizon, num samples)?\n- Can you provide error bars for the numbers in the tables?\n\n______________________________________________\n\nAfter author responses and closer examination of the paper I have some additional concerns about experimental details.  Changing my score from 'Weak Accept' to 'Weak Reject'",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        }
    ]
}