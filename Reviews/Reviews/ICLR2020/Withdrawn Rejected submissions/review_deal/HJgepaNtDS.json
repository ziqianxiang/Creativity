{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper received two weak rejects (3) and one accept (8).  In the discussion phase, the paper received significant discussion between the authors and reviewers and internally between the reviewers (which is tremendously appreciated).  In particular, there was a discussion about the novelty of the contribution and ideas (AnonReviewer3 felt that the ideas presented provided an interesting new thought-provoking perspective) and the strength of the empirical results.  None of the reviewers felt really strongly about rejecting and would not argue strongly against acceptance.   However, AnonReviewer3 was not prepared to really champion the paper for acceptance due to a lack of confidence.  Unfortunately, the paper falls just below the bar for acceptance.  Taking the reviewer feedback into account and adding careful new experiments with strong results would make this a much stronger paper for a future submission.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "A typical Wavelet Transform is built through the dilation and/or rotation of a mother wavelet, which can been viewed as a group action on a mother wavelet. This work proposes to extend this construction beyond the Euclidean group, and to supervisedly learn operators that will be applied on a mother wavelet. Competitive numerical performances are obtained.\n\nOverall, I think that re-thinking the way a Wavelet Transform is designed, is an interesting direction of research, but I think some of the theoretical tools developed in this paper are not dedicated to achieve this purpose. In particular, the group/representation properties seem to not be used, and the authors could simply consider a specific subset of invertible mapping on $\\mathbb{R}^2$ which would be applied on the mother wavelet and lead to a Wavelet Transform. In other words, the overall formulation could be simplified.\n\n\nPros:\n- In general, the numerical experiments are at the level of the state of the art.\n- Parametrizing a subset of the group of increasing function and its application to signal processing tools is novel, to my knowledge.\n\nCons:\n- Some very relevant elements in the literature review are missing. Learning or using an underlying group of symmetry that will be combined with a deep neural network is not novel, cf: https://arxiv.org/abs/1601.04920 ; in particular for reducing the number of parameters, filters or samples: https://arxiv.org/abs/1809.06367 ; http://proceedings.mlr.press/v48/cohenc16.pdf ; https://arxiv.org/abs/1809.10200 ; https://arxiv.org/abs/1605.06644 - I think the authors should discuss at least one or two of those papers, if not all.\n- The performance on the bird detection task is good but the improvement compared to other work is not clear, given that some supervision in the first layer is incorporated.\n- Subsections 2.2 and 2.3 are difficult to parse because the authors introduce a lot of equations or notion that are not useful to understand their algorithm/method. The equation (6) seems wrong to me (one should consider t->s_i(-t) and not t->s_i(t) and b seems missing in the second line).\n- Figure 2 is difficult to read because of the illustrative graphics. Maybe a block schema would be easier to parse.\n- It seems to me that no-where the group properties are used, such as the stability to composition. In this paper, the authors simply try to parametrize a diffeomorphism to dilate the mother wavelet. From my understanding of 3.3, the subset of function used to approximate $G_{inc}$ do not form a subgroup as well, contrary to the Euclidean case, where for instance discrete rotations in the case of images are a finite group.\n- In subsection 4.1(Table 1), a comparison with a wavelet transform followed by a linear operator is compared with the proposed method. I find this result surprising :â€¨LGT/nLGT/cLGT/cnLGT and the WT are some linear methods whereas the STFT is non linear. As the WT should be unitary, if the linear classifier method is reasonably trained, then both methods should lead to the same result, except if the data are poorly conditioned. In which case, this experiment would not be meaningful. I think the authors should comment more this result because it is surprising.\n- I slightly disagree with the sentence \"in the case of WT, the precision in frequency degrades as the frequency increases\". Actually, the heisenberg principle is optimally optimized by wavelets, meaning that the area of the frequency/spatial support on a spectrogram is constant. On the contrary, the STFT has a lack of localisation (and thus the \"precision\" is not constant along frequencies). Maybe this could be rephrased slightly.\n- Given the filter learned in Figure 5, one can wonder if a foveal approach (i.e., Foveal Wavelets) could perform similarly? It would be interesting to display the littlewood-paley plot(i.e., the sum of the modulus of the filters in the Fourier domain) of this representation to understand the nature of this operator in the Fourier domain.\n- I think group actions could be considered instead of representations: it would be simpler to understand for a potential reader.\n\nTypos: \n- abstract \"in order to transform the mother ..\" > \"in order to transform a mother..\"\n- page 7 \"this variation is as not captured as well\" > \"this variation is not captured as well\".\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "*Paper summary* \n\nThe authors design a learnable time-series pre-processing, which they refer to as a learnable group transform (LGT). This is a generalization of the wavelet transform, which maps a time-series signal onto the affine group. In the wavelet transform, multiple scaled and shifted versions of a mother wavelet are \"inner-producted\" with a signal; the resulting coefficents are the output of the transform. In the LGT, a more flexible transform that just scaling and shifting is applied to the shape of the mother wavelet, which is piece-wise linearly stretched. This elegantly encompasses time-warping and many other wavelet-style transforms into one learnable preprocessing step.\n\n*Paper decision* \n\nI would like to recommend this paper be accepted. It is clearly written and the idea is simple; that said, the idea is an elegant generalization of the wavelet transform. (I admit my own expertise is not in time-series data, so I may be mistaken). The experiments are also simple, but straightforward and easy to reimplement.\n\n*Supporting arguments* \n\nWhat I like about this paper is that the idea is a straightforward generalization of the wavelet transform through the lens of group theory. Furthermore, other transforms, such as the short-time fourier transform, or methods such as time-warping are easily encompassed by the method. By making the transform learnable, the authors reduce the number of in-built assumptions in the problem. \n\nI believe the idea is novel; although, since this is not my area of expertise, I defer to other reviewers who may wish to contest this.\n\nI do wonder, however, what the interpretation is behind some of the learned LGTs. Typically, a group transform is favored because of certain symmetry properties of the task at hand. For instance, wavelet transforms are shift-covariant, reflecting the shift-covariance of underlying signal statistics.\n\nExperimentally, I think there was a nice selection of toy and harder examples. I found the visualized filters and group transforms in the appendix really interesting. I think it would have been nicer to see some more analysis of the interpretation of the learned LGT in the main text though. Further, it would have been nicer to see some ablation studies conducted, such as varying the number of degrees of freedom in the LGT and seeing how that affects performance.\n\n\n*Smaller questions/notes for the authors*\n\n- The explanation of what a group is is quite high-level and I think if you did not know what it was beforehand, it would be hard to understand what one is. For instance, just after introducing groups, homomorphisms are mentioned. I would guess that someone who has never seen a group beforehand, would have no idea about homomorphisms.\n\n- In equation 6, is \\star a convolution or a cross-correlation? It looks like a cross-correlation to me.\n\n- I really liked the connection drawn between the group transform and time-warping. This is an aspect I personally had never considered.\n\n- Would it be possible to move some of the material on the learned filter transformations to the main text and write some analysis, even if it is only qualitative? I would find that fascinating.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper defines a set of learnable basis functions and a joint learning algorithm to estimate them. It is based on the premise that the common learning approach in time-series is to first represent them in some spectral domain; thus, the main problem is to define and estimate the basis functions. However, this premise is not accurate and many learning algorithms operate just in the actual time domain (even in speech).\n\nThe paper advocates for a special group of strictly increasing transformations of the basis functions. While in Section 3.2 the authors do explain the connections to the time-warping operation, they never explicitly justify why such a group is interesting. At least, it is not clear why they are not interested in a semi-group of (non-strictly) increasing functions, which is more flexible and used in DTW.\n\nThe authors implement the more general group using piece-wise linear functions, which makes enforcing the strictly increasing property easier. This idea is nice, however I am concerned with speed of such transformations in practice. Unfortunately the authors do not report runtime numbers in the experiments.\n\nWhile the paper has verbose and wordy writing, it also spends a major part of the paper describing the common knowledge about group theory and wavelets. Instead, some crucial design decisions are not well-justified. For example, the description of the constraints in cLGT suddenly appears.\n\nThere are several major problems in the experiments. First, the results in Tables 1 and 2 do not seem to be statistically significant and Table 3 does not have the intervals. Second, the Haptics dataset is too small. Also, choosing a single dataset from the UCR sets only sends the message that this dataset is one of the few datasets on which the algorithm has worked better.\nOverall, the key message out of the experiments is that if you project the data to a parameterized basis set and constrain the parameters, you will get better generalization, which is not very novel. \n\nOther than improving the experiments, the authors can significantly improve the writing by decreasing the emphasis on the background knowledge and elaborating more on the new proposed ideas."
        }
    ]
}