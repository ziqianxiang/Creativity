{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper addresses the problem of visual plan completion in the presence of noisy observations. The heart of the authors' approach is to learn embeddings of actions given distributions over actions (which correspond to steps in an uncertain plan), using various proposed \"affinity models.\" These affinity models are used to generate affinity scores (roughly, given a set of previous distributions over actions, how likely is each possible next action?).\n\nI think that the problem being studied is well-motivated by the authors, and I especially appreciated the introductory material on the importance of a system that reasons about distributions over actions, rather than simply assuming maximum likelihood ones.\n\n--Main Comments--\n\nMy main concern with the current version of the paper is the lack of clarity as to what is being done, and what (formally) the core contribution is. Even after reading through a couple of times, I only have a vague understanding of exactly what a \"affinity model\" is, mathematically. From what I gather, it seems an affinity model is a distribution P(Distr(a_{t+j}) | Distr(a_t))? How do the encodings \"enc\" factor into this definition? Is computing an encoding a necessary characteristic of any affinity model? Without these details, I am unable to understand precisely how the methods proposed in Section 4 (Distr2Vec and RS2Vec) fit into the problem formulation described in Section 3.1. I believe the text could be strongly improved with a clear mathematical definition of what an affinity model is, since this term is used so frequently, and a reframing of Section 4 around this definition.\n\nMy other major concerns lie in the experimental results. The experiments as a whole are rather opaque (some sort of visualization would have been very nice to see; for instance, could you maybe show a plan completion found by your method in one of the domains you experimented in?) Looking at the results in Section 5.3, I found Figure 9 rather unconvincing. The authors state that when PER is high, Distr2Vec \"clearly performs the best,\" when in fact the improved accuracy is only around 5%. I fully appreciate that in some benchmarks, 5% can be a major improvement, but I am unable to place the experimental results provided in this paper in comparison to other state-of-the-art plan recognition methods; only a naive affinity-model-based baseline (GM) is compared against.\n\nOn a similar note, Figure 10 seems to show little to no accuracy gains by using the proposed methods over GM, especially in light of the increase in training times.\n\nFinally, Figure 8 seems to suggest that LSTM and Algorithm 1 perform similarly. In the text, the authors state that \"LSTM requires 29.5 Secs to train and 0.02 Secs to recognize one action in testing phase\" while \"Algorithm 1 requires 8.0 Secs and 0.80 Secs respectively\" -- this, to me, indicates a tradeoff of faster training and slower inference vs. slower training and faster inference; it's not quite clear that one should be preferred over the other, and in fact it probably depends on the application in general.\n\nDue to the issues of clarity as to what is being done, and the unclear experimental significance of the work, I cannot recommend acceptance at this time, but I look forward to hearing the authors' responses in the rebuttal phase.\n\n--Other Comments--\n- Could you give some intuition on why anything useful should be learned when PER is 100%? My intuition would be that your proposed methods will not \"rule out\" the correct actions like GM does; is this correct?\n- The experimental results suggest that Distr2Vec performs mostly comparably to RS2Vec, while only requiring a tiny fraction of the training time. Is there any practical reason to ever prefer RS2Vec?\n- There are also other places where the exposition are unclear. For instance, in the Introduction, Distr2Vec is written to seem like an improvement over RS2Vec, but counter-intuitively, Distr2Vec is described before RS2Vec in Section 4. Making the story and relationship between the two consistent throughout the document would help improve the clarity.\n\n--Minor Points--\n- Make colors in plots consistent, e.g. in Figure 6 the colors for RS2Vec and Distr2Vec are swapped in the rightmost plot compared to the other three.\n- The colors in Figure 10 (left) are too similar, I cannot tell the data apart."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "The paper presents an approach to unsupervised plan recognition from video, which can take into account the uncertainty of actions that have been recognized automatically from video, in this case the 50 salads database. This probabilistic approach seems plausible, but I would honestly be surprised if something like this has not been attempted before. \n\nThe paper contains very little discussion of the relevance of the results to the state of the art and about other related techniques, making it hard for someone not from the field to judge the contribution of this work. The proposed approach seems straightforward and reasonable. I am not sure if the 50 salads database is not maybe considered a toy task, and how the results presented here compare to other work - this should definitely be addressed before publication.\n\nFinally, the paper is not well formatted, and most images have very small captions (font size), making it hard to read.\n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "The paper proposes an action affinity method for plan recognition from a sequence of videos. The method attempts to learn embeddings for distribution of actions. The author proposes a loss function by combing: 1) average KL divergence between log likelihood of an action at certain time step and those at future time steps, and 2) hierarchical softmax loss. Experimental results on a real world dataset and a synthetic dataset are conducted to show that the proposed method has a better sample and computation efficiency. \n\nI think the general direction of the work seems interesting. My main concern exists in novelty/significance. The loss of minimizing KL divergence seems to be very intuitive, and the other part of loss using hierarchical softmax loss does not have justification. It simply mentions that using such a loss results in better performance due to previous work. As a result, from a technical perspective, I think the work seems somewhat incremental. "
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "1: Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #1",
            "review": "Summary \nThe paper proposes several methods for uncertain plan reconstruction, which involve completing a sequence of actions, in which some of the actions are missing, and others are observed with uncertainty and specified with distributions. Several methods are proposed that learn word2vec-style co-occurrence models where the probability of neighboring actions is modeled given observations at a single timestep. Distr2vec represents such observations as a full distribution, RS2vec uses beam search to select the most likely actions from the distribution as observation, and the greedy method selects the most likely action as observation. Alternatively, an LSTM approach is evaluated that takes in the distributional observations (encoded via distr2vec) and predicts the missing actions. Hierarchical softmax is used for prediction.\n\nIt is shown that the proposed approach outperforms the greedy method when the observations are likely to be erroneous, while the greedy method performs better for high-entropy observations as well as observations that are less erroneous. The beam search approach needs a large number of samples to be competitive, which makes it computationally expensive. The LSTM approach is competitive, but takes longer to train than a simple co-occurrence model.\n\nDecision\nThe paper proposes a potentially promising approach, but is poorly presented, evaluates mostly on toy data, shows modest improvements over baselines, and lacks strong baselines. In addition, the paper is poorly motivated and ethical issues are ignored (see below). Due to these significant issues with methodology and evaluation, I recommend reject at this time.\n\nFinally, most of the prior work on plan completion, the problem that the paper addresses, appeared in venues such as AAAI/IJCAI. While I am not closely familiar with this literature (and therefore unable to access the relevance or novelty of this paper), I can only recommend resubmission to one of these venues, where the paper might hopefully get a more fair assessment.\n\nPros\n- The paper proposes to tackle plan completion with probabilistic, instead of deterministic observations, which is a promising direction well-motivated by practical settings.\n\nCons\n- The choices made when designing the method are poorly explained and strong baselines are missing.\n- The experimental evaluation shows that the method outperforms the baselines only in contrived toy settings. Arguably, the high-entropy case, where the action distributions are noisy but often correct, is closer to the practical settings than PER, where the action distributions are corrupted with arbitrary mistakes.  However, the greedy baseline outperforms the proposed method in the high-entropy case. This is confirmed by the experiment on the real-world dataset (with a real-world perception system?) where the greedy baseline performs comparably to the proposed method. \n- The importance of video surveillance, while being the motivation of the paper, is not discussed. On this note, I would like to make clear that as machine learning researchers it is our duty to conduct research responsibly and consider its implications on the real world and society. The paper needs a discussion of why (and whether) video surveillance is a worthy goal and related ethical issues.\n- The paper contains numerous style issues, such as improper handling of parenthetical citations. For instance, in the last paragraph of Sec 2, the author list is duplicated for all 7 cited papers as in “Hodges and Pollack designed machine learning-based systems Hodges and Pollack (2007)“ In all 7 sentences, the first reference should be replaced with \\citet{<HP07>}, and the last reference should be removed. On page 10, a wrapfigure from the previous page causes incorrect formatting, which could be fixed by moving the figure, or, in the worst case, removing the white space after the figure with \\vspace{-20pt}.\n- The paper is about 9.3 pages and as such should be judged with a higher standard according to the ICLR guidelines. I do not see a justifiable reason for this paper to exceed the 8 page limit. \n\nQuestions on methodology\n- Eq (1) is extremely confusing. Notation Pr(a|b), where Pr is highly suggestive of ‘probability’, which is very non-standard given that a and b are themselves distributions and not random variables! The text states that the equation expresses ‘log probability of distributions’. What is a probability of a distribution? What is the graphical model assumed in the paper? It is further stated that Pr(a|b) represents the similarity between a and b. Is the similarity the same as conditional probability? Why is that the case? Finally, it seems that the model Pr is meant to be parametrized and Eq (1) is the objective for further optimization over Pr. This is not explained. \n- Furthermore, it seems that the conditional probability of two distributions is never actually needed in the paper - defining that would involve defining a distribution over distributions, and is unnecessary for the purpose of this paper. Rather, the derivation this paper attempts only needs a conditional probability of a random variable given a distribution (whatever that means) in eqs (1,2). The random variable can have either a zero-entropy distribution (as in prior work) or a full distribution, as proposed in this paper, however, in both cases eq (2) is computed the same way, as the cross-entropy of distributions. To handle the fact that such variable can have a non-degenerate distribution, eqs (1,2) should be re-written in terms of expectations over such distribution.  If derived this way, eqs (3,4), which notice that minimizing cross-entropy is the same as minimizing KL divergence as the entropy of the target distribution is a constant, are unnecessary.\n\nQuestions on comparisons\n- The LSTM baseline seems to be the most natural baseline for this task as it directly learns to output the target values. However, it is never described in detail and not evaluated on the real-world dataset, which makes it hard to evaluate the proposed method. Is the LSTM single-directional? If not, it is missing certain observations. A bi-directional model that makes predictions based on future and past observations would mostly certainly do better than single-directional. It is stated that the LSTM takes 29.5 secs to train. Was truncated back-propagation through time considered for training? Truncated back-propagation allows to cut both training time and memory consumption. Finally, it is stated that the LSTM encoder is pre-trained with distr2vec. Is the distr2vec representation frozen after pre-training? How well does an end-to-end procedure perform?\n- In section 4.3, it is stated that resampling makes RS2vec more computationally expensive. Is the baseline without resampling (just with beam search) more computationally efficient? Was this baseline considered? Additionally, while beam search is computationally expensive as it needs to consider the whole sequence, one can design a simple baseline that is both efficient and powerful by simply taking top S actions from each action individually. This ensures that no actions will be repeated and thus the information is used optimally, thus avoiding both problems of RS2vec. Why is it necessary to consider samples from the trajectories in the RS2vec approach instead of this simple baseline?\n\n--------------------- Update 11.19 -----------------------\n\nIn addition to my general comments about authors' response in a separate  answer below, I provide additional comments here.\n\nRegarding real-world value, I do not believe that PER setting is realistic. I would like an evaluation with a real-world perception system, so as to see whether the method better handles types of perception errors that exist in practice. I also would like to see the (bidirectional end-to-end) LSTM baseline evaluated in the real-world setting as that is the most natural baseline.\n\nIf the paper aspires to propose a method for learning action representations, the value of learning such representation has to be proven, which could be done e.g. by comparing with 1) end-to-end training without special consideration of the representation, 2) action representation baselines, or 3) by qualitative analysis of the representation. It is true, as authors point out, that pre-training of representation is very common in, say computer vision (CV) or natural language processing . (NLP), however, this practice stems out of practical utility, and has only become common in about 2014 for CV and 2019 for NLP once adequate representation learning methods became available. This being said, I believe the paper can have a solid contribution just in terms of performance on plan recognition; however, strong baselines and real-world settings need to be considered for this contribution to be solid.\n\nI appreciate the explanation of the computational cost of resampling. The explanation should be added to the paper. However; I believe that my suggested modification to the baseline will make it more computationally efficient, indepently of the resampling stage.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        }
    ]
}