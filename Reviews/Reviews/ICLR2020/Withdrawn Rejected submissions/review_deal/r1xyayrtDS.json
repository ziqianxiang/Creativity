{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper introduces a new way to eliminate bias in critic estimates, based on Clipped Double Q-learning (CDQ). It shows that CDQ could potentially underestimate the critic, and solves this problem by adaptively taking the mean of CDQ target and a normal critic target which might be an overestimate.\n\nMy main concern about this paper are: 1) The effectiveness of the way to learn beta adaptively is unclear to me. 2) the method proposed is an incremental change on top of CDQ, and introduce new (and maybe too much) complexity in an algorithm to remove the overestimation bias. I tend to reject this paper.\n\nDetailed comments:\n\n1) The algorithm introduce another parameter beta to balance the percentage of using an underestimation or overestimation of Q^\\pi. It’s important that beta needed to be learned adaptively by the algorithm itself. I suspect the current empirical justification is not enough to convince me that using trajectory return as a sort of “ground truth” to determine it is over- or under-estimation. Since Mujoco is deterministic and DDPG has a deterministic policy, the whole dynamics is deterministic which makes trajectory return becomes a very accurate estimate of Q^\\pi. I’m not sure in a more stochastic environment if the algorithm of choosing beta still works or not. \nOn the other side, if the trajectory return is such good evidence of deciding  over-/under-estimation, we can just use it as Q^\\pi which gives REINFORCE instead of actor-critic. One reason we study actor-critic algorithm is that in some cases (such as stochastic environment) function approximator generalizes better than trajectory return. However the proposed BTD3 algorithm is an actor-critic that uses trajectory return to guide it, it seems to conflict with the motivation of using actor-critic.\n\n2) The method in this paper is a straightforward and incremental way to debias the underestimation in CDQ -- adding another overestimate on top of it. However, the original problem is critic with function approximator potentially overestimates true critic. Doing a minus then plus looks more than necessary as a straightforward solution to the original problem. Also, it increases the whole complexity of the algorithm. So it needed to be justified with more intuition/theory/diverse experiment domains.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "\nThis paper proposes an algorithm to address bias in estimating the value function for actor-critic methods. The algorithm, balanced uses a convex combination of the minimum of two value estimates (as in clipped double Q-learning) and a single value estimate (as in DDPG) as a TD target. The weighting between the two is also learnt with stochastic gradient descent during training. Empirical results show that this algorithm produces more accurate value estimates and can improve performance on standard continuous control tasks. \n\nWhile value estimation in actor-critic methods is an important topic and this paper , I am not convinced that this paper makes a significant contribution to the area. The proposed algorithm is not well-motivated with no theoretical justification and the experiments do not show the clear benefits of the algorithm.\n\nThe main reason for my decision is that the proposed algorithm, BCDQ, does not seem to be motivated well enough. While a case is made for the importance of addressing bias in value estimates, the particular mechanism BCDQ uses is not well-motivated, theoretically or otherwise. While it has been observed that CDQ underestimates the true values and regular TD learning in DDPG overestimates the true Q-values, using a convex combination of both seems fairly arbitrary. The paper does not seem to provide any other justification for this choice. For example, a reasonable alternative could be to use \\beta * Q_CDQ(s,a), a scaled version of the CDQ estimate, with \\beta being tuned with SGD (this variant could serve as a baseline). \n\nFor the experiments, for Fig. 1, error bars are missing from the value estimates. This makes it difficult to judge if the results are significant or not and, at a glance, there does not seem to be a large difference with the baseline algorithms. Additionally, the performance of BTD3 on the benchmarks do not seem to produce (statistically) significantly better results than regular TD3. As such, empirical performance is not a strong motivating argument in favor of the algorithm.\n\nThe paper could also be strengthened by providing a deeper analysis of \\beta during training. Currently, in the paper, it is only mentioned that different environments lead to different values of \\beta. I think it would be interesting to further investigate how \\beta varies over training and how this relates to over/underestimation of the value function and training dynamics. For example, looking at Fig. 2, it seems like there is consistent trend of \\beta first dipping down then spiking upwards, before slowly decreasing over the rest of training. Is there a reasonable explanation for this? \n\nOther points:\n1) The objective for beta, eq. 5 seems to be missing a 2 in the exponent. Also, Q^\\pi is not defined.\n2) In Alg 1, similarly, the gradient of beta may need to be fixed.\n3) In table 1, using the maximum episode reward introduces higher variance and is not good practice. See the \"Reporting Evaluation Metrics\" section of \"Deep RL that matters\" by Hendersen et al.\n4) Some uncertainty measure would be helpful in Fig. 2, e.g. plotting the quartiles. \n\nMinor comments and typos:\n- The BTD3 algorithm figure could be placed on the next page\t\n- Fig. 1 Why is BCDQ on a different plot? If it's for clarity, the axes for both rows of plots should match.\n- p.4 \"over the time of the training process\" -> \"over the training process\"\n- p.4  backward quotes in \"min\"\n- p.6 \"suggests that - similary to ensemble methods - the ...\"  use --- instead of -\n- p.6 \"Differently\" \n- p.8 Fig. 4 caption: \"show\" -> \"shown\", \"OpenAi\" -> \"OpenAI\"\n- Many commas are missing. Here are a few: p.1 - \"In recent years\", \"For many important problems\", \"In an actor-critic setting\", \"Similarly to CDQ\", \"In CDQ\", p.2 - \"To guarantee reproducibility\", \"Later\", \"Similarly to our approach\", p.5 - \"For the Hopper task\", p.6 \"From the plots\", \"For example in Walker2d\", p.7 - \"In Table 1\", \"For all tasks\", ... \n\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper proposes the balanced clipped double estimator that uses a weighted averaged of the target from CDQ (leading to underestimations) and the target from a single critic (causing overestimations), to better estimate the Q value. The weight is adjusted by a simple heuristic during the training. Experimental results show that the BTD3 algorithm outperforms TD3, SAC, DDPG on six MuJoCo environments.\n\nHow to get accurate function approximations is a very important problem. The idea that using a weight to balance the target value from CDQ and the target value from the single critic is welled motivated and interesting. Figure 1 gives a good motivation example to show how BCDQ reduces the estimator error of Q value. I appreciate that the authors plot the detail of beta in the training in Figure 2. \n\nThe experimental results are somewhat weak as there are no results on Swimmer and HumanoidStandup. And I do not think BTD3 significantly outperforms other baselines in Ant, Walker2d, and Reacher. The paper would be more convincing with the result of BTD3 with fixed beta in Figure 1.\n\nQuestions:\n1. Are there any statistics of the estimation errors of TD3, DDPG, and BTD3 in all environments?\n\n"
        }
    ]
}