{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "1. Summary: The authors propose a new strategy for fine-tuning natural language generation model called mix-review. The main idea is to mix the target dataset with a sample of the pretraining dataset to fine-tune parameters. This strategy can alleviate the forgetting and overfitting problems existing in traditional fine-tuning strategy. The authors conducted both automatic evaluation and human evaluation to prove the effectiveness of their idea. Meanwhile, the authors also did several experiments to reveal the effect of pretraining on natural language generation.\n2. Overall assessment: The problem studies in this paper is an important issue that has not been studied well. It shed some light on how to alleviate the forgetting problem in the pretraining and fine-tuning framework and analyzes how pretraining affects language generation models. I think it delivers some valuable ideas to the NLP community. Meanwhile, there still exist some problems for this paper to improve on. All in all, I'm leaning towards acceptance.\n3. Comments:\n3.1 In Figure 1(a), it looks the line of mixreview_ccnews_valid does not change at all. Does this indicate the fine-tuning on a dialogue dataset, which should be different from the CCNEWS data, does not change the model's ability in generating CCNEWS at all? Can the authors give some more explanations here?\n3.2 Raters are asked to give overall ratings on generated text in Table 2. While this human evaluation is great plus, readers may also like to see some finer-grained human anlaysis, such as human evaluations from different angles, such as cohere, fluency, engageness and so on. Such detailed evaluations can also better reveal why the mix-review strategy is working better.\n3.3 In Table 4 and Table 6, the NS and MASS pretraining tasks are utilized separately? Why not combine these tasks together? Will there be more gain of doing so?\n3.4 Table 3 is a great analysis. I'm wondering if it is possible to evaluate the context-sensitivity quantitatively?\n3.5 What do you mean by \"trigger\" in Sec. 5.3? Using the triggers as the input context to the generation model? What's the relationship between the reference description and the triggers? It seems the reference descriptions are some factual statements that are crawled independently with the triggers. "
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper studies the forgetting problem in the pretrain-finetune framework, specifically in the dialogue response generation task. It proposes a mix-review strategy to alleviate the forgetting issue. The paper makes three major claims:\n(1) In the finetuning stage, the model forgets part of the language generation skills acquired during pretraining. \n(2) The proposed mix-review strategy effectively alleviates the forgetting problem.\n(3) The proposed method performs better in terms of context-sensitivity and knowledge transfer.\n\nAlthough the forgetting problem pointed out by this paper is interesting and worth studying,  the proposed method (1) lacks novelty and (2) does not perform well empirically.  The writing of the paper also needs to be improved.\n\nThe proposed mix-review strategy is very straightforward and lacks novelty. To prevent catastrophic forgetting, the simplest way is to sample some data from the historical task and jointly train with the new task, which is exactly the proposed method. It is not surprising using this strategy alleviates the forgetting problem, but the question is whether it can make the model generalize better. Empirically, it does not perform better compared with the commonly-used weight decay regularizer. In Table 2, we can see the proposed method didn’t improve much in terms of either perplexity or human evaluation.\n\nThe two analyses are also not convincing to show mix-review performs better. For example, in terms of context-sensitivity, the increase in perplexity after distort the context does not necessarily mean the model’s generation is more context-related.  Overall, the contribution made by this paper is not yet enough for an ICLR publication."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The paper studies the forgetting problem in the pretraining-finetuning framework common to neural language generation models. It makes two main contributions: (1) it analyzes the forgetting problem from the perspective of “context sensitivity” and “knowledge transfer”. (2) In light of these analyzes, it proposes the Mix-Review fine-tuning strategy, which slightly outperforms the Weight Decay (WD) method of [Wiese et al., 2017].\n\nStrengths:\n\n(1) The paper addresses an important problem, as a very large body recent work relies on BERT, GPT-2, and related approaches to improve their models on specific tasks. The problem with these pre-trained approaches is that they bring challenges similar to those pertaining to continuous learning that have been studied for many years, e.g., (catastrophic) forgetting. There is a need to study the forgetting problem specifically in the context of the pretraining-finetuning framework and compare it approaches borrowed from the continuous learning literature, which the paper does to a certain extent (the WD method).\n\n(2) The paper shows some nice empirical gains (though results are mitigated by weakness 2 below) and a better ability to “transfer knowledge” between the pretrained model and its application to the downstream task. \n\nWeaknesses:\n\n(1) The approach of the paper (Mix-Review) is very similar to methods that have been proposed already, and the papers doesn’t acknowledge the huge body of prior work already existing on (catastrophic) forgetting. See [Kemker et al., 2017] for a comprehensive overview on the subject. There are multiple families of methods to mitigate the forgetting problem, including regularization methods, rehearsal (and pseudo-rehearsal) methods, ensemble methods, etc. Mix-Review belongs to the rehearsal category, as it reintroduces instances learned long ago (in the case of the paper, pretraining instances) to the current training regime. There is also a large body of work on rehearsal/pseudo-rehearsal going back to [Robins, 1995]. Robins’ method that seems the closest is “random rehearsal”, which as in the paper mixes randomly selected instances of the older data. See my detailed for other weaknesses in related work.\n\n(2) Experimental results are not convincing. First, results of Figure 1 mainly show that Mix-Review is less prone to “forgetting” than WD, but I find that unsurprising. At the risk of sounding obvious: The notion of forgetting only makes sense in the context of information (here, pretraining instances) that has been seen in the *past*, but those instances continue to be shown to mix-review in the *present*. Contrasting rehearsal methods to non-rehearsal in this manner is akin to comparing training-set perplexity with validation-set perplexity (i.e., in one of the two cases, we compute perplexity on what we are currently training on). So, to me, figure 1 doesn’t tell us very much, and approaches should be mainly evaluated on a downstream task (which the authors do in table 2). \n\nUnfortunately, in the case of table 2, mix-review doesn’t show much gain when compared to the standard WD (aka EWC) method. There is a small gain that seems to be within the confidence intervals (assuming that is what is shown in parentheses). Now, my problem with these results of table 2 is this quote from the paper: “Although the perplexity is still improving, we stop the pre-training for practical reasons to control the duration of the experiments.”  The difference between the baseline and the other systems shows that pretraining is very useful to all 3 downstream tasks, but this performance of the various models could be highly dependent of when the authors applied early stopping. Now, the actual issue is that early stopping is NOT applied consistently between WD and mix-review, as the WD approach has seen the pretraining data only 20 times (“we stop the pre-training after the CCNEWS data is swept 20 time”), but mix-review continues to be shown CCNEWS data even after 20 epochs, so there is effectively no early stopping for mix-review.\n\nConclusion:\n\nOverall, I think the paper has merits in showing that rehearsal methods should probably be explored in the context of pretraining/finetuning architectures, but problematic experimentation and lack of comparison with previous work (there is only one baseline, WD, which is not enough to me given all the prior work on the subject of forgetting mitigation) make me recommend rejection for now. \n\nDetailed comments:\n\nThe reference to WD is [Wiese et al., 2017] is not the right one. Wiese et al. give credit to Kirkpatrick [2017] for their Elastic Weight Consolidation (EWC) method as WD is a special case of EWC. EWC has been widely used to mitigate the forgetting problem and is not mentioned in the submission. \n\nThe mix-review training objective contains a mix-decay parameter, which is unclear as it doesn’t seem to appear in any of the equations (was it meant to be in Eq. 2?). In any case, for the paper to be of practical importance, I think it should show the effect of pre-training data percentage over time. It seems it starts with a high proportion of pre-training data, which could be wasteful as fine-tuning starts off by utilizing mostly pre-training instances. This gets fixed over time thanks to the mix-decay parameter, but as repetitive decaying of the pre-training data occurs, that pre-training data essentially vanishes to ultimately be empty. What prevents catastrophic forgetting in that case, after many epochs? Other methods borrowed from the catastrophic-forgetting literature wouldn’t have this problem. It seems the choice of how long we let the model train could have a significant impact on how much the model forgets. \n\n[Kemker et al., 2017]: Measuring Catastrophic Forgetting in Neural Networks\t\nRonald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, Christopher Kanan\t\nhttps://arxiv.org/abs/1708.02072\n\n[Kirkpatrick, 2016]: Overcoming catastrophic forgetting in neural networks\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, Raia Hadsell\nhttps://arxiv.org/abs/1612.00796 \n\nPlease refer to Kemker et al. for other references mentioned in my review, as they sum up some of the large body of work done in that space.\n"
        }
    ]
}