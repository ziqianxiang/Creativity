{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper examined a pure exploration method for efficient action value estimates in tabular reinforcement learning.  The paper is on the theoretical properties of value estimates in the large sample regime.  The method is shown to outperform baseline algorithms for this task in tabular reinforcement learning.\n\nThe reviewers were divided on the merits of this work.  The use of the central limit theorem was viewed as elegant, and the results were thought to be potentially useful.  However, the reviewers several limitations.  They found the assumption of a communicating MDP to be overly restrictive (reviewer 1).  The algorithm may be computationally inefficient (reviewer 2).  The nature of \"exploration\" in this work is not the conventional meaning in reinforcement learning (reviewer 3).\n\nThe paper is not yet ready for publication at ICLR.  The theoretical results do not clearly convey insights for reinforcement learning with function approximation, and the reviewers are also not in agreement that the current results are applicable to a general MDP setting.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper studies the inference problem of reinforcement learning. With a given exploration policy that satisfies some strong property to collect n data, the paper studies the distribution of the estimated optimal value function and Q-function when n goes to infinity. Both unique and non-unique optimal policy cases are studied. The non-unique case has a very different behavior as it is no longer Gaussian. The paper then uses these estimations to design a method that better explores and proposes a method Q-OCBA. Experiments were performed to compare this method with previous algorithms, e.g., UCRL.\n\nThe inference results for the Q-function is definitely important and will be useful for the community. But the authors should not ignore all the finite-sample bound results in their related work section, especially for those who achieving minimax rates. For instance: \nMG Azar, I Osband, R Munos, Minimax regret bounds for reinforcement learning, 2017\nMG Azar, R Munos, H Kappen, Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model, 2013\nA Sidford, M Wang, X Wu, L Yang, Y Ye, Near-optimal time and sample complexities for solving Markov decision processes with a generative model, 2018\nA Agarwal, S Kakade, L Yang, On the Optimality of Sparse Model-Based Planning for Markov Decision Processes\n\nPlease also see references therein. You can derive an inference result for the generative model as well. \n\nMore comments:\n* The assumption on exploration policy is quite restrictive. It essentially says the bound does not hold if the MDP is not communicating. However, the Q-function is still well defined.\n* The comparison to UCRL and other exploration-based method is not fair. In these results, the probability of picking \"the\" optimal policy is not important. Also, picking the correct action is not feasible if the problem is more complex. It is my understanding that your algorithm has a much larger regret due to that it requires to sample from every state.\n* What about finite horizon MDP?\n\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "This paper studies the asymptotic properties of action value function $Q$ and value function $V$. Specifically, the authors assume that we can collect $n$ data points $\\{(s_t,a_t,r_t(s_t,a_t),s_{t+1})\\}_{t=1}^{n}$. Based on the collected data, the authors calculated the sample mean of the unknown reward function $\\widehat r_n$ and transition probability $\\widehat{P}(\\cdot|s,a)$. They further defined an estimator $\\widehat Q_n$, which is the fixed point of the empirical Bellman operator $\\widehat{\\mathcal{T}}_n$ derived from $\\widehat r_n$ and $\\widehat P_n$. The authors proved under certain assumptions that $\\widehat Q_n\\rightarrow Q^*$ almost surely. Based on this argument, they also derived a similar convergence of value function $\\widehat V_n$ in distribution. Confidence intervals can also be established based on these results.\n\nThe proof of convergence in distribution is established based on the following idea: $\\widehat r_n$ and $\\widehat P_n$ converge to $r$ and $P$ by central limit theorem. By examining the Jacobian matrix of $\\widehat{\\mathcal{T}}_n$ with respect to $\\widehat Q$, it is found that the implicit function $\\widehat{ \\mathcal{T}}_n \\widehat Q -\\widehat Q=0$ can be solved in small neighborhoods of $\\widehat r_n, \\widehat P_n$ and $\\widehat Q_n$ as\n$$\n\\widehat{Q}_n = \\phi(\\widehat{r}_n, \\widehat P_n) \\qquad\\text{(implicit function theorem)}\n$$  \nThen by Slutsky theorem, $\\widehat Q_n = \\phi(\\widehat r_n, \\widehat P_n)$ also converges to $Q^*$ in distribution. The proof is straightforward and easy to follow. However, I have some concerns about the above analysis.\n\n1. The data $\\{(s_t,a_t,r_t(s_t,a_t),s_{t+1})\\}_{t=1}^{n}$ are collected by executing a fixed policy for n steps, which means the data points are not i.i.d. Therefore, the convergence of  $\\widehat r_n$ and $\\widehat P_n$ by central limit theorem may not hold ground, which further leads the later proofs potentially problematic.\n\n2. The $\\phi(\\cdot,\\cdot)$ function in implicit function theorem only exists in neighborhoods of $\\widehat r_n, \\widehat P_n$ and $\\widehat Q_n$. It is unclear from the current proof that $\\widehat Q_n$ fall into the same neighborhood of $Q^*$. Therefore, it needs to be justified that the limit point is $Q^*$.\n\n3. Even for the empirical estimator $\\widehat Q_n$, it is still the fixed point of $\\widehat{ \\mathcal{T}}_n$, which is hard to compute or solve. Thus Algorithm 1 may be inefficient in practice.\n\nOther comments:\n\nThe font and margin of this paper does not conform the format requirement of ICLR.\nWhat is $\\sigma_R(i,j)$ in Theorem 1 and Corollary 1? It seems to be undefined.\n\nI found it hard to read when the authors vectorize all the matrices and tensors to $N$ dimension vectors and $N\\times N$ matrices. In particular, the rearranged notations such $\\mu((i-1)m_a+j)$, $\\tilde P^{\\pi}((i-1)m_a+j,(i’-1)m_a+j’)$ are much more complicated that $\\mu(i,j)$ and $\\tilde P^{\\pi}(i’,j’|i,j)$. Similarly, the mapping $F(Q’,r’,P’)$ defined in the proof of Theorem 1 can be represented by the Bellman operator $\\mathcal{T}$ defined on page 3. \n\n====post rebuttal==\nI read other reviewers' comments and the author's response. I do not think the authors have addressed all my concerns. I will keep my rating. ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Efficient Inference and Exploration for Reinforcement Learning\n================================================================\n\nThis paper presents a pure-exploration algorithm for reinforcement learning.\nThe approach is based on an assymptotic analysis of the Q-values, and their convergence to central limit distribution.\nUsing this analysis, and under specific assumptions, the algorithm outperforms existing algorithms for exploration.\n\n\nThere are several things to like about this paper:\n- Many existing analyses for efficient exploration are all based around more-of-the-same concentration bounds, which end up being quite messy and un-elegant. This approach based on central limit theorem appears to be more insightful and cleaner and I think there is something nice about that!\n- The proposed algorithm is reasonable, for the setting in question, and the experimental results show that it outperforms benchmark exploration algorithms in this setting.\n- The general structure of the paper, quality of writing and technical rigour appears to be of good standard... although I did not check all technical details carefully. [The paper is 24 pages long and we have many reviews]\n\n\nHowever, there are also some significant places where this paper falls short:\n- The problem setting that the authors consider is really not typical of the \"exploration\" problem in RL... I'm not talking about the fact that this is a \"pure exploration\" algorithm (that's fine), but instead that Assumption 3 is really not a good model for the types of problems that are \"hard\" for exploration in RL! For example, in the RiverSwim problem choosing an exploration policy = 0.8 right is essentially saying that you've already solved the hard part of the problem. Note - I am a little bit confused about the experiments in Tables 3 and 4, here it seems that you start with a pi(1|s)=0.6 which again feels like a cheat...\n- Would it be possible to compare this algorithm in a more like-for-like standard RL setting, perhaps using the standardised bsuite https://github.com/deepmind/bsuite (the \"deep sea\" problems might be of particular interest here.)\n- Alternatively, I can imagine a future version of the paper being more upfront about this deviation from the \"standard\" setting and highlighting that this is a special-type of result quite different from typical exploration in RL.\n- I'm not sure that this sort of paper is well-served by a conference like ICLR... certainly there seems very little of \"learning representation\" in this discussion of Tabular RL. That would sort of be OK if the paper made nods to how these *insights* could carry over the deep learning or at least RL with (linear) function approximation... I don't see much of that.\n\n\nOverall I do think this is an interesting paper, with a novel approach to pure exploration in tabular MDPs under specific assumptions.\nHowever, I'm not sure that this paper is well-suited to ICLR and I have some concerns about whether it really does address the sort of \"exploration\" problem in RL that one might expect."
        }
    ]
}