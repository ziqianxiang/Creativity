{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposes two semi-supervised (projective dependency) parsing models, which are first-order graph-based (only head-modiffier scores are used). Both models, local and global, are auto-encoders. The latent variable in the local model is to represent words, thus the model generates trees sequentially. The latent variable in the global model is to present a whole tree, thus the model can consider all the dependency scores at once. One of the key contributions is that, thanks to the first order property, the authors can make use of available dynamic programming algorithms to compute expectations tractably. The experiments show that both the models work across several languages. \n\nI would accept the paper because: \n- it's well written, with clear motivations \n- the idea of using dynamic programming to compute the global model tractably is thoughtful and innovative. The math details seem correct to me (though I didn't check them carefully)\n- the experimental results do support that the semi-supervised learning does work, and are consistent across several languages. \n\nI would reject the paper because: \n- technically speaking, tractability is very cool, but in this case the global model has to sacrifice the expressiveness of higher-order parsing. It's very unclear to me if that's worth or not. The experimental results do not say anything. \n- I don't know why the authors split 10% - 90% for labelled and unlabelled sentences. The gain (L+U vs L) seems not impressive to me. It is also unclear to me where the gain is from. I suggest to adjust this ratio (e.g. the portion of labeled sentences if from 1% to 100%)\n\nTypos: \n- Algorithm 1 and the text should be consistent with each other for the use of the symbol of the encoder's parameters. \n\nQuestion: \n- The two models are graph-based, thus they not necessarily projective. I see why the projectivity is needed for the variant of inside-outside algorithm. But I'm wondering if there are any dynamic programming algorithms for graph-based non-projective parsing?  "
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This work proposes Local Auto-encoding Parser and a Global Auto-encoding Parser for semi-supervised graph-based dependency parsing. The experimental results show that the proposed methods are effective.\n\nThe novelty of this work is limited especially with unclear motivations.\n\n1. The methods proposed in this work are incremental. It seems some minor changes are make to VAE framework. Since the length of this paper is over 8 pages, the novelty in this work is not enough especially under higher standards.\n\n2. The motivation of using global and local auto-encoding parser is not clear to me. The authors proposed to encode each word in local auto-encoder and encode whole sentence in global auto-encoder. But why it takes two encoders is not well explained in the paper.\n\n3. The performance improvements shown in experimental parts (Table 1 and 2) are very marginal. It is hard to attribute the improvements to the proposed methods or training tricks. \n\nSuggestions:\n\n1. The motivations of the proposed methods should be well explained.\n\n2. Better experimental results will make it more convincing."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper contains two semi-supervised dependency parsing algorithms. The two are end-to-end autoencoding models. For one the latent variables are sequences and for the other the latent variables are dependency trees. Experimental results are given showing that unlabeled data help to slightly improve performance. One main contribution is to improve over the algorithm by Corro and Titov at ICLR last year. For this, the paper contains a tractable algorithm for computing the optimal dependency tree structure.\n\nThe paper is not easy to read. A number of important details are not given, either omitted or to be found in references. My main concern is that the main contribution is left in Appendix and that the Appendix does not help much to understand why the assertion stands. Also, using the Eisner algorithm has the drawback that the GAP method is no more end to end. Therefore I am inclined to reject the paper.\n\nDetailed comments and questions.\n\n* First contribution. The paper said that the two methods provide a trade-off speed vs accuracy. But the paper does not consider this question. It should be nice to compare the complexity, both theoretically and experimentally, of the two methods.\n* Second contribution. As said before here is my main concern. The tractable inference is the topic of Section 6.5. But the few lines do not help and refer to the Appendix. The appendix does not help much. It seems that you use a variant of the Eisner algorithm but what are the differences? Also, it is not clear how is computed the transformed scoring matrix $S'$? And why this transformed matrix allows to compute the optimal dependency tree for unlabeled data? The GAP model is no more end-to-end because of using the (a variant of the) Eisner algorithm.\n* Second contribution. It should be nice (in relation to the first item) to give the complexity of the algorithm. The paper should compare time efficiency for the GAP algorithm and the Corro and Titov's algorithm. \n* Related work. Kim et al 2019b is cited two times. You should discuss this paper and recent new developments in unsupervised parsing.\n* Section 3.1. It is not clear whether you use pre-trained word embeddings. For instance, in the Kiperwasser and Goldberg's paper, the distinction between using internal word embeddings and pre-trained word embeddings is made clear with improved performance for the latter. Also, the use of POS tag embeddings is not done in other systems. Please explain why the method uses such embeddings and discuss whether they improve performance. I do not understand the last paragraph of Section 3.1.\n* Section 6.5. Should be an important part of the paper.\n* Section 7. The paper should made explicit the hyperparameters and their default value. It is not clear whether the development set was used. \n* Section 7.1. I do not understand the sentence \"in GAP using POS to POS decoder only yield the satisfactory performance\".\n* Section 7.2. As said before a comparison between the computation time of GAP and Corro and Titov's algorithm should be given.\n* The paper does not consider recent improvements for word embeddings such as ELMo (Peters et al 2018) and BERT (Devlin et al 2018). I wonder whether using such word embeddings would improve greatly dependency parsing algorithms.\n\nTypos.\n* Section 3, l2, a special root token\n* Section 3, l2, length of $l$\n"
        }
    ]
}