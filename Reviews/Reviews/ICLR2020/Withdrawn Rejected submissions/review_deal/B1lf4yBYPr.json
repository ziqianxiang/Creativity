{
    "Decision": "",
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper introduces a metric (called trust score) that measures how well a model is at performing multiple tasks. Further, the authors propose training the classifier in a way that minimizes the loss on a desire task while simultaneously maximizing the loss on random tasks (randomly generated labels). \n\nThere are a number of issues with this paper:\n\n1. The authors claim that models that generate data representations (features) that are capable of capturing multiple learning tasks are bad for privacy. This claim is wrong for the following reasons: (a) if the model is used on-device, then the data & model decisions are hidden from service providers and there’s no privacy violation; (b) if the model is used in the cloud as a machine learning service, then the users have to send their data to the service provide, and the problem here would be that the service provider has access to the data (and not that the models are capable of learning sensitive attributes). This is why researchers in this field have focused on one of two directions: (1) designing approaches for creating representations of the data that do not contain sensitive attributes (such representations could then be shared with a service provider or published), and (2) ensuring that a model is “fair” with respect to protected attributes (say race and/or gender) — i.e., the performance of the model does not vary across different populations/groups. (Note: privacy is usually about training models on user data such that an attacker observing the model cannot recover the training data or discover who participated in the training of the model. This is typically achieved via differential privacy.) \n2. The way the trust score is calculated is unclear and poorly presented. (a) What do the authors mean by |M - T|? The determinant of M - T? The entry-wise absolute value? (b) It’s also unclear how W is computed. If \\mathbb{1}_5 is a 5x5 matrix of ones and \\mathcal{I}_5 is the 5x5 identity matrix, then W is simple 4x\\mathbb{1}_5, which clearly is not the case. So what is \\mathbb{1}_5? (c) What is \\Sigma in Equation (2)? A summation?  (d) More importantly, the authors claim that an ideal classifier that does not capture suppressed tasks has a trust score equal to one. However, in this case T ~= M, so M - T ~= 0, and therefore |M - T| ~= 0 (regardless of how |.| is defined) and the trust score must actually be small. The way the trust score is computed needs to be properly explained.\n3. The authors claim that their proposed approach (maximizing the loss of randomly generated labels) does not require knowing the tasks to be suppressed. This is true. However: (a) you cannot compute the trust score of a classifier on a dataset without knowing the tasks that need to be suppressed (so you cannot verify whether or not the proposed technique is actually helpful in practice), and (b) this technique (unfortunately) hurts the performance of the model on the desired task.\n4. The paper makes no attempt to properly survey the literature on learning representations under censorship and fairness constraints. For example, they have not referenced and compared against: (a) Censoring Representations with an Adversary (https://arxiv.org/abs/1511.05897), (b) Learning Adversarially Fair and Transferable Representations (https://arxiv.org/abs/1802.06309), (c) Context-Aware Generative Adversarial Privacy (https://arxiv.org/abs/1710.09549), (d) Learning Generative Adversarial RePresentations (GAP) under Fairness and Censoring Constraints (https://arxiv.org/abs/1910.00411), and many others.\n\nWithout a clear story and a compelling argument for why we need to design models that aren’t capable of capturing multiple multiple downstream tasks, I cannot convince myself that suppressing unwanted tasks is valuable. Rather, I believe the authors should focus on training the models in a way (a) that makes their decisions “uncorrelated” or “conditionally uncorrelated” with respect to unwanted/undesired tasks (to suppress unfairness) , or (b) that ensures that the data of users who participated in the training model is not leaked to an attacker who has a white-or black-box access the model.\n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper claims to propose a novel framework to measure the trust score of a trained DL model and a solution approach to improve the trust score during training. It provides a new dataset. \n\nThe writing of the paper needs to be improved. The technical contribution is minimal.\n\nIt is unclear what is the goal of the paper.  Is it suppressing additional task learning or privacy preserving? The problem setting does not seem to make sense. For privacy preserving, it makes sense to process the data with privacy preserving techniques and then provide the processed data to the prediction/classification model. What is the point of providing the original data to the classification model and let the classification model perform privacy preserving?  In such case, who is the target you protect the information from?\n\nThe work also assumes the knowledge of suppressing tasks, which is not very practical.\n\nThe trust score is self-defined. It does not seem to impact the classification model training. What is the point of performing evaluation in terms of such a trust score?\n\nThe experimental study is insufficient and unconvincing without comparison to related works. \n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "In this paper, the authors target on an very interesting problem of how to avoid the network learning certain tasks which we do not want it to learn. I understand the problem is very important. The authors adopt gradient reversal branches which can solve both known classes and unknown classes. Experiments on several dataset are conducted to show the effectiveness of the method.\n\nThe paper is not easy for me to understand. I carefully read this paper for a long time and I am still confused about the key details about equation (1).\n\nFollowing are some of my questions:\n(1) f: X->Z, g: Z->Y, so what does f(g(x)) mean in equation (1),  should it be g(f(x))?\n(2) I do not quite understand the loss function. Why could the model learn to avoid recognize certain tasks since the parameters can be set to zero in g, in this way,  the output of g(.) will have no information about f(x) anymore.\n(3) In my understanding, the model should try to learn in an adversarial way in order to suppress the learning of unwanted tasks, however, I fail to find the correspond introductions in this paper.\n(4) Although the equation(2) could be guessed, the authors should make it more clear. There is Sigma in the equation and no indices of summation details provided.\n\nIn my opinion, the paper should be written more clearly and the key idea and the importance of this paper should be further emphasized. I could not support its acceptance under current form."
        }
    ]
}